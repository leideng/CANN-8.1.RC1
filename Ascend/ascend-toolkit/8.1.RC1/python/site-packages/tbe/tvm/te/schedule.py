# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
# pylint: disable=unused-import
"""The computation schedule api of TVM."""
from numbers import Number
import tvm._ffi
from tvm._ffi.base import string_types
from tvm.runtime import Object, convert, const
from tvm.ir import container as _container
from tvm.tir import IterVar, Buffer, Call
from tvm.tir import Simplify, CanonicalSimplify
from tvm.error_mgr import raise_tbe_python_err, TBE_DEFAULT_PYTHON_ERROR_CODE
from . import tensor as _tensor
from tvm.tir import expr as _expr
from . import _ffi_api
from .schedule_extended import BindArgPurpose


@tvm._ffi.register_object
class Split(Object):
    """Split operation on axis."""


@tvm._ffi.register_object
class Fuse(Object):
    """Fuse operation on axis."""


@tvm._ffi.register_object
class Singleton(Object):
    """Singleton axis."""


@tvm._ffi.register_object
class Instruction(Object):
    """Primitive Instruction"""


def create_schedule(ops, use_trace=False):
    """Create a schedule for list of ops

    Parameters
    ----------
    ops : list of Operations
        The source expression.

    Returns
    -------
    sch : schedule.Schedule
        The created schedule.
    """
    if not isinstance(ops, (list, _container.Array)):
        ops = [ops]
    return _ffi_api.CreateSchedule(ops, use_trace)


@tvm._ffi.register_object
class Schedule(Object):
    """Schedule for all the stages."""

    def __getitem__(self, k):
        if isinstance(k, _tensor.Tensor):
            k = k.op
        if not isinstance(k, _tensor.Operation):
            raise ValueError("Expect schedule key to be Tensor or Operation")
        if k not in self.stage_map:
            raise ValueError(
                "Cannot find the operation %s in schedule" % (str(k)))
        stage = self.stage_map[k]
        _ffi_api.StageUpdateSchedule(stage, self)
        return stage

    def copy(self):
        """Get a copy of current schedule

        Returns:
        -------
        sch : Schedule
            a copy of current schedule
        """
        return _ffi_api.ScheduleCopy(self)

    def normalize(self):
        """Build a normalized schedule from the current schedule.

        Insert necessary rebase to make certain iter var to start from 0.
        This is needed before bound inference and followup step.

        Returns
        -------
        sch : Schedule
            The normalized schedule.
        """
        return _ffi_api.ScheduleNormalize(self)

    def create_group(self, outputs, inputs, include_inputs=False):
        """Create stage group by giving output and input boundary.

        The operators between outputs and inputs are placed as member of group.
        outputs are include in the group, while inputs are not included.

        Parameters
        ----------
        outputs : list of Tensors
            The outputs of the group.

        inputs : list of Tensors
            The inputs of the group.

        include_inputs : boolean, optional
            Whether include input operations in the group if they are used by outputs.

        Returns
        -------
        group : Stage
            A virtual stage represents the group, user can use compute_at to move
            the attachment point of the group.
        """
        if isinstance(outputs, _tensor.Tensor):
            outputs = [outputs]
        if isinstance(inputs, _tensor.Tensor):
            inputs = [inputs]
        return _ffi_api.ScheduleCreateGroup(self, outputs, inputs, include_inputs)

    def cache_read(self, tensor, scope, readers, layout_change=False):
        """Create a cache read of original tensor for readers.

        This will mutate the body of the readers.
        A new cache stage will be created for the tensor.
        Call this before doing any split/fuse schedule.

        Parameters
        ----------
        tensor : Tensor
            The tensor to be cached.
        scope : str
            The scope of cached
        readers : list of Tensor or Operation
            The readers to read the cache.

        Returns
        -------
        cache : Tensor
            The created cache tensor.
        """
        if isinstance(readers, (_tensor.Tensor, _tensor.Operation)):
            readers = [readers]
        readers = [t.op if isinstance(
            t, _tensor.Tensor) else t for t in readers]
        return _ffi_api.ScheduleCacheRead(self, tensor, scope, readers, layout_change)

    def cache_write(self, tensor, scope, tail_strategy="round_up_pad", need_transpose=False, dst_shape=[]):
        """Create a cache write of original tensor, before storing into tensor.

        This will mutate the body of the tensor.
        A new cache stage will created before feed into the tensor.

        This function can be used to support data layout transformation.
        If there is a split/fuse/reorder on the data parallel axis of tensor
        before cache_write is called. The intermediate cache stores
        the data in the layout as the iteration order of leave axis.
        The data will be transformed back to the original layout in the original tensor.
        User can further call compute_inline to inline the original layout and keep
        the data stored in the transformed layout.

        Parameters
        ----------
        tensor : Tensor, list or tuple
            The tensors to be feed to. All the tensors must be produced by one computeOp
        scope : str
            The scope of cached
        tail_strategy : String
            The strategy of the tail for padding value.
        need_transpose : bool
            Whether the shape needs to be transposed.
        dst_shape : List of int
            Subscript order of target axis.

        Returns
        -------
        cache : Tensor
            The created cache tensor.
        """
        if need_transpose:
            ori_stage = self[tensor]
            if isinstance(ori_stage.op, _tensor.ScanOp):
                transpose = _ffi_api.ScheduleCacheTransposeWrite(
                    self, tensor, dst_shape, scope, tail_strategy)
            else:
                new_order = [ori_stage.op.axis[dst_shape[i]]
                             for i in range(len(dst_shape))]
                ori_stage.reorder(*new_order)
                transpose = self.cache_write(tensor, scope, tail_strategy)
            return transpose
        else:
            return _ffi_api.ScheduleCacheWrite(self, tensor, scope, tail_strategy)

    def cache_read_sparse(self, tensor, scope, readers):
        """Create a cache sparse read of original tensor for the readers.

          This will mutate the body of the readers.
          A new cache stage will be created for the sparse tensor.
          Call this before doing any split/fuse schedule.

          Parameters
          ----------
          tensor : Tensor
              The tensor with sparse axis to be cached.
          scope : str
              The scope of cached
          readers : list of Tensor or Operation
              The readers to read the cache.
          Returns
          -------
          cache : Tensor
              The created cache tensor.
          """
        if isinstance(readers, (_tensor.Tensor, _tensor.Operation)):
            readers = [readers]
        readers = [t.op if isinstance(
            t, _tensor.Tensor) else t for t in readers]
        return _ffi_api.ScheduleCacheReadSparse(self, tensor, scope, readers)

    def cache_write_sparse(self, tensor, scope, tail_strategy="round_up_pad"):
        """Create a cache sparse write of original tensor, before storing into tensor.

        This will mutate the body of the tensor with sparse axis.
        A new cache stage will created before feed into the tensor.

        Parameters
        ----------
        tensor : Tensor with sparse axis, list or tuple
            The tensors to be feed to. All the tensors must be produced by one computeOp
        scope : str
            The scope of cached

        Returns
        -------
        cache : Tensor
            The created cache tensor.
        """
        return _ffi_api.ScheduleCacheWriteSparse(self, tensor, scope, tail_strategy)

    RfactorModeOverlap = "overlap"
    RfactorModeNormal = "normal"

    def rfactor(self, tensor, axis, factor_axis=0, mode=RfactorModeOverlap):
        """ Factor a reduction axis in tensor's schedule to be an explicit axis.

        This will create a new stage that generated the new tensor with axis
        ----------
        tensor : Tensor
            The tensor to be factored.
        axis : IterVar or IterVar list
            The reduction axis in the schedule to be factored.
        factor_axis : int or int list
            The position where the new axis is placed.
        mode: string
            RfactorModeOverlap:
                before:
                B[i.c, j.c] = A[i.c, rfactor_axis, j.c, red_axis]
                after:
                B.rf[i.c, rfactor_axis, j.c] = A[i.c, rfactor_axis, j.c, red_axis]
                B[i.c, j.c] = B.rf[i.c, rfactor_axis, j.c]
            RfactorModeNormal:
                before:
                B[i.c, j.c] = A[i.c*2 + rfactor_axis, j.c*2 + red_axis]
                after:
                B.rf[i.w, j.c] = A[i.w, j.c*2 + red_axis]
                B[i.c, j.c] = B.rf[i.c*2 + rfactor_axis, j.c]

        Returns
        -------
        tfactor : Tensor or Array of Tensor
            The created factored tensor.
        """
        if mode not in [self.RfactorModeOverlap, self.RfactorModeNormal]:
            raise raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                       "rfactor mode only support RfactorModeOverlap and RfactorModeNormal")

        if isinstance(axis, list):
            if isinstance(factor_axis, list):
                factored = _ffi_api.ScheduleRFactor(
                    self, tensor, axis, factor_axis, mode)
                return factored[0] if len(factored) == 1 else factored

            factor_axises = [factor_axis for _ in range(len(axis))]
            factored = _ffi_api.ScheduleRFactor(
                self, tensor, axis, factor_axises, mode)
            return factored[0] if len(factored) == 1 else factored

        if isinstance(factor_axis, list):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "factor axis cant be a list when only one axis")
        factored = _ffi_api.ScheduleRFactor(
            self, tensor, [axis], [factor_axis], mode)
        return factored[0] if len(factored) == 1 else factored

    def sfactor(self, tensor, axis, add_dim=0):
        """ Factor a sort axis in tensor's schedule to be an explicit axis.

        This will create a new stage that generated the new tensor.
        The tensor's body will be rewritten as a sort tensor
        over the factored tensor.

        Parameters
        ----------
        tensor : Tensor
            The tensor to be factored.
        axis : IterVar
            The sort axis in the schedule to be factored.
        add_dim : Int32
            The proposal axis size

        Returns
        -------
        tfactor : Tensor or Array of Tensor
            The created factored tensor.
        """
        factored = _ffi_api.ScheduleSFactor(self, tensor, axis, add_dim)
        return factored[0] if len(factored) == 1 else factored

    def cache_clone(self, tensor, scope, readers):
        """Create a cache clone of original tensor for readers.

        This will mutate the body of the readers.
        A new cloned cache stage will be created for the tensor.
        Call this before doing any split/fuse schedule.

        Parameters
        ----------
        tensor : Tensor
            The tensor to be cached.
        scope : str
            The scope of cached
        readers : list of Tensor or Operation
            The readers to read the cache.

        Returns
        -------
        cache : Tensor
            The created cache tensor.
        """
        if isinstance(readers, (_tensor.Tensor, _tensor.Operation)):
            readers = [readers]
        readers = [t.op if isinstance(
            t, _tensor.Tensor) else t for t in readers]
        return _ffi_api.ScheduleCacheClone(self, tensor, scope, readers)

    def create_block_sync(self):
        """Compute the current stage via double buffering.

        This can only be applied to intermediate stage.
        This will double the storage cost of the current stage.
        Can be useful to hide load latency.
        """
        return _ffi_api.ScheduleCreateBlockSync(self)

    def get_block_sync_size(self):
        """Compute the current stage via double buffering.

        This can only be applied to intermediate stage.
        This will double the storage cost of the current stage.
        Can be useful to hide load latency.
        """
        return _ffi_api.ScheduleGetBlockSyncSize(self)

    def set_var_value(self, var, value):
        var = Simplify(var) if isinstance(var, _expr.PrimExprWithOp) else var
        if not isinstance(var, _expr.Var):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Variable needed, not %s" % type(var))
        if isinstance(value, Number):
            value = const(value, var.dtype)
        else:
            value = convert(value)
        if not isinstance(value, _expr.PrimExprWithOp):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Expr needed, not %s" % type(var))
        value = Simplify(value)

        if var.dtype != value.dtype:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "set_var_value need var: ({})'s dtype {} same as value: ({})'s dtype {}".format(
                                     var, var.dtype, value, value.dtype))

        _ffi_api.ScheduleSetVarValue(self, var, value)

    def set_var_range(self, var, lower_bound=None, upper_bound=None):
        var = Simplify(var) if isinstance(var, _expr.PrimExprWithOp) else var
        if not isinstance(var, _expr.Var):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Variable needed, not %s" % type(var))
        MAX_INT_VALUE = 2 ** 31 - 1
        MIN_INT_VALUE = -2 ** 31
        if var.dtype == 'int64':
            MAX_INT_VALUE = 2 ** 63 - 1
            MIN_INT_VALUE = -2 ** 63
        lower_bound = MIN_INT_VALUE if lower_bound is None else lower_bound
        upper_bound = MAX_INT_VALUE if upper_bound is None else upper_bound
        upper_bound = upper_bound - \
                      1 if lower_bound == 0 and upper_bound == 2 ** 63 - 1 else upper_bound
        if upper_bound >= 2 ** 63 - 1 + lower_bound:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Extent Out of bound: (-2**63, 2**63 - 1)")
        if not isinstance(lower_bound, int):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "lower_bound should be int or None, but got %s" % type(lower_bound))
        if not isinstance(upper_bound, int):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "upper_bound should be int or None, but got %s" % type(upper_bound))
        if lower_bound > upper_bound:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "lower_bound should not greater than upper_bound")
        _ffi_api.ScheduleSetVarRange(self, var, lower_bound, upper_bound)

    def set_constraint(self, constraint):
        if not isinstance(constraint, _expr.PrimExprWithOp):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Constraint should be Expr, not %s" % type(constraint))
        _ffi_api.ScheduleSetConstraint(self, constraint)

    def compute_with(self, group, loop_level):
        """fuse the group tensor computation under certain common axes
        Parameters
        ----------
        group : list(Tensor)
            the group tensor to be fused
        loop_level : int
            the number of common axes
        """
        if loop_level <= 0:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "invalid loop level %d" % loop_level)
        _ffi_api.ScheduleComputeWith(self, group, loop_level)

    def sequential_malloc(self, scope):
        _ffi_api.ScheduleDisableAllocate(self, scope)

    def bind_axes(self, ivar_list, thread_ivar):
        _ffi_api.ScheduleBindAxes(self, ivar_list, thread_ivar)

    def shift_block_access(self, block_axes, block_axis_extents, shift_axis, overlap_value=[]):
        """bind block var for shift.

        Parameters
        ----------
        block_axes : Array<IterVar>
            The var to be bind.
        block_axis_extents : Array<PrimExpr>
            The extent of var to be bind.
        shift_axis : IterVar
            The axis to shift.
        overlap_value : Array<PrimExpr>
            The condition of Checking whether the cores are overlapped.
        """
        _ffi_api.ScheduleShiftBlockAccess(self, block_axes, block_axis_extents,
                                          shift_axis, overlap_value)

    def cache_reshape(self, ori_shape, scope, readers, new_shape=None, axis_reshape=None):
        """ Create a new tensor after reshape.

        This will create a new tensor
        Convert the specified tensor to a tensor of a specific dimension.
        1.Split the original shape by the specified axis.
        2.Split the original shape according to the new shape of the input.

        Parameters
        ----------
        ori_shape : Tensor
            The tensor to be reshaped.
        scope : String
            The scope of reshaped.
        readers : List of Tensor or Operation
            The readers to cache_reshape.
        new_shape : List of int
            Target axis order.
        axis_reshape : Dict {key:axis value: list}
            Specify the split for a specific axis.
        Returns
        -------
        reshape : Tensor
            The reshaped tensor.
        """
        if axis_reshape != None and new_shape != None:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "axis_reshape and new_shape cannot both be non-NULL")
        if axis_reshape != None:
            for i in axis_reshape:
                now_axis = i
                split_axis = 1
                for j in range(len(axis_reshape[i])):
                    if isinstance(axis_reshape[i][j], int):
                        if axis_reshape[i][j] <= 0:
                            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                                 "The axis of the cut must be greater than 0")
                        split_axis = split_axis * axis_reshape[i][j]
                        if j == len(axis_reshape[i]) - 1 and int(i.dom.extent) != split_axis:
                            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                                 "The product of the split axes should be the same size as the original axes")
                    if isinstance(axis_reshape[i][j], float):
                        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                             "Split axis must be an integer")
                for j in range(len(axis_reshape[i]) - 1):
                    db_o, db_i = self[ori_shape].split(
                        now_axis, factor=axis_reshape[i][len(axis_reshape[i]) - j - 1])
                    now_axis = db_o
        else:
            if not isinstance(new_shape, list):
                raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                     "new_shape is of type non-list")
            ori_axis = 1
            for i in range(len(ori_shape.shape)):
                ori_axis = ori_axis * ori_shape.shape[i]
            split_axis = 1
            for i in range(len(new_shape)):
                if isinstance(new_shape[i], int):
                    if new_shape[i] <= 0:
                        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                             "The axis of the cut must be greater than 0")
                    split_axis = split_axis * new_shape[i]
                    if i == len(new_shape) - 1 and split_axis != int(ori_axis):
                        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                             "The product of the split axes should be the same size as the original axes")
                if isinstance(new_shape[i], float):
                    raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                         "Split axis must be an integer")
            args = [self[ori_shape].op.axis[i]
                    for i in range(len(self[ori_shape].op.axis))]
            now_axis = self[ori_shape].fuse(*args)
            for i in range(len(new_shape) - 1):
                db_o, db_i = self[ori_shape].split(
                    now_axis, factor=new_shape[len(new_shape) - i - 1])
                now_axis = db_o
        reshape = self.cache_read(ori_shape, scope, readers, True)
        return reshape

    def cache_transpose(self, ori_tensor, dst_shape, scope, readers):
        """ Create a new tensor after transpose.

        This will create a new tensor
        Reverse or arrange the axes of a Tensor.

        Parameters
        ----------
        ori_tensor : Tensor
            The tensor to be transposed.
        dst_shape : List of int
            Subscript order of target axis.
        scope : String
            The scope of transposed.
        readers : List of Tensor or Operation
            The readers to cache_transpose.
        mode : String
            cache_read or cache_write, mark mode of tranpose tensor
        Returns
        -------
        reshape : Tensor
            The transposed tensor.
        """
        if isinstance(readers, (_tensor.Tensor, _tensor.Operation)):
            readers = [readers]
        readers = [t.op if isinstance(
            t, _tensor.Tensor) else t for t in readers]
        # normalize dst_shape
        size = len(dst_shape)
        dst_shape = [size + i if i < 0 else i for i in dst_shape]
        dst_shape = [convert(i) for i in dst_shape]
        transpose = _ffi_api.ScheduleCacheTranspose(
            self, ori_tensor, dst_shape, scope, readers)
        return transpose

    def get_trace(self):
        """Get traces of current schedule procedure and 
           RL will use these infos to rebuild schedule procedure 
        """
        return _ffi_api.ScheduleGetTrace(self)

    def memset(self, axis, value, scope, pre_assign=True):
        """ Set the memory scope to certain value

        Parameters
        ----------
        axis : IterVar
            The axis to insert memset operator
        value : PrimExpr
            The value to be filled
        scope : String
            The memory scope that need memset
        pre_assign : Boolean
            Whether memset operator before or after axis loop
        """
        return _ffi_api.ScheduleMemset(self, axis, value, scope, pre_assign)

    def bind_args(self, tensor, value, purpose):
        """ Bind A tensor to certain purpose

        Paramters
        ---------
        tensor : Tensor
            The tensor that need to bind value to achieve a certain purpose
        value : PrimExpr
            The expression that determine whether you enable the bind value
        purpose : BindArgPurpose
            The purpose of binding value
        """
        if not isinstance(purpose, BindArgPurpose):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "purpose %s is not defined in BindArgPurpose" % purpose)
            
        if not isinstance(tensor, (list, _container.Array)):
            tensor = [tensor]

        return _ffi_api.ScheduleBindArgs(self, tensor, value, purpose.value)

    def stage_reorder(self, stage1, stage2):
        """ stage reorder, affect execution order, only reorder two stage

        Paramters
        ---------
        stage1 : Stage
            the order first stage
        stage2 : Stage
            the order second stage
        """
        return _ffi_api.ScheduleReorderStage(self, stage1, stage2)


@tvm._ffi.register_object
class Stage(Object):
    """A Stage represents schedule for one operation."""

    def split(self, parent, factor=None, nparts=None, tail_strategy='guard_with_if'):
        """Split the stage either by factor providing outer scope, or both

        Parameters
        ----------
        parent : IterVar
             The parent iter var.

        factor : Expr, optional
             The splitting factor

        nparts : Expr, optional
             The number of outer parts.

        Returns
        -------
        outer : IterVar
            The outer variable of iteration.

        inner : IterVar
            The inner variable of iteration.
        """
        if nparts is not None:
            if factor is not None:
                raise ValueError(
                    "Do not need to provide both outer and nparts")
            outer, inner = _ffi_api.StageSplitByNParts(
                self, parent, nparts, tail_strategy)
        else:
            if factor is None:
                raise ValueError("Either nparts or factor need to be provided")
            outer, inner = _ffi_api.StageSplitByFactor(
                self, parent, factor, tail_strategy)
        return outer, inner

    def fuse(self, *args):
        """Fuse multiple consecutive iteration variables into a single iteration variable.

        fused = fuse(...fuse(fuse(args[0], args[1]), args[2]),..., args[-1])
        The order is from outer to inner.

        Parameters
        ----------
        args : list of IterVars
            Itervars that proceeds each other

        Returns
        -------
        fused : IterVar
            The fused variable of iteration.
        """
        fused = _ffi_api.StageFuse(self, args)
        return fused

    def set_scope(self, scope):
        """Set the thread scope of this stage

        Parameters
        ----------
        scope : str
            The thread scope of this stage
        """
        return _ffi_api.StageSetScope(self, scope)

    def bind(self, ivar, thread_ivar):
        """Bind ivar to thread index thread_ivar

        Parameters
        ----------
        ivar : IterVar
            The iteration to be binded to thread.

        thread_ivar : IterVar
            The thread to be binded.
        """
        _ffi_api.StageBind(self, ivar, thread_ivar)

    def env_threads(self, threads):
        """Mark threads to be launched at the outer scope of composed op.

        Parameters
        ----------
        threads : list of threads
            The threads to be launched.
        """
        if isinstance(threads, IterVar):
            threads = [threads]
        _ffi_api.StageEnvThreads(self, threads)

    def set_store_predicate(self, predicate, partition=False, rebase_root=False):
        """Set predicate under which store to the array can be performed.

        Use this when there are duplicated threads doing the same store and we only
        need one of them to do the store.

        Parameters
        ----------
        predicate : Expr
            The guard condition fo store.
        """
        if isinstance(predicate, list):
            _ffi_api.StageSetStorePredicateList(self, predicate,
                                                partition, rebase_root)
        else:
            _ffi_api.StageSetStorePredicate(self, predicate,
                                            partition, rebase_root)

    def compute_at(self, parent, scope):
        """Attach the stage at parent's scope

        Parameters
        ----------
        parent : Stage
            The parent stage

        scope : IterVar
            The loop scope t be attached to.
        """
        _ffi_api.StageComputeAt(self, parent, scope)

    def reverse_compute_at(self, child, scope):
        """Attach the stage at child's scope

        Parameters
        ----------
        child : Stage
            The child stage

        scope : IterVar
            The loop scope t be attached to.
        """
        _ffi_api.StageReverseComputeAt(self, child, scope)

    def compute_inline(self, instant=False):
        """Mark stage as inline

        Parameters
        ----------
        parent : Stage
            The parent stage
        instant : Bool
            whether compute inline do instantly
        """
        _ffi_api.StageComputeInline(self, instant)

    def compute_root(self):
        """Attach the stage at parent, and mark it as root

        Parameters
        ----------
        parent : Stage
            The parent stage
        """
        _ffi_api.StageComputeRoot(self)

    def reorder(self, *args):
        """reorder the arguments in the specified order.

        Parameters
        ----------
        args : list of IterVar
            The order to be ordered
        """
        _ffi_api.StageReorder(self, args)

    def tile(self, x_parent, y_parent, x_factor, y_factor):
        """Perform tiling on two dimensions

        The final loop order from outmost to inner most are
        [x_outer, y_outer, x_inner, y_inner]

        Parameters
        ----------
        x_parent : IterVar
            The original x dimension
        y_parent : IterVar
            The original y dimension
        x_factor : Expr
            The stride factor on x axis
        y_factor : Expr
            The stride factor on y axis

        Returns
        -------
        x_outer : IterVar
            Outer axis of x dimension
        y_outer : IterVar
            Outer axis of y dimension
        x_inner : IterVar
            Inner axis of x dimension
        p_y_inner : IterVar
            Inner axis of y dimension
        """
        x_outer, y_outer, x_inner, y_inner = _ffi_api.StageTile(
            self, x_parent, y_parent, x_factor, y_factor
        )
        return x_outer, y_outer, x_inner, y_inner

    def vectorize(self, var):
        """Vectorize the iteration.

        Parameters
        ----------
        var : IterVar
            The iteration to be vectorize
        """
        _ffi_api.StageVectorize(self, var)

    def tensorize(self, var, tensor_intrin):
        """Tensorize the computation enclosed by var with tensor_intrin

        Parameters
        ----------
        var : IterVar
            The iteration boundary of tensorization.

        tensor_intrin : TensorIntrin
            The tensor intrinsic used for computation.
        """
        _ffi_api.StageTensorize(self, var, tensor_intrin)

    def unroll(self, var):
        """Unroll the iteration.

        Parameters
        ----------
        var : IterVar
            The iteration to be unrolled.
        """
        _ffi_api.StageUnroll(self, var)

    def parallel(self, var):
        """Parallelize the iteration.

        Parameters
        ----------
        var : IterVar
            The iteration to be parallelized.
        """
        _ffi_api.StageParallel(self, var)

    def pragma(self, var, pragma_type, pragma_value=None, pragma_tensor=None):
        """Annotate the iteration with pragma

        This will translate to a pragma_scope surrounding
        the corresponding loop generated.
        Useful to support experimental features and extensions.

        Parameters
        ----------
        var : IterVar
            The iteration to be anotated

        pragma_type : str
             The pragma string to be annotated

        pragma_value : Expr, optional
             The pragma value to pass along the pragma

        pragma_tensor : Tensor, optional
             The tensor pragma attr concerned

        Note
        ----
        Most pragmas are advanced/experimental features
        and may subject to change. List of supported pragmas:

        - **debug_skip_region**

          Force skip the region marked by the axis and turn it into no-op.
          This is useful for debug purposes.

        - **parallel_launch_point**

          Specify to launch parallel threads outside the
          specified iteration loop. By default the threads
          launch at the point of parallel construct.
          This pragma moves the launching point to even outer scope.
          The threads are launched once and reused across multiple
          parallel constructs as BSP style program.

        - **parallel_barrier_when_finish**

          Insert a synchronization barrier between working threads
          after the specified loop iteration finishes.

        - **parallel_stride_pattern**

          Hint parallel loop to execute in strided pattern.
          :code:`for (int i = task_id; i < end; i += num_task)`

        """
        if isinstance(pragma_value, string_types):
            pragma_value = convert(pragma_value)
        _ffi_api.StagePragma(self, var, pragma_type,
                             pragma_value, pragma_tensor)

    def prefetch(self, tensor, var, offset):
        """Prefetch the specified variable

        Parameters
        ----------
        tensor : Tensor
            The tensor to be prefetched
        var : IterVar
            The loop point at which the prefetching is applied
        offset : Expr
            The number of iterations to be prefetched before actual execution
        """
        _ffi_api.StagePrefetch(self, tensor, var, offset)

    def storage_align(self, axis, factor, offset):
        """Set alignment requirement for specific axis

        This ensures that stride[axis] == k * factor + offset for some k.
        This is useful to set memory layout to for more friendly memory
        access pattern. For example, we can set alignment to be
        factor=2, offset=1 to avoid bank conflict for thread access on
        higher dimension in GPU shared memory.

        Parameters
        ----------
        axis : IterVar
            The axis dimension to be aligned.
        factor : int
            The factor in alignment specification.
        offset : int
            The offset in the alignment specification.
        """
        _ffi_api.StageStorageAlign(self, axis, factor, offset)

    def double_buffer(self, var=None):
        """Compute the current stage via double buffering.

        This can only be applied to intermediate stage.
        This will double the storage cost of the current stage.
        Can be useful to hide load latency.
        """
        if var is None:
            _ffi_api.StageDoubleBuffer(self)
            return

        if not isinstance(var, _expr.Var):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "tvm.tir.expr.Var is needed, not %s" % type(var))

        _ffi_api.StageDoubleBuffer(self, var)

    def emit_insn(self, var, value, attrs=None):
        """Annotate the iteration to emit insn

        Parameters
        ----------
        var : IterVar
        The iteration to be unrolled.
        value : insn name
        attrs : Dict, optional
        The attribute information to pass along the pragma following "emit_insn" pragma
        """
        value = convert(value)
        _ffi_api.StagePragma(self, var, "emit_insn", value)

        if isinstance(attrs, dict):
            pragma_on_body = {"emit_insn_attr_last_reduce_axis",
                              "emit_insn_attr_last_broadcast_axis"}
            for key, value in attrs.items():
                key = "emit_insn_attr_" + key
                if key in pragma_on_body:
                    if isinstance(value, list):
                        for item in value:
                            if isinstance(value, string_types):
                                item = convert(item)
                            _ffi_api.StagePragmaOnBody(self, key, value)
                    elif isinstance(value, string_types):
                        value = convert(value)
                        _ffi_api.StagePragmaOnBody(self, key, value)
                    else:
                        _ffi_api.StagePragmaOnBody(self, key, value)
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(value, string_types):
                            item = convert(item)
                        _ffi_api.StagePragma(self, var, key, item)
                elif isinstance(value, string_types):
                    value = convert(value)
                    _ffi_api.StagePragma(self, var, key, value)
                else:
                    _ffi_api.StagePragma(self, var, key, value)

    def speel(self, var, value):
        """Annotate the iteration with peel

        Parameters
        ----------
        var : IterVar
            The iteration to be unrolled.
        value : unroll times
        """
        if var.iter_type == 2:  # kCommReduce, 2
            _ffi_api.StagePragma(self, var, "reduce_tile", value)
        else:
            _ffi_api.StagePragma(self, var, "tile", value)

    def buffer_align(self, *args):
        """Set alignment requirements for realize bound of each axis

        Parameters
        ----------
        args : List of alignment requirement for each axis of stage
            Each list alignment element is a tuple (min_align, extent_align)
        """
        new_args = []
        for axis in args:
            new_args.append([int(x) if not isinstance(
                x, (int, _expr.PrimExprWithOp)) else x for x in axis])
        _ffi_api.StageBufferAlign(self, new_args)

    def buffer_tile(self, *args):
        """Set realize bound for each axis"""
        new_args = []
        for axis in args:
            if (axis[0] is None) and (axis[1] is None):
                new_args.append(["default", "default"])
            elif (axis[0] is None) and (axis[1] is not None):
                new_args.append(["default", axis[1]])
            elif (axis[0] is not None) and (axis[1] is None):
                new_args.append([axis[0], "default"])
            else:
                new_args.append([int(x) if not isinstance(
                    x, (int, _expr.PrimExprWithOp)) else x for x in axis])
        _ffi_api.StageBufferTile(self, new_args)

    def preload(self, var=None):
        """Decide if  need preload"""
        if var is None:
            _ffi_api.StagePreload(self)
            return

        if not isinstance(var, _expr.Var):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "tvm.tir.expr.Var is needed, not %s" % type(var))

        _ffi_api.StagePreload(self, var)

    def cycle_double_buffer(self):
        _ffi_api.StageCycleDoubleBuffer(self)

    def n_buffer(self, n):
        _ffi_api.StageNBuffer(self, n)

    def allocate_at(self, parent, scope, run_once_axes=None):
        """Allocate buffer as size by calculating at scope

        Parameters
        ----------
        scope : IterVar
            Where the buffer will be calculated size.

        run_once_axes : list(IterVar)
            Point the axes which only run once
        """
        if run_once_axes is None:
            run_once_axes = []
        _ffi_api.StageAllocateAt(self, parent, scope, run_once_axes)

    def allocate_root(self):
        """Allocate buffer as size by calculating to root
        """
        _ffi_api.StageAllocateRoot(self)

    def reused_by(self, *args, **kwargs):
        """Specify the buffers which will reuse current buffer

        Parameters
        ----------
        args : list of tensors
            The tensors which will reuse current buffer
        reuse_data : bool
            The flag whether to use partial write
        """
        reuse_data = False
        if "reuse_data" in kwargs:
            reuse_data = kwargs["reuse_data"]
        _ffi_api.StageReusedBy(self, args, reuse_data)

    def unreused_by(self, *args):
        """Specify the buffers which will not reuse current buffer

        Parameters
        ----------
        args : list of tensors
            The tensors which will not reuse current buffer
        """
        _ffi_api.StageUnReusedBy(self, args)

    def specify_storage(self, *args):
        """Specify buffer

        Parameters
        ----------
        args : special tensor
            The special buffer
        """
        _ffi_api.StageSpecifyStorage(self, args[0])

    def mem_unique(self):
        """Declare that the current buffer will not reuse any buffer"""
        _ffi_api.StageMemUnique(self)

    def partition(self, var, ranges):
        """Annotate the iteration with partition ranges

        This will split var by specified ranges, e.g.,
        assume that A.op.axis[0] is [0, max], and schedule is written as
        `s[A].partition(A.op.axis[0], ((0,1), (3,3)))`, the result is
        splitting loop as [0, 1], [2, 2], [3, 3], [4, max].

        Parameters
        ----------
        var : IterVar
            The iteration to be annotated

        ranges : tuple
            The ranges to be specified in partition. Every range has 2 elements,
            including min and max, where they are used as closed interval.
        """
        tuples = []
        for range in ranges:
            tuples.append(Call("handle", "tir.tvm_tuple", convert(range)))
        self.pragma(var, "partition", Call("handle", "tir.tvm_tuple", tuples))

    def set_block_sync(self, axis, tensor, bottom=False):
        """Insert multicore sync set logic

        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore sync set

        tensor : Expr
             The tensor on gm to record Core State counter

        bottom : bool, optional
             The multicore sync set position
             bottom = True, add set logic before this intrinsic
             bottom = False(default), add set logic after this intrinsic
        """
        pragma_type = "multicore_sync_set_before"
        if bottom:
            pragma_type = "multicore_sync_set_after"
        _ffi_api.StagePragma(self, axis, pragma_type, tensor)

    def wait_block_sync(self, axis, tensor, bottom=False):
        """Insert multicore sync wait logic

        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore sync wait

        tensor : Expr
             The tensor on gm to record Core State counter

        bottom : bool, optional
             The multicore sync wait position
             bottom = True, add wait logic before this intrinsic
             bottom = False(default), add wait logic after this intrinsic
        """
        pragma_type = "multicore_sync_wait_before"
        if bottom:
            pragma_type = "multicore_sync_wait_after"
        _ffi_api.StagePragma(self, axis, pragma_type, tensor)

    def set_subblock_sync(self, axis, index, bottom=False):
        """Insert multicore subblock sync set logic
        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore sync set
        bottom : bool, optional
             The multicore sync set position
             bottom = True, add set logic before this intrinsic
             bottom = False(default), add set logic after this intrinsic
        """
        pragma_type = "multicore_subblock_sync_set_before"
        if bottom:
            pragma_type = "multicore_subblock_sync_set_after"
        _ffi_api.StagePragma(self, axis, pragma_type, index)

    def wait_subblock_sync(self, axis, index, bottom=False):
        """Insert multicore subblock sync wait logic
        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore sync wait
        bottom : bool, optional
             The multicore sync wait position
             bottom = True, add wait logic before this intrinsic
             bottom = False(default), add wait logic after this intrinsic
        """
        pragma_type = "multicore_subblock_sync_wait_before"
        if bottom:
            pragma_type = "multicore_subblock_sync_wait_after"
        _ffi_api.StagePragma(self, axis, pragma_type, index)

    def block_sync_all(self, axis, index, bottom=False, mode="all"):
        """Insert multicore all block sync set logic
        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore sync set
        bottom : bool, optional
            Set position
            bottom = True, add set logic before this intrinsic
            bottom = False(default), add set logic after this intrinsic
        mode : str
            "all" : inject multicore sync
            "ordered" : inject ordered sync
        """
        from te.platform import cce_conf
        if mode == "all":
            postfix = "_after" if bottom else "_before"
            if cce_conf.spport_hw_sync():
                pragma_type = "multicore_all_block_sync" + postfix
                _ffi_api.StagePragma(self, axis, pragma_type, index)
            else:
                pragma_type = "multicore_sync_wait" + postfix
                _ffi_api.StagePragma(self, axis, pragma_type, index)
                pragma_type = "multicore_sync_set" + postfix
                _ffi_api.StagePragma(self, axis, pragma_type, index)
        if mode == "ordered":
            pragma_type = "inject_ordered_sync"
            _ffi_api.StagePragma(self, axis, pragma_type, index)

    def set_intra_group_sync(self, axis, index, bottom=False):
        """Insert multicore group sync set logic
        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore group sync set
        bottom : bool, optional
             The multicore sync set position
             bottom = True, add set logic before this intrinsic
             bottom = False(default), add set logic after this intrinsic
        """
        pragma_type = "intra_group_sync_set_before"
        if bottom:
            pragma_type = "intra_group_sync_set_after"
        _ffi_api.StagePragma(self, axis, pragma_type, index)

    def wait_intra_group_sync(self, axis, index, bottom=False):
        """Insert multicore group sync wait logic
        Parameters
        ----------
        axis : IterVar
            The For axis to be add multicore group sync wait
        bottom : bool, optional
             The multicore sync wait position
             bottom = True, add wait logic before this intrinsic
             bottom = False(default), add wait logic after this intrinsic
        """
        pragma_type = "intra_group_sync_wait_before"
        if bottom:
            pragma_type = "intra_group_sync_wait_after"
        _ffi_api.StagePragma(self, axis, pragma_type, index)

    def compute_align(self, axis, factor, pad=None):
        """Set alignment requirement for specific axis

        This ensures the root axis to be aligned with factor.
        The bound of axis will be integral multiple of the factor.
        We will set value using pad for the bound larger than
        tensor origin shape and smaller than the aligned bound.
        exp.
        bounds [0,12][0,12], if we set the first axis with the factor 5,
        the bounds will be [0,15][0,12]. If we also set pad as 456, the value
        in first axis in [12,15] will be set as 456.

        Parameters
        ----------
        axis : IterVar
            The axis dimension to be aligned.
        factor : Expr
            The factor for the axis to be aligned.
        pad : Expr
            The pad for the bound in [tensor shape, aligned bound].
        """
        _ffi_api.StageComputeAlign(self, axis, factor, pad)

    def reverse(self, axis, cond=True):
        """ Reverse the iteration variable and wrap it with the condition

        This ensure the iteration variable to be reversed and be wrapped
        by the condition.

        Parameters
        ----------
        axis : IterVar
            The leaf axis should be reversed
        cond: Expr, option(default = True)
            The condition which should be met to reverse
        """
        _ffi_api.StageReverse(self, axis, cond)

    def bind_buffer(self, axis, stride, offset):
        """Set dst memory stride for specific axis

        For A = B + C, the  memory of A is not continuous and needs to
        jump for specific axis and offset buffer.
        stride[axis] = k * stride
        intial offset = offset

        Parameters
        ----------
        axis : IterVar or Int
            The axis dimension to be aligned.
        stride : Expr
            The stride in alignment specification.
        offset : Expr
            The offset in the alignment specification.
        """
        if isinstance(axis, int):
            _ffi_api.StageBindPlaceholder(self, axis, stride, offset)
        elif isinstance(axis, IterVar):
            _ffi_api.StageBindBuffer(self, axis, stride, offset)
        else:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "The type of axis must be int for placeholder,"
                                 "and the type of axis must be IterVar for Tensor")

    def enable_mte4_mte5(self):
        """Change DMA pipeline in current stage.

        This can only be applied to dma_copy stage.
        """
        _ffi_api.StageEnableMTE4MTE5(self)

    def remove_init(self):
        """Remove the initialization of a reduction op.
        """
        _ffi_api.StageRemoveInit(self)

    def set_buffer_size(self, bound):
        """Set storage bound for the current stage.

        Use this if you want to allocate a fixed memory bound
        for the current output tensors.

        Parameters
        ----------
        bound : Expr
            The storage bound expr.
        """
        bound = convert(bound)
        if not isinstance(bound, _expr.PrimExprWithOp):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "bound should be expr, but got %s." % type(bound))
        _ffi_api.StageSetStorageBound(self, bound)

    def specialize(self, axis, conditions):
        """Specialize conditions for compute node in dynamic shape operator

        Parameters
        ----------
        axis : IterVar
            The axis to specialize conditions
        conditions: Array<Expr>
            Conditions for special use
        """
        if isinstance(conditions, _expr.PrimExprWithOp):
            conditions = [conditions]
        _ffi_api.StageSpecialize(self, axis, conditions)

    def skip_bound_check(self):
        """
        The generation of boundary check conditions for all axes in the stage is skipped.
        All axes in the stage are cut or the address access does not exceed the maximum boundary value.
        """
        _ffi_api.StageSkipBoundCheck(self)

    def trace_store(self):
        import _warnings
        from te.platform import cce_conf
        if cce_conf.is_cloud_version() or cce_conf.is_mini_version():
            _ffi_api.StageTraceStore(self)
        else:
            _warnings.warn(
                "trace_store only support cloud and mini, skip using trace_store")

    def set_value(self, fcondtion, value, pre_assign=False):
        """
        Set the tensor value.

        Parameters
        ----------
        fcondtion : lambda function
            point out which should be set.
        value : expr or lambda funcion.
            value to be set.
        pre_assign : Bool
            whether prepend the assignment before original compute
        """
        indices = [x.var for x in self.op.axis]
        if callable(value):
            value = value(*indices)
        if callable(fcondtion):
            condition = fcondtion(*indices)
            _ffi_api.SetValue(self, condition, value, pre_assign)
        elif isinstance(fcondtion, tvm.ir.PrimExpr):
            _ffi_api.SetValue(self, fcondtion, value, pre_assign)

    def reinterpret_cast(self, dtype, axis=None):
        """
        Map the target compute node based on the target type for the buffer forcible forwarding type.

        Parameters
        ----------
        dtype : dtpe
            Type of the forcible transfer target..
        axis :IterVar
            The root axis of buffer for changing extent
        """
        _ffi_api.StageReinterpretCast(self, dtype, axis)

    def bind_sub_block(self, thread_ivar, parent, factor=None, nparts=None):
        """
        Split the axis by either factor or nparts mode, and bind outer axis with 'subBlockIdx'.

        Parameters
        ----------
        thread_ivar : IterVar
            The thread to bind.

        parent : IterVar
            The axis dimension to be split.

        factor : PrimExpr, optional
             The splitting factor

        nparts : PrimExpr, optional
             The number of outer parts.

        Returns
        """
        if thread_ivar is None or parent is None:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Both thread_ivar and parent must be defined.")

        if thread_ivar.thread_tag != "subBlockIdx.x":
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "thread_tag must be subBlockIdx.x.")

        if nparts is not None:
            if factor is not None:
                raise_tbe_python_err(
                    TBE_DEFAULT_PYTHON_ERROR_CODE, "Do not need to provide both factor and nparts.")
            _ffi_api.BindSubBlockByNParts(self, thread_ivar, parent, nparts)
        else:
            if factor is None:
                raise_tbe_python_err(
                    TBE_DEFAULT_PYTHON_ERROR_CODE, "Either nparts or factor need to be provided.")
            _ffi_api.BindSubBlockByFactor(self, thread_ivar, parent, factor)

    def set_vector_scope(self, var, key):
        """
        Annotate the cce scope for mixed coding.
        Parameters
        ----------
        var : IterVar The iteration to be cut scope.
        key : scope including simt, simd and main
        """
        if key == "simt":
            key = "vf_simt"
        elif key == "simd":
            key = "vf"
        key = convert(key)
        _ffi_api.StagePragma(self, var, key, 1)


@tvm._ffi.register_object
class SpecializedCondition(Object):
    """Specialized condition to enable op specialization."""

    def __init__(self, conditions):
        """Create a specialized condition.

        .. note::
            Conditions are represented in conjunctive joint form (CNF).
            Each condition should be a simple expression, e.g., n > 16,
            m % 8 == 0, etc., where n, m are tvm.Var that represents a
            dimension in the tensor shape.

        Parameters
        ----------
        conditions : List of tvm.Expr
            List of conditions in conjunctive joint form (CNF).
        """
        if not isinstance(conditions, (list, _container.Array)):
            conditions = [conditions]
        self.__init_handle_by_constructor__(
            _ffi_api.CreateSpecializedCondition, conditions)

    @staticmethod
    def current():
        """Returns the current specialized condition"""
        return _ffi_api.GetCurrentSpecialization()

    def __enter__(self):
        _ffi_api.EnterSpecializationScope(self)
        return self

    def __exit__(self, ptype, value, trace):
        _ffi_api.ExitSpecializationScope(self)


tvm._ffi._init_api("schedule", __name__)
