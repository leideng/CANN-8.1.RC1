#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
Copyright (C) 2016. Huawei Technologies Co., Ltd. All rights reserved.

This program is free software; you can redistribute it and/or modify
it under the terms of the Apache License Version 2.0.You may not use this
file except in compliance with the License.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
Apache License for more details at
http://www.apache.org/licenses/LICENSE-2.0

common function
"""
# pylint: disable=import-error, unidiomatic-typecheck,no-else-return,consider-using-enumerate
import warnings
from functools import wraps

import tbe
from te import tvm
from te.platform import cce_conf

# the min dim of shaoe
DEFAULT_MIN_SHAPE_DIM = 1
# the max dim of shape
DEFAULT_MAX_SHAPE_DIM = 8
# the max num of each axis of shape
DEFAULT_MAX_SHAPE_NUM = 200000000
# the max num of all reduce axis of shape
MAX_REDUCE_SHAPE_NUM = 200000000
# the max len of kernel_name
MAX_KERNEL_NAEM_LEN = 200
# cloud product
VERSION_CLOUD = "cloud"
# mini product
VERSION_MINI = "mini"
# smallhisi product
VERSION_SHISI = "smallhisi"
VERSION_MINI_NG1 = "mini_ng1"
VERSION_MINI_NG1M = "mini_ng1m"
VERSION_MINI_NG1PG2 = "ng1pg2"
# the max size of SHAPE, value = 2^31
SHAPE_SIZE_LIMIT = 2147483648


def check_input_type_dict(input_dict, input_key, input_name):
    """
    check input parameter type for new type: dict
    rule1: key of input_dict should be in the input_key
    rule2: type of input_dict[shape] should be in (list, tuple), if have shape
    rule3: type of input_dict[dtype] should be in (str), if have dtype

    Parameters
    ----------
    input_dict: dict
        input_dict
    input_key: list or tuple
        all input key list, the key of input must in input_key
    input_name: str
        input param name, only used for error print

    Returns
    -------
    None
    """
    warnings.warn("check_input_type_dict in topi is expired, please do not use it",
                  DeprecationWarning)

    def _check_input_type(input_key, input_type):
        if not isinstance(input_dict[input_key], input_type):
            raise RuntimeError(
                "Input parameter error, please check input parameter!")

    for key in input_dict.keys():
        if key not in input_key:
            raise RuntimeError(
                "Input parameter value must have property, please check!")

        # check shape's type of input_dict, if have shape
        if key == "shape":
            _check_input_type(key, (list, tuple))

        # check dtype's type of input_dict, if have dtype
        if key == "dtype":
            _check_input_type(key, (str,))


# pylint: disable=unused-argument
def check_input_type(*type_args, **type_kwargs):
    """
    check input parameter type
    """
    warnings.warn("check_input_type in topi is expired, please do not use it",
                  DeprecationWarning)

    def out_wrapper(func):
        """
        out_wrapper

        :param func: func
        :return: None
        """
        formal_parameter = func.__code__.co_varnames
        formal_parameter_list = list(zip(formal_parameter, type_args))

        @wraps(func)
        def in_wrapper(*args, **kwargs):
            """
            in_wrapper
            :param args: args
            :param kwargs: kwargs
            :return: None
            """
            for i in range(len(args)):
                # add for new input dict, if dict, will check shape and dtype
                if isinstance(args[i], dict):
                    check_input_type_dict(args[i], args[i].keys(),
                                          formal_parameter_list[i][0])
                if not isinstance(args[i], formal_parameter_list[i][1]):
                    raise RuntimeError(
                        "Input parameter type error, please check!")
            for i in kwargs:
                for j in formal_parameter_list:
                    if i in j:
                        if not isinstance(kwargs[i], j[1]):
                            raise RuntimeError(
                                "Input parameter type error, please check!")
                        break
            return func(*args, **kwargs)

        return in_wrapper

    return out_wrapper


def check_dtype_rule(dtype, check_list):
    """
    The common check rule for tensor dtype
    """
    warnings.warn("check_dtype_rule in topi is expired, please do not use it",
                  DeprecationWarning)
    tbe.common.utils.para_check.check_dtype_rule(dtype, check_list)


def check_shape_rule(shape, min_dim=None, max_dim=None, max_shape_num=None):
    """
    The common check rule for tensor shape
    """
    warnings.warn("check_shape_rule in topi is expired, please do not use it",
                  DeprecationWarning)
    tbe.common.utils.para_check.check_shape_rule(
        shape, min_dim, max_dim, max_shape_num)


def check_reduce_shape_rule(shape):
    """
    check the shape of reduce axis must be less than MAX_REDUCE_SHAPE_NUM
    :param shape: inout shape
    """
    warnings.warn("check_reduce_shape_rule in topi is expired, please do not use it",
                  DeprecationWarning)
    # the shape of reduce axis must be less than MAX_REDUCE_SHAPE_NUM
    return tbe.common.utils.para_check.check_reduce_shape_rule(shape)


def check_reduce_need_refine(shape, reduce_axis, keep_dims):
    """
    # if the reduce axis correspond to shape[axis] is 1,
    we can not refine the shape,or the reduce axis will be wrong
    shape : shape of data

    reduce_axis : list, tuple or int  axis want to reduce

    :return: True or False
    """
    warnings.warn("check_reduce_need_refine in topi is expired, please do not use it",
                  DeprecationWarning)
    # if the reduce axis correspond to shape[axis] is 1,
    # we can not refine the shape,or the reduce axis will be wrong
    return tbe.common.utils.shape_util.check_reduce_need_refine(shape, reduce_axis, keep_dims)


# pylint: disable=too-many-branches
def shape_refine(shape, reduce_axis=None, keep_dims=True):
    """
    refine shape to drop 1 in shape according to reduce axis,
    if input is just shape, result is shape, and if inputs are shape and axis,
    result is a tuple of (shape, axis)

    Parameters
    ----------
    shape : shape of data

    reduce_axis : list, tuple or int
        axis want to reduce

    keepdims: if keepdims = True, we should not refine the shape

    Returns
    -------
    shape : list
        refined shape

    reduce_axis : list
        if input parameters send reduce axis, this will be the output.
        if all the reduce axis is illegal like the length of reduce axis is 1,
        a empty list([]) will be returned.

    """
    warnings.warn("shape_refine in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.shape_util.shape_refine(shape, reduce_axis, keep_dims)


def refine_axis(axis, shape):
    """
    refine axis

    Parameters
    ----------
    axis :
        axis want to reduce

    shape : shape of data

    Returns
    -------
    res_reduce_axis : list
        refined axis
    """
    warnings.warn("refine_axis in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.shape_util.refine_axis(axis, shape)


def get_divisors(num):
    """
    compute the divisors of num

    Parameters
    ----------
    num: Inpute number

    Returns
    -------
    divisors: List
    """
    warnings.warn("get_divisors in topi is expired, please do not use it",
                  DeprecationWarning)
    divisors = []
    tmp_var = num
    while 0 < tmp_var <= num:
        if num % tmp_var == 0:
            divisors.append(tmp_var)
        tmp_var -= 1
    divisors.reverse()
    return divisors


def axis_check(shape_len, axis):
    """
    Check the value of axis and return the sorted axis
    """
    warnings.warn("axis_check in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.shape_util.axis_check(shape_len, axis)


def simplify_axis_shape(shape, axis):
    """
    simplify the shape and aixs
    """
    warnings.warn("simplify_axis_shape in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.shape_util.simplify_axis_shape(shape, axis)


def produce_shapes(shape1, shape2):
    """
    two input shapes produce three output shape
    """
    warnings.warn("produce_shapes in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.shape_util.produce_shapes(shape1, shape2)


def create_param_ptr(value_list, p_type, ptr_name):
    """ alloc a ptr to store value_list"""

    warnings.warn("create_param_ptr in topi is expired, please do not use it",
                  DeprecationWarning)

    def _select_from_list(_i, _list):
        ret = _list[0]
        for j in range(0, len(_list)):
            ret = tvm.select(_i == j, _list[j], ret)
        return ret

    tvm_var_list = [tvm.const(value, dtype=p_type) for value in value_list]
    p_ptr = tvm.compute((len(tvm_var_list),), lambda i: _select_from_list(i, tvm_var_list),
                        name=ptr_name)
    return p_ptr


def get_device_api_dtype(dtype):
    """
    get_device_api_dtype

    Parameters
    ----------
    dtype: type

    Returns
    -------
    number of type
    """
    warnings.warn("get_device_api_dtype in topi is expired, please do not use it",
                  DeprecationWarning)
    device_api_dtype_maps = {"float32": 0,
                             "float16": 1, "int32": 2, "double": 3}
    return device_api_dtype_maps[dtype]


def is_scalar(shape):
    """
    verify that tensor is scalar
    ----------
    shape: shape of data

    Returns
    -------
    True or False
    """
    warnings.warn("is_scalar in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.para_check.is_scalar(shape)


def scalar2tensor_one(shape):
    """
    if the input_shape is [],convert the input_shape to [1]
    ----------
    shape: shape of input tensor

    Returns
    -------
    list:[1]
    """
    warnings.warn("scalar2tensor_one in topi is expired, please do not use it",
                  DeprecationWarning)
    return tbe.common.utils.shape_util.scalar2tensor_one(shape)


def check_shape_size(shape, limit):
    """
    check shape size for operator
    ----------
    shape: shape of data

    limit: limit of the product of all dimension

    Returns
    -------
    None
    """
    warnings.warn("check_shape_size in topi is expired, please do not use it",
                  DeprecationWarning)
    tbe.common.utils.para_check.check_shape_size(shape, limit)


def compare_tensor_dict_key(dict1, dict2, dict_key):
    """
    compare the key value between dict1 and dict2,
    the value is not equal, will raise error

    Parameters
    ----------
    dict1: dict
        input dict1
    dict2: dict
        input dict2
    dict_key: str
        the key that will be compare

    Returns
    -------
    None
    """
    warnings.warn("compare_tensor_dict_key in topi is expired, please do not use it",
                  DeprecationWarning)
    tbe.common.utils.shape_util.compare_tensor_dict_key(dict1, dict2, dict_key)


def axis_transfrom_5d(axis, data_format):
    """
    4d format axis to 5d mapping
    """
    warnings.warn("axis_transfrom_5d in topi is expired, please do not use it",
                  DeprecationWarning)
    if data_format == "NCHW":
        if axis < 0:
            axis = axis - 1
    elif data_format == "NHWC":
        if axis == -4:
            axis = -5
        elif axis == -1:
            axis = -4
        elif axis == 1:
            axis = 2
        elif axis == 2:
            axis = 3
        elif axis == 3:
            axis = 1
    return axis


def check_tensor_shape_size(shape):
    """
    check shape size for operator
    In order not to affect the old operators,create a similar function

    Parameters
    ----------
    shape: list or tuple
        shape of data

    Returns
    -------
    None
    """
    warnings.warn("check_tensor_shape_size in topi is expired, please do not use it",
                  DeprecationWarning)
    shape_size = check_shape_size(shape, SHAPE_SIZE_LIMIT)
    return shape_size


def check_kernel_name(kernel_name):
    """
    check kernel_name
    ----------
    kernel_name: str or None

    Returns
    -------
    None
    """
    warnings.warn("check_kernel_name in topi is expired, please do not use it",
                  DeprecationWarning)
    tbe.common.utils.para_check.check_kernel_name(kernel_name)


def get_product_version():
    """
    get product version
    ----------

    Returns
    -------
    cloud: cloud product
    mini: mini product
    """
    warnings.warn("get_product_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.get_product_version()


def is_v200_version_new():
    """
    check if Ascend610/Ascend310P/Hi3796CV300CS version
    ----------

    Returns
    -------
    True:  Ascend610/Ascend310P/Hi3796CV300CS version
    False: Other version
    """
    warnings.warn("is_v200_version_new in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_v200_version_new()


def is_v200_version():
    """
    check if ng1/ng1m version
    ----------

    Returns
    -------
    True: ng1 version
    False: Other version
    """
    warnings.warn("is_v200_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_v200_version()


def is_lhisi_version():
    """
    check if 3796ES version
    -------

    Returns
    -------
    True: 3796ES version
    False: Other version
    """
    warnings.warn("is_lhisi_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_lhisi_version()


def is_lhisi_cs_version():
    """
    check if 3796CS version
    -------

    Returns
    -------
    True: 3796CS version
    False: Other version
    """
    warnings.warn("is_lhisi_cs_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_lhisi_cs_version()


def is_cloud_version():
    """
    check if cloud-Ascend910 version
    ----------

    Returns
    -------
    True: cloud version
    False: Other version
    """
    warnings.warn("is_cloud_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_cloud_version()


def is_mini_version():
    """
    check if mini version
    -------

    Returns
    -------
    True: mini version
    False: Other version
    """
    warnings.warn("is_mini_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_mini_version()


def is_mini_or_lhisi_version():
    """
    check if mini or lhisi version
    -------

    Returns
    -------
    True: mini version or lhisi
    False: Other version
    """
    warnings.warn("is_mini_or_lhisi_version in topi is expired, please do not use it",
                  DeprecationWarning)
    return cce_conf.is_mini_or_lhisi_version()


def check_load3d_w_out_1_support():
    """
    check if current soc version load3d instruction support w_out==1 or not
    only Ascend310 and Hi3796CS support w_out==1
    when fmap_w(with padding) == filters_w(after dilation)
    -------

    Returns
    -------
    True: support
    False: not support
    """
    warnings.warn("check_load3d_w_out_1_support in topi is expired, please do not use it",
                  DeprecationWarning)
    from impl.util import util_common
    return util_common.check_load3d_w_out_1_support()


def check_and_init_5hdc_reduce_support(input_tensor, axis):
    warnings.warn("check_and_init_5hdc_reduce_support in topi is expired, please do not use it",
                  DeprecationWarning)
    """5HD Special param for 5hd schedule"""

    return tbe.common.utils.para_check.check_and_init_5hdc_reduce_support(input_tensor, axis)


def reuse_input_as_output(output_buffer, input_buffer, kernel_name):
    """
    Declare an input_tensor as output_tensor, 
    designed for adam_apply + assign fusion
    """
    warnings.warn("reuse_input_as_output in topi is expired, please do not use it",
                  DeprecationWarning)
    from impl.util import fusion_util
    fusion_util.reuse_input_as_output(output_buffer, input_buffer, kernel_name)
