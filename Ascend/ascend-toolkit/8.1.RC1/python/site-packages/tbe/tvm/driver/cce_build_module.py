#!/usr/bin/env python
# coding: utf-8
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

import functools
import multiprocessing
import os
import shutil
import stat
import time
import traceback
import typing
from dataclasses import dataclass
from typing import Union, Optional, List, Mapping

from tvm.buffer_manager import get_buffer_manager
from tvm.build_util import buffer_remap, get_l1_fusion_tensors, \
    get_ub_fusion_tensors, get_reshaped_tensors, has_extern_op, is_dynamic_args
from tvm.error_mgr import raise_tbe_python_err, TBE_DEFAULT_PYTHON_ERROR_CODE
from tvm.ir.module import IRModule
from tvm.te import tensor
from tvm.tir import PrimFunc
from tvm.tir.buffer import Buffer
from tvm.tir.expr import Var
from tvm.tir.transform.transform import *
from tvm.tir.transform.transform_extended import *

from . import _ffi_api as ffi
from ..te import clear_expr_attr
from ..te import schedule

count_time = False
use_pass_manager = True
MAX_PRINT_IR_LINE_NUM = 5000
PRINT_PASS_INFO = False


@dataclass
class LowerContext:
    """
    LowerContext struct
    """
    sch: typing.Any
    name: str
    binds: typing.Any
    simple_mode: bool
    arg_list: typing.Any
    cfg: typing.Any
    evaluates: typing.Any
    tiling_key: int
    args: typing.Any
    inputs_num: int


def _get_kernel_name(name, suffix, stmt_aiv):
    if stmt_aiv is None:
        return name
    return name + suffix


def time_statistics(fn):
    @functools.wraps(fn)
    def wrapper(*args, **kw):
        if count_time:
            start = time.time()
        r = fn(*args, **kw)
        if count_time:
            end = time.time()
            print('%s executed in %s s' % (fn.__name__, end - start))
        return r

    return wrapper


def ir_json_file_name(name: str, tiling_key: any, is_dynamic: bool) -> str:
    """
    get unique json file name by name and tiling_key
    """
    file_name = name
    from . import expr
    if isinstance(tiling_key, expr.UIntImm):
        file_name = file_name + "_" + str(tiling_key.value)
    if is_dynamic:
        return file_name + "_dynamic_IR.json"
    return file_name + "_static_IR.json"


def save_ir_to_json_file(stmt: any, name: str, tiling_key: any, is_dynamic: bool) -> None:
    """
    save ir to json, and save json to file
    """
    json_str = tvm.ir.save_json(stmt)
    json_file_name = ir_json_file_name(name, tiling_key, is_dynamic)
    with open(json_file_name, 'w', encoding='utf-8') as f:
        os.chmod(json_file_name, 0o755)
        f.write(json_str)


def remove_ir_json_file(name: str, tiling_key: any, is_dynamic: bool) -> None:
    """
    remove josn file if lower is success
    """
    json_file_name = ir_json_file_name(name, tiling_key, is_dynamic)
    if os.path.exists(os.path.realpath(json_file_name)):
        os.remove(json_file_name)


def reload_ir_from_json_file(file_name: str) -> any:
    """
    reload ir from json file in local environment
    """
    with open(file_name, 'r') as f:
        json_str = f.read()
        stmt = tvm.ir.load_json(json_str)
        return stmt


@tvm.register_func("tvm.add_workspace")
def add_workspace(buffer_name, buffer_size):
    from tvm.runtime import cce_runtime
    cce_runtime.MULTI_CORE_SYNC_WORKSPACE_SIZE_LIST.local_list.append(
        (buffer_name, buffer_size))


@tvm.register_func("tvm.update_add_workspace")
def update_add_workspace(buffer_name, buffer_size, param_idx, is_atomic_clean=False, tik_mode=False):
    from tvm.runtime import cce_runtime
    if not tik_mode:
        local_list = cce_runtime.TBE_WORKSPACE_SIZE_LIST.local_list
    else:
        local_list = cce_runtime.TIK_WORKSPACE_SIZE_LIST.local_list
    local_list.append((buffer_name, buffer_size))
    if is_atomic_clean and not tik_mode:
        cce_runtime.TBE_WORKSPACE_IND_LIST.local_list.append(param_idx)
    elif is_atomic_clean and tik_mode:
        cce_runtime.TIK_ATOMIC_ADD_LIST.local_list.insert(param_idx, 1)


def _count_time(mod: typing.Any) -> None:
    if not count_time:
        return

    for var in mod.get_global_vars():
        func = mod[var.name_hint]
        var_nums = CountVar(func.body)
        ir_lines = str(func.body).count('\n')
        if ir_lines != 0:
            standard_time = ((var_nums / ir_lines) * 0.4 *
                             ir_lines + 1.5 * ir_lines) / 1000
            print('standard_time executed in %s s' % standard_time)


def cce_lower_by_pass_manager(stmt: typing.Any, ctx: LowerContext) -> typing.Any:
    """
    lower function by pass manager
    examples:
    _api_internal _save_json(ctx)/_load_json("*.json")
    :param stmt:
    :param ctx: lower context
    :return: stmt
    """
    stmt = remap_tensor(ctx.sch, ctx.args, stmt)
    if PRINT_PASS_INFO:
        pass_names = ir_pass.GetPassNames()
        for index, pass_name in enumerate(pass_names):
            print(f"[{index}]: {pass_name}")
    name = ctx.name
    pass_ctx = {"schedule": ctx.sch, "stmt": stmt, "name": ctx.name, "binds": ctx.binds,
                "simple_mode": ctx.simple_mode,
                "arg_list": ctx.arg_list, "debug": False}
    if ctx.evaluates is not None:
        pass_ctx["evaluates"] = ctx.evaluates
    pass_ctx = ir_pass.RunPass(pass_ctx)
    stmt = pass_ctx["stmt"]
    vector_functions = pass_ctx["vector_functions"]
    arg_list = list(pass_ctx["arg_list"])
    if not ctx.simple_mode:
        ir_pass.CCECodegen(stmt, name, arg_list, ctx.cfg.restricted_func, ctx.tiling_key,
                           vector_functions)
    return stmt


def update_evaluates(evaluates):
    new_evaluates = {}
    from tbe.tvm import convert
    for k, v in evaluates.items():
        new_k = tvm.tir.Simplify(k) if isinstance(k, tvm.ir.PrimExpr) else k
        if not isinstance(new_k, tvm.Var):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Variable needed, not %s" % type(new_k))
        new_v = convert(v)
        if not isinstance(new_v, tvm.ir.PrimExpr):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Variable needed, not %s" % type(new_v))
        new_evaluates.update({new_k: new_v})
    return new_evaluates


def remap_tensor(sch, args, stmt):
    if get_buffer_manager().get_remapped_buffers():
        if has_extern_op(sch):
            l1_fusion_tensors = get_l1_fusion_tensors(
                args, get_buffer_manager())
            reshaped_tensors = get_reshaped_tensors(args, get_buffer_manager())
            if l1_fusion_tensors:
                stmt = ir_pass.ChangeL1FusionDataFlow(stmt, l1_fusion_tensors)
            if reshaped_tensors:
                stmt = ir_pass.RemapBufferAddress(stmt, reshaped_tensors)
            get_buffer_manager().clear_remapped_buffers()
    return stmt


def check_reset_op():
    from tbe.common.context import get_context
    context = get_context()
    if context:
        from tbe.common.buildcfg import get_current_build_config
        from tbe.common.buildcfg.buildcfg_mapping import build_sub_function
        if not context.get_addition("compile_reset_op"):
            if context.get_addition("reset_op_info"):
                reset_op_info = context.get_addition("reset_op_info")
                vector_random_kernel_name = ""
                cube_random_kernel_name = ""
                bin_path = ""
                for info in reset_op_info:
                    random_kernel_name = info.get("kernel_name")
                    if "vector" in random_kernel_name:
                        vector_random_kernel_name = random_kernel_name
                    if "cube" in random_kernel_name:
                        cube_random_kernel_name = random_kernel_name
                bin_path = "/".join(os.path.realpath(info.get("bin_path")
                                                     ).split("/")[:-1])
                # for now, index 0 for vector, index 1 for cube
                from tvm.tir import transform
                transform.SetRandomKernelName(
                    vector_random_kernel_name, cube_random_kernel_name, bin_path)
        elif not get_current_build_config(build_sub_function):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "build_sub_funciton must on when compile random kernel")


def _append_args(sch, arg_list):
    inputs_num = 0
    from tbe.common.platform import get_soc_spec
    short_soc_version = get_soc_spec("SHORT_SOC_VERSION")
    from tbe.common.buildcfg import get_current_build_config
    from tbe.common.buildcfg.buildcfg_mapping import l2_mode
    # FE pass 999 to l2_mode to enable rc cache
    if short_soc_version in ["Ascend310P", "Ascend910", "Ascend610", "BS9SX1A",
                             "Ascend610B"] and get_current_build_config(l2_mode) == 999:
        stack = []
        visited = set()
        for op in sch.outputs:
            stack.append(sch.stage_map[op].op)
            visited.add(sch.stage_map[op].op)
        while len(stack) != 0:
            op = stack[-1]
            stack.pop(-1)
            input_list = op.input_tensors
            if len(input_list) == 0 and isinstance(op, tvm.tensor.PlaceholderOp):
                inputs_num = inputs_num + 1
            for input_value in input_list:
                if input_value.op is not None:
                    if input_value.op not in visited:
                        visited.add(input_value.op)
                        stack.append(input_value.op)
    for i in range(0, inputs_num):
        arg_name = '%s%s' % ("readmode", str(i))
        arg_list.append(tvm.var(arg_name, dtype="uint64_t"))
    return inputs_num, arg_list


def get_kernel_meta_path(is_sub: bool):
    """
    Get kernel_meta path
    :param is_sub:
    :return:
    """
    from tbe.common.platform.platform_info import KernelName
    from tbe.common.buildcfg import get_current_build_config
    from tbe.common.buildcfg.buildcfg_mapping import kernel_meta_parent_dir
    from te.platform import cce_params
    if not is_sub:
        res_path = os.path.join(get_current_build_config(kernel_meta_parent_dir),
                                cce_params.KERNEL_META_PATH)
    else:
        res_path = os.path.join(get_current_build_config(kernel_meta_parent_dir),
                                cce_params.KERNEL_META_PATH,
                                KernelName.get_kernel_name())
    return res_path


def get_final_mix_dict(final_mix_dict: dict, sub_mix_dict: dict):
    from te.platform import cce_params
    for idx in range(len(final_mix_dict[cce_params.JSON_KEY_PARAMETERS])):
        if final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] is None and \
                sub_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] is None:
            final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] = sub_mix_dict[
                cce_params.JSON_KEY_PARAMETERS][idx]
        elif isinstance(final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx], dict) and isinstance(
                sub_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx], dict):
            final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] = sub_mix_dict[
                cce_params.JSON_KEY_PARAMETERS][idx]
        elif isinstance(final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx], dict) and \
                sub_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] is None:
            final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] = final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx]
        elif final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] is None and \
                isinstance(sub_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx], dict):
            final_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx] = sub_mix_dict[cce_params.JSON_KEY_PARAMETERS][idx]
        else:
            raise ValueError("unsupported parameters type!")
    return final_mix_dict


def parse_mix_json_file(index: int, mix_json_file: str, final_mix_dict: dict,
                        max_param_size: int):
    """
    Collect the mix aic json or aiv json in each kernel, and combine them to a json.
    """
    if not os.path.isfile(mix_json_file):
        raise ValueError(
            "can't find json file {json_file}, probably due to error in subprocess "
            "or cases with duplicate names.".format(json_file=mix_json_file))
    with open(mix_json_file, 'r') as fp:
        sub_mix_dict = json.load(fp)
    # deal with mix aic
    from te.platform import cce_params
    if sub_mix_dict[cce_params.JSON_KEY_OP_PARA_SIZE] > max_param_size:
        max_param_size = sub_mix_dict[cce_params.JSON_KEY_OP_PARA_SIZE]
    if index == 0:
        final_mix_dict = sub_mix_dict
    else:
        final_mix_dict[cce_params.JSON_KEY_KERNEL_LIST] += sub_mix_dict[
            cce_params.JSON_KEY_KERNEL_LIST]
        final_mix_dict = get_final_mix_dict(final_mix_dict, sub_mix_dict)
    if cce_params.JSON_KEY_WORKSPACE not in final_mix_dict or cce_params.JSON_KEY_SIZE not in \
            final_mix_dict[
                cce_params.JSON_KEY_WORKSPACE]:
        raise ValueError("final_mix_dict not contain workspace or size")

    if len(final_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE]) != \
            len(sub_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE]):
        raise ValueError(
            "workspace numbers are not solid! final_mix_dict size:{}, sub_mix_dict:{}"
            .format(final_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE],
                    sub_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE]))

    for j in range(len(final_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE])):
        if sub_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE][j] > \
                final_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE][j]:
            final_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE][j] = \
                sub_mix_dict[cce_params.JSON_KEY_WORKSPACE][cce_params.JSON_KEY_SIZE][j]
    return final_mix_dict, max_param_size


def get_final_dict(final_aiv_dict: dict, final_aic_dict: dict):
    from te.platform import cce_params
    for k in range(len(final_aiv_dict[cce_params.JSON_KEY_PARAMETERS])):
        if isinstance(final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k], int) and isinstance(
                final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k], int):
            final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k] = \
                final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k] & \
                final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k]
        elif isinstance(final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k], dict) and isinstance(
                final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k], dict):
            final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k] = final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k]
        elif final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k] is None and \
                final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k] is None:
            final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k] = final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k]
        else:
            raise ValueError("unsupported parameters type!")
        final_aiv_dict[cce_params.JSON_KEY_PARAMETERS][k] = \
            final_aic_dict[cce_params.JSON_KEY_PARAMETERS][k]
    return [final_aic_dict, final_aiv_dict]


def merge_json_file(final_aic_dict: dict, final_aiv_dict: dict,
                    final_dict: dict, max_param_size: int):
    """
    keep mix aic json and mix aiv json info consistent and generate final dict.
    """
    from te.platform import cce_params
    if final_aic_dict and final_aiv_dict and final_dict:
        if len(final_aiv_dict[cce_params.JSON_KEY_PARAMETERS]) != len(
                final_aic_dict[cce_params.JSON_KEY_PARAMETERS]):
            raise ValueError("parameters numbers are not solid!")
        final_aic_dict, final_aiv_dict = get_final_dict(final_aiv_dict, final_aic_dict)
        final_aic_dict[cce_params.JSON_KEY_OP_PARA_SIZE] = max_param_size
        final_aic_dict[cce_params.JSON_KEY_BLOCK_DIM] = -1
        final_aiv_dict[cce_params.JSON_KEY_OP_PARA_SIZE] = max_param_size
        final_aiv_dict[cce_params.JSON_KEY_BLOCK_DIM] = -1
        final_dict[cce_params.JSON_KEY_BLOCK_DIM] = -1
        return [final_dict, final_aic_dict, final_aiv_dict]
    elif final_aic_dict and final_aiv_dict:
        if len(final_aiv_dict[cce_params.JSON_KEY_PARAMETERS]) != len(
                final_aic_dict[cce_params.JSON_KEY_PARAMETERS]):
            raise ValueError("parameters numbers are not solid!")
        final_aic_dict, final_aiv_dict = get_final_dict(final_aiv_dict, final_aic_dict)
        final_aic_dict[cce_params.JSON_KEY_OP_PARA_SIZE] = max_param_size
        final_aic_dict[cce_params.JSON_KEY_BLOCK_DIM] = -1
        final_aiv_dict[cce_params.JSON_KEY_OP_PARA_SIZE] = max_param_size
        final_aiv_dict[cce_params.JSON_KEY_BLOCK_DIM] = -1
        res = [final_aic_dict, final_aiv_dict]
    else:
        final_dict[cce_params.JSON_KEY_OP_PARA_SIZE] = max_param_size
        final_dict[cce_params.JSON_KEY_BLOCK_DIM] = -1
        res = [final_dict]

    return res


def _parse_main_json_dict(json_dir, name, rules):
    json_name = "".join(
        [name, "_", str(get_large_uint_value(rules) & 0xffffffffffffffff), ".json"])
    json_file = os.path.join(json_dir, json_name)
    if not os.path.isfile(json_file):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "can't find json file {json_file}, probably due to error in subprocess "
                             "or cases with duplicate names.".format(json_file=json_file))
    with open(json_file, 'r') as fp:
        main_dict = json.load(fp)
    return main_dict


def _get_sub_json_file_name(json_dir, json_dict):
    from te.platform import cce_params
    json_list_size = 2
    if len(json_dict.get(cce_params.JSON_KEY_JSON_LIST)) != json_list_size:
        raise ValueError("json list size is error.")
    mix_aic_json_name = "{}".format(json_dict.get(cce_params.JSON_KEY_JSON_LIST)[
                                        0].get(cce_params.JSON_KEY_JSON_FILE_NAME))
    mix_aic_json_file = os.path.join(json_dir, mix_aic_json_name)
    mix_aiv_json_name = "{}".format(json_dict.get(cce_params.JSON_KEY_JSON_LIST)[
                                        1].get(cce_params.JSON_KEY_JSON_FILE_NAME))
    mix_aiv_json_file = os.path.join(json_dir, mix_aiv_json_name)
    return mix_aic_json_file, mix_aiv_json_file


# combine json and link object file
def parse_json_file(name, tiling_keys):
    """Collection the json in each kernel, and combine them to a json.

    Returns
    -------
    final_dict : dict
        The json write to file.
    """
    from tbe.common.buildcfg import get_current_build_config
    from tbe.common.buildcfg.buildcfg_mapping import build_fatbin
    from te.platform import cce_params
    json_dir = get_kernel_meta_path(False)
    if get_current_build_config(build_fatbin):
        json_dir = get_kernel_meta_path(True)
    final_dict = {}
    final_aic_dict = {}
    final_aiv_dict = {}
    max_param_size = 0
    aic_aiv_mix_combined = False
    sub_dict_list = []
    core_type_dict = {}
    for i, j in enumerate(tiling_keys):
        sub_dict = _parse_main_json_dict(json_dir, name, j)
        sub_dict_list.append(sub_dict)
        core_type_dict[sub_dict[cce_params.JSON_KEY_CORE_TYPE]] = 1

    if len(core_type_dict) > 1 and "MIX" in core_type_dict:
        aic_aiv_mix_combined = True

    for i, sub_dict in enumerate(sub_dict_list):
        if cce_params.JSON_KEY_JSON_LIST in sub_dict:
            mix_aic_json_file, mix_aiv_json_file = _get_sub_json_file_name(
                json_dir, sub_dict)
            final_aic_dict, max_param_size = \
                parse_mix_json_file(i, mix_aic_json_file,
                                    final_aic_dict, max_param_size)
            final_aiv_dict, max_param_size = \
                parse_mix_json_file(i, mix_aiv_json_file,
                                    final_aiv_dict, max_param_size)
            if i == 0:
                final_dict = sub_dict
            else:
                final_dict[cce_params.JSON_KEY_KERNEL_LIST] += sub_dict[
                    cce_params.JSON_KEY_KERNEL_LIST]
        elif cce_params.JSON_KEY_MIX_NAME in sub_dict:
            mix_aic_json_name = "{}.json".format(sub_dict.get(
                cce_params.JSON_KEY_MIX_NAME).get(cce_params.JSON_KEY_MIX_AIC_NAME))
            mix_aic_json_file = os.path.join(json_dir, mix_aic_json_name)
            final_aic_dict, max_param_size = \
                parse_mix_json_file(i, mix_aic_json_file,
                                    final_aic_dict, max_param_size)
            mix_aiv_json_name = "{}.json".format(sub_dict.get(
                cce_params.JSON_KEY_MIX_NAME).get(cce_params.JSON_KEY_MIX_AIV_NAME))
            mix_aiv_json_file = os.path.join(json_dir, mix_aiv_json_name)
            final_aiv_dict, max_param_size = \
                parse_mix_json_file(i, mix_aiv_json_file,
                                    final_aiv_dict, max_param_size)
        else:
            if (sub_dict[cce_params.JSON_KEY_OP_PARA_SIZE] > max_param_size
                    and not (aic_aiv_mix_combined and sub_dict[cce_params.JSON_KEY_CORE_TYPE] == "MIX")):
                max_param_size = sub_dict[cce_params.JSON_KEY_OP_PARA_SIZE]
            if i == 0:
                final_dict = sub_dict
            else:
                final_dict[cce_params.JSON_KEY_KERNEL_LIST] += sub_dict[cce_params.JSON_KEY_KERNEL_LIST]
                for idx in range(len(final_dict[cce_params.JSON_KEY_PARAMETERS])):
                    if isinstance(sub_dict.get(cce_params.JSON_KEY_PARAMETERS)[idx], dict):
                        final_dict[cce_params.JSON_KEY_PARAMETERS][idx] = sub_dict.get(
                            cce_params.JSON_KEY_PARAMETERS)[idx]
                    elif isinstance(final_dict.get(cce_params.JSON_KEY_PARAMETERS)[idx], dict):
                        final_dict[cce_params.JSON_KEY_PARAMETERS][idx] = final_dict[
                            cce_params.JSON_KEY_PARAMETERS][idx]

            # Add coretype, parameters, taskRation, magic to sub kernel in kernelList
            # For GE compatibility, use values from non-MIX sub kernel json in main json.
            # CAUTION: atomic-clean of workspace in MIX sub kernel is not supported, as parameters in
            # MIX and non-MIX kernels are different, we can not tell which parameters to atomic-clean
            if aic_aiv_mix_combined and final_dict[cce_params.JSON_KEY_CORE_TYPE] == "MIX":
                if sub_dict[cce_params.JSON_KEY_CORE_TYPE] != "MIX":
                    final_dict[cce_params.JSON_KEY_CORE_TYPE] = sub_dict[cce_params.JSON_KEY_CORE_TYPE]
                    final_dict[cce_params.JSON_KEY_PARAMETERS] = sub_dict[cce_params.JSON_KEY_PARAMETERS]
                    final_dict["magic"] = sub_dict["magic"]
                    final_dict.pop('taskRation', None)
            if aic_aiv_mix_combined and sub_dict[cce_params.JSON_KEY_CORE_TYPE] == "MIX":
                final_dict[cce_params.JSON_KEY_KERNEL_LIST][-1][cce_params.JSON_KEY_CORE_TYPE] = "MIX"
                final_dict[cce_params.JSON_KEY_KERNEL_LIST][-1][cce_params.JSON_KEY_PARAMETERS] = sub_dict[
                    cce_params.JSON_KEY_PARAMETERS]
                final_dict[cce_params.JSON_KEY_KERNEL_LIST][-1]["taskRation"] = sub_dict["taskRation"]
                final_dict[cce_params.JSON_KEY_KERNEL_LIST][-1]["magic"] = sub_dict["magic"]
            # merge deterministic
            if cce_params.JSON_KEY_DETERMINISTIC in final_dict and cce_params.JSON_KEY_DETERMINISTIC in sub_dict:
                if final_dict[cce_params.JSON_KEY_DETERMINISTIC] == "ignore":
                    final_dict[cce_params.JSON_KEY_DETERMINISTIC] = sub_dict[cce_params.JSON_KEY_DETERMINISTIC]

    if "MIX_AIV" in core_type_dict:
        final_dict[cce_params.JSON_KEY_CORE_TYPE] = "MIX_AIV"
    return merge_json_file(final_aic_dict, final_aiv_dict, final_dict, max_param_size)


def read_deterministic_from_compiled_json(compiled_json):
    if 'kernelList' not in compiled_json:
        deterministic = compiled_json.get("deterministic")
    else:  # kernelList in compiled_json
        kernels_deterministic = list(
            map(lambda kernel: kernel.get("deterministic"),
                compiled_json.get("kernelList")))
        has_true = 'true' in kernels_deterministic
        has_false = 'false' in kernels_deterministic
        if has_true and has_false:
            logger.error("Both deterministic and non-deterministic results exist in the file kernel list (%s).",
                         compiled_json_path)
            return None

        if has_true:
            deterministic = 'true'
        elif has_false:
            deterministic = 'false'
        else:
            deterministic = 'ignore'

    return deterministic


class FatbinContext:
    fatbin_info = None

    class __FatbinInfo:
        def __init__(self):
            self.json_list = []
            self.target_path_list = []
            self.kernel_list = []

    def __init__(self, name, tiling_keys, exception_que, relocation_que):
        self.exception_que = exception_que
        self.relocation_que = relocation_que
        self.name = name
        self.tiling_keys = tiling_keys
        self.save_temp_file = False

    def __enter__(self):
        FatbinContext.fatbin_info = FatbinContext.__FatbinInfo()
        from tbe.common.platform.platform_info import KernelName
        KernelName.set_kernel_name(self.name)

    def __exit__(self, ptype, value, trace):
        from tbe.common.buildcfg import get_current_build_config
        from tbe.common.buildcfg.buildcfg_mapping import enable_op_prebuild
        if get_current_build_config(enable_op_prebuild):
            return
        # build_json and object file
        is_mid_process = self.__merge_multi_process_rules()
        if is_mid_process:
            return

        if not self.exception_que.empty():
            raise ValueError(self.exception_que.get())
        json_dict = parse_json_file(self.name, self.tiling_keys)

        self.__update_link_compile_info()

        from tbe.tvm.runtime.cce_runtime import gen_fatbin_file
        from tbe.common.buildcfg.buildcfg_mapping import save_temp_cce_file, op_debug_config, tbe_debug_level
        from te.platform import cce_params
        mix_dict_json_len = 2
        mix_dict_main_sub_json_len = 3
        if len(json_dict) == mix_dict_main_sub_json_len:
            gen_fatbin_file(self.name + cce_params.MIX_AIC_SUFFIX,
                            json_dict[1], False, True)
            gen_fatbin_file(self.name + cce_params.MIX_AIV_SUFFIX,
                            json_dict[2], False, True)
            gen_fatbin_file(self.name, json_dict[0], True, False)
        elif len(json_dict) == mix_dict_json_len:
            gen_fatbin_file(self.name + cce_params.MIX_AIC_SUFFIX,
                            json_dict[0], True, True)
            gen_fatbin_file(self.name + cce_params.MIX_AIV_SUFFIX,
                            json_dict[1], True, True)
        else:
            gen_fatbin_file(self.name, json_dict[0], True, True)

        from tbe.common.platform.platform_info import KernelName
        if (not get_current_build_config(save_temp_cce_file) and "dump_cce" not in get_current_build_config(
                op_debug_config) and get_current_build_config(tbe_debug_level) == 0):
            shutil.rmtree(get_kernel_meta_path(True))
        KernelName.set_kernel_name('')
        self.__update_jit_compile_info(json_dict)

    def __update_link_compile_info(self):
        # parent process collect linking info from child processes
        if self.relocation_que.qsize() == 0:
            self.relocation_que.close()
            return

        from tbe.common.context import get_context
        context = get_context()
        if context:
            context.add_compile_info("is_need_relocating", True)
        self.relocation_que.close()

    def __update_jit_compile_info(self, json_dict):
        from tbe.common.buildcfg import get_current_build_config
        from tbe.common.buildcfg.buildcfg_mapping import jit_compile_mode
        if get_current_build_config(jit_compile_mode) != 2:
            return
        from tbe.common.context import get_context
        context = get_context()
        context.add_compile_info("jit_kernel_func", self.name)
        from tbe.common.platform.platform_info import get_soc_spec
        from tbe.common.platform import FULL_SOC_VERSION
        from tbe.common.platform import AICORE_TYPE
        context.add_compile_info("soc_version", get_soc_spec(FULL_SOC_VERSION))
        context.add_compile_info("core_type", get_soc_spec(AICORE_TYPE))
        compile_res = json_dict[0]
        deterministic_mode = read_deterministic_from_compiled_json(compile_res)
        context.add_compile_info("jit_deterministic_mode", deterministic_mode)
        # update core_type with pass info
        if "core_type" in compile_res and compile_res["core_type"] == "AIV":
            context.add_compile_info("core_type", "VectorCore")

    def __merge_multi_process_rules(self):
        from tbe.dsl.base import operation
        context = operation.get_context()
        if context is None:
            return False
        op_process_num = context.get_op_compile_process_vars().get("num")
        op_process_idx = context.get_op_compile_process_vars().get("idx")
        if op_process_num > 1:
            build_list_ruls = context.get_op_compile_process_vars().get("build_list_ruls")
            op_process_value = context.get_op_compile_process_vars().get("op_process_value")
            with op_process_value.get_lock():
                op_process_value.value += 1

                if op_process_value.value < op_process_num:
                    build_list_ruls.put(self.tiling_keys)
                    return True
                else:
                    for process_idx in range(op_process_num - 1):
                        self.tiling_keys.extend(build_list_ruls.get())

        return False


@dataclass
class LowerToCceArgs:
    """
    LowerToCceArgs struct
    """
    index: int
    build_config: typing.Any
    args: typing.Any
    target: typing.Any
    target_host: typing.Any
    name: typing.Any
    tiling_key: typing.Any
    binds: typing.Any
    evaluates: typing.Any


def _update_relocation_que(relocation_que):
    if relocation_que is None:
        return
    from tbe.common.context import get_context
    context = get_context()
    if context and context.get_compile_info("is_need_relocating"):
        relocation_que.put(1)


def enable_vector_core_lower_to_cce(arg: LowerToCceArgs, exception_que, relocation_que):
    """
    Lower one schedule to cce when enable vector core.
    """
    from tvm.ir.transform import _ffi_transform_api
    from tvm.tir import transform
    _ffi_transform_api.EnterPassContext(arg.build_config)
    sche = transform.GetSchedule(arg.index)
    try:
        from tbe.common.platform.platform_info import set_soc_spec, get_soc_spec
        core_type = get_soc_spec("AICORE_TYPE")
        mix_core_type_list = ["AiCore", "VectorCore"]
        for mix_core_type in mix_core_type_list:
            set_soc_spec(mix_core_type)
            build_cce(sche, arg.args, target=arg.target, target_host=arg.target_host,
                      name=arg.name, tiling_key=arg.tiling_key, binds=arg.binds,
                      evaluates=arg.evaluates, relocation_que=relocation_que)
        set_soc_spec(core_type)
    except tvm.TVMError:
        var = traceback.format_exc()
        index = var.find('Current IR Stmt')
        exception_que.put(var[0:index + MAX_PRINT_IR_LINE_NUM])
    finally:
        pass
    exception_que.cancel_join_thread()
    relocation_que.cancel_join_thread()
    _ffi_transform_api.ExitPassContext(arg.build_config)


def is_v200():
    from tbe.common.platform.platform_info import get_soc_spec
    from tbe.common.platform.platform_info import ASCEND_310P
    return ASCEND_310P in get_soc_spec("SHORT_SOC_VERSION")


def lower_to_cce(arg: LowerToCceArgs, exception_que, relocation_que):
    """
    Lower one schedule to cce.
    """
    from tbe.common.buildcfg.buildcfg_mapping import enable_vector_core
    from tbe.common.buildcfg import get_current_build_config

    if get_current_build_config(enable_vector_core):
        if not is_v200():
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "enable_vector_core must be used in v200")
        enable_vector_core_lower_to_cce(arg, exception_que, relocation_que)
        return

    from tvm.ir.transform import _ffi_transform_api
    from tvm.tir import transform
    _ffi_transform_api.EnterPassContext(arg.build_config)
    sche = transform.GetSchedule(arg.index)
    try:
        build_cce(sche, arg.args, target=arg.target, target_host=arg.target_host,
                  name=arg.name, tiling_key=arg.tiling_key, binds=arg.binds,
                  evaluates=arg.evaluates, relocation_que=relocation_que)
    except tvm.TVMError:
        var = traceback.format_exc()
        index = var.find('Current IR Stmt')
        exception_que.put(var[0:index + MAX_PRINT_IR_LINE_NUM])
    finally:
        pass
    exception_que.cancel_join_thread()
    relocation_que.cancel_join_thread()
    _ffi_transform_api.ExitPassContext(arg.build_config)


def get_large_uint_value(large_uint_imm):
    return large_uint_imm.args[1].value << 32 | large_uint_imm.args[0].value


def parse_args(args_list, tiling_keys):
    arg_format = []
    # check for every case's args
    for i, x in enumerate(args_list):
        from tbe.common.buildcfg import get_current_build_config
        from tbe.common.buildcfg.buildcfg_mapping import parse_ddr_args
        if not get_current_build_config(parse_ddr_args):
            if not arg_format:
                for arg in x:
                    arg_format.append(arg.name)
            else:
                if len(arg_format) != len(x):
                    raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                         "input name of args must be same")
                for j, y in enumerate(arg_format):
                    if y != args_list[i][j].name:
                        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                             "input name of args must be same")

        if not isinstance(tiling_keys[i], int):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "tiling select rule must be int.")

        # when tiling key(rule) is Negative number, it will convert to number of a uint64,
        # then will not be a minus sign in kernel name.
        tiling_keys[i] &= 0xffffffffffffffff
        from tvm.runtime import _ffi_node_api
        tiling_keys[i] = _ffi_node_api.LargeUIntImm(
            "uint64", tiling_keys[i] & ((1 << 32) - 1), tiling_keys[i] >> 32, None)


def check_necessary_input(build_config_list, schedule_list, args_list, tiling_keys):
    from tbe.common.buildcfg.buildcfg_mapping import random_cce_file_location
    """check input which couldn't be none"""
    if not isinstance(schedule_list, (list, tuple)):
        schedule_list = [schedule_list]

    if not isinstance(build_config_list, list):
        build_config_list = [build_config_list] * len(schedule_list)
    elif len(build_config_list) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of build_config_list must be same with schedule_list or 1")

    """check all random_cce_file_location in build_config_list are same"""
    if len(build_config_list) > 1:
        random_cce_file_location_config = dict(build_config_list[0].config)[
            random_cce_file_location]
        for i in range(1, len(build_config_list)):
            if dict(build_config_list[i].config)[
                random_cce_file_location] != random_cce_file_location_config:
                raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                     "random_cce_file_location in build_config_list must be same")

    if not isinstance(args_list[0], list):
        args_list = [args_list] * len(schedule_list)
    elif len(args_list) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of args_list must be same with schedule_list or 1")

    if not isinstance(tiling_keys, list):
        if len(schedule_list) == 1:
            tiling_keys = [tiling_keys]
        else:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "size of tiling_key must be same with schedule_list")
    elif len(tiling_keys) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of tiling_key must be same with schedule_list")

    return [build_config_list, schedule_list, args_list, tiling_keys]


def check_other_input(schedule_list, target_list=None,
                      target_host_list=None, binds_list=None, evaluates_list=None):
    """check input which could be none"""
    if not isinstance(target_list, list):
        target_list = [target_list] * len(schedule_list)
    elif len(target_list) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of target_list must be same with schedule_list or 1")

    if not isinstance(target_host_list, list):
        target_host_list = [target_host_list] * len(schedule_list)
    elif len(target_host_list) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of target_host_list must be same with schedule_list or 1")

    if not binds_list or not isinstance(binds_list[0], list):
        binds_list = [binds_list] * len(schedule_list)
    elif len(binds_list) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of binds_list must be same with schedule_list or 1")

    if not isinstance(evaluates_list, list):
        evaluates_list = [evaluates_list] * len(schedule_list)
    elif len(evaluates_list) != len(schedule_list):
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "size of evaluates_list must be same with schedule_list or 1")
    return [target_list, target_host_list, binds_list, evaluates_list]


@dataclass
class BuildFatbinArgs:
    """
    BuildFatbinArgs struct
    """
    name: typing.Any
    build_config_list: typing.Any
    schedule_list: typing.Any
    args_list: typing.Any
    tiling_keys: typing.Any
    target_list: typing.Any
    target_host_list: typing.Any
    binds_list: typing.Any
    evaluates_list: typing.Any


def handle_process_in_list(process_list, thread_cnt, exception_que):
    while len(process_list) >= thread_cnt:
        end_task = []
        for p in process_list:
            if not p.is_alive():
                if p.exitcode == -11:
                    exception_que.put(
                        "probably due to Segmentation fault in subprocess.")
                end_task.append(p)
        for p in end_task:
            process_list.remove(p)


def parallel_fatbin_process(arg: BuildFatbinArgs, exception_que, process_num, relocation_que):
    """
    build_fatbin enable multithreading
    """
    from tvm.tir import transform
    process_list = []
    # transmit schedule, args and binds from python to c++,
    # then those could be used in subprocess
    for sch in arg.schedule_list:
        if not isinstance(sch, schedule.Schedule):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "Fatbin input must be a list of schedules.")
        transform.StoreResource(sch)
    deamon_status = multiprocessing.current_process().daemon
    multiprocessing.current_process().daemon = False
    # compile each kernel to object file and generate each json
    mp = multiprocessing.get_context('fork')
    for i, sch in enumerate(arg.schedule_list):
        lower_to_cce_args = LowerToCceArgs(i, arg.build_config_list[i], arg.args_list[i],
                                           arg.target_list[i], arg.target_host_list[i],
                                           arg.name, arg.tiling_keys[i], arg.binds_list[i],
                                           arg.evaluates_list[i])
        p = mp.Process(
            target=lower_to_cce, args=(lower_to_cce_args, exception_que, relocation_que))
        p.name = tvm.TBE_SUBPROCESS_PREFIX_FATBIN + p.name
        process_list.append(p)
        p.start()
        handle_process_in_list(process_list, process_num, exception_que)
    handle_process_in_list(process_list, 1, exception_que)
    multiprocessing.current_process().daemon = deamon_status
    transform.ClearResource()


def enable_vector_core_serial_fatbin_process(arg: BuildFatbinArgs, relocation_que):
    """
    build_fatbin prohibit multithreading when enable vector core
    """
    for i, sch in enumerate(arg.schedule_list):
        with arg.build_config_list[i]:
            from tbe.common.platform.platform_info import set_soc_spec, get_soc_spec
            core_type = get_soc_spec("AICORE_TYPE")
            mix_core_type_list = ["AiCore", "VectorCore"]
            for mix_core_type in mix_core_type_list:
                set_soc_spec(mix_core_type)
                build_cce(sch, arg.args_list[i], target=arg.target_list[i],
                          target_host=arg.target_host_list[i], name=arg.name,
                          tiling_key=arg.tiling_keys[i], binds=arg.binds_list[i],
                          evaluates=arg.evaluates_list[i], relocation_que=relocation_que)
            set_soc_spec(core_type)


def serial_fatbin_process(arg: BuildFatbinArgs, relocation_que):
    """
    build_fatbin prohibit multithreading
    """
    from tbe.common.buildcfg.buildcfg_mapping import enable_vector_core
    from tbe.common.buildcfg import get_current_build_config
    if get_current_build_config(enable_vector_core):
        if not is_v200():
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "enable_vector_core must be used in v200")
        enable_vector_core_serial_fatbin_process(arg, relocation_que)
        return

    for i, sch in enumerate(arg.schedule_list):
        with arg.build_config_list[i]:
            build_cce(sch, arg.args_list[i], target=arg.target_list[i],
                      target_host=arg.target_host_list[i], name=arg.name,
                      tiling_key=arg.tiling_keys[i], binds=arg.binds_list[i],
                      evaluates=arg.evaluates_list[i], relocation_que=relocation_que)


def get_tbe_parallel_compiler():
    parallel_fatbin = True
    process_num = 128
    tbe_parallel_compiler = os.getenv('TBE_PARALLEL_COMPILER')
    if tbe_parallel_compiler is None:
        return parallel_fatbin, process_num
    if tbe_parallel_compiler.isdigit():
        digit = int(tbe_parallel_compiler)
        if digit == 0:
            parallel_fatbin, process_num = False, 1
        elif digit > 0 and digit <= 128:
            parallel_fatbin, process_num = True, digit
    return parallel_fatbin, process_num


def build_fatbin(build_config_list, schedule_list, args_list, rules, name,
                 target_list="cce", target_host_list=None, binds_list=None, evaluates_list=None):
    """Build fatbin case, with parallel compilation or serial compilation"""
    from tbe.common.platform import get_soc_spec
    from tbe.common.buildcfg import get_current_build_config
    from tbe.common.buildcfg.buildcfg_mapping import enable_op_prebuild
    from tbe.common.buildcfg.buildcfg_mapping import kernel_meta_parent_dir
    if get_current_build_config(enable_op_prebuild) and get_soc_spec("cube_vector_combine") != "split":
        return
    tiling_keys = rules
    [build_config_list, schedule_list, args_list, tiling_keys] = check_necessary_input(
        build_config_list, schedule_list, args_list, tiling_keys)
    [target_list, target_host_list, binds_list, evaluates_list] = check_other_input(
        schedule_list, target_list, target_host_list, binds_list, evaluates_list)

    check_reset_op()
    from tvm.tir import transform

    output_dir = os.path.join(get_current_build_config(
        kernel_meta_parent_dir), "kernel_meta", name)
    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir, stat.S_IRWXU +
                        stat.S_IRGRP + stat.S_IXGRP, exist_ok=True)
        except OSError as err:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE, err)

    with build_config_list[0]:
        parse_args(args_list, tiling_keys)
        exception_que = multiprocessing.Queue()
        relocation_que = multiprocessing.Queue()
        args = BuildFatbinArgs(name, build_config_list, schedule_list, args_list, tiling_keys,
                               target_list, target_host_list, binds_list, evaluates_list)
        with FatbinContext(name, tiling_keys, exception_que, relocation_que):
            parallel_fatbin = True
            for i, sch in enumerate(schedule_list):
                if parallel_fatbin:
                    # if args range is different with python and schedule, can't use multiprocess
                    parallel_fatbin &= transform.VerifyArgs(
                        sch, args_list[i])
            if get_current_build_config(enable_op_prebuild):
                parallel_fatbin = False

            parallel_fatbin, process_num = get_tbe_parallel_compiler()
            if parallel_fatbin:
                parallel_fatbin_process(args, exception_que, process_num, relocation_que)
            else:
                serial_fatbin_process(args, relocation_que)


def check_block_sync(inputs, args):
    """set multicore sync shape for workspace"""
    from tbe.tvm.runtime import cce_runtime
    from tbe.common.platform.platform_info import multicore_sync_shape
    from tbe.common.buildcfg import get_current_build_config
    from tbe.common.buildcfg.buildcfg_mapping import enable_multicore_sync_with_atomic
    from tbe.common.platform import get_soc_spec
    from tbe.common.buildcfg.buildcfg_mapping import enable_vector_core
    cce_runtime.TBE_WORKSPACE_SIZE_LIST.local_list = []
    cce_runtime.TBE_WORKSPACE_SIZE_LIST.local_list = []
    cce_runtime.MULTI_CORE_SYNC_WORKSPACE_SIZE_LIST.local_list = []
    cce_runtime.TBE_WORKSPACE_IND_LIST.local_list = []
    soc_version = get_soc_spec("SHORT_SOC_VERSION")
    if soc_version != "Ascend310" and get_current_build_config(enable_multicore_sync_with_atomic):
        return
    block_sync_number = 0
    if isinstance(inputs, schedule.Schedule):
        block_sync_number = inputs.get_block_sync_size()
    elif isinstance(inputs, list):
        for i in inputs:
            if isinstance(i, schedule.Schedule):
                block_sync_number = max(
                    block_sync_number, i.get_block_sync_size())

    if block_sync_number != 0:
        tensor_num = 0
        for arg in args:
            if isinstance(arg, tvm.te.tensor.Tensor):
                tensor_num = tensor_num + 1
        for i in range(block_sync_number):
            tensor_name = "".join(["sync_", str(i)])
            tensor_block_sync = tvm.placeholder(
                (multicore_sync_shape(),), name=tensor_name, dtype="int64")
            cce_runtime.TBE_WORKSPACE_IND_LIST.local_list.append(
                tensor_num + i)
            # Determine if the multicore sync shape has been for workspace
            sync_not_exit = True
            if get_current_build_config(enable_vector_core):
                for arg in args:
                    if isinstance(arg, tvm.te.tensor.Tensor) and arg.op.name == tensor_name:
                        sync_not_exit = False
            if sync_not_exit:
                args.insert(tensor_num + i, tensor_block_sync)
            cce_runtime.TBE_WORKSPACE_SIZE_LIST.local_list.append(
                (tensor_name, multicore_sync_shape() * 8))  # 8 Byte/int64


def check_auto_atomic(enable_auto_atomic):
    from tbe.tvm.runtime import cce_runtime
    cce_runtime.TBE_ATUO_ATOMIC_IND_LIST.local_list = []
    for i, param in enumerate(enable_auto_atomic):
        if param != "":
            cce_runtime.TBE_ATUO_ATOMIC_IND_LIST.local_list.append(i)


@time_statistics
def build_cce(inputs,
              args=None,
              target=None,
              target_host=None,
              name="default_function",
              tiling_key=None,
              binds=None,
              evaluates=None,
              relocation_que=None) -> IRModule:
    """Build a function with arguments as sinature."""

    from tbe.common.platform import get_soc_spec
    from tbe.common.buildcfg import get_current_build_config
    from tbe.common.buildcfg import set_current_build_config
    # if CUBE_VECTOR_SPLIT is true, need continue to run until deduce the core type over
    from tbe.common.buildcfg.buildcfg_mapping import enable_op_prebuild
    from tbe.common.buildcfg.buildcfg_mapping import enable_auto_atomic
    from tbe.common.buildcfg.buildcfg_mapping import dynamic_shape
    from tbe.common.buildcfg.buildcfg_mapping import ub_fusion_tensors, ub_fusion_buffers
    if get_current_build_config(enable_op_prebuild) and "split" not in get_soc_spec("cube_vector_combine"):
        return
    from tbe.common.platform import get_soc_spec

    check_buffer_remap = (not get_current_build_config(dynamic_shape) or
                          not is_dynamic_args(args)) and \
                         get_buffer_manager().get_remapped_buffers() and \
                         not has_extern_op(inputs)
    if check_buffer_remap:
        buffer_remap(inputs, args, get_buffer_manager())

    clear_expr_attr()

    if get_buffer_manager().get_remapped_buffers():
        if has_extern_op(inputs):
            l1_fusion_tensors = get_l1_fusion_tensors(args, get_buffer_manager())
            set_current_build_config("tir.l1_fusion_tensors", l1_fusion_tensors)
            short_soc_version = get_soc_spec("SHORT_SOC_VERSION")
            if short_soc_version == "Ascend035":
                ub_tensors = get_ub_fusion_tensors(args, get_buffer_manager())
                set_current_build_config(ub_fusion_tensors, ub_tensors)
                ub_buffers=[]
                for bu in ub_tensors:
                    if bu in binds:
                        ub_buffers.append(binds[bu])
                set_current_build_config(ub_fusion_buffers, ub_buffers)
            reshaped_tensors = get_reshaped_tensors(args, get_buffer_manager())
            set_current_build_config("tir.reshaped_tensors", reshaped_tensors)
        get_buffer_manager().clear_remapped_buffers()

    check_block_sync(inputs, args)
    check_auto_atomic(get_current_build_config(enable_auto_atomic))

    from tbe.common.buildcfg import set_current_build_config
    from tbe.common.buildcfg.buildcfg_mapping import tbe_workspace_size_list_length
    set_current_build_config(tbe_workspace_size_list_length,
                             tvm.runtime.cce_runtime.tbe_workspace_size_list_length())

    from tbe.common.buildcfg.buildcfg_mapping import workspace_tensor_size
    from tbe.common.context import get_context
    context = get_context()
    if context:
        workspace_list = [e[0] for e in context.get_workspaces()]
        jsons = context.get_build_json_result(None)
        if "workspace" in jsons and not workspace_list:
            for key, value in jsons.items():
                if key == "workspace":
                    workspace_list = value["size"]
        from tvm.runtime import cce_runtime
        if cce_runtime.TBE_WORKSPACE_SIZE_LIST.local_list:
            pass_sync_workspaces_size = \
                [size for _, size in cce_runtime.TBE_WORKSPACE_SIZE_LIST.local_list]
            workspace_list = workspace_list + pass_sync_workspaces_size
        set_current_build_config(workspace_tensor_size, workspace_list)

    simple_mode = False
    from tbe.common.buildcfg.buildcfg_mapping import build_fatbin
    if not get_current_build_config(build_fatbin):
        check_reset_op()

    if tiling_key is None:
        from tvm.tir import expr
        tiling_key = expr.IntImm("int64", -1)

    if isinstance(inputs, schedule.Schedule):
        if evaluates is not None:
            from tbe.common.buildcfg.buildcfg_mapping import evaluate
            evaluates = update_evaluates(evaluates)
            set_current_build_config(evaluate, evaluates)

        if args is None:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "args must be given for build from schedule")

        schedule.CheckSchedule(inputs)
        input_mod = ffi.lower_schedule(
            inputs, args, name, binds, tiling_key, simple_mode)
        _count_time(input_mod)

    else:
        raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                             "Expected input to be an IRModule, PrimFunc or Schedule, but got, " + type(
                                 inputs))

    if get_current_build_config(enable_op_prebuild) and "split" in get_soc_spec("cube_vector_combine") and \
            len(input_mod.get_global_vars()) == 0:
        # enable_op_prebuild scene, prebuild stage do not need codegen
        return

    _generate_cce_code(input_mod, target, target_host)
    _update_relocation_que(relocation_que)


def _generate_cce_code(input_mod, target, target_host):
    from tvm.target import Target
    from tvm.driver import _ffi_api as _driver_ffi

    target_input_mod = {target: input_mod}

    for tar, mod in target_input_mod.items():
        if not isinstance(tar, (str, Target)):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "The key of inputs must be str or " "Target when inputs is dict.")
        if not isinstance(mod, tvm.IRModule):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "inputs must be Schedule, IRModule," "or dict of str to IRModule.")

    if not target_host:
        target_host = "llvm" if tvm.runtime.enabled("llvm") else "stackvm"

    target_input_mod, target_host = Target.check_and_update_host_consist(
        target_input_mod, target_host
    )

    _driver_ffi.preprocess_module(target_input_mod, target_host)


class NetworkParser:
    def __init__(self, json_config_params, debug_mode):
        self.json_config_dict = json_config_params
        self.compile_options = None
        self.fusion_kernel = None
        self.fusion_kernel_name = None
        self.kernel_list = None
        self.debug_mode = debug_mode
        self.attr_list = {"is_main_kernel": True}

    def parse_json(self):
        self.compile_options = self.json_config_dict["compile_options"]
        self.fusion_kernel = self.json_config_dict["fusion_kernel"]
        self.fusion_kernel_name = self.fusion_kernel["kernel_name"]
        self.kernel_list = self.json_config_dict["kernel_list"]

    def check_parse_params(self):
        try:
            value = bool(self.compile_options["kernel_fusion"])
        except Exception as e:
            raise RuntimeError("return value %s is not 'bool' type" % value) from e
        if self.compile_options["kernel_fusion"] is False:
            print("Do not generate cce when enable_model_fusion is false")
            return False
        idx = 0
        for kernel_item in self.kernel_list:
            kernel_name = kernel_item["kernel_name"]
            try:
                value = str(kernel_name)
            except Exception as e:
                raise RuntimeError("return value %s is not 'str' type" % value) from e

            kernel_args = kernel_item["kernel_args"]
            for args_offset in kernel_args:
                try:
                    value = int(args_offset)
                except Exception as e:
                    raise RuntimeError("return value %s is not 'str' type" % value) from e

            kernel_impl_header = kernel_item["kernel_impl_header"]
            try:
                value = str(kernel_impl_header)
            except Exception as e:
                raise RuntimeError("return value %s is not 'str' type" % value) from e
            try:
                with open(kernel_impl_header):
                    self.attr_list["header" + str(idx)] = kernel_impl_header
                    idx = idx + 1
            except OSError as e:
                raise RuntimeError("The header %s is not exit" % kernel_impl_header) from e
        self.attr_list["kernel_name"] = self.fusion_kernel_name
        self.attr_list["kernel_number"] = idx
        self.attr_list["debug_mode"] = self.debug_mode
        return True

    def get_call_args(self, kernel_name, kernel_args):
        call_args = ["uint64", kernel_name]
        for args_offset in kernel_args:
            if self.debug_mode:
                call_args.append(kernel_name + "_" + str(args_offset) + "_offset")
            else:
                call_args.append(args_offset)
        return call_args

    def build_ib_ir(self):
        ib = tvm.ir_builder.create()
        for kernel_item in self.kernel_list:
            kernel_name = kernel_item["kernel_name"]
            kernel_args = kernel_item["kernel_args"]
            call_args = self.get_call_args(kernel_name, kernel_args)
            ib.emit(tvm.call_extern(*call_args))
        body = ib.get()
        return body


def task_fusion(json_config_params, debug_mode=False):
    network_parser = NetworkParser(json_config_params, debug_mode)
    network_parser.parse_json()
    is_valid_params = network_parser.check_parse_params()
    if is_valid_params:
        from tbe.common.platform.platform_info import KernelName
        from tbe.common.buildcfg import set_current_build_config
        from tbe.common.buildcfg.buildcfg_mapping import enable_model_fusion
        from tvm.driver.build_module import get_binds
        body = network_parser.build_ib_ir()
        new_attrs = network_parser.attr_list
        new_attrs = tvm.ir.make_node("DictAttrs", **new_attrs)
        binds, arg_list = get_binds([])
        primfunc = tvm.IRModule.from_expr(tvm.tir.PrimFunc([], body, None, binds, new_attrs))
        KernelName.set_kernel_name(network_parser.fusion_kernel_name)
        set_current_build_config(enable_model_fusion, network_parser.compile_options["kernel_fusion"])
        from tbe.common.buildcfg import GlobalInfoContainer
        GlobalInfoContainer.global_info["is_supernetwork_header"] = True
        _generate_cce_code(primfunc, "cce", None)
    else:
        print("task fusion fail")
