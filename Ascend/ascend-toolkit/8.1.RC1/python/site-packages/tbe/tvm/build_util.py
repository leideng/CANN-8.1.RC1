#!/usr/bin/env python
# coding: utf-8
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
some util functions for build module
"""
from .te.tensor import PlaceholderOp
from .te import tensor
from .tir import expr as _expr
from .error_mgr import raise_tbe_python_err, TBE_DEFAULT_PYTHON_ERROR_CODE


def _is_zero(shape):
    if not shape:
        return True
    for it in shape:
        if it != 0:
            return False
    return True


def _l1_buffer_tile(sch, l1_op, shape):
    l1_bound = [(0, x) for x in shape]
    sch[l1_op].buffer_tile(*l1_bound)


def _buffer_fusion_data_flow_control(sch, args, buffer_manager):
    fusion_ops = {}
    from tbe.common.platform.platform_info import scope_cbuf
    from tbe.common.platform.platform_info import scope_cbuf_fusion
    from tbe.common.platform.platform_info import scope_ubuf_fusion, scope_ubuf
    from tbe.common.platform.platform_info import scope_gm
    from tbe.common.platform.platform_info import get_soc_spec
    for rb in buffer_manager.get_remapped_buffers():
        if rb.get_buffer_index() is None or \
                rb.get_buffer_index() >= len(args):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE, "buffer remap index exceed \
                                build tensor arguments list")
        cur_tensor = args[rb.get_buffer_index()]
        # set buffer scope
        scope = rb.get_buffer_scope()
        if scope == scope_cbuf_fusion:
            fusion_ops[cur_tensor.op] = rb.get_buffer_shape()
            if _is_zero(fusion_ops.get(cur_tensor.op)):
                fusion_ops[cur_tensor.op] = cur_tensor.shape
        if scope == scope_ubuf_fusion:
            fusion_ops[cur_tensor.op] = rb.get_buffer_shape()
            if _is_zero(fusion_ops.get(cur_tensor.op)):
                fusion_ops[cur_tensor.op] = cur_tensor.shape
        if scope != scope_gm:
            sch[cur_tensor].set_scope(scope)
    stack = []
    visited = set()
    for op in sch.outputs:
        stack.append(sch.stage_map[op].op)
        visited.add(sch.stage_map[op].op)
    while len(stack) != 0:
        op = stack[-1]
        stack.pop(-1)
        input_list = op.input_tensors
        for cur_input in input_list:
            if cur_input.op is None:
                continue
            is_nano_035 = get_soc_spec("SHORT_SOC_VERSION") == "Ascend035"
            if (cur_input.op in fusion_ops and
                ((sch[op].scope == scope_cbuf or
                sch[op].scope == scope_cbuf_fusion or
                sch[op].scope == scope_ubuf_fusion)
                or (is_nano_035 and sch[op].scope == scope_ubuf))):
                sch[op].compute_inline()
                _l1_buffer_tile(sch, cur_input.op, fusion_ops.get(cur_input.op))
            if cur_input.op not in visited:
                visited.add(cur_input.op)
                stack.append(cur_input.op)


def _lx_jump_data_bound(sch, args, buffer_manager):
    remap_args = buffer_manager.get_remapped_buffers()
    strides = buffer_manager.get_binds_strides(args)
    ori_strides = buffer_manager.get_original_strides(args)
    if not strides or not ori_strides:
        return
    for rb, stride, ori_stride in zip(remap_args, strides, ori_strides):
        if stride is None or ori_stride is None:
            continue
        if rb.get_buffer_index() is None or \
                rb.get_buffer_index() >= len(args):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE, "buffer remap index exceed \
                                build tensor arguments list")
        arg_tensor = args[rb.get_buffer_index()]
        # bind stride
        for ind, (axis, ori_axis) in enumerate(zip(stride, ori_stride)):
            if axis == ori_axis:
                continue
            if isinstance(arg_tensor.op, PlaceholderOp):
                sch[arg_tensor].bind_buffer(ind, axis, 0)
            else:
                sch[arg_tensor].bind_buffer(arg_tensor.op.axis[ind], axis, 0)


def _lx_add_arg_offset(sch, args, buffer_manager):
    remap_args = buffer_manager.get_remapped_buffers()
    for rb in remap_args:
        # bind offset
        offset = rb.get_buffer_offset()
        arg_tensor = args[rb.get_buffer_index()]
        if offset is None or offset == 0:
            continue
        _, dtype_bytes = get_align_factor(arg_tensor.dtype)
        if offset % dtype_bytes != 0:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE, "error offset for dtype", arg_tensor.dtype,
                                 "with offset", offset)
        offset = offset // dtype_bytes
        max_index = len(arg_tensor.shape) - 1
        if isinstance(arg_tensor.op, PlaceholderOp):
            sch[arg_tensor].bind_buffer(max_index, 1, offset)
        else:
            sch[arg_tensor].bind_buffer(
                arg_tensor.op.axis[max_index], 1, offset)


def buffer_remap(sch, args, buffer_manager):
    """remap the buffer for graph thread

    :param sch: schedule
    :param args: build arg tensors
    :param buffer_manager: buffer manager
    """

    # jump data read and write
    _lx_jump_data_bound(sch, args, buffer_manager)
    # l2 offset
    _lx_add_arg_offset(sch, args, buffer_manager)
    # l1/ub fusion data flow control
    _buffer_fusion_data_flow_control(sch, args, buffer_manager)
    buffer_manager.clear_remapped_buffers()


def has_extern_op(sch):
    """judge whether tensor in schedule outputs with extern op type exits
    to distinguish between tbe operator or tik/ir operator

    Parameters
    ----------
    sch: schedule

    Returns
    -------
    has_extern: true or false
    """
    has_extern = False
    for output in sch.outputs:
        if isinstance(output, tensor.ExternOp):
            has_extern = True
            break
    return has_extern


def get_l1_fusion_tensors(args, buffer_manager):
    """Internal function to get tensors which should do l1 fusion.

    Parameters
    ----------
    args : list of Tensor, the argument lists to the function.

    buffer_manager: buffer manager that interface with fe

    Returns
    -------
    l1_tensors: the list of tensors which should do l1 fusion.
    """
    l1_tensors = []
    remap_args = buffer_manager.get_remapped_buffers()
    from tbe.common.platform.platform_info import scope_cbuf_fusion
    for arg in remap_args:
        if arg.get_buffer_index() is None or \
                arg.get_buffer_index() >= len(args):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "buffer remap index exceed \
                                  build tensor arguments list")
        cur_tensor = args[arg.get_buffer_index()]
        if arg.get_buffer_scope() == scope_cbuf_fusion:
            l1_tensors.append(cur_tensor)
    return l1_tensors

def get_ub_fusion_tensors(args, buffer_manager):
    """Internal function to get tensors which should do ub fusion.

    Parameters
    ----------
    args : list of Tensor, the argument lists to the function.

    buffer_manager: buffer manager that interface with fe

    Returns
    -------
    ub_tensors: the list of tensors which should do ub fusion.
    """
    ub_tensors = []
    remap_args = buffer_manager.get_remapped_buffers()
    from tbe.common.platform.platform_info import scope_ubuf_fusion
    for arg in remap_args:
        if arg.get_buffer_index() is None or \
                arg.get_buffer_index() >= len(args):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "buffer remap index exceed \
                                  build tensor arguments list")
        cur_tensor = args[arg.get_buffer_index()]
        if arg.get_buffer_scope() == scope_ubuf_fusion:
            ub_tensors.append(cur_tensor)
    return ub_tensors


def get_reshaped_tensors(args, buffer_manager):
    """Internal function to get tensors binds their shapes
    which should do stride read or write.

    Parameters
    ----------
    args : list of Tensor, the argument lists to the function.

    buffer_manager: buffer manager that interface with fe

    Returns
    -------
    reshaped_tensors: the list of tensors binds their shapes
    which should do stride read or write.
    """
    reshaped_tensors = {}
    remap_args = buffer_manager.get_remapped_buffers()
    for arg in remap_args:
        if arg.get_buffer_index() is None or \
                arg.get_buffer_index() >= len(args):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "buffer remap index exceed \
                                  build tensor arguments list")
        cur_tensor = args[arg.get_buffer_index()]
        shape = arg.get_buffer_shape()
        thread_shape = arg.get_buffer_thread_shape()
        if shape is not None and thread_shape is not None and \
                len(thread_shape) != 0 and \
                shape != thread_shape:
            reshaped_tensors[cur_tensor] = [shape, thread_shape]
    return reshaped_tensors


def get_align_factor(dtype):
    """
    get_align_factor
    """
    # base on the diff data type, get the align_factor
    align_factor = 16
    dtype_bytes = 2
    if dtype in ('int8', 'uint8'):
        align_factor = 32
        dtype_bytes = 1
    elif dtype in ('float16', 'int16', 'uint16'):
        align_factor = 16
        dtype_bytes = 2
    else:
        align_factor = 8
        dtype_bytes = 4
    return align_factor, dtype_bytes


def is_dynamic_args(args):
    for arg in args:
        if isinstance(arg, _expr.Var) and (not arg.specified_range or arg.lower_bound != arg.upper_bound):
            return True
    return False
