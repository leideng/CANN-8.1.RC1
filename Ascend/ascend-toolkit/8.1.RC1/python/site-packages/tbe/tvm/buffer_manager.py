#!/usr/bin/env python
# coding: utf-8
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
buffer remap before lower
"""

import threading
from tvm.tir.expr import Var as _var
from tvm.error_mgr import raise_tbe_python_err, TBE_DEFAULT_PYTHON_ERROR_CODE


def is_all_shape_var_zero(shape):
    """Check shape var is or not zero"""
    for it in shape:
        if it != 0:
            return False
    return True


class RemappedBuffer:
    """RemappedBuffer

    Parameters
    ----------
    buffer_scope : str
        scope of input buffer.

    shape : tuple
        sliced shape of input buffer.

    thread_shape : tuple
        original shape of input buffer.

    arg_index : int
        input buffer argument position of build

    offset : int
        bytes offset of input buffer

    attr: dictionary
        extra attr for LXFusion
    """

    # Example: pylint: disable=too-many-arguments
    def __init__(self, arg_index, buffer_scope="global", shape=None, thread_shape=None, offset=None, attr=None):
        attr = {} if attr is None else attr
        self._buffer_scope = buffer_scope
        self._arg_index = arg_index
        self._offset = offset
        if not shape or is_all_shape_var_zero(shape):
            shape = ()
        self._shape = shape
        if not thread_shape or is_all_shape_var_zero(thread_shape):
            thread_shape = ()
        self._thread_shape = thread_shape
        self._attr = attr

    def set_buffer_scope(self, scope):
        """set buffer scope

        :param scope: str, "global" or "local.L1_Fusion"
        :return: None
        """
        self._buffer_scope = scope

    def get_buffer_scope(self):
        """get buffer scope

        :return: get scope of buffer
        """
        return self._buffer_scope

    def set_buffer_shape(self, shape):
        """set buffer shape

        :param shape: list[int]
        :return: None
        """
        if is_all_shape_var_zero(shape):
            shape = ()
        self._shape = shape

    def get_buffer_shape(self):
        """get buffer shape

        :return: list[int]
        """
        return self._shape

    def set_buffer_thread_shape(self, thread_shape):
        """set buffer thread shape

        :param thread_shape: list[int]
        :return: None
        """
        if is_all_shape_var_zero(thread_shape):
            thread_shape = ()
        self._thread_shape = thread_shape

    def get_buffer_thread_shape(self):
        """get buffer thread shape

        :return: list[int]
        """
        return self._thread_shape

    def set_buffer_index(self, arg_index):
        """set buffer index for tvm.build argument

        :param arg_index: int
        :return: None
        """
        self._arg_index = arg_index

    def get_buffer_index(self):
        """get buffer index of tvm.build argument

        :return: int
        """
        return self._arg_index

    def set_buffer_offset(self, offset):
        """set buffer bytes offset

        :param offset: int
        :return: None
        """
        self._offset = offset

    def get_buffer_offset(self):
        """get buffer bytes offset

        :return: int
        """
        return self._offset

    def set_buffer_attr(self, attr):
        """set buffer attribute for LXFusion

        :param attr: dictionary
        :return: None
        """
        self._attr = attr

    def get_buffer_attr(self):
        """get buffer attribute for LXFusion

        :return: dictionary
        """
        return self._attr


def get_strides(thread_shape, ori_shape, tensor_shape, stride):
    ori_stride_prev = 1
    tmp_thr = 1
    inc = len(tensor_shape) - 1
    for i in range(len(thread_shape) - 1, -1, -1):
        tmp_thr *= thread_shape[i]
        ori_stride_prev = ori_shape[i] * ori_stride_prev
        if tmp_thr < tensor_shape[inc]:
            if tmp_thr == 1:
                stride[-1] = ori_stride_prev
            continue
        for j in range(inc, -1, -1):
            if tensor_shape[j] != 0:
                tmp_thr = tmp_thr // tensor_shape[j]
                if tmp_thr != 1:
                    stride.append(stride[-1] * tensor_shape[j])
                else:
                    stride.append(ori_stride_prev)
                    inc = j - 1
                    break
        if tmp_thr != 1:
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 ("can not remap buffer" + str(tensor_shape) +
                                  "with" + str(thread_shape)))


class BufferManager:
    """BufferManager

    Parameters
    ----------
    _remapped_buffers : dict{int: RemappedBuffer}
        list of remapped buffers information.

    _l1_fusion_type : int in [-1, 0, 1]
        l1 fusion type, -1 for no fusion, 0 for deep fusion and
        1 for wide fusion
    _ub_fusion_type : int in [-1, 0, 1]
        ub fusion type, -1 for no fusion, 0 for deep fusion and
        1 for wide fusion

    _tensors2build_index: dict{tensor: int}
        map tensor to build index
    """

    def __init__(self):
        self._remapped_buffers = {}
        self._l1_fusion_type = -1
        self._ub_fusion_type = -1
        self._tensors2build_index = {}

    def set_remapped_buffers(self, remap_buffer_infos):
        """set remapped buffers information

        :param remap_buffer_infos: [RemappedBuffer]
        :return: None
        """
        for remap_info in remap_buffer_infos:
            self._remapped_buffers[remap_info.get_buffer_index()] = remap_info

    def get_remapped_buffers(self):
        """get remapped buffers information

        :return: buffers: [RemappedBuffer]
        """
        remap_list = sorted(self._remapped_buffers.items(),
                            key=lambda item: item[0])
        return [item[1] for item in remap_list]

    def clear_remapped_buffers(self):
        """clear global buffer remap information

        :return: None
        """
        self._remapped_buffers.clear()
        self._tensors2build_index.clear()

    def set_tensor_list(self, tensor_args):
        """get build args from buffer remapped index

        :param tensor_args: [tensors], arguments of tvm.build
        :return: None
        """
        self._tensors2build_index.clear()
        for idx, cur_tensor in enumerate(tensor_args):
            self._tensors2build_index[cur_tensor] = idx

    def get_tensor_list(self):
        """get remapped tensors if registered

        :return: [tensors], remapped arguments of tvm.build
        """
        tensor_list = sorted(self._tensors2build_index.items(),
                             key=lambda item: item[1])
        return [item[0] for item in tensor_list]

    def set_tensor_index(self, cur_dict):
        """set tensor index

        :param dict: {tensor:int}, map tensor to index
        :return: None
        """
        for tensor, index in cur_dict.items():
            self._tensors2build_index[tensor] = index

    def get_tensor_info(self, tensor):
        """get tensor l1 info

        :param tensor: tvm tensor: tensor to get l1 info
        :return: RemappedBuffer: l1info
        """
        if tensor not in self._tensors2build_index or \
                self._tensors2build_index.get(tensor) not in self._remapped_buffers:
            return None
        return self._remapped_buffers.get(self._tensors2build_index.get(tensor))

    def set_l1_fusion_type(self, l1_fusion_type):
        """set l1 fusion type

        :param l1_fusion_type: int in [-1, 0, 1]
        :return: None
        """
        if l1_fusion_type not in (-1, 0, 1):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "unsupport l1 fusion type " + l1_fusion_type)
        self._l1_fusion_type = l1_fusion_type

    def get_l1_fusion_type(self):
        """get l1 fusion type

        :return: int in [-1, 0, 1]
        """
        return self._l1_fusion_type

    def set_ub_fusion_type(self, ub_fusion_type):
        """set ub fusion type

        :param ub_fusion_type: int in [-1, 0, 1]
        :return: None
        """
        if ub_fusion_type not in (-1, 0, 1):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 "unsupport ub fusion type " + ub_fusion_type)
        self._ub_fusion_type = ub_fusion_type

    def get_ub_fusion_type(self):
        """get ub fusion type

        :return: int in [-1, 0, 1]
        """
        return self._ub_fusion_type

    def get_binds_strides(self, args):
        """get bind strides of remapped buffers

        :param args: [tensors] arguments of tvm.build
        :return: list[list[int]], get strides of remapped buffers
        """
        ret_strides = []
        for arg_index, r_b in self._remapped_buffers.items():
            ori_shape = r_b.get_buffer_shape()
            thread_shape = r_b.get_buffer_thread_shape()
            self._check_index_valid(arg_index, args)
            tensor_shape = args[arg_index].shape

            is_compress_op = False
            for it in tensor_shape:
                if isinstance(it, _var):
                    is_compress_op = True
                    break
            if is_compress_op:
                ret_strides.append(None)
                continue

            tensor_shape = [x.value for x in tensor_shape]
            stride = self._get_stride_from_shape(tensor_shape)
            # if no need buffer remap
            if not thread_shape or not ori_shape or thread_shape == ori_shape:
                ret_strides.append(stride)
                continue

            stride = [1, ]
            get_strides(thread_shape, ori_shape, tensor_shape, stride)
            stride = stride[:-1][::-1]
            ret_strides.append(stride)
        return ret_strides

    def get_original_strides(self, args):
        """get original strides of tvm.build tensors

        :param args: arguments of tvm.build
        :return: list[list[int]], original strides of tvm.build tensors
        """
        ret_strides = []
        for arg_index, rb in self._remapped_buffers.items():
            arg_index = rb.get_buffer_index()
            self._check_index_valid(arg_index, args)
            ori_shape = args[arg_index].shape

            is_compress_op = False
            for it in ori_shape:
                if isinstance(it, _var):
                    is_compress_op = True
                    break
            if is_compress_op:
                ret_strides.append(None)
                continue

            ori_shape = [x.value for x in ori_shape]
            strides = self._get_stride_from_shape(ori_shape)
            ret_strides.append(strides)
        return ret_strides

    @staticmethod
    def _check_index_valid(arg_index, args):
        if arg_index >= len(args):
            raise_tbe_python_err(TBE_DEFAULT_PYTHON_ERROR_CODE,
                                 ("buffer manager index " + arg_index +
                                  "exceed size of build args" + args))

    @staticmethod
    def _get_stride_from_shape(shape):
        strides = [
            1,
        ]
        for i in reversed(shape):
            strides.append(i * strides[-1])
        return strides[:-1][::-1]


BUFFER_MANAGER_LOCAL = threading.local()


def get_buffer_manager():
    """
    return local thread buffer manager
    :return: BufferManager
    """
    # 'pylint: disable=global-statement
    global BUFFER_MANAGER_LOCAL
    if not hasattr(BUFFER_MANAGER_LOCAL, 'buffer_manager'):
        BUFFER_MANAGER_LOCAL.buffer_manager = BufferManager()
    return BUFFER_MANAGER_LOCAL.buffer_manager
