#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d/conv3d backprop filter base info class
"""
from itertools import product

from tbe.common.utils import log
from tbe.common.utils.conv_util import ConvDilations
from tbe.common.utils.conv_util import ConvFeatureMap
from tbe.common.utils.conv_util import ConvFilterSize
from tbe.common.utils.conv_util import ConvGrads
from tbe.common.utils.conv_util import ConvKernel
from tbe.common.utils.conv_util import ConvPads
from tbe.common.utils.conv_util import ConvStrides
from tbe.common.utils.conv_util import CubeConstantConfig
from tbe.common.utils.errormgr import error_manager_cube

from .util import LoadModeType


class ConvBpFilterBase:

    def __init__(self, inputs_list, op_name, fusion_mode=False, options=None):
        """
        inputs_list: x, filter_size, out_backprop, y, strides, pads, dilations, groups, data_format, kernel_name
        """
        self.inputs_list = inputs_list
        self.fusion_mode = fusion_mode
        self.op_name = op_name
        self._print_inputs_for_debug(options)
        self._check_inputs_format()
        self._new_or_update_self_mem()

    @staticmethod
    def _format_unknown_rank_shape(x, is_origin):
        if is_origin:
            cur_format = x.get("ori_format")
            cur_shape = x.get("ori_shape")
            shape_name = "ori_shape"
            range_name = "ori_range"
        else:
            cur_format = x.get("format")
            cur_shape = x.get("shape")
            shape_name = "shape"
            range_name = "range"
        shape_len = CubeConstantConfig.FROMAT_TO_FIX_DIMS.get(cur_format)

        if list(cur_shape) == CubeConstantConfig.DYNAMIC_RANK_SHAPE:
            x[shape_name] = [-1] * shape_len
            x[range_name] = [CubeConstantConfig.NO_RANGE] * shape_len

    @staticmethod
    def _format_empty_tensor(x, is_origin):
        """
        not support empty tensor now
        just change 0 to -1 to ensure compile success. FE ensure it will not execute actually.
        """
        if is_origin:
            shape_name = "ori_shape"
        else:
            shape_name = "shape"
        x[shape_name] = list(-1 if dim == 0 else dim for dim in x.get(shape_name))

    @staticmethod
    def _format_range(x, is_origin):
        if is_origin:
            cur_shape = x.get("ori_shape")
            range_name = "ori_range"
        else:
            cur_shape = x.get("shape")
            range_name = "range"
        shape_len = len(cur_shape)

        # init range
        if not x.get(range_name):
            x[range_name] = [CubeConstantConfig.NO_RANGE] * shape_len
        else:
            if shape_len != len(x[range_name]):
                error_manager_cube.raise_err_specific(CubeConstantConfig.CONV2D_BACKPROP_FILTER_OP_NAME,
                                                      "length of shape should be equal with length of range")
            new_range = []
            for axis_range in x[range_name]:
                new_axis_range = list(axis_range) if axis_range else CubeConstantConfig.NO_RANGE
                new_range.append(new_axis_range)
            x[range_name] = new_range

        # axis ==> range
        # n    ==> [n, n]
        # -1   ==> [1, -1] -> [1, None]
        #      ==> [0, n]  -> [1, n]
        #      ==> [m, n]  -> [m, n]
        for i in range(shape_len):
            if cur_shape[i] != -1:
                x[range_name][i] = [cur_shape[i]] * 2
            else:
                if list(x[range_name][i]) == [1, -1]:
                    x[range_name][i] = CubeConstantConfig.NO_RANGE
                else:
                    x[range_name][i] = [max(x[range_name][i][0], 1), x[range_name][i][1]]

    def format_shape_and_range(self):
        x, _, out_backprop, y, _, _, _, _, _, _ = self.inputs_list
        tensor_list = [x, out_backprop, y]
        for input_tensor, is_origin in list(product(tensor_list, [True, False])):
            self._format_unknown_rank_shape(input_tensor, is_origin)
            self._format_empty_tensor(input_tensor, is_origin)
            self._format_range(input_tensor, is_origin)
        self._new_or_update_self_mem()

    def _print_inputs_for_debug(self, options=None):
        inputs_names = [
            "x", "filter_size", "out_backprop", "y", "strides", "pads", "dilations", "groups", "data_format",
            "kernel_name", "options", "fusion_mode"
        ]
        for i_name, i_value in zip(inputs_names, self.inputs_list + [options] + [self.fusion_mode]):
            log.debug("[{}] {} = {}".format(self.op_name, i_name, i_value))

    def _check_inputs_format(self):
        pass

    def _new_or_update_self_mem(self):
        x, filter_size, out_backprop, y, strides, pads, dilations, groups, data_format, kernel_name = self.inputs_list
        self.fm = ConvFeatureMap(x)
        self.grads = ConvGrads(out_backprop)
        self.kernel = ConvKernel(y)
        self.filter_size = ConvFilterSize(filter_size, self.kernel.ori_format)
        self.strides = ConvStrides(strides, data_format)
        self.pads = ConvPads(pads)
        self.dilations = ConvDilations(dilations, data_format)
        self.groups = groups
        self.data_format = data_format
        self.kernel_name = kernel_name


class ComputeTemplate:

    def __init__(self) -> None:
        self.c04_flag = False  # only support static
        self.conv1d_flag = False  # no need in dynamic
        self.flag_all_one_case = False  # load2d mode; static and dynami
        self.flag_load3d_special_case = False  # just static; load3d_special_multiply instand dynamic
        self.flag_load3d_w_split_case = False  # static and dynamic
        self.l0b_dma_flag = False  # static and dynamic
        self.strideh_read_flag = 0  # static and dynamic
        self.load_mode = LoadModeType.LOAD3D
        self.linear_embedding_opti_flag = False #static and dynamic

    def get_debug_info(self):
        return self.__dict__
