#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
classifier of shape in tuple reduce
"""
import copy
from enum import Enum
from enum import auto
from functools import reduce
from itertools import groupby
# Standard Packages
from typing import AnyStr
from typing import Dict
from typing import List
from typing import Tuple

# Ascend Packages
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.dsl.base.operation import get_context

from . import shape_classifier
from . import util

# Constants
TUPLE_REDUCE = "tuple_reduce"
MAX_DIM_LEN = 8


class EliminateMode(Enum):
    """
    Enumerate eliminate mode
    """
    NORMAL = auto()
    LAST = auto()


def _raise_error(message: AnyStr):
    """
    Raise error
    @param message:
    @return:
    """
    dict_args = {"errCode": "E90001", "detailed_cause": message}
    raise RuntimeError(dict_args, get_error_message(dict_args))


def _binary_mode_detection(ins: List) -> List[List]:
    """
    Generate (-1, ...) if find -2
    @param ins:
    @return:
    """
    inputs = [list(tensor.get('shape')) for tensor in ins]
    for idx, input_shape in enumerate(inputs):
        if -2 in input_shape:
            inputs[idx] = [-1 for _ in range(MAX_DIM_LEN)]
    return inputs


def _one_hot_encoding(lst, length): return [1 if i in lst else 0 for i in range(length)]


def _one_hot_decoding(lst): return [i for i, v in enumerate(lst) if v == 1]


def bin2dec(lst): return sum([v * (2 ** (len(lst) - i - 1)) for i, v in enumerate(lst)])


def _get_broadcast_axes(inputs, extra_params):
    """
    Get broadcast axes from extra_params
    @param inputs:
    @param extra_params:
    @return:
    """
    inputs_num = len(inputs)
    max_shape_len = max([len(_shape) for _shape in inputs])
    broadcast_axes_one_hot = [_one_hot_encoding([], max_shape_len) for i in range(inputs_num)]
    if "compile_broadcast_axis" not in extra_params:
        return broadcast_axes_one_hot

    compile_broadcast_axis = extra_params.get("compile_broadcast_axis")
    for k, v in compile_broadcast_axis.items():
        broadcast_axes_one_hot[k] = _one_hot_encoding(v, len(inputs[k]))
    return broadcast_axes_one_hot


def _check_bundle_eliminate(bundle):
    """
    check whether bundle enable eliminate
    @param bundle:
    @return:
    """
    last_column = [v[-1] for v in bundle.fused_inputs]
    if not len(set(last_column)) == 1:
        return [EliminateMode.NORMAL]
    if last_column[0] not in [-1, 1]:
        return [EliminateMode.NORMAL]
    if bundle.fused_reduce_axis_one_hot[-1] == 1:
        return [EliminateMode.NORMAL]
    if all(bundle.fused_reduce_axis_one_hot[:-1]):
        return [EliminateMode.NORMAL]
    if last_column[0] == 1:
        return [EliminateMode.LAST]
    else:
        return [EliminateMode.NORMAL, EliminateMode.LAST]
    return [EliminateMode.NORMAL]


@shape_classifier.register_classifier(shape_classifier.TUPLE_REDUCE)
def classify(ins: List, extra_params: Dict) -> List[List]:
    """
    classify
    This module has the following steps:
    1. Make the shape length consistent by adding one in front of the shape.
    2. Merge consecutive axes of the same type according to the reduce and broadcast axes.
    3. For the last unknown axis of non-reduce non-broadcast type, generate a scenario where the axis is 1.
    @param ins: inputs list, the last element defaults to the reduce_axis.
    @param extra_params:
        "compile_broadcast_axis": dict, key is input index, value is compile broadcast axes.
    @return: list of all possible inputs scenarios.
    """
    # ParamCheck
    if extra_params is not None and not isinstance(extra_params, Dict):
        _raise_error("extra_params must be a dict or None when mode is {mode}".format(mode=TUPLE_REDUCE))
    if extra_params is None:
        extra_params = {}
    if len(ins) < 2 or not isinstance(ins[-1], (List, Tuple)):
        _raise_error("The last element in the {mode} classify must be a List or Tuple "
                     "which is reduce_axis".format(mode=TUPLE_REDUCE))

    # Analysis Inputs
    _ins = copy.deepcopy(ins)
    reduce_axis = _ins.pop()
    inputs = _binary_mode_detection(_ins)

    # Get max shape length / inputs num
    inputs_num = len(inputs)
    max_shape_len = max([len(_shape) for _shape in inputs])
    # Get Reduce/Broadcast Axes OneHot
    reduce_axis_one_hot = _one_hot_encoding(reduce_axis, max_shape_len)
    broadcast_axes_one_hot = _get_broadcast_axes(inputs, extra_params)
    add_compile_info_inner("_reduce_axis", reduce_axis_one_hot[:])

    # Adjust shape length
    for i, (vi, vb) in enumerate(zip(inputs, broadcast_axes_one_hot)):
        inputs[i] = [1] * (max_shape_len - len(vi)) + vi
        broadcast_axes_one_hot[i] = [1] * (max_shape_len - len(vb)) + vb

    # Infershape
    for j in range(max_shape_len):
        for i in range(inputs_num):
            if broadcast_axes_one_hot[i][j] == 1:
                inputs[i][j] = 1
    for j in range(max_shape_len):
        vals = [inputs[i][j] for i in range(inputs_num)]
        val = -1 if -1 in vals else max(vals)
        for i in range(inputs_num):
            if broadcast_axes_one_hot[i][j] == 0:
                inputs[i][j] = val

    # Construct instance
    bundle = TupleReduceInsBundle(inputs, broadcast_axes_one_hot, reduce_axis_one_hot, ins, extra_params)
    basic = TupleReduceClassifierBasic(bundle)
    eliminate = TupleReduceClassifierEliminate(bundle)

    classify_outs = []
    eliminate_mode = _check_bundle_eliminate(bundle)
    if EliminateMode.NORMAL in eliminate_mode:
        classify_outs.append(basic.out())
    if EliminateMode.LAST in eliminate_mode:
        classify_outs.append(eliminate.out())

    return classify_outs


class TupleReduceClassifierBasic:
    """
    Tuple Reduce Classifier Basic
    """

    def __init__(self, bundle) -> None:
        self.ins = copy.deepcopy(bundle.ins)
        self.fused_inputs = copy.deepcopy(bundle.fused_inputs)
        self.fused_reduce_axis_one_hot = copy.deepcopy(bundle.fused_reduce_axis_one_hot)
        self.classify_key = None

    def out(self):
        self.classify_key = bin2dec([1] + self.fused_reduce_axis_one_hot)
        _ins = copy.deepcopy(self.ins)
        _ins[-1] = _one_hot_decoding(self.fused_reduce_axis_one_hot)
        for i, v in enumerate(self.fused_inputs):
            _ins[i].update({"shape": v})
            _ins[i].update({"range": [(1, None) if value == -1 else (value, value) for value in v]})
        return _ins


class TupleReduceClassifierEliminate(TupleReduceClassifierBasic):
    """
    Tuple Reduce Classifier Eliminate
    """

    def __init__(self, bundle) -> None:
        super().__init__(bundle)
        self.fused_reduce_axis_one_hot = self.fused_reduce_axis_one_hot[:-1]
        for i, v in enumerate(self.fused_inputs):
            self.fused_inputs[i] = v[:-1]


class TupleReduceInsBundle:
    """
    Tuple Reduce ins Bundle
    """

    def __init__(self, inputs, broadcast_axes_one_hot, reduce_axis_one_hot, ins, extra_params) -> None:
        # save args
        self.inputs = inputs
        self.broadcast_axes_one_hot = broadcast_axes_one_hot
        self.reduce_axis_one_hot = reduce_axis_one_hot
        self.ins = ins
        self.extra_params = extra_params

        self.inputs_num = len(inputs)
        self.max_shape_len = max([len(v) for v in inputs])
        self.discard_axis_one_hot = None
        self.insert_top_normal_flag = False

        self.fusible_code = None
        self.fused_inputs = None
        self.fused_reduce_axis_one_hot = None
        self.fused_broadcast_axes_one_hot = None
        self.fused_inputs_num = None
        self.fused_max_shape_len = None

        self.eliminate_useless_reduce()
        self.insert_top_reduce()
        self.insert_top_normal()
        self.calc_fusible_code()
        self.fuse_axes()
        self.is_const()
        self.add_compile_info()

    def eliminate_useless_reduce(self):
        """
        If we already known which axis is 1 at compile time,
        and the op semantic still want to reduce in this direction,
        just eliminate this axis and remove the reduce direction.
        """
        def eliminate_by_one_hot(x, e): return [v for i, v in enumerate(x) if e[i] == 0]

        # Find useless reduce axis
        self.discard_axis_one_hot = [0] * self.max_shape_len
        for j in range(self.max_shape_len):
            if all(self.inputs[i][j] == 1 for i in range(self.inputs_num)):
                self.discard_axis_one_hot[j] = 1
            else:
                self.discard_axis_one_hot[j] = 0
        # update
        for i in range(self.inputs_num):
            self.inputs[i] = eliminate_by_one_hot(self.inputs[i], self.discard_axis_one_hot)
            self.broadcast_axes_one_hot[i] = eliminate_by_one_hot(self.broadcast_axes_one_hot[i],
                                                                  self.discard_axis_one_hot)
        self.reduce_axis_one_hot = eliminate_by_one_hot(self.reduce_axis_one_hot, self.discard_axis_one_hot)
        self.max_shape_len = max(len(v) for v in self.inputs)


    def insert_top_reduce(self):
        """
        This function is used to deal with some unexpected situations,
        such as do reduce operation on an axis which is already one.
        Insert a reduce axis on the top of the shape,
        to keep the pattern of the compute graph still as tuple-reduce.
        """
        if self.reduce_axis_one_hot and max(self.reduce_axis_one_hot):
            return

        # update
        self.reduce_axis_one_hot = [1] + self.reduce_axis_one_hot
        for i in range(self.inputs_num):
            self.inputs[i] = [1] + self.inputs[i]
            self.broadcast_axes_one_hot[i] = [0] + self.broadcast_axes_one_hot[i]
        self.max_shape_len = max(len(v) for v in self.inputs)


    def insert_top_normal(self):
        """
        This function is used to deal with some unexpected situations,
        such as all reduce without any normal axis to split.
        Insert a normal axis on the top of the shape,
        to keep the iteration type has at least 2 types.
        """
        if not all(self.reduce_axis_one_hot):
            return
        
        # update
        self.reduce_axis_one_hot = [0] + self.reduce_axis_one_hot
        for i in range(self.inputs_num):
            self.inputs[i] = [1] + self.inputs[i]
            self.broadcast_axes_one_hot[i] = [0] + self.broadcast_axes_one_hot[i]
        self.max_shape_len = max(len(v) for v in self.inputs)
        self.insert_top_normal_flag = True


    def calc_fusible_code(self):
        def f_encode(reduce_bool, broadcast_list):
            length = len(broadcast_list)
            broadcast_part = bin2dec([1] + broadcast_list)
            reduce_part = bin2dec([1, reduce_bool, 0] + [0] * len(broadcast_list))
            return reduce_part + broadcast_part

        self.fusible_code = [0] * self.max_shape_len
        for j in range(self.max_shape_len):
            reduce_bool = self.reduce_axis_one_hot[j]
            broadcast_list = [self.broadcast_axes_one_hot[i][j] for i in range(self.inputs_num)]
            self.fusible_code[j] = f_encode(reduce_bool, broadcast_list)


    def fuse_axes(self):
        def product(lst): return reduce(lambda x, y: x * y, lst)
        fuse_rules = [(k, list(g)) for k, g in groupby(tuple(enumerate(self.fusible_code)), lambda i: i[1])]
        self.fused_inputs = []
        self.fused_broadcast_axes_one_hot = []

        self.fused_reduce_axis_one_hot = [self.reduce_axis_one_hot[g[0][0]] for k, g in fuse_rules]
        for i in range(self.inputs_num):
            fused_shape = []
            fused_broadcast = []
            for k, g in fuse_rules:
                g_i = [_pair[0] for _pair in g]
                fused_shape_g_v = [self.inputs[i][j] for j in g_i]
                fused_shape_v = -1 if -1 in fused_shape_g_v else product(fused_shape_g_v)
                fused_shape.append(fused_shape_v)

                fused_broadcast_g_v = [self.broadcast_axes_one_hot[i][j] for j in g_i]
                fused_broadcast_v = fused_broadcast_g_v[0]
                fused_broadcast.append(fused_broadcast_v)
            self.fused_inputs.append(fused_shape)
            self.fused_broadcast_axes_one_hot.append(fused_broadcast)

        self.fused_inputs_num = len(self.fused_inputs)
        self.fused_max_shape_len = max([len(v) for v in self.fused_inputs])


    def is_const(self):
        """
        const mode or dynamic mode
        """
        is_const = True
        for shape in self.fused_inputs:
            if -1 in shape:
                is_const = False
                break
        add_compile_info_inner("_is_const", is_const)


    def add_compile_info(self):
        """
        add compile info
        """
        add_compile_info_inner("_discard_axis", self.discard_axis_one_hot[:])
        add_compile_info_inner("_fusible_code", self.fusible_code[:])
        add_compile_info_inner("_insert_top_normal_flag", self.insert_top_normal_flag)
        add_compile_info_inner("_dim_vars", {})
