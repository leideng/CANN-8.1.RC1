#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
classifier of shape in elewise
"""
from functools import reduce as product
from typing import Any
from typing import Dict
from typing import Optional

from tbe.common.buildcfg import get_current_build_config
from tbe.dsl.base import expr_compare
from tbe.dsl.base import operation

from . import shape_classifier
from . import util


C0_MAPPING = {
    "uint1": 256,
    "int4": 64,
    "bool": 32,
    "int8": 32,
    "uint8": 32,
    "float16": 16,
    "int16": 16,
    "uint16": 16,
    "float32": 16,
    "int32": 16,
    "uint32": 16,
    "int64": 4,
    "uint64": 4,
    "bfloat16": 16,
    "complex32": 8,
    "complex64": 4
}


class ClassifyConst:
    """
    elewise classify const
    """
    FHD_SHAPE_LEN = 5
    INT32_MAX = 2147483647
    MAX_DIM_LEN = 8
    NCHW_PAD_C_AXIS = 1
    NHWC_PAD_C_AXIS = 3
    UNKNOWN_RANK = -2
    LEAST_BROADCAST_INPUTS = 2
    MAX_CONST_RES_NUM = 2
    MAX_CHANNEL_DIFF_NUM = 2


class ElewiseMode:
    """
    elewise classify mode
    """
    ALL_FUSE = "all_fuse"
    CONST = "const"
    EMPTY = "empty"
    DISABLE_FUSE = "disable_fuse"


class ElewisePattern:
    """
    elewise classify pattern
    """
    BROADCAST_SCALAR = "broadcast_scalar"
    CONST_SHAPE_DIFF = "const_shape_diff"
    CONST_SHAPE_SAME = "const_shape_same"
    DISABLE_FUSE = "disable_fuse"
    EMPTY = "empty"
    FRACTAL_FORMAT = "fractal_format"
    ONE_RANK = "one_rank"
    PURE_ELEWISE = "pure_elewise"
    SCALAR_BROADCAST = "scalar_broadcast"


class EleOptParams:
    """
    elewise schedule compile options
    "disable_optimization"(bool): determine whether only gen no fuse res, default False
    "ignore_fractal_format"(bool): determine whether sensitive with fractal format, default True
    "is_pure_elewsie"(bool): determine if only generate shape same classify res, defult True
    """
    DISABLE_OPTIMIZATION = "disable_optimization"
    IGNORE_FRACTAL_FORMAT = "ignore_fractal_format"
    IS_PURE_ELEWISE = "is_pure_elewise"


class EleCompileInfo:
    """
    elewise compile info for optiling
    """
    ELEWISE_DISABLE_FUSE = "_elewise_disable_fuse"
    INPUTS_ELE_IN_BLOCK = "_inputs_ele_in_block"
    IS_PURE_ELEWISE = "_is_pure_elewise"
    ENABLE_FRACTAL_FORMAT = "_enable_fractal_format"


@shape_classifier.register_classifier(shape_classifier.ELEWISE)
def classify(ins: list, extra_params: Optional[Dict[str, Any]] = None):
    """elewise classify to generate all possible res, including base_func res and optimized res
    Args:
        ins (list): original or processed or selective inputs from ops and UB fusion
        extra_params (Optional[Dict[str, Any]], optional): _optimize attributes, default None
    Returns:
        list[Dict]: classify ins
    """
    classify_ins = ElewiseClassifier(ins, extra_params).classify()

    return [classify_ins[-1]] if get_current_build_config("enable_op_prebuild") else classify_ins


def element_multiply(inputs):
    """get product res
    Args:
        inputs (list/tuple): inputs which may include None
    Returns:
        int: product of inputs
    """
    if -1 in inputs:
        return -1
    if None in inputs:
        return None
    return product(lambda x, y: x * y, inputs)


def get_original_channels():
    """
    get original channels from operation, elewise only support all inputs channel value same
    """
    all_input_channels = operation.get_op_context().get_addition("input_c_values")
    expr_compare.is_true(all_input_channels,
                         {"errCode": "E90001",
                          "detailed_cause": "original channels can not be None while matching 5hd scene."})

    # check elewise channels valid
    channel_diff_shapes = set(all_input_channels)
    expr_compare.is_true(len(channel_diff_shapes) <= ClassifyConst.MAX_CHANNEL_DIFF_NUM,
                         {"errCode": "E90001",
                          "detailed_cause": f"elewise 5hd not support more than two channel values while now exists: "
                                            f"{channel_diff_shapes}."})

    if len(channel_diff_shapes) == ClassifyConst.MAX_CHANNEL_DIFF_NUM:
        expr_compare.is_true(-1 in channel_diff_shapes,
                            {"errCode": "E90001",
                             "detailed_cause": f"elewise 5hd not support two channels without -1 while now is: "
                                               f"{channel_diff_shapes}."})
        channel_diff_shapes.remove(-1)
    return channel_diff_shapes.pop()


class ElewiseClassifier:
    """
    elewise classifier
    """
    def __init__(self, ins: list, extra_params: Optional[Dict[str, Any]]):
        """elewise classify params init
        Args:
            ins (list[Dict]): classify inputs list, each input at least contain 'shape', 'dtype' and 'range'.
            extra_params (Optional[Dict[str, Any]]): optional classify assist params, enable some special funcs.
        """
        # record origin information
        self.ins = ins
        self.classify_inputs_num = len(ins)
        extra_params = {} if extra_params is None else extra_params
        self.disable_optimization = extra_params.get(EleOptParams.DISABLE_OPTIMIZATION, False)
        self.ignore_fractal_format = extra_params.get(EleOptParams.IGNORE_FRACTAL_FORMAT, True)
        self.is_pure_elewise = extra_params.get(EleOptParams.IS_PURE_ELEWISE, True)
        self.elewise_ori_c = None

        # unknown rank inputs check and update
        self.is_unknown_rank = self._check_update_unknown_rank()
        self.max_dim_len = max(len(_ins.get("shape")) for _ins in self.ins)

        # empty check
        self.maybe_empty_tensor = self._check_update_empty()

        # 5hd check
        self.elewise_ori_c = None
        self.enable_5hd_format = self._check_strict_5hd_conditions()
        if self.enable_5hd_format:
            expr_compare.is_true(not self.is_unknown_rank,
                                 {"errCode": "E90001",
                                  "detailed_cause": "5hd scene not support shape binary input."})

        # align all inputs to same len
        self.processed_ins = self._inputs_lens_align()
        self.processed_shapes = [_ins.get("shape") for _ins in self.processed_ins]
        self.processed_ranges = [_ins.get("range") for _ins in self.processed_ins]
        self._update_shape_range()

        # add compile info
        input_ele_in_block = \
            [C0_MAPPING.get(_ins.get("dtype", _ins.get("data_type"))) for _ins in self.ins]
        operation.add_compile_info_inner(EleCompileInfo.INPUTS_ELE_IN_BLOCK, input_ele_in_block)
        operation.add_compile_info_inner(EleCompileInfo.ELEWISE_DISABLE_FUSE, self.disable_optimization)
        operation.add_compile_info_inner(EleCompileInfo.IS_PURE_ELEWISE, self.is_pure_elewise)
        operation.add_compile_info_inner(EleCompileInfo.ENABLE_FRACTAL_FORMAT, self.enable_5hd_format)

    def classify(self):
        """
        classify
        :return: classify res
        """
        return self._classify_const() if self._is_const() else self._classify_var()

    def _check_update_unknown_rank(self):
        """
        check if inputs contains unknown rank shapes
        """
        is_unknown_rank = False
        for _ins in self.ins:
            shapes = list(_ins.get('shape'))
            if ClassifyConst.UNKNOWN_RANK in shapes:
                expr_compare.is_true(len(shapes) == 1,
                                     {"errCode": "E90001",
                                      "detailed_cause": f"unknown rank only support [-2] or (-2,), now is {shapes}."})
                _ins["shape"] = [-1] * ClassifyConst.MAX_DIM_LEN
                _ins["range"] = [(1, None)] * ClassifyConst.MAX_DIM_LEN
                is_unknown_rank = True
        return is_unknown_rank

    def _check_update_empty(self):
        """
        check for empty shape, modify ins and set flag for maybe_empty_tensor
        """
        is_empty_tensor = False
        for _ins in self.ins:
            _shape = list(_ins.get("shape"))
            _range = list(_ins.get("range", util.generate_range(_shape)))
            for index, (dim, (r_l, r_r)) in enumerate(zip(_shape, _range)):
                if dim == 0:
                    _range[index] = (0, 0)
                    is_empty_tensor = True
                if r_l == 0:
                    _range[index] = (1, r_r)
                    is_empty_tensor = True
                if r_r == 0:
                    _range[index] = (0, 0)
                    is_empty_tensor = True
            _ins["range"] = _range
        return is_empty_tensor

    def _check_strict_5hd_conditions(self):
        """
        check if all inputs strictly satisify 5hd inputs requirements
        """
        # 5hd all inputs shape len must be five, and last dim must be C0 related with its dtype
        if self.ignore_fractal_format:
            return False
        for _ins in self.ins:
            if _ins.get("format") != 'NC1HWC0':
                return False
            if len(_ins.get("shape")) != ClassifyConst.FHD_SHAPE_LEN:
                return False
            if _ins.get("shape")[-1] != C0_MAPPING.get(_ins.get("dtype", _ins.get("data_type"))):
                return False
        self.elewise_ori_c = get_original_channels()
        max_dtype_byte = max(C0_MAPPING.get(_ins.get("dtype", _ins.get("data_type"))) for _ins in self.ins)
        # original c value known and align will not thought as 5HD
        if self.elewise_ori_c > 0 and self.elewise_ori_c % max_dtype_byte == 0:
            return False
        return True

    def _inputs_lens_align(self):
        """
        align all inputs to same len
        """
        def clone_inputs(_in):
            _shape, _range = list(_in.get("shape")), _in.get("range")
            len_diff = self.max_dim_len - len(_shape)

            new_in = _in.copy()
            new_in["shape"] = [1] * len_diff + _shape
            new_in["range"] = (util.generate_range(new_in.get("shape")) if _range is None else
                               [(1, 1)] * len_diff + list(_range))
            return new_in

        return [clone_inputs(_ins) for _ins in self.ins]

    def _update_shape_range(self):
        """
        update shape and range for better generate classify ins
        """
        def get_range_intersection(ranges):

            def range_intersection(range_a, range_b):
                if range_a is None or range_b is None:
                    return None
                a_lower, a_upper = range_a
                b_lower, b_upper = range_b
                if max(a_lower, b_lower) > min(a_upper, b_upper):
                    return None
                return max(a_lower, b_lower), min(a_upper, b_upper)

            return product(range_intersection, ranges)

        def fixed_shape_range():
            for _range in self.processed_ranges:
                for index, (r_l, r_r) in enumerate(_range):
                    if r_l is None and r_r is None:
                        _range[index] = (util.VAR_BOUND_LIMIT, util.VAR_BOUND_LIMIT)
                    elif r_l is None:
                        _range[index] = (util.VAR_BOUND_LIMIT, r_r)
                    elif r_r is None:
                        _range[index] = (r_l, util.VAR_BOUND_LIMIT)
            for _shape, _range in zip(self.processed_shapes, self.processed_ranges):
                for index, (_dim, (r_l, r_r)) in enumerate(zip(_shape, _range)):
                    if _dim != -1:
                        _range[index] = (_dim, _dim)
                    elif r_l == r_r:
                        _shape[index] = r_l
                    else:
                        continue

        fixed_shape_range()
        t_shapes = list(map(list, zip(*self.processed_shapes)))
        t_ranges = list(map(list, zip(*self.processed_ranges)))
        for _range in t_ranges:
            no_one_range = [r for r in _range if r[0] > 1]
            if len(no_one_range) > 0:
                mied_range = get_range_intersection(no_one_range)
                expr_compare.is_true(mied_range is not None,
                                     {"errCode": "E90001",
                                      "detailed_cause": "input shape error, shape range has no intersection."
                                      })
                for i, cur_range in enumerate(_range):
                    if 1 in cur_range:
                        if cur_range[1] < mied_range[0]:
                            _range[i] = (1, 1)
                        elif cur_range[1] > mied_range[1]:
                            _range[i] = (1, mied_range[1])
                    else:
                        _range[i] = mied_range
        self.processed_shapes = list(map(list, zip(*t_shapes)))
        self.processed_ranges = list(map(list, zip(*t_ranges)))
        fixed_shape_range()

    def _is_const(self):
        """
        check if all dims is known or can be infer
        1. dynamic: all dim of shapes can be infer
        2. static: all dim of shapes is known
        """
        if operation.get_context().get_mode() == "static":
            return True

        # dynamic shape infer is only enable while input num is two
        if self.classify_inputs_num > ClassifyConst.LEAST_BROADCAST_INPUTS:
            return False

        for index in range(self.max_dim_len):
            dim_values = [_shape[index] for _shape in self.processed_shapes]
            min_dim_value = min(dim_values)
            max_dim_value = max(dim_values)
            if min_dim_value == -1 and max_dim_value in (-1, 1):
                return False
        return True

    def _classify_const(self):
        """
        classify const include static context and inferable shape.
        Elewise const infer shape only support two inputs, including:
        1.all shapes same, such as [4, 5] and [4, 5]
        2.one of the inferred shape product is 1, such as [1, 1] and [4, 5]
        """
        def const_infer(index, _shapes):
            """infer all possible inferable classify res
            Args:
                index (int): shape dim index
                _shapes (List): processed input shapes
            Returns:
                List: all inferable classify res list
            """
            if index == self.max_dim_len:
                return [_shapes]

            dims_index = [s[index] for s in self.processed_shapes]
            min_index_value = min(dims_index)
            max_index_value = max(dims_index)

            # 1. don't need infer, all dim value is known in current axis
            if min_index_value != -1:
                append_known(_shapes, index)
                return const_infer(index + 1, _shapes)

            # 2. need infer, -1 divide to 1 and max value in current axis, which may contains broadcast
            ret_shapes = []

            # 2.1. -1 infer to be max_dim value
            _shapes_copy = shape_copy(_shapes)
            append_shape_infer(_shapes_copy, index, max_index_value)
            ret_shapes.extend(const_infer(index + 1, _shapes_copy))

            # pure elewise only support all shape same scene
            if self.is_pure_elewise:
                return ret_shapes

            # 2.2 -1 infer to be 1
            _shapes_copy = shape_copy(_shapes)
            append_shape_infer(_shapes_copy, index, 1)
            ret_shapes.extend(const_infer(index + 1, _shapes_copy))

            return ret_shapes

        def append_known(_shapes, dim_index):
            for index, _shape in enumerate(_shapes):
                _shape.append(self.processed_shapes[index][dim_index])

        def append_shape_infer(_shapes, dim_index, dim_value):
            for index, _shape in enumerate(_shapes):
                _shape.append(max(self.processed_shapes[index][dim_index], dim_value))

        def shape_copy(_shapes):
            return [_shape.copy() for _shape in _shapes]

        def get_elewise_shapes(infer_shapes):
            """get elewise shapes from all classified shapss
            Args:
                infer_shapes (List): all classified shapes after infer
            Returns:
                List: shapes list consists of all elewise classify res
            """
            elewise_res = []
            for _shapes in infer_shapes:
                if len(set(tuple(_shape) for _shape in _shapes)) == 1:
                    elewise_res.append(_shapes)
                else:
                    fused_shapes = [util.combine_dim(_shape) for _shape in _shapes]
                    if 1 in fused_shapes:
                        elewise_res.append(_shapes)
                if len(elewise_res) == ClassifyConst.MAX_CONST_RES_NUM:
                    break
            return elewise_res

        def gen_const_ranges(_shapes):
            ranges = []
            for _shape in _shapes:
                ranges.append([(_dim, _dim) for _dim in _shape])
            return ranges

        elewise_const_shapes = get_elewise_shapes(const_infer(0, [[] for _ in range(self.classify_inputs_num)]))
        elewise_const_ranges = [gen_const_ranges(_shapes) for _shapes in elewise_const_shapes]

        const_res = []
        if self.enable_5hd_format:
            for (_shapes, _ranges) in zip(elewise_const_shapes, elewise_const_ranges):
                const_res.append(ConstMode.gen_const_fractal_format(
                    _shapes, _ranges, self.elewise_ori_c, disable_optimization=self.disable_optimization))
            return const_res

        if self.disable_optimization:
            for (_shapes, _ranges) in zip(elewise_const_shapes, elewise_const_ranges):
                shape_same = len(set(util.combine_dim(_shape) for _shape in _shapes)) == 1
                const_pattern = ElewisePattern.CONST_SHAPE_SAME if shape_same else ElewisePattern.CONST_SHAPE_DIFF
                const_res.append(ConstMode.gen_const_disable_fuse(_shapes, _ranges, const_pattern))
            return const_res

        for (_shapes, _ranges) in zip(elewise_const_shapes, elewise_const_ranges):
            shape_same = len(set(util.combine_dim(_shape) for _shape in _shapes)) == 1
            const_pattern = ElewisePattern.CONST_SHAPE_SAME if shape_same else ElewisePattern.CONST_SHAPE_DIFF
            const_res.append(ConstMode.gen_const_all_fuse(_shapes, _ranges, const_pattern))
        return const_res

    def _infer_5hd_shape_range(self):
        """
        infer 5hd shapes and ranges, elewise 5hd must not contain any broadcast including vector_dup
        return shapes and ranges after infer
        """
        shape_infer = []
        range_infer = []
        for index in range(self.max_dim_len):
            cur_shape = max(_shape[index] for _shape in self.processed_shapes)
            cur_range_l = max(_range[index][0] for _range in self.processed_ranges)
            cur_range_r = min(_range[index][1] for _range in self.processed_ranges)
            if cur_shape > 0 or cur_range_l == cur_range_r:
                shape_infer.append(cur_shape)
                range_infer.append((cur_shape, cur_shape))
            else:
                shape_infer.append(-1)
                range_infer.append((cur_range_l, cur_range_r))
        if not self.disable_optimization:
            # fuse h and w axis
            if -1 in (shape_infer[2], shape_infer[3]):
                shape_infer = [shape_infer[0], shape_infer[1], -1, shape_infer[4]]
            else:
                shape_infer = [shape_infer[0], shape_infer[1], shape_infer[2] * shape_infer[3], shape_infer[4]]
            if range_infer[2][1] * range_infer[3][1] >= ClassifyConst.INT32_MAX:
                range_infer = [range_infer[0], range_infer[1], (range_infer[2][0] *
                               range_infer[3][0], ClassifyConst.INT32_MAX), range_infer[4]]
            else:
                range_infer = [range_infer[0], range_infer[1], (range_infer[2][0] *
                               range_infer[3][0], range_infer[2][1]*range_infer[3][1]), range_infer[4]]
        return [shape_infer] * self.classify_inputs_num, [range_infer] * self.classify_inputs_num

    def _maybe_pure_elewise(self):
        """
        check if generate pure elewise mode classify res, if range of all dims has intersection will return true
        """
        for index in range(self.max_dim_len):
            cur_range_l = max(_range[index][0] for _range in self.processed_ranges)
            cur_range_r = min(_range[index][1] for _range in self.processed_ranges)
            if cur_range_l <= cur_range_r:
                continue
            if self.is_pure_elewise is True:
                raise RuntimeError("extra_param.is_pure_elewise is True,"
                                   "does not match the pattern obtained during actual compilation,"
                                   "cur_range_l is ", cur_range_l, ", cur_range_r is ", cur_range_r)
            return False
        return True

    def _maybe_broadcast_scalar(self):
        """
        check if generate broadcast scalar mode classify res, only generate while classify inputs num is two
        """
        if self.classify_inputs_num != ClassifyConst.LEAST_BROADCAST_INPUTS or self.is_pure_elewise:
            return False
        # get range after all fuse
        first_min_range_right = max(_range[1] for _range in self.processed_ranges[0])
        second_fuse_range_left = (util.combine_range(self.processed_ranges[1]))[0]
        if second_fuse_range_left == 1 and first_min_range_right > 1:
            return True
        return False

    def _maybe_scalar_broadcast(self):
        """
        check if generate scalar broadcast mode classify res, only generate while classify inputs num is two
        """
        if self.classify_inputs_num != ClassifyConst.LEAST_BROADCAST_INPUTS or self.is_pure_elewise:
            return False
        # get range after all fuse
        first_fuse_range_left = (util.combine_range(self.processed_ranges[0]))[0]
        second_min_range_right = max(_range[1] for _range in self.processed_ranges[1])
        if first_fuse_range_left == 1 and second_min_range_right > 1:
            return True
        return False

    def _maybe_one_rank(self):
        """
        check if generate one rank mode classify res, only generate while classify inputs num bigger than two
        """
        if self.classify_inputs_num <= ClassifyConst.LEAST_BROADCAST_INPUTS or self.is_pure_elewise:
            return False
        # known_broadcast must be all shape 1
        max_out_shape = []
        for dim_index in range(len(self.processed_shapes[0])):
            max_out_shape.append(max(
                self.processed_shapes[in_index][dim_index] for in_index in range(self.classify_inputs_num)))
        for cur_shape in self.processed_shapes:
            known_broad_axis = 0
            for cur_dim, out_dim in zip(cur_shape, max_out_shape):
                if cur_dim == 1 and out_dim != 1 and out_dim > 0:
                    known_broad_axis += 1
            if known_broad_axis > 0 and max(cur_shape) > 1:
                return False
        for _range in self.processed_ranges:
            if (util.combine_range(_range))[0] == 1:
                return True
        return False

    def _classify_var(self):
        """
        dynamic classify may contain following conditions:
        1.all fuse mode
          1.1 pure elewise: all shape same
          1.2 scalar broadcast: input num is two and left shape product is one
          1.3 broadcast scalar: input num is two and right shape product is one
          1.4 broadcast: input num bigger than two and at least one of input shape product is one
        2.disable fuse mode
          2.1 disable fuse under fractal format
          2.2 disable fuse ignore fractal format
        3.empty mode
        """
        var_res = []
        # empty mode
        if self.maybe_empty_tensor or self.is_unknown_rank:
            var_res.append(EmptyMode.gen_empty(len(self.ins)))

        if self.enable_5hd_format:
            infer_shapes, infer_ranges = self._infer_5hd_shape_range()
            var_res.append(DisableFuseMode.gen_fractal_format(
                infer_shapes, infer_ranges, self.elewise_ori_c, self.disable_optimization))
            # dynamic fractal format may be all fuse if no need pad compute
            if not self.disable_optimization:
                var_res.append(AllFuseMode.gen_pure_elewise(self.classify_inputs_num))
            return var_res

        if self.disable_optimization:
            var_res.append(DisableFuseMode.gen_disable_fuse(self.processed_shapes, self.processed_ranges))
            return var_res

        # all fuse res
        if self._maybe_pure_elewise():
            var_res.append(AllFuseMode.gen_pure_elewise(self.classify_inputs_num))

        if self._maybe_broadcast_scalar():
            var_res.append(AllFuseMode.gen_broadcast_scalar(self.processed_ranges[0]))

        if self._maybe_scalar_broadcast():
            var_res.append(AllFuseMode.gen_scalar_broadcast(self.processed_ranges[1]))

        if self._maybe_one_rank():
            var_res.append(AllFuseMode.gen_one_rank(self.processed_shapes, self.processed_ranges))

        return var_res


class AllFuseMode:
    """
    all fuse mode contains pure elewise and pure vector_dup broadcast
    """

    @classmethod
    def gen_pure_elewise(cls, input_num):
        """
        compute without broadcast
        """
        pure_elewise_res = [{"shape": [-1],
                             "range": [(1, None)],
                             "mode": ElewiseMode.ALL_FUSE,
                             "pattern": ElewisePattern.PURE_ELEWISE,
                             "classify_mode": util.ClassifyInsMode.ELEWISE_CLASSIFY}]
        return pure_elewise_res * input_num

    @classmethod
    def gen_broadcast_scalar(cls, left_range):
        """
        1.compute only contains vector_dup broadcast
        2.input num is 2
        3.right shape multiply equals to 1
        """
        left_res = {"shape": [-1], "range": [util.combine_range(left_range)]}
        right_res = {"shape": [1], "range": [(1, 1)]}
        res = [left_res, right_res]
        for _ins in res:
            _ins['mode'] = ElewiseMode.ALL_FUSE
            _ins['pattern'] = ElewisePattern.BROADCAST_SCALAR
            _ins['classify_mode'] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return res

    @classmethod
    def gen_scalar_broadcast(cls, right_range):
        """
        1.compute only contains vector_dup broadcast
        2.input num is 2
        3.left shape multiply equals to 1
        """
        left_res = {"shape": [1], "range": [(1, 1)]}
        right_res = {"shape": [-1], "range": [util.combine_range(right_range)]}
        res = [left_res, right_res]
        for _ins in res:
            _ins['mode'] = ElewiseMode.ALL_FUSE
            _ins['pattern'] = ElewisePattern.SCALAR_BROADCAST
            _ins['classify_mode'] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return res

    @classmethod
    def gen_one_rank(cls, shapes, ranges):
        """
        1.compute only contains vector_dup broadcast
        2.input num > 2
        3.at least one input shape multi is 1
        """
        res = [{} for _ in range(len(shapes))]
        for index, (_shape, _range) in enumerate(zip(shapes, ranges)):
            res[index]['shape'] = [util.combine_dim(_shape)]
            res[index]['range'] = [util.combine_range(_range)]
            res[index]['mode'] = ElewiseMode.ALL_FUSE
            res[index]['pattern'] = ElewisePattern.ONE_RANK
            res[index]['classify_mode'] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return res


class ConstMode:
    """
    ConstMode
    """

    @classmethod
    def gen_const_all_fuse(cls, shapes, ranges, pattern):
        """generate all fuse res for const classify
        Args:
            shapes (List): list of shapes after infer
            ranges (List): list of ranges after infer
            pattern (ElewisePattern): depends if all shape same
        Returns:
            List[Dict]: axis all fuse classified res
        """
        res = [{} for _ in shapes]
        for index, (_shape, _range) in enumerate(zip(shapes, ranges)):
            res[index]["const_shape"] = _shape
            res[index]["shape"] = [util.combine_dim(_shape)]
            res[index]["range"] = [util.combine_range(_range)]
            res[index]["mode"] = ElewiseMode.CONST
            res[index]["pattern"] = pattern
            res[index]["classify_mode"] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return res

    @classmethod
    def gen_const_disable_fuse(cls, shapes, ranges, pattern):
        """generate disable fuse res for const classify
        Args:
            shapes (List): list of shapes after infer
            ranges (List): list of ranges after infer
            pattern (ElewisePattern): depends if all shape same
        Returns:
            List[Dict]: disable fuse classified res
        """
        res = [{} for _ in shapes]
        for index, (_shape, _range) in enumerate(zip(shapes, ranges)):
            res[index]["const_shape"] = _shape
            res[index]["shape"] = _shape
            res[index]["range"] = _range
            res[index]["mode"] = ElewiseMode.CONST
            res[index]["pattern"] = pattern
            res[index]["classify_mode"] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return res

    @classmethod
    def gen_const_fractal_format(cls, shapes, ranges, c_value, disable_optimization):
        """generate fractal format classify res
        Args:
            shapes (List): list of shapes after infer
            ranges (List): list of ranges after infer
            origin_c_value(Int): value of original c value, elewise all inputs only support same C value
            disable_optimization (bool): dicide if disable fuse axes
        Returns:
            List[Dict]: fractal format classified res
        """
        fractal_res = [{} for _ in range(len(shapes))]
        for (_res, _shape, _range) in zip(fractal_res, shapes, ranges):
            _res["shape"] = _shape if disable_optimization else [_shape[0], _shape[1], _shape[2]*_shape[3], _shape[4]]
            _res["const_shape"] = _shape
            _res["range"] = _range if disable_optimization else [(_dim, _dim) for _dim in _res.get("shape")]
            _res["mode"] = ElewiseMode.CONST
            _res["pattern"] = ElewisePattern.CONST_SHAPE_SAME
            _res["format"] = "NC1HWC0"
            _res["s_format"] = \
                [["N"], ["C1"], ["H"], ["W"], ["C0"]] if disable_optimization else [["N"], ["C1"], ["H", "W"], ["C0"]]
            _res["pad_axes"] = c_value
            _res['np_mapping'] = {'C1': 'C', 'C0': 'C'}
            _res["mode_5hd"] = True
            _res["classify_mode"] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return fractal_res


class DisableFuseMode:
    """
    Elewise DisableFuseMode, disable fuse all axis
    """

    @classmethod
    def gen_disable_fuse(cls, shapes, ranges):
        """disable fuse axis
        Args:
            shapes (list): all inputs shape after dims align
            ranges (list): all inputs range after dims align
        """
        res = [{} for _ in range(len(shapes))]
        for index, (_shape, _range) in enumerate(zip(shapes, ranges)):
            res[index]["shape"] = _shape
            res[index]["range"] = list(_range)
            res[index]["mode"] = ElewiseMode.DISABLE_FUSE
            res[index]["pattern"] = ElewisePattern.DISABLE_FUSE
            res[index]["classify_mode"] = util.ClassifyInsMode.ELEWISE_CLASSIFY
        return res

    @classmethod
    def gen_fractal_format(cls, shapes, ranges, c_value, disable_optimization):
        """
        disable fuse for C1 and C0
        """
        s_format = \
            [["N"], ["C1"], ["H"], ["W"], ["C0"]] if disable_optimization else [["N"], ["C1"], ["H", "W"], ["C0"]]
        pad_axes = c_value
        res = [{} for _ in range(len(shapes))]
        for index, (_shape, _range) in enumerate(zip(shapes, ranges)):
            res[index]["shape"] = _shape
            res[index]["range"] = \
                [dim_range if dim_range[1] != ClassifyConst.INT32_MAX else (dim_range[0], None) for dim_range in _range]
            res[index]["format"] = "NC1HWC0"
            res[index]["pad_axes"] = pad_axes
            res[index]["mode"] = ElewiseMode.DISABLE_FUSE
            res[index]["pattern"] = ElewisePattern.FRACTAL_FORMAT
            res[index]["classify_mode"] = util.ClassifyInsMode.ELEWISE_CLASSIFY
            res[index]['np_mapping'] = {'C1': 'C', 'C0': 'C'}
            res[index]["s_format"] = s_format
            res[index]["mode_5hd"] = True

        return res


class EmptyMode:
    """
    Empty Mode
    """

    @classmethod
    def gen_empty(cls, input_num):
        """
        all input shape known
        """
        empty_res = {"shape": (0,),
                     "range": [(0, 0)],
                     "mode": ElewiseMode.EMPTY,
                     "pattern": ElewisePattern.EMPTY,
                     "classify_mode": util.ClassifyInsMode.ELEWISE_CLASSIFY}
        res = [empty_res] * input_num
        return res
