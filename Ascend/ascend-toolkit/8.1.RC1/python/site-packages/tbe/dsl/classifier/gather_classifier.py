#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
classifier of shape in gather
"""
from itertools import chain
from typing import Any
from typing import Dict
from typing import Optional

from tbe.dsl.base import operation
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform.platform_info import ASCEND_310B
from tbe.common.platform.platform_info import AS31XM1
from tbe.common.platform.platform_info import ASCEND_031
from tbe.common.platform.platform_info import ASCEND_035
from tbe.common.platform.platform_info import ASCEND_310P
from tbe.common.platform.platform_info import ASCEND_610LITE
from tbe.common.platform.platform_info import BS9SX2A
from tbe.common.platform.platform_info import MC61AM21A
from tbe.common.platform.platform_info import SHORT_SOC_VERSION

from . import shape_classifier
from . import util
from .util import DTYPE_BYTE_MAPPING

UNKNOWN = "unknown"


@shape_classifier.register_classifier(shape_classifier.GATHER)
def classify_gather(ins: list, extra_params: Optional[Dict[str, Any]]):
    """
    GatherClassifier
    :param ins:
    :param extra_params:
    :return:
    """
    check_ids = 0
    if extra_params is not None and extra_params.get("impl_mode") == "support_out_of_bound_index":
        check_ids = 1
    return GatherClassifier(ins, check_ids).classify()


@shape_classifier.register_classifier(shape_classifier.GATHER_ND)
def classify_gather_nd(ins: list, extra_params: Optional[Dict[str, Any]]):
    """
    GatherNdClassifier
    :param ins:
    :param extra_params:
    :return:
    """
    return GatherNdClassifier(ins).classify()


class GatherClassifier:
    def __init__(self, ins: list, check_ids: int):
        self.is_zeros_shape = False
        self.is_zeros_range = False
        self.check_ids = check_ids
        self.org_params_shape_info = list(ins[0]["shape"])
        self.len_org_params_shape = len(self.org_params_shape_info)
        self.org_params_range_info = list(ins[0]["range"])
        self.org_indices_shape_info = list(ins[1]["shape"])
        self.org_indices_range_info = list(ins[1]["range"])

        self.unknown_batch_dims = ins[3] == UNKNOWN

        # check status dynamic or static
        self.is_static = operation.get_op_mode() == "static"

        self.params_dtype = ins[0]["dtype"]
        self.indices_dtype = ins[1]["dtype"]
        self.params_size = DTYPE_BYTE_MAPPING.get(self.params_dtype)
        self.indices_size = DTYPE_BYTE_MAPPING.get(self.indices_dtype)

        self.indices_loop_shape = None
        self.indices_loop_range = None
        self.batch_shape = None
        self.batch_range = None
        self.unkonw_rank = False
        self.move_pad = False
        cur_version = get_soc_spec(SHORT_SOC_VERSION)
        if tbe_platform_info.api_check_support("tik.data_move_pad") and \
                (cur_version not in [ASCEND_310B, AS31XM1, ASCEND_031, ASCEND_610LITE, 
                                     ASCEND_035, BS9SX2A, MC61AM21A]):
            self.move_pad = True
        self.is_310p = cur_version == ASCEND_310P
        if self.is_static:
            # params
            self.params_shape = self.org_params_shape_info
            self.params_range = self.org_params_range_info

            # indices
            self.indices_shape = self.org_indices_shape_info
            self.indices_range = self.org_indices_range_info

            self.batch_dims = ins[3] if ins[3] >= 0 else ins[3] + len(self.indices_shape)
            self.org_batch_dims = self.batch_dims

            if ins[2] is None:
                self.axis = self.batch_dims
            else:
                self.axis = ins[2] if ins[2] >= 0 else ins[2] + len(self.params_shape)
        else:
            if ins[2] is None:
                self.params_shape = [-1, 1, -1, -1]
                self.params_range = [[1, None], [1, 1], [1, None], [1, None]]

                self.indices_shape = [-1, -1]
                self.indices_range = [[1, None], [1, None]]
            else:
                self.params_shape = [-1, -1, -1, -1]
                self.params_range = [[1, None], [1, None], [1, None], [1, None]]

                self.indices_shape = [-1, -1]
                self.indices_range = [[1, None], [1, None]]

            self.batch_dims = 1
            self.axis = 2

            # batch dims
            # binary condition or fuzzy condition
            self.org_batch_dims = 0 if self.unknown_batch_dims else ins[3]

        # gather axes rank
        self.gather_rank = 1

        # fuzzy condition
        if -2 in chain(self.org_params_shape_info + self.org_indices_shape_info) or self.unknown_batch_dims:
            self.is_zeros_range = True
            self.unkonw_rank = True

        operation.get_context().add("_batch_dims", self.batch_dims)
        operation.get_context().add("_org_batch_dims", self.org_batch_dims)
        operation.get_context().add("_unknown_batch_dims", self.unknown_batch_dims)
        operation.get_context().add("_gather_mode", "gather")
        operation.get_context().add("_check_ids", self.check_ids)
        operation.get_context().add("_move_pad", self.move_pad)
        operation.get_context().add("_is_310p", self.is_310p)

        self._check_zero_shape()
        self.is_b8 = self.params_dtype in ("int8", "uint8", "bool")
        self.is_spe = False
        self.is_trans_special = False

    def classify(self):
        """
        classify
        :return:
        """
        # zeros shape
        gather_instances = []
        if self.is_zeros_shape:
            gather_instances.append(_classify_gather_zero_shape(self.params_dtype, self.indices_dtype, "gather"))
            return gather_instances

        # zeros range
        if self.is_zeros_range:
            gather_instances.append(_classify_gather_zero_shape(self.params_dtype, self.indices_dtype, "gather"))

            # change range 0 to 1
            for i, v in enumerate(self.params_range):
                if 0 == v[0]:
                    self.params_range[i] = [1, v[1]]
            for i, v in enumerate(self.indices_range):
                if 0 == v[0]:
                    self.indices_range[i] = [1, v[1]]

        # normal classify
        if self.is_static:
            gather_instances.extend(self._classify_in_static())
        else:
            gather_instances.extend(self._classify_in_dynamic())

        return gather_instances

    def _check_zero_shape(self):
        # shape value zero
        for dim_value in chain(self.org_params_shape_info + self.org_indices_shape_info):
            if 0 == dim_value:
                self.is_zeros_shape = True
                break

        # range value zero
        if not self.is_zeros_shape:
            for dim_range in chain(self.org_params_range_info + self.org_indices_range_info):
                if 0 == dim_range[0]:
                    self.is_zeros_range = True
                    break

    def _handle_indices_loops(self):
        # indices loop
        if self.batch_dims == len(self.indices_shape):
            self.indices_loop_shape = 1
            self.indices_loop_range = (1, 1)
        else:
            self.indices_loop_shape = util.combine_dim(self.indices_shape[self.batch_dims:])
            self.indices_loop_range = util.combine_range(self.indices_range[self.batch_dims:])

    def _handle_batch_dims(self):
        # fuse batch dims
        if self.batch_dims == 0:
            self.batch_shape = 1
            self.batch_range = (1, 1)
        else:
            self.batch_shape = util.combine_dim(self.params_shape[:self.batch_dims])
            self.batch_range = util.combine_range(self.params_range[:self.batch_dims])

    def _support_trans_base(self, pre_loop_shape, gather_axis_shape, after_loop_shape):
        if self.is_spe or (not self.is_310p and not self.move_pad):
            return False
        if (self.gather_rank > 1) or (self.params_dtype in ("int64", "uint64")):
            return False
        if self.axis != self.len_org_params_shape - 1:
            return False
        if (self.org_batch_dims != 0) or (after_loop_shape != 1):
            return False
        if gather_axis_shape[-1] == 1:
            return False
        num_core = get_soc_spec("CORE_NUM")
        if pre_loop_shape < num_core:
            return False
        if (pre_loop_shape // gather_axis_shape[-1]) < 5000:
            # 5000 is an empirical value
            return False
        if not self.is_trans_special and gather_axis_shape[-1] > 16:
            return False
        return True

    def _compute_align(self, gather_axis_shape):
        co2co = 16
        co2co_v = 16
        row_align = 32
        index_align = 32
        if self.params_size == 1:
            row_align = 32
            co2co = 32
        elif self.params_size == 4:
            row_align = 8
            co2co_v = 32
        elif self.params_size == 2:
            row_align = 16
        else:
            row_align = 4
        if self.indices_size == 1:
            index_align = 32
        elif self.indices_size == 4:
            index_align = 8
        elif self.indices_size == 2:
            index_align = 16
        else:
            index_align = 4
        co2co_align = ((self.indices_loop_shape + co2co - 1) // co2co) * co2co
        shape_align = ((gather_axis_shape[-1] + row_align - 1) // row_align) * row_align
        last_row_v = ((gather_axis_shape[-1] + co2co_v - 1) // co2co_v) * co2co_v
        idx_align = ((self.indices_loop_shape + index_align - 1) // index_align) * index_align
        return [last_row_v, shape_align, idx_align, co2co_align, co2co]

    def _ub_support(self, bound_list, params_list):
        if self.is_trans_special and not self.is_b8:
            if bound_list[0] < params_list[0]:
                return False
        else:
            if bound_list[0] < params_list[1]:
                return False
            if bound_list[1] < params_list[2]:
                return False
            max_row = bound_list[0] // params_list[1]
            if max_row < params_list[4]:
                return False
            if bound_list[2] < params_list[3]:
                return False
            gather_ub_row = params_list[3] * params_list[4]
            if not self.is_trans_special and bound_list[2] < gather_ub_row:
                return False
        return True

    def _cal_support(self, gather_axis_shape):
        ub_size = get_soc_spec("UB_SIZE")
        params_storage_bound = 1
        indices_storage_bound = 1
        gather_storage_bound = 1
        if self.is_trans_special and self.is_b8:
            tensor_space = ((ub_size - 32) // 2 // 256) * 256
            params_storage_bound = int(tensor_space // self.params_size)
            indices_storage_bound = 32 // self.indices_size
            gather_storage_bound = int(tensor_space // self.params_size)
        elif self.is_trans_special and not self.is_b8:
            indices_storage_bound = 32 // self.indices_size
            if gather_axis_shape[-1] in (2, 4):
                reg_buf = 64 if self.is_310p else 32
                tensor_space = (ub_size - reg_buf) // 32 * 32
                params_storage_bound = int(tensor_space // self.params_size)
                gather_storage_bound = int(tensor_space // self.params_size)
            else:
                tensor_space = (((ub_size - 2080) * 3) // 4) // 32 * 32
                params_storage_bound = int(tensor_space // self.params_size)
                gather_storage_bound = int(tensor_space // 3 // self.params_size)
        else:
            tensor_space = ub_size // 8 // 32 * 32
            params_storage_bound = (tensor_space * 2) // self.params_size
            indices_storage_bound = tensor_space // self.indices_size
            gather_storage_bound = (tensor_space * 3) // self.params_size
        return params_storage_bound, indices_storage_bound, gather_storage_bound

    def _support_trans(self, after_loop_shape, pre_loop_shape, gather_axis_shape):
        self.is_trans_special = self.indices_loop_shape == 1
        self.is_spe = self.is_310p and not (gather_axis_shape[-1] in (2, 4) and \
                      self.is_trans_special and not self.is_b8)
        if not self._support_trans_base(pre_loop_shape, gather_axis_shape, after_loop_shape):
            return False
        if self.is_trans_special and not self.is_b8 and self.check_ids:
            return False
        params_list = self._compute_align(gather_axis_shape)
        if self.is_trans_special and (not self.is_b8) and params_list[0] > 128:
            return False
        x_bound, ids_bound, gather_bound = self._cal_support(gather_axis_shape)
        if not self._ub_support([x_bound, ids_bound, gather_bound], params_list):
            return False
        return True

    def _classify_in_static(self):
        self._handle_indices_loops()

        self._handle_batch_dims()

        # pre loops params
        if self.axis == self.batch_dims:
            pre_loop_shape = 1
            pre_loop_range = (1, 1)
        else:
            pre_loop_shape = util.combine_dim(self.params_shape[self.batch_dims:self.axis])
            pre_loop_range = util.combine_range(self.params_range[self.batch_dims:self.axis])

        # gather axis
        gather_axis_shape = list(self.params_shape[self.axis:self.axis + self.gather_rank])
        gather_axis_range = list(self.params_range[self.axis:self.axis + self.gather_rank])

        # gather after index
        if self.axis + self.gather_rank >= len(self.params_shape):
            after_loop_shape = 1
            after_loop_range = (1, 1)
        else:
            after_loop_shape = util.combine_dim(self.params_shape[self.axis + self.gather_rank:])
            after_loop_range = util.combine_range(self.params_range[self.axis + self.gather_rank:])

        # assemble
        gather_ins_trans = []
        gather_ins = []
        classify_result = []
        if self._support_trans(after_loop_shape, pre_loop_shape, gather_axis_shape):
            params_dict = _asseble_params_info(
                [pre_loop_shape, gather_axis_shape],
                [pre_loop_range, gather_axis_range],
                self.params_dtype)
            indices_dict = _asseble_indices_info(
                [self.indices_loop_shape],
                [self.indices_loop_range],
                self.indices_dtype)
            gather_ins_trans.append(params_dict)
            gather_ins_trans.append(indices_dict)
            gather_ins_trans.append(1)
            gather_ins_trans.append(0)
            classify_result.append(gather_ins_trans)
        else:
            params_dict = _asseble_params_info(
                [self.batch_shape, pre_loop_shape, gather_axis_shape, after_loop_shape],
                [self.batch_range, pre_loop_range, gather_axis_range, after_loop_range],
                self.params_dtype)

            indices_dict = _asseble_indices_info(
                [self.batch_shape, self.indices_loop_shape],
                [self.batch_range, self.indices_loop_range],
                self.indices_dtype)

            gather_ins.append(params_dict)
            gather_ins.append(indices_dict)
            gather_ins.append(2)
            gather_ins.append(1)

            classify_result.append(gather_ins)
        return classify_result

    def _cal_move_pad(self, gather_instances, gather_ins3, gather_ins4, params_dict, indices_list):
        gather_ins1 = []
        gather_ins2 = []
        gather_ins1.append(params_dict)
        gather_ins1.append(indices_list[0])
        gather_ins1.append(1)
        gather_ins1.append(0)
        gather_instances.append(gather_ins1)
        if self.indices_loop_shape != 1:
            gather_ins2.append(params_dict)
            gather_ins2.append(indices_list[1])
            gather_ins2.append(1)
            gather_ins2.append(0)
            if not self.is_b8 and not self.check_ids:
                gather_instances.append(gather_ins3)
                gather_instances.append(gather_ins4)
                gather_instances.append(gather_ins2)
            if self.is_b8:
                gather_instances.append(gather_ins2)

    def _cal_dynamic_trans(self, gather_instances=None):
        gather_ins3 = []
        gather_ins4 = []
        trans_support = self.params_dtype not in ("int64", "uint64") and \
                (self.org_batch_dims == 0)
        support_310p = self.is_310p and not self.is_b8 and not self.check_ids
        if (self.move_pad or support_310p) and trans_support:
            params_dict = _asseble_params_info(
            [-1, -1],
            [[1, None], [1, None]],
            self.params_dtype)
            params_dict_2 = _asseble_params_info(
            [-1, 2],
            [[1, None], [2, 2]],
            self.params_dtype)
            params_dict_4 = _asseble_params_info(
            [-1, 4],
            [[1, None], [4, 4]],
            self.params_dtype)

            indices_dict = _asseble_indices_info(
                [self.indices_loop_shape],
                [self.indices_loop_range],
                self.indices_dtype)
            indices_dict1 = _asseble_indices_info([1], [[1, 1]], self.indices_dtype)
            gather_ins3.append(params_dict_2)
            gather_ins3.append(indices_dict1)
            gather_ins3.append(1)
            gather_ins3.append(0)
            gather_ins4.append(params_dict_4)
            gather_ins4.append(indices_dict1)
            gather_ins4.append(1)
            gather_ins4.append(0)
            if not self.is_310p:
                self._cal_move_pad(gather_instances, gather_ins3, gather_ins4,
                                   params_dict, [indices_dict, indices_dict1])
            else:
                gather_instances.append(gather_ins3)
                gather_instances.append(gather_ins4)

    def _classify_in_dynamic(self):
        gather_instances = []

        self._handle_indices_loops()

        self._handle_batch_dims()

        # gather axis pre loop
        if self.axis == self.batch_dims:
            pre_loop_shape = 1
            pre_loop_range = (1, 1)
        else:
            pre_loop_shape = util.combine_dim(self.params_shape[self.batch_dims:self.axis])
            pre_loop_range = util.combine_range(self.params_range[self.batch_dims:self.axis])

        # params hand
        # gather axis
        gather_axis_shape = list(self.params_shape[self.axis:self.axis + self.gather_rank])
        gather_axis_range = list(self.params_range[self.axis:self.axis + self.gather_rank])

        # gather after index
        if self.axis + self.gather_rank >= len(self.params_shape):
            after_loop_shape = 1
            after_loop_range = (1, 1)
        else:
            after_loop_shape = util.combine_dim(self.params_shape[self.axis + self.gather_rank:])
            after_loop_range = util.combine_range(self.params_range[self.axis + self.gather_rank:])

        # assemble
        gather_ins = []

        params_dict = _asseble_params_info(
            [self.batch_shape, pre_loop_shape, gather_axis_shape, after_loop_shape],
            [self.batch_range, pre_loop_range, gather_axis_range, after_loop_range],
            self.params_dtype)

        indices_dict = _asseble_indices_info(
            [self.batch_shape, self.indices_loop_shape],
            [self.batch_range, self.indices_loop_range],
            self.indices_dtype)

        gather_ins.append(params_dict)
        gather_ins.append(indices_dict)
        gather_ins.append(2)
        gather_ins.append(1)
        self._cal_dynamic_trans(gather_instances)

        gather_instances.append(gather_ins)

        return gather_instances


class GatherNdClassifier:
    def __init__(self, ins: list):
        self.is_zeros_shape = False
        self.is_zeros_range = False
        self.is_broadcast_shape = False
        self.is_broadcast_range = False
        self.unknown_batch_dims = False

        self.org_params_shape_info = list(ins[0]["shape"])
        self.org_params_range_info = list(ins[0]["range"])
        self.org_indices_shape_info = list(ins[1]["shape"])
        self.org_indices_range_info = list(ins[1]["range"])

        self.params_dtype = ins[0]["dtype"]
        self.indices_dtype = ins[1]["dtype"]

        # check status dynamic or static
        self.is_static = operation.get_op_mode() == "static"

        self.indices_loop_shape = None
        self.indices_loop_range = None
        self.batch_shape = None
        self.batch_range = None

        if self.is_static:
            # params
            self.params_shape = self.org_params_shape_info
            self.params_range = self.org_params_range_info

            # indices
            self.indices_shape = self.org_indices_shape_info
            self.indices_range = self.org_indices_range_info
        else:
            # params
            params_shape_len = len(self.org_params_shape_info)
            self.params_shape = [-1, ] * params_shape_len
            self.params_range = [[1, None]] * params_shape_len

            # indices
            indices_shape_len = len(self.org_indices_shape_info)
            self.indices_shape = [-1, ] * indices_shape_len
            self.indices_range = [[1, None]] * indices_shape_len

        if len(self.indices_shape) == 1:
            self.indices_shape.insert(0, 1)
            self.indices_range.insert(0, (1, 1))

        # binary condition
        if -2 in chain(self.org_params_shape_info + self.org_indices_shape_info):
            self.params_shape = [-1, -1, -1, -1, -1, -1, -1, -1, -1]
            self.params_range = [[1, None], [1, None], [1, None], [1, None], [1, None],
                                 [1, None], [1, None], [1, None], [1, None]]

            self.indices_shape = [-1, -1, -1]
            self.indices_range = [[1, None], [1, None], [1, None]]

            self.is_zeros_range = True
            self.is_broadcast_range = True
            self.unknown_batch_dims = True

        self._check_zero_shape()

        # gather axis
        if self.unknown_batch_dims:
            self.batch_dims = 1
            self.org_batch_dims = ins[2]
        else:
            self.batch_dims = ins[2] + len(self.indices_shape) if ins[2] < 0 else ins[2]
            self.org_batch_dims = self.batch_dims

        operation.get_context().add("_batch_dims", self.batch_dims)
        operation.get_context().add("_org_batch_dims", self.org_batch_dims)
        operation.get_context().add("_unknown_batch_dims", False)
        operation.get_context().add("_gather_mode", "gather_nd")

        self.axis = self.batch_dims

        # gather axes rank
        self.gather_rank = self.indices_shape[-1]

    def classify(self):
        """
        classify
        :return:
        """
        # zeros shape
        gather_instances = []
        if self.is_zeros_shape:
            gather_instances.append(_classify_gather_zero_shape(self.params_dtype, self.indices_dtype, "gather_nd"))
            return gather_instances

        # broadcast shape
        if self.is_broadcast_shape:
            gather_instances.append(
                _classify_gather_broadcast_shape(self.batch_dims, self.params_shape, self.indices_shape,
                                                 self.params_range, self.indices_range, self.params_dtype,
                                                 self.indices_dtype))
            return gather_instances

        if self.is_zeros_range:
            gather_instances.append(_classify_gather_zero_shape(self.params_dtype, self.indices_dtype, "gather_nd"))

        if self.is_broadcast_range:
            gather_instances.append(
                _classify_gather_broadcast_shape(self.batch_dims, self.params_shape, self.indices_shape,
                                                 self.params_range, self.indices_range, self.params_dtype,
                                                 self.indices_dtype))

        # change range 0 to 1
        if self.is_zeros_range or self.is_broadcast_range:
            for i, v in enumerate(self.params_range):
                if 0 == v[0]:
                    self.params_range[i] = [1, v[1]]
            for i, v in enumerate(self.indices_range):
                if 0 == v[0]:
                    self.indices_range[i] = [1, v[1]]

        # normal classify
        if self.is_static:
            gather_instances.extend(self._classify_in_static())
        else:
            gather_instances.extend(self._classify_in_dynamic())

        return gather_instances

    def _check_zero_shape(self):
        # shape value zero
        for dim_value in chain(self.org_params_shape_info + self.org_indices_shape_info[:-1]):
            if 0 == dim_value:
                self.is_zeros_shape = True
                break

        # range value zero
        if not self.is_zeros_shape:
            for range_value in chain(self.org_params_range_info + self.org_indices_range_info[:-1]):
                if 0 == range_value[0]:
                    self.is_zeros_range = True
                    break

        # shape value broadcast
        if self.org_indices_shape_info[-1] == 0:
            self.is_broadcast_shape = True

        # range value broadcast
        if self.org_indices_range_info[-1][0] == 0:
            self.is_broadcast_range = True

    def _handle_indices_loop(self):
        # indices loop
        self.indices_loop_shape = util.combine_dim(self.indices_shape[self.batch_dims:-1])
        self.indices_loop_range = util.combine_range(self.indices_range[self.batch_dims:-1])

    def _handle_batch_dims(self):
        # fuse batch dims
        if self.batch_dims == 0:
            self.batch_shape = 1
            self.batch_range = (1, 1)
        else:
            self.batch_shape = util.combine_dim(self.params_shape[:self.batch_dims])
            self.batch_range = util.combine_range(self.params_range[:self.batch_dims])

    def _classify_in_static(self):
        self._handle_indices_loop()
        self._handle_batch_dims()

        gather_axis_shape = list(self.params_shape[self.axis:self.axis + self.gather_rank])
        gather_axis_range = list(self.params_range[self.axis:self.axis + self.gather_rank])

        # gather after index
        if self.axis + self.gather_rank >= len(self.params_shape):
            after_loop_shape = 1
            after_loop_range = (1, 1)
        else:
            after_loop_shape = util.combine_dim(self.params_shape[self.axis + self.gather_rank:])
            after_loop_range = util.combine_range(self.params_range[self.axis + self.gather_rank:])

        # assemble
        gather_ins = []
        params_dict = _asseble_params_info(
            [self.batch_shape, gather_axis_shape, after_loop_shape],
            [self.batch_range, gather_axis_range, after_loop_range],
            self.params_dtype)

        # indices gather axis
        indices_index_shape = self.gather_rank
        indices_index_range = (self.gather_rank, self.gather_rank)

        indices_dict = _asseble_indices_info(
            [self.batch_shape, self.indices_loop_shape, indices_index_shape],
            [self.batch_range, self.indices_loop_range, indices_index_range],
            self.indices_dtype)

        gather_ins.append(params_dict)
        gather_ins.append(indices_dict)
        gather_ins.append(1)

        return [gather_ins]

    def _classify_in_dynamic(self):
        gather_instance = []

        self._handle_indices_loop()
        self._handle_batch_dims()

        # know rank
        if self.gather_rank != -1:
            return self._classify_in_static()
        else:
            # gather nd
            # gather rank
            rank_range = len(self.params_shape) - self.axis + 1
            for one_rank in range(1, rank_range):
                gather_axis_shape = list(self.params_shape[self.axis:self.axis + one_rank])
                gather_axis_range = list(self.params_range[self.axis:self.axis + one_rank])

                # gather after index
                if self.axis + one_rank == len(self.params_shape):
                    after_loop_shape = 1
                    after_loop_range = (1, 1)
                else:
                    after_loop_shape = util.combine_dim(self.params_shape[self.axis + one_rank:])
                    after_loop_range = util.combine_range(self.params_range[self.axis + one_rank:])

                # indices gather axis update
                indices_index_shape = one_rank
                indices_index_range = (one_rank, one_rank)

                # assemble info
                gather_ins = []
                params_dict = _asseble_params_info(
                    [self.batch_shape, gather_axis_shape, after_loop_shape],
                    [self.batch_range, gather_axis_range, after_loop_range],
                    self.params_dtype)

                indices_dict = _asseble_indices_info(
                    [self.batch_shape, self.indices_loop_shape, indices_index_shape],
                    [self.batch_range, self.indices_loop_range, indices_index_range],
                    self.indices_dtype)

                gather_ins.append(params_dict)
                gather_ins.append(indices_dict)
                gather_ins.append(1)

                gather_instance.append(gather_ins)
            return gather_instance


def _classify_gather_zero_shape(params_dtype, indices_dtype, gather_type="gather"):
    params_dict = {
        "shape": (0, 0, 0, 0),
        "range": ((0, 0), (0, 0), (0, 0), (0, 0)),
        "dtype": params_dtype
    }

    indices_dict = {
        "shape": (0, 0, 0),
        "range": ((0, 0), (0, 0), (0, 0)),
        "dtype": indices_dtype
    }

    if gather_type == "gather_nd":
        return [params_dict, indices_dict, 1]

    return [params_dict, indices_dict, 1, 1]


def _classify_gather_broadcast_shape(batch_dims, params_shape, indices_shape, params_range,
                                     indices_range, params_dtype, indices_dtype):
    # params like [parmas_batch, params_data]
    # indices like [indices_batch, indices_loops, 0]

    # fuse batch dims
    if batch_dims == 0:
        batch_shape = 1
        batch_range = (1, 1)
    else:
        batch_shape = util.combine_dim(params_shape[:batch_dims])
        batch_range = util.combine_range(params_range[:batch_dims])

    params_data_shape = util.combine_dim(params_shape[batch_dims:])
    params_data_range = util.combine_range(params_range[batch_dims:])

    if batch_dims == len(indices_shape):
        indices_loop_shape = 1
        indices_loop_range = (1, 1)
    else:
        indices_loop_shape = util.combine_dim(indices_shape[batch_dims:-1])
        indices_loop_range = util.combine_range(indices_range[batch_dims:-1])

    params_dict = {
        "shape": (batch_shape, params_data_shape),
        "range": (batch_range, params_data_range),
        "dtype": params_dtype
    }

    indices_dict = {
        "shape": (batch_shape, indices_loop_shape, 0),
        "range": (batch_range, indices_loop_range, (0, 0)),
        "dtype": indices_dtype
    }

    return [params_dict, indices_dict, 1]


def _asseble_params_info(shape_info, range_info, dtype_info):
    params_info = {}
    params_info["dtype"] = dtype_info

    params_shape = []
    params_range = []
    for one_shape, one_range in zip(shape_info, range_info):
        if isinstance(one_shape, list):
            for gather_idx, gather_range in zip(one_shape, one_range):
                params_shape.append(gather_idx)
                params_range.append(gather_range)
        else:
            params_shape.append(one_shape)
            params_range.append(one_range)

    params_info["shape"] = params_shape
    params_info["range"] = params_range

    return params_info


def _asseble_indices_info(shape_info, range_info, dtype_info):
    indices_info = {}
    indices_info["dtype"] = dtype_info
    indices_info["shape"] = shape_info
    indices_info["range"] = range_info

    return indices_info
