#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
helper for reduce classifier
"""
from copy import deepcopy
from enum import Enum
from enum import auto
from tbe.dsl.base import expr_compare

from .. import util


COMMON = "common"
REDUCE = "reduce"
SPECIAL = "special"
BEFORE = "before"
BEFORE_ALL_ONE = "before_all_one"
AFTER = "after"
AFTER_ALL_ONE = "after_all_one"
AXIS = "axis"
ZERO = "zero"


def is_const(shape):
    """
    check const shape
    :param shape:
    :return:
    """
    return all(x >= 0 for x in shape)


def generate_ins(reduce_axis_size, dim_len):
    """
    generate all possible reduce ins, with assigned reduce axis size and dim length
    :param reduce_axis_size:
    :param dim_len:
    :return:
    """
    ret = []
    reduce_axis_size = min(reduce_axis_size, (dim_len + 1) // 2)
    for size_i in range(reduce_axis_size):
        for p in generate_patterns(size_i):
            ret.append(generate_in(p))

    for p in generate_patterns(reduce_axis_size):
        if len(p) <= dim_len + 1:
            ret.append(generate_in(p))

    return ret


def generate_patterns(reduce_axis_size):
    """
    generate last nlast axis reduce pattern.
    such as:
    one reduce axis: (a, r), (a, r, a)
    three reduce axis: (a, r, a, r, a, r), (a, r, a, r, a, r, a)
    'a mean common axis, r means reduce axis'
    :param reduce_axis_size:
    :return:
    """
    if reduce_axis_size == 0:
        return [[COMMON]]

    ptl = [COMMON, REDUCE]
    last_pattern = [ptl[i % 2] for i in range(2 * reduce_axis_size)]
    nlast_pattern = [ptl[i % 2] for i in range(2 * reduce_axis_size + 1)]

    return [last_pattern, nlast_pattern]


def generate_in(pattern):
    """
    generate reduce input by pattern, contains data dict and reduce axes
    :param pattern:
    :return:
    """
    input_x = {
        "shape": [-1] * len(pattern),
        "range": [(1, None)] * len(pattern),
        "mode": SPECIAL,
        "rel_pos_to_reduce": BEFORE
    }

    reduce_axes = []
    for i, p in enumerate(pattern):
        if p == REDUCE:
            reduce_axes.append(i)

    return [input_x, reduce_axes]


def simplify(shape, ranges, reduce_axes):
    """
    simplify shape, range, reduce axis.
    fuse continuous reduce axis or non-reduce axis.
    :param shape:
    :param ranges:
    :param reduce_axes:
    :return:
    """
    f_shape, f_ranges = [1], [(1, 1)]
    f_reduce_axes = []

    state = ShapeSimplifier.State.ONE
    for i, (d, r) in enumerate(zip(shape, ranges)):
        if d == 1:
            continue

        is_reduce_axis = i in reduce_axes
        state_i = ShapeSimplifier.get_state(d, is_reduce_axis)
        operator = ShapeSimplifier.get_operator(state, state_i)

        if operator == ShapeSimplifier.Operator.FUSED:
            f_shape[-1] = util.combine_dim([f_shape[-1], d])
            f_ranges[-1] = util.combine_range([f_ranges[-1], r])
        else:
            f_shape.append(d)
            f_ranges.append(r)

        if is_reduce_axis:
            reduce_axis = len(f_shape) - 1
            if not f_reduce_axes or f_reduce_axes[-1] != reduce_axis:
                f_reduce_axes.append(reduce_axis)

        if state_i != ShapeSimplifier.State.ONE:
            state = state_i

    return f_shape, f_ranges, f_reduce_axes


def is_all_one_tensor(single_input):
    shape = single_input.get("shape")
    if shape:
        return all(x == 1 for x in shape)
    return False


def inputs_classify(inputs):
    """
    classify inputs_before_reduce and inputs_after_reduce

    all one tensor is used for broadcast input , no need used as classify tensor
    so the tensor won't be add to inputs_after_reduce or inputs_before_reduce.
    """
    inputs_before_reduce, inputs_after_reduce, input_axis, inputs_classification = [], [], [], []
    inputs_before_reduce_all_one_tmp = None
    for single_input in inputs:
        input_type = single_input.get("rel_pos_to_reduce")
        if input_type == AXIS:
            input_axis.append(deepcopy(single_input))
            inputs_classification.append(AXIS)
        elif input_type == AFTER:
            if is_all_one_tensor(single_input):
                inputs_classification.append(AFTER_ALL_ONE)
            else:
                inputs_classification.append(AFTER)
                inputs_after_reduce.append(deepcopy(single_input))
        else:
            if is_all_one_tensor(single_input):
                inputs_classification.append(BEFORE_ALL_ONE)
                if not inputs_before_reduce_all_one_tmp:
                    inputs_before_reduce_all_one_tmp = single_input
            else:
                inputs_classification.append(BEFORE)
                inputs_before_reduce.append(deepcopy(single_input))

    if not inputs_before_reduce:
        expr_compare.is_true(inputs_before_reduce_all_one_tmp is not None,
                             {"errCode": "E90001",
                              "detailed_cause": "inputs of classify must include the dict extra_params "
                                                "with the key keepdims when mode is reduce"
                              })
        inputs_before_reduce.append(inputs_before_reduce_all_one_tmp)

    return [inputs_before_reduce, inputs_after_reduce, input_axis, inputs_classification]


def _process_all_unknown_shape(shape_list, range_list):
    """
    process input include shape -2
    """
    all_unknown_shape_len = 8
    for single_shape in shape_list:
        if tuple(single_shape) != (-2, ):
            all_unknown_shape_len = len(single_shape)
            break

    for idx, single_shape in enumerate(shape_list):
        if tuple(single_shape) == (-2, ):
            shape_list[idx] = [-1] * all_unknown_shape_len
            range_list[idx] = [(0, None)] * all_unknown_shape_len


def generate_reduce_input(inputs_before_reduce, inputs_after_reduce=None, reduce_axis=None, keep_dims=None):
    """
    obtain the shape and range to classify
    """
    if inputs_after_reduce:
        for single_input in inputs_after_reduce:
            ori_shape = list(single_input["shape"])
            ori_range = list(single_input.get("range") if single_input.get("range") else [(1, None)] * len(ori_shape))
            for axis in reduce_axis:
                # the dim corresponding to reduce_axis of input_after_reduce is not working in judging const and
                # should be set to -1
                if not keep_dims:
                    ori_shape.insert(axis, -1)
                    ori_range.insert(axis, (0, None))
                else:
                    ori_shape[axis] = -1
                    ori_range[axis] = (0, None)
            single_input["shape"] = ori_shape
            single_input["range"] = ori_range
        inputs_before_reduce.extend(inputs_after_reduce)

    shape_local = [x["shape"] for x in inputs_before_reduce]
    range_local = \
        [x.get("range") if x.get("range") else [(1, None)] * len(shape_local[0]) for x in inputs_before_reduce]

    _process_all_unknown_shape(shape_local, range_local)

    def _get_dim(i):
        return max([s[i] for s in shape_local])

    shape_out = [_get_dim(i) for i in range(len(shape_local[0]))]

    def _select_min_upper_bound(input_list):
        min_ele = util.VAR_BOUND_LIMIT + 1
        for ele in input_list:
            if ele is None:
                continue
            if ele < min_ele:
                min_ele = ele
        return min_ele if min_ele != util.VAR_BOUND_LIMIT + 1 else None

    def _get_range(i):
        if shape_out[i] != -1:
            return shape_out[i], shape_out[i]
        else:
            return max([r[i][0] for r in range_local]), _select_min_upper_bound([r[i][1] for r in range_local])

    range_out = [_get_range(i) for i in range(len(range_local[0]))]

    for i, _ in enumerate(shape_out):
        if range_out[i][0] == range_out[i][1]:
            shape_out[i] = range_out[i][0]

    return {"shape": shape_out, "range": range_out}


def generate_ins_of_after_reduce(input_x, input_axis, keep_dims):
    """
    generate ins of inputs after reduce
    """
    if isinstance(input_axis, dict):
        reduce_axis = input_axis.get("value")
    else:
        reduce_axis = input_axis
    out_shape, out_range = [], []
    for i in range(len(input_x["shape"])):
        if i in reduce_axis:
            if not keep_dims:
                continue
            else:
                out_shape.append(1)
                out_range.append((1, 1))
        else:
            out_shape.append(input_x["shape"][i])
            out_range.append(input_x["range"][i])

    out_ins = {"shape": out_shape, "range": out_range,
               "mode": input_x["mode"], "rel_pos_to_reduce": AFTER}

    return out_ins


def replace_shape_to_all_one(input_tensor):
    shape = input_tensor.get("shape")
    input_tensor["shape"] = (1,) * len(shape)
    input_tensor["range"] = ((1, 1),) * len(shape)
    return input_tensor


def generate_ins_of_all(ins_before_reduce, ins_after_reduce, reduce_axis, inputs_classification):
    """
    generate ins of all inputs
    """
    # const shape passes reduce_axis as a dict with the key "ori_axis" and var shape passes a list
    if isinstance(reduce_axis, dict):
        ins_axis = reduce_axis
    else:
        ins_axis = {"shape": [len(reduce_axis), ], "value": reduce_axis, "rel_pos_to_reduce": AXIS}
    ins = []
    for symbol in inputs_classification:
        if symbol == AXIS:
            ins.append(deepcopy(ins_axis))
        elif symbol == BEFORE_ALL_ONE:
            ins.append(replace_shape_to_all_one(deepcopy(ins_before_reduce)))
        elif symbol == AFTER_ALL_ONE:
            ins.append(replace_shape_to_all_one(deepcopy(ins_after_reduce)))
        elif symbol == AFTER:
            ins.append(deepcopy(ins_after_reduce))
        else:
            ins.append(deepcopy(ins_before_reduce))
    return ins


def refine_ins(ins_before_reduce, reduce_axis):
    """
    if reduce axis is None, should refine ins
    """
    if not reduce_axis:
        ins_before_reduce["shape"] = [1] + ins_before_reduce["shape"]
        ins_before_reduce["range"] = [(1, 1)] + ins_before_reduce["range"]
        reduce_axis.append(0)


def ins_of_prebuild(ins, reduce_axis):
    """
    generate ins when build_config is "disable"
    """
    out_ins = []
    for single in ins:
        input_type = single.get("rel_pos_to_reduce")
        if input_type == AXIS:
            out_ins.append({"shape": [len(reduce_axis), ], "value": reduce_axis, "rel_pos_to_reduce": AXIS})
        else:
            out_ins.append(single)

    return out_ins


def normalize_ins(ins):
    _dim_len = -1
    _ins_deep_copy = deepcopy(ins)

    # pad 1 from high dimension for ins before reduce
    for _single_input in _ins_deep_copy:
        if _single_input.get("rel_pos_to_reduce") != BEFORE:
            continue
        _single_shape = _single_input.get("shape")
        _dim_len = max(_dim_len, len(_single_shape))
    for _single_input in _ins_deep_copy:
        if _single_input.get("rel_pos_to_reduce") != BEFORE:
            continue
        _shape_tuple = tuple(_single_input.get("shape"))
        _range_list = _single_input.get("range")
        if len(_shape_tuple) < _dim_len:
            _single_input["shape"] = [1] * (_dim_len - len(_shape_tuple)) + list(_shape_tuple)
            _single_input["range"] = [(1, 1)] * _dim_len if _range_list is None else \
                                     [(1, 1)] * (_dim_len - len(_range_list)) + list(_range_list)

    # infer shape value by range
    for _single_input in _ins_deep_copy:
        if _single_input.get("rel_pos_to_reduce") not in [BEFORE, AFTER]:
            continue
        _single_range = _single_input.get("range")
        _single_shape = list(_single_input.get("shape"))
        if _single_range:
            for _index, _ in enumerate(_single_shape):
                if _single_range[_index][0] == _single_range[_index][1]:
                    _single_shape[_index] = _single_range[_index][0]
            _single_input["shape"] = tuple(_single_shape)
    return _ins_deep_copy


class ShapeSimplifier:
    """
    ShapeSimplifier
    """

    class State(Enum):
        """
        the axis type
        """
        # dim is init
        INIT = auto()
        # dim is one
        ONE = auto()
        # not reduce axis
        COMMON = auto()
        # reduce axis
        REDUCE = auto()

    class Operator(Enum):
        """
        the fusion behavior of two contiguous axis
        """
        # can fuse axis
        FUSED = auto()
        # can not fuse axis
        ALONE = auto()

    @classmethod
    def get_state(cls, dim: int, is_reduce_axis: bool):
        """
        get_state
        :param dim:
        :param is_reduce_axis:
        :return:
        """
        if dim == 1:
            return cls.State.ONE

        return cls.State.REDUCE if is_reduce_axis else cls.State.COMMON

    @classmethod
    def get_operator(cls, state1, state2):
        """
        get_operator
        :param state1:
        :param state2:
        :return:
        """
        if state1 == cls.State.ONE or state2 == cls.State.ONE:
            return cls.Operator.FUSED
        if state1 == state2:
            return cls.Operator.FUSED

        return cls.Operator.ALONE


def generate_dynamic_zero_ins(keepdims, inputs_classification):
    """

    :return:
    """
    ins = []

    ins_x_0 = {
        "shape": (-1, 0),
        "range": [(1, None), (0, 0)],
        "mode": ZERO,
        "rel_pos_to_reduce": BEFORE
    }
    ins_axis_0 = {
        "shape": [1],
        "value": [1],
        "rel_pos_to_reduce": AXIS,
        "ori_axis": [1]
    }
    ins_after_reduce = generate_ins_of_after_reduce(ins_x_0, ins_axis_0, keepdims)
    ins.append(generate_ins_of_all(ins_x_0, ins_after_reduce, ins_axis_0, inputs_classification))

    return ins


def generate_const_zero_ins(shape, reduce_axis, keepdims, inputs_classification):
    """
    :return:
    """
    non_reduce_dim_total = 1
    exist_reduce_axis_zero = False
    for i, shape_i in enumerate(shape):
        if i not in reduce_axis:
            non_reduce_dim_total *= shape_i
        elif shape_i == 0:
            exist_reduce_axis_zero = True

    expr_compare.is_true(exist_reduce_axis_zero,
                         {"errCode": "E90001",
                          "detailed_cause": "reduce not support non reduce axis contains zero."
                          })

    ins_x_0 = {
        "shape": (non_reduce_dim_total, 0),
        "range": [(non_reduce_dim_total, non_reduce_dim_total), (0, 0)],
        "mode": ZERO,
        "rel_pos_to_reduce": BEFORE
    }
    ins_axis_0 = {
        "shape": [1],
        "value": [1],
        "rel_pos_to_reduce": AXIS,
        "ori_axis": [1]
    }

    ins_after_reduce = generate_ins_of_after_reduce(ins_x_0, ins_axis_0, keepdims)
    return [generate_ins_of_all(ins_x_0, ins_after_reduce, ins_axis_0, inputs_classification)]


class ZeroAxisStatus(Enum):
    """
    shape have zero axis status
    """

    EXIST = auto()
    MAYBE = auto()
    NON_EXIST = auto()


def handle_zero_axis(input_x):
    shape_x = input_x["shape"]
    range_x = input_x["range"]

    exist, maybe = False, False
    for i, dim_i in enumerate(shape_x):
        if dim_i == 0:
            exist = True
            break
        elif range_x[i][0] == 0:
            maybe = True

    if exist:
        return ZeroAxisStatus.EXIST
    elif maybe:
        for i, r in enumerate(range_x):
            if range_x[i][0] == 0:
                range_x[i] = (1, range_x[i][1])
        return ZeroAxisStatus.MAYBE
    else:
        return ZeroAxisStatus.NON_EXIST
