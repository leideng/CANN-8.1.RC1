#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
nn
"""
from __future__ import absolute_import

from enum import Enum
from enum import auto
from functools import reduce

from tbe import tvm
from tbe.common.platform import intrinsic_check_support
from tbe.common.platform import platform_info
from tbe.common.testing.dsl_source_info import source_info_decorator
from tbe.common.utils import shape_util
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.base import operation
from tbe.dsl.base.expr_compare import expr_equal as equal
from tbe.dsl.base.record.decorators import broadcast as broadcast_decorator
from tbe.dsl.base.record.decorators import elewise as elewise_decorator
from tbe.dsl.compute.constants import ComputeType

from ..base import d_format_util
from .math import __multiple_elewise_op
from .math import __single_elewise_op
from .math import _cast_tensors_for_instr
from .math import _check_multi_compute_pattern
from .math import vadd
from .math import vadds
from .math import vmax
from .math import vmin
from .math import vmul
from .math import vmuls
from .math import vsub
from .util import check_input_tensor_shape
from .util import disable_broadcast_optimization
from .util import dtype_check_decorator
from .util import dynamic_static_unify_fractal_format
from .util import in_dynamic_and_static_unify
from .util import judge_var

NAME_INDEX = [0]
BROADCAST_VAR_TYPES = (tvm.Var, tvm.expr.Max, tvm.expr.Mul)


@elewise_decorator.triple
@source_info_decorator()
@dtype_check_decorator
def vmaddrelu(tensor_0, tensor_1, tensor_2):
    """
    calculate relu(tensor_0*tensor_2 + tensor_1), only support  float16, float32
    Parameters
    ----------
    tensor_0 : wrapped_tensor or tvm.tensor
    tensor_1 : wrapped_tensor or tvm.tensor
    tensor_2 : wrapped_tensor or tvm.tensor
    Returns
    -------
    wrapped_tensor : relu(tensor_0*tensor_2 + tensor_1)
    """
    if not isinstance(tensor_1, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The second input type must be tvm.tensor, while type is {type(tensor_1)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))
    if not isinstance(tensor_2, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The third input type must be tvm.tensor, while type is {type(tensor_2)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))
    # 5hd scene can not enable paddinng for ternary instructions due to complex memory reuse
    if dynamic_static_unify_fractal_format():
        mul_res = vmul(tensor_0, tensor_2)
        vadd_res = vadd(mul_res, tensor_1)
        vrelu_support = intrinsic_check_support("Intrinsic_vrelu") and \
            platform_info.api_check_support('tbe.dsl.vrelu', tensor_0.dtype)
        tensor_zero = broadcast(tvm.const(0, tensor_0.dtype), tensor_0.shape)
        res = vrelu(vadd_res) if vrelu_support else vmax(vadd_res, tensor_zero)
        return res

    return __multiple_elewise_op(tensor_0, tensor_1, tensor_2, "elewise_multiple_maddrelu")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vaddrelu(lhs, rhs):
    """
    calculate relu(lhs + rhs)

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.tensor
        left hand tensor

    Returns
    -------
    wrapped_tensor : relu (lhs + rhs)
    """
    if not isinstance(lhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The lhs input type must be tvm.tensor while now is {type(lhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if not isinstance(rhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The rhs input type must be tvm.tensor while now is {type(rhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if lhs.dtype != rhs.dtype:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"dtype must be the same, while lhs is {lhs.dtype} and rhs is {rhs.dtype}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    is_current_chip_support = intrinsic_check_support("Intrinsic_vaddrelu")
    if not is_current_chip_support:
        add = vadd(lhs, rhs)
        res = vrelu(add)
        return res

    shape = lhs.shape
    op_name = "elewise_binary_addrelu"
    lambda_func = lambda *indice: tvm.relu(lhs(*indice) + rhs(*indice))

    name = f"{op_name.split('_')[-1]}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    with tvm.tag_scope(op_name):
        tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

    return tmp


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vsubrelu(lhs, rhs):
    """
    calculate relu(lhs - rhs)

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.tensor
        left hand tensor

    Returns
    -------
    wrapped_tensor : relu (lhs - rhs)
    """
    if not isinstance(lhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The lhs input type must be tvm.tensor, while type is {type(lhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if not isinstance(rhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The rhs input type must be tvm.tensor, while type is {type(rhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if lhs.dtype != rhs.dtype:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"dtype must be the same, while lhs is {lhs.dtype} and rhs is {rhs.dtype}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    is_current_chip_support = intrinsic_check_support("Intrinsic_vsubrelu")
    if not is_current_chip_support:
        sub = vsub(lhs, rhs)
        res = vrelu(sub)
        return res

    shape = lhs.shape
    op_name = "elewise_binary_subrelu"
    lambda_func = lambda *indice: tvm.relu(lhs(*indice) - rhs(*indice))

    name = f"{op_name.split('_')[-1]}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    with tvm.tag_scope(op_name):
        tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

    return tmp


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vrelu(raw_tensor):
    """
    calculate vrelu(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor

    Returns
    -------
    wrapped_tensor : vrelu(raw_tensor)
    """
    _is_current_chip_support_vaddrelu = intrinsic_check_support("Intrinsic_vaddrelu")
    vaddrelu_pattern = _check_multi_compute_pattern(("elewise_binary_add",), raw_tensor)
    if _is_current_chip_support_vaddrelu and vaddrelu_pattern:
        input_tensor = tuple(raw_tensor.op.input_tensors)
        return vaddrelu(*input_tensor)

    _is_current_chip_support_vsubrelu = intrinsic_check_support("Intrinsic_vsubrelu")
    vsubrelu_pattern = _check_multi_compute_pattern(("elewise_binary_sub",), raw_tensor)
    if _is_current_chip_support_vsubrelu and vsubrelu_pattern:
        input_tensor = tuple(raw_tensor.op.input_tensors)
        return vsubrelu(*input_tensor)

    dtype = raw_tensor.dtype

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_relu')


@elewise_decorator.vlrelu
@source_info_decorator()
@dtype_check_decorator
def vlrelu(raw_tensor, alpha=0):
    """
    calculate leaky_relu

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor

    Returns
    -------
    wrapped_tensor : vlrelu(raw_tensor)
    """
    dtype = raw_tensor.dtype
    shape = raw_tensor.shape

    if judge_var(alpha) == "tvm_const":
        if alpha.dtype != dtype:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"The alpha dtype: {alpha.dtype} need be equal to raw_tensor's: {dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        alpha_value = alpha.value
    elif judge_var(alpha) == "python_const":
        alpha_value = alpha
        alpha = tvm.const(alpha, dtype=dtype)

    is_current_chip_support = intrinsic_check_support("Intrinsic_vlrelu", dtype)
    if not is_current_chip_support:
        if judge_var(alpha) in ("tvm_const", "python_const"):
            if alpha_value == 0:
                if dtype in ("float32", "int32"):
                    tensor_zero = broadcast(tvm.const(0, dtype), shape)
                    data_res = vmax(raw_tensor, tensor_zero)
                else:
                    data_res = vrelu(raw_tensor)

                return data_res

            muls_tmp = vmuls(raw_tensor, alpha)
            if alpha_value <= 1:
                res = vmax(raw_tensor, muls_tmp)
            else:
                res = vmin(raw_tensor, muls_tmp)

            return res
        else:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "The alpha not support var type while current soc not support vlrelu."}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    op_name = "elewise_single_lrelu"
    lambda_func = lambda *indice: tvm.lrelu(raw_tensor(*indice), alpha)

    name = f"{op_name.split('_')[-1]}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    with tvm.tag_scope(op_name):
        tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

    return tmp


@elewise_decorator.clip
@source_info_decorator()
def clip(data, max_value, min_value):
    """
    round data to [min_value,max_value]

    Parameters
    ----------
    data : tvm.tensor
        tensors need to change dtype

    max_value/min_value : float
        the range of res

    Returns
    -------
    tensor : tvm.tensor ,elements in tensor is in range [min_value,max_value]
    """
    if isinstance(data, tvm.Tensor):
        check_input_tensor_shape(data)
    tensor_vmuls = _cast_tensors_for_instr("vmuls", [data, 0])
    data_tmp = vmuls(tensor_vmuls[0], tensor_vmuls[1])
    tensor_vadds = _cast_tensors_for_instr("vadds", [data_tmp, min_value])
    data_min = vadds(tensor_vadds[0], tensor_vadds[1])
    tensor_vadds_tmp = _cast_tensors_for_instr("vadds", [data_tmp, max_value])
    data_max = vadds(tensor_vadds_tmp[0], tensor_vadds_tmp[1])
    tensor_vmax = _cast_tensors_for_instr("vmax", [data, data_min])
    data1 = vmax(tensor_vmax[0], tensor_vmax[1])
    tensor_vmin = _cast_tensors_for_instr("vmin", [data1, data_max])
    data1 = vmin(tensor_vmin[0], tensor_vmin[1])
    return data1


@broadcast_decorator.broadcast
@source_info_decorator()
@dtype_check_decorator
def broadcast(var, shape, output_dtype=None):
    """
    broadcast scalar or tensor to tensor

    Parameters
    ----------
    var : can be tensor, tvm_const, python_const and tvm_const

    shape : target tensor shape

    output_dtype : tensor dtype , default : var.dtype

    Returns
    -------
    wrapped_tensor : broadcast tensor
    """
    if not isinstance(shape, (list, tuple, tvm.container.Array)):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"the input parameter shape must be list or tuple, but now is {type(shape)}."}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if isinstance(var, tvm.Tensor):
        return _tensor_broadcast(var, shape)
    return _scalar_broadcast(var, shape, output_dtype)


@broadcast_decorator.broadcast
@source_info_decorator()
@dtype_check_decorator
def full(var, shape, dtype=None):
    """
    return a new tensor of given shape and type, filled with var

    Parameters
    ----------
    var : can be tvm_const, python_const

    shape : target tensor shape

    dtype : tensor dtype , default : int32 or float32

    Returns
    -------
    wrapped_tensor : tensor
    """
    if not isinstance(shape, (list, tuple, tvm.container.Array)):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"the input parameter shape must be list or tuple, but now is {type(shape)}."}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if isinstance(var, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": "the input parameter var must not be a tensor."}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    return _scalar_full(var, shape, dtype)


def _scalar_full(var, shape, dtype=None):
    var_type = judge_var(var)
    tmp_args = var
    if var_type == "python_const":
        if isinstance(tmp_args, float):
            tmp_args = tvm.const(tmp_args, dtype="float16")
        else:
            tmp_args = tvm.const(tmp_args, dtype="int32")

    if not dtype:
        dtype = tmp_args.dtype

    tmp_args = tmp_args.astype(dtype)

    lambda_func = lambda *indice: tmp_args

    name = "full_" + str(NAME_INDEX[0])
    NAME_INDEX[0] += 1

    _op = 'scalar_fill'
    with tvm.tag_scope(_op):
        out = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.BRC_SCALAR})
    return out


def _is_inplace_compute():
    if operation.get_context() is None or operation.get_context().get_current_compute() is None:
        return False

    return operation.get_context().get_current_compute().get("is_inplace_compute")


def _scalar_broadcast(var, shape, output_dtype):
    var_type = judge_var(var)
    tmp_args = var
    if var_type == "python_const":
        if isinstance(tmp_args, float):
            tmp_args = tvm.const(tmp_args, dtype="float16")
        else:
            tmp_args = tvm.const(tmp_args, dtype="int32")

    if not output_dtype:
        output_dtype = tmp_args.dtype

    tmp_args = tmp_args.astype(output_dtype)

    lambda_func = lambda *indice: tmp_args

    name = "broadcast_" + str(NAME_INDEX[0])
    NAME_INDEX[0] += 1

    _op = 'broadcast'
    with tvm.tag_scope(_op):
        out = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.BRC_SCALAR})
    return out


def _tensor_broadcast(var, shape) -> tvm.Tensor:
    """
    broadcast tensor to tensor
    """

    def check_tensor_shape(src_shape, dst_shape):
        check_input_tensor_shape(src_shape)
        if len(src_shape) > len(dst_shape):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"broadcast src_shape len must not be greater than dst_shape, but now "
                                           f"src_shape len is {len(src_shape)}, and dst_shape len is {len(dst_shape)}."}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    def _get_op_tag():
        is_inplace_compute = _is_inplace_compute()
        op_tag = "broadcast_for_tensor"
        ori_shape_list = shape_util.shape_to_list(ori_shape)
        if in_dynamic_and_static_unify():
            if reduce(lambda x, y: x * y, ori_shape_list) == 1 and not disable_broadcast_optimization() \
                and not is_inplace_compute:
                op_tag = "one_shape_broadcast"
            elif len(ori_shape) == 1 and not disable_broadcast_optimization() and not is_inplace_compute:
                op_tag = "one_rank_broadcast"
            elif is_unknown_broadcast:
                op_tag = "unknown_broadcast"
            else:
                op_tag = "unified_broadcast"

        return op_tag

    def lambda_func(*indices):
        index = []
        for i, compelet_shape_i in enumerate(compelet_shape):
            if equal(compelet_shape_i, shape[i]):
                index.append(indices[i])
            elif equal(compelet_shape_i, 1):
                index.append(0)
            else:
                index.append(tvm.select(compelet_shape_i == 1, 0, indices[i]))
        return tensor(*(index[difference:]))

    class AxisCompareState(Enum):
        """
        AxisCompareState
        """
        # axis equal, ori_axis equal
        EQUAL = auto()
        # axis not equal
        NOT_EQUAL = auto()
        # axis can not be determined
        UNKNOWN = auto()
        # axis equal, ori_axis not equal
        NOT_ALL_EQUAL = auto()

    def compare_axis(axis_0, axis_1, is_5hd=False):
        is_axis_equal = equal(axis_0, axis_1)
        if is_axis_equal:
            axis_type = d_format_util.get_axis_type(axis_0)
            is_pad_axis = d_format_util.eq_axis_type(axis_type, "C0") or d_format_util.eq_axis_type(axis_type, "C1")
            is_ignore_format = not is_5hd or not is_pad_axis
            if is_ignore_format:
                return AxisCompareState.EQUAL

            ori_c_src = d_format_util.get_original(axis_0)
            ori_c_dst = d_format_util.get_original(axis_1)
            ori_c_equal = equal(ori_c_src, ori_c_dst)
            if ori_c_equal:
                return AxisCompareState.EQUAL
            return AxisCompareState.NOT_ALL_EQUAL

        contains_varibale = isinstance(axis_0, BROADCAST_VAR_TYPES) or \
                            isinstance(axis_1, BROADCAST_VAR_TYPES)
        if contains_varibale:
            return AxisCompareState.UNKNOWN

        return AxisCompareState.NOT_EQUAL

    def check_broadcast_shape(src_shape, dst_shape):
        is_5hd = d_format_util.is_5hd_format(src_shape)
        for src_axis, dst_axis in zip(src_shape, dst_shape):
            comare_state = compare_axis(src_axis, dst_axis, is_5hd)
            if comare_state == AxisCompareState.NOT_EQUAL and not equal(src_axis, 1):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"for tensor broadcast, shape must be same or one of them is 1, but now"
                                               f" the src shape is {src_shape}, and the dst shape is {dst_shape}."}
                raise RuntimeError(dict_args, get_error_message(dict_args))

    tensor = var
    ori_shape = list(tensor.shape)
    check_tensor_shape(ori_shape, shape)
    difference = len(shape) - len(ori_shape)
    compelet_shape = difference * [1] + ori_shape
    axis_equal_num = 0
    is_unknown_broadcast = False
    is_5hd = d_format_util.is_5hd_format(shape)
    check_broadcast_shape(compelet_shape, shape)

    for src_axis, dst_axis in zip(compelet_shape, shape):
        # vector_broadcast: default, any(NOT_ALL_EQUAL, NOT_EQUAL && src_axis == 1, UNKNOWN && src_axis == 1)
        compare_state = compare_axis(src_axis, dst_axis, is_5hd)
        if compare_state == AxisCompareState.EQUAL:
            axis_equal_num += 1
            continue
        if compare_state == AxisCompareState.UNKNOWN and not equal(src_axis, 1):
            is_unknown_broadcast = True
            continue

    if axis_equal_num == len(shape) and not disable_broadcast_optimization():
        return tensor

    name = "broadcast_tensor_" + str(NAME_INDEX[0])
    NAME_INDEX[0] += 1
    _op = _get_op_tag()
    with tvm.tag_scope(_op):
        out = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.BRC_TENSOR})

    return out
