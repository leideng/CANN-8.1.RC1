#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2021-2021 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
c04_transdata
"""
from copy import copy
from tbe import tvm
from tbe.dsl.base import operation
from tbe.common.utils.shape_util import shape_to_list
from .transdata_compute import TransdataComputation
from .transdata_op import math_prod
from .transdata_op import set_align


DIM_16 = 16
DIM_4 = 4


class C04ForwardComputation(TransdataComputation):
    """
    C04ForwardComputation
    """

    def __init__(self, tensor, dst_shape, axes_map, pad_value):
        self.tensor = tensor
        self.src_shape = shape_to_list(tensor.shape)
        self.dst_shape = shape_to_list(dst_shape)
        self.branch = axes_map
        self.pad_value = pad_value
        self.dtype = self.tensor.dtype

    @staticmethod
    def pad(tensor, dst_shape, axes, pad_value, name="pad"):
        """
        Pad [A,B,C] to [Ax,B,C]
        """
        def func(i, pad_axis, tensor_):
            pad_cond = i[pad_axis] >= src_shape[pad_axis]
            origin_value = tensor_[i]
            return tvm.select(pad_cond, pad_value, origin_value)

        src_shape = shape_to_list(tensor.shape)
        with tvm.tag_scope("transdata|pad"):
            res = tvm.compute(dst_shape, lambda *i: func(i, axes, tensor), name=name, attrs={"axes": axes})
        return res

    @staticmethod
    def s_reshape(tensor, dst_shape, axes, name="s_reshape"):
        """
        Split [A,B,C] to [A.o, A.i, B, C]
        """
        def func(idx):
            mapped_idx = []
            for axis in axes:
                if isinstance(axis, int):
                    mapped_idx.append(idx[axis])
                else:
                    s, e = axis[0], axis[-1] + 1
                    strides = (math_prod(dst_shape[(i + 1):e]) for i in axis)
                    mapped_idx.append(sum(a * b for a, b in zip(idx[s:e], strides)))
            return mapped_idx

        with tvm.tag_scope("transdata|s_reshape"):
            res = tvm.compute(dst_shape, lambda *i: tensor(*func(i)), name=name, attrs={"axes": axes})
        return res

    @staticmethod
    def transpose(tensor, dst_shape, permute, name="transpose"):
        with tvm.tag_scope("transdata|transpose"):
            res = tvm.compute(dst_shape, lambda *i: tensor(*[idx for _, idx in sorted(zip(permute, i))]),
                              name=name, attrs={"permute": permute})
        return res

    @staticmethod
    def f_reshape(tensor, dst_shape, axes, name="f_reshape"):
        """
        Fuse [A.o, A.i, B, C] to [A, B, C]
        """
        def func(idx):
            mapped_idx = []
            for i, axis in enumerate(axes):
                if isinstance(axis, int):
                    mapped_idx.append(idx[i])
                else:
                    remained = idx[i]
                    for x in axis:
                        stride = math_prod(src_shape[(x + 1):(axis[-1] + 1)])
                        mapped_idx.append(remained // stride)
                        remained = remained % stride
            return mapped_idx

        src_shape = shape_to_list(tensor.shape)
        with tvm.tag_scope("transdata|f_reshape"):
            res = tvm.compute(dst_shape, lambda *i: tensor(*func(i)), name=name, attrs={"axes": axes})
        return res


    @classmethod
    def get_category(cls):
        return "general.c04.forward"


    def do_compute(self):
        # N,C,H -> Nx,C,H
        pad_tensor = self._pad_n(self.tensor)
        # Nx,C,H -> No,16,C,H
        split_tensor = self._split_n(pad_tensor)
        # No,16,C,H -> No,C,H,16
        transpose_tensor = self._transpose_0(split_tensor)
        # No,C,H,16 -> No,C4,H,16
        pad_tensor = self._pad_c(transpose_tensor)
        # No,C4,H,16 -> No,H,C4,16
        transpose_tensor = self._transpose_1(pad_tensor)
        # No,H,C4,16 -> No,HC4,16
        fuse_tensor = self._fuse_hc4(transpose_tensor)
        # No,HC4,16 -> No,T,16
        pad_tensor = self._pad_hc4(fuse_tensor)
        # No,HC4,16 -> No,T1,T0,16
        split_tensor = self._split_hc4(pad_tensor)
        # No,T1,T0,16 -> T1,No,T0,16
        transpose_tensor = self._transpose_2(split_tensor)
        # T1,No,T0,16 -> T1,No,16,T0
        transpose_tensor = self._transpose_3(transpose_tensor)
        return transpose_tensor

    def _pad_n(self, tensor):
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [set_align(src_shape[0], DIM_16), src_shape[1], src_shape[2]]
        pad_value = tvm.const(self.pad_value, dtype=self.dtype)
        axes = 0
        return self.pad(tensor, dst_shape, axes, pad_value, "padN")
    
    def _split_n(self, tensor):
        axes = [[0, 1], 2, 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[0] / 16, 16, src_shape[1], src_shape[2]]
        return self.s_reshape(tensor, dst_shape, axes, name="splitN")

    def _transpose_0(self, tensor):
        permute = [0, 2, 3, 1]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose0")

    def _pad_c(self, tensor):
        src_shape = shape_to_list(tensor.shape)
        dst_shape = copy(src_shape)
        dst_shape[1] = set_align(dst_shape[1], DIM_4)
        pad_value = tvm.const(self.pad_value, dtype=self.dtype)
        axes = 1
        return self.pad(tensor, dst_shape, axes, pad_value, "padC")

    def _transpose_1(self, tensor):
        permute = [0, 2, 1, 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose1")

    def _fuse_hc4(self, tensor):
        axes = [0, [1, 2], 3]
        dst_shape = shape_to_list(tensor.shape)
        dst_shape = [dst_shape[0], dst_shape[1] * dst_shape[2], dst_shape[3]]
        return self.f_reshape(tensor, dst_shape, axes, name="fuseHC4")

    def _pad_hc4(self, tensor):
        src_shape = shape_to_list(tensor.shape)
        dst_shape = copy(src_shape)
        dst_shape[1] = set_align(dst_shape[1], DIM_16)
        pad_value = tvm.const(self.pad_value, dtype=self.dtype)
        axes = 1
        return self.pad(tensor, dst_shape, axes, pad_value, "padHC4")

    def _split_hc4(self, tensor):
        axes = [0, [1, 2], 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[0], src_shape[1] / 16, 16, src_shape[2]]
        return self.s_reshape(tensor, dst_shape, axes, name="splitHC4")

    def _transpose_2(self, tensor):
        permute = [1, 0, 2, 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose2")

    def _transpose_3(self, tensor):
        permute = [0, 1, 3, 2]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose3")
        

class C04BackwardComputation(TransdataComputation):
    """
    C04BackwardComputation
    """

    def __init__(self, tensor, dst_shape, axes_map, pad_value):
        self.tensor = tensor
        self.src_shape = shape_to_list(tensor.shape)
        self.dst_shape = shape_to_list(dst_shape)
        self.branch = axes_map
        self.pad_value = pad_value
        self.dtype = self.tensor.dtype


    @staticmethod
    def depad(tensor, dst_shape, axes, name="depad"):
        """
        dePad [Ax,B,C] to [A,B,C]
        """
        src_shape = shape_to_list(tensor.shape)
        with tvm.tag_scope("transdata|depad"):
            res = tvm.compute(dst_shape, lambda *i: tensor[i], name=name, attrs={"axes": axes})
        return res


    @staticmethod
    def s_reshape(tensor, dst_shape, axes, name="s_reshape"):
        """
        Split [A,B,C] to [A.o, A.i, B, C]
        """
        def func(idx):
            mapped_idx = []
            for axis in axes:
                if isinstance(axis, int):
                    mapped_idx.append(idx[axis])
                else:
                    s, e = axis[0], axis[-1] + 1
                    strides = (math_prod(dst_shape[(i + 1):e]) for i in axis)
                    mapped_idx.append(sum(a * b for a, b in zip(idx[s:e], strides)))
            return mapped_idx

        with tvm.tag_scope("transdata|s_reshape"):
            res = tvm.compute(dst_shape, lambda *i: tensor(*func(i)), name=name, attrs={"axes": axes})
        return res


    @staticmethod
    def transpose(tensor, dst_shape, permute, name="transpose"):
        with tvm.tag_scope("transdata|transpose"):
            res = tvm.compute(dst_shape, lambda *i: tensor(*[idx for _, idx in sorted(zip(permute, i))]),
                              name=name, attrs={"permute": permute})
        return res


    @staticmethod
    def f_reshape(tensor, dst_shape, axes, name="f_reshape"):
        """
        Fuse [A.o, A.i, B, C] to [A, B, C]
        """
        def func(idx):
            mapped_idx = []
            for i, axis in enumerate(axes):
                if isinstance(axis, int):
                    mapped_idx.append(idx[i])
                else:
                    remained = idx[i]
                    for x in axis:
                        stride = math_prod(src_shape[(x + 1):(axis[-1] + 1)])
                        mapped_idx.append(remained // stride)
                        remained = remained % stride
            return mapped_idx

        src_shape = shape_to_list(tensor.shape)
        with tvm.tag_scope("transdata|f_reshape"):
            res = tvm.compute(dst_shape, lambda *i: tensor(*func(i)), name=name, attrs={"axes": axes})
        return res


    @classmethod
    def get_category(cls):
        """
        Return tag of transdata
        """
        return "general.c04.backward"

    
    def do_compute(self):
        """
        Process
        """
        # C1,N1,N0,C0 -> C1,N1,C0,N0
        transpose_0_tensor = self._transpose_0(self.tensor)
        # C1,N1,C0,N0 -> C1,C0,N1,N0 (UB2UB)
        transpose_1_tensor = self._transpose_1(transpose_0_tensor)
        # C1,C0,N1,N0 -> HC4_align16,N1,N0
        fuse_c_tensor = self._fuse(transpose_1_tensor)
        # HC4_align16,N1,N0 -> HC4,N1,N0
        depad_tensor = self._depad_hc4(fuse_c_tensor)
        # HC4,N1,N0 -> H,C4,N1,N0
        split_tensor = self._split_hc4(depad_tensor)
        # H,C4,N1,N0 -> H,C,N1,N0
        depad_c_tensor = self._depad_c(split_tensor)
        # H,C,N1,N0 -> C,H,N1,N0 (UB2UB)
        transpose_2_tensor = self._transpose_2(depad_c_tensor)
        # C,H,N1,N0 -> N1,N0,C,H(CH do storageAlign)
        transpose_3_tensor = self._transpose_3(transpose_2_tensor)
        # N1,N0,C,H(CH do storageAlign) -> Nx,C,H
        fuse_n_tensor = self._fuse(transpose_3_tensor)
        # Nx,C,H -> N,C,H
        depad_n_tensor = self._depad_n(fuse_n_tensor)
        
        return depad_n_tensor

    
    def _transpose_0(self, tensor):
        permute = [0, 1, 3, 2]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose0")

    
    def _transpose_1(self, tensor):
        permute = [0, 2, 1, 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose1")


    def _transpose_2(self, tensor):
        permute = [1, 0, 2, 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose2")


    def _transpose_3(self, tensor):
        permute = [2, 3, 0, 1]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [src_shape[i] for i in permute]
        return self.transpose(tensor, dst_shape, permute, name="transpose3")

    
    def _fuse(self, tensor):
        axes = [[0, 1], 2, 3]
        dst_shape = shape_to_list(tensor.shape)
        dst_shape = [dst_shape[0] * dst_shape[1], dst_shape[2], dst_shape[3]]
        return self.f_reshape(tensor, dst_shape, axes, name="fuse")


    def _depad_hc4(self, tensor):
        src_shape = shape_to_list(tensor.shape)
        dst_shape = copy(src_shape)
        dst_shape[0] = set_align(self.dst_shape[1], DIM_4) * self.dst_shape[2]
        axes = 0
        return self.depad(tensor, dst_shape, axes, name="depadHC4")

    
    def _split_hc4(self, tensor):
        axes = [[0, 1], 2, 3]
        src_shape = shape_to_list(tensor.shape)
        dst_shape = [self.dst_shape[2], DIM_4, src_shape[1], src_shape[2]]
        return self.s_reshape(tensor, dst_shape, axes, name="splitHC4")

    
    def _depad_n(self, tensor):
        src_shape = shape_to_list(tensor.shape)
        dst_shape = copy(src_shape)
        dst_shape[0] = self.dst_shape[0]
        axes = 0
        return self.depad(tensor, dst_shape, axes, name="depadN")


    def _depad_c(self, tensor):
        src_shape = shape_to_list(tensor.shape)
        dst_shape = copy(src_shape)
        dst_shape[1] = self.dst_shape[1]
        axes = 1
        return self.depad(tensor, dst_shape, axes, name="depadC")
