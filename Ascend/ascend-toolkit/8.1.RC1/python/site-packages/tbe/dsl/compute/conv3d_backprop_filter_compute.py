#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv3d backprop filter DSL interface.
"""
import functools

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.dsl.compute import util as compute_util
from tbe.dsl.base.operation import get_te_var
from tbe.common import utils as tbe_utils
from tbe.common.utils.errormgr import error_manager_util
from tbe.common.utils.errormgr import error_manager_cube as cube_err
from tbe.dsl.compute import cube_util
from tbe.common.platform import platform_info as tbe_platform_info

# for load3d_special_case, fmap width must be in [1,63]
_FMAP_W_MAX = 63
# fractal size, only support 16 for now
_BLOCK_SIZE = 16
# maximum of int64 (2**63 - 1)
_DATA_SIZE_LIMIT_INT64 = 9223372036864776807
_DYNAMIC_BATCH = 0X0001
_DYNAMIC_DEPTH = 0X0002
_DYNAMIC_HEIGHT = 0X0004
_DYNAMIC_WIDTH = 0X0008


def _check_shape_rule(shape, dim, name):
    """
    check shape

    """
    if len(shape) != dim:
        args_dict = {
            'errCode': 'E60006',
            'param_name': name,
            'expected_length': str(dim),
            'length': str(len(shape))
        }
        raise RuntimeError(args_dict,
                           error_manager_util.get_error_message(args_dict))
    for dim_x in shape:
        if (isinstance(dim_x, int) and dim_x <= 0):
            cube_err.raise_err_one_para('E62509', 'conv3d_backprop_filter', name)


def _check_attr_rule(attr, dim, attr_limit, name):
    """
    check attribute

    """
    attr_min = attr_limit[0]
    attr_max = attr_limit[1]
    if len(attr) != dim:
        args_dict = {
            'errCode': 'E60006',
            'param_name': name,
            'expected_length': str(dim),
            'length': str(len(attr))
        }
        raise RuntimeError(args_dict,
                           error_manager_util.get_error_message(args_dict))
    for attr_x in attr:
        if (not isinstance(attr_x, int)) or attr_x < attr_min or attr_x > attr_max:
            cube_err.raise_err_attr_range_invalid("conv3d_backprop_filter",
                "[{},{}]".format(attr_min, attr_max),
                name,
                str(attr_x))


def _check_variable_range(variable, minimum, maximum, name):
    """
    check variable range

    """
    if variable < minimum or variable > maximum:
        cube_err.raise_err_attr_range_invalid("conv3d_backprop_filter",
            "[{},{}]".format(minimum, maximum),
            name,
            str(variable))


def _check_addressing_rule(shape, byte_count, limit):
    """
    check addressing limit

    """
    # product of all dimension
    product = functools.reduce(lambda x, y: x * y, shape[:])
    if product * byte_count > limit:
        cube_err.raise_err_attr_range_invalid("conv3d_backprop_filter",
            "(0,{}]".format(limit),
            "address_byte (data_size is limited to int64)",
            str(product * byte_count))


class Conv3dBackpropFilter:
    """
    Conv3dBackpropFilter: compute definition of conv3d_backprop_filter
    """

    def __init__(self, input_x, out_backprop, filter_sizes, para_dict):
        """
        initialization

        Parameters:
        ----------
        input_x : the featuremap data, tvm.placeholder, 6hd shape
        [N, D, C1, H, W, C0]

        out_backprop : the grads data, tvm.placeholder, 6hd shape
        [N, DO, GRADS_C1, HO, WO, GRADS_C0]

        filter_sizes : 5-D shape, specifies the filter sizes
        [GRADS_C, KD, C, KH, KW]

        strides : 3-D shape in depth, height and width dimension
        [STRIDE_D, STRIDE_H, STRIDE_W]

        padding : 6-D shape in front/back/up/down/left/right dimension
        [PAD_D, PAD_D, PAD_H, PAD_H, PAD_W, PAD_W]

        dilations : 5-D shape in batch/channel/depth/height/width dimension

        res_dtype : the output data type

        Returns
        -------
        None
        """
        self.stride = list(para_dict.get("strides"))
        self.pad = list(para_dict.get("pads"))
        self.group_dict = para_dict.get("group_dict")
        self.dilation = list(para_dict.get("dilations"))
        self.res_dtype = para_dict.get("res_dtype")
        self.kernel_name = para_dict.get("kernel_name")
        self.support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        binary_flag = para_dict.get("binary_flag", False)
        ct = para_dict.get("compute_template")
        fmap_format_in_gm = para_dict.get("fmap_format_in_gm")

        self.shape_list = {}
        self.fmap, self.grads, self.weight_shape = input_x, out_backprop, list(filter_sizes)
        self.fmap_dtype, self.grads_dtype, self.res_dtype = input_x.dtype, out_backprop.dtype, self.res_dtype
        self.op_tag = "conv3d_backprop_filter"
        self.mmad_type = "f162f32" if self.res_dtype != "float16" else "f162f16"

        # 6hd shape
        # N, DO, GRADS_C1, HO, WO, GRADS_C0
        self.shape_grads_6hd = cube_util.shape_to_list(self.grads.shape)
        # N, D, C1, H, W, C0
        self.shape_x_6hd = cube_util.shape_to_list(self.fmap.shape)

        self.shape_list['grads_6hd'] = self.shape_grads_6hd
        self.shape_list['fmap_6hd'] = self.shape_x_6hd

        # flag of special case
        self.var_map = self._get_var_map()
        self.dw_ddr = []
        self.res_tensor = self.dw_ddr  # return tensor of this file to topi
        self.flag_all_one_case = self._get_load2d_flag()
        self.flag_load3d_special_case = self._get_load3d_special_flag()

        tiling_grads_shape = self.shape_grads_6hd[:]
        tiling_grads_shape[2] = self.group_dict['cout_g'] // tiling_grads_shape[-1]
        tiling_fmap_shape = self.shape_x_6hd[:]
        tiling_fmap_shape[2] = self.group_dict['cin1_g']

        # for dynamic
        self.dynamic_mode = self._get_dynamic_mode()
        DynamicConv3dBpFilterParams.binary_mode = self._get_binary_mode(binary_flag, fmap_format_in_gm)
        DynamicConv3dBpFilterParams.flag_all_one_case = self.flag_all_one_case
        DynamicConv3dBpFilterParams.load_mode = ct.load_mode if ct else 0
        DynamicConv3dBpFilterParams.dynamic_mode = self.dynamic_mode
        DynamicConv3dBpFilterParams.var_map = self.var_map
        DynamicConv3dBpFilterParams.group_dict = self.group_dict
        DynamicConv3dBpFilterParams.support_l0c2out = self.support_l0c2out
        DynamicConv3dBpFilterParams.tiling_info_dict = {
            "op_type": 'conv3d_backprop_filter',
            "a_shape": tiling_grads_shape,
            "b_shape": tiling_fmap_shape,
            "c_shape": [self.group_dict['cout_g'], self.weight_shape[2],
                        self.weight_shape[3], self.weight_shape[4],
                        self.group_dict['cin1_g'] * _BLOCK_SIZE],
            "a_dtype": self.grads.dtype,
            "b_dtype": self.fmap.dtype,
            "c_dtype": self.res_dtype,
            "mad_dtype": self.res_dtype,
            "pad": self.pad,
            "stride": self.stride,
            "strideH_expand": 1,
            "strideW_expand": 1,
            "dilation": [self.dilation[2], self.dilation[3], self.dilation[4]],
            "group": self.group_dict['real_g'],
            "fused_coefficient": [0, 0, 0],
            "bias_flag": False,
            "kernel_name": self.kernel_name,
            "dynamic_shape_flag": True
        }

    @staticmethod
    def _get_var_map():
        var_names = ["batch_n", "dedy_d", "dedy_h", "dedy_w", "fmap_d", "fmap_h", "fmap_w"]
        return {v: get_te_var(v).get_bound() for v in var_names if get_te_var(v)}

    @staticmethod
    def _get_binary_mode(binary_flag, fmap_format_in_gm):
        if binary_flag and fmap_format_in_gm == "NDC1HWC0":
            return cube_util.BinaryMode.NDC1HWC0
        return cube_util.BinaryMode.NON_BINARY

    def deconv_dw_access(self):
        """
        complete compute generation, including input check,
        compute definition and result record

        """
        if not DynamicConv3dBpFilterParams.binary_mode:
            self._deconv_dw_input_check_1()
            self._deconv_dw_input_check_2()
        self._deconv_dw_compute()
        self.res_tensor = self.dw_ddr

    def _get_load2d_flag(self):
        # special supporting for a unique case, there are 2 conditions:
        # (1) height & weight of x/output_backprop/filter are all 1
        # (2) strides is [1,1]
        if (self.stride[1:] == [1, 1] and self.shape_x_6hd[3:5] == [1, 1] and self.shape_grads_6hd[3:5] == [1, 1]
            and self.weight_shape[3:5] == [1, 1]):
            return True
        return False

    def _get_load3d_special_flag(self):
        if not self.var_map and cube_util.is_load3d_constraint() and self.shape_grads_6hd[3] != 1 \
            and self.shape_grads_6hd[4] == 1:
            return True
        return False

    def _get_dynamic_mode(self):
        mode = 0
        if not isinstance(self.fmap.shape[0], (tvm.tir.IntImm, int)):
            mode |= _DYNAMIC_BATCH

        if not isinstance(self.fmap.shape[1], (tvm.tir.IntImm, int)):
            mode |= _DYNAMIC_DEPTH

        if not isinstance(self.fmap.shape[3], (tvm.tir.IntImm, int)):
            mode |= _DYNAMIC_HEIGHT

        if not isinstance(self.fmap.shape[4], (tvm.tir.IntImm, int)):
            mode |= _DYNAMIC_WIDTH
        return mode

    def _deconv_dw_input_check_1(self):
        """
        do input parameters check part1

        """
        # check of data type
        tbe_utils.para_check.check_dtype_rule(self.fmap_dtype, ('float16', 'bfloat16'), "fmap")
        tbe_utils.para_check.check_dtype_rule(self.grads_dtype, ('float16', 'bfloat16'), "grads")

        if not tbe_platform.intrinsic_check_support("Intrinsic_mmad", "f162f32"):
            tbe_utils.para_check.check_dtype_rule(self.res_dtype, ('float16'), "res_dtype_lhisi")
        else:
            tbe_utils.para_check.check_dtype_rule(self.res_dtype, ('float32'), "res_dtype")

        # check shape
        # each element must be positive int
        _check_shape_rule(self.shape_x_6hd, 6, "x")
        _check_shape_rule(self.shape_grads_6hd, 6, "out_backprop")
        _check_shape_rule(self.weight_shape, 5, "filter_sizes")

        _, grads_depth, _, grads_height, grads_width, _ = self.shape_grads_6hd
        _, fmap_depth, _, fmap_height, fmap_width, _ = self.shape_x_6hd
        _, _, kernel_depth, kernel_height, kernel_width = self.weight_shape

        if self.flag_load3d_special_case and (fmap_width + self.pad[4] + self.pad[5]) > _FMAP_W_MAX:
            cube_err.raise_err_one_para('E62006', 'conv3d_backprop_filter',
                    'fmap width > 63 is not support when w_out == 1 and h_out != 1')

        if compute_util.int_ceil_div(self.weight_shape[0], 16) != self.shape_grads_6hd[2]:
            cube_err.raise_err_two_paras('E62504', 'conv3d_backprop_filter',
                    str(self.shape_grads_6hd[2]),
                    str(compute_util.int_ceil_div(self.weight_shape[0], 16)))

        # individual range check
        if self.dynamic_mode & _DYNAMIC_DEPTH == 0:
            _check_variable_range(fmap_depth, 1, 4096, "depth of x")
            _check_variable_range(grads_depth, 1, 4096, "depth of out_backprop")
        if self.dynamic_mode & _DYNAMIC_HEIGHT == 0:
            _check_variable_range(fmap_height, 1, 4096, "height of x")
            _check_variable_range(grads_height, 1, 4096, "height of out_backprop")
        if self.dynamic_mode & _DYNAMIC_WIDTH == 0:
            _check_variable_range(fmap_width, 1, 4096, "width of x")
            _check_variable_range(grads_width, 1, 4096, "width of out_backprop")
        _check_attr_rule(self.stride, 3, [1, 63], "stride")
        if self.dynamic_mode == _DYNAMIC_BATCH:
            _check_attr_rule(self.pad, 6, [0, 255], "pad")
        return True

    def _deconv_dw_input_check_2(self):
        """
        do input parameters check part2

        """
        stride_depth, stride_height, stride_width = self.stride
        pad_front, pad_back, pad_top, pad_bottom, pad_left, pad_right = self.pad
        _, grads_depth, grads_channel_1, grads_height, grads_width, grads_c0 = self.shape_grads_6hd
        _, fmap_depth, fmap_channel_1, fmap_height, fmap_width, fmap_c0 = self.shape_x_6hd
        _, _, kernel_depth, kernel_height, kernel_width = self.weight_shape
        dilationn, dilationc, dilationd, dilationh, dilationw = self.dilation

        def _check_dilation():
            dilation_min = 1
            dilation_max = 255
            if dilationn != 1 or dilationc != 1:

                args_dict = {
                    'errCode': 'E62510'
                }
                raise RuntimeError(args_dict,
                    error_manager_util.get_error_message(args_dict))
            dilation_dhw = [dilationd, dilationh, dilationw]
            for item in dilation_dhw:
                if item < dilation_min or item > dilation_max:
                    cube_err.raise_err_attr_range_invalid("conv3d_backprop_filter",
                        "[{},{}]".format(dilation_min, dilation_max),
                        "dilation_dhw",
                        str(item))

        _check_dilation()

        if self.shape_x_6hd[5] != _BLOCK_SIZE:
            cube_err.raise_err_one_para('E62511',
                                        'conv3d_backprop_filter',
                                        str(self.shape_x_6hd[5]))

        if self.shape_grads_6hd[5] != _BLOCK_SIZE:
            cube_err.raise_err_one_para('E62511',
                                        'conv3d_backprop_filter',
                                        str(self.shape_grads_6hd[5]))

        # batch_size should be same
        if self.shape_x_6hd[0] != self.shape_grads_6hd[0]:
            cube_err.raise_err_two_paras('E62503', 'conv3d_backprop_filter',
                    str(self.shape_grads_6hd[0]), str(self.shape_x_6hd[0]))

        if self.dynamic_mode & _DYNAMIC_DEPTH == 0:
            # coupling range check
            fmap_depth_after_pad = fmap_depth + pad_front + pad_back
            dilation_kernel_depth = (kernel_depth - 1) * dilationd + 1
            computed_grads_depth = (fmap_depth - dilation_kernel_depth +
                                    pad_front + pad_back)//stride_depth + 1
            if computed_grads_depth != grads_depth:
                cube_err.raise_err_input_params_not_expected(
                    "conv3d_backprop_filter", "grads_depth",
                    str(grads_depth), str(computed_grads_depth))

            if dilation_kernel_depth > fmap_depth_after_pad:
                cube_err.raise_err_specific("conv3d_backprop_filter",
                    "depth of filter cannot exceed that of x.")

            if pad_front >= dilation_kernel_depth or pad_back >= dilation_kernel_depth:
                cube_err.raise_err_specific("conv3d",
                    "pad in front/back should less than depth of filter.")

        if self.dynamic_mode & _DYNAMIC_HEIGHT == 0:
            fmap_height_after_pad = fmap_height + pad_top + pad_bottom
            dilation_kernel_height = (kernel_height - 1) * dilationh + 1
            computed_grads_height = (fmap_height - dilation_kernel_height +
                                     pad_top + pad_bottom)//stride_height + 1
            if computed_grads_height != grads_height:
                cube_err.raise_err_input_params_not_expected(
                    "conv3d_backprop_filter", "grads_height",
                    str(grads_height), str(computed_grads_height))

            if dilation_kernel_height > fmap_height_after_pad:
                cube_err.raise_err_specific("conv3d_backprop_filter",
                    "height of filter cannot exceed that of x.")

            if (pad_top >= dilation_kernel_height or pad_bottom >= dilation_kernel_height):
                cube_err.raise_err_specific("conv3d_backprop_filter",
                    "pad in up/down should less than height of filter.")

        if self.dynamic_mode & _DYNAMIC_WIDTH == 0:
            fmap_width_after_pad = fmap_width + pad_left + pad_right
            dilation_kernel_width = (kernel_width - 1) * dilationw + 1
            computed_grads_width = (fmap_width - dilation_kernel_width +
                                    pad_left + pad_right)//stride_width + 1
            if computed_grads_width != grads_width:
                cube_err.raise_err_input_params_not_expected(
                    "conv3d_backprop_filter", "grads_width",
                    str(grads_width), str(computed_grads_width))

            if dilation_kernel_width > fmap_width_after_pad:
                cube_err.raise_err_specific("conv3d_backprop_filter",
                    "width of filter cannot exceed that of x.")

            if (pad_left >= dilation_kernel_width or pad_right >= dilation_kernel_width):
                cube_err.raise_err_specific("conv3d_backprop_filter",
                    "pad in left/right should less than width of filter.")

        if not self.dynamic_mode:
            _check_addressing_rule(self.shape_grads_6hd, 2, _DATA_SIZE_LIMIT_INT64)
            _check_addressing_rule(self.shape_x_6hd, 2, _DATA_SIZE_LIMIT_INT64)

        # int64 addressing limit of tvm
        kernel_fractal = (kernel_depth * fmap_channel_1 * kernel_height *
                          kernel_width, grads_channel_1 * grads_c0, fmap_c0)
        _check_addressing_rule(kernel_fractal, 4, _DATA_SIZE_LIMIT_INT64)
        return True

    def _deconv_dw_compute(self):
        """
        complete compute definition

        """
        fmap_dtype = self.fmap_dtype

        (batch_size, grads_depth, grads_channel_1, grads_height, grads_width,
         grads_c0) = self.shape_grads_6hd
        (_, fmap_depth, fmap_channel_1, fmap_height, fmap_width,
         fmap_c0) = self.shape_x_6hd
        _, _, kernel_depth, kernel_height, kernel_width = self.weight_shape
        real_g = self.group_dict['real_g']
        fmap_channel1_g = self.group_dict['cin1_g']
        grads_channel_g = self.group_dict['cout_g']

        if self.flag_load3d_special_case:
            dilation_w = self.dilation[4]
            self.stride[2] = fmap_width + self.pad[4] + self.pad[5]
            self.pad[5] += (kernel_width - 1) * dilation_w + 1
            # add a blank line
            grads_width = grads_width * 2
            self.shape_grads_6hd[-2] = grads_width

        # align to 16
        hw_ori = grads_height * grads_width
        hw_mad_1 = (hw_ori + _BLOCK_SIZE - 1) // _BLOCK_SIZE

        # move grads to L1
        grads_shape_matrix = (batch_size * grads_depth, grads_channel_1,
                              hw_ori, grads_c0)
        self.shape_list['grads_matrix'] = grads_shape_matrix

        grads_matrix = self._grads_2_matrix(grads_shape_matrix, self.grads)

        # move grads_matrix to L0A and do transpose
        grads_shape_fractal = (real_g, batch_size * grads_depth,
                               grads_channel_g // grads_c0,
                               hw_mad_1, grads_c0, _BLOCK_SIZE)

        self.shape_list['grads_fractal'] = grads_shape_fractal
        grads_fractal = self._grads_2_fractal(grads_shape_fractal, grads_matrix)

        if not self.flag_all_one_case:
            # shape of fmap_original_matrix, corresponding to set_fmatrix
            fmap_shape_original_matrix = (real_g, batch_size * grads_depth, hw_ori, kernel_depth * fmap_channel1_g,
                                          kernel_height, kernel_width, fmap_c0)
            self.shape_list['fmap_original_matrix'] = fmap_shape_original_matrix
            fmap_shape_fmap_matrix = (real_g, batch_size * grads_depth,
                                      hw_mad_1, kernel_depth * fmap_channel1_g * kernel_height * kernel_width,
                                      fmap_c0, _BLOCK_SIZE)
            self.shape_list['fmap_fmap_matrix'] = fmap_shape_fmap_matrix
            if not DynamicConv3dBpFilterParams.binary_mode:
                fmap_l1 = self._out_to_bl1()

                fmap_matrix = self._fmap_2_matrix(fmap_shape_original_matrix, fmap_l1, fmap_dtype)
                # move fmap to L0B
                fmap_fractal = self._fmap_2_fractal(fmap_shape_fmap_matrix, fmap_matrix, fmap_dtype)
            else:
                fmap_l1 = self._fmap_to_l1_process()
                fmap_shape_fmap_matrix = (real_g, batch_size * grads_depth,
                                          hw_mad_1, kernel_depth * fmap_channel1_g * kernel_height * kernel_width,
                                          fmap_c0, _BLOCK_SIZE)
                fmap_fractal = self._im2col_fractal_v2(fmap_shape_fmap_matrix, fmap_l1)


        # else: all_one_case, using load_2d instead of load_3d
        else:
            # shape of fmap_matrix
            fmap_shape_matrix = (batch_size * grads_depth,
                                 kernel_depth * fmap_channel_1,
                                 fmap_height * fmap_width, fmap_c0)

            self.shape_list['fmap_matrix'] = fmap_shape_matrix

            fmap_matrix = self._fmap_2_matrix_load2d(fmap_shape_matrix, self.fmap)
            # move fmap to L0B
            fmap_shape_fractal = (real_g, batch_size * grads_depth, hw_mad_1,
                                  kernel_depth * fmap_channel1_g *
                                  kernel_height * kernel_width, fmap_c0,
                                  _BLOCK_SIZE)
            self.shape_list['fmap_fractal'] = fmap_shape_fractal

            fmap_fractal = self._fmap_2_fractal_load2d(fmap_shape_fractal, fmap_matrix)
        # shape of result dw [n1,m,n0]
        dw_shape = (real_g, kernel_depth * fmap_channel1_g * kernel_height *
                    kernel_width, grads_channel_g, fmap_c0)
        self.shape_list['dw'] = dw_shape

        # do mmad
        self.dw_ddr = self._mad(dw_shape, grads_fractal, fmap_fractal)
        DynamicConv3dBpFilterParams.attr_dict = {
            'pad': self.pad,
            'stride': self.stride,
            'dilation': self.dilation,
            'kernel_size': self.weight_shape,
            'load2d_flag': self.flag_all_one_case,
            'group_dict': self.group_dict,
            'flag_load3d_special_case': self.flag_load3d_special_case,
            'mode': self.mmad_type,
            'kernel_name': self.kernel_name
        }

        return 1

    def _out_to_bl1(self):
        (batch_size, grads_depth, *_) = self.shape_grads_6hd
        (_, fmap_depth, fmap_channel_1, fmap_height, fmap_width, fmap_c0) = self.shape_x_6hd
        _, _, kernel_depth, *_ = self.weight_shape
        stride_depth, *_ = self.stride
        pad_front, *_ = self.pad
        real_g = self.group_dict['real_g']
        fmap_channel1_g = self.group_dict['cin1_g']
        dilation_d = self.dilation[2]
        kernel_d_dilation = (kernel_depth - 1) * dilation_d + 1
        fmap_al1_shape = (batch_size * grads_depth, real_g * kernel_depth * fmap_channel1_g, fmap_height,
                          fmap_width, fmap_c0)
        fmap_l1 = tvm.compute(
            fmap_al1_shape,
            lambda n_dout, g_dk_cing, h, w, al1_c0: tvm.select(
                tvm.all(((n_dout % grads_depth) * stride_depth
                        + (g_dk_cing // fmap_channel1_g * dilation_d) % kernel_d_dilation)
                        >= pad_front,
                        ((n_dout % grads_depth) * stride_depth
                        + (g_dk_cing // fmap_channel1_g * dilation_d) % kernel_d_dilation)
                        < pad_front + fmap_depth,
                        g_dk_cing // (kernel_depth * fmap_channel1_g)
                        * fmap_channel1_g + g_dk_cing % fmap_channel1_g < fmap_channel_1),
                self.fmap(n_dout // grads_depth,
                            (n_dout % grads_depth * stride_depth
                            + (g_dk_cing // fmap_channel1_g * dilation_d) % kernel_d_dilation - pad_front),
                            g_dk_cing // (kernel_depth * fmap_channel1_g)
                            * fmap_channel1_g + g_dk_cing % fmap_channel1_g,
                            h, w, al1_c0)),
            name='fmap_l1',
            tag='fmap_l1')
        return fmap_l1

    def _fmap_to_l1_process(self):
        def __fmap_2_matrix_compute(indices):
            g_index, n_index, dc_index, h_index, w_index, c0_index = indices
            batch_index = n_index // grads_depth
            d_index = n_index % grads_depth * stride_depth + (dc_index // fmap_channel1_g * dilation_d) % \
                kernel_d_dilation - pad_front
            c1_index = g_index * fmap_channel1_g + dc_index % fmap_channel1_g
            c1_index_valid = c1_index < fmap_channel_1
            fmap_l1_tensor_condition = tvm.all(d_index >= 0, d_index < fmap_depth, c1_index_valid)
            return tvm.select(fmap_l1_tensor_condition,
                              self.fmap(batch_index, d_index, c1_index, h_index, w_index, c0_index))

        (batch_size, grads_depth, *_) = self.shape_grads_6hd
        (_, fmap_depth, fmap_channel_1, fmap_height, fmap_width, fmap_c0) = self.shape_x_6hd
        _, _, kernel_depth, *_ = self.weight_shape
        stride_depth, *_ = self.stride
        pad_front, *_ = self.pad
        dilation_d = self.dilation[2]
        kernel_d_dilation = (kernel_depth - 1) * dilation_d + 1
        real_g = self.group_dict['real_g']
        fmap_channel1_g = self.group_dict['cin1_g']
        fmap_bl1_shape = (real_g, batch_size * grads_depth, kernel_depth * fmap_channel1_g,
                          fmap_height, fmap_width, fmap_c0)
        fmap_l1 = tvm.compute(fmap_bl1_shape, lambda *idx: __fmap_2_matrix_compute(idx), name='fmap_l1', tag='fmap_l1')
        return fmap_l1

    def _im2col_fractal_v2(self, fmap_shape_fractal, fmap_l1):

        def __im2col_idx(idx):
            group_idx, batch, col_h, dk_c1_hk_wk, fmap_c0_indices, hw_mad_0_indices = idx

            dk_idx = dk_c1_hk_wk // kernel_w // kernel_h // fmap_channel1_g
            cin1_g_idx = dk_c1_hk_wk // kernel_w // kernel_h % fmap_channel1_g
            dk_c1g = dk_idx * fmap_channel1_g + cin1_g_idx

            virtual_h = col_h * _BLOCK_SIZE + fmap_c0_indices

            back_h = (virtual_h // grads_w) * stride_h + (dk_c1_hk_wk // kernel_w % kernel_h) * dilation_h
            back_w = (virtual_h % grads_w) * stride_w + (dk_c1_hk_wk % kernel_w) * dilation_w

            return tvm.select(
                tvm.any(back_h < self.pad[2], back_h > fmap_h + self.pad[2] - 1, back_w < self.pad[4],
                        back_w > fmap_w + self.pad[4] - 1,
                        group_idx * fmap_channel1_g + cin1_g_idx > fmap_channel_1 - 1),
                tvm.const(0, self.fmap_dtype),
                fmap_l1(group_idx, batch, dk_c1g, back_h - self.pad[2], back_w - self.pad[4], hw_mad_0_indices))

        _, _, _, kernel_h, kernel_w = self.weight_shape
        grads_w = self.shape_grads_6hd[4]
        (_, _, fmap_channel_1, fmap_h, fmap_w, _) = self.shape_x_6hd
        _, _, _, dilation_h, dilation_w = self.dilation
        _, stride_h, stride_w = self.stride
        fmap_channel1_g = self.group_dict['cin1_g']

        return tvm.compute(fmap_shape_fractal,
                           lambda *idx: __im2col_idx(idx),
                           name='fmap_2_col_fractal',
                           tag='fmap_2_col_fractal')

    def _grads_2_matrix(self, grads_shape_matrix, grads):
        """
        compute definiton of loading grads to L1

        Parameters:
        ----------
        grads_shape_matrix : shape of result tensor in L1

        grads : input tensor in ddr

        Returns
        -------
        None
        """
        def __grads_2_matrix_compute(indices, grads):
            """
            do coordinate calculation
            """
            grads_depth = self.shape_list.get('grads_6hd')[1]
            grads_width = self.shape_list.get('grads_6hd')[4]

            batch_indices, grads_c1_indices, hw_indices, grads_c0_indices \
                = indices

            # calculate index of grads according to indice of grads_matrix
            batch_size_index = batch_indices // grads_depth
            grads_depth_index = batch_indices % grads_depth
            grads_channel_1_index = grads_c1_indices
            grads_height_index = (hw_indices // grads_width)
            grads_width_index = (hw_indices % grads_width)
            grads_c0_index = grads_c0_indices

            if self.flag_load3d_special_case:
                grads_width_index = hw_indices % (grads_width // 2)
            return grads(batch_size_index, grads_depth_index,
                         grads_channel_1_index, grads_height_index,
                         grads_width_index, grads_c0_index)

        return tvm.compute(
            grads_shape_matrix,
            lambda n_dout, grads_c1, hw_ori, grads_c0:
                __grads_2_matrix_compute((n_dout,
                                          grads_c1,
                                          hw_ori,
                                          grads_c0), grads),
            name='grads_2_matrix',
            tag='grads_2_matrix')

    def _fmap_2_matrix(self, fmap_shape_original_matrix, fmap, fmap_dtype):
        """
        compute definiton of set_fmatrix

        Parameters:
        ----------
        fmap_shape_original_matrix : shape of result tensor in L1
        in shape (batch_size*grads_depth,
                  grads_height*grads_width,
                  kernel_depth*fmap_channel_1,
                  kernel_height,
                  kernel_width,
                  fmap_c0)

        fmap : input tensor in L1

        fmap_dtype : data type of fmap
        in shape (batch_size, fmap_depth, fmap_channel_1,
                  fmap_height, fmap_width, C0)

        Returns
        -------
        None
        """
        def __fmap_2_matrix_compute(indices,
                                    fmap,
                                    pad_left=0,
                                    pad_right=0,
                                    pad_top=0,
                                    strideh=1,
                                    stridew=1,
                                    dilationh=1,
                                    dilationw=1):
            """
            do coordinate calculation

            """
            _, _, kernel_depth, _, kernel_width = self.weight_shape
            _, _, _, fmap_height, fmap_width, _ = self.shape_list.get('fmap_6hd')

            (g_indices, batch_indices, hw_fuse_indices, fmap_c1_indices, kernel_height_indices,
             kernel_width_indices, fmap_c0_indices) = indices

            dilation_kernel_width = kernel_width + (kernel_width - 1) * (dilationw - 1)
            fmap_width_after_pad = fmap_width + pad_left + pad_right
            width_out = (fmap_width_after_pad - dilation_kernel_width) // stridew + 1

            n_index = batch_indices
            depth_index = fmap_c1_indices // self.group_dict['cin1_g']
            cin_index = fmap_c1_indices % self.group_dict['cin1_g']
            c1_index = (g_indices * self.group_dict['cin1_g'] * kernel_depth +
                        depth_index * self.group_dict['cin1_g'] + cin_index)
            h_index = (hw_fuse_indices // width_out) * strideh + kernel_height_indices * dilationh
            w_index = (hw_fuse_indices % width_out) * stridew + kernel_width_indices * dilationw
            c0_index = fmap_c0_indices
            # if index belongs to padding and 16 align, select 0
            return tvm.select(tvm.any(h_index < pad_top,
                                      h_index > fmap_height + pad_top - 1,
                                      w_index < pad_left,
                                      w_index > fmap_width + pad_left - 1),
                              tvm.const(0.0, fmap_dtype),
                              fmap(n_index, c1_index, h_index - pad_top,
                                   w_index - pad_left, c0_index))

        _, _, pad_top, _, pad_left, pad_right = self.pad
        _, strideh, stridew = self.stride
        _, _, _, dilationh, dilationw = self.dilation

        return tvm.compute(
            fmap_shape_original_matrix,
            lambda g, n_dout, hw_ori, dk_cing, hk, wk, fmap_c0:
                __fmap_2_matrix_compute((g, n_dout, hw_ori,
                                         dk_cing, hk, wk, fmap_c0),
                                        fmap,
                                        pad_left=pad_left,
                                        pad_right=pad_right,
                                        pad_top=pad_top,
                                        strideh=strideh,
                                        stridew=stridew,
                                        dilationh=dilationh,
                                        dilationw=dilationw),
            name='fmap_2_col_matrix',
            tag='fmap_2_col_matrix')

    def _fmap_2_matrix_load2d(self, fmap_shape_matrix, fmap):
        """
        compute definiton of loading fmap to L1

        Parameters:
        ----------
        fmap_shape_matrix : shape of result tensor in L1

        fmap : input tensor in ddr

        Returns
        -------
        None
        """
        def __fmap_2_matrix_load2d_compute(indices, fmap):
            """
            do coordinate calculation

            """
            pad_front = self.pad[0]
            fmap_depth = self.shape_list.get('fmap_6hd')[1]
            fmap_width = self.shape_list.get('fmap_6hd')[4]
            fmap_channel_1 = self.shape_list.get('fmap_6hd')[2]
            grads_depth = self.shape_list.get('grads_6hd')[1]
            stride_depth = self.stride[0]
            batch_indices, fmap_c1_indices, hw_mad_indices, fmap_c0_indices = indices
            fmap_depth_index = batch_indices % grads_depth * stride_depth + fmap_c1_indices // fmap_channel_1
            return tvm.select(
                tvm.all(fmap_depth_index >= pad_front,
                        fmap_depth_index < pad_front + fmap_depth),
                fmap(batch_indices // grads_depth, fmap_depth_index - pad_front,
                     fmap_c1_indices % fmap_channel_1, (hw_mad_indices // fmap_width),
                     (hw_mad_indices % fmap_width), fmap_c0_indices))

        return tvm.compute(
            fmap_shape_matrix,
            lambda n_dout, dk_fmap_c1, hk_wk, fmap_c0:
                __fmap_2_matrix_load2d_compute((n_dout, dk_fmap_c1, hk_wk, fmap_c0), fmap),
            name='fmap_2_matrix',
            tag='fmap_2_matrix')

    def _fmap_2_fractal(self, fmap_shape_fmap_matrix, fmap_2_col_matrix,
                       fmap_dtype):
        """
        compute definiton of loading fmap to L0B

        Parameters:
        ----------
        fmap_shape_fmap_matrix : shape of result tensor in L0B
        in shape (batch_size*grads_depth,
                  hw_mad//block_size_K,
                  kernel_depth*fmap_channel_1*kernel_height*kernel_width,
                  fmap_c0,
                  block_size_K)

        fmap_2_col_matrix : input tensor in L1
        in shape (batch_size*grads_depth,
                  grads_height*grads_width,
                  kernel_depth*fmap_channel_1,
                  kernel_height,
                  kernel_width,
                  fmap_c0)

        fmap_dtype : data type of fmap_2_col_matrix


        Returns
        -------
        None
        """
        def __fmap_2_fractal_compute(indices, fmap_2_col_matrix):
            """
            do coordinate calculation

            """
            fmap_channel1_g = self.group_dict['cin1_g']
            fmap_channel_1 = self.shape_x_6hd[2]
            _, _, hw_fuse, _, kernel_height, kernel_width, _ = self.shape_list.get('fmap_original_matrix')

            group_index, n_vm_index, hw_mad_1_indices, fkk_indices, \
                fmap_c0_indices, hw_mad_0_indices = indices

            hw_vm_index = hw_mad_1_indices * _BLOCK_SIZE + hw_mad_0_indices
            c1_vm_index = ((
                (fkk_indices * _BLOCK_SIZE + fmap_c0_indices) // _BLOCK_SIZE) //
                kernel_width) // kernel_height
            cin1_g_idx = c1_vm_index % fmap_channel1_g
            kh_vm_index = ((
                (fkk_indices * _BLOCK_SIZE + fmap_c0_indices) // _BLOCK_SIZE) //
                kernel_width) % kernel_height
            kw_vm_index = ((fkk_indices * _BLOCK_SIZE + fmap_c0_indices) //
                           _BLOCK_SIZE) % kernel_width
            c0_vm_index = (fkk_indices * _BLOCK_SIZE + fmap_c0_indices) % _BLOCK_SIZE

            # select padding and 16 align
            return tvm.select(
                tvm.any(hw_vm_index < 0, hw_vm_index > hw_fuse - 1,
                        group_index * fmap_channel1_g + cin1_g_idx > fmap_channel_1 - 1),
                tvm.const(0.0, fmap_dtype),
                fmap_2_col_matrix(group_index, n_vm_index, hw_vm_index, c1_vm_index,
                                  kh_vm_index, kw_vm_index, c0_vm_index))

        return tvm.compute(fmap_shape_fmap_matrix,
                           lambda g, n_dout, hw_mad_1, dk_cing_hk_wk, fmap_c0, unit_c0:
                                __fmap_2_fractal_compute((g, n_dout,
                                                          hw_mad_1,
                                                          dk_cing_hk_wk,
                                                          fmap_c0, unit_c0), fmap_2_col_matrix),
                           name='fmap_2_col_fractal',
                           tag='fmap_2_col_fractal')

    def _fmap_2_fractal_load2d(self, fmap_shape_fractal, fmap_2_matrix):
        """
        compute definiton of loading fmap_matrix to L0B

        Parameters:
        ----------
        fmap_shape_fractal : shape of result tensor in L0B

        fmap_2_matrix : input tensor in L1

        Returns
        -------
        None
        """
        def __fmap_2_fractal_load2d_compute(indices, fmap_2_matrix):
            """
            do coordinate calculation
            """
            (group_indices, batch_indices, hw_mad_1_indices, fmap_c1_indices,
             fmap_c0_indices, hw_mad_0_indices) = indices

            batch_size_index = batch_indices
            depth_index = fmap_c1_indices // self.group_dict['cin1_g']
            cin_index = fmap_c1_indices % self.group_dict['cin1_g']
            c1_index = (depth_index * self.shape_x_6hd[2] + group_indices *
                        self.group_dict['cin1_g'] + cin_index)
            fmap_hw_index = hw_mad_1_indices * _BLOCK_SIZE + hw_mad_0_indices
            fmap_c0_index = fmap_c0_indices

            return fmap_2_matrix(batch_size_index, c1_index, fmap_hw_index, fmap_c0_index)

        return tvm.compute(
            fmap_shape_fractal,
            lambda g, n_dout, hw_mad_1, dk_cing_hk_wk, fmap_c0, unit_size:
                __fmap_2_fractal_load2d_compute((g, n_dout, hw_mad_1, dk_cing_hk_wk, fmap_c0, unit_size),
                                                fmap_2_matrix),
            name='famp_2_fractal',
            tag='famp_2_fractal')

    def _grads_2_fractal(self, grads_shape_fractal, grads_2_matrix):
        """
        compute definiton of loading grads_matrix to L0A

        Parameters:
        ----------
        grads_shape_fractal : shape of result tensor in L0A

        grads_2_matrix : input tensor in L1

        Returns
        -------
        None
        """
        def __grads_2_fractal_compute(indices, grads_2_matrix):
            """
            do coordinate calculation

            """
            group_dict = self.group_dict
            grads_c1 = self.shape_grads_6hd[2]
            (group_indices, batch_indices, grads_c1_indices,
                hw_mad_1_indices, grads_c0_indices, hw_mad_0_indices) = indices

            batch_size_index = batch_indices
            grads_channel_1_index = (
                group_indices *
                (group_dict['cout_g'] // 16) + grads_c1_indices)
            grads_hw_index = hw_mad_1_indices * _BLOCK_SIZE + hw_mad_0_indices
            grads_c0_index = grads_c0_indices

            return tvm.select(tvm.all(grads_channel_1_index < grads_c1),
                grads_2_matrix(batch_size_index, grads_channel_1_index, grads_hw_index, grads_c0_index),
                tvm.const(0, self.grads_dtype))

        return tvm.compute(grads_shape_fractal,
                            lambda g, n_dout, grads_c1, hw_mad_1, grads_c0, hw_mad_0:
                            __grads_2_fractal_compute((g,
                                                        n_dout,
                                                        grads_c1,
                                                        hw_mad_1,
                                                        grads_c0,
                                                        hw_mad_0),
                                                        grads_2_matrix),
                            name='grads_2_fractal',
                            tag='grads_2_fractal')

    def _mad(self, mad_shape, grads, fmap):
        """
        calculate mad result tensor
        Parameters
        ----------
        mad_shape : result shape
        (real_g, kernel_depth*fmap_channel_1*kernel_height*kernel_width,
         grads_channel, fmap_c0)

        grads : tensor in L0A
        grads_shape_fractal = (real_g, batch_size*grads_depth,
                               grads_channel_1,
                               hw_mad//block_size_K,
                               grads_c0,
                               block_size_K)

        fmap : tensor in L0B
        fmap_shape_fmap_matrix = (real_g, batch_size*grads_depth,
                                  hw_mad//block_size_K,
                                  kernel_depth*fmap_channel_1* kernel_height*kernel_width,
                                  fmap_c0,
                                  block_size_K)

        Returns
        -------
        None
        """

        batch_size, grads_depth, _, grads_height, grads_width, _ = self.shape_list.get('grads_6hd')

        batch_axis = tvm.reduce_axis((0, batch_size * grads_depth), name='axis_b')
        k_axis = tvm.reduce_axis((0, grads_height * grads_width), name='axis_k')

        k_0 = k_axis.var % 16

        c_col = tvm.compute(
            mad_shape,
            lambda g, fkk, grads_c, fmap_c0: tvm.sum(
                (grads[g, batch_axis, grads_c // 16, k_axis.var // 16, grads_c % 16, k_0] *
                 fmap[g, batch_axis, k_axis.var // 16, fkk, fmap_c0, k_0]).astype(self.res_dtype),
                axis=[batch_axis, k_axis]),
            name='dw_ddr',
            tag=self.op_tag + "dw_ddr")
        return c_col



@tbe_utils.para_check.check_input_type(tvm.Tensor,
                             tvm.Tensor,
                             (list, tuple),
                             dict)
def conv3d_dw(x,
              out_backprop,
              filter_size,
              para_dict):
    """
    DSL interface of conv3d bp dx

    Parameters
    ----------
    x : the featuremap data, tvm.placeholder, 6hd shape

    out_backprop : the grads data, tvm.placeholder, 6hd shape

    filter_size : 5-D shape, specifies the filter sizes

    para_dict : dict of parameters
    strides : 3-D shape, specifies in depth, height and width dimension
    pads : 6-D shape, specifies in up/down/left/right dimension
    dilations : 5-D shape, specifies in batch/channel/depth/height/width dimension
    res_dtype : the output data type
    kernel_name : conv3d_backprop_filter_cce by default
    group_dict : group of parameters

    Returns
    -------
    result tensor of conv3d_backprop_filter compute
    """
    deconv_dw_object = Conv3dBackpropFilter(x, out_backprop, filter_size, para_dict)
    deconv_dw_object.deconv_dw_access()

    return deconv_dw_object.res_tensor


class DynamicConv3dBpFilterParams:
    """
    the Conv3dBpFilterParams of dynamic
    """
    support_l0c2out = False
    load_mode = 0
    binary_mode = 0
    dynamic_mode = None
    attr_dict = {}
    tensor_map = {}
    tiling_info_dict = {}
    var_map = {}
    flag_all_one_case = None
    group_dict = {}
