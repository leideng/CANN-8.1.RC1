#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2033 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gemm_compute
"""
import json
import functools
from functools import reduce as functools_reduce
import tbe.common.platform as tbe_platform
import tbe.common.utils as tbe_utils
from tbe import tvm
from tbe.common.context import op_context
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils import para_check
from tbe.common.utils import shape_util
from tbe.common.utils import broadcast_shapes
from tbe.common.utils.const import ComputeFlow
from tbe.common.utils.errormgr import error_manager_cube
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.compute import cube_util
from tbe.dsl.compute.gemm_compute_util import FormatCompute
from tbe.dsl.compute.gemm_compute_util import ShapeAInfo
from tbe.dsl.compute.gemm_compute_util import ShapeBInfo
from tbe.dsl.compute.util import align
from tbe.dsl.compute.util import int_ceil_div
from tbe.dsl.compute.gemm_compute_util import GEMMComputeParam

OP_TAG = "weight_quant_bmm_"


class WQBmmCompute(object):
    def __init__(self, tensor_a, tensor_b, tensor_diag, para_dict):
        self.input_a = tensor_a
        self.input_b = tensor_b
        self.input_diag = tensor_diag
        self.input_q_bias = para_dict.get("tensor_q_bias")
        self.input_deq_scale = para_dict.get("deq_scale")
        self.input_bias = para_dict.get("bias")
        self.trans_a = para_dict.get("trans_a")
        self.trans_b = para_dict.get("trans_b")
        self.op_type = para_dict.get("op_type")
        self.op_tag = para_dict.get("op_tag")
        self.format_a = self.format_b = "ND"
        self.ori_format_a = self.ori_format_b = "ND"
        self.ori_shape_a = self.ori_shape_b = None
        self.batch_a = self.batch_b = None
        # tiling KL0, use 4 in stastic
        self.k_l0 = 4 if not in_dynamic() else get_te_var("k_l0").get_tvm_var()
        self.format_out = para_dict.get("format_out")
        self.shape_out = para_dict.get("shape_out")
        self.dtype_out = para_dict.get("dtype_out")
        self.kernel_name = para_dict.get("kernel_name")
        self.deq_scale_vector_flag = True
        self.deq_scale_value = 1

    def compute(self):
        self._pre_process()
        tensor_weight_fp16 = self._compute_weight_int82fp16()
        tensor_out = self._compute_gemm_fp16(tensor_weight_fp16)
        return tensor_out

    def get_batch_index(self, idx_reduce, list_batch_broad, list_batch_ori):
        """
        get index in batch_ori
        e.g. A x B = C
            batch_A = [B1, 1, 1, B4]
            batch_B = [B2, B3, 1]
            batch_C = [B1, B2, B3, B4]

            idx_reduce is in range of 0 ~ B1*B2*B3*B4-1,
            b1 = idx_reduce // (B2*B3*B4)
            b2 = idx_reduce // (B3*B4) % B2
            b3 = idx_reduce // (B4) % B3
            b4 = idx_reduce // (1) % B4

            idx_a = B4*b1 + b4,
            idx_b = B3*b2 + b3
        """
        if list_batch_broad == list_batch_ori:
            return idx_reduce
        idx_ori = 0
        for idx, (val_broad, val_ori) in enumerate(zip(list_batch_broad, list_batch_ori)):
            # [val_ori != 1] means current index has offset val
            if val_ori != 1:
                # Using "list + [1]" in case of reduce blank list
                rest_length_broad = functools_reduce(lambda x, y: x * y, list_batch_broad[idx + 1:] + [1])
                rest_length_ori = functools_reduce(lambda x, y: x * y, list_batch_ori[idx + 1:] + [1])
                current_offset = idx_reduce // rest_length_broad
                if idx != 0:
                    current_offset = current_offset % val_ori
                idx_ori += current_offset * rest_length_ori * (val_ori // val_broad)
        return idx_ori

    def _update_input_info(self, input_tensor):
        ori_format = input_tensor.op.attrs["ori_format"]
        ori_shape = input_tensor.op.attrs["ori_shape"]
        batch_shape = ori_shape[:-4] if ori_format == "FRACTAL_NZ" else ori_shape[:-2]
        cur_format = input_tensor.op.attrs["format"]
        if cur_format != "FRACTAL_NZ":
            cur_format = "ND"
            ori_nd_shape = shape_util.shape_to_list(input_tensor.shape)
        else:
            nz_shape = shape_util.shape_to_list(input_tensor.shape)
            ori_nd_shape = [nz_shape[0],
                nz_shape[-3] * nz_shape[-2], nz_shape[-1] * nz_shape[-4]]

        return (cur_format, ori_format, ori_nd_shape, batch_shape)

    def _get_dynamic_out_shape(self):
        shape_out = []
        "Output shape from var"
        if self.format_out == "ND" and not all(len(x.shape) in (2, )
                                               for x in (self.input_a, self.input_b)) or get_te_var("batch"):
            shape_out = [get_te_var("batch").get_tvm_var()] if get_te_var("batch") else []
            shape_out.extend((get_te_var("m_ori").get_tvm_var(),
                                get_te_var("n_ori").get_tvm_var()))
        return shape_out

    def _pre_process(self):
        (
            self.format_a, self.ori_format_a,
            self.ori_shape_a, self.batch_a
        ) = self._update_input_info(self.input_a)
        (
            self.format_b, self.ori_format_b,
            self.ori_shape_b, self.batch_b
        ) = self._update_input_info(self.input_b)
        deq_scale_shape = shape_util.shape_to_list(self.input_deq_scale.op.attrs["ori_shape"])
        total_dim = functools.reduce(lambda x, y: x * y, deq_scale_shape[:])
        if total_dim == 1:
            self.deq_scale_vector_flag = False
            self.deq_scale_value = self.input_deq_scale(0, 0, 0, 0, 0)

        if in_dynamic() and self.op_tag == OP_TAG:
            self.shape_out = self._get_dynamic_out_shape()


    def _compute_weight_int82fp16(self):
        tensor_a_zz_int8 = self._compute_tensor_a_int8(self.input_diag)
        tensor_b_zn_int8 = self._compute_tensor_b_int8(self.input_b)
        tensor_bias_int32 = self._compute_tensor_bias_int32(self.input_q_bias)
        tensor_mad_int32 = self._compute_tensor_mad_int32(tensor_a_zz_int8,
                                                          tensor_b_zn_int8,
                                                          tensor_bias_int32)
        tensor_weight_fp16 = self._compute_tensor_weight_fp16(tensor_mad_int32,
                                                              self.input_deq_scale)
        return tensor_weight_fp16


    def _compute_gemm_fp16(self, tensor_weight_fp16):
        tensor_a_zz_fp16 = self._compute_tensor_a_fp16(self.input_a)
        tensor_b_zn_fp16 = self._compute_tensor_b_fp16(tensor_weight_fp16)
        tensor_bias_fp32 = self._compute_tensor_bias_fp32(self.input_bias)
        tensor_mad_fp32 = self._compute_tensor_mad_fp32(tensor_a_zz_fp16,
                                                        tensor_b_zn_fp16,
                                                        tensor_bias_fp32)
        tensor_out = self._compute_tensor_out(tensor_mad_fp32)
        return tensor_out


    def _compute_tensor_out(self, tensor_mad_fp32):
        op_dict = {"post_transform": "NZ2ND"}
        if self.dtype_out == "float16":
            op_dict["pre_conv"] = "F322F16"
        block_in = 16
        tensor_fixpipe_fp16 = tvm.compute(
            tensor_mad_fp32.shape,
            lambda *indices:
            tvm.fixpipe_op(tensor_mad_fp32(*indices),
                self.dtype_out, op_dict=op_dict),
            name="tensor_fixpipe_fp16",
            tag="weight_quant_bmm")
        self.shape_out[0] = shape_util.shape_to_list(tensor_mad_fp32.shape)[0]
        tensor_out = tvm.compute(
            self.shape_out,
            lambda b, m, n: tensor_fixpipe_fp16(
                b, n // block_in, m // block_in, m % block_in, n % block_in
            ),
            name="tensor_out",
            tag="weight_quant_bmm")
        return tensor_out


    def _get_batch_broadcast(self, tensor_a, tensor_b):
        shape_tensor_a = shape_util.shape_to_list(tensor_a.shape)
        shape_tensor_b = shape_util.shape_to_list(tensor_b.shape)
        list_batch_a = shape_tensor_a[:-4]
        list_batch_b = shape_tensor_b[:-4]
        if "ori_batch_shape" in tensor_a.op.attrs and "ori_batch_shape" in tensor_b.op.attrs:
            list_batch_a = shape_util.shape_to_list(tensor_a.op.attrs["ori_batch_shape"])
            list_batch_b = shape_util.shape_to_list(tensor_b.op.attrs["ori_batch_shape"])
            if get_te_var("batch_c1"):
                list_batch_broad = [get_te_var("batch_c1").get_tvm_var(), get_te_var("batch_c2").get_tvm_var(),
                                    get_te_var("batch_c3").get_tvm_var(), get_te_var("batch_c4").get_tvm_var()]
                # the max batch dim is 4
                return list_batch_a, list_batch_b, list_batch_broad[4 - max(len(list_batch_a), len(list_batch_b)):]
        if list_batch_a or list_batch_b:
            list_batch_a, list_batch_b, list_batch_broad = broadcast_shapes(list_batch_a, list_batch_b)
        else:
            # in case of bug in broadcast
            list_batch_broad = []
        return list_batch_a, list_batch_b, list_batch_broad

    def _compute_tensor_mad_fp32(self, tensor_a, tensor_b, tensor_bias):
        '''
        (b, m1, k1, m0, k0) * (b, k1, n1, n0, k0)
        (b, m1, k1, m0, k0) * (1, k1, n1, n0, k0)
        Multi-dimensional batch broadcast is not supported
        '''
        shape_tensor_a = shape_util.shape_to_list(tensor_a.shape)
        shape_batch_a, shape_m1, shape_k1, shape_m0, shape_k0 = shape_tensor_a
        shape_tensor_b = shape_util.shape_to_list(tensor_b.shape)
        shape_batch_b, _, shape_n1, shape_n0, _ = shape_tensor_b
        shape_mad = (shape_batch_a, shape_n1, shape_m1, shape_m0, shape_n0)
        ori_k = self.ori_shape_a[-2] if self.trans_a else self.ori_shape_a[-1]
        ori_m = self.ori_shape_a[-1] if self.trans_a else self.ori_shape_a[-2]
        ori_n = self.ori_shape_b[-2] if self.trans_b else self.ori_shape_b[-1]

        k1 = tvm.reduce_axis((0, shape_k1), name="k1")
        k0 = tvm.reduce_axis((0, shape_k0), name="k0")
        fixpipe_op_dict = {
            "quant_scale_0": 1,
            "relu_weight_0": 0,
            "relu_weight_1": 0,
            "quant_scale_1": 0,
            "eltwise_src": 0
        }
        attrs = {
            "trans_a": self.trans_a,
            "trans_b": self.trans_b,
            "a_ori_shape": self.input_a.op.attrs["ori_shape"],
            "b_ori_shape": self.input_b.op.attrs["ori_shape"],
            "format_out": self.format_out,
            "shape_out": self.shape_out,
            "format_a": self.format_a,
            "format_b": self.format_b,
            "q_bias_ori_shape": self.input_q_bias.op.attrs["ori_shape"],
            "fixpipe_op_dict": fixpipe_op_dict
        }

        # batch info
        if "ori_batch_shape" in self.input_a.op.attrs:
            tensor_a.op.attrs["ori_batch_shape"] = self.input_a.op.attrs["ori_batch_shape"]
            tensor_b.op.attrs["ori_batch_shape"] = self.input_b.op.attrs["ori_batch_shape"]
        if in_dynamic():
            list_batch_a, list_batch_b, list_batch_broad = self._get_batch_broadcast(tensor_a, tensor_b)
        else:
            list_batch_a = tensor_a.op.attrs["ori_batch_shape"]
            list_batch_b = tensor_b.op.attrs["ori_batch_shape"]
            list_batch_a, list_batch_b, list_batch_broad = broadcast_shapes(list_batch_a, list_batch_b)
        batch_reduce = [functools_reduce(lambda x, y: x * y, list_batch_broad)] if list_batch_broad else []
        shape_mmad = batch_reduce + [shape_n1, shape_m1, shape_m0, shape_n0]
        def __mmad_kernel_func(indices_a, indices_b):
            m1_idx, _, m0_idx, _ = indices_a[-4:]
            k1_idx, n1_idx, n0_idx, k0_idx = indices_b[-4:]
            a_expr = tensor_a(*indices_a)
            b_expr = tensor_b(*indices_b)
            mad_expr = (a_expr * b_expr).astype("float32")
            if tensor_bias is not None:
                bias_expr = tensor_bias(n1_idx, n0_idx)
                mad_expr = mad_expr + bias_expr
            if ori_k != 0:
                # Mad support origin m/n shape in non-quantized scenarios and ND output
                if ori_m != 0 and ori_n != 0 and self.dtype_out == "float32" and self.format_out == "ND":
                    mad_expr = tvm.select(tvm.all(k1_idx.var * shape_k0 + k0_idx.var < ori_k,
                                                m1_idx * shape_m0 + m0_idx < ori_m,
                                                n1_idx * shape_n0 + n0_idx < ori_n),
                                                mad_expr)
                else:
                    mad_expr = tvm.select(tvm.all(k1_idx.var * shape_k0 + k0_idx.var < ori_k), mad_expr)
            sum_expr = tvm.sum(mad_expr, axis=[k1, k0])
            return sum_expr

        def __mmad_with_batch(batch, n1, m1, m0, n0):
            batch_prefix_a = batch_prefix_b = []
            batch_prefix_a = [self.get_batch_index(batch, list_batch_broad, list_batch_a)]
            batch_prefix_b = [self.get_batch_index(batch, list_batch_broad, list_batch_b)]
            indices_a = batch_prefix_a + [m1, k1, m0, k0]
            indices_b = batch_prefix_b + [k1, n1, n0, k0]
            return __mmad_kernel_func(indices_a, indices_b)

        __mmad_func = __mmad_with_batch
        tensor_mad_fp32 = tvm.compute(shape_mmad, __mmad_func,
                            name="tensor_mad_fp32",
                            tag=OP_TAG + "tensor_mad_fp32",
                            attrs=attrs)
        return tensor_mad_fp32

    def _compute_tensor_bias_fp32(self, tensor_bias):
        if tensor_bias is None:
            return tensor_bias
        tensor_bias_fp32 = self._compute_tensor_bias(tensor_bias, "tensor_bias_fp32")
        return tensor_bias_fp32


    def _compute_tensor_b_fp16(self, nz_tensor):
        '''
        (shape_batch, k1/k_l0, n1, k_l0, k0, n0)
          -->
        (shape_batch, n1, k1, n0, k0)
        '''
        ori_k = self.ori_shape_b[-1] if self.trans_b else self.ori_shape_b[-2]
        shape_nz = shape_util.shape_to_list(nz_tensor.shape)
        (shape_batch, _, shape_n1, shape_kl0, shape_k0, shape_n0) = shape_nz
        shape_k1 = int_ceil_div(ori_k, shape_k0)
        shape_zn = (shape_batch, shape_k1, shape_n1, shape_n0, shape_k0)
        tensor_b_zn_fp16 = tvm.compute(shape_zn,
            lambda b, k1, n1, n0, k0: nz_tensor(
                b, k1 // shape_kl0, n1, k1 % shape_kl0, k0, n0),
            name="tensor_b_zn_fp16",
            tag=OP_TAG + "tensor_b_zn_fp16")
        return tensor_b_zn_fp16


    def _compute_tensor_a_fp16(self, input_tensor):
        if self.format_a == "FRACTAL_NZ":
            tensor_a_zz_fp16 = self._get_tensor_a_fp16_nz2zz(input_tensor)
        else:
            tensor_a_zz_fp16 = self._get_tensor_a_fp16_nd2zz(input_tensor)

        return tensor_a_zz_fp16


    def _get_tensor_a_fp16_nd2zz(self, nd_tensor):
        tensor_a_nz_fp16 = self._get_tensor_a_fp16_nd2nz(nd_tensor)
        tensor_a_zz_fp16 = self._get_tensor_a_fp16_nz2zz(tensor_a_nz_fp16)
        return tensor_a_zz_fp16


    def _get_tensor_a_fp16_nz2zz(self, nz_tensor):
        '''
        FALSE (b, k1, m1, m0, k0) -> (b, m1, k1, m0, k0)
        TRUE  (b, m1, k1, k0, m0) -> (b, m1, k1, m0, k0)
        '''
        block_in = 16
        shape_batch = self.ori_shape_a[0]
        shape_m, shape_k = self.ori_shape_a[-2], self.ori_shape_a[-1]
        if self.trans_a:
            shape_m, shape_k = shape_k, shape_m
        shape_zz = [shape_batch,
                    int_ceil_div(shape_m, block_in),
                    int_ceil_div(shape_k, block_in),
                    block_in, block_in]
        if self.trans_a:
            tensor_a_zz_fp16 = tvm.compute(shape_zz,
                lambda b, m1, k1, m0, k0: nz_tensor(b, m1, k1, k0, m0),
                name="tensor_a_zz_fp16",
                tag=OP_TAG + "tensor_a_zz_fp16")
        else:
            tensor_a_zz_fp16 = tvm.compute(shape_zz,
                lambda b, m1, k1, m0, k0: nz_tensor(b, k1, m1, m0, k0),
                name="tensor_a_zz_fp16",
                tag=OP_TAG + "tensor_a_zz_fp16")
        return tensor_a_zz_fp16


    def _get_tensor_a_fp16_nd2nz(self, nd_tensor):
        '''
        FALSE (b, m, k) -> (b, k1, m1, m0, k0)
        TRUE  (b, k, m) -> (b, m1, k1, k0, m0)
        '''
        nd_shape = shape_util.shape_to_list(nd_tensor.shape)
        block_in = 16
        shape_batch, shape_out, shape_in = nd_shape
        shape_nz = (shape_batch,
                    int_ceil_div(shape_in, block_in),
                    int_ceil_div(shape_out, block_in),
                    block_in, block_in)

        tensor_a_nz_fp16 = tvm.compute(shape_nz,
            lambda *indices: tvm.select(
                tvm.all(indices[-3] * block_in + indices[-2] < shape_out), tvm.select(
                        indices[-4] * block_in + indices[-1] < shape_in,
                nd_tensor(
                    indices[0],
                    indices[-3] * block_in + indices[-2],
                    indices[-4] * block_in + indices[-1]))),
            name="tensor_a_nz_fp16",
            tag=OP_TAG + "tensor_a_nz_fp16")

        return tensor_a_nz_fp16


    def _compute_tensor_weight_fp16(self, tensor_mad_int32, tensor_deq_scale):
        op_dict = {"pre_conv": "S322F16"}
        if self.deq_scale_vector_flag:
            op_dict = {"pre_conv": "VS322F16"}
        tensor_weight_fp16 = tvm.compute(
            tensor_mad_int32.shape,
            lambda *indices:
            tvm.fixpipe_op(tensor_mad_int32(*indices),
                "float16",
                pre_conv_param=tensor_deq_scale(0, indices[-4], 0, 0, indices[-1]) \
                    if self.deq_scale_vector_flag else self.deq_scale_value,
                op_dict=op_dict),
            name="tensor_weight_fp16",
            attrs=op_dict,
            tag=OP_TAG)
        return tensor_weight_fp16


    def _compute_tensor_mad_int32(self, tensor_a, tensor_b, tensor_bias):
        '''
        FALSE (b, k, n) -> (b, n1, k1, k0, n0) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        TRUE  (b, n, k) -> (b, k1, n1, n0, k0*2) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        '''
        shape_tensor_a = shape_util.shape_to_list(tensor_a.shape)
        shape_m1, _, shape_m0, _ = shape_tensor_a
        shape_tensor_b = shape_util.shape_to_list(tensor_b.shape)
        shape_batch, shape_kg, shape_k1, shape_n1, shape_n0, shape_k0 = shape_tensor_b
        shape_mad = (shape_batch, shape_kg, shape_n1, shape_m1, shape_m0, shape_n0)
        k1 = tvm.reduce_axis((0, shape_k1), name="k1")
        k0 = tvm.reduce_axis((0, shape_k0), name="k0")
        ori_k = self.ori_shape_b[-1] if self.trans_b else self.ori_shape_b[-2]

        def __mmad_func(batch, g, n1, m1, m0, n0):
            a_expr = tensor_a(m1, k1, m0, k0)
            b_expr = tensor_b(batch, g, k1, n1, n0, k0)
            bias_expr = tensor_bias(n1, n0)
            mad_expr = (a_expr * b_expr).astype("int32") + bias_expr
            sum_expr = tvm.sum(mad_expr, axis=[k1, k0])
            return sum_expr

        tensor_mad_int32 = tvm.compute(shape_mad, __mmad_func,
                                       name="tensor_mad_int32",
                                       tag=OP_TAG + "tensor_mad_int32")

        return tensor_mad_int32


    def _compute_tensor_bias_int32(self, tensor_bias):
        tensor_bias_int32 = self._compute_tensor_bias(tensor_bias, "tensor_bias_int32")
        return tensor_bias_int32


    def _compute_tensor_bias(self, tensor_bias, name):
        block_in = 16
        shape_ori_n = shape_util.shape_to_list(tensor_bias.shape)[0]
        shape_bias_align = (int_ceil_div(shape_ori_n, block_in), block_in)
        tensor_bias_align = tvm.compute(shape_bias_align,
            lambda n1, n0:
            tvm.select(
                n1 * block_in + n0 < shape_ori_n,
                tensor_bias(n1 * block_in + n0)
            ),
            name=name,
            tag=OP_TAG + name)
        if tensor_bias.dtype == "float16":
            tensor_bias_align = FormatCompute.cast_dtype(
                tensor_bias_align, "float32", name=name + "_cast",
                tag=OP_TAG + name + "_cast")
        return tensor_bias_align


    def _compute_tensor_a_int8(self, input_tensor):
        '''
        (32, 32) -> (32, 32) -> (k_l0, k_l0/2, 16, 32)
              GM       ->         L1     ->          L0A
        '''
        block_in = 16
        tensor_a_nz_int8 = tvm.compute(
            input_tensor.shape, lambda *i: input_tensor(*i),
            name="tensor_a_nz_int8",
            tag=OP_TAG + "tensor_a_nz_int8")
        shape_a_zz = (self.k_l0, self.k_l0 // 2, 16, 32)
        tensor_a_zz_int8 = tvm.compute(
            shape_a_zz, lambda m1, k1, m0, k0:
            tvm.select(
                k1 == m1 // 2,
                tensor_a_nz_int8((m1 % 2) * block_in + m0, k0)),
            name="tensor_a_zz_int8",
            tag=OP_TAG + "tensor_a_zz_int8")
        return tensor_a_zz_int8


    def _compute_tensor_b_int8(self, input_tensor):
        '''
        FALSE (b, k, n) -> (b, n1, k1, k0, n0) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        TRUE  (b, n, k) -> (b, k1, n1, n0, k0*2) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        '''
        if self.format_b == "FRACTAL_NZ":
            tensor_b_zn_int8 = self._get_tensor_b_int8_nz2zn_group(input_tensor)
        else:
            tensor_b_zn_int8 = self._get_tensor_b_int8_nd2zn(input_tensor)

        return tensor_b_zn_int8


    def _get_tensor_b_int8_nz2zn_group(self, nz_tensor):
        '''
        FALSE (b, n1, k1, k0, n0) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        TRUE  (b, k1, n1, n0, k0*2) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        '''
        block_in = 16
        block_reduce = 32
        shape_batch = self.ori_shape_b[0]
        shape_k, shape_n = self.ori_shape_b[-2], self.ori_shape_b[-1]
        if self.trans_b:
            shape_n, shape_k = shape_k, shape_n
        shape_zn = [shape_batch, int_ceil_div(shape_k, self.k_l0 * block_reduce // 2),
                    self.k_l0 // 2, int_ceil_div(shape_n, block_in),
                    block_in, block_reduce]
        if self.trans_b:
            tensor_b_zn_int8 = tvm.compute(shape_zn,
                lambda b, k_g, k_in, n1, n0, k0:
                tvm.select(
                    k_g * (self.k_l0 // 2) + k_in < int_ceil_div(shape_k, block_reduce),
                    nz_tensor(b, k_g * (self.k_l0 // 2) + k_in, n1, n0, k0)
                ),
                name="tensor_b_zn_int8",
                tag=OP_TAG + "tensor_b_zn_int8")
        else:
            shape_zn[-3] = align(shape_zn[-3], 2)
            tensor_b_zn_int8 = tvm.compute(shape_zn,
                lambda b, k_g, k_in, n1, n0, k0:
                tvm.select(
                    (k_g * (self.k_l0 // 2) + k_in) * 2 < align(int_ceil_div(shape_k, block_in), 2),
                    nz_tensor(
                        b, (n1 * block_in + n0) // 32,
                        ((k_g * (self.k_l0 // 2) + k_in) * block_reduce + k0) // 16,
                        ((k_g * (self.k_l0 // 2) + k_in) * block_reduce + k0) % 16,
                        (n1 * block_in + n0) % 32)
                ),
                name="tensor_b_zn_int8",
                tag=OP_TAG + "tensor_b_zn_int8")

        return tensor_b_zn_int8


    def _get_tensor_b_int8_nz2zn_new(self, nz_tensor):
        '''
        FALSE (b, k1/k_l0, n1, k_l0, k0, n0) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        TRUE  (b, k1/k_l0, k_l0/2, n1, n0, k0*2) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        '''
        block_in = 16
        block_reduce = 32
        shape_batch = self.ori_shape_b[0]
        shape_k, shape_n = self.ori_shape_b[-2], self.ori_shape_b[-1]
        if self.trans_b:
            shape_n, shape_k = shape_k, shape_n
        shape_zn = [shape_batch, int_ceil_div(shape_k, self.k_l0 * block_reduce // 2),
                    self.k_l0 // 2, int_ceil_div(shape_n, block_in),
                    block_in, block_reduce]
        if self.trans_b:
            tensor_b_zn_int8 = tvm.compute(shape_zn,
                lambda b, k_g, k_in, n1, n0, k0:
                    nz_tensor(b, k_g, k_in, n1, n0, k0),
                name="tensor_b_zn_int8",
                tag=OP_TAG + "tensor_b_zn_int8")
        else:
            shape_zn[-3] = align(shape_zn[-3], 2)
            tensor_b_zn_int8 = tvm.compute(shape_zn,
                lambda b, k_g, k_in, n1, n0, k0:
                    nz_tensor(
                        b, k_g,
                        (n1 * block_in + n0) // 32,
                        (k_in * block_reduce + k0) // 16,
                        (k_in * block_reduce + k0) % 16,
                        (n1 * block_in + n0) % 32),
                name="tensor_b_zn_int8",
                tag=OP_TAG + "tensor_b_zn_int8")

        return tensor_b_zn_int8


    def _get_tensor_b_int8_nd2nz(self, nd_tensor):
        '''
        FALSE (b, k, n) -> (b, n1, k1, k0, n0)
        TRUE  (b, n, k) -> (b, k1, n1, n0, k0*2)
        '''
        nd_shape = shape_util.shape_to_list(nd_tensor.shape)
        block_in = 16
        block_reduce = 32
        shape_batch, shape_out, shape_in = nd_shape
        shape_nz = [shape_batch, int_ceil_div(shape_in, block_reduce),
                    int_ceil_div(shape_out, block_in), block_in, block_reduce]
        if not self.trans_b:
            shape_nz[2] = align(shape_nz[2], 2)
        tensor_b_nz_int8 = tvm.compute(shape_nz,
                lambda *indices: tvm.select(
                    tvm.all(indices[-3] * block_in + indices[-2] < shape_out), tvm.select(
                            indices[-4] * block_reduce + indices[-1] < shape_in,
                    nd_tensor(
                        indices[0],
                        indices[-3] * block_in + indices[-2],
                        indices[-4] * block_reduce + indices[-1]))
                ),
                name="tensor_b_nz_int8",
                tag=OP_TAG + "tensor_b_nz_int8")

        return tensor_b_nz_int8


    def _get_tensor_b_int8_nd2nz_group(self, nd_tensor):
        '''
        FALSE (b, k, n) -> (b, k1/k_l0, n1, k_l0, k0, n0)
        TRUE  (b, n, k) -> (b, k1/k_l0, k_l0/2, n1, n0, k0*2)
        '''
        nd_shape = shape_util.shape_to_list(nd_tensor.shape)
        block_in = 16
        block_reduce = 32
        shape_batch, shape_out, shape_in = nd_shape
        shape_nz = (shape_batch, int_ceil_div(shape_out, self.k_l0 * block_in),
                    int_ceil_div(shape_in, block_reduce), self.k_l0,
                    block_in, block_reduce)
        if self.trans_b:
            shape_nz = (shape_batch,
                        int_ceil_div(shape_in, self.k_l0 * block_reduce // 2),
                        self.k_l0 // 2,
                        int_ceil_div(shape_out, block_in),
                        block_in, block_reduce)
        if self.trans_b:
            tensor_b_nz_int8 = tvm.compute(shape_nz,
                    lambda b, g, k1, n1, n0, k0: tvm.select(
                        tvm.all(n1 * block_in + n0 < shape_out,
                                (g * self.k_l0 // 2 + k1) * block_reduce + k0 < shape_in),
                        nd_tensor(
                            b,
                            n1 * block_in + n0,
                            (g * self.k_l0 // 2 + k1) * block_reduce + k0)
                    ),
                    name="tensor_b_nz_int8",
                    tag=OP_TAG + "tensor_b_nz_int8")
        else:
            tensor_b_nz_int8 = tvm.compute(shape_nz,
                    lambda b, g, n1, k1, k0, n0: tvm.select(
                        tvm.all((g * self.k_l0 + k1) * block_in + k0 < shape_out,
                                n1 * block_reduce + n0 < shape_in),
                        nd_tensor(
                            b,
                            (g * self.k_l0 + k1) * block_in + k0,
                            n1 * block_reduce + n0)
                    ),
                    name="tensor_b_nz_int8",
                    tag=OP_TAG + "tensor_b_nz_int8")

        return tensor_b_nz_int8


    def _get_tensor_b_int8_nd2zn(self, nd_tensor):
        '''
        FALSE (b, k, n) -> (b, k_g, n1, k_l0, k0, n0)
        TRUE  (b, n, k) -> (b, k_g, k_l0, n1, n0, k0*2)
        '''
        tensor_b_nz_int8 = self._get_tensor_b_int8_nd2nz(nd_tensor)
        tensor_b_zn_int8 = self._get_tensor_b_int8_nz2zn_group(tensor_b_nz_int8)

        return tensor_b_zn_int8


def weight_quant_bmm_compute(tensor_a, tensor_b, tensor_diag, para_dict):
    compute_obj = WQBmmCompute(tensor_a, tensor_b, tensor_diag, para_dict)
    tensor_out = compute_obj.compute()
    return tensor_out
