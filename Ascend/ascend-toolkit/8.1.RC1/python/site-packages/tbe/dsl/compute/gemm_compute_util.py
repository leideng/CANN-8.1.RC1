#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gemm compute util
"""
from copy import copy
from collections.abc import Iterable
from functools import reduce as functools_reduce
from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils import broadcast_shapes
from tbe.common.utils import shape_util
from tbe.common.utils.errormgr import error_manager_cube
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.compute.util import align
from tbe.dsl.compute.util import int_ceil_div
from tbe import tvm

BMM_LEN_ND = 3
BMM_LEN_NZ = 5
ALIGN_NUM_TWO = 2


class MatmulShapeInfo:
    """
    the class to manage matmul shape info
    """
    batch_nd_len = 3
    batch_frac_len = 5
    dict_format_list = {
        # the format list of tensor in no trans situation
        "ND": ["m", "n"],
        "FRACTAL_Z": ["m1", "n1", "m0", "n0"],
        "FRACTAL_NZ": ["n1", "m1", "m0", "n0"],
        "FRACTAL_ZN_RNN": ["m1", "n1", "m0", "n0"],
    }

    def __init__(self, shape, tensor_format, trans, pad_flag):
        self.shape = shape_util.shape_to_list(shape)
        self.tensor_format = tensor_format
        self.trans = trans
        self.format_list = self.get_format_list()
        self.__set_shape_val()
        self.pad_flag = pad_flag

    def have_batch(self):
        """if hava batch"""
        return len(self.shape) in (self.batch_nd_len, self.batch_frac_len)

    def get_base_format_list(self, key):
        """get the format list by key from the current dict directly"""
        self._check_invalid_format(key)
        prefix = ["batch"] if self.have_batch() else []
        format_list = prefix + self.dict_format_list.get(key)
        return format_list

    def get_format_list(self, key=None):
        """
        get real format list by key with trans info
        """
        if key is None:
            key = self.tensor_format.upper()
        self._check_invalid_format(key)
        base_list = self.dict_format_list.get(key)
        format_list = []
        if self.trans:
            # swap axes string by 2 step
            # NOTE: format_list's length must be a multiple of 2
            for idx, _ in enumerate(base_list[::2]):
                format_list.append(base_list[idx*2 + 1])
                format_list.append(base_list[idx*2])
        else:
            format_list = copy(base_list)
        if self.have_batch():
            format_list = ["batch"] + format_list
        return format_list

    def _check_invalid_format(self, tensor_format):
        if tensor_format not in self.dict_format_list:
            reason = f"Not support current format {tensor_format}."
            error_manager_cube.raise_err_specific("GEMM", reason)

    def set_shape_info(self, shape, tensor_format, trans):
        self.shape = shape_util.shape_to_list(shape)
        self.tensor_format = tensor_format
        self.trans = trans
        self.format_list = self.get_format_list()
        self.__set_shape_val()

    def __set_shape_val(self):
        if len(self.format_list) != len(self.shape):
            reason = f"Length of format_list({self.format_list}) cannot match length of shape({self.shape})."
            error_manager_cube.raise_err_specific("GEMM", reason)
        for shape_k, shape_v in zip(self.format_list, self.shape):
            self.__dict__[shape_k] = shape_v


class ShapeAInfo(MatmulShapeInfo):
    """
    the class to manage tensor_a shape info
    """
    dict_format_list = {
        "ND": ["m", "k"],
        "FRACTAL_Z": ["m1", "k1", "m0", "k0"],
        "FRACTAL_NZ": ["k1", "m1", "m0", "k0"],
        "NC1HWC0": ["n", "c1", "h", "w", "c0"],
    }

    def __init__(self, shape, tensor_format, trans, pad_flag):
        super().__init__(shape, tensor_format, trans, pad_flag)
        self._check_input()
        self.pad_flag = pad_flag
        self._get_pad()

    def have_batch(self):
        """override the parent function"""
        if self.tensor_format == "NC1HWC0":
            return False
        return super().have_batch()

    def get_full_m_dim(self):
        """get full m value"""
        if self.tensor_format == "ND":
            full_m = self.m
        elif self.tensor_format == "NC1HWC0":
            full_m = self.n
        else:
            full_m = self.m1 * self.m0
        return full_m

    def get_full_k_dim(self):
        """get full k value"""
        if self.tensor_format == "ND":
            full_k = self.k
        elif self.tensor_format == "NC1HWC0":
            full_k = self.c1 * self.h * self.w * self.c0
        else:
            full_k = self.k1 * self.k0
        return full_k

    def get_shape_nz(self, dtype="float16", block_in=None, block_reduce=None):
        """get shape a in Nz fractal"""
        block_in = tbe_platform.BLOCK_OUT if block_in is None else block_in
        block_reduce = tbe_platform.CUBE_MKN.get(dtype).get("mac")[1] if block_reduce is None else block_reduce
        if self.trans:
            shape_a_nz = [self._get_m1_dim(block_reduce), self._get_k1_dim(block_in), block_in, block_reduce]
        else:
            shape_a_nz = [self._get_k1_dim(block_reduce), self._get_m1_dim(block_in), block_in, block_reduce]
        if self.have_batch():
            shape_a_nz = [self.batch] + shape_a_nz
        return shape_a_nz

    def get_shape_a_zz(self, dtype="float16", block_in=None, block_reduce=None):
        """get shape a in Zz fractal"""
        block_in = tbe_platform.BLOCK_OUT if block_in is None else block_in
        block_reduce = tbe_platform.CUBE_MKN.get(dtype).get("mac")[1] if block_reduce is None else block_reduce
        support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        block_in_real = block_reduce if (self.trans and (dtype == "int8") and support_l0c2out) else block_in
        shape_a_zz = [self._get_m1_dim(block_in_real), self._get_k1_dim(block_reduce), block_in, block_reduce]
        if self.trans and (dtype == "int8") and support_l0c2out:
            shape_a_zz[0] *= ALIGN_NUM_TWO
            if self.tensor_format == "FRACTAL_NZ" and in_dynamic():
                shape_a_zz[1] = int_ceil_div(shape_a_zz[1], ALIGN_NUM_TWO)
        if self.have_batch():
            shape_a_zz = [self.batch] + shape_a_zz
        return shape_a_zz

    def _check_input(self):
        if self.tensor_format == "NC1HWC0" and self.trans:
            error_manager_cube.raise_err_specific("GEMM", "Please check the input's format & trans info.")

    def _get_pad(self):
        if self.pad_flag == 0:
            return
        self.ori_m = self.m
        self.ori_k = self.k
        self.m_pad = get_te_var("m_pad").get_tvm_var()
        self.k_pad = get_te_var("k_pad").get_tvm_var()
        self.m = self.m_pad
        self.k = self.k_pad

    def _get_m1_dim(self, block_in):
        # NOTE: need to recalculate when dynamic/binary mode support int8/float32
        m_dim = self.get_full_m_dim()
        m_dim_valid = isinstance(m_dim, int) or self.tensor_format in ("ND", "NC1HWC0")
        m1 = int_ceil_div(m_dim, block_in) if m_dim_valid else self.m1
        return m1

    def _get_k1_dim(self, block_reduce):
        # NOTE: need to recalculate when dynamic/binary mode support int8/float32
        k_dim = self.get_full_k_dim()
        k_dim_valid = isinstance(k_dim, int) or self.tensor_format in ("ND", "NC1HWC0")
        k1 = int_ceil_div(k_dim, block_reduce) if k_dim_valid else self.k1
        return k1


class ShapeBInfo(MatmulShapeInfo):
    """
    the class to manage tensor_b shape info
    """
    dict_format_list = {
        "ND": ["k", "n"],
        "FRACTAL_Z": ["k1", "n1", "n0", "k0"],
        "FRACTAL_NZ": ["n1", "k1", "k0", "n0"],
        "FRACTAL_ZN_RNN": ["k1", "n1", "n0", "k0"],
    }

    def __init__(self, shape, tensor_format, trans, pad_flag):
        super().__init__(shape, tensor_format, trans, pad_flag)
        self.pad_flag = pad_flag
        self._get_pad()

    def get_full_n_dim(self):
        """get full n value"""
        full_n = self.n if self.tensor_format == "ND" else self.n1 * self.n0
        return full_n

    def get_full_k_dim(self):
        """get full k value"""
        full_k = self.k if self.tensor_format == "ND" else self.k1 * self.k0
        return full_k

    def get_shape_b_zn(self, dtype="float16", block_out=None, block_reduce=None):
        """get shape b in Zn fractal"""
        block_out = tbe_platform.BLOCK_OUT if block_out is None else block_out
        block_reduce = tbe_platform.CUBE_MKN.get(dtype).get("mac")[1] if block_reduce is None else block_reduce
        support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        need_fix_block_out = ((not self.trans) and (dtype == "int8") and support_l0c2out and
                              (self.tensor_format != "FRACTAL_Z"))
        block_out_real = block_reduce if need_fix_block_out else block_out
        shape_b_zn = [self._get_k1_dim(block_reduce), self._get_n1_dim(block_out_real), block_out, block_reduce]
        if (not self.trans) and (dtype == "int8") and support_l0c2out and (self.tensor_format != "FRACTAL_Z"):
            shape_b_zn[1] *= ALIGN_NUM_TWO
            if self.tensor_format == "FRACTAL_NZ" and in_dynamic():
                shape_b_zn[0] = int_ceil_div(shape_b_zn[0], ALIGN_NUM_TWO)
        if self.have_batch():
            shape_b_zn = [self.batch] + shape_b_zn
        return shape_b_zn

    def get_shape_nz(self, dtype="float16", block_out=None, block_reduce=None):
        """get shape b in Nz fractal"""
        block_out = tbe_platform.BLOCK_OUT if block_out is None else block_out
        block_reduce = tbe_platform.CUBE_MKN.get(dtype).get("mac")[1] if block_reduce is None else block_reduce
        if self.trans:
            shape_b_nz = [self._get_k1_dim(block_reduce), self._get_n1_dim(block_out), block_out, block_reduce]
        else:
            shape_b_nz = [self._get_n1_dim(block_reduce), self._get_k1_dim(block_out), block_out, block_reduce]
        if self.have_batch():
            shape_b_nz = [self.batch] + shape_b_nz
        return shape_b_nz

    def _get_pad(self):
        if self.pad_flag == 0:
            return
        self.ori_k = self.k
        self.ori_n = self.n
        self.k_pad = get_te_var("k_pad").get_tvm_var()
        self.n_pad = get_te_var("n_pad").get_tvm_var()
        self.k = self.k_pad
        self.n = self.n_pad

    def _get_n1_dim(self, block_out):
        # NOTE: need to recalculate when dynamic/binary mode support int8/float32
        n_dim = self.get_full_n_dim()
        n1 = int_ceil_div(n_dim, block_out) if isinstance(n_dim, int) or self.tensor_format == "ND" else self.n1
        return n1

    def _get_k1_dim(self, block_reduce):
        # NOTE: need to recalculate when dynamic/binary mode support int8/float32
        k_dim = self.get_full_k_dim()
        k1 = int_ceil_div(k_dim, block_reduce) if isinstance(k_dim, int) or self.tensor_format == "ND" else self.k1
        return k1


class FormatCompute(object):
    """
    the format compute for gemm
    """
    def __init__(self):
        pass

    @staticmethod
    def cast_dtype(tensor_src, dtype_dst, **kwargs):
        """
        cast tensor_src to dtype_dst
        """
        tensor_dst = tvm.compute(
            tensor_src.shape,
            lambda *indices: tensor_src(*indices).astype(dtype_dst),
            **kwargs
        )
        return tensor_dst

    @staticmethod
    def p2p_copy(tensor_src, shape_src, **kwargs):
        """
        do point to point copy
        input params:
            tensor_src: the source tensor
            shape_src: the shape of the source tensor
            tensor_name: the name of dst tensor
        return:
            the dst tensor
        """
        tensor_dst = tvm.compute(
            shape_src,
            lambda *indices: tensor_src(*indices),
            **kwargs
        )
        return tensor_dst

    @staticmethod
    def elementwise_add(tensor_lhs, tensor_rhs, str_dst_indices, **kwargs):
        if not isinstance(str_dst_indices, Iterable):
            error_manager_cube.raise_err_specific("GEMM", "String of destination indices must be Iterable.")

        indices = ', '.join(str_dst_indices)
        str_lambda = f"lambda {indices}: tensor_lhs({indices}) + tensor_rhs({indices})"
        shape_dst = tensor_lhs.shape

        return tvm.compute(shape_dst, eval(str_lambda, locals()), **kwargs)

    @staticmethod
    def transpose_axes(tensor_src, src_format_list, dst_format_list, **kwargs):
        """
        transpose axes by format_list
        eg:
            ["k1", "m1", "m0", "k0"] -> ["m1", "k1", "m0", "k0"]
        """
        if len(tensor_src.shape) != len(src_format_list) or sorted(src_format_list) != sorted(dst_format_list):
            error_manager_cube.raise_err_specific("GEMM", "Please check the tensor's shape and src/dst format_list.")
        src_format_str = ', '.join(src_format_list)
        dst_format_str = ', '.join(dst_format_list)
        shape_src = shape_util.shape_to_list(tensor_src.shape)
        shape_dst = list(shape_src[src_format_list.index(dim)] for dim in dst_format_list)
        lambda_str = "lambda {}: tensor_src[{}]".format(dst_format_str, src_format_str)
        res = tvm.compute(shape_dst, eval(lambda_str, {"tensor_src": tensor_src}), **kwargs)
        return res

    @staticmethod
    def compute_nd2zz(ori_tensor, compute_params):
        """
        reshape nd2Zz by normal way
        input params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
           the tensor format Zz
        """
        tensor_name = compute_params.get("tensor_name")
        block_in = compute_params.get("block_in")
        block_reduce = compute_params.get("block_reduce")
        data_flow = compute_params.get("data_flow")
        mode_info = compute_params.get("mode_info", "none")
        trans = compute_params.get("trans")
        int82int32_trans_flag = (data_flow == "int82int32") and trans
        if int82int32_trans_flag:
            return FormatCompute._compute_nd2zz_int8_trans(ori_tensor, compute_params)
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        if trans:
            tensor_matrix_shape = ori_tensor_shape[:-2] + [
                int_ceil_div(ori_tensor_shape[-1], block_in),
                int_ceil_div(ori_tensor_shape[-2], block_reduce),
                block_in,
                block_reduce
            ]
            tensor_l1_shape = tensor_matrix_shape[:-2] + [block_reduce, block_in]
            l1_tensor = tvm.compute(tensor_l1_shape,
                                    lambda *indices: ori_tensor(*indices[:-4],
                                                                indices[-3]*block_reduce + indices[-2],
                                                                indices[-4]*block_in + indices[-1]),
                                    name="tensor_a_l1")
            lambda_expression = lambda *indices: l1_tensor(*indices[:-2], indices[-1], indices[-2])
        else:
            tensor_matrix_shape = ori_tensor_shape[:-2] + [
                int_ceil_div(ori_tensor_shape[-2], block_in),
                int_ceil_div(ori_tensor_shape[-1], block_reduce),
                block_in,
                block_reduce
            ]
            lambda_expression = lambda *indices: ori_tensor(*indices[:-4],
                                                            indices[-4]*block_in + indices[-2],
                                                            indices[-3]*block_reduce + indices[-1])
        res = tvm.compute(
            tensor_matrix_shape,
            lambda_expression,
            name=tensor_name,
            attrs={"mode": mode_info}
        )
        return res

    @staticmethod
    def compute_nd2zz_gevm(ori_tensor, compute_params):
        """
        reshape nd2Zz by normal way and gevm mode
        input params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format Zz
        """
        data_flow = compute_params.get("data_flow")
        mode_info = compute_params.get("mode_info", "none")
        trans = compute_params.get("trans")
        block_in = compute_params.get("block_in")
        block_reduce = compute_params.get("block_reduce")
        int82int32_trans_flag = (data_flow == "int82int32") and trans
        if int82int32_trans_flag:
            return FormatCompute._compute_nd2zz_int8_trans(ori_tensor, compute_params)
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        if trans:
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                tensor_matrix_shape = ori_tensor_shape[:-2] + [
                    int_ceil_div(ori_tensor_shape[-1], block_in),
                    int_ceil_div(ori_tensor_shape[-2], block_reduce),
                    block_reduce, block_in
                ] # [m1, k1, k0, m0]
                lambda_expression = lambda *indices: ori_tensor(*indices[:-4],
                                                                indices[-3] * block_reduce + indices[-2],
                                                                indices[-4] * block_in + indices[-1])  # [k, m]
            else:
                tensor_matrix_shape = ori_tensor_shape[:-2] + [
                    int_ceil_div(ori_tensor_shape[-1], block_in),
                    int_ceil_div(ori_tensor_shape[-2], block_reduce),
                    block_in, block_reduce
                ]
                lambda_expression = lambda *indices: ori_tensor(*indices[:-4],
                                                                indices[-3] * block_reduce + indices[-1],
                                                                indices[-4] * block_in + indices[-2])
        else:
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                tensor_matrix_shape = ori_tensor_shape[:-2] + [
                    int_ceil_div(ori_tensor_shape[-1], block_reduce),
                    int_ceil_div(ori_tensor_shape[-2], block_in),
                    block_in, block_reduce
                ] # [k1, m1, m0, k0]
                lambda_expression = lambda *indices: ori_tensor(*indices[:-4], indices[-3] * block_in + indices[-2],
                                                                indices[-4] * block_reduce + indices[-1]) # [m, k]
            else:
                tensor_matrix_shape = ori_tensor_shape[:-2] + [
                    int_ceil_div(ori_tensor_shape[-2], block_in),
                    int_ceil_div(ori_tensor_shape[-1], block_reduce),
                    block_in, block_reduce
                ]
                lambda_expression = lambda *indices: ori_tensor(*indices[:-4], indices[-4] * block_in + indices[-2],
                                                                indices[-3] * block_reduce + indices[-1])
        res_fract = tvm.compute(
            tensor_matrix_shape,
            lambda_expression,
            name=compute_params.get("tensor_name"),
            attrs={"mode": mode_info}
        )
        if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
            res_matrix = tvm.compute(
                tensor_matrix_shape,
                lambda *indices: res_fract(*indices[:-4], indices[-4], 0, 0, indices[-1]),
                name="tensor_a_nz",
                attrs={"mode": mode_info}
            )
        else:
            res_matrix = tvm.compute(
                tensor_matrix_shape,
                lambda *indices: res_fract(*indices[:-4], 0, indices[-3], 0, indices[-1]),
                name="tensor_a_zz",
                attrs={"mode": mode_info}
            )
        return res_matrix

    @staticmethod
    def compute_nd2zn(ori_tensor, compute_params):
        """
        reshape nd2Zn by normal way
        input params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format Zn
        """
        tensor_name = compute_params.get("tensor_name")
        block_out = compute_params.get("block_out")
        block_reduce = compute_params.get("block_reduce")
        mode_info = compute_params.get("mode_info")
        trans = compute_params.get("trans")
        data_flow = compute_params.get("data_flow")
        int82int32_no_trans_flag = (data_flow == "int82int32") and (not trans)
        if int82int32_no_trans_flag:
            return FormatCompute._compute_nd2zn_int8_no_trans(ori_tensor, compute_params)

        if trans:
            fract_shape, lambda_expression = FormatCompute._lambda_nd2nz(ori_tensor, block_out, block_reduce)
        else:
            fract_shape, lambda_expression = FormatCompute._lambda_nd2zn(ori_tensor, block_reduce, block_out)

        res = tvm.compute(
            fract_shape,
            lambda_expression,
            name=tensor_name,
            attrs={"mode": mode_info}
        )

        return res

    @staticmethod
    def compute_nd2zn_vnchwconv(ori_tensor, compute_params):
        """
        reshape nd2Zn by vnchwconv [k, n] --> [k / block_reduce, n, block_reduce] -- >
        [k / block_reduce, n / block_out, block_out, block_reduce]
        input params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format is FRACTAL_Z
        """
        tensor_name = compute_params.get("tensor_name")
        block_out = compute_params.get("block_out")
        block_reduce = compute_params.get("block_reduce")
        trans = compute_params.get("trans")
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        tensor_fract_shape = ori_tensor_shape[:-2] + [
            ori_tensor_shape[-2] // block_reduce, ori_tensor_shape[-1] // block_out, block_out, block_reduce
        ]
        if not in_dynamic():
            tensor_fract_shape = ori_tensor_shape[:-2] + [
                ori_tensor_shape[-2] // block_reduce, ori_tensor_shape[-1], block_reduce
            ]

        fract_lambda = lambda *indices: ori_tensor(*indices[:-3], indices[-3]*block_reduce + indices[-1],
                                                   indices[-2])
        fract_lambda_unaligned = lambda *indices: ori_tensor(*indices[:-4], indices[-4]*block_reduce + indices[-1],
                                                             indices[-3]*block_out + indices[-2])
        tensor_fract = tvm.compute(
            tensor_fract_shape,
            fract_lambda if not in_dynamic() else fract_lambda_unaligned,
            name="{}_fract".format(tensor_name)
        )
        if trans:
            if not in_dynamic():
                tensor_matrix_shape = tensor_fract_shape[:-3] + [
                    tensor_fract_shape[-2] // block_out, tensor_fract_shape[-3], block_reduce, block_out
                ]
            else:
                tensor_matrix_shape = tensor_fract_shape[:-4] + [
                    tensor_fract_shape[-3], tensor_fract_shape[-4], block_reduce, block_out
                ]
            lambda_expression = lambda *indices: tensor_fract(*indices[:-4], indices[-3],
                                                              indices[-4]*block_reduce + indices[-1],
                                                              indices[-2])
            lambda_expression_unaligned = lambda *indices: tensor_fract(*indices[:-4], indices[-3], indices[-4],
                                                                        indices[-1], indices[-2])
        else:
            tensor_matrix_shape = tensor_fract_shape
            if not in_dynamic():
                tensor_matrix_shape = tensor_fract_shape[:-3] + [
                    tensor_fract_shape[-3], tensor_fract_shape[-2] // block_out, block_out, block_reduce
                ]
            lambda_expression = lambda *indices: tensor_fract(*indices[:-4], indices[-4],
                                                              indices[-3]*block_out + indices[-2],
                                                              indices[-1])
            lambda_expression_unaligned = lambda *indices: tensor_fract(*indices)
        tensor_matrix = tvm.compute(
            tensor_matrix_shape,
            lambda_expression if not in_dynamic() else lambda_expression_unaligned,
            name=tensor_name,
            attrs={"mode": "nd2Zn_vnchwconv"}
        )
        return tensor_matrix

    @staticmethod
    def compute_nd2nz(ori_tensor, compute_params):
        """
        reshape ori_shape's shape nd2Nz by normal way
        input_params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format is FRACTAL_Z
        """
        block_in = compute_params.get("block_in")
        block_reduce = compute_params.get("block_reduce")
        tensor_name = compute_params.get("tensor_name")
        trans = compute_params.get("trans")
        mode_info = compute_params.get("mode_info")
        if not trans:
            fract_shape, lambda_expression = FormatCompute._lambda_nd2nz(ori_tensor, block_in, block_reduce)
        else:
            fract_shape, lambda_expression = FormatCompute._lambda_nd2zn(ori_tensor, block_in, block_reduce)

        res = tvm.compute(
            fract_shape,
            lambda_expression,
            name=tensor_name,
            attrs={"mode": mode_info}
        )

        return res

    @staticmethod
    def compute_nz2zz_int82fp32(ori_tensor, compute_params):
        """
        reshape Nz2Zz and int8 cast to fp32
        input_params:
            ori_tensor: the tensor format is Zz ,dtype is int8
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format is FRACTAL_Z
        """
        tensor_name = compute_params.get("tensor_name")
        trans = compute_params.get("trans")
        mode_info = compute_params.get("mode_info")
        ori_shape = shape_util.shape_to_list(ori_tensor.shape)
        if not trans:
            tensor_a_normalize_shape = ori_shape[:-4] + [
                ori_shape[-3],
                ori_shape[-4] * 2,
                ori_shape[-2],
                ori_shape[-1] // 2
            ]
            lambda_expression = lambda *indices: ori_tensor(*indices[:-4], indices[-3] // 2,
                                                            indices[-4], indices[-2],
                                                            (indices[-3]*16 + indices[-1]) % 32)
        else:
            tensor_a_normalize_shape = ori_shape[:-4] + [
                ori_shape[-4] * 2,
                ori_shape[-3],
                ori_shape[-2],
                ori_shape[-1] // 2
            ]
            lambda_expression = lambda *indices: ori_tensor(*indices[:-4], indices[-4] // 2,
                                                            indices[-3], indices[-2],
                                                            (indices[-4]*16 + indices[-1]) % 32)
        res = tvm.compute(
            tensor_a_normalize_shape,
            lambda_expression,
            name=tensor_name,
            attrs={"mode": mode_info}
        )

        return res

    @staticmethod
    def compute_zn2zn_int82fp32(ori_tensor, compute_params):
        """
        reshape Zn-int8 to Zn-fp16
        input_params:
            ori_tensor: the tensor format is Zn ,dtype is int8
            compute_params: dict, the info need trans to schedule
        return:
            the tensor is Zn-fp16
        """
        tensor_name = compute_params.get("tensor_name")
        trans = compute_params.get("trans")
        mode_info = compute_params.get("mode_info")
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        if not trans:
            tensor_normalize_shape = ori_tensor_shape[:-4] + [
                ori_tensor_shape[-4] * 2,
                ori_tensor_shape[-3],
                ori_tensor_shape[-2],
                ori_tensor_shape[-1] // 2
            ]
            lambda_expression = lambda *indices: ori_tensor(*indices[:-4], indices[-4] // 2,
                                                            indices[-3], indices[-2],
                                                            (indices[-4]*16 + indices[-1]) % 32)
        else:
            tensor_normalize_shape = ori_tensor_shape[:-4] + [
                ori_tensor_shape[-3],
                ori_tensor_shape[-4] * 2,
                ori_tensor_shape[-1] // 2,
                ori_tensor_shape[-2]
            ]
            lambda_expression = lambda *indices: ori_tensor(*indices[:-4], indices[-3] // 2,
                                                            indices[-4], indices[-1],
                                                            (indices[-3]*16 + indices[-2]) % 32)
        res = tvm.compute(
            tensor_normalize_shape,
            lambda_expression,
            name=tensor_name,
            attrs={"mode": mode_info}
        )

        return res

    @staticmethod
    def compute_nz2nd(ori_tensor, output_shape=None, tensor_name="tensor_nz2nd", res_tag="", attrs_dict=None):
        """
        reshape the ori_tensor Nz to nd
        input params:
            ori_tensor: the tensor format is Nz
            output_shape: the out tensor's shape
            tensor_name: the tensor's name after reshape in IR
            res_tag: the tag used in the out tensor
            attrs_dict: dict, the info need trans to schedule
        return:
            the format is ND tensor
        """
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        block_out = ori_tensor_shape[-1]
        block_in = ori_tensor_shape[-2]
        shapes = ori_tensor_shape[:-4] + [
            ori_tensor_shape[-3]*block_in, ori_tensor_shape[-4]*block_out
        ]
        if in_dynamic() and tensor_name == "tensor_c_gm":
            shape_trans = ori_tensor_shape[:-4] + [
                ori_tensor_shape[-3], block_in, ori_tensor_shape[-4], block_out
            ]
            before_c_gm = tvm.compute(shape_trans,
                                      lambda *indices: ori_tensor(*indices[:-4],
                                                                  indices[-2], indices[-4],
                                                                  indices[-3], indices[-1]),
                                      name="before_c_gm")
            lambda_expression = lambda *indices: before_c_gm(*indices[:-2],
                                                             indices[-2] // block_in,
                                                             indices[-2] % block_in,
                                                             indices[-1] // block_out,
                                                             indices[-1] % block_out)
        else:
            lambda_expression = lambda *indices: ori_tensor(*indices[:-2],
                                                            indices[-1] // block_out,
                                                            indices[-2] // block_in,
                                                            indices[-2] % block_in,
                                                            indices[-1] % block_out)
        if output_shape is not None:
            shapes = output_shape
        if attrs_dict is None:
            attrs_dict = dict()
        attrs_dict["not_reuse_pre_tensors"] = True
        if res_tag != "":
            tensor_c = tvm.compute(shapes, lambda_expression, name=tensor_name, tag=res_tag, attrs=attrs_dict)
        else:
            tensor_c = tvm.compute(shapes, lambda_expression, name=tensor_name, attrs=attrs_dict)
        return tensor_c

    @staticmethod
    def tvm_compute_nd_add_nz_to_nd(tensor_beta_bias, tensor_alpha_c, tensor_name):
        """
        compute C_matrix = A_matrix + B_matrix, A_matrix's format is ND, B_matrix's format is
        Nz, the result C_matrix's format is ND. support batch
        input args:
            tensor_beta_bias: the tensor with ND format
            tensor_alpha_c: the tensor with Nz format
            tensor_name: the result tensor's name in IR
        return:
            res: the add result, format is ND
        """
        res_shape = tensor_beta_bias.shape
        if len(res_shape) == 2:
            lambda_expression = lambda i, j: (tensor_beta_bias[i, j]
                + tensor_alpha_c[j // 16, i // 16, i % 16, j % 16])
        else:
            lambda_expression = lambda batch, i, j: (tensor_beta_bias[batch, i, j]
                + tensor_alpha_c[batch, j // 16, i // 16, i % 16, j % 16])

        res = tvm.compute(res_shape, lambda_expression, name=tensor_name)
        return res

    @staticmethod
    def tvm_compute_nd_add_nz_to_nz(tensor_beta_bias, tensor_alpha_c, tensor_name):
        """
        compute C_matrix = A_matrix + B_matrix, A_matrix's format is ND, B_matrix's format is
        Nz, the result C_matrix's format is Nz. support batch
        input args:
            tensor_beta_bias: the tensor with ND format
            tensor_alpha_c: the tensor with Nz format
            tensor_name: the result tensor's name in IR
        return:
            res: the add result, format is Nz
        """
        res_shape = tensor_alpha_c.shape
        block_in = res_shape[-2]
        block_out = res_shape[-1]
        if len(res_shape) == 4:
            lambda_expression = lambda i, j, k, l: tensor_beta_bias[j * block_in + k, i * block_out + l]\
                + tensor_alpha_c[i, j, k, l]
        else:
            lambda_expression = lambda batch, i, j, k, l: tensor_beta_bias[batch, j * block_in + k, i * block_out + l]\
                + tensor_alpha_c[batch, i, j, k, l]

        res = tvm.compute(res_shape, lambda_expression, name=tensor_name)
        return res

    @staticmethod
    def compute_nd2zz_vnchwconv(ori_tensor, compute_params):
        """
        use vnchwconv to reshape nd shape: (m, k) --> (m/m0, k, m0) --> (m/m0, k/k0, m0, k0)
        inpute params:
            ori_tensor: tensor, format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format is FRACTAL_Z
        """
        tensor_name = compute_params.get("tensor_name")
        block_in = compute_params.get("block_in")
        block_reduce = compute_params.get("block_reduce")
        trans = compute_params.get("trans")
        mode_info = compute_params.get("mode_info")
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        tensor_fract_k_shape = ori_tensor_shape[:-2] + [
            ori_tensor_shape[-2] // block_in, ori_tensor_shape[-1] // block_reduce, block_reduce, block_in
        ]
        if not in_dynamic():
            tensor_fract_k_shape = ori_tensor_shape[:-2] + [
                int_ceil_div(ori_tensor_shape[-2], block_in), ori_tensor_shape[-1], block_in
            ]
        fract_lambda = lambda *indices: ori_tensor(*indices[:-3], indices[-3]*block_in + indices[-1], indices[-2])
        fract_lambda_unaligned = lambda *indices: ori_tensor(*indices[:-4], indices[-4]*block_in + indices[-1],
                                                             indices[-3]*block_reduce + indices[-2])
        tensor_fract_k = tvm.compute(
            tensor_fract_k_shape,
            fract_lambda if not in_dynamic() else fract_lambda_unaligned,
            name="{}_fract_k".format(tensor_name)
        )
        if trans:
            if not in_dynamic():
                if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                    tensor_matrix_shape = tensor_fract_k_shape[:-3] + [
                        tensor_fract_k_shape[-3], int_ceil_div(tensor_fract_k_shape[-2], block_reduce), block_reduce,
                        block_in
                    ] # [m1, k1, k0, m0]
                else:
                    tensor_matrix_shape = tensor_fract_k_shape[:-3] + [
                        int_ceil_div(tensor_fract_k_shape[-2], block_reduce), tensor_fract_k_shape[-3], block_reduce,
                        block_in
                    ] # [k1, m1, k0, m0]
            else: 
                tensor_matrix_shape = tensor_fract_k_shape[:-4] + [
                    tensor_fract_k_shape[-3], tensor_fract_k_shape[-4], block_reduce, block_in
                ]
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                # static [m1, k, m0] -> [m1, k1, k0, m0]
                lambda_expression = lambda *indices: tensor_fract_k(*indices[:-4], indices[-4],
                                                                    indices[-3] * block_in + indices[-2],
                                                                    indices[-1])
            else:
                # static [m1, k, m0] -> [k1, m1, k0, m0]
                lambda_expression = lambda *indices: tensor_fract_k(*indices[:-4], indices[-3],
                                                                    indices[-4] * block_in + indices[-2],
                                                                    indices[-1])
            # dynamic branch
            lambda_expression_unaligned = lambda *indices: tensor_fract_k(*indices[:-4], indices[-3],
                                                                          indices[-4], indices[-2], indices[-1])
        else:
            if not in_dynamic():
                if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                    tensor_matrix_shape = tensor_fract_k_shape[:-3] + [
                        int_ceil_div(tensor_fract_k_shape[-2], block_reduce), tensor_fract_k_shape[-3],
                        block_in, block_reduce
                    ] # [k1, m1, m0, k0]
                else:
                    tensor_matrix_shape = tensor_fract_k_shape[:-3] + [
                        tensor_fract_k_shape[-3],
                        int_ceil_div(tensor_fract_k_shape[-2], block_reduce), block_in, block_reduce
                    ] # [m1, k1, m0, k0]
            else:
                tensor_matrix_shape = tensor_fract_k_shape[:-4] + [
                    tensor_fract_k_shape[-4], tensor_fract_k_shape[-3], block_in, block_reduce
                ]
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                # [m1, k, m0] -> [k1, m1, m0, k0]
                lambda_expression = lambda *indices: tensor_fract_k(*indices[:-4], indices[-3],
                                                                    indices[-4] * block_reduce + indices[-1],
                                                                    indices[-2])
            else:
                # [m1, k, m0] -> [m1, k1, m0, k0]
                lambda_expression = lambda *indices: tensor_fract_k(*indices[:-4], indices[-4],
                                                                    indices[-3] * block_reduce + indices[-1],
                                                                    indices[-2])
            lambda_expression_unaligned = lambda *indices: tensor_fract_k(*indices[:-4], indices[-4],
                                                                          indices[-3], indices[-1], indices[-2])
        if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
            tensor_name = "tensor_a_nz"
        res = tvm.compute(
            tensor_matrix_shape,
            lambda_expression if not in_dynamic() else lambda_expression_unaligned,
            name=tensor_name,
            attrs={"mode": mode_info}
        )
        return res

    @staticmethod
    def get_batch_index(idx_reduce, list_batch_broad, list_batch_ori):
        """
        get index in batch_ori
        e.g. A x B = C
            batch_A = [B1, 1, 1, B4]
            batch_B = [B2, B3, 1]
            batch_C = [B1, B2, B3, B4]

            idx_reduce is in range of 0 ~ B1*B2*B3*B4-1,
            b1 = idx_reduce // (B2*B3*B4)
            b2 = idx_reduce // (B3*B4) % B2
            b3 = idx_reduce // (B4) % B3
            b4 = idx_reduce // (1) % B4

            idx_a = B4*b1 + b4,
            idx_b = B3*b2 + b3
        """
        if list_batch_broad == list_batch_ori:
            return idx_reduce
        idx_ori = 0
        for idx, (val_broad, val_ori) in enumerate(zip(list_batch_broad, list_batch_ori)):
            # [val_ori != 1] means current index has offset val
            if val_ori != 1:
                # Using "list + [1]" in case of reduce blank list
                rest_length_broad = functools_reduce(lambda x, y: x * y, list_batch_broad[idx + 1:] + [1])
                rest_length_ori = functools_reduce(lambda x, y: x * y, list_batch_ori[idx + 1:] + [1])
                current_offset = idx_reduce // rest_length_broad
                if idx != 0:
                    current_offset = current_offset % val_ori
                idx_ori += current_offset * rest_length_ori * (val_ori // val_broad)
        return idx_ori

    @staticmethod
    def _get_var_name(tensor_format, var_name, cache_tiling_flag):
        ori_flag = (tensor_format == "ND" and not cache_tiling_flag) or (var_name == "k" and cache_tiling_flag)
        if ori_flag:
            var_name += "_ori"
        return var_name

    @staticmethod
    def _lambda_nd2nz(ori_tensor, block_in, block_reduce):
        """
        the express of nd format trans to Nz format. support batch
        input args:
            ori_tensor: the tensor need trans to Nz
            block_in: the block_in of chip
            block_reduce: the block_reduce of chip
        return:
            fract_shape: the shape after reshape
            lambda_expression the expression of trans nd to nz
        """
        ori_nd_shape = shape_util.shape_to_list(ori_tensor.shape)
        fract_shape = ori_nd_shape[:-2] + [
            int_ceil_div(ori_nd_shape[-1], block_reduce),
            int_ceil_div(ori_nd_shape[-2], block_in),
            block_in,
            block_reduce
        ]
        if len(ori_nd_shape) == 3:
            lambda_expression = lambda batch, k1, n1, n0, k0: ori_tensor[batch, n1 * block_in + n0,
                                k1 * block_reduce + k0]
        else:
            lambda_expression = lambda k1, n1, n0, k0: ori_tensor[n1 * block_in + n0, k1 * block_reduce + k0]
        return fract_shape, lambda_expression

    @staticmethod
    def _lambda_nd2zn(ori_tensor, block_in, block_reduce):
        """
        the express of nd format trans to Zz format. support batch
        input args:
            ori_tensor: the tensor need trans to Nz
            block_in: the block_in of chip
            block_reduce: the block_reduce of chip
        return:
            fract_shape: the shape after reshape
            lambda_expression the expression of trans nd to nz
        """
        ori_nd_shape = shape_util.shape_to_list(ori_tensor.shape)
        fract_shape = ori_nd_shape[:-2] + [
            int_ceil_div(ori_nd_shape[-2], block_reduce),
            int_ceil_div(ori_nd_shape[-1], block_in),
            block_in,
            block_reduce
        ]
        fract_l1_shape = fract_shape[:-2] + [block_reduce, block_in]
        if len(ori_nd_shape) == 3:
            tensor_l1 = tvm.compute(fract_l1_shape, lambda batch, k1, n1, k0, n0:
                                    ori_tensor[batch, k1 * block_reduce + k0, n1 * block_in + n0],
                                    name="tensor_b_l1")
            lambda_expression = lambda batch, k1, n1, n0, k0: tensor_l1[batch, k1, n1, k0, n0]
        else:
            tensor_l1 = tvm.compute(fract_l1_shape, lambda k1, n1, k0, n0:
                                    ori_tensor[k1 * block_reduce + k0, n1 * block_in + n0],
                                    name="tensor_b_l1")
            lambda_expression = lambda k1, n1, n0, k0: tensor_l1[k1, n1, k0, n0]

        return fract_shape, lambda_expression

    @staticmethod
    def _compute_nd2zz_int8_trans(ori_tensor, compute_params):
        """
        this func is reshape nd2Zz when dtype is int8 and trans and ori_shape is ([m, k]
        input params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format is FRACTAL_Z
        """
        block_in = compute_params.get("block_in")
        block_reduce = compute_params.get("block_reduce")
        mode_info = compute_params.get("mode_info")
        ori_tensor_shape = shape_util.shape_to_list(ori_tensor.shape)
        ori_tensor_shape[-1], ori_tensor_shape[-2] = ori_tensor_shape[-2], ori_tensor_shape[-1]
        tensor_matrix_shape = ori_tensor_shape[:-2] + [
            ori_tensor_shape[-2] // block_in,
            ori_tensor_shape[-1] // block_reduce,
            block_in,
            block_reduce
        ]
        tensor_a_transpose = tvm.compute(
            ori_tensor_shape,
            lambda *indices: ori_tensor(*indices[:-2], indices[-1], indices[-2]),
            name="tensor_a_transpose")
        lambda_expression = lambda *indices: tensor_a_transpose(*indices[:-4],
                                                                indices[-4]*block_in + indices[-2],
                                                                indices[-3]*block_reduce + indices[-1])
        res = tvm.compute(
            tensor_matrix_shape,
            lambda_expression,
            name="tensor_a_zz",
            attrs={"mode": mode_info})

        return res

    @staticmethod
    def _compute_nd2zn_int8_no_trans(ori_tensor, compute_params):
        """
        this func is reshape nd2Zn when dtype is int8 and not trans and ori_shape is [k, n]
        input params:
            ori_tensor: the tensor format is nd
            compute_params: dict, the info need trans to schedule
        return:
            the tensor format is FRACTAL_Z
        """
        block_out = compute_params.get("block_out")
        block_reduce = compute_params.get("block_reduce")
        mode_info = compute_params.get("mode_info")

        normalize_shape = shape_util.shape_to_list(ori_tensor.shape)
        normalize_shape[-1], normalize_shape[-2] = normalize_shape[-2], normalize_shape[-1]
        tensor_fract_shape = normalize_shape[:-2] + [
            normalize_shape[-1] // block_reduce,
            normalize_shape[-2] // block_out,
            block_out,
            block_reduce
        ]
        tensor_transpose = tvm.compute(
            normalize_shape,
            lambda *indices: ori_tensor(*indices[:-2], indices[-1], indices[-2]),
            name="tensor_b_transpose"
        )
        res = tvm.compute(
            tensor_fract_shape,
            lambda *indices: tensor_transpose(*indices[:-4],
                                              indices[-3]*block_out + indices[-2],
                                              indices[-4]*block_reduce + indices[-1]),
            name="tensor_b_zn",
            attrs={"mode": mode_info}
        )
        return res

    @staticmethod
    def _get_batch_broadcast(tensor_a, tensor_b):
        shape_tensor_a = shape_util.shape_to_list(tensor_a.shape)
        shape_tensor_b = shape_util.shape_to_list(tensor_b.shape)
        list_batch_a = shape_tensor_a[:-4]
        list_batch_b = shape_tensor_b[:-4]
        if "ori_batch_shape" in tensor_a.op.attrs and "ori_batch_shape" in tensor_b.op.attrs:
            list_batch_a = shape_util.shape_to_list(tensor_a.op.attrs["ori_batch_shape"])
            list_batch_b = shape_util.shape_to_list(tensor_b.op.attrs["ori_batch_shape"])
            if get_te_var("batch_c1"):
                list_batch_broad = [get_te_var("batch_c1").get_tvm_var(), get_te_var("batch_c2").get_tvm_var(),
                                    get_te_var("batch_c3").get_tvm_var(), get_te_var("batch_c4").get_tvm_var()]
                # the max batch dim is 4
                return list_batch_a, list_batch_b, list_batch_broad[4 - max(len(list_batch_a), len(list_batch_b)):]
        if list_batch_a or list_batch_b:
            list_batch_a, list_batch_b, list_batch_broad = broadcast_shapes(list_batch_a, list_batch_b)
        else:
            # in case of bug in broadcast
            list_batch_broad = []
        return list_batch_a, list_batch_b, list_batch_broad

    def compute_mad(self, tensor_list, mmad_args, **kwargs):
        # shape length is 5 means have batch
        len_with_batch = 5
        tensor_a, tensor_b, tensor_bias_in_mmad = tensor_list
        offset_a, dtype_mmad, shape_k_ori, shape_m_ori, shape_n_ori, format_out, trans_a, trans_b = mmad_args
        attrs_dict = kwargs.get("attrs")
        sparse_4to2_flag = attrs_dict.get("sparse_4to2_flag", False)
        # shape info
        shape_tensor_a = shape_util.shape_to_list(tensor_a.shape)
        shape_tensor_b = shape_util.shape_to_list(tensor_b.shape)
        shape_m1, shape_k1, shape_m0, shape_k0 = shape_tensor_a[-4:]
        _, shape_n1, shape_n0, _ = shape_tensor_b[-4:]
        if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
            shape_k1, shape_m1, shape_m0, shape_k0 = shape_tensor_a[-4:]
        # batch info
        list_batch_a, list_batch_b, list_batch_broad = self._get_batch_broadcast(tensor_a, tensor_b)
        batch_reduce = [functools_reduce(lambda x, y: x * y, list_batch_broad)] if list_batch_broad else []
        # shape mmad
        shape_m0 = tbe_platform.BLOCK_IN if shape_m0 == tbe_platform.BLOCK_VECTOR else shape_m0
        shape_n0 = tbe_platform.BLOCK_OUT if shape_n0 == tbe_platform.BLOCK_VECTOR else shape_n0
        shape_mmad = batch_reduce + [shape_n1, shape_m1, shape_m0, shape_n0]
        # reduce axis
        k1 = tvm.reduce_axis((0, shape_k1), name="k1")
        k0 = tvm.reduce_axis((0, shape_k0), name="k0")

        def __mmad_kernel_func(indices_a, indices_b):
            m1_idx, _, m0_idx, _ = indices_a[-4:]
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                _, m1_idx, m0_idx, _ = indices_a[-4:]
            k1_idx, n1_idx, n0_idx, k0_idx = indices_b[-4:]
            a_expr = tensor_a(*indices_a) - offset_a
            b_expr = tensor_b(*indices_b) if not sparse_4to2_flag else tensor_b(k1_idx // 2, n1_idx, n0_idx, k0_idx)
            mad_expr = (a_expr * b_expr).astype(dtype_mmad)
            mad_sum_func = tvm.mad_sp if sparse_4to2_flag else tvm.sum
            if tensor_bias_in_mmad is not None:
                bias_index = (n1_idx * shape_n0 + n0_idx, ) if len(tensor_bias_in_mmad.shape) == 1 else (n1_idx, n0_idx)
                mad_expr = mad_expr + tensor_bias_in_mmad(*bias_index)
            if shape_k_ori != 0:
                cond_k = k1_idx.var * shape_k0 + k0_idx.var < shape_k_ori
                cond_m = m1_idx * shape_m0 + m0_idx < shape_m_ori
                cond_n = n1_idx * shape_n0 + n0_idx < shape_n_ori
                if shape_m_ori != 0 and shape_n_ori != 0 and format_out == "ND":
                    conditions = [cond_k, cond_m, cond_n]
                    # if m0 is 32 in L1, m1 in L0 may large than ceil(m_ori, 16), mmad not support use origin m shape
                    if dtype_mmad == "int32" and trans_a:
                        conditions.remove(cond_m)
                    if dtype_mmad == "int32" and not trans_b:
                        conditions.remove(cond_n)
                else:
                    conditions = [cond_k]
                # Mad support origin m/n/k shape in ND output
                mad_expr = tvm.select(tvm.all(*conditions), mad_expr)
            sum_expr = mad_sum_func(mad_expr, axis=[k1, k0])
            return sum_expr

        def __mmad_none_batch(n1, m1, m0, n0):
            indices_a = [m1, k1, m0, k0]
            indices_b = [k1, n1, n0, k0]
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                indices_a = [k1, m1, m0, k0]
                indices_b = [k1, n1, n0, k0]
            return __mmad_kernel_func(indices_a, indices_b)

        def __mmad_with_batch(batch, n1, m1, m0, n0):
            batch_prefix_a = batch_prefix_b = []
            if len(shape_tensor_a) == len_with_batch:
                batch_prefix_a = [self.get_batch_index(batch, list_batch_broad, list_batch_a)]
            if len(shape_tensor_b) == len_with_batch:
                batch_prefix_b = [self.get_batch_index(batch, list_batch_broad, list_batch_b)]
            indices_a = batch_prefix_a + [m1, k1, m0, k0]
            indices_b = batch_prefix_b + [k1, n1, n0, k0]
            if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
                indices_a = batch_prefix_a + [k1, m1, m0, k0]
                indices_b = batch_prefix_b + [k1, n1, n0, k0]
            return __mmad_kernel_func(indices_a, indices_b)

        __mmad_func = __mmad_with_batch if len(shape_mmad) == len_with_batch else __mmad_none_batch
        tensor_mmad = tvm.compute(shape_mmad, __mmad_func, **kwargs)
        return tensor_mmad


class GEMMComputeParam:
    """
    be used by gemm_tilingcase
    """
    def __init__(self):
        self.tiling_info_dict = {}
        self.dynamic_mode = None
        self.batch_a = False
        self.batch_b = False
        self.format_a = "Fractal_NZ"
        self.format_b = "Fractal_NZ"
        self.format_out = "Fractal_NZ"
        self.out_dtype = "float16"
        self.batch_var_name = None
        self.m_var_name = None
        self.k_var_name = None
        self.n_var_name = None
        self.k_ori_var_name = None
        self.block_in = tbe_platform.BLOCK_IN
        self.block_out = tbe_platform.BLOCK_OUT
        self.block_reduce = tbe_platform.BLOCK_REDUCE
        self.split_k_flag = False
        self.need_aub = False
        self.need_bub = False
        self.sparse_4to2_flag = False
        self.pre_conv_mode = None
        self.pad_flag = 0
        self.deq_vec_flag = False
        self.nz_fusion_flag = 0

    @staticmethod
    def get_block_reduce(ops_data_flow_mode):
        block_reduce_dict = {
            "int82int32": tbe_platform.BLOCK_REDUCE_INT8,
            "int42int32": tbe_platform.BLOCK_REDUCE_INT4,
            "fp162fp32": tbe_platform.BLOCK_REDUCE,
            "fp162fp16": tbe_platform.BLOCK_REDUCE,
            "int82fp32": tbe_platform.BLOCK_REDUCE,
            "bf162bf16": tbe_platform.BLOCK_REDUCE,
            "fp322fp32": tbe_platform.CUBE_MKN.get("float32").get("mac")[1],
            "fp162int8": tbe_platform.BLOCK_REDUCE,
        }
        block_reduce = block_reduce_dict.get(ops_data_flow_mode)
        return block_reduce

    def update_dynamic_para(self, tiling_info_dict, tensor_map, para_map):
        self.tiling_info_dict = tiling_info_dict
        self.format_a = para_map.get("format_a")
        self.format_b = para_map.get("format_b")
        self.format_out = para_map.get("format_out")
        self.sparse_4to2_flag = bool(para_map.get("alg") == "weight_sparse_4_2")
        self.split_k_flag = bool(para_map.get("split_k"))
        self.batch_var_name = "batch"
        self.m_var_name = "m"
        self.n_var_name = "n"
        self.k_var_name = "k"
        self.k_ori_var_name = "k_ori"
        self.block_in = tbe_platform.BLOCK_IN
        self.block_out = tbe_platform.BLOCK_OUT
        self.block_reduce = tbe_platform.BLOCK_REDUCE
        if tensor_map.get("a_placehold") is not None:
            self.batch_a = len(tensor_map["a_placehold"].shape) in (BMM_LEN_ND, BMM_LEN_NZ)
        else:
            self.batch_a = len(tensor_map.get("a_l0a").shape) == BMM_LEN_NZ
        if tensor_map.get("b_placehold") is not None:
            self.batch_b = len(tensor_map["b_placehold"].shape) in (BMM_LEN_ND, BMM_LEN_NZ)
        else:

            self.batch_b = len(tensor_map.get("b_l0b").shape) == BMM_LEN_NZ
        if self.batch_a or self.batch_b:
            self.dynamic_mode = "dynamic_mknb"
        else:
            self.dynamic_mode = "dynamic_mkn"
        if para_map.get("pre_conv_mode", None) is not None:
            self.pre_conv_mode = para_map.get("pre_conv_mode")
        if para_map.get("deq_vec_flag", False):
            self.deq_vec_flag = para_map.get("deq_vec_flag")
        self.need_aub = para_map.get("need_aub")
        self.need_bub = para_map.get("need_bub")
        self.pad_flag = para_map.get("pad_flag", 0)
        self.nz_fusion_flag = para_map.get("nz_fusion_flag", 0)
