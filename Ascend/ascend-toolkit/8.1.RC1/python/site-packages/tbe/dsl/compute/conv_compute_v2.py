#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Compute of Conv2d in v220.
"""
from __future__ import division
import math
from enum import Enum
from tbe import tvm
from tbe.dsl.compute.conv_compute_wino import conv_wino_compute
from tbe.common.platform import CUBE_MKN
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.common.utils import log
from tbe.common.utils.op_util import op_util_conv2d
from tbe.common.utils.op_util.op_util_conv2d import Conv2dTensorName
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe.common.utils.op_util.op_util_conv2d import is_support_nd2nz
from tbe.common.utils.op_util.op_util_conv2d import is_l0a_layout_zn
from tbe.common.utils.op_util.op_util_conv2d import get_binary_infos
from tbe.common.utils.op_util.op_util_conv2d import BinaryInfoKey
from tbe.common.utils.op_util.op_util_conv2d import get_cur_soc
from tbe.common.utils.op_util.op_util_conv2d import get_min_al1_ci1
from tbe.common.utils.op_util.op_util_conv2d import BINARY_LOAD_MODES
from tbe.common.utils.op_util.op_util_conv2d import BIT_RATIO_MAP
from tbe.common.utils.op_util.op_util_conv2d import ConstValue
from tbe.common.utils.op_util.op_util_conv2d import C0_BYTE_LEN
from tbe.common.utils.op_util.op_util_conv2d import check_load3dv2_postk_params_invalid
from tbe.common.utils.op_util.op_util_conv2d import Conv2DL0aDmaScene

OP_TAG = "convolution_"


def ceil_div(num_a, num_b):
    """
    Do upper division.
    """
    if num_b == 0:
        err_man.raise_err_specific("conv2d", "num_b == 0")
    return (num_a + num_b - 1) // num_b


class ConvType(Enum):
    """
    Enum class of conv dtype.
    """
    FP16 = 1
    INT8 = 2
    FP32 = 3
    BF16 = 4
    INT4 = 5


def conv_v220_compute(fmap, weight, para_dict, optim_dict, dsl_flag, conv_param):
    """
    Compute of conv2d in v220.
    """
    def get_input_nd_flag():
        # input_nd_flag
        if 'format' not in fmap.op.attrs:
            log.debug("can not find the format attr in fmap tensor, get input nd flag set False")
            return False
        return fmap.op.attrs['format'] == "NHWC"

    def get_input_nd_flag_mode():
        """
        get input_nd_flag and mode, support NCHW, NHWC
        """
        _input_nd_flag = get_input_nd_flag()
        _input_nd_mode = None
        if _input_nd_flag:
            _input_nd_mode = "NHWC"
        return _input_nd_flag, _input_nd_mode

    def get_fmpload_mode(mode):
        load_mode_vale = [value for key, value in optim_dict.items() if key in BINARY_LOAD_MODES]
        if load_mode_vale != [] and sum(load_mode_vale) != 1:
            err_man.raise_err_message_cube("only support one load mode!")
        return optim_dict.get(mode, False)

    def get_binary_feature_flag(feature_flag):
        if feature_flag not in op_util_conv2d.BINARY_FEATURE_FLAG_OFFSET_MAP.keys():
            err_man.raise_err_message_cube("{} is not supported for binary".format(feature_flag))
        return optim_dict.get(feature_flag, False)

    def get_load2d_flag():
        if conv_param.binary_mode:
            return get_fmpload_mode(BinaryInfoKey.LOAD2D_FLAG)

        if lxfusion_para["l1_fusion_type"] in (0, 1) or lxfusion_para["input_memory_type"][0] == 1:
            return False

        if (fmap.op.tag == "NHWC_trans_5HD" and not is_support_fixpipe()) or fmap.op.tag == "NCHW_trans_5HD":
            return False

        # load2d not support splitw, when l1 size can not load when not cut w, load2d is disable
        if check_l1_size_invalid_split_w():
            return False

        if padding == (0, 0, 0, 0) and stride == (1, 1) and kernel == (1, 1) and conv_param.h_in != 1 and \
            weight_dtype in ("float16", "bfloat16"):
            return True

        return False

    def get_groupopt_flag():
        if conv_param.binary_mode:
            return get_binary_feature_flag(BinaryInfoKey.GROUPOPT_FLAG)

        group_flag = True
        if para_dict.get("group_opt") == 1:
            group_flag = False

        return group_flag

    def get_nx1_flag():
        if conv_param.binary_mode:
            return get_binary_feature_flag(BinaryInfoKey.Nx1_FLAG)

        return False

    def get_conv1d_split_w_flag():
        if conv_param.binary_mode:
            return get_binary_feature_flag(BinaryInfoKey.CONV1D_FLAG)
        if conv_param.h_in == 1 and kernel_h == 1 and (pad_top + pad_bottom) == 0 and \
            not l0a_load2d_flag and not l0a_dma_flag:
            return True
        return False

    def get_splitw_flag_for_conv1d():
        # only static case has this flag for get tiling from tiling space
        if not conv_param.binary_mode:
            return conv1d_split_w_flag and check_l1_size_invalid_split_w()
        return False

    def get_al0boundcheck_flag():
        if conv_param.binary_mode:
            return get_binary_feature_flag(BinaryInfoKey.AL0BoundCheck_Flag)

        return False

    def get_available_l1_size():
        """
        get available l1 size for fm
        """
        l1_size = get_soc_spec("L1_SIZE")

        if not is_support_fixpipe():
            return l1_size
        if op_util_conv2d.is_support_v300():
            extra_buffers = op_util_conv2d.EXTRA_BUFFERS_V300
        else:
            extra_buffers = op_util_conv2d.EXTRA_BUFFERS_V220
        for buffer in extra_buffers:
            l1_size -= int(get_soc_spec(buffer))
        if sparse_4to2_flag:
            n1_bl1 = 2  # 2:  when res_dtype is int8, need 2 c0 to do channelmerge
            # to be fix in tiling later, minimum bl1 is k0*n0
            weight_l1_size = kernel_h * kernel_w * n1_bl1 * block_n0 * in_c0 * BIT_RATIO_MAP.get(weight_dtype)
            l1_size -= weight_l1_size + weight_l1_size//4  # 4: index size is a quarter of weight
            if l1_size <= 0:
                err_man.raise_err_specific("conv2d", "weight size is too large for sparse_4to2.")
        return l1_size

    def check_l1_size_invalid_split_w():
        """
        check l1 size valid in splitw scenario
        """
        temp_in_c0 = in_c0
        if c04_flag:
            temp_in_c0 = 4
        h_out_tmp = math.floor(CUBE_MKN[weight_dtype]['mac'][0] / out_width) + 2  # 2: extra lines loads in l1
        tmp = min(in_height, ((int(h_out_tmp) - 1) * stride_h + filter_h_dilation)) * in_width
        min_fmap_l1_size = get_min_al1_ci1(sparse_4to2_flag, in_c1, kernel_h, kernel_w) * \
                           temp_in_c0 * tmp * BIT_RATIO_MAP.get(weight_dtype)
        fm_available_l1_size = get_available_l1_size()
        if min_fmap_l1_size > fm_available_l1_size:
            return True
        return False

    def check_l1_size_invalid_dma():
        """
        check l1 size valid in dma scenario
        """
        ho_upper = ceil_div(CUBE_MKN[weight_dtype]['mac'][0], out_width)
        hi_infer_by_ho = min(in_height, ((ho_upper - 1) * stride_h + filter_h_dilation))
        wi_infer_by_wo = min(in_width, ((CUBE_MKN[weight_dtype]['mac'][0] - 1) * stride_w + filter_w_dilation))
        min_fmap_l1_size = get_min_al1_ci1(sparse_4to2_flag, in_c1, kernel_h, kernel_w) * \
                           in_c0 * hi_infer_by_ho * wi_infer_by_wo * BIT_RATIO_MAP.get(weight_dtype)
        fm_available_l1_size = get_available_l1_size()
        if min_fmap_l1_size > fm_available_l1_size:
            return True
        return False

    def check_l1_size_invalid_dma_split_k1():
        """
        check l1 size valid in dma split k1 scenario
        """
        min_fmap_l1_size_split_k1 = get_min_al1_ci1(sparse_4to2_flag, in_c1, kernel_h, kernel_w) * \
                                    kernel_h * kernel_w * block_m0 * block_k0 * BIT_RATIO_MAP.get(weight_dtype)
        fm_available_l1_size = get_available_l1_size()
        if min_fmap_l1_size_split_k1 > fm_available_l1_size:
            return True
        return False

    def check_load3dv2_attr_invalid():
        """
        check load3d insn invalid attr, use dma instead
        """
        invalid_attr_list = [
            kernel_h > ConstValue.FILTER_HW_MAX,
            kernel_w > ConstValue.FILTER_HW_MAX,
            stride_h > ConstValue.STRIDE_MAX,
            stride_w > ConstValue.STRIDE_MAX,
            pad_top > ConstValue.PAD_MAX,
            pad_bottom > ConstValue.PAD_MAX,
            pad_left > ConstValue.PAD_MAX,
            pad_right > ConstValue.PAD_MAX,
            dilate_h > ConstValue.DILATE_MAX,
            dilate_w > ConstValue.DILATE_MAX,
        ]
        for case in invalid_attr_list:
            if case:
                return True
        return False

    def get_dma_flag():
        """
        Get dma flag.
        """
        if conv_param.binary_mode:
            return get_fmpload_mode(BinaryInfoKey.DMA_FLAG)

        if check_load3dv2_attr_invalid():
            return True

        if is_support_fixpipe() and check_load3dv2_postk_params_invalid(kernel_h, kernel_w, fmap_dtype):
            return True

        return check_l1_size_invalid_dma()

    def get_dma_conv1d_without_pad_flag():
        if conv_param.binary_mode:
            return optim_dict.get("l0a_dma_scene", -1) == Conv2DL0aDmaScene.DMA_CONV1D_WITHOUT_PAD
        return False

    def get_l0a_dma_split_k1_flag():
        if l0a_dma_flag and not conv_param.binary_mode:
            return check_l1_size_invalid_dma_split_k1()
        return False

    def get_split_w_flag():
        # if dma mode use dma mode
        if get_dma_flag():
            return False

        if get_load2d_flag():
            return False

        if dynamic_flag:
            return False

        if get_conv1d_split_w_flag():
            return False

        return check_l1_size_invalid_split_w()

    def get_broadcast_fusion_flag():
        if conv_param.binary_mode:
            return get_binary_feature_flag(BinaryInfoKey.BROADCAST_FLAG)

        return False

    def al1_compute(fmap):
        """
        Compute of al1.
        """
        # ================================== dma ===========================================
        if l0a_dma_flag:
            if is_support_fixpipe():
                return fmap
            fmap_ub_shape = [group_opt,
                             batch,
                             ci1_opt,
                             in_height + pad_top + pad_bottom,
                             in_width + pad_left + pad_right,
                             in_c0]
            temp_offset_x = 0 if offset_x is None else offset_x
            fmap_ub_for_dma_im2col = tvm.compute(fmap_ub_shape,
                                                 lambda g, n, c1, h, w, c0:
                                                 tvm.select(
                                                     tvm.any(h < pad_top,
                                                             h > in_height + pad_top - 1,
                                                             w < pad_left,
                                                             w > in_width + pad_left - 1),
                                                 tvm.const(temp_offset_x, fmap.dtype),
                                                 tvm.select(
                                                     tvm.any((c1 + g*ci1_opt) < in_c1),
                                                     fmap(n, c1 + g*ci1_opt, h - pad_top, w - pad_left, c0))),
                                                 name="fmap_ub_for_dma_im2col")
            return fmap_ub_for_dma_im2col

        # ===============================l0a load2d optimization==============================
        """
        when load2d and howo is not aligned by 16, there might be some L1 dirty data
        need conv op to align howo up to howo_mad, and the area howo_mad > howo need to be filled with 0
        for nd input cases, use "dma_copy" instruction and set_value
        for 5hd input cases, use "dma_padding" instruction
        above description is currently only applicable to static cases, and dynamic scenes will not be processed now
        """
        if l0a_load2d_flag and not dynamic_flag:
            # input nd flag and l0a load2d
            if input_nd_flag:
                src_n, src_h, src_w, src_c = tuple(i.value for i in fmap.shape)
                al1_load2d_shape = (batch, in_c1, howo_mad, in_c0)

                al1_load2d_zero = tvm.compute(
                    al1_load2d_shape,
                    lambda n_idx, ci1_idx, m_idx, ci0_idx:
                        tvm.const(0, fmap.dtype), name="al1_load2d_zero", tag=OP_TAG + "al1_load2d_zero")

                al1_load2d_real = tvm.compute(
                    al1_load2d_shape,
                    lambda n_idx, ci1_idx, m_idx, ci0_idx:
                        tvm.select(
                            ci1_idx * in_c0 + ci0_idx < src_c,
                            tvm.select(
                                m_idx < in_width * in_height,
                                fmap(n_idx, m_idx // in_width, m_idx % in_width, ci1_idx * in_c0 + ci0_idx))),
                    name="al1_load2d_real", tag=OP_TAG + "al1_load2d_real")

                al1_load2d_vir_add = tvm.compute(
                    al1_load2d_shape,
                    lambda *indice:
                        al1_load2d_real(*indice) + al1_load2d_zero(*indice),
                    name="al1_load2d_vir_add", tag=OP_TAG + "al1_load2d_vir_add")

                conv_param.tensor_map.update({"al1_load2d_real": al1_load2d_real, "al1_load2d_zero": al1_load2d_zero})
                return al1_load2d_vir_add
            # input 5hd and l0a load2d
            else:
                al1_load2d_shape = (batch, in_c1, howo_mad, in_c0)

                al1_load2d = tvm.compute(
                    al1_load2d_shape,
                    lambda n_idx, ci1_idx, m_idx, ci0_idx:
                    tvm.select(
                        m_idx < in_width * in_height,
                        fmap(n_idx, ci1_idx, m_idx // in_width, m_idx % in_width, ci0_idx),
                        tvm.const(0, fmap.dtype)),
                    name="al1_load2d",
                    tag=OP_TAG + "al1_load2d")

                return al1_load2d

        if l0a_load2d_flag:
            log.warn("load2d and howo is not aligned by 16, there might be some L1 dirty data, which cause inf/nan!")
            # input nd flag and l0a load2d
            if input_nd_flag:
                src_n, src_h, src_w, src_c = tuple(i.value for i in fmap.shape)
                al1_load2d_shape = (batch, in_c1, in_width * in_height, in_c0)

                al1_load2d = tvm.compute(
                    al1_load2d_shape,
                    lambda n_idx, ci1_idx, m_idx, ci0_idx:
                    tvm.select(
                    tvm.any(ci1_idx * in_c0 + ci0_idx < src_c),
                    fmap(n_idx, m_idx // in_width, m_idx % in_width, ci1_idx * in_c0 + ci0_idx)),
                    name="al1_load2d",
                    tag=OP_TAG + "al1_load2d")

                return al1_load2d
            # input 5hd and l0a load2d
            else:
                al1_load2d_shape = (batch, in_c1, in_width * in_height, in_c0)

                al1_load2d = tvm.compute(
                    al1_load2d_shape,
                    lambda n_idx, ci1_idx, m_idx, ci0_idx:
                    fmap(n_idx, ci1_idx, m_idx // in_width, m_idx % in_width, ci0_idx),
                    name="al1_load2d",
                    tag=OP_TAG + "al1_load2d")

                return al1_load2d

        # ===============================dynamic shape========================================
        if dynamic_flag:
            c0_l1 = ConstValue.C04_CONST if dma_c04_flag else in_c0
            fmap_l1_shape = (group_opt,
                             batch,
                             ci1_opt,
                             (in_height - 1) // stride_h + 1 if strideh_opti_flag else in_height,
                             in_width,
                             c0_l1)
            fmap_l1 = tvm.compute(
                fmap_l1_shape,
                lambda group_idx, n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx:
                tvm.select(
                    tvm.any(ci1_idx + group_idx * ci1_opt < in_c1),
                fmap(n_idx,
                     ci1_idx + group_idx * ci1_opt,
                     hi_idx * stride_h if strideh_opti_flag else hi_idx,
                     wi_idx,
                     ci0_idx)),
                name="fmap_l1",
                tag=OP_TAG + "fmap_l1")

            return fmap_l1

        # ===========================strideh optimization=====================================
        if strideh_opti_flag:
            fmap_l1_h = (in_height - 1) // stride_h + 1
            fmap_l1_shape = batch, in_c1, fmap_l1_h, in_width, in_c0

            fmap_l1 = tvm.compute(
                fmap_l1_shape,
                lambda n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx:
                fmap(n_idx,
                     ci1_idx,
                     hi_idx * stride_h,
                     wi_idx,
                     ci0_idx),
                name="fmap_l1",
                tag=OP_TAG + "fmap_l1")

            return fmap_l1

        # =========================C0=4====================================================
        if dma_c04_flag:
            # [N, C1, H, W, C0] â€”> [N, C1, H, W, 4]
            fmap_l1_c04_shape = batch, 1, in_height, in_width, 4
            fmap_l1_c04 = tvm.compute(
                fmap_l1_c04_shape,
                lambda n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx:
                fmap(n_idx,
                     ci1_idx,
                     hi_idx,
                     wi_idx,
                     ci0_idx),
                name="fmap_l1_c04",
                tag=OP_TAG + "fmap_l1_c04")

            return fmap_l1_c04

        # =========================input fmap format is NHWC======================================
        if input_nd_flag:
            src_n, src_h, src_w, src_c = tuple(i.value for i in fmap.shape)
            nhwc_2_5hd_shape = (batch, in_c1, in_height, in_width, in_c0)

            al1_nhwc_2_5hd = tvm.compute(
                nhwc_2_5hd_shape,
                lambda n_idx, ci1_idx, h_idx, w_idx, ci0_idx:
                tvm.select(
                tvm.any(ci1_idx * in_c0 + ci0_idx < src_c),
                fmap(n_idx, h_idx, w_idx, ci1_idx * in_c0 + ci0_idx)),
                name="res_nc1hwc0",
                tag=OP_TAG + "NHWC_trans_5HD")

            return al1_nhwc_2_5hd

        return fmap

    def bias_bt_compute(bias_tensor):
        """
        compute bias_l1 and bias_bt
        """
        bias_tensor_map = {}
        if bias_tensor is None:
            return None, bias_tensor_map

        bias_shape = group_opt, 1, co1_opt, 1, 1, block_n0
        bias_co_ori = weight_ori_shape_nchw[0]
        bias_dtype = bias_tensor.dtype
        bias_l1 = tvm.compute(bias_shape,
                              lambda group_idx, n_idx, co1_idx, h_idx, w_idx, co0_idx:
                                  tvm.select(
                                      (group_idx*co1_opt*block_n0 + co1_idx*block_n0 + co0_idx) < bias_co_ori,
                                      bias_tensor(group_idx*co1_opt*block_n0 + co1_idx*block_n0 + co0_idx)),
                              name="bias_l1",
                              tag=OP_TAG + "bias_l1")
        # static scene:
        # FP16: bias l1(fp16), bias bt(fp32)
        # FP32: bias l1(fp32) -> bias l1(fp32) not aligned, bias bt(fp32)
        # binary scene:
        # FP16: bias l1(fp16), bias bt(fp32)
        # FP32: bias l1(fp32) -> bias l1(fp32) not aligned, bias bt(fp32)
        bias_dtype_size = BIT_RATIO_MAP.get(bias_dtype)
        bias_ori_align_n0 = group_opt * co1_opt * block_n0 * bias_dtype_size
        bias_ori_align_c0 = ceil_div(bias_co_ori * bias_dtype_size, C0_BYTE_LEN) * C0_BYTE_LEN
        # bias gm -> l1 use nd2nz, which always align data to C0 32B
        # b32 bias data need to align by 64B, so there is 32B l1 dirty data
        # use set_2d to set 0 in l1 to fix the bug
        if (bias_ori_align_n0 != bias_ori_align_c0) or conv_param.binary_mode:
            bias_l1_zero = tvm.compute(bias_shape,
                                       lambda *indice: tvm.const(0, bias_dtype),
                                       name="bias_l1_zero",
                                       tag=OP_TAG + "bias_l1_zero")
            bias_l1_vir_add = tvm.compute(bias_shape,
                                          lambda *indice:
                                              bias_l1_zero(*indice) + bias_l1(*indice),
                                          name="bias_l1_vir_add",
                                          tag=OP_TAG + "bias_l1_vir_add")
            conv_param.bias_init_align_dim_flag = True
            conv_param.tensor_map.update({"bias_l1_real": bias_l1,
                                          "bias_l1_zero": bias_l1_zero})
            bias_l1 = bias_l1_vir_add
        bias_l1_to_bt_map = {
            "float16": "float32",
        }
        bias_bt_dtype = bias_l1_to_bt_map.get(bias_dtype, bias_dtype)
        bias_bt = tvm.compute(bias_shape,
                              lambda *indice:
                                  bias_l1(*indice).astype(bias_bt_dtype),
                                  name="bias_bt",
                              tag=OP_TAG + "bias_bt")
        bias_tensor_map[Conv2dTensorName.BIAS_BT] = bias_bt
        bias_tensor_map[Conv2dTensorName.BIAS_L1] = bias_l1
        return bias_bt, bias_tensor_map

    def bias_ub_compute(bias_tensor):
        """
        compute for bias ub
        """
        if bias_tensor is None:
            return bias_tensor, {}

        bias_real_dim_len = bias_tensor.shape[0]
        # load bias into UB and do 32Byte align
        bias_32byte_align_shape = []
        bias_tensor_map = {}
        # when bias needs to align
        bias_32byte_align_shape.append(ceil_div(bias_real_dim_len, 16)*16)
        # move bias from ddr to ub
        init_value = tvm.const(0, dtype=bias_tensor.dtype)
        bias_ub = tvm.compute(bias_32byte_align_shape,
                              lambda bias_index:
                                  tvm.select(bias_index < bias_real_dim_len,
                                             bias_tensor(bias_index),
                                             init_value),
                              name='bias_ub')

        # save three bias tensor in TENSOR MAP
        bias_tensor_map[Conv2dTensorName.BIAS_UB] = bias_ub
        return bias_ub, bias_tensor_map

    def need_set_zero():
        """
        Determine whether to add zeros to shapes that do not support nd2nz.
        """
        c_out = weight_ori_shape_nchw[0]
        co1 = ceil_div(c_out, block_n0)

        if c_out % block_n0 or group_opt * co1_opt > co1:
            return True
        return False

    def bias_not_support_nd2nz_compute(bias_tensor):
        """
        Compute bias ub, bias l1 and bias bt when not support out2l1_nd2nz.
        """
        bias_dtype_support_list = ["float16", "int32"]
        bias_tensor_map = {}
        if bias_tensor is None:
            return None, bias_tensor_map

        log.debug("Enter the branch that does not support nd2nz and needs to set zero to bias in ub.")
        conv_param.tiling_query_param.update({"bias_not_support_nd2nz_set_zero_flag": True})
        # bias_tenosr -> bias_ub [group_opt, 1, co1_opt, 1, 1, co0]
        bias_shape = group_opt, 1, co1_opt, 1, 1, block_n0
        bias_co_ori = weight_ori_shape_nchw[0]
        bias_dtype = bias_tensor.dtype
        if bias_dtype not in bias_dtype_support_list:
            err_man.raise_err_check_type("conv2d", "bias", ",".join(bias_dtype_support_list), bias_dtype)

        bias_ub = tvm.compute(bias_shape,
                              lambda group_idx, n_idx, co1_idx, h_idx, w_idx, co0_idx:
                                  tvm.select(
                                      (group_idx * co1_opt * block_n0 + co1_idx * block_n0 + co0_idx) < bias_co_ori,
                                      bias_tensor(group_idx * co1_opt * block_n0 + co1_idx * block_n0 + co0_idx)),
                              name="bias_ub",
                              tag=OP_TAG + "bias_ub")

        # bias_ub -> bias_l1
        bias_l1 = tvm.compute(bias_shape, lambda *indice: bias_ub(*indice), name="bias_l1", tag=OP_TAG + "bias_l1")

        # bias_l1 -> bias_bt
        bias_l1_to_bt_map = {
            "float16": "float32",
        }
        bias_bt_dtype = bias_l1_to_bt_map.get(bias_dtype, bias_dtype)
        bias_bt = tvm.compute(bias_shape,
                              lambda *indice:
                                  bias_l1(*indice).astype(bias_bt_dtype),
                                  name="bias_bt",
                              tag=OP_TAG + "bias_bt")

        bias_tensor_map[Conv2dTensorName.BIAS_UB] = bias_ub
        bias_tensor_map[Conv2dTensorName.BIAS_L1] = bias_l1
        bias_tensor_map[Conv2dTensorName.BIAS_BT] = bias_bt
        return bias_bt, bias_tensor_map

    def load2d_l0a_compute(fmap_l1):
        """
        Compute of al0 in fmap load2d.
        """
        tensor_name = OP_TAG + "al0_load2d"
        if l0a_layout_zn_flag:
            al0_load2d_shape = (group_opt,
                                batch,
                                ci1_opt,
                                ceil_div(in_height * in_width, block_m0),
                                block_m0,
                                in_c0)

            al0_load2d = tvm.compute(
                al0_load2d_shape,
                lambda group_idx, n_idx, ci1_idx, m1_idx, m0_idx, ci0_idx:
                fmap_l1(n_idx, group_idx * ci1_opt + ci1_idx, m0_idx + block_m0 * m1_idx, ci0_idx),
                name=tensor_name)
        else:
            al0_load2d_shape = (group_opt,
                                batch,
                                ceil_div(in_height * in_width, block_m0),
                                ci1_opt,
                                block_m0,
                                in_c0)

            al0_load2d = tvm.compute(
                al0_load2d_shape,
                lambda group_idx, n_idx, m1_idx, ci1_idx, m0_idx, ci0_idx:
                fmap_l1(n_idx, group_idx * ci1_opt + ci1_idx, m0_idx + block_m0 * m1_idx, ci0_idx),
                name=tensor_name)

        return al0_load2d

    def one_step_fmap_im2col_compute(fmap_l1):
        """
        Compute of al0 in dynamic shape.
        """
        def im2col(fmap, im2col_shape, im2col_para):
            """
            Calculate im2col result.
            """
            fmap, kernel_h, kernel_w, padding, stride, dilate, out_width = im2col_para

            def _im2col_idx(group_idx, n_idx, m1_idx_array, k1_idx, m0_idx, k0_idx, fmap_l1_zero=None):
                """
                Calculate im2col result main compute.
                """
                if split_w_flag:
                    ho_idx, m1_idx = m1_idx_array
                else:
                    m1_idx = m1_idx_array[0]

                virtual_m = m1_idx * block_m0 + m0_idx
                virtual_k = k1_idx * block_k0 + k0_idx
                dilate_h, dilate_w = dilate

                c0_l1 = ConstValue.C04_CONST if dma_c04_flag else block_k0
                back_c1 = virtual_k // c0_l1 // kernel_w // kernel_h

                if split_w_flag:
                    ho_idx = ho_idx
                    wo_idx = virtual_m
                elif l0a_dma_flag and not is_support_fixpipe():
                    # Add condition to avoid AUB_offset exceeds UB size
                    ho_idx = tvm.select(tvm.any(virtual_m < out_height*out_width),
                                        virtual_m // out_width,
                                        out_height - 1)
                    wo_idx = virtual_m % out_width
                else:
                    ho_idx = virtual_m // out_width
                    wo_idx = virtual_m % out_width

                c1_kh_kw_idx = virtual_k // c0_l1
                back_h = ho_idx * stride[0] + (c1_kh_kw_idx // kernel_w % kernel_h) * dilate_h
                back_w = wo_idx * stride[1] + (c1_kh_kw_idx % kernel_w) * dilate_w

                if l0a_dma_flag:
                    if is_support_fixpipe():
                        if l0a_dma_conv1d_without_pad_flag:
                            # limitation of pass EliminateBranch:
                            # the inner axis on which the command is issued must be able to calculate its extent if 
                            # it appears in the condition.
                            # cin select due to condition 1 and 2 will cause tvm error, use k1_idx to limit cin
                            # back_h infer from virtual_m(align_m0) // out_w, which is larger than hin
                            return tvm.select(tvm.all(k1_idx + group_idx*ci1_opt*kernel_w*kernel_h
                                                          < in_c1*kernel_w*kernel_h,
                                                      back_h < in_height),
                                              fmap(n_idx,
                                                   back_c1 + group_idx*ci1_opt,
                                                   back_h,
                                                   back_w,
                                                   k0_idx),
                                              fmap_l1_zero(group_idx, n_idx, m1_idx, k1_idx, m0_idx, k0_idx))
                        if l0a_layout_zn_flag:
                            return tvm.select(tvm.any(back_h < padding[0],
                                                      back_h > fmap.shape[2] + padding[0] - 1,
                                                      back_w < padding[2],
                                                      back_w > fmap.shape[3] + padding[2] - 1),
                                              fmap_l1_zero(group_idx, n_idx, k1_idx, m1_idx, m0_idx, k0_idx),
                                              tvm.select(
                                                  tvm.any((back_c1 + group_idx * ci1_opt) < in_c1),
                                              fmap(n_idx,
                                                   back_c1 + group_idx*ci1_opt,
                                                   back_h - padding[0],
                                                   back_w - padding[2],
                                                   k0_idx)))
                        else:
                            return tvm.select(tvm.any(back_h < padding[0],
                                                      back_h > fmap.shape[2] + padding[0] - 1,
                                                      back_w < padding[2],
                                                      back_w > fmap.shape[3] + padding[2] - 1),
                                              fmap_l1_zero(group_idx, n_idx, m1_idx, k1_idx, m0_idx, k0_idx),
                                              tvm.select(
                                                  tvm.any((back_c1 + group_idx * ci1_opt) < in_c1),
                                              fmap(n_idx,
                                                   back_c1 + group_idx*ci1_opt,
                                                   back_h - padding[0],
                                                   back_w - padding[2],
                                                   k0_idx)))
                    return fmap(group_idx,
                                n_idx,
                                back_c1,
                                back_h,
                                back_w,
                                k0_idx)

                return tvm.select(tvm.any(back_h < padding[0],
                                          back_h > fmap.shape[3] + padding[0] - 1,
                                          back_w < padding[2],
                                          back_w > fmap.shape[4] + padding[2] - 1),
                                  tvm.const(offset_x if conv_param.binary_static_flag else 0, fmap.dtype),
                                  fmap(group_idx,
                                       n_idx,
                                       back_c1,
                                       back_h - padding[0],
                                       back_w - padding[2],
                                       k0_idx % c0_l1))

            name = "im2col_fractal_v2"
            tag = OP_TAG + 'im2col_fractal_v2'
            attrs = {'fmap_shape': fmap.shape,
                     'kernel_h': kernel_h,
                     'kernel_w': kernel_w,
                     'padding': padding,
                     'stride': stride}

            if split_w_flag:
                return tvm.compute(im2col_shape,
                                   lambda group_idx, n_idx, ho_idx, m1_idx, k1_idx, m0_idx, k0_idx:
                                   _im2col_idx(group_idx, n_idx, [ho_idx, m1_idx], k1_idx, m0_idx, k0_idx),
                                   name=name,
                                   tag=tag,
                                   attrs=attrs)

            if l0a_dma_flag and is_support_fixpipe():
                temp_offset_x = 0 if offset_x is None else offset_x
                if l0a_layout_zn_flag:
                    fmap_l1_zero = tvm.compute(im2col_shape,
                                               lambda group_idx, n_idx, k1_idx, m1_idx, m0_idx, k0_idx:
                                               tvm.const(temp_offset_x, fmap.dtype),
                                               name="fmap_l1_dma_zero",
                                               tag=OP_TAG + "fmap_l1_dma_zero")
                    fmap_l1_dma_im2col = tvm.compute(im2col_shape,
                        lambda group_idx, n_idx, k1_idx, m1_idx, m0_idx, k0_idx:
                        _im2col_idx(group_idx, n_idx, [m1_idx], k1_idx, m0_idx, k0_idx, fmap_l1_zero),
                        name="fmap_l1_dma_im2col",
                        tag=OP_TAG + "fmap_l1_dma_im2col",
                        attrs=attrs)
                else:
                    fmap_l1_zero = tvm.compute(im2col_shape,
                                               lambda group_idx, n_idx, m1_idx, k1_idx, m0_idx, k0_idx:
                                               tvm.const(temp_offset_x, fmap.dtype),
                                               name="fmap_l1_dma_zero",
                                               tag=OP_TAG + "fmap_l1_dma_zero")
                    fmap_l1_dma_im2col = tvm.compute(im2col_shape,
                        lambda group_idx, n_idx, m1_idx, k1_idx, m0_idx, k0_idx:
                        _im2col_idx(group_idx, n_idx, [m1_idx], k1_idx, m0_idx, k0_idx, fmap_l1_zero),
                        name="fmap_l1_dma_im2col",
                        tag=OP_TAG + "fmap_l1_dma_im2col",
                        attrs=attrs)
                fmap_l1_virtual_add = tvm.compute(im2col_shape,
                                                  lambda *indice:
                                                  fmap_l1_zero(*indice) + fmap_l1_dma_im2col(*indice),
                                                  name=name,
                                                  tag=tag,
                                                  attrs=attrs)
                update_tensormap.update({Conv2dTensorName.FMAP_L1_DMA_ZERO: fmap_l1_zero,
                                         Conv2dTensorName.FMAP_L1_DMA_IM2COL: fmap_l1_dma_im2col})
                return fmap_l1_virtual_add

            if l0a_layout_zn_flag:
                return tvm.compute(im2col_shape,
                                   lambda group_idx, n_idx, k1_idx, m1_idx, m0_idx, k0_idx:
                                   _im2col_idx(group_idx, n_idx, [m1_idx], k1_idx, m0_idx, k0_idx),
                                   name=name,
                                   tag=tag,
                                   attrs=attrs)

            return tvm.compute(im2col_shape,
                               lambda group_idx, n_idx, m1_idx, k1_idx, m0_idx, k0_idx:
                               _im2col_idx(group_idx, n_idx, [m1_idx], k1_idx, m0_idx, k0_idx),
                               name=name,
                               tag=tag,
                               attrs=attrs)

        if l0a_layout_zn_flag:
            im2col_shape = (group_opt,
                            batch,
                            ci1_opt * kernel_h * kernel_w,
                            howo_mad // block_m0,
                            block_m0,
                            block_k0)
        elif split_w_flag:
            im2col_shape = (group_opt,
                            batch,
                            out_height,
                            out_width_aligned // block_m0,
                            ci1_opt * kernel_h * kernel_w,
                            block_m0,
                            block_k0)
        elif dma_c04_flag:
            im2col_shape = (group_opt,
                            batch,
                            howo_mad // block_m0,
                            k1_size,
                            block_m0,
                            block_k0)
        else:
            im2col_shape = (group_opt,
                            batch,
                            howo_mad // block_m0,
                            ci1_opt * kernel_h * kernel_w,
                            block_m0,
                            block_k0)

        im2col_para = (fmap_l1,
                       kernel_h,
                       kernel_w,
                       padding, # (pad_top, pad_bottom, pad_left, pad_right)
                       (1, stride_w) if strideh_opti_flag else stride,
                       dilate,
                       out_width)

        fmap_im2col = im2col(fmap_l1, im2col_shape, im2col_para)
        return fmap_im2col

    def row_major_compute(fmap_l1):
        """
        Compute of row major tensor.
        """
        def im2col_row_major(fmap_l1, kernel_w, padding, stride, dilate, compute_dtype):
            def __im2col_row_major_indices(group_idx, batch_idx, howo_idx_array, cin_1_idx,
                                           k_h_idx, k_w_idx, cin_0_idx, fmap_l1,
                                           kernel_w, padding,
                                           stride, dilate):

                def get_h_w_index(howo_idx_array):
                    if split_w_flag:
                        ho_idx, wo_idx = howo_idx_array
                        h_index_ = ho_idx * stride_h + k_h_idx * dilate_h
                        w_index_ = wo_idx * stride_w + k_w_idx * dilate_w
                        return h_index_, w_index_

                    howo_idx = howo_idx_array[0]
                    h_index_ = (howo_idx // width_out) * stride_h + k_h_idx * dilate_h
                    w_index_ = (howo_idx % width_out) * stride_w + k_w_idx * dilate_w
                    return h_index_, w_index_

                _, _, in_height, in_weight, _ = fmap_l1.shape
                stride_h, stride_w = stride
                dilate_h, dilate_w = dilate
                padding_top, _, padding_left, padding_right = padding
                width_out = (in_weight.value + padding_left + padding_right -
                             ((kernel_w - 1)*dilate_w + 1)) // (stride_w) + 1

                h_index, w_index = get_h_w_index(howo_idx_array)

                if l0a_dma_flag:
                    return fmap_l1(batch_idx,
                                   cin_1_idx + group_idx * ci1_opt,
                                   h_index - padding_top,
                                   w_index - padding_left,
                                   cin_0_idx)
                return tvm.select(
                    tvm.any(h_index < padding_top,
                            h_index > in_height.value + padding_top - 1,
                            w_index < padding_left,
                            w_index > in_weight.value + padding_left - 1),
                    tvm.const(offset_x, compute_dtype),
                    fmap_l1(batch_idx,
                            cin_1_idx + group_idx * ci1_opt,
                            h_index - padding_top,
                            w_index - padding_left,
                            cin_0_idx))

            tensor_name = Conv2dTensorName.FMAP_ROW_MAJOR
            tensor_tag = OP_TAG + "im2col_row_major"
            if split_w_flag:
                fmap_row_major_shape = (group_opt, batch, out_height, out_width,
                                        ci1_opt, kernel_h, kernel_w, row_major_c0)
                return tvm.compute(fmap_row_major_shape,
                                   lambda group_idx, batch_idx, ho_idx, wo_idx, cin_1_idx, k_h_idx,
                                          k_w_idx, cin_0_idx:
                                   __im2col_row_major_indices(group_idx, batch_idx,
                                                              [ho_idx, wo_idx], cin_1_idx,
                                                              k_h_idx, k_w_idx, cin_0_idx,
                                                              fmap_l1, kernel_w,
                                                              padding, stride, dilate),
                                   name=tensor_name,
                                   tag=tensor_tag)

            fmap_row_major_shape = (group_opt, batch, out_height * out_width, ci1_opt, kernel_h, kernel_w, row_major_c0)
            return tvm.compute(fmap_row_major_shape,
                               lambda group_idx, batch_idx, howo_idx, cin_1_idx, k_h_idx, k_w_idx, cin_0_idx:
                               __im2col_row_major_indices(group_idx, batch_idx, [howo_idx], cin_1_idx,
                                                          k_h_idx, k_w_idx, cin_0_idx,
                                                          fmap_l1, kernel_w, padding, stride,
                                                          dilate),
                               name=tensor_name,
                               tag=tensor_tag)

        def im2col_row_major_reshape_default(fmap_row_major, compute_dtype, name, tag):
            """
            Merge the in_c1_opt, kernel_h, kernel_w, in_c0 axes into k axis.
            """
            row_major_reshape_shape = (group_opt, batch, howo_mad, k_size)
            _, _, out_hw, in_c1_opt, kernel_h, kernel_w, in_c0 = fmap_row_major.shape
            row_major_reshape = tvm.compute(
                row_major_reshape_shape,
                lambda group_idx, n_idx, howo_idx, k_idx:
                tvm.select(
                    tvm.all(k_idx < in_c1_opt * kernel_h * kernel_w * in_c0,
                            howo_idx < out_hw,
                            group_idx * k_size + k_idx < reduce_value),
                    fmap_row_major(group_idx,
                                   n_idx,
                                   howo_idx,
                                   k_idx // (kernel_h * kernel_w * in_c0),
                                   k_idx // (kernel_w * in_c0) % kernel_h,
                                   k_idx // (in_c0) % (kernel_w),
                                   k_idx % in_c0),
                    tvm.const(0.0, compute_dtype)),
                name=name,
                tag=tag)
            return row_major_reshape

        def im2col_row_major_reshape_split_w(fmap_row_major, compute_dtype, name, tag):
            """
            Merge the in_c1_opt, kernel_h, kernel_w, in_c0 axes into k axis.
            """
            row_major_reshape_shape = (group_opt, batch, out_height, out_width_aligned, k_size)
            _, _, _, _, in_c1_opt, kernel_h, kernel_w, in_c0 = fmap_row_major.shape
            row_major_reshape = tvm.compute(
                row_major_reshape_shape,
                lambda group_idx, n_idx, ho_idx, wo_align_idx, k_idx:
                tvm.select(
                    tvm.all(k_idx < in_c1_opt * kernel_h * kernel_w * in_c0,
                            wo_align_idx < out_width,
                            group_idx * k_size + k_idx < reduce_value),
                    fmap_row_major(group_idx,
                                   n_idx,
                                   ho_idx,
                                   wo_align_idx,
                                   k_idx // (kernel_h * kernel_w * in_c0),
                                   k_idx // (kernel_w * in_c0) % kernel_h,
                                   k_idx // (in_c0) % (kernel_w),
                                   k_idx % in_c0),
                    tvm.const(0.0, compute_dtype)),
                name=name,
                tag=tag)
            return row_major_reshape

        def im2col_row_major_reshape(fmap_row_major, compute_dtype):
            tensor_name = Conv2dTensorName.FMAP_RAW_MAJOR_RESHAPE
            tensor_tag = OP_TAG + "row_major_reshape"
            if split_w_flag:
                return im2col_row_major_reshape_split_w(fmap_row_major, compute_dtype, tensor_name, tensor_tag)
            return im2col_row_major_reshape_default(fmap_row_major, compute_dtype, tensor_name, tensor_tag)

        def get_stride():
            if strideh_opti_flag:
                return 1, stride_w

            return stride_h, stride_w

        stride = get_stride()
        fmap_row_major = im2col_row_major(fmap_l1, kernel_w, padding, stride, dilate, fmap_dtype)
        fmap_row_major_reshape = im2col_row_major_reshape(fmap_row_major, fmap_dtype)

        return fmap_row_major, fmap_row_major_reshape

    def l0a_compute(l0a_src):
        """
        Compute of al0.
        """
        def im2col_fractal_compute_default(name, tag):
            if l0a_layout_zn_flag:
                fmap_im2col_shape = (group_opt, batch, k1_size, howo_mad // block_m0, block_m0, block_k0)
                res_im2col_fractal = tvm.compute(
                    fmap_im2col_shape,
                    lambda group_idx, n_idx, k1_idx, m1_idx, m0_idx, k0_idx:
                    l0a_src(group_idx, n_idx, m1_idx * block_m0 + m0_idx, k1_idx * block_k0 + k0_idx),
                    name=name,
                    tag=tag)
            else:
                fmap_im2col_shape = (group_opt, batch, howo_mad // block_m0, k1_size, block_m0, block_k0)
                res_im2col_fractal = tvm.compute(
                    fmap_im2col_shape,
                    lambda group_idx, n_idx, m1_idx, k1_idx, m0_idx, k0_idx:
                    l0a_src(group_idx, n_idx, m1_idx * block_m0 + m0_idx, k1_idx * block_k0 + k0_idx),
                    name=name,
                    tag=tag)
            return res_im2col_fractal

        def im2col_fractal_compute_split_w(name, tag):
            if l0a_layout_zn_flag:
                fmap_im2col_shape = (group_opt, batch, out_height, k1_size,
                                     out_width_aligned // block_m0, block_m0, block_k0)
                res_im2col_fractal = tvm.compute(
                    fmap_im2col_shape,
                    lambda group_idx, n_idx, ho_idx, k1_idx, wo_align_idx, m0_idx, k0_idx:
                    l0a_src(group_idx, n_idx, ho_idx,
                            wo_align_idx * block_m0 + m0_idx,
                            k1_idx * block_k0 + k0_idx),
                    name=name,
                    tag=tag)
            else:
                fmap_im2col_shape = (group_opt, batch, out_height, out_width_aligned // block_m0,
                                     k1_size, block_m0, block_k0)
                res_im2col_fractal = tvm.compute(
                    fmap_im2col_shape,
                    lambda group_idx, n_idx, ho_idx, wo_align_idx, k1_idx, m0_idx, k0_idx:
                    l0a_src(group_idx, n_idx, ho_idx,
                            wo_align_idx * block_m0 + m0_idx,
                            k1_idx * block_k0 + k0_idx),
                    name=name,
                    tag=tag)
            return res_im2col_fractal

        tensor_name = Conv2dTensorName.FMAP_IMG2COL
        tensor_tag = OP_TAG + "im2col_fractal"
        if split_w_flag:
            return im2col_fractal_compute_split_w(tensor_name, tensor_tag)

        return im2col_fractal_compute_default(tensor_name, tensor_tag)

    def l0b_compute():
        l0b_tensor_map_ = {}

        tiny_weight_fractal_flag = conv_param.get_tiny_weight_fractal_flag()
        if tiny_weight_fractal_flag:
            cin1_opt_tiny = weight.shape[0]
            weight_broadcast_shape = (group_opt * ci1_opt * kernel_h * kernel_w, co1_opt, block_n0, block_k0)
            weight_l0b = tvm.compute(weight_broadcast_shape,
                                    lambda k1_idx, n1_idx, n0_idx, k0_idx: weight(
                                        k1_idx // (kernel_h * kernel_w) % cin1_opt_tiny, n1_idx, n0_idx, k0_idx),
                                    name=Conv2dTensorName.BL0)
            l0b_tensor_map_[Conv2dTensorName.BL0] = weight_l0b
            return weight_l0b, l0b_tensor_map_

        if sparse_4to2_flag:
            weight_index = conv_param.compress_index
            # 4: weight's shape is four times weight_index
            weight_l0b = tvm.compute(weight.shape,
                                     lambda k1_idx, n1_idx, n0_idx, k0_idx: tvm.load_sparse(
                                         weight(k1_idx, n1_idx, n0_idx, k0_idx),
                                         weight_index(k1_idx, n1_idx, n0_idx, k0_idx // 4)),
                                     name=Conv2dTensorName.BL0)
            l0b_tensor_map_[Conv2dTensorName.BL0] = weight_l0b
            l0b_tensor_map_[Conv2dTensorName.WEIGHT_INDEX] = weight_index
            return weight_l0b, l0b_tensor_map_

        weight_l0b = tvm.compute(weight.shape,
                                 lambda k1_idx, n1_idx, n0_idx, k0_idx: weight(k1_idx, n1_idx,
                                                                               n0_idx, k0_idx),
                                 name=Conv2dTensorName.BL0)
        l0b_tensor_map_[Conv2dTensorName.BL0] = weight_l0b
        return weight_l0b, l0b_tensor_map_

    def l0c_compute(fmap_im2col, weight_for_cube, bias_add_l0c=None):
        """
        compute for mad
        """
        def get_mad_attrs():
            remove_pad_m = out_height * out_width
            if v200_width_out_1_flag:
                remove_pad_m = remove_pad_m // 2
            attrs = {"remove_pad_M": remove_pad_m,
                     "split_w_flag": int(split_w_flag),  # bool in op.attrs is UIntImm, change to int type
                     "out_width": out_width,
                     "out_width_aligned": out_width_aligned}

            return attrs

        def get_mad_shape():
            """
            get mad shape
            """
            if split_w_flag:
                mad_shape = (group_opt, batch, out_height, co1_opt, out_width_aligned, block_n0)
                return mad_shape

            mad_m = ceil_div(out_height * out_width, block_m0) * block_m0
            mad_shape = (group_opt, batch, co1_opt, mad_m, block_n0)
            return mad_shape

        def get_reduce_sum_axis():
            """
            get mad reduce axis
            """
            weight_k1, _, _, _ = weight_fracz_shape
            if sparse_4to2_flag:
                weight_k1 = group_opt * ci1_opt * kernel_h * kernel_w
            reduce_k1 = weight_k1 // group_opt
            axis_k1 = tvm.reduce_axis((0, reduce_k1), name='cin_1_kh_kw')
            axis_k0 = tvm.reduce_axis((0, block_k0), name='cin_0')
            return reduce_k1, axis_k1, axis_k0

        def get_mad_sum_func():
            mad_sum_func = tvm.sum
            if sparse_4to2_flag:
                mad_sum_func = tvm.mad_sp
            return mad_sum_func

        def get_mad_k1_factor():
            k1_factor = 1
            if sparse_4to2_flag:
                k1_factor = 2  # 2: k1_sparse is 1/2 of k1_origin
            return k1_factor

        def get_fmap_im2col_idx(fmap_im2col, *indice):
            """
            get fmap im2col idx
            """
            if split_w_flag:
                group_idx, batch_idx, ho_idx, axis_k1, wo_aligned_idx, axis_k0 = indice
                if l0a_layout_zn_flag:
                    return fmap_im2col[group_idx,
                                       batch_idx,
                                       ho_idx,
                                       axis_k1,
                                       wo_aligned_idx // block_m0,
                                       wo_aligned_idx % block_m0,
                                       axis_k0]
                else:
                    return fmap_im2col[group_idx,
                                       batch_idx,
                                       ho_idx,
                                       wo_aligned_idx // block_m0,
                                       axis_k1,
                                       wo_aligned_idx % block_m0,
                                       axis_k0]
            else:
                group_idx, batch_idx, axis_k1, howo_idx, axis_k0 = indice
                if l0a_layout_zn_flag:
                    return fmap_im2col[group_idx,
                                       batch_idx,
                                       axis_k1,
                                       howo_idx // block_m0,
                                       howo_idx % block_m0,
                                       axis_k0]
                else:
                    return fmap_im2col[group_idx,
                                       batch_idx,
                                       howo_idx // block_m0,
                                       axis_k1,
                                       howo_idx % block_m0,
                                       axis_k0]

        def l0c_compute_mad_split_w(fmap_im2col, weight_for_cube, name, tag):
            """
            compute for mad split_w
            """
            mad_shape = get_mad_shape()
            mad_sum_func = get_mad_sum_func()
            reduce_k1, axis_k1, axis_k0 = get_reduce_sum_axis()
            k1_factor = get_mad_k1_factor()

            c_col = tvm.compute(
                mad_shape,
                lambda group_idx, batch_idx, ho_idx, co1_idx, wo_aligned_idx, co0_idx:
                mad_sum_func(
                    tvm.select(
                        tvm.all((group_idx * reduce_k1 + axis_k1) * block_k0 + axis_k0 < reduce_value),
                        (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, ho_idx,
                                             axis_k1, wo_aligned_idx, axis_k0) *
                         weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                         co1_idx,
                                         co0_idx,
                                         axis_k0]).astype(mad_dtype)),
                    axis=[axis_k1, axis_k0]),
                name=name,
                tag=tag,
                attrs=get_mad_attrs())
            return c_col

        def l0c_compute_mad_common(fmap_im2col, weight_for_cube, name, tag):
            """
            compute for mad
            """
            mad_shape = get_mad_shape()
            mad_sum_func = get_mad_sum_func()
            reduce_k1, axis_k1, axis_k0 = get_reduce_sum_axis()
            k1_factor = get_mad_k1_factor()

            # This branch aims to fix accuracy issue for load2d or dma in multi-group-opt scene.
            # Load2d/DMA procedure will ensure redundant date points all be ZERO,
            # thus removing 'select' statement
            if (l0a_load2d_flag or l0a_dma_flag) and groupopt_flag:
                c_col = tvm.compute(
                    mad_shape,
                    lambda group_idx, batch_idx, co1_idx, howo_idx, co0_idx:
                    mad_sum_func(
                        (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, axis_k1, howo_idx, axis_k0) *
                         weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                         co1_idx,
                                         co0_idx,
                                         axis_k0]).astype(mad_dtype),
                        axis=[axis_k1, axis_k0]),
                    name=name,
                    tag=tag,
                    attrs=get_mad_attrs())
                return c_col

            c_col = tvm.compute(
                mad_shape,
                lambda group_idx, batch_idx, co1_idx, howo_idx, co0_idx:
                mad_sum_func(
                    tvm.select(
                        tvm.all((group_idx * reduce_k1 + axis_k1) * block_k0 + axis_k0 < reduce_value),
                        (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, axis_k1, howo_idx, axis_k0) *
                         weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                         co1_idx,
                                         co0_idx,
                                         axis_k0]).astype(mad_dtype)),
                    axis=[axis_k1, axis_k0]),
                name=name,
                tag=tag,
                attrs=get_mad_attrs())
            return c_col

        def l0c_compute_mad(fmap_im2col, weight_for_cube):
            name = Conv2dTensorName.CL0
            tag = OP_TAG + "c_col"
            if split_w_flag:
                return l0c_compute_mad_split_w(fmap_im2col, weight_for_cube, name, tag)

            return l0c_compute_mad_common(fmap_im2col, weight_for_cube, name, tag)

        def l0c_compute_mad_with_bias_split_w(fmap_im2col, weight_for_cube, name, tag, bias):
            """
            compute for mad with bias split_w
            """
            mad_shape = get_mad_shape()
            mad_sum_func = get_mad_sum_func()
            reduce_k1, axis_k1, axis_k0 = get_reduce_sum_axis()
            k1_factor = get_mad_k1_factor()

            c_col = tvm.compute(
                mad_shape,
                lambda group_idx, batch_idx, ho_idx, co1_idx, wo_aligned_idx, co0_idx:
                mad_sum_func(
                    tvm.select(
                        tvm.all(axis_k1.var == 0, axis_k0.var == 0),
                        tvm.select(
                            tvm.all((group_idx * reduce_k1 + axis_k1) * block_k0 + axis_k0 < reduce_value),
                            (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, ho_idx,
                                                 axis_k1, wo_aligned_idx, axis_k0) *
                                weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                                co1_idx,
                                                co0_idx,
                                                axis_k0]).astype(mad_dtype) +
                            bias[group_idx, 0, co1_idx, 0, 0, co0_idx]),
                        tvm.select(
                            tvm.all((group_idx * reduce_k1 + axis_k1) * block_k0 + axis_k0 < reduce_value),
                            (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, ho_idx,
                                                 axis_k1, wo_aligned_idx, axis_k0) *
                                weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                                co1_idx,
                                                co0_idx,
                                                axis_k0]).astype(mad_dtype))),
                    axis=[axis_k1, axis_k0]),
                name=name,
                tag=tag,
                attrs=get_mad_attrs()
            )
            return c_col

        def l0c_compute_mad_with_bias_common(fmap_im2col, weight_for_cube, name, tag, bias):
            """
            compute for mad with bias
            """
            mad_shape = get_mad_shape()
            mad_sum_func = get_mad_sum_func()
            reduce_k1, axis_k1, axis_k0 = get_reduce_sum_axis()
            k1_factor = get_mad_k1_factor()

            # This branch aims to fix accuracy issue for load2d or dma in multi-group-opt scene.
            if (l0a_load2d_flag or l0a_dma_flag) and groupopt_flag:
                c_col = tvm.compute(
                    mad_shape,
                    lambda group_idx, batch_idx, co1_idx, howo_idx, co0_idx:
                    mad_sum_func(
                        tvm.select(
                            tvm.all(axis_k1.var == 0, axis_k0.var == 0),
                                (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, axis_k1, howo_idx, axis_k0) *
                                 weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                                 co1_idx,
                                                 co0_idx,
                                                 axis_k0]).astype(mad_dtype) +
                            bias[group_idx, 0, co1_idx, 0, 0, co0_idx],
                                (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, axis_k1, howo_idx, axis_k0) *
                                 weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                                 co1_idx,
                                                 co0_idx,
                                                 axis_k0]).astype(mad_dtype)),
                        axis=[axis_k1, axis_k0]),
                    name=name,
                    tag=tag,
                    attrs=get_mad_attrs()
                )
                return c_col

            c_col = tvm.compute(
                mad_shape,
                lambda group_idx, batch_idx, co1_idx, howo_idx, co0_idx:
                mad_sum_func(
                    tvm.select(
                        tvm.all(axis_k1.var == 0, axis_k0.var == 0),
                        tvm.select(
                            tvm.all((group_idx * reduce_k1 + axis_k1) * block_k0 + axis_k0 < reduce_value),
                            (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, axis_k1, howo_idx, axis_k0) *
                                weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                                co1_idx,
                                                co0_idx,
                                                axis_k0]).astype(mad_dtype) +
                            bias[group_idx, 0, co1_idx, 0, 0, co0_idx]),
                        tvm.select(
                            tvm.all((group_idx * reduce_k1 + axis_k1) * block_k0 + axis_k0 < reduce_value),
                            (get_fmap_im2col_idx(fmap_im2col, group_idx, batch_idx, axis_k1, howo_idx, axis_k0) *
                                weight_for_cube[(group_idx * reduce_k1 + axis_k1) // k1_factor,
                                                co1_idx,
                                                co0_idx,
                                                axis_k0]).astype(mad_dtype))),
                    axis=[axis_k1, axis_k0]),
                name=name,
                tag=tag,
                attrs=get_mad_attrs()
            )
            return c_col

        def l0c_compute_mad_with_bias(fmap_im2col, weight_for_cube, bias):
            name = Conv2dTensorName.CL0
            tag = OP_TAG + "c_col_bias"
            if split_w_flag:
                return l0c_compute_mad_with_bias_split_w(fmap_im2col, weight_for_cube, name, tag, bias)

            return l0c_compute_mad_with_bias_common(fmap_im2col, weight_for_cube, name, tag, bias)

        if bias_add_l0c is None:
            return l0c_compute_mad(fmap_im2col, weight_for_cube)

        if is_support_fixpipe():
            return l0c_compute_mad_with_bias(fmap_im2col, weight_for_cube, bias_add_l0c)

        err_man.raise_err_message_cube("bias add in l0c is not supported for {}".format(get_cur_soc()))
        return None

    def l0c_to_buffer_compute(l0c, dst_shape, name, tag, attrs):
        """
        compute for l0c to another buffer
        """
        if split_w_flag:
            # howo is ho * out_width_aligned
            if groupopt_flag:
                l0c_to_buffer = tvm.compute(dst_shape,
                                    lambda batch_idx, co1_idx, howo_idx, co0_idx:
                                    l0c(co1_idx // co1_opt,
                                        batch_idx,
                                        howo_idx // out_width_aligned,
                                        co1_idx % co1_opt,
                                        howo_idx % out_width_aligned,
                                        co0_idx).astype(res_dtype),
                                name=name,
                                tag=tag,
                                attrs=attrs)
            else:
                l0c_to_buffer = tvm.compute(dst_shape,
                                    lambda batch_idx, co1_idx, howo_idx, co0_idx:
                                    l0c(0,
                                        batch_idx,
                                        howo_idx // out_width_aligned,
                                        co1_idx,
                                        howo_idx % out_width_aligned,
                                        co0_idx).astype(res_dtype),
                                name=name,
                                tag=tag,
                                attrs=attrs)
            return l0c_to_buffer

        if groupopt_flag:
            l0c_to_buffer = tvm.compute(dst_shape,
                                lambda batch_idx, co1_idx, howo_idx, co0_idx:
                                l0c(co1_idx // co1_opt,
                                    batch_idx,
                                    co1_idx % co1_opt,
                                    howo_idx,
                                    co0_idx).astype(res_dtype),
                            name=name,
                            tag=tag,
                            attrs=attrs)
        else:
            l0c_to_buffer = tvm.compute(dst_shape,
                                lambda batch_idx, co1_idx, howo_idx, co0_idx:
                                l0c(0,
                                    batch_idx,
                                    co1_idx,
                                    howo_idx,
                                    co0_idx).astype(res_dtype),
                            name=name,
                            tag=tag,
                            attrs=attrs)

        return l0c_to_buffer

    def cub_compute(l0c):
        """
        compute for l0c to cub
        """
        m_axis_shape = howo_mad
        if split_w_flag:
            m_axis_shape = out_height * out_width_aligned
        cub_shape = (batch, out_c1, m_axis_shape, block_n0)
        name = Conv2dTensorName.CUB
        tag = OP_TAG + name
        attrs = None
        return l0c_to_buffer_compute(l0c, cub_shape, name, tag, attrs)

    def get_remove_padded_column_shape():
        """The true shape is a half of fake shape, cause padw padded in [conv_compute.py]"""
        remove_padded_mad_m = out_height * (out_width // 2)
        remove_padded_column_shape = (
            batch, out_c1, remove_padded_mad_m, block_n0
        )
        return remove_padded_column_shape

    def get_res_shape():
        # out_width in result shape should be half of origin. padded column have been removed.
        if v200_width_out_1_flag:
            return batch, out_c1, out_height * (out_width // 2), block_n0
        # normal scenario
        return batch, out_c1, out_height * out_width, block_n0

    def get_res_fp32_shape():
        # used in fixpipe, do not use in binary mode and v200_width_out_1_flag
        return batch, out_c1 * ConstValue.DOUBLE_VALUE, out_height * out_width, ConstValue.HALF_CUBE_UNIT

    def remove_padded_column_compute(padded_tensor):
        """
        For v200 & load3d & Nx1, padw has add with stridew before, cause fake tensor.
        So, add a compute to get the true tensor with remove padded.
        The operation taken is to select one line every 2 lines. [0,1,2,3...] -> [0, 2...]

        Args:
            padded_tensor: fake tensor [cub]

        Returns:
            true tensor with excluded rows.
        """
        if not v200_width_out_1_flag:
            return padded_tensor
        remove_padded_column_shape = get_remove_padded_column_shape()
        remove_padded_tensor = tvm.compute(
            remove_padded_column_shape,
            lambda batch_idx, co1_idx, howo_idx, co0_idx:
            padded_tensor(batch_idx, co1_idx, howo_idx * 2, co0_idx),
            name=Conv2dTensorName.REMOVE_PADDED_COLUMN,
            tag=OP_TAG + Conv2dTensorName.REMOVE_PADDED_COLUMN
        )
        l0c_to_res_tensor_map[Conv2dTensorName.REMOVE_PADDED_COLUMN] = remove_padded_tensor
        return remove_padded_tensor

    def remove_pad_compute(res_in, invalid_data_rm_flag):
        """
        compute for remove pad(axis M)
        """
        tensor_tag = OP_TAG + "C"
        if invalid_data_rm_flag:
            return tvm.compute(res_in.shape,
                               lambda n_idx, co1_idx, howo_idx, co0_idx:
                                   res_in(n_idx, co1_idx, howo_idx, co0_idx),
                               name=Conv2dTensorName.INVALID_REMOVE_PAD,
                               attrs={"format": "NC1HWC0"})

        tensor_name = Conv2dTensorName.REMOVE_PAD_CC
        attrs = {"conv_shape": conv_param.dim_map["output_conv_res_shape"],
                 "width_out": out_width,
                 "format": "NC1HWC0"}
        if split_w_flag:
            def get_howo_res_idx(howo_idx):
                ho_idx = howo_idx // out_width
                wo_idx = howo_idx % out_width
                howo_res_idx = ho_idx * out_width_aligned + wo_idx
                return howo_res_idx

            return tvm.compute(res_shape,
                               lambda n_idx, co1_idx, howo_idx, co0_idx:
                                   res_in(n_idx, co1_idx, get_howo_res_idx(howo_idx), co0_idx),
                               name=tensor_name,
                               tag=tensor_tag,
                               attrs=attrs)

        return tvm.compute(res_shape,
                           lambda n_idx, co1_idx, howo_idx, co0_idx:
                               res_in(n_idx, co1_idx, howo_idx, co0_idx),
                           name=tensor_name,
                           tag=tensor_tag,
                           attrs=attrs)

    def bias_add_ub_compute(tensor, bias_ub):
        """
        compute for bias add
        """
        if bias_ub is None:
            return tensor

        bias_add = tvm.compute(
            tensor.shape,
            lambda n_idx, co1_idx, howo_idx, co0_idx:
            tensor(n_idx, co1_idx, howo_idx, co0_idx) +
            bias_ub(co1_idx * CUBE_MKN[tensor.dtype]['mac'][op_util_conv2d.CUBE_MKN_IDX_K] +
                    co0_idx),
            name='bias_add',
            attrs={'width_out': out_width})
        return bias_add

    def fixpipe_res_compute(l0c):
        """
        Compute of conv2d res. merge group axis && remove M/C padding.
        """
        # L0C â€”> out / L1
        name = Conv2dTensorName.RES_CONV2D
        tag = OP_TAG + Conv2dTensorName.RES_CONV2D
        attrs = {"conv_shape": conv_param.dim_map["output_conv_res_shape"],
                 "width_out": out_width,
                 "height_out": out_height}
        m_axis_shape = howo_mad
        if split_w_flag:
            m_axis_shape = out_height * out_width_aligned
        fixpipe_out_shape = (batch, out_c1, m_axis_shape, block_n0)
        res = l0c_to_buffer_compute(l0c, fixpipe_out_shape, name, tag, attrs)
        return res

    def res_channel_split_compute(res_in):
        if conv_type == ConvType.FP32:
            # split channel
            batch, out_c1, m_shape, _ = res_in.shape
            res_k0 = CUBE_MKN.get(res_dtype).get('mac')[op_util_conv2d.CUBE_MKN_IDX_K]
            cout_ori_align = ceil_div(weight_ori_shape_nchw[0], res_k0)
            out_shape = (batch, cout_ori_align, m_shape, res_k0)
            res_fp32 = tvm.compute(
                out_shape,
                lambda n_idx, co1_idx, howo_idx, co0_idx:
                res_in(n_idx,
                    co1_idx // op_util_conv2d.CHANNEL_SPLIT_FACTOR,
                    howo_idx,
                    res_k0 * (co1_idx % op_util_conv2d.CHANNEL_SPLIT_FACTOR) + co0_idx),
                name=Conv2dTensorName.RES_FP32_CONV2D,
                tag=OP_TAG + Conv2dTensorName.RES_FP32_CONV2D)
            return res_fp32

        return res_in

    def update_src_shape_attr():
        """
        get ori shape and attr for binary mode
        """
        if conv_param.binary_mode:
            conv_param.src_shape_attr = para_dict.get("ori_shape_attr")

    def get_l0c_add_bias(l0c, bias_ub):
        """
        process l0c tensor add bias_l0c tensor at l0c space
        """
        if bias_ub is None:
            return l0c, None

        cout1_opt = conv_param.para_dict.get("cout1_opt")
        mad_shape = (conv_param.para_dict.get("group_opt"),
                     conv_param.para_dict.get("a_shape")[0],
                     cout1_opt,
                     howo_mad,
                     CUBE_MKN[res_dtype].get('mac')[op_util_conv2d.CUBE_MKN_IDX_N])
        bias_l0c = tvm.compute(
            mad_shape,
            lambda group_idx, batch_idx, co1_idx, howo_idx, co0_idx:
            bias_ub(group_idx * cout1_opt * CUBE_MKN[l0c.dtype]['mac'][op_util_conv2d.CUBE_MKN_IDX_N]
                    + co1_idx * CUBE_MKN[l0c.dtype]['mac'][op_util_conv2d.CUBE_MKN_IDX_N] + co0_idx),
            name='bias_l0c')
 
        l0c_bias_add = tvm.compute(
            mad_shape,
            lambda group_idx, batch_idx, co1_idx, howo_idx, co0_idx:
            bias_l0c(group_idx, batch_idx, co1_idx, howo_idx, co0_idx) +
            l0c(group_idx, batch_idx, co1_idx, howo_idx, co0_idx),
            name=OP_TAG + "c_col_bias")
 
        return l0c_bias_add, bias_l0c
 
    def l0c_add_bias_merge_group_axis_and_remove_pad_quant(l0c_bias_add, invalid_data_rm_flag):
        """
        compute for l0c_add_bias merge group axis && remove M/C padding
        """
        m_axis_shape = howo_mad
        if split_w_flag:
            m_axis_shape = out_height * out_width_aligned
        res_shape = (batch, out_c1, m_axis_shape, block_n0)
        name = "remove_pad_cc"
        tag = name
        attrs = {"conv_shape": conv_param.dim_map["output_conv_res_shape"],
                 "invalid_data_rm_flag": int(invalid_data_rm_flag),
                 "remove_padded_column_in_next_op": conv_param.v200_width_out_1_flag
                 }
        return l0c_to_buffer_compute(l0c_bias_add, res_shape, name, tag, attrs)

    if conv_param.get_winograd_conv_flag():
        return conv_wino_compute(fmap, weight, para_dict, conv_param)
    #=================parse_parameters===========================
    # common parameters
    fmap_dtype, weight_dtype = fmap.dtype, weight.dtype

    bias_tensor = para_dict["bias_tensor"]

    kernel_h, kernel_w = para_dict["filter_h"], para_dict["filter_w"]
    pad_top, pad_bottom = para_dict["pad_h"]
    pad_left, pad_right = para_dict["pad_w"]
    stride_h, stride_w = para_dict["stride_h"], para_dict["stride_w"]
    dilate_h, dilate_w = para_dict["dilate_h"], para_dict["dilate_w"]
    offset_x = para_dict["offset_x"]

    padding = pad_top, pad_bottom, pad_left, pad_right
    stride = stride_h, stride_w
    dilate = dilate_h, dilate_w
    kernel = kernel_h, kernel_w

    filter_h_dilation = conv_param.filter_h_dilation
    filter_w_dilation = conv_param.filter_w_dilation
    out_height = conv_param.h_out
    out_width = conv_param.w_out

    update_src_shape_attr()

    # group conv parameters
    group = para_dict["group"]
    ci1_opt, co1_opt, group_opt = para_dict["c1_opt"], para_dict["cout1_opt"], para_dict["group_opt"]

    # shape
    fmap_5hd_shape = para_dict["a_shape"]
    weight_fracz_shape = para_dict["weight_fracz_shape"]
    weight_ori_shape_nchw = para_dict["weight_ori_shape_nchw"]
    batch, in_c1, in_height, in_width, in_c0 = fmap_5hd_shape

    # lxfusion parameters
    lxfusion_para = conv_param.fusion_para

    # flag
    dynamic_flag = conv_param.dynamic_flag
    sparse_4to2_flag = conv_param.sparse_4to2_flag
    l0a_layout_zn_flag = is_l0a_layout_zn()

    #======================config fractal unit size=======================================
    block_m0, block_k0, block_n0 = CUBE_MKN[fmap.dtype]["mac"]

    #========================config conv type========================================
    conv_type_dict = {
        "float16": ConvType.FP16,
        "bfloat16": ConvType.BF16,
        "float32": ConvType.FP32,
        "int8": ConvType.INT8,
        "int4": ConvType.INT4,
    }
    conv_type = conv_type_dict.get(fmap_dtype)

    mad_type_dict = {
        ConvType.FP16: "float32",
        ConvType.BF16: "float32",
        ConvType.FP32: "float32",
        ConvType.INT8: "int32",
        ConvType.INT4: "int32",
    }
    mad_dtype = mad_type_dict.get(conv_type)

    support_res_dtype_dict = {
        ConvType.FP16: ["float16", "float32"],
        ConvType.BF16: ["bfloat16", "float32"],
        ConvType.FP32: ["float32"],
        ConvType.INT8: ["int32"],
        ConvType.INT4: ["int32"],
    }
    res_dtype = para_dict.get("res_dtype", support_res_dtype_dict.get(conv_type)[0])
    if res_dtype not in support_res_dtype_dict.get(conv_type):
        err_man.raise_err_check_type("conv2d", "output", ",".join(support_res_dtype_dict.get(conv_type)), res_dtype)

    #================config various optimization flag================================
    # strided_read_flag
    if fmap.op.tag == "strided_read":
        conv_param.strided_read_flag = True

    # aipp_fuse_flag
    if fmap.op.tag == "aipp_res_convolution":
        conv_param.aipp_fuse_flag = True

    # c04_mode
    c04_flag = optim_dict.get("c0_optim_flg")
    dma_c04_flag = c04_flag and not conv_param.aipp_fuse_flag and is_support_fixpipe()

    # dma load3d flag
    l0a_dma_flag = False if c04_flag else get_dma_flag()
    l0a_dma_split_k1_flag = get_l0a_dma_split_k1_flag()
    l0a_dma_conv1d_without_pad_flag = get_dma_conv1d_without_pad_flag()
    if l0a_dma_flag and not conv_param.binary_mode:
        log.warn("The conv2d has entered the DMA processing process. " +
                 "A timeout AiCore error may be reported. " +
                 "If a timeout AiCore error is reported," +
                 "reduce the conv2d specifications and try again.")
    # strideh_opti_flag
    strideh_opti_flag = (kernel_h == 1 and stride_h > 1) and \
        padding == (0, 0, 0, 0) and \
        not c04_flag and \
        not l0a_dma_flag
    if lxfusion_para["l1_fusion_type"] == 1 or lxfusion_para["input_memory_type"][0] == 1:
        # for L1 breadth fusion, fmap must load all at once
        strideh_opti_flag = False

    # l0a_load2d_flag
    l0a_load2d_flag = get_load2d_flag() and not l0a_dma_flag
    if padding == (0, 0, 0, 0) and stride == (1, 1) and kernel == (1, 1) and weight_dtype in ("float16", "bfloat16"):
        c04_flag = False

    # input_nd_flag, input_nd_mode
    input_nd_flag, input_nd_mode = get_input_nd_flag_mode()
    if input_nd_flag:  # to be completed
        strideh_opti_flag = False

    # weight_nd_flag
    weight_nd_flag = weight.op.tag == "NHWC_trans_FZ"

    if weight_nd_flag and group > 1:
        err_man.raise_err_specific("conv2d", "Group > 1 is not supported when weight nd2nz.")

    split_w_flag = get_split_w_flag()

    broadcast_fusion_flag = get_broadcast_fusion_flag()

    # groupopt_flag
    groupopt_flag = get_groupopt_flag()

    # set nx1 flag, get from optim_dict.
    v200_width_out_1_flag = get_nx1_flag()

    # set conv1d split w binary flag
    conv1d_split_w_flag = get_conv1d_split_w_flag()

    # conv1d case with splitw flag, use splitw tiling flag to get tiling
    tiling_splitw_flag_for_conv1d = get_splitw_flag_for_conv1d()

    #======================calculate certain shape params for compute to use=================
    # M
    howo_mad = (out_height * out_width + block_m0 - 1) // block_m0 * block_m0
    out_width_aligned = (out_width + block_m0 - 1) // block_m0 * block_m0
    conv_param.out_width_aligned = out_width_aligned

    # Kz
    row_major_c0 = 4 if c04_flag else in_c0
    k1_size = (ci1_opt * kernel_h * kernel_w * row_major_c0 + block_k0 - 1) // block_k0
    k_size = (ci1_opt * kernel_h * kernel_w * in_c0 + block_k0 - 1) // block_k0 * block_k0

    reduce_value = in_c1 * kernel_h * kernel_w * block_k0
    out_c1 = ceil_div(weight_ori_shape_nchw[0], block_n0)
    res_shape = get_res_shape()
    if conv_type == ConvType.FP32:
        res_fp32_shape = get_res_fp32_shape()

    #============================check parameters=========================================
    if not conv_param.binary_mode and c04_flag and kernel_h * kernel_w * 4 > 65535:
        err_man.raise_err_specific(
            "conv2d",
            "In v220, small channel case, the 4 * Hk * Wk must be smaller than " +
            "or equal to 65535. you can try to disable the small channel.")

    if c04_flag and input_nd_flag:
        err_man.raise_err_specific(
            "conv2d",
            "transdata prefusion is forbidden when conv2d enable c04 optimization."
            )

    if c04_flag and l0a_dma_flag:
        err_man.raise_err_specific(
            "conv2d",
            "can not enable both c04 & l0a dma on current soc version."
            )

    if c04_flag and split_w_flag:
        err_man.raise_err_specific(
            "conv2d",
            "can not enable both c04 & splitw on current soc version."
            )

    if l0a_dma_flag and conv_param.aipp_fuse_flag:
        err_man.raise_err_specific(
            "conv2d",
            "dma im2col does not support pre-fusion, please try to disable the aipp-conv2d fusion and compile again."
            )

    #==========================conv compute begin==============================
    # tensormap
    update_tensormap = {}

    # al1
    fmap_l1 = al1_compute(fmap)

    # al0
    if l0a_load2d_flag:
        fmap_im2col = load2d_l0a_compute(fmap_l1)
    elif dynamic_flag or (l0a_dma_flag and is_support_fixpipe()):
        fmap_im2col = one_step_fmap_im2col_compute(fmap_l1)
    else:
        fmap_row_major, fmap_row_major_reshape = row_major_compute(fmap_l1)
        fmap_im2col = l0a_compute(fmap_row_major_reshape)

    # bias
    if is_support_fixpipe():
        if not is_support_nd2nz() and need_set_zero():
            bias_bt, bias_tensor_map = bias_not_support_nd2nz_compute(bias_tensor)
        else:
            bias_bt, bias_tensor_map = bias_bt_compute(bias_tensor)
        bias_add_l0c = bias_bt
        bias_add_ub = None
    else:
        bias_ub, bias_tensor_map = bias_ub_compute(bias_tensor)
        bias_add_l0c = None
        bias_add_ub = bias_ub

    # l0b
    l0b, l0b_tensor_map = l0b_compute()

    # l0c
    l0c = l0c_compute(fmap_im2col, l0b, bias_add_l0c)

    # res
    l0c_to_res_tensor_map = {}
    if is_support_fixpipe():
        fixpipe_res = fixpipe_res_compute(l0c)
        remove_pad_res = remove_pad_compute(fixpipe_res, conv_param.invalid_data_rm_flag)
        conv_res = res_channel_split_compute(remove_pad_res)
        l0c_to_res_tensor_map[Conv2dTensorName.FIXPIPE_RES] = fixpipe_res
        l0c_to_res_tensor_map[Conv2dTensorName.REMOVE_PAD_CC] = remove_pad_res
    elif conv_param.binary_static_flag:
        l0c_bias_add, bias_l0c = get_l0c_add_bias(l0c, bias_add_ub)
        update_tensormap[Conv2dTensorName.BIAS_L0C] = bias_l0c
        update_tensormap[Conv2dTensorName.L0C_BIAS_ADD] = l0c_bias_add
        conv_res = l0c_add_bias_merge_group_axis_and_remove_pad_quant(l0c_bias_add, conv_param.invalid_data_rm_flag)
    else:
        cub = cub_compute(l0c)
        cub_remove_padded_column = remove_padded_column_compute(cub)
        cub_bias_add = bias_add_ub_compute(cub_remove_padded_column, bias_add_ub)
        conv_res = remove_pad_compute(cub_bias_add, conv_param.invalid_data_rm_flag)
        l0c_to_res_tensor_map[Conv2dTensorName.CUB] = cub
        l0c_to_res_tensor_map[Conv2dTensorName.CUB_BIAS_ADD] = cub_bias_add

    #===========================update tensormap=============================
    update_tensormap.update({
        Conv2dTensorName.CL0: l0c,
        Conv2dTensorName.FILTER: weight,
        Conv2dTensorName.FMAP_IMG2COL: fmap_im2col,
        Conv2dTensorName.FMAP_L1: fmap_l1
    })

    update_tensormap.update(bias_tensor_map)
    update_tensormap.update(l0b_tensor_map)

    if not dynamic_flag and not l0a_load2d_flag and not l0a_dma_flag:
        update_tensormap.update({
            Conv2dTensorName.FMAP_ROW_MAJOR: fmap_row_major,
            Conv2dTensorName.FMAP_RAW_MAJOR_RESHAPE: fmap_row_major_reshape,
        })

    if conv_param.strided_read_flag or conv_param.aipp_fuse_flag:
        update_tensormap.update({Conv2dTensorName.FMAP: fmap.op.input_tensors[0]})
    else:
        update_tensormap.update({Conv2dTensorName.FMAP: fmap})

    update_tensormap.update(l0c_to_res_tensor_map)
    conv_param.tensor_map.update(update_tensormap)

    #===========================update dimmap=============================
    filter_matrix_dim = (
        (ci1_opt * in_c0 * kernel_h * kernel_w + block_k0 - 1) // block_k0,
        (out_c1 * block_n0 + block_n0 - 1) // block_n0,
        block_n0,
        block_k0)

    fmap_single_group_shape = batch, ci1_opt, in_height, in_width, in_c0

    update_dimmap = {
        "img_shape": fmap_single_group_shape,
        "filter_matrix_dim": filter_matrix_dim,
        "fmap_5hd_shape": fmap_5hd_shape,
    }

    conv_param.dim_map.update(update_dimmap)

    #=======================update tiling_query_param========================
    c_shape = [batch, co1_opt, out_height, out_width, block_n0]
    conv_param.tiling_query_param.update({"c_shape": c_shape})

    #======================update flag=======================================
    conv_param.strideh_opti_flag = strideh_opti_flag
    conv_param.input_nd_flag = input_nd_flag
    conv_param.input_nd_mode = input_nd_mode
    conv_param.weight_nd_flag = weight_nd_flag
    conv_param.impl_mode = para_dict.get("impl_mode", "")
    conv_param.l0a_load2d_flag = l0a_load2d_flag
    conv_param.l0a_dma_flag = l0a_dma_flag
    conv_param.l0a_dma_split_k1_flag = l0a_dma_split_k1_flag
    conv_param.c04_flag = c04_flag
    conv_param.dma_c04_flag = dma_c04_flag
    conv_param.split_w_flag = split_w_flag
    conv_param.conv1d_split_w_flag = conv1d_split_w_flag
    conv_param.tiling_splitw_flag_for_conv1d = tiling_splitw_flag_for_conv1d
    conv_param.broadcast_fusion_flag = broadcast_fusion_flag
    conv_param.v200_width_out_1_flag = v200_width_out_1_flag
    conv_param.groupopt_flag = groupopt_flag
    conv_param.al0boundcheck_flag = get_al0boundcheck_flag()
    conv_param.show_conv_param_flag()

    #==============save tiling_info_dict for conv2d_tiling_case==============
    tiling_query_param = conv_param.tiling_query_param
    conv_param.tiling_info_dict = {
        "op_type": 'conv2d',
        "a_shape": fmap_5hd_shape,
        "placeholder_fmap_5hd_shape": fmap_5hd_shape,
        "b_shape": list(tiling_query_param["shape_w_nc1hwc0"]),
        "c_shape": tiling_query_param["c_shape"],
        "a_dtype": fmap_dtype,
        "b_dtype": weight_dtype,
        "c_dtype": res_dtype,
        "mad_dtype": mad_dtype,
        "pad": padding,
        "stride": stride,
        "dilation": dilate,
        "group": group_opt,
        "bias_flag": tiling_query_param["bias_flag"],
        "fused_coefficient": [0, 0, 0],
        "fused_channel_wise": [0, 0, 0],
        "in_fm_memory_type": [],
        "out_fm_memory_type": [],
        "l1_fusion_type": -1,
        "fusion_type": 0,
        "kernel_name": para_dict.get("kernel_name"),
        "dynamic_shape_flag": True
        }

    return conv_res
