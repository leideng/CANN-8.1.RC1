#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
math
"""
import tbe.dsl
from tbe import tvm
from tbe.common.platform import ASCEND_310, ASCEND_310B, AS31XM1
from tbe.common.platform import intrinsic_check_support
from tbe.common.platform import platform_info
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.testing.dsl_source_info import source_info_decorator
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.base import operation as operation_context
from tbe.dsl.base.expr_compare import expr_equal as equal
from tbe.dsl.base.record.decorators import elewise as elewise_decorator
from tbe.dsl.compute.constants import ComputeType
from tbe.tvm.tir import expr

from .cast import _cast
from .cast import cast_to
from .cast import floor
from .util import auto_cast_tensor
from .util import dtype_check_decorator
from .util import dynamic_static_unify_fractal_format
from .util import get_tvm_scalar
from .util import in_dynamic_and_static_unify
from .util import is_cast_support
from .util import judge_var
from .util import shape_to_list
from .util import util_astype

NAME_INDEX = [0]

REPLACE_INTR = {
    "vdiv": "vrec",
    "vrsqrt": "vsqrt",
    "vlog": "vln",
    "vmaxs": "vmax",
    "vmins": "vmin"
}

NANO_BLOCK_SIZE = 16


def _cast_tensors_for_instr(instr, input_tensors):

    def _process_scalar():
        """
        process when second input is not a tensor
        """
        temp_tensor = input_tensors[0]
        scalar = input_tensors[1]
        dtype = temp_tensor.dtype
        is_support_dtype = intrinsic_check_support(f"Intrinsic_{instr}", dtype)
        if not is_support_dtype:
            if is_support_fp32 and is_cast_support(dtype, "float32"):
                temp_tensor = _cast(temp_tensor, "float32")
                dtype = "float32"
            else:
                temp_tensor = _cast(temp_tensor, "float16")
                dtype = "float16"

        tmp_arg = scalar
        scalar_type = judge_var(scalar)
        if scalar_type == "tvm_const" and scalar.dtype != dtype:
            tmp_arg = tvm.const(scalar.value, dtype=dtype)

        if scalar_type == "python_const":
            tmp_arg = tvm.const(scalar, dtype=dtype)
        return [temp_tensor, tmp_arg]

    instr = _intrinsic_check(instr)
    is_support_fp32 = intrinsic_check_support(f"Intrinsic_{instr}", "float32")

    if len(input_tensors) == 1:
        input_tensor = input_tensors[0]
        if not intrinsic_check_support(f"Intrinsic_{instr}", input_tensor.dtype):
            if is_support_fp32:
                input_tensor_new = _cast(input_tensor, "float32")
            else:
                input_tensor_new = _cast(input_tensor, "float16")

            return [input_tensor_new]

    if len(input_tensors) == 2:
        if isinstance(input_tensors[1], tvm.Tensor):
            lhs = input_tensors[0]
            rhs = input_tensors[1]
            dtype_l = lhs.dtype
            dtype_r = rhs.dtype

            lhs_t = lhs
            rhs_t = rhs
            is_support_ldtype = intrinsic_check_support(f"Intrinsic_{instr}", dtype_l)
            is_support_rdtype = intrinsic_check_support(f"Intrinsic_{instr}", dtype_r)
            if not is_support_ldtype or not is_support_rdtype or dtype_l != dtype_r:
                if is_support_fp32 and is_cast_support(dtype_l, "float32") and is_cast_support(dtype_r, "float32"):
                    lhs_t = _cast(lhs, "float32")
                    rhs_t = _cast(rhs, "float32")
                else:
                    lhs_t = _cast(lhs, "float16")
                    rhs_t = _cast(rhs, "float16")

            return [lhs_t, rhs_t]
        return _process_scalar()

    return input_tensors


def _intrinsic_check(intr):
    ret_intr = intr
    if not intrinsic_check_support(f"Intrinsic_{intr}"):
        if intr in REPLACE_INTR:
            ret_intr = REPLACE_INTR.get(intr)

    return ret_intr


def _check_scalar(scalar, position="second"):
    if isinstance(scalar, tvm.Tensor):
        dict_args = {"errCode":  "E90001",
                     "detailed_cause": f"The {position} input type must be scalar, while type is {type(scalar)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))


def _check_tensor(tensor, position="second"):
    if not isinstance(tensor, tvm.Tensor):
        dict_args = {"errCode":  "E90001",
                     "detailed_cause": f"The {position} input type must be tvm.te.tensor, while type is {type(tensor)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))


@elewise_decorator.single_scalar
@source_info_decorator()
@dtype_check_decorator
def vmuls(raw_tensor, scalar):
    """
    multiply a tensor by a scalar, dtype of raw_tensor
    and scalar must be the same

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    scalar : float, int, or tvm const

    Returns
    -------
    wrapped_tensor : raw_tensor*scalar
    """
    dtype = raw_tensor.dtype

    _check_scalar(scalar)

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_VS_mul', args=[scalar])


@elewise_decorator.single_scalar
@source_info_decorator()
@dtype_check_decorator
def vadds(raw_tensor, scalar):
    """
    add a tensor by a scalar, dtype of raw_tensor and scalar must be the same

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    scalar : float, int, or tvm const

    Returns
    -------
    wrapped_tensor : raw_tensor + scalar
    """
    dtype = raw_tensor.dtype

    _check_scalar(scalar)

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_VS_add', args=[scalar])


@elewise_decorator.single_scalar
@source_info_decorator()
@dtype_check_decorator
def vmaxs(raw_tensor, scalar):
    """
    Calculate elewise compare, return the max one of scalar or tensor's element,
    dtype of raw_tensor and scalar must be the same

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    scalar : float, int, or tvm const

    Returns
    -------
    wrapped_tensor : max(raw_tensor, scalar)
    """

    dtype = raw_tensor.dtype

    _check_scalar(scalar)

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_VS_max', args=[scalar])


@elewise_decorator.single_scalar
@source_info_decorator()
@dtype_check_decorator
def vmins(raw_tensor, scalar):
    """
    Calculate elewise compare, return the min one of scalar or tensor's element,
     dtype of raw_tensor and scalar must be the same

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    scalar : float, int, or tvm const

    Returns
    -------
    wrapped_tensor : min(raw_tensor, scalar)
    """

    dtype = raw_tensor.dtype

    _check_scalar(scalar)

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_VS_min',  args=[scalar])


def __vlog_calculate_by_taylor(data_x):
    """
    calculate ln(raw_tensor), use taylor expansion to calculate log
    """
    # 'pylint: disable=too-many-locals, too-many-statements
    # Log threshold
    const_log_threshold_1 = 0.6666666666666667
    const_log_threshold_2 = 0.3333333333333333
    # Log value
    log_four_three = 0.28768207245178085
    log_five_three = 0.5108256237659907
    log_five_two = 0.916290731874155
    # const value
    const_neg_one = -1
    const_one = 1
    const_two = 2
    const_one_three = 0.3333333333333333
    const_half = 1.0 / 2
    const_three_four = 0.75
    const_one_five = 0.2
    const_one_four_neg = -0.25
    const_five_two = 0.4
    const_dot_six = 0.6
    float_16_max = 32768
    const_half_neg = -0.5

    const_one_nine = 0.111111111111111
    const_one_eight_neg = -0.125
    const_one_seven = 0.142857142857143
    const_one_six_neg = -0.166666666666667

    dtype = data_x.dtype
    shape = data_x.shape

    def _taylor_compute(data):
        # 'pylint: disable=too-many-locals
        taylor_nine = vmuls(data, tvm.const(const_one_nine, dtype))
        taylor_eight_1 = vadds(taylor_nine, tvm.const(const_one_eight_neg, dtype))
        taylor_eight_2 = vmul(taylor_eight_1, data)
        taylor_seven_1 = vadds(taylor_eight_2, tvm.const(const_one_seven, dtype))
        taylor_seven_2 = vmul(taylor_seven_1, data)
        taylor_six_1 = vadds(taylor_seven_2, tvm.const(const_one_six_neg, dtype))
        taylor_six_2 = vmul(taylor_six_1, data)
        taylor_five_1 = vadds(taylor_six_2, tvm.const(const_one_five, dtype))
        taylor_five_2 = vmul(taylor_five_1, data)
        taylor_four_1 = vadds(taylor_five_2, tvm.const(const_one_four_neg, dtype))
        taylor_four_2 = vmul(taylor_four_1, data)
        taylor_three_1 = vadds(taylor_four_2, tvm.const(const_one_three, dtype))
        taylor_three_2 = vmul(taylor_three_1, data)
        taylor_two_1 = vadds(taylor_three_2, tvm.const(const_half_neg, dtype))
        taylor_two_2 = vmul(taylor_two_1, data)
        taylor_one = vadds(taylor_two_2, tvm.const(const_one, dtype))
        taylor = vmul(taylor_one, data)

        return taylor

    def _log_compute_block_gt_2(data_x, res, shape):
        """
        when data > 2, use vlog directly
        when data > 32768, float16 will overflow, use log(x/2.5)+log(2.5)

        Parameters
        ----------
        data: input tensor that we want to calculate log

        Returns
        -------
        res : return of log

        """
        # 'pylint: disable=too-many-locals
        # if data > 2, use vlog
        threshold_3 = tbe.dsl.broadcast(tvm.const(const_two, dtype), shape)
        if in_dynamic_and_static_unify():
            res = vcmpsel(data_x, threshold_3, 'ge', vlog(data_x), res)
        else:
            index_3 = vcmp(data_x, threshold_3, 'ge')
            res = vsel(index_3, vlog(data_x), res)
        # if data > 32768, use log(x/2.5)+log(2.5)
        float_16_max_tensor = tbe.dsl.broadcast(tvm.const(float_16_max, dtype), shape)
        overflow_value = vmuls(data_x, const_five_two)
        res_overflow = vadds(vlog(overflow_value), log_five_two)
        if in_dynamic_and_static_unify():
            res = vcmpsel(data_x, float_16_max_tensor, 'ge', res_overflow, res)
        else:
            index_4 = vcmp(data_x, float_16_max_tensor, 'ge')
            res = vsel(index_4, res_overflow, res)
        res = cast_to(res, dtype)

        return res

    def _log_compute_block_lt_2_gt_1(data_x, shape):
        # 'pylint: disable=too-many-locals
        # phase1: index_1:data>(5/3)&&data<2
        data = vadds(data_x, tvm.const(const_neg_one, dtype))
        threshold_1 = tbe.dsl.broadcast(tvm.const(const_log_threshold_1, dtype), shape)
        data_1 = vadds(data, tvm.const(const_neg_one * const_log_threshold_1, dtype))
        data1_vmuls = vmuls(data_1, tvm.const(const_dot_six, dtype))
        if in_dynamic_and_static_unify():
            data_sel = vcmpsel(data, threshold_1, 'ge', data1_vmuls, data)
        else:
            index_1 = vcmp(data, threshold_1, 'ge')
            data_sel = vsel(index_1, data1_vmuls, data)
        data_sel = cast_to(data_sel, dtype)

        # phase2:index_2:data>(4/3)&&data<(5/3)
        threshold_2 = tbe.dsl.broadcast(tvm.const(const_log_threshold_2, dtype), shape)
        data_2 = vadds(data_sel, tvm.const(const_neg_one * const_log_threshold_2, dtype))
        data2_vmuls = vmuls(data_2, tvm.const(const_three_four, dtype))

        if in_dynamic_and_static_unify():
            data_sel = vcmpsel(data_sel, threshold_2, 'ge', data2_vmuls, data_sel)
        else:
            index_2 = vcmp(data_sel, threshold_2, 'ge')
            data_sel = vsel(index_2, data2_vmuls, data_sel)
        data_sel = cast_to(data_sel, dtype)

        # phase3: taylor expands
        taylor = _taylor_compute(data_sel)

        # phase4:return back to original data
        # add log(4/3)
        if in_dynamic_and_static_unify():
            res = vcmpsel(data_sel, threshold_2, 'ge', vadds(taylor, tvm.const(log_four_three, dtype)), taylor)
            res = cast_to(res, dtype)
            # add log(5/3)
            res = vcmpsel(data, threshold_1, 'ge', vadds(taylor, tvm.const(log_five_three, dtype)), res)
        else:
            res = vsel(index_2, vadds(taylor, tvm.const(log_four_three, dtype)), taylor)
            res = cast_to(res, dtype)
            # add log(5/3)
            res = vsel(index_1, vadds(taylor, tvm.const(log_five_three, dtype)), res)
        res = _cast(res, dtype)
        # d: vlog:

        return res

    def _log_compute_block_gt_1(data_x, shape):
        res = _log_compute_block_lt_2_gt_1(data_x, shape)
        res = _log_compute_block_gt_2(data_x, res, shape)

        return res

    def _log_compute_block_gt_half_lt_1(data_x, res, shape):
        threshold_5 = tbe.dsl.broadcast(tvm.const(const_one, dtype), shape)
        data = vadds(data_x, tvm.const(const_neg_one, dtype))
        taylor = _taylor_compute(data)
        if in_dynamic_and_static_unify():
            res = vcmpsel(data_x, threshold_5, 'le', taylor, res)
        else:
            index_6 = vcmp(data_x, threshold_5, 'le')
            res = vsel(index_6, taylor, res)
        res = cast_to(res, dtype)

        return res

    def _log_compute_block_lt_half(data_x, res, shape):
        threshold_4 = tbe.dsl.broadcast(tvm.const(const_half, dtype), shape)
        if in_dynamic_and_static_unify():
            res = vcmpsel(data_x, threshold_4, 'le',
                          vmuls(_log_compute_block_gt_1(vrec(data_x, "high_precision"), shape), const_neg_one), res)
        else:
            index_5 = vcmp(data_x, threshold_4, 'le')
            res = vsel(index_5, vmuls(_log_compute_block_gt_1(vrec(data_x, "high_precision"), shape),
                                      const_neg_one), res)
        res = cast_to(res, dtype)

        return res

    res = _log_compute_block_gt_1(data_x, shape)

    res = _log_compute_block_gt_half_lt_1(data_x, res, shape)

    res = _log_compute_block_lt_half(data_x, res, shape)

    return res


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vlog(raw_tensor, impl_mode="high_performance"):
    """
    calculate ln(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor
    impl_mode : only support high_performance and high_precision

    Returns
    -------
    wrapped_tensor : log(raw_tensor)
    """
    if not intrinsic_check_support("Intrinsic_vln", "float32") and impl_mode == "high_precision":
        return __vlog_calculate_by_taylor(raw_tensor)

    dtype = raw_tensor.dtype
    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_log')


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vexp(raw_tensor):
    """
    calculate exp(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    Returns
    -------
    wrapped_tensor : exp(raw_tensor)
    """
    dtype = raw_tensor.dtype

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_exp')


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vabs(raw_tensor):
    """
    calculate abs(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    Returns
    -------
    wrapped_tensor : abs(raw_tensor)
    """
    dtype = raw_tensor.dtype

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_abs')


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vsignbit(raw_tensor):
    """
    calculate vsignbit(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    Returns
    -------
    wrapped_tensor : vsignbit(raw_tensor)
    """
    dtype = raw_tensor.dtype

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_signbit')


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vrec(raw_tensor, impl_mode="high_performance"):
    """
    calculate vrec(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor
    impl_mode : only support high_performance and high_precision

    Returns
    -------
    wrapped_tensor : vrec(raw_tensor)
    """
    dtype = raw_tensor.dtype

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_rec', args=[impl_mode])


def _check_multi_compute_pattern(pattern, *tensors):
    """
    check tensors.op is matched with pattern or not.
    """
    for pat in pattern:
        if isinstance(pat, tuple):
            if len(tensors) != len(pat):
                return False
            tmp_tensors = ()
            for idx, tag in enumerate(pat):
                try:
                    tensor_tag = tensors[idx].op.tag
                except Exception:       # 'pylint: disable=broad-except
                    tensor_tag = "unknown"
                if tag not in tensor_tag:
                    return False
                if not isinstance(tensors[idx].op, tvm.PlaceholderOp):
                    tmp_tensors += tuple(tensors[idx].op.input_tensors)
            tensors = tmp_tensors
        elif isinstance(pat, str):
            if len(tensors) != 1:
                return False
            try:
                tensor_tag = tensors[0].op.tag
            except Exception:       # 'pylint: disable=broad-except
                tensor_tag = "unknown"
            if pat not in tensor_tag:
                return False
            if isinstance(tensors[0].op, tvm.PlaceholderOp):
                tensors = []
            else:
                tensors = tuple(tensors[0].op.input_tensors)
        else:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"Valid pattern list should be a string or tuple, but now is {type(pat)}."}
            raise RuntimeError(dict_args, get_error_message(dict_args))
    return True


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vnot(raw_tensor):
    """
    calculate vnot(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    Returns
    -------
    wrapped_tensor : vnot(raw_tensor)
    """
    dtype = raw_tensor.dtype

    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_not')


def __vsqrt_calculate_by_newton(raw_tensor):
    """
    calculate vsqrt(raw_tensor), use newton iteration to calcuate sqrt

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    Returns
    -------
    wrapped_tensor : vsqrt(raw_tensor)
    """

    const_half = 1.0 / 2
    sqrt_const_iter = 3

    dtype = raw_tensor.dtype

    raw_tensor_news = _cast_tensors_for_instr("vlog", [raw_tensor])
    init_res = vlog(raw_tensor_news[0])
    init_res = vmuls(init_res, tvm.const(const_half))
    init_res = vexp(init_res)

    for _ in range(sqrt_const_iter):
        vdiv_inputs = _cast_tensors_for_instr("vdiv", [raw_tensor, init_res])
        res = vdiv(vdiv_inputs[0], vdiv_inputs[1])
        vadd_inputs = _cast_tensors_for_instr("vadd", [res, init_res])
        res = vadd(vadd_inputs[0], vadd_inputs[1],)
        res = vmuls(res, tvm.const(const_half, dtype))
        init_res = res
    return res


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vsqrt(raw_tensor, impl_mode="high_performance"):
    """
    calculate vsqrt(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor
    impl_mode : only support high_performance and high_precision

    Returns
    -------
    wrapped_tensor : vsqrt(raw_tensor)
    """
    if not intrinsic_check_support("Intrinsic_vsqrt"):
        if impl_mode == "high_precision":
            return __vsqrt_calculate_by_newton(raw_tensor)
        dtype = raw_tensor.dtype
        res = __single_elewise_op(raw_tensor, dtype, 'elewise_single_rsqrt')
        return vrec(res, "high_precision")
    dtype = raw_tensor.dtype
    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_sqrt')


def __vrsqrt_calculate_by_newton(raw_tensor):
    """
    calculate vrsqrt(raw_tensor), use newton iteration to calcuate sqrt

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor

    Returns
    -------
    wrapped_tensor : vrsqrt(raw_tensor)
    """
    const_half = 1.0 / 2
    sqrt_const_iter = 3

    dtype = raw_tensor.dtype
    raw_tensor_news = _cast_tensors_for_instr("vlog", [raw_tensor])
    init_res = vlog(raw_tensor_news[0])
    init_res = vmuls(init_res, tvm.const(const_half))
    init_res = vexp(init_res)

    for _ in range(sqrt_const_iter):
        vdiv_inputs = _cast_tensors_for_instr("vdiv", [raw_tensor, init_res])
        res = vdiv(vdiv_inputs[0], vdiv_inputs[1])
        vadd_inputs = _cast_tensors_for_instr("vadd", [res, init_res])
        res = vadd(vadd_inputs[0], vadd_inputs[1],)
        res = vmuls(res, tvm.const(const_half, dtype))
        init_res = res
    return vrec(res, "high_precision")


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def vrsqrt(raw_tensor, impl_mode="high_performance"):
    """
    calculate vrsqrt(raw_tensor)

    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.te.tensor
    impl_mode : only support high_performance and high_precision

    Returns
    -------
    wrapped_tensor : vrsqrt(raw_tensor)
    """
    if not intrinsic_check_support("Intrinsic_vsqrt") and impl_mode == "high_precision":
        return __vrsqrt_calculate_by_newton(raw_tensor)
    dtype = raw_tensor.dtype
    return __single_elewise_op(raw_tensor, dtype, 'elewise_single_rsqrt')


def _check_elewise_single_shape(input_tensor):
    """
    check the input_tensor's shape whether is positive integer
    :param input_tensor
    """
    for _, shape_i in enumerate(input_tensor.shape):
        if shape_i.value < 0 or not isinstance(shape_i.value, int):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"The input shape value {shape_i.value} must be a positive integer or -1!"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

# 'pylint: disable=too-many-locals, too-many-branches, too-many-statements


def __single_elewise_op(input_tensor, dtype, op_name, args=None):
    """
    factory method of single elewise operations
    """
    if not operation_context.in_dynamic():
        _check_elewise_single_shape(input_tensor)
    shape = input_tensor.shape
    if op_name == "elewise_single_log":
        lambda_func = lambda *indice: tvm.log(input_tensor(*indice))
    elif op_name == "elewise_single_exp":
        lambda_func = lambda *indice: tvm.exp(input_tensor(*indice))
    elif op_name == "elewise_single_rec":
        lambda_func = lambda *indice: 1 / input_tensor(*indice)
    elif op_name == "elewise_single_VS_add":
        lambda_func = lambda *indice: input_tensor(*indice) + util_astype(args[0], dtype)
    elif op_name == "elewise_single_VS_mul":
        lambda_func = lambda *indice: input_tensor(*indice) * util_astype(args[0], dtype)
    elif op_name == "elewise_single_VS_max":
        lambda_func = lambda *indice: tvm.max(input_tensor(*indice), util_astype(args[0], dtype))
    elif op_name == "elewise_single_VS_min":
        lambda_func = lambda *indice: tvm.min(input_tensor(*indice), util_astype(args[0], dtype))
    elif op_name == "elewise_single_real":
        lambda_func = lambda *indice: tvm.real(input_tensor(*indice))
    elif op_name == "elewise_single_sigmoid":
        lambda_func = lambda *indice: tvm.sigmoid(input_tensor(*indice))
    elif op_name == "elewise_single_tanh":
        lambda_func = lambda *indice: tvm.tanh(input_tensor(*indice))
    elif op_name == "elewise_single_abs":
        if input_tensor.dtype in ("int64", "complex32", "complex64"):
            lambda_func = lambda *indice: tvm.abs(input_tensor(*indice))
        else:
            lambda_func = lambda *indice: tvm.select(input_tensor(*indice) >= 0, input_tensor(*indice),
                                                    - input_tensor(*indice))
    elif op_name == "elewise_single_relu":
        lambda_func = lambda *indice: tvm.select(input_tensor(*indice) >= 0, input_tensor(*indice),
                                                 tvm.const(0, dtype=dtype))
    elif op_name == "elewise_single_not":
        lambda_func = lambda *indice: ~input_tensor(*indice)
    elif op_name == "elewise_single_signbit":
        lambda_func = lambda *indice: tvm.sign_bit(input_tensor(*indice))
    elif op_name == "elewise_single_sqrt":
        lambda_func = lambda *indice: tvm.sqrt(input_tensor(*indice))
    elif op_name == "elewise_single_rsqrt":
        lambda_func = lambda *indice: tvm.rsqrt(input_tensor(*indice))
    else:
        dict_args = {"errCode": "E90003",
                     "detailed_cause": f"operation {op_name} is not support yet!"}
        raise RuntimeError(dict_args, get_error_message(dict_args))
    name = f"{op_name.split('_')[-1]}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    with tvm.tag_scope(op_name):
        tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

    is_use_newton_iter = False
    if op_name == "elewise_single_rec" and args[0] == "high_precision":
        is_use_newton_iter = True

    if is_use_newton_iter:
        newton_iter_num = 1 if get_soc_spec("SHORT_SOC_VERSION") in (ASCEND_310, ASCEND_310B, AS31XM1) else 2
        name_pre = f"{op_name.split('_')[-1]}_"
        const_num_neg_one = tvm.const(-1, dtype=dtype)
        const_num_two = tvm.const(2, dtype=dtype)

        # newton iteration formula is x(n) = x(n-1)(2 - ax(n-1))
        for _ in range(newton_iter_num):
            # the name of each compute
            name = f"{name_pre}{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            # compute tmp_mul = a*x(n-1)
            with tvm.tag_scope("elewise_binary_mul"):
                tmp_mul = tvm.compute(shape, lambda *indice: input_tensor(*indice) * tmp(*indice), name=name,
                                      attrs={"_type": ComputeType.ELEWISE})

            name = f"{name_pre}{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            # compute tmp_negative = -1*temp_mul
            # 'pylint: disable=cell-var-from-loop
            with tvm.tag_scope("elewise_single_VS_mul"):
                tmp_negative = tvm.compute(shape, lambda *indice: tmp_mul(*indice) * const_num_neg_one, name=name,
                                           attrs={"_type": ComputeType.ELEWISE})

            name = f"{name_pre}{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            # compute tmp_plus = 2 + tmp_negative
            # 'pylint: disable=cell-var-from-loop
            with tvm.tag_scope("elewise_single_VS_add"):
                tmp_plus = tvm.compute(shape, lambda *indice: tmp_negative(*indice) + const_num_two, name=name,
                                       attrs={"_type": ComputeType.ELEWISE})
            name = f"{name_pre}{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            # compute tmp = x(n-1)*tmp_plus
            # 'pylint: disable=cell-var-from-loop
            with tvm.tag_scope("elewise_binary_mul"):
                tmp = tvm.compute(shape, lambda *indice: tmp_plus(*indice) * tmp(*indice), name=name,
                                  attrs={"_type": ComputeType.ELEWISE})

    return tmp


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vmul(lhs, rhs):
    """
    calculate elewise multiply

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.te.tensor
        right hand tensor

    Returns
    -------
    wrapped_tensor : lhs*rhs
    """
    if not isinstance(rhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The second input type must be tvm.te.tensor, while type is {type(rhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    return __binary_elewise_op(lhs, rhs, "elewise_binary_mul")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vdiv(lhs, rhs):
    """
    calculate elewise div

    Parameters
    -----
    lhs: wrapped_tensor or tvm.te.tensor
         divisor tensor
    rhs: wrapped_tensor or tvm.te.tensor
         divided tensor

    returns
    -----
    wrapped_tensor: lhs / rhs
    """
    _check_tensor(rhs)

    if (lhs.dtype == rhs.dtype and lhs.dtype == "int64"):
        return __binary_elewise_op(lhs, rhs, "elewise_binary_div")

    if not intrinsic_check_support("Intrinsic_vdiv"):
        dtype = rhs.dtype
        reciprocal_rhs = __single_elewise_op(rhs, dtype, 'elewise_single_rec', ['high_precision'])
        vdiv_value = __binary_elewise_op(lhs, reciprocal_rhs, "elewise_binary_mul")
        return vdiv_value

    return __binary_elewise_op(lhs, rhs, "elewise_binary_div")


def __vmod_small_hisi(lhs, rhs):
    # small hisi
    dtype = lhs.dtype
    res_div = vdiv(lhs, rhs)
    res_floor = floor(res_div)
    res_floor = _cast(res_floor, dtype)
    res_mul = vmul(rhs, res_floor)
    res = vsub(lhs, res_mul)

    return _cast(res, dtype)


def __vmod_cloud(lhs, rhs):
    # cloud
    dtype = lhs.dtype
    lhs = _cast(lhs, "float32")
    rhs = _cast(rhs, "float32")
    res_div = vdiv(lhs, rhs)
    if platform_info.api_check_support("tbe.dsl.floor", "f322f32"):
        res_floor = floor(res_div, "float32")
    else:
        res_floor = floor(res_div)
        res_floor = _cast(res_floor, "float32")
    res_mul = vmul(rhs, res_floor)
    res = vsub(lhs, res_mul)

    return _cast(res, dtype)


def __vmod_int64(lhs, rhs):
    # int64
    res = __binary_elewise_op(lhs, rhs, "elewise_binary_mod")
    return res


# 'pylint: disable=too-many-locals
def __vmod_mini(lhs, rhs):
    # mini
    dtype = rhs.dtype
    rhs_f16 = rhs
    # 1. calculate result for testing, using float32 for better precision
    lhs = _cast(lhs, "float32")
    rhs = _cast(rhs, "float32")
    test_div = vmul(lhs, vrec(rhs, "high_precision"))
    test_div = _cast(test_div, "float16")
    test_floor = _cast(floor(test_div), "float32")
    test_res = vsub(lhs, vmul(rhs, test_floor))

    # 2. correct the floor result, using float16
    test_res = _cast(test_res, dtype)
    test_floor = _cast(test_floor, dtype)
    zero = tbe.dsl.broadcast(0.0, lhs.shape, dtype)

    if in_dynamic_and_static_unify():
        # rhs positive: 0 <= res < rhs
        prhs_floor = vcmpsel(test_res, zero, 'lt', vadds(test_floor, -1.0), test_floor)
        # rhs negative: rhs < res <= 0
        nrhs_floor = vcmpsel(test_res, zero, 'gt', vadds(test_floor, -1.0), test_floor)

        # according to positive and negative rhs to choose p_floor or n_floor
        result_floor = vcmpsel(rhs_f16, zero, 'gt', prhs_floor, nrhs_floor)
    else:
        # rhs positive: 0 <= res < rhs
        prhs = vcmp(test_res, zero, 'lt', mode='bool')
        prhs_floor = vsel(prhs, vadds(test_floor, -1.0), test_floor)
        # rhs negative: rhs < res <= 0
        nrhs = vcmp(test_res, zero, 'gt', mode='bool')
        nrhs_floor = vsel(nrhs, vadds(test_floor, -1.0), test_floor)

        # according to positive and negative rhs to choose p_floor or n_floor
        rhs_f16_gt_zero = vcmp(rhs_f16, zero, 'gt', mode='bool')
        result_floor = vsel(rhs_f16_gt_zero, prhs_floor, nrhs_floor)

    # 3. calculate the final result, using float32 for better precision
    result_floor = _cast(result_floor, "float32")
    result = vsub(lhs, vmul(rhs, result_floor))

    return _cast(result, dtype)


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vmod(lhs, rhs):
    """
    calculate element-wise remainder of division

    Parameters
    -----
    lhs : wrapped_tensor or tvm.te.tensor
          left hand tensor

    rhs : wrapped_tensor or tvm.te.tensor
          right hand tensor

    Returns
    -----
    wrapped_tensor : lhs - floor(lhs/rhs) * rhs
    """
    _check_tensor(lhs, position="first")
    _check_tensor(rhs, position="second")

    _check_elewise_binary_shape(lhs, rhs)
    if lhs.dtype != rhs.dtype:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"dtype must be same, while lhType is {lhs.dtype}, rhType is {rhs.dtype}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    # Using scalar achieve int64 compute, which only running in some slug versions.
    if(lhs.dtype == "int64"):
        res = __vmod_int64(lhs, rhs)
        return res
    # cloud using vdiv. mini using vrec for division calculation,
    # and mini should improve vmod calculation accuracy.
    if not intrinsic_check_support("Intrinsic_vdiv") and not intrinsic_check_support("Intrinsic_vconv", "f322s32f"):
        res = __vmod_mini(lhs, rhs)
    elif not intrinsic_check_support("Intrinsic_vconv", "f322s32f"):
        if lhs.dtype not in ("float16",):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"dtype must be float16, while dtype is {lhs.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        res = __vmod_small_hisi(lhs, rhs)
    else:
        if lhs.dtype not in ("float16", "float32"):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"dtype must be in support_list, while dtype is {lhs.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        res = __vmod_cloud(lhs, rhs)

    return res


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vadd(lhs, rhs):
    """
    calculate elewise add

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    Returns
    -------
    wrapped_tensor : lhs + rhs
    """
    _check_tensor(rhs, position="second")

    def is_conv_oper(tensor):
        if hasattr(tensor.op, "reduce_axis") and len(tensor.op.reduce_axis) == 2 and \
                hasattr(tensor.op, "tag") and "conv" in tensor.op.tag:
            return True
        if tensor.op.input_tensors:
            for input_tensor in tensor.op.input_tensors:
                return is_conv_oper(input_tensor)
        else:
            return False

    if is_conv_oper(rhs):
        return __binary_elewise_op(rhs, lhs, "elewise_binary_add")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_add")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vsub(lhs, rhs):
    """
    calculate elewise sub

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    Returns
    -------
    wrapped_tensor : lhs - rhs
    """
    _check_tensor(rhs, position="second")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_sub")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vgcd(lhs, rhs):
    """
    calculate elewise greatest common divisor

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    Returns
    -------
    wrapped_tensor : vgcd(lhs, rhs)
    """
    _check_tensor(rhs, position="second")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_gcd")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vmin(lhs, rhs):
    """
    calculate elewise compare, return the min one
    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    Returns
    -------
    wrapped_tensor : min(lhs , rhs)
    """
    _check_tensor(rhs, position="second")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_min")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vmax(lhs, rhs):
    """
    calculate elewise compare, return the min one
    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    Returns
    -------
    wrapped_tensor : max(lhs , rhs)
    """
    _check_tensor(rhs, position="second")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_max")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vor(lhs, rhs):
    """
    calculate bitwise or op, return the or value
    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    Returns
    -------
    wrapped_tensor : or(lhs , rhs)
    """
    _check_tensor(rhs, position="second")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_or")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def vand(lhs, rhs):
    """
    calculate bitwise and op, return the and value
    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    Returns
    -------
    wrapped_tensor : max(lhs , rhs)
    """
    _check_tensor(rhs, position="second")

    return __binary_elewise_op(lhs, rhs, "elewise_binary_and")


@elewise_decorator.binary_scalar
@source_info_decorator()
@dtype_check_decorator
def vaxpy(lhs, rhs, scalar):
    """
    calculate elewise scalar*lhs + rhs, return the min one
    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    rhs : wrapped_tensor or tvm.te.tensor
        left hand tensor
    Returns
    -------
    wrapped_tensor : max(lhs , rhs)
    """
    _check_tensor(rhs, position="second")
    _check_scalar(scalar, position="third")
    # 5hd scene can not enable paddinng for ternary instructions due to complex memory reuse
    if dynamic_static_unify_fractal_format():
        lhs = cast_to(lhs, rhs.dtype)
        scalar = util_astype(scalar, rhs.dtype)
        return vadd(vmuls(lhs, scalar), rhs)

    return __binary_elewise_op(lhs, rhs, "elewise_binary_scalar_axpy", args=[scalar])


def _get_rhs_index(rhs, indice):
    # get the _rhs indices
    if isinstance(rhs, tvm.Tensor):
        return rhs(*indice)
    return rhs


def _vcmp_input_check(lhs, rhs, operation, mode):
    # check vcmp input validity
    _check_tensor(lhs, position="first")

    if isinstance(rhs, tvm.Tensor):
        if lhs.dtype != rhs.dtype:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"dtype must be the same, while lhs is {lhs.dtype}, rhs is {rhs.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    if operation not in ['eq', 'ne', 'lt', 'gt', 'ge', 'le']:
        dict_args = {"errCode": "E90002",
                     "detailed_cause": f"vcmp only support eq, ne, lt, gt, ge and le, not support {operation}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if mode not in ['bool', 'bit']:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The op's mode must be bit and bool, while mode is {mode}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))


def _vcmp_in_unify(lhs, rhs, operation, mode):
    def _generate_dynamic_lambda_func(_lhs, _rhs, _operation, _dtype):
        """
        get dynamic lambda func under different dtypes
        """
        if _operation == 'lt':
            lambda_func = lambda *indice: (_lhs(*indice) < _get_rhs_index(_rhs, indice)).astype(_dtype)
        elif _operation == 'gt':
            lambda_func = lambda *indice: (_lhs(*indice) > _get_rhs_index(_rhs, indice)).astype(_dtype)
        elif _operation == 'le':
            lambda_func = lambda *indice: (_lhs(*indice) <= _get_rhs_index(_rhs, indice)).astype(_dtype)
        elif _operation == 'ge':
            lambda_func = lambda *indice: (_lhs(*indice) >= _get_rhs_index(_rhs, indice)).astype(_dtype)
        elif _operation == 'eq':
            lambda_func = lambda *indice: (tvm.expr.EQ(_lhs(*indice), _get_rhs_index(_rhs, indice))).astype(_dtype)
        else:
            lambda_func = lambda *indice: (tvm.expr.NE(_lhs(*indice), _get_rhs_index(_rhs, indice))).astype(_dtype)

        return lambda_func

    def _dynamic_bit_vcmp(_lhs, _rhs, _operation):
        """
        dynamic vcmp bit mode use dtype uint1
        """
        dyn_bit_lambda_func = _generate_dynamic_lambda_func(_lhs, _rhs, _operation, "uint1")

        op_tag = f"elewise_binary_vcmpv_{_operation}"

        dyn_bit_op_name = f"vcmp_bit_res_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1

        with tvm.tag_scope(op_tag):
            dyn_vcmp_bit_res = tvm.compute(shape, dyn_bit_lambda_func, name=dyn_bit_op_name,
                                           attrs={"_type": ComputeType.ELEWISE})

        return dyn_vcmp_bit_res

    def _dynamic_b64_bool_vcmp(_lhs, _rhs, _operation):
        """
        dynamic vcmp b64 bool mode use dtype int8
        """
        dync_bool_lambda_func = _generate_dynamic_lambda_func(_lhs, _rhs, _operation, "int8")

        op_tag = f"elewise_binary_vcmpv_{_operation}"

        dyn_bool_op_name = f"vcmp_{_lhs.dtype}_res_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1

        with tvm.tag_scope(op_tag):
            dyn_vcmp_bool_res = tvm.compute(shape, dync_bool_lambda_func, name=dyn_bool_op_name,
                                            attrs={"_type": ComputeType.ELEWISE})

        return dyn_vcmp_bool_res

    def _if_regbase_b32(_lhs, _rhs):
        soc_name = get_soc_spec("SHORT_SOC_VERSION")
        if _lhs.dtype in ("int32", "uint32", "float32") \
            and _rhs.dtype in ("int32", "uint32", "float32") \
            and soc_name in ("Ascend610Lite", "BS9SX2A", "MC61AM21A"):
            return True

        return False

    def _if_nano_br():
        return int(get_soc_spec("ubblock_size")) == NANO_BLOCK_SIZE

    def _dynamic_common_bool_vcmp(_lhs, _rhs, _operation):
        """
        dynamic vcmp common bool mode impl
        """
        # dynamic vcmp common impl consist three steps:
        # 1. bit mode with uint1
        # 2. vsel with bit mode res
        # 3. cast vsel res
        bool_bit_tag = f"elewise_binary_vcmpv_{_operation}"

        dyn_bit_lambda_func = _generate_dynamic_lambda_func(_lhs, _rhs, _operation, "uint1")

        dyn_bool_bit_name = f"vcmp_bool_bit_res_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1

        with tvm.tag_scope(bool_bit_tag):
            dyn_bool_bit_res = tvm.compute(shape, dyn_bit_lambda_func, name=dyn_bool_bit_name,
                                           attrs={"_type": ComputeType.ELEWISE})

        # vsel calculation with bit mode res
        bool_vsel_tag = "elewise_multiple_sel"

        vsel_dtype = "float32" if _if_regbase_b32(_lhs, _rhs) else "float16"
        vsel_left_const = tvm.const(1, vsel_dtype)
        vsel_right_const = tvm.const(0, vsel_dtype)

        dyn_vsel_lambda_func = lambda *indice: tvm.select(dyn_bool_bit_res(*indice), vsel_left_const, vsel_right_const)

        dyn_bool_vsel_name = f"vcmp_bool_vsel_res_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1
        with tvm.tag_scope(bool_vsel_tag):
            attr_dict = {"_type": ComputeType.ELEWISE}
            if _if_nano_br():
                attr_dict["_mode"] = 1
            dyn_bool_vsel_res = tvm.compute(shape, dyn_vsel_lambda_func, name=dyn_bool_vsel_name, attrs=attr_dict)

        if _if_regbase_b32(_lhs, _rhs):
            return cast_to(dyn_bool_vsel_res, "bool")
        
        if _if_nano_br():
            return cast_to(dyn_bool_vsel_res, "int8")

        # cast vsel res from float16 to bool
        bool_cast_tag = "elewise_single_cast"

        dyn_cast_lambda_func = lambda *indice: dyn_bool_vsel_res(*indice).astype("bool")

        dyn_bool_cast_name = f"vcmp_bool_cast_res_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1
        with tvm.tag_scope(bool_cast_tag):
            dyn_bool_cast_res = tvm.compute(shape, dyn_cast_lambda_func, name=dyn_bool_cast_name,
                                            attrs={"_type": ComputeType.CAST})

        return dyn_bool_cast_res

    if not isinstance(rhs, tvm.Tensor):
        rhs = get_tvm_scalar(rhs, lhs.dtype)
    shape = lhs.shape
    if mode == "bit":
        return _dynamic_bit_vcmp(lhs, rhs, operation)
    # bool mode
    if lhs.dtype in ("uint64", "int64"):
        # vcmp impl by combined_instructions
        return _dynamic_b64_bool_vcmp(lhs, rhs, operation)

    return _dynamic_common_bool_vcmp(lhs, rhs, operation)


def _vcmp_in_pre_static(lhs, rhs, operation, mode):
    def _pre_static_auto_cast(_lhs, _rhs, _mode):
        """
        pre static input dtype cast
        """
        # the get_cmpmask need 16b aligned, so if is float32, should cast to float16
        # bit model using vcmpv. v200 support float32. v100 only support float16
        supported_types = None
        if _mode == "bit":
            supported_types = ['float16']
        # the output is bool or uint8, is not the same as input, no need to cast to back in auto schedule
        _lhs = auto_cast_tensor(_lhs, 'vcmp', supported_types, is_auto_cast=False)
        if isinstance(_rhs, tvm.Tensor):
            _rhs = auto_cast_tensor(_rhs, 'vcmp', supported_types, is_auto_cast=False)
        else:
            _rhs = get_tvm_scalar(_rhs, _lhs.dtype)

        return _lhs, _rhs

    def _generate_pre_static_lambda_func(_lhs, _rhs, _operation):
        """
        get pre-static lambda func under different dtypes
        """
        if _operation == 'lt':
            lambda_func = lambda *indice: _lhs(*indice) < _get_rhs_index(_rhs, indice)
        elif _operation == 'gt':
            lambda_func = lambda *indice: _lhs(*indice) > _get_rhs_index(_rhs, indice)
        elif _operation == 'le':
            lambda_func = lambda *indice: _lhs(*indice) <= _get_rhs_index(_rhs, indice)
        elif _operation == 'ge':
            lambda_func = lambda *indice: _lhs(*indice) >= _get_rhs_index(_rhs, indice)
        elif _operation == 'eq':
            lambda_func = lambda *indice: tvm.expr.EQ(_lhs(*indice), _get_rhs_index(_rhs, indice))
        else:
            lambda_func = lambda *indice: tvm.expr.NE(_lhs(*indice), _get_rhs_index(_rhs, indice))

        return lambda_func

    def _pre_static_bit_vcmp(_operation):
        """
        pre-static vcmp bit mode impl
        """
        k = tvm.reduce_axis((0, 8), name='k')
        res_shape = shape
        res_shape[-1] = res_shape[-1] // 8

        def _compute(*index):
            res_index = []
            for i, value in enumerate(index):
                if i == len(index) - 1:
                    res_index.append(value * 8 + k)
                else:
                    res_index.append(value)
            tensor = tvm.bit(lambda_func(*res_index).astype('uint8'), axis=k)
            return tensor

        pre_static_bit_tag = f"{cmp_op}|{_operation}|bit"

        with tvm.tag_scope(pre_static_bit_tag):
            pre_static_bit_res = tvm.compute(res_shape, _compute, name='output')

        return pre_static_bit_res

    def _pre_static_bool_vcmp(_operation):
        """
        pre-static vcmp bool mode impl
        """
        pre_static_bool_tag = f"{cmp_op}|{_operation}|bool"

        with tvm.tag_scope(pre_static_bool_tag):
            pre_static_bool_res = tvm.compute(shape, lambda_func, name=name)

        return pre_static_bool_res

    shape = shape_to_list(lhs.shape)
    lhs, rhs = _pre_static_auto_cast(lhs, rhs, mode)

    lambda_func = _generate_pre_static_lambda_func(lhs, rhs, operation)

    cmp_op = "emit_insn_elewise_binary_cmp"

    name = f"{cmp_op.split('_')[-1]}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    if mode == "bit":
        return _pre_static_bit_vcmp(operation)

    return _pre_static_bool_vcmp(operation)


# 'pylint: disable=too-many-branches, too-many-statements
@elewise_decorator.vcmp
@source_info_decorator()
@dtype_check_decorator
def vcmp(lhs, rhs, operation='lt', mode='bool'):
    """
    calculate elewise compare

    Parameters
    ----------
    lhs : left input, only support tensor

    rhs : right input, support tensor and scalar

    operation : operator type, eq, ne, lt, gt, ge, le

    mode : bool, the dtype of return value is bool
           bit, the dtype of return value is uint8(dynamic is uint1)

    Returns
    -------
    wrapped_tensor
    """
    _vcmp_input_check(lhs, rhs, operation, mode)
    if in_dynamic_and_static_unify():
        return _vcmp_in_unify(lhs, rhs, operation, mode)

    return _vcmp_in_pre_static(lhs, rhs, operation, mode)


@source_info_decorator()
@dtype_check_decorator
def vlogic(lhs, rhs=None, operation='logic_and'):
    """
    calculate elewise logic operation

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        left hand tensor

    rhs : wrapped_tensor or tvm.te.tensor
        right hand tensor

    operation : operator type, logic_and, logic_or, logic_not

    Returns
    -------
    wrapped_tensor
    """
    if in_dynamic_and_static_unify():
        dict_args = {"errCode": "E90003",
                     "detailed_cause": "dynamic and static shape not support vlogic"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if operation not in ['logic_and', 'logic_or', 'logic_not']:
        dict_args = {"errCode": "E90002",
                     "detailed_cause": f"vlogic only support logic_and, logic_or and logic_not, but now: {operation}."}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if not isinstance(lhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The lhs input type must be tvm.te.tensor, while type is {type(lhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if operation != "logic_not" and not isinstance(rhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The rhs input type must be tvm.te.tensor, while type is {type(rhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if operation == "logic_not":
        rhs = tvm.placeholder(lhs.shape, name="rhs", dtype=lhs.dtype)
    # the output is bool is not the same as input,
    # no need to cast to back in auto schedule
    return __binary_elewise_op(lhs, rhs, "elewise_binary_logic", args=[operation[6:]])


# 'pylint: disable=too-many-locals, too-many-branches, too-many-statements
def __binary_elewise_op(tensor_l, tensor_r, op_name, args=None):
    """
    factory method of binary elewise operations
    """
    _check_elewise_binary_shape(tensor_l, tensor_r)
    if tensor_l.dtype != tensor_r.dtype and op_name != "elewise_binary_scalar_axpy":
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"dtype must be same, while lhType: {tensor_l.dtype}, rhType :{tensor_r.dtype}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))
    shape = tensor_l.shape
    tag_name = op_name.split("_")[-1]
    name = f"{tag_name}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    if op_name == "elewise_binary_add":
        lambda_func = lambda *indice: tensor_l(*indice) + tensor_r(*indice)
    elif op_name == "elewise_binary_sub":
        lambda_func = lambda *indice: tensor_l(*indice) - tensor_r(*indice)
    elif op_name == "elewise_binary_gcd":
        lambda_func = lambda *indice: tvm.gcd(tensor_l(*indice), tensor_r(*indice))
    elif op_name == "elewise_binary_div":
        if tensor_l.dtype == "int64":
            lambda_func = lambda *indice: tvm.floordiv(tensor_l(*indice), tensor_r(*indice))
        else:
            lambda_func = lambda *indice: tensor_l(*indice) / tensor_r(*indice)
    elif op_name == "elewise_binary_mul":
        lambda_func = lambda *indice: tensor_l(*indice) * tensor_r(*indice)
    elif op_name == "elewise_binary_mod":
        lambda_func = lambda *indice: tensor_l(*indice) % tensor_r(*indice)
    elif op_name == "elewise_binary_min":
        lambda_func = lambda *indice: tvm.min(tensor_l(*indice), tensor_r(*indice))
    elif op_name == "elewise_binary_max":
        lambda_func = lambda *indice: tvm.max(tensor_l(*indice), tensor_r(*indice))
    elif op_name == "elewise_binary_and":
        lambda_func = lambda *indice: tensor_l(*indice) & tensor_r(*indice)
    elif op_name == "elewise_binary_or":
        lambda_func = lambda *indice: tensor_l(*indice) | tensor_r(*indice)
    elif op_name == "elewise_binary_powi":
        lambda_func = lambda *indice: tvm.tbepower(tensor_l(*indice), tensor_r(*indice))
    elif op_name == "elewise_binary_complex":
        lambda_func = lambda *indice: tvm.complex(tensor_l(*indice), tensor_r(*indice))
    elif op_name == "elewise_binary_scalar_axpy":
        intr = f"v{tag_name}"
        is_support_dtype = intrinsic_check_support(f"Intrinsic_{intr}", tensor_l.dtype)
        if tensor_l.dtype != tensor_r.dtype:
            if tensor_l.dtype != "float16" or tensor_r.dtype != "float32":
                dict_args = {"errCode": "E90002",
                             "detailed_cause": f"vaxpy not support mix dtype: {tensor_l.dtype} and {tensor_r.dtype}."}
                raise RuntimeError(dict_args, get_error_message(dict_args))
        elif not is_support_dtype:
            dict_args = {"errCode": "E90002",
                         "detailed_cause": f"vaxpy not support such dtype {tensor_l.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        rtype = tensor_r.dtype
        lambda_func = \
            lambda *indice: tvm.expr.Cast(rtype, tensor_l(*indice)) * util_astype(args[0], rtype) + tensor_r(*indice)
        op_name = op_name + ("|1,1" if tensor_l == tensor_r else "|0,0")
    elif op_name == "elewise_binary_logic":
        operation = args[0]
        if operation == 'and':
            lambda_func = lambda *indice: tensor_l(*indice) & tensor_r(*indice)
        elif operation == 'or':
            lambda_func = lambda *indice: tensor_l(*indice) | tensor_r(*indice)
        elif operation == 'not':
            lambda_func = lambda *indice: ~tensor_l(*indice)
        else:
            dict_args = {"errCode": "E90002",
                         "detailed_cause": f"vlogic do not support the input operation {operation}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    else:
        dict_args = {"errCode": "E90003",
                     "detailed_cause": f"operation {op_name} not support yet!"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if op_name in ("elewise_binary_logic",):
        for arg in args:
            op_name = f"{op_name}|{arg}"

    with tvm.tag_scope(op_name):
        tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

    return tmp


def _check_elewise_binary_shape(lhs, rhs):
    """
    check elewise binary shape
    :param lhs: left tensor
    :param rhs: right tensor
    :return:
    """
    if len(lhs.shape) != len(rhs.shape):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The lhs len: {len(lhs.shape)} and rhs len: {len(rhs.shape)} need be equal!"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    for _l, _r in zip(lhs.shape, rhs.shape):
        # fix cube fusion new vars problem
        skip_judge_type = (tvm.Var, expr.Max, expr.Mul)
        if isinstance(_l, skip_judge_type) or isinstance(_r, skip_judge_type):
            continue

        if not equal(_l, _r):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"lhs shape {lhs.shape} and rhs shape {rhs.shape} need be equal!"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    if not operation_context.in_dynamic():
        for sh_value in lhs.shape:
            if sh_value.value < 0 or not isinstance(sh_value.value, int):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"The input shape value {sh_value.value} must be a positive integer!"}
                raise RuntimeError(dict_args, get_error_message(dict_args))


def _check_is_equal(lhs, rhs):
    """
    check lhs and rhs value is equal
    :param lhs: left tensor
    :param rhs: right tensor
    :return:
    """
    if lhs.value == rhs.value:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"lhs and rhs value {lhs.value}, {rhs.value} must be unequal while all scalar!"}
        raise RuntimeError(dict_args, get_error_message(dict_args))


@elewise_decorator.triple
@source_info_decorator()
@dtype_check_decorator
def vmla(tensor_0, tensor_1, tensor_2):
    """
    calculate x*tensor_1 + tensor_2,  only support float16, float32
    Parameters
    ----------
    x : wrapped_tensor or tvm.te.tensor
    tensor_1 : wrapped_tensor or tvm.te.tensor
    tensor_2 : wrapped_tensor or tvm.te.tensor
    Returns
    -------
    wrapped_tensor : X*tensor_1 + tensor_2
    """
    _check_tensor(tensor_1, position="second")
    _check_tensor(tensor_2, position="third")
    # 5hd scene can not enable paddinng for ternary instructions due to complex memory reuse
    if dynamic_static_unify_fractal_format():
        mul_res = vmul(tensor_0, tensor_1)
        cast_res = cast_to(mul_res, tensor_2.dtype)
        res = vadd(cast_res, tensor_2)
        return res

    return __multiple_elewise_op(tensor_0, tensor_1, tensor_2, "elewise_multiple_mla")


@elewise_decorator.triple
@source_info_decorator()
@dtype_check_decorator
def vmadd(tensor_0, tensor_1, tensor_2):
    """
    calculate tensor_0*tensor_2 + tensor_1,  only support  float16, float32
    Parameters
    ----------
    tensor_0 : wrapped_tensor or tvm.te.tensor
    tensor_1 : wrapped_tensor or tvm.te.tensor
    tensor_2 : wrapped_tensor or tvm.te.tensor
    Returns
    -------
    wrapped_tensor : tensor_0*tensor_2 + tensor_1
    """
    _check_tensor(tensor_1, position="second")
    _check_tensor(tensor_2, position="third")
    # 5hd scene can not enable paddinng for ternary instructions due to complex memory reuse
    if dynamic_static_unify_fractal_format():
        mul_res = vmul(tensor_0, tensor_2)
        res = vadd(mul_res, tensor_1)
        return res

    return __multiple_elewise_op(tensor_0, tensor_1, tensor_2, "elewise_multiple_madd")


def __multiple_elewise_op(tensor_0, tensor_1, tensor_2, op_name):
    """
    factory method of binary multiple operations
    """
    intr = f"v{op_name.split('_')[-1]}"
    is_support_dtype = intrinsic_check_support(f"Intrinsic_{intr}", tensor_0.dtype)

    _check_multiple_elewise_op_shape(tensor_0, tensor_1, tensor_2)
    if tensor_0.dtype != tensor_1.dtype or tensor_0.dtype != tensor_2.dtype \
            or tensor_2.dtype != tensor_1.dtype:
        if op_name != "elewise_multiple_mla" \
                or tensor_0.dtype != tensor_1.dtype \
                or tensor_0.dtype != "float16" \
                or tensor_2.dtype != "float32":
            dict_args = {"errCode": "E90002",
                         "detailed_cause": f"vmla not support: {tensor_0.dtype}|{tensor_1.dtype}|{tensor_2.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
    elif not is_support_dtype:
        dict_args = {"errCode": "E90002",
                     "detailed_cause": f"{intr} not support input dtype is: {tensor_0.dtype}!"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    shape = tensor_0.shape
    if op_name == "elewise_multiple_mla":
        ztype = tensor_2.dtype
        lambda_func = lambda *indice: tvm.expr.Cast(ztype, tensor_0(*indice) * tensor_1(*indice)) + tensor_2(*indice)
    elif op_name == "elewise_multiple_madd":
        lambda_func = lambda *indice: tensor_0(*indice) * tensor_2(*indice) + tensor_1(*indice)
    elif op_name == "elewise_multiple_maddrelu":
        lambda_func = lambda *indice: tvm.relu(tensor_0(*indice) * tensor_2(*indice) + tensor_1(*indice))
    else:
        dict_args = {"errCode": "E90003",
                     "detailed_cause": f"{op_name} is not support yet!"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    name = f"{op_name.split('_')[-1]}_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1

    # list cases of same input, "1"s in same_list means same input
    if tensor_0 == tensor_1 and tensor_0 == tensor_2:
        same_list = [1, 1, 1]
    elif tensor_0 == tensor_1:
        same_list = [1, 1, 0]
    elif tensor_0 == tensor_2:
        same_list = [1, 0, 1]
    elif tensor_1 == tensor_2:
        same_list = [0, 1, 1]
    else:
        same_list = [0, 0, 0]

    str_same_list = ",".join([str(i) for i in same_list])
    with tvm.tag_scope(f"{op_name}|{str_same_list}"):
        tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

    return tmp


def _check_multiple_elewise_op_shape(tensor_0, tensor_1, tensor_2):
    """
    check multiple elewise op's shape
    :param tensor_0:
    :param tensor_1:
    :param tensor_2:
    :return:
    """
    if len(tensor_0.shape) != len(tensor_1.shape) or len(tensor_0.shape) != len(tensor_2.shape) \
            or len(tensor_2.shape) != len(tensor_1.shape):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"input shape len must be equal, while now is: "
                                       f"{len(tensor_0.shape)}, {len(tensor_1.shape)} and {len(tensor_2.shape)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    for _a, _b, _c in zip(tensor_0.shape, tensor_1.shape, tensor_2.shape):
        if not (equal(_a, _b) and equal(_b, _c)):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"shape at each dim must be equal, while now is: "
                                           f"{tensor_0.shape}, {tensor_1.shape} and {tensor_2.shape}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    if not operation_context.in_dynamic():
        for _, shape_i in enumerate(tensor_0.shape):
            if shape_i.value <= 0 or not isinstance(shape_i.value, int):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"The input shape value must be a positive integer, now is {shape_i}"}
                raise RuntimeError(dict_args, get_error_message(dict_args))


def _vsel_bit_shape_check(condition, input_tensor):
    """
    check vsel_bit's shape
    :param condition:
    :param input_tensor:
    :return:
    """
    if len(condition.shape) != len(input_tensor.shape):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"condition and input_tensor len must be equal, while now is "
                                       f"{len(condition.shape)} and {len(input_tensor.shape)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    for i, condition_shape in enumerate(condition.shape):
        if i == len(condition.shape) - 1:
            if (input_tensor.shape[i].value % 8 != 0) or (input_tensor.shape[i].value // 8 != condition_shape.value):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"the sel tensor's last dim {input_tensor.shape[i].value} must be "
                                               f"multiple of 8, and div the last dim of condition shape "
                                               f"{input_tensor.shape[i].value} divided by {condition_shape.value} is 8"}
                raise RuntimeError(dict_args, get_error_message(dict_args))
        else:
            if condition_shape.value != input_tensor.shape[i].value:
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"The lhs shape {condition_shape.value} must be equal to the rhs "
                                               f"shape {input_tensor.shape[i].value}"}
                raise RuntimeError(dict_args, get_error_message(dict_args))

    for i, input_shape in enumerate(input_tensor.shape):
        if input_shape.value <= 0 or not isinstance(input_shape.value, int):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"The input shape value {input_shape.value} must be a positive integer"}
            raise RuntimeError(dict_args, get_error_message(dict_args))


# 'pylint: disable=too-many-branches, too-many-statements
@elewise_decorator.vsel
@source_info_decorator()
@dtype_check_decorator
def vsel(condition, lhs, rhs):
    """
    if condition = ture, the result is lhs,
        select

    Parameters
    ----------
    condition : wrapped_tensor or tvm.te.tensor, the dtype is bool or uint8(dynamic is uint1)

    lhs : wrapped_tensor or tvm.te.tensor or scalar

    rhs : wrapped_tensor or tvm.te.tensor or scalar

    Returns
    -------
    wrapped_tensor :
    """
    def __vsel_input_check(condition):
        _check_tensor(condition, position="first")
    __vsel_input_check(condition)

    src_dtype = "float16"

    def _get_vsel_input_type(inputs):
        type_strs = []
        for one in inputs:
            if isinstance(one, tvm.Tensor):
                type_strs.append("TENSOR")
            else:
                type_strs.append("SCALAR")

        return "_".join(type_strs)

    input_type_str = _get_vsel_input_type([lhs, rhs])

    # two const input check
    def _check_two_scalar_inputs(lhs, rhs):
        # if lhs,rhs are all scalar, only support float16
        if judge_var(lhs) == "tvm_const" and lhs.dtype != "float16":
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"lhs dtype only support float16 when lhs and rhs are all scalar, "
                                           f"while now is {lhs.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if judge_var(rhs) == "tvm_const" and rhs.dtype != "float16":
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"rhs dtype only support float16 when lhs and rhs are all scalar, "
                                           f"while now is {rhs.dtype}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    # dynamic realize
    def _dynamic_check_shape(condition_shape):
        if isinstance(condition_shape[-1], int) and condition_shape[-1] % 8 != 0:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"vsel last dim must be multiply of 8, while now is {condition_shape[-1]}"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
    
    def _check_int64_lhs_rhs(lhs, rhs):
        if input_type_str == "TENSOR_TENSOR" or input_type_str == "TENSOR_SCALAR":
            return lhs.dtype == "int64"
        if input_type_str == "SCALAR_TENSOR":
            return rhs.dtype == "int64"
        return False

    if in_dynamic_and_static_unify():
        shape = condition.shape
        _dynamic_check_shape(shape)

        def _check_dynamic_condition_dtype(condition_dtype):
            if condition_dtype not in ("uint1", "bool"):
                dict_args = {"errCode": "E90003",
                             "detailed_cause": f"dync shape vsel only support uint1/bool, not now {condition_dtype}."}
                raise RuntimeError(dict_args, get_error_message(dict_args))

        condition_dtype = condition.dtype
        _check_dynamic_condition_dtype(condition_dtype)

        if condition.dtype == "bool" and not _check_int64_lhs_rhs(lhs, rhs):
            # when lhs or rhs is int64, do not change condition dtype
            cast_lambda_func = lambda *index: condition(*index).astype("float16")
            vector_cast_op = "elewise_single_cast"
            cast_name = f"vsel_bool_cast_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            with tvm.tag_scope(vector_cast_op):
                cast_float16 = tvm.compute(shape, cast_lambda_func, name=cast_name,
                                           attrs={"_type": ComputeType.CAST})

            cmp_lambda_func = lambda *indice: (cast_float16(*indice) > tvm.const(0, "float16")).astype("uint1")
            cmp_op = "elewise_binary_vcmpv_gt"
            cmp_gt_name = f"vsel_bool_vcmpv_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            with tvm.tag_scope(cmp_op):
                condition = tvm.compute(shape, cmp_lambda_func, name=cmp_gt_name,
                                        attrs={"_type": ComputeType.ELEWISE})

        # mode bit dtype = "uint1"
        op_name = "elewise_multiple_sel"

        def _get_vsel_dynamic_lambda_func(input_type_str, condition, lhs, rhs):
            if input_type_str == "TENSOR_TENSOR":
                _check_multiple_elewise_op_shape(condition, lhs, rhs)
                dynamic_lambda_func = lambda *indice: tvm.select(condition(*indice), lhs(*indice), rhs(*indice))
            elif input_type_str == "SCALAR_TENSOR":
                _check_elewise_binary_shape(condition, rhs)
                lhs = get_tvm_scalar(lhs, rhs.dtype)
                dynamic_lambda_func = lambda *indice: tvm.select(condition(*indice), lhs, rhs(*indice))
            elif input_type_str == "TENSOR_SCALAR":
                _check_elewise_binary_shape(condition, lhs)
                rhs = get_tvm_scalar(rhs, lhs.dtype)
                dynamic_lambda_func = lambda *indice: tvm.select(condition(*indice), lhs(*indice), rhs)
            else:
                # if lhs,rhs are all scalar, only support float16
                _check_two_scalar_inputs(lhs, rhs)
                lhs = get_tvm_scalar(lhs, "float16")
                rhs = get_tvm_scalar(rhs, "float16")
                dynamic_lambda_func = lambda *indice: tvm.select(condition(*indice), lhs, rhs)

            return dynamic_lambda_func

        dynamic_lambda_func = _get_vsel_dynamic_lambda_func(input_type_str, condition, lhs, rhs)
        name = f"sel_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1
        with tvm.tag_scope(op_name):
            tmp = tvm.compute(shape, dynamic_lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})
        return tmp

    op_name = "emit_insn_elewise_multiple_sel"
    if condition.dtype == "bool":
        mode = 'bool'
        shape = condition.shape
        if input_type_str == "TENSOR_TENSOR":
            src_dtype = lhs.dtype
            lhs = auto_cast_tensor(lhs, 'vsel')
            rhs = auto_cast_tensor(rhs, 'vsel')
            _check_multiple_elewise_op_shape(condition, lhs, rhs)
            lambda_func = lambda *indice: tvm.select(condition(*indice), lhs(*indice), rhs(*indice))
        elif input_type_str == "SCALAR_TENSOR":
            _check_elewise_binary_shape(condition, rhs)
            src_dtype = rhs.dtype
            rhs = auto_cast_tensor(rhs, 'vsel')
            lhs = get_tvm_scalar(lhs, rhs.dtype)
            lambda_func = lambda *indice: tvm.select(condition(*indice), lhs, rhs(*indice))
        elif input_type_str == "TENSOR_SCALAR":
            _check_elewise_binary_shape(condition, lhs)
            src_dtype = lhs.dtype
            lhs = auto_cast_tensor(lhs, 'vsel')
            rhs = get_tvm_scalar(rhs, lhs.dtype)
            lambda_func = lambda *indice: tvm.select(condition(*indice), lhs(*indice), rhs)
        else:
            # if lhs,rhs are all scalar, only support float16
            _check_two_scalar_inputs(lhs, rhs)

            lhs = get_tvm_scalar(lhs, "float16")
            rhs = get_tvm_scalar(rhs, "float16")
            _check_is_equal(lhs, rhs)

            lambda_func = lambda *indice: tvm.select(condition(*indice), lhs, rhs)

        name = f"sel_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1
        op_name = f"{op_name}|{mode}"
        with tvm.tag_scope(op_name):
            tmp = tvm.compute(shape, lambda_func, name=name)
    elif condition.dtype == "uint8":
        mode = 'bit'
        shape_condition = shape_to_list(condition.shape)
        shape = shape_condition
        shape[-1] = shape[-1] * 8

        supported_type = ["float16"]

        def get_indice(indice):
            """
            get indice
            """
            res_index = []
            for i, value in enumerate(indice):
                if i == len(indice) - 1:
                    res_index.append(value // 8)
                else:
                    res_index.append(value)
            return res_index

        if input_type_str == "TENSOR_TENSOR":
            _check_elewise_binary_shape(lhs, rhs)
            _vsel_bit_shape_check(condition, lhs)
            src_dtype = lhs.dtype
            lhs = auto_cast_tensor(lhs, 'vsel', supported_type)
            rhs = auto_cast_tensor(rhs, 'vsel', supported_type)

            def _compute(*indice):
                res_index = get_indice(indice)
                return tvm.select(condition(*res_index).astype('bool'), lhs(*indice), rhs(*indice))
        elif input_type_str == "SCALAR_TENSOR":
            _vsel_bit_shape_check(condition, rhs)
            src_dtype = rhs.dtype
            rhs = auto_cast_tensor(rhs, 'vsel', supported_type)
            lhs = get_tvm_scalar(lhs, rhs.dtype)

            def _compute(*indice):
                res_index = get_indice(indice)
                return tvm.select(condition(*res_index).astype('bool'), lhs, rhs(*indice))
        elif input_type_str == "TENSOR_SCALAR":
            _vsel_bit_shape_check(condition, lhs)
            src_dtype = lhs.dtype
            lhs = auto_cast_tensor(lhs, 'vsel', supported_type)
            rhs = get_tvm_scalar(rhs, lhs.dtype)

            def _compute(*indice):
                res_index = get_indice(indice)
                return tvm.select(condition(*res_index).astype('bool'), lhs(*indice), rhs)
        else:
            # if lhs,rhs are all scalar, only support float16
            _check_two_scalar_inputs(lhs, rhs)

            lhs = get_tvm_scalar(lhs, "float16")
            rhs = get_tvm_scalar(rhs, "float16")
            _check_is_equal(lhs, rhs)

            def _compute(*indice):
                res_index = get_indice(indice)
                return tvm.select(condition(*res_index).astype('bool'), lhs, rhs)

        name = f"sel_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1

        op_name = f"{op_name}|{mode}"
        with tvm.tag_scope(op_name):
            tmp = tvm.compute(shape, _compute, name=name)
    else:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"condition only support bool and uint8, while now is {condition.dtype}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    res_dtype = tmp.dtype
    if src_dtype != res_dtype:
        tmp = _cast(tmp, src_dtype, is_auto_cast=False)
    return tmp


def _vcmpsel_data_shape_check(*args):
    """
    check vcmpsel's data shape
    :param args:
    :return:
    """
    arg_temp = args[0]

    for sh_value in arg_temp.shape:
        if not operation_context.in_dynamic():
            if sh_value.value <= 0 or not isinstance(sh_value.value, int):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"Input shape value must be positive integer, now is {sh_value.value}."}
                raise RuntimeError(dict_args, get_error_message(dict_args))

    for arg in args:
        if len(arg.shape) != len(arg_temp.shape):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"The input shape len must be equal, while arg len is {len(arg.shape)}, and "
                                           f"arg_temp len is {len(arg_temp.shape)}."}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    for i, arg_temp_shape in enumerate(arg_temp.shape):
        for arg in args:
            if not equal(arg_temp_shape, arg.shape[i]):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": f"The lhs shape must be equal to the rhs shape, while now lhs shape is "
                                               f"{arg_temp.shape} and rhs shape is {arg.shape}."}
                raise RuntimeError(dict_args, get_error_message(dict_args))


def _vcmpsel_data_dtype_check(*args):
    """
    check vcmpsel's data type
    :param args:
    :return:
    """
    arg_temp = args[0]

    for arg in args:
        if arg.dtype != arg_temp.dtype:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": f"The input dtype must be the same while arg.dtype is {arg.dtype} and "
                                           f"arg_temp.dtype is {arg_temp.dtype}."}
            raise RuntimeError(dict_args, get_error_message(dict_args))


@elewise_decorator.vcmpsel
@source_info_decorator()
@dtype_check_decorator
def vcmpsel(lhs, rhs=None, operation='lt', slhs=None, srhs=None):
    """
    calculate elewise compare

    Parameters
    ----------
    lhs : wrapped_tensor or tvm.te.tensor
        compare left hand tensor
    rhs : wrapped_tensor or tvm.te.tensor or scalar
        compare right hand tensor or scalar
    operation : operator type, eq, ne, lt, gt, ge, le
    slhs : wrapped_tensor or tvm.te.tensor or scalar
        select left hand tensor or scalar
    srhs : wrapped_tensor or tvm.te.tensor or scalar
        select right hand tensor or scalar

    Returns
    -------
    wrapped_tensor
    """
    def _check_operation(operation):
        if operation not in ['eq', 'ne', 'lt', 'gt', 'ge', 'le']:
            dict_args = {"errCode": "E90002",
                         "detailed_cause": f"vcmpsel only support eq, ne, lt, gt, ge and le, while now is {operation}."}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    _check_operation(operation)

    if rhs is None:
        rhs = 2.0

    if slhs is None:
        slhs = lhs

    if srhs is None:
        if isinstance(rhs, tvm.Tensor):
            srhs = rhs
        else:
            srhs = 0.0

    shape = lhs.shape

    def _get_cmpvs_lambda_func(operation, lhs, rhs):
        if in_dynamic_and_static_unify():
            dynamic_cmpvs_lambda_func_dict = {
                'lt': lambda *indice: (lhs(*indice) < rhs).astype("uint1"),
                'gt': lambda *indice: (lhs(*indice) > rhs).astype("uint1"),
                'le': lambda *indice: (lhs(*indice) <= rhs).astype("uint1"),
                'ge': lambda *indice: (lhs(*indice) >= rhs).astype("uint1"),
                'eq': lambda *indice: (tvm.expr.EQ(lhs(*indice), rhs)).astype("uint1"),
                'ne': lambda *indice: (tvm.expr.NE(lhs(*indice), rhs)).astype("uint1")
            }
            lambda_func = dynamic_cmpvs_lambda_func_dict.get(operation)
        else:
            cmpvs_lambda_func_dict = {
                'lt': lambda *indice: lhs(*indice) < rhs,
                'gt': lambda *indice: lhs(*indice) > rhs,
                'le': lambda *indice: lhs(*indice) <= rhs,
                'ge': lambda *indice: lhs(*indice) >= rhs,
                'eq': lambda *indice: tvm.expr.EQ(lhs(*indice), rhs),
                'ne': lambda *indice: tvm.expr.NE(lhs(*indice), rhs)
            }
            lambda_func = cmpvs_lambda_func_dict.get(operation)
        return lambda_func

    def _get_cmpv_lambda_func(operation, lhs, rhs):
        if in_dynamic_and_static_unify():
            dynamic_cmpv_lambda_func_dict = {
                'lt': lambda *indice: (lhs(*indice) < rhs(*indice)).astype("uint1"),
                'gt': lambda *indice: (lhs(*indice) > rhs(*indice)).astype("uint1"),
                'le': lambda *indice: (lhs(*indice) <= rhs(*indice)).astype("uint1"),
                'ge': lambda *indice: (lhs(*indice) >= rhs(*indice)).astype("uint1"),
                'eq': lambda *indice: (tvm.expr.EQ(lhs(*indice), rhs(*indice))).astype("uint1"),
                'ne': lambda *indice: (tvm.expr.NE(lhs(*indice), rhs(*indice))).astype("uint1")
            }
            lambda_func = dynamic_cmpv_lambda_func_dict.get(operation)
        else:
            cmpv_lambda_func_dict = {
                'lt': lambda *indice: lhs(*indice) < rhs(*indice),
                'gt': lambda *indice: lhs(*indice) > rhs(*indice),
                'le': lambda *indice: lhs(*indice) <= rhs(*indice),
                'ge': lambda *indice: lhs(*indice) >= rhs(*indice),
                'eq': lambda *indice: tvm.expr.EQ(lhs(*indice), rhs(*indice)),
                'ne': lambda *indice: tvm.expr.NE(lhs(*indice), rhs(*indice))
            }
            lambda_func = cmpv_lambda_func_dict.get(operation)
        return lambda_func

    cmp_op = f"elewise_binary_vcmpv_{operation}"
    sel_op = "elewise_multiple_sel"
    cmpsel_op = f"elewise_binary_cmpsel_{operation}"

    def get_vcmpsel_input_type(rhs, slhs, srhs):
        type_strs = []
        if isinstance(rhs, tvm.Tensor):
            type_strs.append("TENSOR")
        else:
            type_strs.append("SCALAR")

        if isinstance(slhs, tvm.Tensor):
            type_strs.append("TENSOR")
        else:
            type_strs.append("SCALAR")

        if isinstance(srhs, tvm.Tensor):
            type_strs.append("TENSOR")
        else:
            type_strs.append("SCALAR")

        return "_".join(type_strs)

    input_type_str = get_vcmpsel_input_type(rhs, slhs, srhs)

    if input_type_str == "SCALAR_SCALAR_SCALAR":
        if not in_dynamic_and_static_unify():
            lhs = auto_cast_tensor(lhs, "vsel")
        rhs = get_tvm_scalar(rhs, lhs.dtype)
        slhs = get_tvm_scalar(slhs, lhs.dtype)
        srhs = get_tvm_scalar(srhs, lhs.dtype)

        def _vcmpsel_tsss_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpvs_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs, srhs)
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_tsss_lambda_func(operation, lhs, rhs, slhs, srhs):
                cmpsel_tsss_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs, slhs, srhs),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs, slhs, srhs),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs, slhs, srhs),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs, slhs, srhs),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs, slhs, srhs),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs, slhs, srhs)
                }
                return cmpsel_tsss_lambda_func_dict[operation]

            lambda_func = _get_cmpsel_tsss_lambda_func(operation, lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_tsss_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    if input_type_str == "TENSOR_SCALAR_SCALAR":
        _vcmpsel_data_shape_check(lhs, rhs)
        _vcmpsel_data_dtype_check(lhs, rhs)
        if not in_dynamic_and_static_unify():
            lhs = auto_cast_tensor(lhs, "vsel")
            rhs = auto_cast_tensor(rhs, "vsel")
        slhs = get_tvm_scalar(slhs, lhs.dtype)
        srhs = get_tvm_scalar(srhs, lhs.dtype)

        def _vcmpsel_ttss_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpv_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs, srhs)
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_ttss_lambda_func(operation, lhs, rhs, slhs, srhs):
                cmpsel_ttss_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs(*indice), slhs, srhs),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs(*indice), slhs, srhs),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs(*indice), slhs, srhs),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs(*indice), slhs, srhs),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs(*indice), slhs, srhs),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs(*indice), slhs, srhs)
                }
                return cmpsel_ttss_lambda_func_dict[operation]

            lambda_func = _get_cmpsel_ttss_lambda_func(operation, lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_ttss_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    if input_type_str == "SCALAR_TENSOR_SCALAR":
        _vcmpsel_data_shape_check(lhs, slhs)
        _vcmpsel_data_dtype_check(lhs, slhs)
        if not in_dynamic_and_static_unify():
            lhs = auto_cast_tensor(lhs, "vsel")
            slhs = auto_cast_tensor(slhs, "vsel")
        rhs = get_tvm_scalar(rhs, lhs.dtype)
        srhs = get_tvm_scalar(srhs, lhs.dtype)

        def _vcmpsel_tsts_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpvs_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs(*indice), srhs)
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_tsts_lambda_func(operation, lhs, rhs, slhs, srhs):
                cmpsel_tsts_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs, slhs(*indice), srhs),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs, slhs(*indice), srhs),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs, slhs(*indice), srhs),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs, slhs(*indice), srhs),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs, slhs(*indice), srhs),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs, slhs(*indice), srhs)
                }
                return cmpsel_tsts_lambda_func_dict.get(operation)

            lambda_func = _get_cmpsel_tsts_lambda_func(operation, lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_tsts_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    if input_type_str == "SCALAR_SCALAR_TENSOR":
        _vcmpsel_data_shape_check(lhs, srhs)
        _vcmpsel_data_dtype_check(lhs, srhs)
        if not in_dynamic_and_static_unify():
            srhs = auto_cast_tensor(srhs, "vsel")
            lhs = auto_cast_tensor(lhs, "vsel")
        rhs = get_tvm_scalar(rhs, lhs.dtype)
        slhs = get_tvm_scalar(slhs, lhs.dtype)

        def _vcmpsel_tsst_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpvs_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs, srhs(*indice))
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_tsst_lambda_func(operation, lhs, rhs, slhs, srhs):
                get_cmpsel_tsst_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs, slhs, srhs(*indice)),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs, slhs, srhs(*indice)),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs, slhs, srhs(*indice)),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs, slhs, srhs(*indice)),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs, slhs, srhs(*indice)),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs, slhs, srhs(*indice))
                }
                return get_cmpsel_tsst_lambda_func_dict[operation]

            lambda_func = _get_cmpsel_tsst_lambda_func(operation, lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_tsst_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    if input_type_str == "TENSOR_TENSOR_SCALAR":
        _vcmpsel_data_shape_check(lhs, rhs, slhs)
        _vcmpsel_data_dtype_check(lhs, rhs, slhs)
        if not in_dynamic_and_static_unify():
            lhs = auto_cast_tensor(lhs, "vsel")
            rhs = auto_cast_tensor(rhs, "vsel")
            slhs = auto_cast_tensor(slhs, "vsel")
        srhs = get_tvm_scalar(srhs, lhs.dtype)

        def _vcmpsel_ttts_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpv_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs(*indice), srhs)
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_ttts_lambda_func(operation, lhs, rhs, slhs, srhs):
                get_cmpsel_ttts_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs(*indice), slhs(*indice), srhs),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs(*indice), slhs(*indice), srhs),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs(*indice), slhs(*indice), srhs),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs(*indice), slhs(*indice), srhs),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs(*indice), slhs(*indice), srhs),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs(*indice), slhs(*indice), srhs)
                }
                return get_cmpsel_ttts_lambda_func_dict[operation]

            lambda_func = _get_cmpsel_ttts_lambda_func(operation, lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_ttts_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    if input_type_str == "TENSOR_SCALAR_TENSOR":
        _vcmpsel_data_shape_check(lhs, rhs, srhs)
        _vcmpsel_data_dtype_check(lhs, rhs, srhs)
        if not in_dynamic_and_static_unify():
            lhs = auto_cast_tensor(lhs, "vsel")
            rhs = auto_cast_tensor(rhs, "vsel")
            srhs = auto_cast_tensor(srhs, "vsel")
        slhs = get_tvm_scalar(slhs, lhs.dtype)

        def _vcmpsel_ttst_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpv_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs, srhs(*indice))
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_ttst_lambda_func(operation, lhs, rhs, slhs, srhs):
                get_cmpsel_ttst_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs(*indice), slhs, srhs(*indice)),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs(*indice), slhs, srhs(*indice)),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs(*indice), slhs, srhs(*indice)),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs(*indice), slhs, srhs(*indice)),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs(*indice), slhs, srhs(*indice)),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs(*indice), slhs, srhs(*indice))
                }
                return get_cmpsel_ttst_lambda_func_dict[operation]

            lambda_func = _get_cmpsel_ttst_lambda_func(operation, lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_ttst_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    if input_type_str == "SCALAR_TENSOR_TENSOR":
        _vcmpsel_data_shape_check(lhs, slhs, srhs)
        _vcmpsel_data_dtype_check(lhs, slhs, srhs)
        if not in_dynamic_and_static_unify():
            lhs = auto_cast_tensor(lhs, "vsel")
            slhs = auto_cast_tensor(slhs, "vsel")
            srhs = auto_cast_tensor(srhs, "vsel")
        rhs = get_tvm_scalar(rhs, lhs.dtype)

        def _vcmpsel_tstt_compute(cmp_op, sel_op, cmpsel_op, shape,
                                  lhs, rhs, operation, slhs, srhs):
            if lhs.dtype == "float16":
                lambda_func_cmp = _get_cmpvs_lambda_func(operation, lhs, rhs)
                name = f"vcmpv_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(cmp_op):
                    condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

                lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs(*indice), srhs(*indice))
                name = f"vsel_{NAME_INDEX[0]}"
                NAME_INDEX[0] += 1
                with tvm.tag_scope(sel_op):
                    tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

                return tmp

            def _get_cmpsel_tstt_lambda_func(operation, lhs, rhs, slhs, srhs):
                get_cmpsel_tstt_lambda_func_dict = {
                    'lt': lambda *indice: tvm.select(lhs(*indice) < rhs, slhs(*indice), srhs(*indice)),
                    'gt': lambda *indice: tvm.select(lhs(*indice) > rhs, slhs(*indice), srhs(*indice)),
                    'le': lambda *indice: tvm.select(lhs(*indice) <= rhs, slhs(*indice), srhs(*indice)),
                    'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs, slhs(*indice), srhs(*indice)),
                    'eq': lambda *indice: tvm.select(lhs(*indice) == rhs, slhs(*indice), srhs(*indice)),
                    'ne': lambda *indice: tvm.select(lhs(*indice) != rhs, slhs(*indice), srhs(*indice))
                }
                return get_cmpsel_tstt_lambda_func_dict[operation]

            lambda_func = _get_cmpsel_tstt_lambda_func(operation,  lhs, rhs, slhs, srhs)
            name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            cmpsel_op = f"{cmpsel_op}|{operation}"
            with tvm.tag_scope(cmpsel_op):
                tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        return _vcmpsel_tstt_compute(cmp_op, sel_op, cmpsel_op, shape,
                                     lhs, rhs, operation, slhs, srhs)

    _vcmpsel_data_shape_check(lhs, rhs, slhs, srhs)
    _vcmpsel_data_dtype_check(lhs, rhs, slhs, srhs)

    if not in_dynamic_and_static_unify():
        lhs = auto_cast_tensor(lhs, "vsel")
        rhs = auto_cast_tensor(rhs, "vsel")
        slhs = auto_cast_tensor(slhs, "vsel")
        srhs = auto_cast_tensor(srhs, "vsel")

    def _vcmpsel_tttt_compute(cmp_op, sel_op, cmpsel_op, shape,
                              lhs, rhs, operation, slhs, srhs):
        if lhs.dtype == "float16":
            lambda_func_cmp = _get_cmpv_lambda_func(operation, lhs, rhs)
            name = f"vcmpv_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            with tvm.tag_scope(cmp_op):
                condition = tvm.compute(shape, lambda_func_cmp, name=name, attrs={"_type": ComputeType.ELEWISE})

            lambda_func_sel = lambda *indice: tvm.select(condition(*indice), slhs(*indice), srhs(*indice))
            name = f"vsel_{NAME_INDEX[0]}"
            NAME_INDEX[0] += 1
            with tvm.tag_scope(sel_op):
                tmp = tvm.compute(shape, lambda_func_sel, name=name, attrs={"_type": ComputeType.ELEWISE})

            return tmp

        def _get_cmpsel_tttt_lambda_func(operation, lhs, rhs, slhs, srhs):
            get_cmpsel_tttt_lambda_func_dict = {
                'lt': lambda *indice: tvm.select(lhs(*indice) < rhs(*indice), slhs(*indice), srhs(*indice)),
                'gt': lambda *indice: tvm.select(lhs(*indice) > rhs(*indice), slhs(*indice), srhs(*indice)),
                'le': lambda *indice: tvm.select(lhs(*indice) <= rhs(*indice), slhs(*indice), srhs(*indice)),
                'ge': lambda *indice: tvm.select(lhs(*indice) >= rhs(*indice), slhs(*indice), srhs(*indice)),
                'eq': lambda *indice: tvm.select(lhs(*indice) == rhs(*indice), slhs(*indice), srhs(*indice)),
                'ne': lambda *indice: tvm.select(lhs(*indice) != rhs(*indice), slhs(*indice), srhs(*indice))
            }
            return get_cmpsel_tttt_lambda_func_dict[operation]

        lambda_func = _get_cmpsel_tttt_lambda_func(operation, lhs, rhs, slhs, srhs)
        name = f"{cmpsel_op.split('_')[-2]}_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1
        cmpsel_op = f"{cmpsel_op}|{operation}"
        with tvm.tag_scope(cmpsel_op):
            tmp = tvm.compute(shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})

        return tmp

    return _vcmpsel_tttt_compute(cmp_op, sel_op, cmpsel_op, shape,
                                 lhs, rhs, operation, slhs, srhs)


@elewise_decorator.single
@source_info_decorator()
def vsigmoid(raw_tensor):
    """
    Mapping any input value to a value between 0 and 1.

    Parameters:
    ----------
    tensor: raw_tensor
    Returns
    -------
    tensor
    """
    if not isinstance(raw_tensor, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The input tensor type must be tvm.te.tensor, "
                                       f"while type is {type(raw_tensor)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    return __single_elewise_op(raw_tensor, raw_tensor.dtype, "elewise_single_sigmoid")


@elewise_decorator.single
@source_info_decorator()
def vtanh(raw_tensor):
    """
    Mapping any input value to a value between -1 and 1,
    compressing the input values towards the center of the range.

    Parameters:
    ----------
    tensor: raw_tensor

    Returns:
    -------
    tensor
    """
    if not isinstance(raw_tensor, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The input tensor type must be tvm.te.tensor, "
                                       f"while type is {type(raw_tensor)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    return __single_elewise_op(raw_tensor, raw_tensor.dtype, "elewise_single_tanh")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def powi(lhs, rhs):
    """
    Raises the first input tensor to the power of second input tensor.

    Parameters:
    ----------
    lhs: base number

    rhs:  exponent number

    Returns
    -------
    exponent tensor
    """
    if not isinstance(lhs, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The base number component type must be tvm.te.tensor, "
                                       f"while current object type is {type(lhs)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if rhs.dtype != lhs.dtype:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The DataType of the imaginary component Tensor and"
                                       f" the real component Tensor must be the same,"
                                       f"while current DataType is {rhs.dtype} and {lhs.dtype}."}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    return __binary_elewise_op(lhs, rhs, "elewise_binary_powi")


@elewise_decorator.binary
@source_info_decorator()
@dtype_check_decorator
def complex(re, im):
    """
    Convert real and imaginary inputs to complex

    Parameters:
    ----------
    re: Real part of complex output

    im: Imaginary part of complex output

    Returns
    -------
    complex : complex(re, im)
    """
    # The imaginary tensor is REQUIRED
    # The object must be tvm.te.Tensor
    # The datatype of imaginary tensor only support float16/float32
    # The real tensor and the imaginary tensor must have the same dtype
    if not isinstance(im, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The imaginary component type must be tvm.te.tensor, "
                                       f"while current object type is {type(im)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if re.dtype != im.dtype:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The DataType of the imaginary component Tensor and"
                                       f" the real component Tensor must be the same,"
                                       f"while current DataType is {re.dtype} and {im.dtype}."}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    return __binary_elewise_op(re, im, "elewise_binary_complex")


@elewise_decorator.single
@source_info_decorator()
@dtype_check_decorator
def real(raw_tensor):
    """
    Extracting real numbers from complex numbers. If input is already real, it is returned unchanged.
    Parameters
    ----------
    tensor: raw_tensor
    Returns
    -------
    tensor: real(tensor)
    """
    valid_float_dtype_list = ["float16", "float32"]
    valid_complex_dtype_list = ["complex32", "complex64"]
    vaild_dtype_list = valid_float_dtype_list + valid_complex_dtype_list

    if not isinstance(raw_tensor, tvm.Tensor):
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The input tensor type must be tvm.te.tensor, "
                                       f"while type is {type(raw_tensor)}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if raw_tensor.dtype not in vaild_dtype_list:
        dict_args = {"errCode": "E90001",
                     "detailed_cause": f"The DataType of the input Tensor must be in {vaild_dtype_list},"
                                       f"while current DataType is {raw_tensor.dtype}"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    if raw_tensor.dtype in valid_float_dtype_list:
        return __single_elewise_op(raw_tensor, raw_tensor.dtype, 'elewise_single_VS_add', args=[0])

    if raw_tensor.dtype in valid_complex_dtype_list:
        return __single_elewise_op(raw_tensor, raw_tensor.dtype, "elewise_single_real")

    return None
