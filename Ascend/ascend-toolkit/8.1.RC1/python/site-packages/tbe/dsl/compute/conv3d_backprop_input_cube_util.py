#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
cube util.
"""
from tbe import tvm

from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils import const
from tbe.common.utils.errormgr import error_manager_cube as cube_err
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.compute import cube_util


class Conv3DDxMadPattern():
    """
    Conv3ddx mad helper class.
    """
    def __init__(self, mad_config):
        self.tag = mad_config.get("tag", "")
        self.stride_d = mad_config.get("stride_d")
        self.dilation_d = mad_config.get("dilation_d")
        self.kernel_d = mad_config.get("kernel_d")
        self.dedx_d = mad_config.get("dedx_d")
        self.c_dtype = mad_config.get("c_dtype")
        self.pad_head = mad_config.get("pad_h")
        self.pad_tail = mad_config.get(("pad_t"))
        self.binary_flag = mad_config.get(const.BINARY_KEY, False)
        self.sd_kd_mode = mad_config.get(const.SD_KD_MODE_KEY)

    def generate_c(self, tensor_a, tensor_b):
        """
        calculate the mad result tensor

        Parameters
        ----------
        tensor_a: Tensor of A matrix.
        tensor_b: Tensor of B matrix.

        Returns
        -------
        tensor_c: Mad result.
        """
        if self.binary_flag:
            return self._generate_c_binary(tensor_a, tensor_b)
        else:
            return self._generate_c_static(tensor_a, tensor_b)

    def _generate_c_binary(self, tensor_a, tensor_b):
        a_group, a_batch, a_depth, a_m1, a_k1, a_m0, a_k0 = cube_util.shape_to_list(tensor_a.shape)
        _, b_kd, _, b_n1, b_n0, _ = cube_util.shape_to_list(tensor_b.shape)
        shape_c = (a_group, a_batch, self.dedx_d, b_n1, a_m1 * a_m0, b_n0)

        axis_k0 = tvm.reduce_axis([0, a_k0], name='axis_k0')
        axis_k1 = tvm.reduce_axis([0, a_k1], name='axis_k1')
        axis_kd = tvm.reduce_axis([0, b_kd], name='axis_kd')
        kernel_d_dilation = (self.kernel_d - 1) * self.dilation_d + 1

        def __mad_sd_gt_kd(indices, axis_k1, axis_k0, tensor_a, tensor_b):
            g_index, n_index, deep_index, cin1_g_index, m_index, cin0_index = indices
            tensor_c = tvm.select(
                tvm.all((deep_index + self.pad_head) % self.stride_d < kernel_d_dilation,
                        (deep_index + self.pad_head) // self.stride_d < a_depth),
                tensor_a(g_index, n_index, (deep_index + self.pad_head) // self.stride_d, m_index // a_m0, axis_k1,
                         m_index % a_m0, axis_k0).astype(self.c_dtype) *
                tensor_b(g_index, (deep_index + self.pad_head) % self.stride_d, axis_k1, cin1_g_index, cin0_index,
                         axis_k0).astype(self.c_dtype), tvm.const(0.0, self.c_dtype))

            return tensor_c

        def __mad_sd_le_kd(indices, tensor_a, tensor_b, reduce_axes):
            g_index, n_index, deep_index, cin1_g_index, m_index, cin0_index = indices
            axis_kd, axis_k1, axis_k0 = reduce_axes
            tensor_c = tvm.select(
                tvm.all((deep_index - axis_kd + self.pad_head) >= 0,
                        (deep_index - axis_kd + self.pad_head) % self.stride_d == 0,
                        (deep_index - axis_kd + self.pad_head) // self.stride_d < a_depth),
                tensor_a(g_index, n_index, (deep_index - axis_kd + self.pad_head) // self.stride_d, m_index // a_m0,
                         axis_k1, m_index % a_m0, axis_k0).astype(self.c_dtype) *
                tensor_b(g_index, axis_kd, axis_k1, cin1_g_index, cin0_index, axis_k0).astype(self.c_dtype),
                tvm.const(0.0, self.c_dtype))

            return tensor_c

        if self.sd_kd_mode == const.SD_EQ_KD_FLAG:
            tensor_c = tvm.compute(
                shape_c,
                lambda g_index, n_index, deep_index, cin1_g_index, m_index, cin0_index: tvm.sum(
                    (tensor_a(g_index, n_index,
                              (deep_index + self.pad_head) // self.stride_d, m_index // a_m0, axis_k1, m_index % a_m0,
                              axis_k0) * tensor_b(g_index, (deep_index + self.pad_head) % self.stride_d, axis_k1,
                                                  cin1_g_index, cin0_index, axis_k0)).astype(self.c_dtype),
                    axis=[axis_k1, axis_k0]),
                name="C",
                tag=self.tag + "mad")
        elif self.sd_kd_mode == const.SD_GT_KD_FLAG:
            tensor_c = tvm.compute(
                shape_c,
                lambda *indices: tvm.sum(__mad_sd_gt_kd(indices, axis_k1, axis_k0, tensor_a, tensor_b),
                                         axis=[axis_k1, axis_k0]),
                name="C",
                tag=self.tag + "mad")
        # SD_LE_KD_FLAG condition
        else:
            reduce_axes = (axis_kd, axis_k1, axis_k0)
            tensor_c = tvm.compute(
                shape_c,
                lambda *indices: tvm.sum(__mad_sd_le_kd(indices, tensor_a, tensor_b, reduce_axes),
                                         axis=[axis_kd, axis_k1, axis_k0]),
                name="C",
                tag=self.tag + "mad")

        return tensor_c

    def _generate_c_static(self, tensor_a, tensor_b):
        def __mad_condition(mad_condition_param_dict):
            indices = mad_condition_param_dict.get('indices')
            axis_kd = mad_condition_param_dict.get('axis_kd')
            axis_k1 = mad_condition_param_dict.get('axis_k1')
            axis_k0 = mad_condition_param_dict.get('axis_k0')
            tensor_a = mad_condition_param_dict.get('tensor_a')
            tensor_b = mad_condition_param_dict.get('tensor_b')
            g_index, n_index, deep_index, co1_index, m_index, co0_index = indices
            tensor_c = tvm.select(
                tvm.all((deep_index - axis_kd + self.pad_head) >= 0,
                        (deep_index - axis_kd + self.pad_head) % self.stride_d == 0,
                        (deep_index - axis_kd + self.pad_head) // self.stride_d < tensor_a.shape[2]),
                tensor_a(g_index, n_index, (deep_index - axis_kd + self.pad_head) // self.stride_d, m_index // a_m0,
                         axis_k1, m_index % a_m0, axis_k0).astype(self.c_dtype) *
                tensor_b(g_index, axis_kd, axis_k1, co1_index, co0_index, axis_k0).astype(self.c_dtype),
                tvm.const(0.0, self.c_dtype))
            return tensor_c

        def __mad_condition_stride1(param_dict):
            indices = param_dict.get('indices')
            axis_kd = param_dict.get('axis_kd')
            axis_k1 = param_dict.get('axis_k1')
            axis_k0 = param_dict.get('axis_k0')
            tensor_a = param_dict.get('tensor_a')
            tensor_b = param_dict.get('tensor_b')
            g_index, n_index, deep_index, co1_index, m_index, co0_index = indices
            tensor_c = tvm.select(
                tvm.all((deep_index - axis_kd + self.pad_head) >= 0,
                        (deep_index - axis_kd + self.pad_head) < tensor_a.shape[2]),
                tensor_a(g_index, n_index, (deep_index - axis_kd + self.pad_head), m_index // a_m0, axis_k1,
                         m_index % a_m0, axis_k0).astype(self.c_dtype) *
                tensor_b(g_index, axis_kd, axis_k1, co1_index, co0_index, axis_k0).astype(self.c_dtype),
                tvm.const(0.0, self.c_dtype))
            return tensor_c

        def __mad_condition_noverlap(indices, axis_k1, axis_k0, tensor_a, tensor_b):
            g_index, n_index, deep_index, co1_index, m_index, co0_index = indices
            tensor_c = tvm.select(
                tvm.all((deep_index + self.pad_head) % self.stride_d < self.kernel_d,
                        (deep_index + self.pad_head) // self.stride_d < tensor_a.shape[2]),
                tensor_a(g_index, n_index,
                         (deep_index + self.pad_head) // self.stride_d, m_index // a_m0, axis_k1, m_index % a_m0,
                         axis_k0).astype(self.c_dtype) * tensor_b(g_index,
                                                                 (deep_index + self.pad_head) % self.stride_d, axis_k1,
                                                                 co1_index, co0_index, axis_k0).astype(self.c_dtype),
                tvm.const(0.0, self.c_dtype))
            return tensor_c

        def __conv3d_backprop_input_mad(indices, tensor_a, tensor_b):
            mad_condition_param_dict = {
                'indices': indices,
                'axis_kd': axis_kd,
                'axis_k1': axis_k1,
                'axis_k0': axis_k0,
                'tensor_a': tensor_a,
                'tensor_b': tensor_b
            }
            tensor_c = tvm.sum(__mad_condition(mad_condition_param_dict), axis=[axis_kd, axis_k1, axis_k0])
            return tensor_c

        def __conv3d_backprop_input_mad_stride1(indices, tensor_a, tensor_b):
            param_dict = {
                'indices': indices,
                'axis_kd': axis_kd,
                'axis_k1': axis_k1,
                'axis_k0': axis_k0,
                'tensor_a': tensor_a,
                'tensor_b': tensor_b
            }
            tensor_c = tvm.sum(__mad_condition_stride1(param_dict), axis=[axis_kd, axis_k1, axis_k0])
            return tensor_c

        def __conv3d_backprop_input_mad_noverlap(indices, tensor_a, tensor_b):
            tensor_c = tvm.sum(__mad_condition_noverlap(indices, axis_k1, axis_k0, tensor_a, tensor_b),
                               axis=[axis_k1, axis_k0])
            return tensor_c

        a_group, a_batch, a_deep, a_m1, a_k1, a_m0, a_k0 = cube_util.shape_to_list(tensor_a.shape)
        _, b_kd, _, b_n1, b_n0, _ = [i.value for i in tensor_b.shape]
        shape_c = (a_group, a_batch, self.dedx_d, b_n1, a_m1 * a_m0, b_n0)
        axis_k0 = tvm.reduce_axis([0, a_k0], name='axis_k0')
        axis_k1 = tvm.reduce_axis([0, a_k1], name='axis_k1')
        axis_kd = tvm.reduce_axis([0, b_kd], name='axis_kd')

        if self.stride_d == self.kernel_d and (self.dedx_d + self.pad_head +
                                               self.pad_tail) == a_deep * self.stride_d:
            tensor_c = tvm.compute(
                shape_c,
                lambda g_index, n_index, deep_index, co1_index, m_index, co0_index: tvm.sum(
                    (tensor_a(g_index, n_index, (deep_index + self.pad_head) // self.stride_d, m_index // a_m0, axis_k1,
                              m_index % a_m0, axis_k0) * tensor_b(g_index,
                                                                  (deep_index + self.pad_head) % self.stride_d, axis_k1,
                                                                  co1_index, co0_index, axis_k0)).astype(self.c_dtype),
                    axis=[axis_k1, axis_k0]),
                name="C",
                tag=self.tag + "mad")
        elif self.kernel_d <= self.stride_d:
            tensor_c = tvm.compute(shape_c,
                                   lambda *indices: __conv3d_backprop_input_mad_noverlap(indices, tensor_a, tensor_b),
                                   name="C",
                                   tag=self.tag + "mad")
        elif self.stride_d == 1:
            tensor_c = tvm.compute(shape_c,
                                   lambda *indices: __conv3d_backprop_input_mad_stride1(indices, tensor_a, tensor_b),
                                   name="C",
                                   tag=self.tag + "mad")
        else:
            tensor_c = tvm.compute(shape_c,
                                   lambda *indices: __conv3d_backprop_input_mad(indices, tensor_a, tensor_b),
                                   name="C",
                                   tag=self.tag + "mad")

        return tensor_c


class Conv3DDxTensorAPattern():
    """
    Conv3DDxTensorAPattern

    Parameters
    ----------
    kernel_h: height of filter

    kernel_w: width of filter

    stride : list of strides, [strided, strideh, stridew]

    pad: list of padding, [pad_up, pad_down, pad_left, pad_right]

    dilation: list of dilation, [dilationd, dilationh, dilationw]

    Returns
    -------
    conv_pattern_instance : instance of conv pattern
    """

    def __init__(self, attr_dict, var_map):
        self.var_map = var_map
        self.kernel_h = attr_dict.get("kernel_h")
        self.kernel_w = attr_dict.get("kernel_w")
        self.stride_d, self.stride_h, self.stride_w = attr_dict.get("bp_strides")
        (self.pad_head, self.pad_tail, self.pad_up, self.pad_down, self.pad_left,
         self.pad_right) = attr_dict.get("bp_pads")
        self.dilation_d, self.dilation_h, self.dilation_w = attr_dict.get("dilations")
        self.group_dict = attr_dict.get("group_dict")
        self.tag = attr_dict.get("tag")
        self.block_size_m0 = 16
        self.binary_flag = attr_dict.get(const.BINARY_KEY)
        self.load3d_special_flag = False

    def generate_a(self, dedy_l1):
        """
        calculate im2col_fractal tensor

        Returns
        -------
        a_col : a_im2col_fractal tensor
        """
        self._validate_params()

        batch_n, dedy_d, _, dedy_h, dedy_w, dedy_cout0 = cube_util.shape_to_list(dedy_l1.shape)
        dedx_h = self.var_map.get("dedx_h") if self.var_map.get("dedx_h") is not None else self._cal_howo(
            dedy_h, dedy_w)[0]
        dedx_w = self.var_map.get("dedx_w") if self.var_map.get("dedx_w") is not None else self._cal_howo(
            dedy_h, dedy_w)[1]
        self._update_load3d_special_flag(self.var_map, dedx_h, dedx_w)
        if self.load3d_special_flag:
            dedx_w += 1
            self.pad_right += 1

        real_g = self.group_dict.get("real_g")
        cout_g = self.group_dict.get("cout_g")
        cout1_g = self.group_dict.get("cout1_g")
        dedy_cout1_g = cout1_g if cout1_g is not None else cout_g // dedy_cout0

        a_im2col_row_major_shape = (real_g,
                                    batch_n,
                                    dedy_d,
                                    dedx_h * dedx_w,
                                    dedy_cout1_g,
                                    self.kernel_h,
                                    self.kernel_w,
                                    dedy_cout0)

        a_im2col_fractal_shape = (real_g,
                                  batch_n,
                                  dedy_d,
                                  (dedx_h * dedx_w + self.block_size_m0 - 1) // self.block_size_m0,
                                  dedy_cout1_g * self.kernel_h * self.kernel_w,
                                  self.block_size_m0,
                                  dedy_cout0)

        im2col_row_major_param_dict = {
            'a_im2col_row_major_shape': a_im2col_row_major_shape,
            'feature_map': dedy_l1,
            'kernel_w': self.kernel_w,
            'cout_g': cout_g,
            'bp_pads': [self.pad_up, self.pad_down, self.pad_left, self.pad_right],
            'stride': [self.stride_h, self.stride_w],
            'compute_dtype': dedy_l1.dtype,
            'var_map': self.var_map,
            'dilation': [self.dilation_d, self.dilation_h, self.dilation_w],
            'special_load3d_flag': self.load3d_special_flag
        }
        a_row_major = self._im2col_row_major(im2col_row_major_param_dict)
        a_col = self._im2col_fractal(a_im2col_fractal_shape, a_row_major)

        return a_col

    def _cal_howo(self, height_in, width_in):
        """
        calculate the height and width of convolution output tensor

        Parameters
        ----------
        height_in : height of input tensor

        width_in : width of input tensor

        Returns
        ----------
        height_out : height of output tensor

        width_out : width of output tensor
        """
        new_hw = [height_in, width_in]
        kernel_h, kernel_w = self.kernel_h, self.kernel_w
        new_pad_before = (self.pad_up, self.pad_left)
        new_pad_after = (self.pad_down, self.pad_right)
        stride = [self.stride_h, self.stride_w]
        dilation = [self.dilation_h, self.dilation_w]

        height_out, width_out = [
            ((i + p_before + p_after - ((kernel - 1) * d + 1)) // s + 1)
            for i, p_before, p_after, kernel, s, d in
            zip(new_hw,
                new_pad_before,
                new_pad_after,
                [kernel_h, kernel_w],
                stride,
                dilation)]
        return height_out, width_out

    def _im2col_row_major(self, im2col_row_major_param_dict):
        """
        calculate im2col_row_major tensor
        Parameters
        ----------
        im2col_row_major_param_dict:

            a_im2col_vm_shape : shape of a_im2col_row_major

            tensor_a : feature map

            kernel_w : width of filter

            cout_g : new filter batch for group

            bp_pads: backprop pad list

            stride: the stride value

            compute_dtype: dtype of compute result

            var_map: the parameters for dynamic shape

            tag : tag for different compute stage, '' by default

            dilation: the dilation value, (1, 1, 1) by default
        -------
        Returns : a_im2col_row_major tensor

        """
        a_im2col_vm_shape = im2col_row_major_param_dict.get('a_im2col_row_major_shape')
        tensor_a = im2col_row_major_param_dict.get('feature_map')
        kernel_w = im2col_row_major_param_dict.get('kernel_w')
        cout_g = im2col_row_major_param_dict.get('cout_g')
        bp_pads = im2col_row_major_param_dict.get('bp_pads')
        stride = im2col_row_major_param_dict.get('stride')
        compute_dtype = im2col_row_major_param_dict.get('compute_dtype')
        var_map = im2col_row_major_param_dict.get('var_map')
        special_load3d_flag = im2col_row_major_param_dict.get('special_load3d_flag', False)
        dilation = im2col_row_major_param_dict.get('dilation', (1, 1, 1))

        def __im2col_row_major_indices(indices, tensor_a, padding_var, stride, dilation):
            """
            calculate im2col_row_major tvm lambda function
            Parameters
            ----------
            indices : indices in lambda function

            tensor_a : feature map

            kernel_w: width of filter

            bp_pads: backprop pad list

            stride: the stride value

            dilation: the dilation value
            -------
            Returns : im2col_row_major tensor
            """
            _, _, _, a_height, a_width, c_0 = tensor_a.shape
            g_index, n_index, deep_index, hw_index, c1_index, kh_index, kw_index, c0_index = indices
            stride_h, stride_w = stride
            _, dilate_h, dilate_w = dilation
            info_padding_up, _, info_padding_left, _ = padding_var

            c1_index = g_index * (cout_g // c_0) + c1_index
            h_index = (hw_index // width_out) * stride_h + kh_index * dilate_h
            w_index = (hw_index % width_out) * stride_w + kw_index * dilate_w

            return tvm.select(
                tvm.any(h_index < info_padding_up, h_index > a_height + info_padding_up - 1,
                        w_index < info_padding_left, w_index > a_width + info_padding_left - 1),
                tvm.const(0.0, compute_dtype),
                tensor_a(n_index, deep_index, c1_index, h_index - info_padding_up, w_index - info_padding_left,
                         c0_index))

        if self.binary_flag:
            padding_var = bp_pads
            width_out = get_te_var("dedx_w").get_tvm_var()
        elif var_map:
            info_padding_up = tvm.var("info_padding_up")
            info_padding_bottom = tvm.var("info_padding_bottom")
            info_padding_left = tvm.var("info_padding_left")
            info_padding_right = tvm.var("info_padding_right")
            padding_var = [info_padding_up, info_padding_bottom, info_padding_left, info_padding_right]
            width_out = tvm.var("width_out")
        else:
            padding_var = bp_pads
            width_out = (tensor_a.shape[-2] + bp_pads[-1] + bp_pads[-2] -
                        ((kernel_w - 1)*dilation[-1] + 1)) // stride[-1] + 1

        return tvm.compute(
            a_im2col_vm_shape,
            lambda *indices: __im2col_row_major_indices(indices, tensor_a, padding_var, stride, dilation),
            name='im2col_row_major',
            tag=self.tag + 'im2col_row_major',
            attrs={
                'padding': bp_pads,
                'bp_pads': bp_pads,
                "dilation": dilation,
                'padding_var': padding_var,
                'width_out_var': width_out,
                'special_load3d_flag': special_load3d_flag
            })

    def _im2col_fractal(self, a_im2col_shape, tensor_a_row_major):
        """
        calculate im2col_fractal tensor
        Parameters
        ----------
        a_im2col_shape : shape of a_im2col

        tensor_a_row_major : feature map after row major
        -------
        Returns : a_im2col_fractal tensor
        """
        def __im2col_fractal_indices(indices, tensor_a_row_major):
            """
            calculate im2col_fractal tvm lambda function
            Parameters
            ----------
            indices : indices in lambda function

            tensor_a_row_major : feature map

            -------
            Returns : im2col_fractal tvm lambda function
            """
            _, _, _, _, _, a_col_m0, _ = a_im2col_shape
            _, _, _, a_row_major_hw, _, kernel_h, kernel_w, _ = tensor_a_row_major.shape
            g_index, n_index, deep_index, m1_index, k1_index, m0_index, k0_index = indices

            hw_index = m1_index * a_col_m0 + m0_index

            c1_index = k1_index // kernel_w // kernel_h
            kh_index = k1_index // kernel_w % kernel_h

            kw_index = k1_index % kernel_w

            c0_index = k0_index

            return tvm.select(
                tvm.any(hw_index < 0, hw_index > a_row_major_hw - 1), tvm.const(0.0, tensor_a_row_major.dtype),
                tensor_a_row_major(g_index, n_index, deep_index, hw_index, c1_index, kh_index, kw_index, c0_index))

        return tvm.compute(a_im2col_shape,
                           lambda *indices: __im2col_fractal_indices(indices, tensor_a_row_major),
                           name='im2col_fractal',
                           tag=self.tag + 'im2col_fractal')

    def _update_load3d_special_flag(self, var_map, h_out, w_out):
        """
        get special_load3d_flag

        Parameters
        ----------
        var_map : the parameters for dynamic shape, {} by default

        h_out : height of output tensor

        w_out : width of output tensor

        Returns
        ----------
        w_out : width of output tensor
        """
        if (tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION") not in ("Hi3796CV300CS", "Ascend310")
            and cube_util.is_load3d_constraint()
            and not var_map
            and int(h_out) != 1
            and int(w_out) == 1):
            self.load3d_special_flag = True

    def _validate_params(self):
        if not self.var_map:
            if int(self.pad_up) < cube_util.Load3DParam.pad_min() or \
                int(self.pad_up) > cube_util.Load3DParam.pad_max():
                cube_err.raise_err_one_para('E62006', 'conv3d_backprop_input',
                    'backprop_pad_up=((kh - 1) * dilation_h - pad_up)=[{}] is invalid, \
it should be in [0,255]'.format(self.pad_up))
            if int(self.pad_left) < cube_util.Load3DParam.pad_min() or \
                int(self.pad_left) > cube_util.Load3DParam.pad_max():
                cube_err.raise_err_one_para('E62006', 'conv3d_backprop_input',
                    'backprop_pad_left=((kw - 1) * dilation_w - pad_left)=[{}] is invalid, \
it should be in [0,255]'.format(self.pad_left))
            for i in (self.pad_up, self.pad_down, self.pad_left, self.pad_right):
                if int(i) < cube_util.Load3DParam.pad_min() or int(i) > cube_util.Load3DParam.pad_max():
                    cube_err.raise_err_one_para('E62006', 'conv3d_backprop_input',
                                                'pad value in reverse process of convolution should be in [0,255]')