#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d backprop input DSL interface.
"""
import copy
import json

from tbe import tvm
from tbe.common import utils as tbe_utils
from tbe.common.context import op_context
from tbe.common.platform import platform_info
from tbe.common.platform import get_cube_mkn
from tbe.common.utils.const import SplitAxisMode
from tbe.common.utils.const import SPLIT_AXIS_MODE_STR
from tbe.common.utils.const import IS_CONV1D_SITUATION_STR
from tbe.common.utils.const import QUANT_DTYPES
from tbe.common.utils.const import WEIGHT_SPARSE_4_2
from tbe.common.utils.errormgr import error_manager_cube
from tbe.common.utils.errormgr import error_manager_util
from tbe.dsl.compute import cube_util
from tbe.dsl.compute.conv2d_backprop_input_compute_util import CalL1Size
from tbe.dsl.compute.conv2d_backprop_input_general_compute import DeConvPattern
from tbe.dsl.compute.conv2d_backprop_input_ub_compute import DeConvUbPattern
from tbe.dsl.compute.conv2d_backprop_input_opti_compute import DeConvKernelSize1Pattern
from tbe.dsl.compute.cube_util import BIT_RATIO_DICT
from tbe.dsl.compute.util import int_ceil_div
from tbe.dsl.base.operation import get_te_var
from tbe.tvm import Tensor

NoneType = type(None)

# shape dim
DY_SHAPE_DIM = 5
FILTER_SHAPE_DIM = 4
FILTER_DIM = 4
DX_SHAPE_DIM = 4
STRIDES_DIM = 2
PADDING_DIM = 4
DILATION_DIM = 4

# padH, padW must be in [0,255]
PAD_MIN = 0
PAD_MAX = 255

# dilation must be in [1,255]
DILATION_MIN = 1
DILATION_MAX = 255

# filterH, filterW must >= 1
FILTER_HW_MIN = 1
FILTER_HW_LOAD3D_MAX = 255

DY_FILLING_HW_MIN = 1
DY_FILLING_H_MAX = 200000
DY_FILLING_W_MAX = 4096

# fmapH, fmapW must be in [1,4096]
DX_HW_MIN = 1
DX_H_MAX = 200000
DX_W_MAX = 4096

# conv1d and split_w situation support w not larger than 2^31-1
FMAP_W_MAX = 2147483647

# stride must >= 1
STRIDE_MIN = 1
STRIDE_MUL_MIN = 1

# same as (2**63-1)
DATA_SIZE_MAX = 9223372036854775807
SUPPORT_FIXPIPE_INTRINSIC = "Intrinsic_fix_pipe_l0c2out"
GENERAL_SCHEDULE_WHITE_LIST = (
    ((2, 3, 16, 16), (1, 6, 1, 75, 16), (1, 32, 1, 1), (1, 32, 1, 223), (1, 3), (1, 1, 149, 3), 31),
    ((8, 1, 16, 16), (3481, 8, 1, 24, 16), (1, 128, 1, 1), (3481, 128, 1, 212), (1, 9), (1, 1, 253, 1), 123)
)


class DeconvParam:
    """
    class of deconvParam
    """

    def __init__(self):
        pass


    al1_size = 0
    bl1_size = 0
    var_map = {}
    dx_multioutput_flag = False

    @classmethod
    def set_default(cls):
        """
        set default l1_size in deconvparam
        """
        cls.al1_size = 0
        cls.bl1_size = 0

    def get_l1_size(self):
        """
        get the l1_size in deconvparam
        """
        return [self.al1_size, self.bl1_size]


def _check_variable_range(attr_value, attr_name, attr_min=None, attr_max=None):
    """
    check variable range

    """
    if attr_min is None and attr_max is None:
        return
    if attr_min is None:
        if (not isinstance(attr_value, int)) or attr_value > attr_max:
            args_dict = {
                "errCode": "E60114",
                "reason": "{} exceed max_value."
                " max_value={}.".format(attr_name, attr_max),
                "value": "attr_value = {}".format(attr_value)
            }
            raise RuntimeError(args_dict,
                               error_manager_util.get_error_message(args_dict))
    elif attr_max is None:
        if (not isinstance(attr_value, int)) or attr_value < attr_min:
            args_dict = {
                "errCode": "E60114",
                "reason": "{} less than min_value. "
                "min_value={}.".format(attr_name, attr_min),
                "value": "attr_value = {}".format(attr_value)
            }
            raise RuntimeError(args_dict,
                               error_manager_util.get_error_message(args_dict))
    elif (not isinstance(attr_value, int)) or attr_value < attr_min or attr_value > attr_max:
        args_dict = {
            "errCode": "E60011",
            "range": "[{},{}]".format(attr_min, attr_max),
            "attr_name": attr_name,
            "value": attr_value
        }
        raise RuntimeError(args_dict,
                           error_manager_util.get_error_message(args_dict))


def _get_fixpipe_fusion_flag():
    context = op_context.get_context()
    if context is None:
        return False
    build_options = context.get_addition("build_options")
    if build_options:
        options = json.loads(build_options)
        return options.get("fixpipe_fusion", False)
    return False


def _check_equal_rule(param_1, param_2, param_name1, param_name2):
    """
    check variable equal

    """
    if param_1 != param_2:
        dict_args = {}
        dict_args["errCode"] = "E65007"
        dict_args["param1"] = param_name1
        dict_args["param2"] = param_name2
        dict_args["actual_value"] = "{}, {}".format(param_1, param_2)
        raise RuntimeError(dict_args, error_manager_util.get_error_message(dict_args))


def _is_conv1d_situation(inputs, attrs):
    """
    check conv1d situation and optimize stride h

    """
    out_backprop, filter_sizes = inputs
    input_sizes, strides, padding, dilations, output_padding = attrs
    _, _, dy_h, _, _ = cube_util.shape_to_list(out_backprop.shape)
    _, _, filter_h, _ = filter_sizes
    _, _, dx_h, _ = input_sizes
    stride_h, stride_w = strides
    pad_up, pad_down, _, _ = padding
    _, _, dilation_h, _ = dilations
    _, _, output_padding_h, _ = output_padding
    filter_h_dilation = (filter_h - 1) * dilation_h + output_padding_h + 1
    dx_h_after_pad = dx_h + pad_up + pad_down

    if dx_h_after_pad == 1 and filter_h_dilation == 1 and dy_h == 1:
        if stride_h != 1:
            strides[0] = 1
        # skip conv1d template because of poor performance when stride_w > 1
        if stride_w > 1 and platform_info.intrinsic_check_support(
                "Intrinsic_fix_pipe_l0c2out") and out_backprop.dtype == "float16":
            return False
        return True
    return False


def _check_input_params(
        filters,
        out_backprop,
        filter_sizes,
        input_sizes,
        strides,
        group_dict,
        para_dict,
        fusion_para=None
):
    """
    check the input params of conv2d_backprop_input_compute

    Parameters
    ----------
    filters : the weight tensor in fractal_z format

    out_backprop : the dE/dY tensor in 5HD format

    filter_sizes : shape of weight, [N, C, H, W]

    input_sizes : shape of dE/dX, [N, C, H, W]

    strides : list of strides, [strideh, stridew]

    group_dict : The params of group convolution.

    para_dict: params dict
        padding : list of padding, [pad_up, pad_down, pad_left, pad_right]
        dilations : [1, 1, 1, 1] by default
        res_dtype : dE/dX data type, "float16" by default
        offset_w: the offset for weight
        output_padding: a list/tuple of output_padding, the attr of Conv2DTransposeD

    fusion_para: the l1 fusion para, default is None

    Returns
    ----------
    split_axis_mode: the split axis mode, 0: split_hw, 1: split_w, default is 0
    """

    def _check_shape_rule(shape_arg, shape_dim, shape_dtype, shape_name):
        if len(shape_arg) != shape_dim:
            dict_args = {}
            dict_args["errCode"] = "E60006"
            dict_args["param_name"] = shape_name
            dict_args["expected_length"] = str(shape_dim)
            dict_args["length"] = str(len(shape_arg))
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )
        axis_i = 0
        for i in shape_arg:
            if not isinstance(i, shape_dtype):
                dict_args = {}
                dict_args["errCode"] = "E65001"
                dict_args["param_name"] = shape_name
                dict_args["axis_rule"] = str(shape_dtype)
                dict_args["wrong_axis"] = str(axis_i)
                dict_args["actual_value"] = str(i)
                raise RuntimeError(
                    dict_args, error_manager_util.get_error_message(dict_args)
                )
            axis_i = axis_i + 1

    # check dtype
    def _check_dtype(valid_dtype_dict):
        def _gen_dict_args(name, dtype_list, type_value):
            dict_args = {}
            dict_args["errCode"] = "E60011"
            dict_args["attr_name"] = name
            dict_args["range"] = str(dtype_list)
            dict_args["value"] = type_value
            return dict_args

        if filters.dtype not in valid_dtype_dict["filter"]:
            dict_args = _gen_dict_args(
                "filter dtype", valid_dtype_dict["filter"], filters.dtype
            )
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )

        if out_backprop.dtype not in valid_dtype_dict["dedy"]:
            dict_args = _gen_dict_args(
                "out_backprop dtype", valid_dtype_dict["dedy"], out_backprop.dtype
            )
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )

        if filters.dtype != out_backprop.dtype:
            dict_args = {}
            dict_args["errCode"] = "E65002"
            dict_args["param_1"] = "filter"
            dict_args["param_2"] = "out_backprop"
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )

        if res_dtype not in valid_dtype_dict["dx"]:
            dict_args = _gen_dict_args("dx dtype", valid_dtype_dict["dx"], res_dtype)
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )

    # check shape
    def _check_shape():
        if len(filters.shape) != FILTER_SHAPE_DIM:
            dict_args = {}
            dict_args["errCode"] = "E65003"
            dict_args["param_name"] = "filter.shape"
            dict_args["format"] = "[k1, n1, n0, k0]"
            dict_args["expect_dim"] = str(FILTER_SHAPE_DIM)
            dict_args["dim"] = str(len(filters.shape))
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )

        if len(out_backprop.shape) != DY_SHAPE_DIM:
            dict_args = {}
            dict_args["errCode"] = "E65003"
            dict_args["param_name"] = "out_backprop.shape"
            dict_args["format"] = "[No, Co1, Ho, Wo, Co0]"
            dict_args["expect_dim"] = str(DY_SHAPE_DIM)
            dict_args["dim"] = str(len(out_backprop.shape))
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )

        _check_shape_rule(filter_sizes, FILTER_DIM, int, "filter_sizes")
        _check_shape_rule(strides, STRIDES_DIM, int, "strides")
        _check_shape_rule(dilations, DILATION_DIM, int, "dilations")

        if not var_map:
            _check_shape_rule(input_sizes, DX_SHAPE_DIM, int, "input_sizes")
            _check_shape_rule(padding, PADDING_DIM, int, "padding")

    # limitation by chip
    def _is_load3d_special_case():
        # limitation by chip:
        # load3d instruction not support out_w = 1
        # only Ascend310/Hi3796CS/SD3403 can support
        # in dx, out is fmap
        if (cube_util.is_load3d_constraint() and dx_h != 1 and dx_w == 1 and var_map):
            return True
        return False

    padding = para_dict.get("padding")
    dilations = para_dict.get("dilations")
    res_dtype = para_dict.get("res_dtype", "float16")
    offset_w = para_dict.get("offset_w")
    output_padding = para_dict.get("output_padding", (0, 0, 0, 0))
    alg = para_dict.get("alg")
    # get the value of group_dict
    g_extend = group_dict.get(cube_util.GroupDictKeys.g_extend)
    dx_c1_extend = group_dict.get(cube_util.GroupDictKeys.ci1g)
    dy_c1_extend = group_dict.get(cube_util.GroupDictKeys.co1g)
    dx_c_ori = group_dict.get(cube_util.GroupDictKeys.dx_c_ori)
    filter_c_ori = group_dict.get(cube_util.GroupDictKeys.filter_c_ori)

    var_map = DeconvParam.var_map

    valid_dtype_dict = {}
    valid_dtype_dict["filter"] = ("float16", "int8", "int4")
    valid_dtype_dict["dedy"] = ("float16", "int8", "int4")
    valid_dtype_dict["dx"] = ("float16", "float32", "int32")
    if platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
        valid_dtype_dict["filter"] += ("bfloat16", "float32")
        valid_dtype_dict["dedy"] += ("bfloat16", "float32")
        valid_dtype_dict["dx"] += ("bfloat16", "float32")
    _check_dtype(valid_dtype_dict)
    _check_shape()

    # begin to fetch params
    if filters.dtype in QUANT_DTYPES:
        filter_cout1, _, filter_cin0, filter_cout0 = cube_util.shape_to_list(filters.shape)
        filter_cout1 = filter_cout1 / filter_sizes[2] / filter_sizes[3] / g_extend
    else:
        _, filter_cout1, filter_cout0, filter_cin0 = cube_util.shape_to_list(filters.shape)
    dy_batch, dy_c1, dy_h, dy_w, dy_c0 = cube_util.shape_to_list(out_backprop.shape)
    filter_cout, _, filter_h, filter_w = filter_sizes
    dx_batch, dx_c, dx_h, dx_w = input_sizes
    stride_h, stride_w = strides
    pad_up, pad_down, pad_left, pad_right = padding
    dilation_n, dilation_c, dilation_h, dilation_w = dilations
    _, _, output_padding_h, output_padding_w = output_padding
    # output_padding is only for Conv2DTransposeD. For other operators, output_padding is 0, so it can be ignored.
    filter_h_dilation = (filter_h - 1) * dilation_h + output_padding_h + 1
    filter_w_dilation = (filter_w - 1) * dilation_w + output_padding_w + 1
    dx_h_after_pad = dx_h + pad_up + pad_down
    dx_w_after_pad = dx_w + pad_left + pad_right
    _, dedy_k0, _ = get_cube_mkn(out_backprop.dtype)
    _, w_k0, w_n0 = get_cube_mkn(filters.dtype)


    # filter_cout is aligned to a multiple of w_k0
    filter_cout = (filter_cout + w_k0 - 1) // w_k0 * w_k0

    # special cases
    dy_filling_hw_min, dx_hw_min = DY_FILLING_HW_MIN, DX_HW_MIN
    dy_filling_w_max, dx_w_max = FMAP_W_MAX, FMAP_W_MAX
    dy_filling_h_max, dx_h_max = DY_FILLING_H_MAX, DX_H_MAX

    # limitation by chip:
    # load3d instruction not support out_w = 1
    # only Ascend310/Hi3796CS/SD3403 can support
    # in dx, out is fmap
    if _is_load3d_special_case():
        dy_filling_hw_min = 2
        dx_hw_min = 2

    if "batch_n" in var_map:
        batch_n_bound = get_te_var("batch_n").get_bound()
    if "dedy_h" in var_map:
        dedy_h_bound = get_te_var("dedy_h").get_bound()
        dx_h_bound = get_te_var("dx_h").get_bound()
    if "dedy_w" in var_map:
        dedy_w_bound = get_te_var("dedy_w").get_bound()
        dx_w_bound = get_te_var("dx_w").get_bound()

    if offset_w is not None:
        dict_args = {}
        dict_args["errCode"] = "E65004"
        raise RuntimeError(dict_args, error_manager_util.get_error_message(dict_args))

    # dy
    def _check_dy():
        _check_equal_rule(dy_c0, dedy_k0, "dy_c0", str(dedy_k0))
        _check_variable_range(
            dy_h * stride_h, "dy_h*stride_h", dy_filling_hw_min, dy_filling_h_max
        )
        if filter_h == 1 and filter_w == 1:
            _check_variable_range(
                dy_w * stride_w * stride_h,
                "dy_w*stride_w*stride_h",
                dy_filling_hw_min,
                dy_filling_w_max
            )
        else:
            _check_variable_range(
                dy_w * stride_w,
                "dy_w*stride_w",
                dy_filling_hw_min,
                dy_filling_w_max
            )

    # w
    # check filter shape and filter_sizes from topi
    def _check_filter():
        if filters.dtype == "float32":
            _check_equal_rule(filter_cout0, w_n0, "filter_cout0", str(w_n0))
            _check_equal_rule(filter_cin0, w_k0, "filter_cin0", str(w_k0))
        else:
            _check_equal_rule(filter_cout0, w_k0, "filter_cout0", str(w_k0))
            _check_equal_rule(filter_cin0, w_n0, "filter_cin0", str(w_n0))

        if alg != WEIGHT_SPARSE_4_2:
            _check_equal_rule(filter_cout1, dy_c1_extend, "filter_cout1", "dy_c1_extend")
        else:
            # 2: cout_sparse is 1/2 of cout_origin
            _check_equal_rule(filter_cout1, int_ceil_div(dy_c1_extend, 2),  "filter_cout1", "dy_c1_extend")

        _check_variable_range(filter_h, "filter_h", FILTER_HW_MIN)

        _check_variable_range(filter_w, "filter_w", FILTER_HW_MIN)

        def _check_max(x_1, x_2, name_1, name_2):
            if x_1 > x_2:
                dict_args = {}
                dict_args["errCode"] = "E65005"
                dict_args["param_1"] = name_1
                dict_args["param_2"] = name_2
                raise RuntimeError(
                    dict_args, error_manager_util.get_error_message(dict_args)
                )

        if "dedy_h" not in var_map:
            _check_max(
                filter_h_dilation,
                dx_h_after_pad,
                "filter_h after dilation",
                "dx_h after pad"
            )
        if "dedy_w" not in var_map:
            _check_max(
                filter_w_dilation,
                dx_w_after_pad,
                "filtre_w after dilation",
                "dx_w after pad"
            )

    # dx
    def _check_dx():
        _check_variable_range(dx_h, "dx_h", dx_hw_min, dx_h_max)
        _check_variable_range(dx_w, "dx_w", dx_hw_min, dx_w_max)
        _check_equal_rule(dx_batch, dy_batch, "dx_batch", "dy_batch")
        _check_equal_rule((dx_h_after_pad - filter_h_dilation) // stride_h + 1, dy_h,
                          "(dx_h_after_pad - filter_h_dilation) // stride_h + 1", "dy_h")

        _check_equal_rule((dx_w_after_pad - filter_w_dilation) // stride_w + 1, dy_w,
                          "(dx_w_after_pad - filter_w_dilation) // stride_w + 1", "dy_h")

        _check_equal_rule(dx_c_ori, filter_c_ori, "dx_cin", "filter_cin")

    # strides
    def _check_strides():
        _check_variable_range(stride_h, "stride_h", STRIDE_MIN)

        _check_variable_range(stride_w, "stride_w", STRIDE_MIN)

    # padding
    def _check_padding():
        if fusion_para.get("l1_fusion_type") == -1:
            _check_variable_range(pad_up, "pad_up", PAD_MIN)

            _check_variable_range(pad_down, "pad_down", PAD_MIN)

            _check_variable_range(pad_left, "pad_up", PAD_MIN)

            _check_variable_range(pad_right, "pad_down", PAD_MIN)

    # dilation
    def _check_dilation():
        if dilation_n != 1 or dilation_c != 1:
            dict_args = {}
            dict_args["errCode"] = "E60023"
            dict_args["dilation_n"] = str(dilation_n)
            dict_args["dilation_c"] = str(dilation_c)
            raise RuntimeError(
                dict_args, error_manager_util.get_error_message(dict_args)
            )
        _check_variable_range(dilation_h, "dilation_h", DILATION_MIN)
        _check_variable_range(dilation_w, "dilation_w", DILATION_MIN)

    # 64 bits limitation check
    def _check_chip_limitation():
        def _align(x_1, x_2):
            return (x_1 + x_2 - 1) // x_2 * x_2

        def _check_64bits_limitation(attr_value, dtype=None):
            bit_ratio = BIT_RATIO_DICT.get("float16") if (dtype is None) else BIT_RATIO_DICT.get(dtype)
            if attr_value * bit_ratio > DATA_SIZE_MAX:
                dict_args = {"errCode": "E60020"}
                raise RuntimeError(
                    dict_args, error_manager_util.get_error_message(dict_args)
                )

        _, dedy_k0, _ = get_cube_mkn(out_backprop.dtype)
        _, w_k0, w_n0 = get_cube_mkn(filters.dtype)

        if "batch_n" in var_map:
            dy_batch_upper, dx_batch_upper = batch_n_bound[1], batch_n_bound[1]
        else:
            dy_batch_upper, dx_batch_upper = dy_batch, dx_batch
        if "dedy_h" in var_map:
            dy_h_upper, dx_h_upper = dedy_h_bound[1], dx_h_bound[1]
        else:
            dy_h_upper, dx_h_upper = dy_h, dx_h
        if "dedy_w" in var_map:
            dy_w_upper, dx_w_upper = dedy_w_bound[1], dx_w_bound[1]
        else:
            dy_w_upper, dx_w_upper = dy_w, dx_w
        if dy_batch_upper and dy_h_upper and dy_w_upper:
            fmap_size = dx_batch_upper * _align(dx_c, dedy_k0) * dx_h_upper * dx_w_upper
            dedy_size = dy_batch_upper * dy_c1 * dy_h_upper * dy_w_upper * dy_c0
            _check_64bits_limitation(fmap_size, dtype=res_dtype)
            _check_64bits_limitation(dedy_size, dtype=out_backprop.dtype)
        filter_size = (_align(dy_c1_extend, w_k0) *
                       _align(dx_c1_extend, w_n0)) * filter_h * filter_w * g_extend
        _check_64bits_limitation(filter_size, dtype=filters.dtype)

    if "dx_h" not in var_map and "dx_w" not in var_map:
        _check_dy()
        _check_dx()
        if not platform_info.intrinsic_check_support(SUPPORT_FIXPIPE_INTRINSIC):
            _check_padding()

    _check_filter()
    _check_strides()
    _check_dilation()
    _check_chip_limitation()


def _is_dynamic_with_nonerange(input_dict):
    if not isinstance(input_dict.get("filter_shape_nchw")[2], int) or \
        not isinstance(input_dict.get("filter_shape_nchw")[3], int):
        return True
    if not isinstance(input_dict.get("dy_shape_nc1hwc0")[2], int) or \
        not isinstance(input_dict.get("dy_shape_nc1hwc0")[3], int):
        return True
    if not isinstance(input_dict.get("dx_shape_nchw")[1], int) or \
        not isinstance(input_dict.get("dx_shape_nchw")[2], int) or \
        not isinstance(input_dict.get("dx_shape_nchw")[3], int):
        return True
    if not isinstance(input_dict.get("strides")[0], int) or input_dict.get("strides")[0] == -1 or \
        not isinstance(input_dict.get("strides")[1], int) or input_dict.get("strides")[1] == -1:
        return True
    if not isinstance(input_dict.get("dilations")[0], int) or input_dict.get("dilations")[0] == -1 or \
        not isinstance(input_dict.get("dilations")[1], int) or input_dict.get("dilations")[1] == -1:
        return True
    return False


def _check_l1_buffer_and_get_axis_mode(input_list, para_dict, fusion_para, switch_to_general_scheme):
    def _l1fusion_size_limit(l1_size):
        padding = para_dict.get("padding")
        strides = para_dict.get("strides")
        _, _, filter_h, filter_w =  filter_sizes
        stride_h, stride_w = strides
        l1fusion_l1_size = 0
        if (
                list(padding)[::2] != [0, 0]
                or [filter_h, filter_w] != [1, 1]
                or switch_to_general_scheme
        ):
            if stride_h > 1 or stride_w > 1:
                l1fusion_l1_size = l1_size
        return l1fusion_l1_size

    def _cal_default_tiling_size_static(cal_size, tiling_dict):
        flag_dict = {"static_flag": True, "default_tiling_flag": True}
        return cal_size.cal_l1_size(flag_dict, tiling_dict)

    def _cal_tilingspace_size_static(cal_size, tiling_dict):
        flag_dict = {"static_flag": True}
        return cal_size.cal_l1_size(flag_dict, tiling_dict)

    def _cal_conv2d_split_w_size_static(cal_size, tiling_dict):
        flag_dict = {"static_flag": True, "conv2d_split_w_flag": True}
        return cal_size.cal_l1_size(flag_dict, tiling_dict)

    def _cal_min_load_size_dynamic(cal_size, tiling_dict):
        flag_dict = {"dynamic_flag": True, "default_tiling_flag": True}
        return cal_size.cal_l1_size(flag_dict, tiling_dict)

    # inputs
    filters, out_backprop, filter_sizes, input_sizes = input_list
    input_dict = copy.deepcopy(para_dict)
    input_dict["filter_shape_nchw"] = list(filter_sizes)
    input_dict["dy_shape_nc1hwc0"] = cube_util.shape_to_list(out_backprop.shape)
    input_dict["dx_shape_nchw"] = list(input_sizes)
    input_dict["dy_dtype"] = out_backprop.dtype
    input_dict["filter_dtype"] = filters.dtype
    if input_dict.get("bias_tensor") is not None:
        input_dict["bias_dtype"] = input_dict.get("bias_tensor").dtype
    # dynamic, when shape dim is -1, repalce by it's range[1]
    if "batch_n" in DeconvParam.var_map:
        batch_n_bound = get_te_var("batch_n").get_bound()
        input_dict["dy_shape_nc1hwc0"][0] = batch_n_bound[1]
    if "dedy_h" in DeconvParam.var_map:
        dedy_h_bound = get_te_var("dedy_h").get_bound()
        dx_h_bound = get_te_var("dx_h").get_bound()
        input_dict["dy_shape_nc1hwc0"][2] = dedy_h_bound[1]
        input_dict["dx_shape_nchw"][2] = dx_h_bound[1]
    if "dedy_w" in DeconvParam.var_map:
        dedy_w_bound = get_te_var("dedy_w").get_bound()
        dx_w_bound = get_te_var("dx_w").get_bound()
        input_dict["dy_shape_nc1hwc0"][3] = dedy_w_bound[1]
        input_dict["dx_shape_nchw"][3] = dx_w_bound[1]
    # avgpoolgrad/deconvlution/depthwise_2ddx only suopport dynamic, it may have None range
    # this situation, we cann't calculate l1_size
    if _is_dynamic_with_nonerange(input_dict):
        return -1
    m0, k_block_size, n0 = get_cube_mkn(filters.dtype)
    k_al1 = 2 if filters.dtype == "float32" else 1
    k_bl1 = k_al1
    tiling_dict = {"m_al1": 1, "m": 1, "m0": m0, "n_bl1": 1, "n": 1, "n0": n0, "k_al1": k_al1,
                   "k_bl1": k_bl1, "k_block_size": k_block_size, "db_al1": 1, "db_bl1": 1, "db_bias_l1": 1}
    cal_size = CalL1Size(input_dict)

    l1_size = platform_info.get_soc_spec("L1_SIZE")
    inputs = (out_backprop, filter_sizes)
    attrs = (input_sizes, para_dict.get("strides"), para_dict.get(
        "padding"), para_dict.get("dilations"), para_dict.get("output_padding"))
    if not DeconvParam.var_map:
        # default tiling size
        default_tiling_al1_size, default_tiling_bl1_size, default_tiling_bias_l1_size = \
                                                        _cal_default_tiling_size_static(cal_size, tiling_dict)
        default_tiling_l1_size = default_tiling_al1_size + \
            default_tiling_bl1_size + default_tiling_bias_l1_size
        # min tilingspace size
        tilingspace_al1_size, tilingspace_bl1_size, tilingspace_bias_l1_size = \
                                                    _cal_tilingspace_size_static(cal_size, tiling_dict)
        tilingspace_l1_size = tilingspace_al1_size + \
            tilingspace_bl1_size + tilingspace_bias_l1_size
        # split_w size
        split_w_al1_size, split_w_bl1_size, split_w_bias_l1_size = _cal_conv2d_split_w_size_static(
            cal_size, tiling_dict)
        split_w_l1_size = split_w_al1_size + split_w_bl1_size + split_w_bias_l1_size

        DeconvParam.al1_size = default_tiling_al1_size
        DeconvParam.bl1_size = default_tiling_bl1_size
        if fusion_para.get("l1_fusion_type") != -1:
            default_tiling_al1_size = _l1fusion_size_limit(default_tiling_al1_size)
            split_w_al1_size = _l1fusion_size_limit(split_w_al1_size)

        if default_tiling_l1_size > l1_size and split_w_l1_size > l1_size:
            error_manager_cube.raise_err_specific("Conv2dBackpropInputD", \
                "Current load size is {}, L1 size is {}".format(split_w_l1_size, l1_size))
        dy_w_stride = out_backprop.shape[3] * para_dict.get("strides")[1]
        dx_w = input_sizes[3]
        if ((max(dy_w_stride, dx_w) > DX_W_MAX or tilingspace_l1_size > l1_size)
                and not _is_conv1d_situation(inputs, attrs) and split_w_l1_size <= l1_size):
            DeconvParam.al1_size = split_w_al1_size
            return SplitAxisMode.split_w.value
        return SplitAxisMode.split_hw.value
    else:
        al1_size, bl1_size, bias_l1_size = _cal_min_load_size_dynamic(cal_size, tiling_dict)
        min_load_size = al1_size + bl1_size + bias_l1_size
        if min_load_size > l1_size:
            error_manager_cube.raise_err_specific("Conv2dBackpropInput", \
                "Current load size is {}, L1 size is {}".format(min_load_size, l1_size))



def _get_var_map(out_backprop, input_sizes):
    """
    Get the value of DeconvParam.var_map

    Parameters
    ----------
    out_backprop : 5D dE/dY tensor
    input_sizes : shape of dE/dX, [N, C, H, W]

    Returns
    ----------
    var_map: a dict.
    """
    var_map = {}
    if not isinstance(out_backprop.shape[0], tvm.tir.expr.IntImm):
        var_map["batch_n"] = out_backprop.shape[0]
    if not isinstance(out_backprop.shape[2], tvm.tir.expr.IntImm):
        var_map["dedy_h"] = out_backprop.shape[2]
        var_map["dx_h"] = input_sizes[2]
    if not isinstance(out_backprop.shape[3], tvm.tir.expr.IntImm):
        var_map["dedy_w"] = out_backprop.shape[3]
        var_map["dx_w"] = input_sizes[3]
    return var_map


def _get_dx_shape(input_sizes, filters, binary_mode, res_dtype):
    """
    Configure the shape of dx in different scenarios
    """
    dx_batch, dx_c, dx_h, dx_w = input_sizes
    _, dx_k0, dx_n0 = get_cube_mkn(filters.dtype)
    shape_dx = {'dx_batch':dx_batch, 'dx_c1':int_ceil_div(dx_c, dx_n0), 'dx_h':dx_h, 'dx_w':dx_w, 'dx_c0':dx_n0}
    if binary_mode:
        shape_dx = {'dx_batch':dx_batch, 'dx_c1':get_te_var('dx_c1').get_tvm_var(),
                    'dx_h':dx_h, 'dx_w':dx_w, 'dx_c0':dx_n0
        }
    support_l0c2out = platform_info.intrinsic_check_support(SUPPORT_FIXPIPE_INTRINSIC)
    if support_l0c2out and (filters.dtype == "float32" or res_dtype in QUANT_DTYPES):
        shape_dx = {'dx_batch':dx_batch, 'dx_c1':int_ceil_div(dx_c, dx_k0), 'dx_h':dx_h, 'dx_w':dx_w, 'dx_c0':dx_k0}
    return shape_dx


@tbe_utils.para_check.check_input_type(Tensor, Tensor, (list, tuple), (list, tuple), dict)
def conv2d_backprop_input_compute(filters, out_backprop, filter_sizes, input_sizes, para_dict):
    """
    DSL interface of conv2d backprop input

    Parameters
    ----------
    filters : weight tensor of fractal shape

    out_backprop : 5D dE/dY tensor

    filter_sizes : shape of weight, [N, C, H, W]

    input_sizes : shape of dE/dX, [N, C, H, W]

    para_dict:

        strides : list of strides, [strideh, stridew]

        padding : list of padding, [pad_up, pad_down, pad_left, pad_right]

        dilations : list of dilations, [dilation_n, dilation_c, dilation_h, dilation_w]

        res_dtype : dE/dX data type, "float16" by default

        offset_x : offset of x

        offset_w : offset of w

        fusion_para: the l1 fuison para

        split_axis_mode: indicates whether to enable w-axis splitting (in binary scenarios),
                 the value can be 0 (disabled) or 1 (enabled).

        kernel_name : cce kernel name

        group_dict : The params of group convolution.

        alg: type of weight compress

    Returns
    ----------
    dx_ddr: dE/dX tensor
    """

    strides = list(para_dict.get("strides"))
    padding = para_dict.get("padding")
    dilations = para_dict.get("dilations")
    res_dtype = para_dict.get("res_dtype", "float16")
    tensor_bias = para_dict.get("tensor_bias")
    offset_x = para_dict.get("offset_x", 0)
    offset_w = para_dict.get("offset_w")
    fusion_para = para_dict.get("fusion_para")
    kernel_name = para_dict.get("kernel_name", "conv2d_backprop_input_cce")
    group_dict = para_dict.get("group_dict")
    dynamic_split_axis_mode = para_dict.get("split_axis_mode", SplitAxisMode.split_hw.value)
    pooling_mode = para_dict.get("pooling_mode")
    binary_mode = para_dict.get("binary_mode", False)
    output_padding = para_dict.get("output_padding", (0, 0, 0, 0))
    alg = para_dict.get("alg")
    compress_index = para_dict.get("compress_index")
    bias_ori_shape = para_dict.get("bias_ori_shape")

    DeconvParam.set_default()
    if fusion_para is None:
        fusion_para = {
            "input_memory_type": 0,
            "output_memory_type": 0,
            "l1_fusion_type": -1,
            "fmap_l1_addr_flag": False,
            "fmap_l1_valid_size": 0
        }

    if group_dict is None:
        group_dict = {
            cube_util.GroupDictKeys.g_extend: 1,
            cube_util.GroupDictKeys.multiple_extend: 1,
            cube_util.GroupDictKeys.groups: 1,
            cube_util.GroupDictKeys.ci1g: int_ceil_div(input_sizes[1], platform_info.C0_SIZE),
            cube_util.GroupDictKeys.co1g: cube_util.shape_to_list(out_backprop.shape)[1],
            cube_util.GroupDictKeys.dx_c_ori: input_sizes[1],
            cube_util.GroupDictKeys.dy_c_ori: filter_sizes[0],
            cube_util.GroupDictKeys.filter_batch_ori: filter_sizes[0],
            cube_util.GroupDictKeys.filter_c_ori: filter_sizes[1],
            cube_util.GroupDictKeys.filter_ori_format: "NCHW"
        }

    # opti -> general:
    # 1) In quantified non-fusion scene, opti strategy maybe exceed UB size;
    # 2) In quantified fusion scene, convert to general strategy for increasing bias in l0c
    # 3) split_w

    def _is_switch_to_general_scheme(template_key):
        if (tensor_bias is not None and filters.dtype in QUANT_DTYPES
                and (strides[0] > 1 or strides[1] > 1)):
            return True
        support_fixpipe = platform_info.intrinsic_check_support(SUPPORT_FIXPIPE_INTRINSIC)
        if support_fixpipe:
            if isinstance(strides[0], tvm.Var):
                return True
            # (stride > 1 and has bias) or (in fixpipe fusion) can't use opti pattern
            # because fixpipe fusion flag will not effect in AOE, so it needs to judge by kernel_name
            if (((strides[0] > 1 or strides[1] > 1) and tensor_bias is not None) or _get_fixpipe_fusion_flag()
                    or "fix_pipe" in kernel_name or template_key in GENERAL_SCHEDULE_WHITE_LIST):
                return True
        return False

    def _check_l0a_dma_flag():
        """
        In these load3d unsupports scenes, dma_copy is used from ddr to l0a.
        """
        if DeconvParam.var_map:
            return False
        dilation_h, dilation_w = dilations[2:4]
        dy_h, dy_w = cube_util.shape_to_list(out_backprop.shape)[2:4]
        forward_padu, _, forward_padl, _ = padding
        backprop_padu = (filter_h - 1) * dilation_h - forward_padu
        backprop_padl = (filter_w - 1) * dilation_w - forward_padl
        backprop_padd = dx_h - dy_h * strides[0] + forward_padu
        backprop_padr = dx_w - dy_w * strides[1] + forward_padl
        filter_oversize = filter_h > FILTER_HW_LOAD3D_MAX or filter_w > FILTER_HW_LOAD3D_MAX
        dilation_oversize = dilation_h > DILATION_MAX or dilation_w > DILATION_MAX
        pad_oversize = backprop_padu > PAD_MAX or backprop_padl > PAD_MAX or backprop_padd > PAD_MAX or \
                       backprop_padr > PAD_MAX
        return filter_oversize or dilation_oversize or pad_oversize

    def _update_split_axis_mode(split_axis_mode):
        """
        exclude l0a_dma and binary scene for split_w
        """
        if DeconvParam.var_map:
            split_axis_mode = dynamic_split_axis_mode
        elif l0a_dma_flag:
            split_axis_mode = SplitAxisMode.split_hw.value
        return split_axis_mode

    def _check_opti_support(is_conv1d_situation):
        if not platform_info.intrinsic_check_support("Intrinsic_data_move_ub2l1"):
            return True
        _, _, dx_h, dx_w = input_sizes
        _, _, dy_h, dy_w, _ = cube_util.shape_to_list(out_backprop.shape)
        if "dedy_h" in DeconvParam.var_map:
            dedy_h_bound = get_te_var("dedy_h").get_bound()
            dx_h_bound = get_te_var("dx_h").get_bound()
            dy_h = dedy_h_bound[1]
            dx_h = dx_h_bound[1]
        if "dedy_w" in DeconvParam.var_map:
            dedy_w_bound = get_te_var("dedy_w").get_bound()
            dx_w_bound = get_te_var("dx_w").get_bound()
            dy_w = dedy_w_bound[1]
            dx_w = dx_w_bound[1]
        ub_size = platform_info.get_soc_spec("UB_SIZE")
        m0, _, n0 = get_cube_mkn(filters.dtype)
        if is_conv1d_situation:
            # mad calculate base on 1 * 16 * 1 * 16
            c_ub_size = 1 * m0 * 1 * n0 * cube_util.BIT_RATIO_DICT.get(res_dtype)
            # except stride still need pad 0
            output_pad_w = dx_w - ((dy_w - 1) * strides[1] + 1)
            # get really w axis
            w_extend = min(dx_w, (m0 - 1) * strides[1] + 1 + output_pad_w)
            # expend base on n1 * n0 * w_extend
            c_ub_dilation = 1 * n0 * w_extend * cube_util.BIT_RATIO_DICT.get(res_dtype)
        else:
            # align to 16
            dy_w_align = cube_util.align(dy_w, m0)
            # one dy_w size, dy_w_align is m * n1 * n0
            c_ub_size = dy_w_align * 1 * n0 * cube_util.BIT_RATIO_DICT.get(res_dtype)
            # except stride still need pad 0 (only h axis, w axis fullload)
            output_pad_h = dx_h - ((dy_h - 1) * strides[0] + 1)
            # when dy_h is not 16-aligned, need consider stride_h
            ho_need_load = int_ceil_div(dy_w_align, dy_w)
            # get really h axis
            h_extend = min(dx_h, (ho_need_load - 1) * strides[0] + 1 + output_pad_h)
            # after padding c_ub size
            c_ub_dilation = 1 * n0 * dx_w * h_extend * cube_util.BIT_RATIO_DICT.get(res_dtype)

        if ub_size < c_ub_size + c_ub_dilation:
            return False

        return True

    def _modify_strides_hw(shape_dx, shape_dy, strides):
        # when hi=1 and ho=1, need to modify strides_h=1, Similarly, wi=1 and wo=1, modify strides_w=1
        if shape_dx[2] == 1 and shape_dy[2] == 1:
            strides[0] = 1
        if shape_dx[3] == 1 and shape_dy[3] == 1:
            strides[1] = 1
        return strides

    DeconvParam.var_map = _get_var_map(out_backprop, input_sizes)

    shape_dx_dict = _get_dx_shape(input_sizes, filters, binary_mode, res_dtype)
    shape_dx = (shape_dx_dict.get('dx_batch'), shape_dx_dict.get('dx_c1'), shape_dx_dict.get('dx_h'),
                shape_dx_dict.get('dx_w'), shape_dx_dict.get('dx_c0'))
    shape_dy = cube_util.shape_to_list(out_backprop.shape)
    strides = _modify_strides_hw(shape_dx, shape_dy, strides)
    # modify origin strides in para_dict
    para_dict["strides"] = strides
    template_key = (tuple(filters.shape), tuple(out_backprop.shape),
                    tuple(filter_sizes), tuple(input_sizes),
                    tuple(para_dict.get("strides", (1, ))),
                    tuple(para_dict.get("dilations", (1, ))),
                    para_dict.get("group_dict", {}).get("groups"))
    switch_to_general_scheme = _is_switch_to_general_scheme(template_key)
    split_axis_mode = SplitAxisMode.split_hw.value
    is_conv1d_situation = False

    if platform_info.intrinsic_check_support("Intrinsic_conv_ub_to_ub"):
        pattc = DeConvUbPattern(
            filter_sizes,
            strides=strides,
            pad=padding,
            output_shape=shape_dx,
            output_dtype=res_dtype,
            dilations=dilations,
            offset_x=offset_x,
            fusion_para=fusion_para,
            kernel_name=kernel_name,
            group_dict=group_dict
        )
        dy_col = pattc.generate_a(out_backprop)
        w_col = pattc.generate_b(filters)
        dx_ddr = pattc.generate_c(dy_col, w_col, bias=tensor_bias)

        return dx_ddr

    if not binary_mode:
        _check_input_params(
            filters, out_backprop, filter_sizes, input_sizes, strides, group_dict, para_dict, fusion_para=fusion_para)
        input_list = [filters, out_backprop, filter_sizes, input_sizes]
        split_axis_mode = _check_l1_buffer_and_get_axis_mode(
            input_list, para_dict,
            fusion_para=fusion_para, switch_to_general_scheme=switch_to_general_scheme)

    if not DeconvParam.var_map:
        inputs = (out_backprop, filter_sizes)
        attrs = (input_sizes, strides, padding, dilations, output_padding)
        is_conv1d_situation = _is_conv1d_situation(inputs, attrs)
    _, _, filter_h, filter_w = filter_sizes
    _, _, dx_h, dx_w = input_sizes
    _, dx_k0, dx_n0 = get_cube_mkn(filters.dtype)
    g_extend = group_dict.get(cube_util.GroupDictKeys.g_extend)
    dy_c1_extend = group_dict.get(cube_util.GroupDictKeys.co1g)
    dx_c1_extend = group_dict.get(cube_util.GroupDictKeys.ci1g)
    dy_6gd_shape = [g_extend, shape_dy[0], dy_c1_extend] + shape_dy[2:]
    dx_6gd_shape = [g_extend, shape_dx[0], dx_c1_extend] + list(shape_dx)[2:]
    bias_flag = tensor_bias is not None
    # ------------------------------------------------------------------
    #    split_axis_mode     |    l0a_dma_flag      |     scenes
    # ------------------------------------------------------------------
    #    split_hw.value      |       False          |   general scene
    # ------------------------------------------------------------------
    #    split_w.value       |       False          |   split_w scene
    # ------------------------------------------------------------------
    #    split_w.value       |       True           | l0a_dma_copy scene
    # ------------------------------------------------------------------
    #    split_hw.value      |       True           | l0a_dma_copy scene
    # ------------------------------------------------------------------
    #    both split_w and dma_copy exceed l1 size    |   error scene
    # ------------------------------------------------------------------
    l0a_dma_flag = _check_l0a_dma_flag()
    split_axis_mode = _update_split_axis_mode(split_axis_mode)
    switch_to_general_scheme = switch_to_general_scheme or split_axis_mode == SplitAxisMode.split_w.value

    if split_axis_mode == SplitAxisMode.split_w.value:
        op_context.get_context().add_addition("split_w_flag", True)

    DynamicConv2dBpInputParams.ori_tensor = para_dict.get("ori_tensors")
    DynamicConv2dBpInputParams.tiling_info_dict = {
        "op_type": "conv2d_backprop_input",
        "A_shape": dy_6gd_shape[1:],
        "B_shape": [dy_c1_extend * dx_k0, dx_c1_extend, filter_h, filter_w, dx_n0],
        "C_shape": dx_6gd_shape[1:],
        "A_dtype": out_backprop.dtype,
        "B_dtype": filters.dtype,
        "C_dtype": res_dtype,
        "mad_dtype": "float32",
        "padl": padding[2],
        "padr": padding[3],
        "padu": padding[0],
        "padd": padding[1],
        "strideH": 1,
        "strideW": 1,
        "strideH_expand": strides[0],
        "strideW_expand": strides[1],
        "dilationH": dilations[2],
        "dilationW": dilations[3],
        "group": group_dict.get(cube_util.GroupDictKeys.g_extend),
        "bias_flag": bias_flag,
        "fused_double_operand_num": 0,
        "in_fm_memory_type": [],
        "out_fm_memory_type": [],
        "l1_fusion_type": -1,
        "fusion_type": 1,
        "kernel_name": kernel_name,
        "dynamic_shape_flag": True,
        "split_axis_mode": split_axis_mode
    }
    DynamicConv2dBpInputParams.binary_mode = binary_mode
    DynamicConv2dBpInputParams.var_map = DeconvParam.var_map
    DynamicConv2dBpInputParams.dynamic_para = {"correct_range_flag": para_dict.get("correct_range_flag", False),
                                               "op_type": para_dict.get("op_type", "")}
    if pooling_mode == "AVG":
        DynamicConv2dBpInputParams.tiling_info_dict["fused_coefficient"] = [3, 0, 0]

    control_flag_dict = {
        "pooling_mode": pooling_mode,
        "binary_mode": binary_mode,
        SPLIT_AXIS_MODE_STR: split_axis_mode,
        IS_CONV1D_SITUATION_STR: is_conv1d_situation,
        "alg": alg
    }

    if (filter_h == 1 and filter_w == 1 and cube_util.check_pad_zero(padding)
            and not switch_to_general_scheme and _check_opti_support(is_conv1d_situation)):
        pattc = DeConvKernelSize1Pattern(
            filter_sizes,
            strides=strides,
            pad=padding,
            output_shape=shape_dx,
            output_dtype=res_dtype,
            fusion_para=fusion_para,
            kernel_name=kernel_name,
            offset_x=offset_x,
            group_dict=group_dict,
            var_map=DeconvParam.var_map,
            control_flag_dict=control_flag_dict
        )
    else:
        # dynamic avg_pool_grad: dy_grad is dy_grad / mean_matrix
        if pooling_mode == "AVG":
            mean_matrix_shape = [shape_dy[2], shape_dy[3], shape_dy[4]]
            mean_matrix = tvm.compute(
                mean_matrix_shape,
                lambda h, w, c0:
                (tvm.max(
                    (tvm.min(h * strides[0] - padding[0] + filter_h, dx_h) - tvm.max(h * strides[0] - padding[0], 0)) *
                    (tvm.min(w * strides[1] - padding[2] + filter_w, dx_w) - tvm.max(w * strides[1] - padding[2], 0)),
                    1)).astype("int"),
                name="mean_matrix",
                tag="mean_matrix")
            mean_matrix_fp16 = tvm.compute(
                mean_matrix_shape,
                lambda *index: mean_matrix(*index).astype(out_backprop.dtype),
                name="mean_matrix_fp16",
                tag="mean_matrix_fp16")
            if  platform_info.get_soc_spec("SHORT_SOC_VERSION") in ("Ascend310",):
                mean_matrix_rec = tvm.compute(
                    mean_matrix_shape,
                    lambda *index: 1 / mean_matrix_fp16(*index),
                    name="mean_matrix_rec",
                    tag="mean_matrix_rec")
                dy_avg = tvm.compute(
                    shape_dy,
                    lambda n, c1, h, w, c0:
                    (out_backprop(n, c1, h, w, c0) * mean_matrix_rec(h, w, c0)).astype(out_backprop.dtype),
                    name="dy_avg",
                    tag="dy_avg")
            else:
                dy_avg = tvm.compute(
                    shape_dy,
                    lambda n, c1, h, w, c0:
                    tvm.div(out_backprop(n, c1, h, w, c0), mean_matrix_fp16(h, w, c0)).astype(out_backprop.dtype),
                    name="dy_avg",
                    tag="dy_avg")
            out_backprop = dy_avg
        control_flag_dict["l0a_dma_flag"] = l0a_dma_flag
        pattc = DeConvPattern(
            filter_sizes,
            strides=strides,
            pad=padding,
            output_shape=shape_dx,
            output_dtype=res_dtype,
            dilations=dilations,
            offset_x=offset_x,
            fusion_para=fusion_para,
            kernel_name=kernel_name,
            group_dict=group_dict,
            var_map=DeconvParam.var_map,
            control_flag_dict=control_flag_dict
        )

    dy_col = pattc.generate_a(out_backprop)
    w_col = pattc.generate_b(filters, compress_index)
    dx_ddr = pattc.generate_c(dy_col, w_col, tensor_bias=tensor_bias, bias_ori_shape=bias_ori_shape)

    return dx_ddr


class DynamicConv2dBpInputParams:
    """
    Dynamic Conv2dBpInput Params
    """


    tiling_info_dict = {}
    var_map = {}
    dynamic_para = {}
    ori_tensor = {}

    def __init__(self):
        pass
