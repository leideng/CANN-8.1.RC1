#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
cube util.
"""
import math
from collections import namedtuple
from enum import IntEnum
from typing import Any
from typing import Dict
from typing import List
from typing import Union

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils.const import SPLIT_AXIS_MODE_STR
from tbe.common.utils.const import SplitAxisMode
from tbe.common.utils.errormgr import error_manager_cube
from tbe.common.utils.errormgr import error_manager_util
from tbe.dsl.compute.util import align
from tbe.dsl.compute.util import int_ceil_div

# broadcast should be 16
BRC_STANDARD_BLOCK_SIZE = 16

# the bytes length of several dtype
BIT_RATIO_DICT = {"uint1": 0.125,
                  "uint4": 0.5, "int4": 0.5,
                  "bool": 1, "int8": 1, "uint8": 1,
                  "bfloat16": 2, "float16": 2, "int16": 2, "uint16": 2,
                  "float32": 4, "int32": 4, "uint32": 4,
                  "int64": 8, "uint64": 8}


class Conv3DConsts:
    # the dim of shape in conv_backprop must be 5
    _CONV_BACKPROP_SHAPE_DIM = 5
    # the dim of pads in conv3d_backprop must be 6
    _CONV_BACKPROP_PAD_SHAPE_DIM = 6
    # the dim of strides in conv_backprop must be 5
    _STRIDES_SHAPE_DIM = 5
    # the dim of dilations in conv_backprop must be 5
    _DILATIONS_SHAPE_DIM = 5


class GroupDictKeys:
    """
    The keys of group_dict
    """
    groups = "groups"
    g_extend = "g_extend"
    multiple_extend = "multiple_extend"
    ci1g = "dx_c1_extend"
    co1g = "dy_c1_extend"
    dx_c_ori = "dx_c_ori"
    dy_c_ori = "dy_c_ori"
    filter_batch_ori = "filter_batch_ori"
    filter_c_ori = "filter_c_ori"
    filter_ori_format = "filter_ori_format"


def get_value(shape_dim):
    """
    get tvm shape value
    """
    return shape_dim.value if isinstance(shape_dim, tvm.expr.ConstExpr) else shape_dim


def shape_to_list(shape):
    """
    translate tvm.shape to list type in python
    """
    tmp = [0 for i in range(len(shape))]
    j = 0
    for i in shape:
        if isinstance(i, tvm.tir.IntImm):
            tmp[j] = i.value
        else:
            tmp[j] = i
        j += 1
    return tmp


def convert_tvm_map_to_dict(map_tvm):
    """
    translate tvm.ir.container.Map to dict type in python
    """
    return {k: v.value if hasattr(v, 'value') else v for k, v in map_tvm.items()}


def check_pad_zero(pads):
    """
    check if pad is [0, x, 0, x]
    """
    for pad in pads[::2]:
        if isinstance(pad, (int, tvm.tir.IntImm)) and pad != 0:
            return False
    return True


def ceil_div(x_1, x_2):
    """
    ceil div
    """
    if x_2 == 0:
        dict_args = {
            'errCode': 'E62502',
            'first_operand': str(x_1),
            'second_operand': str(x_2),
        }
        error_manager_util.raise_runtime_error(dict_args)
    return (x_1 + x_2 - 1) // x_2


def raise_cube_util_err(msg):
    """
    In common component: cube_util, [%s] % (msg)
    msg for discribe the error info
    the error info only for cube_util's developers
    """
    args_dict = {"errCode": "E60108", "reason": msg}
    msg = error_manager_util.get_error_message(args_dict)
    raise RuntimeError(args_dict, msg)


def im2col_row_major(
        a_im2col_vm_shape,
        tensor_a,
        kernel_w,
        padding,
        stride,
        compute_dtype,
        opti_h_flag=False,
        tag="",
        dilation=(1, 1),
        offset_x=0,
        slice_offset=0,
        l0a_dma_flag=False,
        load3d_special_multiply=1):
    """
    calculate im2col_row_major tensor
    Parameters
    ----------
    a_im2col_vm_shape : shape of a_im2col_row_major

    tensor_a : feature map

    kernel_w: width of filter

    padding: the padding shape

    stride: the stride value

    dilation: the dilation value

    compute_dtype: dtype of compute result

    offset_x: offset of x
    -------
    Returns : a_im2col_row_major tensor
    """
    def __im2col_row_major_indices(
            indices,
            tensor_a,
            kernel_w,
            padding,
            stride,
            dilation,
            slice_offset=0):
        """
        calculate im2col_row_major tvm lambda function
        Parameters
        ----------
        indices : indices in lambda function

        tensor_a : feature map

        kernel_w: width of filter

        padding: the padding shape

        stride: the stride value

        dilation: the dilation value
        -------
        Returns : im2col_row_major tensor
        """
        _, a_co1, a_height, a_width, _ = tensor_a.shape
        n_index, *hw_index, c1_index, kh_index, kw_index, c0_index = indices
        stride_h, stride_w = stride
        if opti_h_flag:
            stride_h = 1
        dilate_h, dilate_w = dilation
        padding_up, _, padding_left, padding_right = padding
        width_out = (a_width.value + padding_left + padding_right -
                     ((kernel_w - 1) * dilate_w + 1)) // (stride_w) + 1
        if len(hw_index) == 1:
            h_indice = hw_index[0] // width_out
            w_indice = hw_index[0] % width_out
        else:
            h_indice, w_indice = hw_index
        h_index = h_indice * stride_h + kh_index * dilate_h
        w_index = w_indice * stride_w + kw_index * dilate_w
        if not l0a_dma_flag:
            return tvm.select(tvm.any(h_index < padding_up,
                                      h_index > a_height.value + padding_up - 1,
                                      w_index < padding_left,
                                      w_index > a_width.value + padding_left - 1,
                                      c1_index > a_co1.value - 1),
                              tvm.const(offset_x, compute_dtype),
                              tensor_a(n_index, c1_index, h_index - padding_up + slice_offset,
                                       w_index - padding_left, c0_index))
        return tensor_a(n_index, c1_index, h_index + slice_offset, w_index, c0_index)

    return tvm.compute(a_im2col_vm_shape,
                       lambda *indices: __im2col_row_major_indices(indices, tensor_a, kernel_w, padding,
                                                                   stride, dilation, slice_offset),
                       name="im2col_row_major",
                       tag=tag + "im2col_row_major",
                       attrs={
                           "padding": padding,
                           "dilation": dilation,
                           "l0a_dma_flag": l0a_dma_flag,
                           "load3d_special_multiply": load3d_special_multiply
                       })


def im2col_fractal(a_im2col_shape, tensor_a_row_major, l0a_dma_flag=False, split_axis_mode=0):
    """
    calculate im2col_fractal tensor
    Parameters
    ----------
    a_im2col_shape : shape of a_im2col

    tensor_a_row_major : feature map after row major

    config: the config of cube

    compute_dtype: dtype of compute result
    -------
    Returns : a_im2col_fractal tensor
    """
    def __im2col_fractal_indices_split_w(indices, tensor_a_row_major):
        """
        calculate im2col_fractal tvm lambda function in split w scene

        Parameters
        ----------
        indices : indices in lambda function
        tensor_a_row_major : feature map matrix

        Returns
        -------
        im2col_fractal tvm lambda function
        """
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            *_, a_col_k1, _, a_col_m0, a_col_k0 = a_im2col_shape
            g_index, n_index, h_index, k1_index, w1_index, m0_index, k0_index = indices
        else:
            *_,  a_col_k1, a_col_m0, a_col_k0 = a_im2col_shape
            g_index, n_index, h_index, w1_index, k1_index, m0_index, k0_index = indices
        _, a_row_major_h, a_row_major_w, _, kernel_h, kernel_w, _ = shape_to_list(tensor_a_row_major.shape)

        w_index = w1_index * a_col_m0 + m0_index
        k_axis_index = (g_index * a_col_k1 + k1_index) * a_col_k0 + k0_index
        c1_index = ((k_axis_index // a_col_k0) // kernel_w) // kernel_h
        kh_index = ((k_axis_index // a_col_k0) // kernel_w) % kernel_h
        kw_index = (k_axis_index // a_col_k0) % kernel_w
        c0_index = k_axis_index % a_col_k0
        return tvm.select(
            tvm.any(w1_index * a_col_m0 < 0, w_index > a_row_major_w - 1,
                    h_index < 0, h_index > a_row_major_h - 1),
            tvm.const(0.0, tensor_a_row_major.dtype),
            tensor_a_row_major(n_index, h_index, w_index, c1_index, kh_index, kw_index, c0_index)
        )

    def __im2col_fractal_indices(indices, tensor_a_row_major):
        """
        calculate im2col_fractal tvm lambda function
        Parameters
        ----------
        indices : indices in lambda function

        a : feature map

        -------
        Returns : im2col_fractal tvm lambda function
        """
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            *_, a_col_k1, _, a_col_m0, a_col_k0 = a_im2col_shape
            g_index, n_index, k1_index, m1_index, m0_index, k0_index = indices
        else:
            *_, a_col_k1, a_col_m0, a_col_k0 = a_im2col_shape
            g_index, n_index, m1_index, k1_index, m0_index, k0_index = indices
        _, a_row_major_hw, _, kernel_h, kernel_w, _ = shape_to_list(tensor_a_row_major.shape)

        hw_index = m1_index * a_col_m0 + m0_index
        k_axis_index = (g_index * a_col_k1 + k1_index) * a_col_k0 + k0_index

        c1_index = ((k_axis_index // a_col_k0) // kernel_w) // kernel_h

        kh_index = ((k_axis_index // a_col_k0) // kernel_w) % kernel_h

        kw_index = (k_axis_index // a_col_k0) % kernel_w

        c0_index = k_axis_index % a_col_k0

        # dtype is compute_dtype
        if not l0a_dma_flag:
            return tvm.select(
                tvm.any(hw_index < 0, hw_index > a_row_major_hw - 1),
                tvm.const(0.0, tensor_a_row_major.dtype),
                tensor_a_row_major(n_index, hw_index, c1_index, kh_index, kw_index, c0_index))
        return tvm.select(
            tvm.all(hw_index >= 0, hw_index < a_row_major_hw),
            tensor_a_row_major(n_index, hw_index, c1_index, kh_index, kw_index, c0_index))
    if split_axis_mode == SplitAxisMode.split_w.value:
        func_name = __im2col_fractal_indices_split_w
        _, _, _, _, kernel_h, kernel_w, _ = shape_to_list(tensor_a_row_major.shape)
    else:
        func_name = __im2col_fractal_indices
        _, _, _, kernel_h, kernel_w, _ = shape_to_list(tensor_a_row_major.shape)
    return tvm.compute(a_im2col_shape,
                       lambda *indices: func_name(indices, tensor_a_row_major),
                       name='im2col_fractal',
                       tag='im2col_fractal',
                       attrs={
                           SPLIT_AXIS_MODE_STR: split_axis_mode,
                           "kernel_h": kernel_h,
                           "kernel_w": kernel_w
                       })


def im2col_fractal_3d(
        a_im2col_shape,
        tensor_a_row_major,
        fmap_c1,
        d_out,
        filter_d,
        stride_d,
        cin1_g,
        cyclebuffer_flag,
        tag=""):
    """
    calculate 3d im2col_fractal tensor
    Parameters
    ----------
    a_im2col_shape : shape of a_im2col

    tensor_a_row_major : feature map after row major

    fmap_c1 : channel c1

    d_out : output d

    filter_d : kernel_d

    strided : stride d

    cyclebuffer_flag : whether to do  cyclebuffer
    -------
    Returns : a_im2col_fractal tensor
    """
    def __im2col_fractal_indices(indices, tensor_a_row_major):
        """
        calculate im2col_fractal tvm lambda function
        Parameters
        ----------
        indices : indices in lambda function

        a : feature map

        -------
        Returns : im2col_fractal tvm lambda function
        """
        _, _, _, _, a_col_m0, a_col_k0 = a_im2col_shape
        _, a_row_major_hw, _, kernel_h, kernel_w, _ = tensor_a_row_major.shape

        g_index, n_index, m1_index, k1_index, m0_index, k0_index = indices
        hw_index = m1_index * a_col_m0 + m0_index

        # ====================== new ====================
        kdc1_index = k1_index // (kernel_h.value * kernel_w.value)
        kd_index = (kdc1_index // cin1_g + (n_index % d_out * stride_d) * cyclebuffer_flag) % filter_d
        cin_index = kdc1_index % cin1_g
        c1_index = kd_index * fmap_c1 + g_index * cin1_g + cin_index

        kh_index = (((k1_index * a_col_k0 + k0_index) // a_col_k0) // kernel_w.value) % kernel_h.value

        kw_index = ((k1_index * a_col_k0 + k0_index) // a_col_k0) % kernel_w.value

        c0_index = (k1_index * a_col_k0 + k0_index) % a_col_k0

        # dtype is compute_dtype
        return tvm.select(tvm.any(hw_index < 0, hw_index > a_row_major_hw.value - 1),
                          tvm.const(0.0, tensor_a_row_major.dtype),
                          tensor_a_row_major(n_index, hw_index, c1_index, kh_index, kw_index, c0_index))

    return tvm.compute(a_im2col_shape,
                       lambda *indices: __im2col_fractal_indices(indices, tensor_a_row_major),
                       name="im2col_fractal",
                       tag=tag + "im2col_fractal")


def im2col_fractal_v2(shape, img2col_para, split_axis_mode=0):
    """
    calculate im2col_fractal tensor without tensor row_major
    Parameters
    ----------
    shape : shape of a_im2col

    img2col_para : tensor of fmap, kernel_h, kernel_w, padding, stride,
    fmap_wo, dilation
    -------
    Returns : a_im2col_fractal tensor
    """

    block_size = 16
    fmap, kernel_h, kernel_w, padding, stride, fmap_wo, dilation, c1_extend = img2col_para
    dilation_h, dilation_w = dilation

    def __im2col_idx(idx):
        if split_axis_mode == SplitAxisMode.split_hw.value:
            group, batch, col_h, col_w, block_size_h, block_size_w = idx
        else:  # split_axis_mode == SplitAxisMode.split_w.value
            group, batch, ho, col_h, col_w, block_size_h, block_size_w = idx

        # when w-axis splitting is enabled, virtual_h contains only the w-axis
        virtual_h = col_h * block_size + block_size_h
        virtual_w = col_w * block_size + block_size_w

        back_c1 = virtual_w // block_size // kernel_w // kernel_h
        if split_axis_mode == SplitAxisMode.split_hw.value:
            back_h = (virtual_h // fmap_wo) * stride[0] + (col_w // kernel_w % kernel_h) * dilation_h
            back_w = (virtual_h % fmap_wo) * stride[1] + (col_w % kernel_w) * dilation_w
        else:
            back_h = ho * stride[0] + (col_w // kernel_w % kernel_h) * dilation_h
            back_w = virtual_h * stride[1] + (col_w % kernel_w) * dilation_w

        if len(fmap.shape) == 5:
            fmap_element = fmap(batch, back_c1 + group * c1_extend, back_h - padding[0], back_w - padding[2],
                                block_size_w)
        else:
            fmap_element = fmap(group, batch, back_c1, back_h - padding[0], back_w - padding[2], block_size_w)

        return tvm.select(
            tvm.any(back_h < padding[0],
                    back_h > fmap.shape[2] + padding[0] - 1,
                    back_w < padding[2],
                    back_w > fmap.shape[3] + padding[2] - 1),
                tvm.const(0, fmap.dtype),
                fmap_element)

    return tvm.compute(shape,
                       lambda *idx: __im2col_idx(idx),
                       name="img2col_fractal_v2",
                       tag="im2col_fractal_v2",
                       attrs={
                           "fmap_shape": fmap.shape,
                           "kernel_h": kernel_h,
                           "kernel_w": kernel_w,
                           "padding": padding,
                           "stride": stride,
                           "dilation": dilation,
                           SPLIT_AXIS_MODE_STR: split_axis_mode
                       })


def _get_attr_from_tensor(tensor, attr_name):
    if tensor.op.attrs is None:
        return None
    if attr_name in tensor.op.attrs:
        return tensor.op.attrs[attr_name].value
    return None


class CubeDslPattern:
    """
    class of cube mmad calculation

    Parameters
    ----------
    None

    Returns
    -------
    cube_pattern_instance : instance of cube mmad pattern
    """

    type_c_map = {}

    def __init__(self, ci1g=1):
        self._tensor_c = None
        self._cin1_g = ci1g

    @staticmethod
    def get_type_c(type_a, type_b, type_bias=None):
        """
        get the data type of mad result tensor

        Parameters
        ----------
        type_a : data type of tensor a

        type_b : data type of tensor b

        type_bias : data type of bias

        Returns
        ----------
        type_c : data type of tensor c
        """
        def cal_hash(tp_a, tp_b, tp_bias):
            return hash(str(tp_a) + str(tp_b) + str(tp_bias))

        if CubeDslPattern.type_c_map == {}:
            CubeDslPattern.type_c_map[cal_hash("uint8", "uint8", None)] = "int32"
            CubeDslPattern.type_c_map[cal_hash("int8", "int8", None)] = "int32"
            CubeDslPattern.type_c_map[cal_hash("uint4", "uint4", None)] = "int32"
            CubeDslPattern.type_c_map[cal_hash("int4", "int4", None)] = "int32"
            CubeDslPattern.type_c_map[cal_hash("float16", "float16", None)] = "float16"
            CubeDslPattern.type_c_map[cal_hash("float16", "float16", "float32")] = "float32"
            CubeDslPattern.type_c_map[cal_hash("float16", "float16", "float16")] = "float16"

        type_c_key = cal_hash(type_a, type_b, type_bias)
        type_c = CubeDslPattern.type_c_map.get(type_c_key)

        return type_c

    @staticmethod
    def _mad_expression_split_w(tensor_a, tensor_b, param_dict, tensor_bias=None):
        _, _, b_n1, b_n0, b_k0 = shape_to_list(tensor_b.shape)
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            a_group, a_batch, h_o, a_k1, w_o1, a_m0, a_k0 = shape_to_list(tensor_a.shape)
        else:
            a_group, a_batch, h_o, w_o1, a_k1, a_m0, a_k0 = shape_to_list(tensor_a.shape)
        shape_c = (a_group, a_batch, h_o, b_n1, w_o1 * a_m0, b_n0)
        axis_k0 = tvm.reduce_axis([0, a_k0], name='axis_k0')
        axis_k1 = tvm.reduce_axis([0, a_k1], name='axis_k1')
        type_c = param_dict.get("type_c")
        offset_x = param_dict.get("offset_x")
        bias_table_flag = param_dict.get("bias_table_flag")
        _cin1_g = param_dict.get("cin1_g")
        mad_func = tvm.mad_sp if param_dict.get("sparse_4to2_flag") else tvm.sum
        co1_sparse_factor = 2 if param_dict.get("sparse_4to2_flag") else 1 # 2: cout_sparse is 1/2 of cout_origin

        def __mad_idx(idx):
            g_index, n_index, ho_index, co1_index, m_index, co0_index = idx
            if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                elem_tensor_a = tensor_a(g_index, n_index, ho_index, axis_k1, m_index // a_m0, m_index % a_m0, axis_k0)
            else:
                elem_tensor_a = tensor_a(g_index, n_index, ho_index, m_index // a_m0, axis_k1, m_index % a_m0, axis_k0)

            if bias_table_flag:
                if tensor_b.dtype == "float32":
                    return tvm.sum(
                        ((elem_tensor_a - offset_x) *
                        tensor_b(g_index, axis_k1, co1_index, co0_index, axis_k0)).astype(type_c) +
                        tensor_bias(g_index * _cin1_g * b_k0 + co1_index * b_n0 + co0_index),
                        axis=[axis_k1, axis_k0])
                else:
                    return mad_func(
                        ((elem_tensor_a - offset_x) *
                        tensor_b(g_index, axis_k1 // co1_sparse_factor, co1_index, co0_index, axis_k0)).astype(type_c) +
                        tensor_bias(g_index * b_n1 * b_n0 + co1_index * b_n0 + co0_index),
                        axis=[axis_k1, axis_k0])
            else:
                return tvm.sum(
                    ((elem_tensor_a - offset_x) *
                     tensor_b(g_index, axis_k1, co1_index, co0_index, axis_k0)).astype(type_c),
                    axis=[axis_k1, axis_k0])
        return tvm.compute(shape_c, lambda *idx: __mad_idx(idx), name="C", tag="mad")

    @staticmethod
    def _mad_expression_default(tensor_a, tensor_b, param_dict, tensor_bias=None):
        _, _, b_n1, b_n0, b_k0 = shape_to_list(tensor_b.shape)
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            a_group, a_batch, a_k1, a_m1, a_m0, a_k0 = shape_to_list(tensor_a.shape)
        else:
            a_group, a_batch, a_m1, a_k1, a_m0, a_k0 = shape_to_list(tensor_a.shape)
        shape_c = (a_group, a_batch, b_n1, a_m1 * a_m0, b_n0)
        axis_k0 = tvm.reduce_axis([0, a_k0], name='axis_k0')
        axis_k1 = tvm.reduce_axis([0, a_k1], name='axis_k1')
        type_c = param_dict.get("type_c")
        offset_x = param_dict.get("offset_x")
        bias_table_flag = param_dict.get("bias_table_flag")
        _cin1_g = param_dict.get("cin1_g")
        mad_func = tvm.mad_sp if param_dict.get("sparse_4to2_flag") else tvm.sum
        co1_sparse_factor = 2 if param_dict.get("sparse_4to2_flag") else 1 # 2: cout_sparse is 1/2 of cout_origin

        def __mad_idx(idx):
            g_index, n_index, co1_index, m_index, co0_index = idx
            if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                elem_tensor_a = tensor_a(g_index, n_index, axis_k1, m_index // a_m0, m_index % a_m0, axis_k0)
            else:
                elem_tensor_a = tensor_a(g_index, n_index, m_index // a_m0, axis_k1, m_index % a_m0, axis_k0)

            if bias_table_flag:
                # for tensor_bias, _cin1_g * b_k0 means the value of cin in one group
                # it has same meaning in _mad_expression_split_w function
                if tensor_b.dtype == "float32":
                    return tvm.sum(
                        ((elem_tensor_a - offset_x) *
                        tensor_b(g_index, axis_k1, co1_index, co0_index, axis_k0)).astype(type_c) +
                        tensor_bias(g_index * _cin1_g * b_k0 + co1_index * b_n0 + co0_index),
                        axis=[axis_k1, axis_k0])
                else:
                    return mad_func(
                        ((elem_tensor_a - offset_x) *
                        tensor_b(g_index, axis_k1 // co1_sparse_factor, co1_index, co0_index, axis_k0)).astype(type_c) +
                        tensor_bias(g_index * b_n1 * b_n0 + co1_index * b_n0 + co0_index),
                        axis=[axis_k1, axis_k0])
            else:
                return tvm.sum(
                    ((elem_tensor_a - offset_x) *
                     tensor_b(g_index, axis_k1, co1_index, co0_index, axis_k0)).astype(type_c),
                    axis=[axis_k1, axis_k0])

        return tvm.compute(shape_c, lambda *idx: __mad_idx(idx), name="C", tag="mad")

    def generate_c(
            self, tensor_a, tensor_b, mad_param, tensor_bias=None):
        """
        calculate the mad result tensor

        Parameters
        ----------
        tensor_a : tensor a

        tensor_b : tensor b

        mad_param: param dict
            c_type : data type of c
            offset_x : offset_x of a
            cin1_g: cin1_g
            split_axis_mode: split_axis_mode
            bias_table_flag: use bias table
            sparse_4to2_flag: sparse_4to2_flag

        tensor_bias : bias tensor

        Returns
        ----------
        tensor_c : mad result tensor
        """
        split_axis_mode = mad_param.get("split_axis_mode")
        if mad_param.get("type_c") is None:
            mad_param["type_c"] =  CubeDslPattern.get_type_c(tensor_a.dtype, tensor_b.dtype)
        if not is_support_v200():
            mad_param["offset_x"] = 0

        tensor_c = self._get_mad_tensor(tensor_a, tensor_b, mad_param, tensor_bias)
        if tensor_bias is None or mad_param.get("bias_table_flag"):
            return tensor_c

        shape_c = shape_to_list(tensor_c.shape)
        bias_ub_brc_shape = list(shape_c)
        bias_ub_brc_shape[-2] = bias_ub_brc_shape[-2] // BRC_STANDARD_BLOCK_SIZE
        co_k = tbe_platform.CUBE_MKN.get(tensor_bias.dtype).get("mac")[2]
        bias_ub_brc = tvm.compute(
            bias_ub_brc_shape,
            lambda *indices: tensor_bias(indices[0] * shape_c[-3] * co_k + indices[-3] * co_k + indices[-1]),
            name="bias_ub_brc")
        if split_axis_mode == SplitAxisMode.split_w.value:
            bias_l0c = tvm.compute(
                shape_c,
                lambda g_index, n_index, h_index, c1_index, w_index, c0_index:
                    bias_ub_brc(g_index, n_index, h_index, c1_index, w_index // BRC_STANDARD_BLOCK_SIZE, c0_index),
                name="bias_l0c"
            )
        else:
            bias_l0c = tvm.compute(
                shape_c,
                lambda g_index, n_index, c1_index, hw_index, c0_index:
                    bias_ub_brc(g_index, n_index, c1_index, hw_index // BRC_STANDARD_BLOCK_SIZE, c0_index),
                name="bias_l0c"
            )
        tensor_c = tvm.compute(
            shape_c,
            lambda *indices: bias_l0c(*indices) + tensor_c(*indices),
            name="c_add_bias"
        )
        self._tensor_c = tensor_c
        return tensor_c

    def _get_mad_tensor(self, tensor_a, tensor_b, param_dict, tensor_bias=None):
        split_axis_mode = param_dict.get(SPLIT_AXIS_MODE_STR)
        if split_axis_mode == SplitAxisMode.split_w.value:
            mad_func = self._mad_expression_split_w
        else:
            mad_func = self._mad_expression_default
        return mad_func(tensor_a, tensor_b, param_dict, tensor_bias)


class ConvDslPattern(CubeDslPattern):
    """
    class of convolution

    Parameters
    ----------
    kernel_h: height of filter

    kernel_w: width of filter

    stride : list of strides, [strideh, stridew]

    pad: list of padding, [pad_up, pad_down, pad_left, pad_right]

    Returns
    -------
    conv_pattern_instance : instance of conv pattern
    """
    def __init__(self, kernel_hw, stride, pad, dilations, param_dict):
        super().__init__()
        self._kernel_h, self._kernel_w = kernel_hw
        self._stride_h, self._stride_w = stride
        self._pad_up, self._pad_down, self._pad_left, self._pad_right = pad
        self._dilate_h, self._dilate_w = dilations
        self._m0 = 16
        self._offset_x = param_dict.get("offset_x", 0)
        self.l0a_dma_flag = param_dict.get("l0a_dma_flag", False)
        self.load3d_special_multiply = param_dict.get("load3d_special_multiply", 1)

    @staticmethod
    def _is_load3d_special(var_map, h_out, w_out):
        if (tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION") not in ("Hi3796CV300CS", "Ascend310")
            and not tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
            and not var_map
            and int(h_out) != 1
            and int(w_out) == 1):
            return True
        return False

    def cal_howo(self, height_in, width_in):
        """
        calculate the height and width of convolution output tensor

        Parameters
        ----------
        height_in : height of input tensor

        width_in : width of input tensor

        Returns
        ----------
        height_out : height of output tensor

        width_out : width of output tensor
        """
        new_hw = [height_in, width_in]
        kernel_h, kernel_w = self._kernel_h, self._kernel_w
        new_pad_before = (self._pad_up, self._pad_left)
        new_pad_after = (self._pad_down, self._pad_right)
        stride = [self._stride_h, self._stride_w]

        height_out, width_out = list(((i + p_before + p_after - (kernel - 1) * d - 1) // s + 1)
                                     for i, p_before, p_after, kernel, d, s in zip(
                                         new_hw, new_pad_before, new_pad_after, [kernel_h, kernel_w],
                                         [self._dilate_h, self._dilate_w], stride))

        return height_out, width_out

    def generate_a(
        self,
        feature_map,
        g_after,
        c1_extend,
        var_map=None,
        slice_offset=0,
        valid_shape=()):
        """
        calculate im2col_fractal tensor

        Parameters
        ----------
        feature_map : feature map tensor in the shape of NC1HWC0

        g_after : the group after optimization

        c1_extend : the C1 after group extension

        var_map : dict of vars for dynamic shape

        slice_offset : offset of fmap

        valid_shape: valid shape of fmap

        Returns
        -------
        a_col : a_im2col_fractal tensor
        """
        if not var_map:
            var_map = {}
        kernel_h, kernel_w = self._kernel_h, self._kernel_w
        stride = [self._stride_h, self._stride_w]
        a_batch, new_pad, height_out, width_out, a_c0 = self._cal_a_params(feature_map, var_map, valid_shape)
        split_axis_mode = _get_attr_from_tensor(feature_map, SPLIT_AXIS_MODE_STR)

        if not var_map:
            # for 2ddx, a_im2col_row_major_shape is the shape after completing the traditional img2col.
            # i.e. (n, howo, c1, hk, wk, c0)
            # more specifically, (dy_batch, dx_h * dx_w, cout_1, kernel_h, kernel_w, cout_0)
            a_im2col_row_major_shape = (a_batch, height_out * width_out, g_after * c1_extend, kernel_h, kernel_w, a_c0)
            if split_axis_mode == SplitAxisMode.split_w.value:
                a_im2col_row_major_shape = (a_batch, height_out, width_out, g_after * c1_extend,
                                            kernel_h, kernel_w, a_c0)
            # img2col_row_major function can be seen as an intermediate step in 3D Load
            a_row_major = im2col_row_major(a_im2col_row_major_shape,
                                           feature_map,
                                           kernel_w,
                                           padding=new_pad,
                                           stride=stride,
                                           compute_dtype=feature_map.dtype,
                                           dilation=(self._dilate_h, self._dilate_w),
                                           offset_x=self._offset_x,
                                           slice_offset=slice_offset,
                                           l0a_dma_flag=self.l0a_dma_flag,
                                           load3d_special_multiply=self.load3d_special_multiply)
        if split_axis_mode == SplitAxisMode.split_w.value:
            aligned_w = int_ceil_div(width_out, self._m0)
            if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                a_im2col_fractal_shape = (g_after, a_batch, height_out, c1_extend * kernel_h * kernel_w, aligned_w,
                                          self._m0, a_c0)
            else:
                a_im2col_fractal_shape = (g_after, a_batch, height_out, aligned_w, c1_extend * kernel_h * kernel_w,
                                          self._m0, a_c0)
        else:
            howo = align(height_out * width_out, self._m0)
            if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                a_im2col_fractal_shape = (
                    g_after, a_batch, c1_extend * kernel_h * kernel_w, howo // self._m0, self._m0, a_c0)
            else:
                a_im2col_fractal_shape = (
                    g_after, a_batch, howo // self._m0, c1_extend * kernel_h * kernel_w, self._m0, a_c0)

        if not var_map:
            a_col = im2col_fractal(a_im2col_fractal_shape, a_row_major, self.l0a_dma_flag, split_axis_mode)
        else:
            img2col_para = (feature_map, kernel_h, kernel_w, new_pad, stride, width_out,
                            (self._dilate_h, self._dilate_w), c1_extend)
            a_col = im2col_fractal_v2(a_im2col_fractal_shape, img2col_para, split_axis_mode)
        return a_col

    def generate_c(
            self, tensor_a, tensor_b, tensor_bias=None, c_type=None, offset_x=0):
        """
        calculate convolution output tensor

        Parameters
        ----------
        tensor_a : tensor a

        tensor_b : tensor b

        tensor_bias : bias tensor

        c_type : data type of c

        offset_x : offset_x of a

        Returns
        ----------
        tensor_c: convolution output tensor
        """
        tensor_c = super().generate_c(tensor_a, tensor_b, tensor_bias, c_type, offset_x)
        row_major = tensor_a.op.input_tensors[0]
        ho_wo = row_major.shape[1].value
        _, _, c_m, _ = shape_to_list(tensor_c.shape)
        m_0 = self._m0
        m_1 = c_m // m_0
        if not ((m_1 - 1) * m_0) < ho_wo <= c_m:
            raise_cube_util_err("HoWo param error!")
        return tensor_c

    def _cal_a_params(self, feature_map, var_map, valid_shape):
        """
        calculate params for a_matrix
        """
        a_batch, _, a_h, a_w, a_c0 = shape_to_list(feature_map.shape)[-5:]
        if valid_shape:
            a_batch, _, a_h, a_w, a_c0 = valid_shape
        new_pad = [self._pad_up, self._pad_down, self._pad_left, self._pad_right]

        if "dedy_h" in var_map:
            height_out = var_map.get("dx_h")
        else:
            height_out, _ = self.cal_howo(a_h, a_w)

        if "dedy_w" in var_map:
            width_out = var_map.get("dx_w")
        else:
            _, width_out = self.cal_howo(a_h, a_w)

        if self._is_load3d_special(var_map, height_out, width_out):
            self.load3d_special_multiply = 2
            width_out *= self.load3d_special_multiply
            self._pad_right += 1
            new_pad = [self._pad_up, self._pad_down, self._pad_left, self._pad_right]
        elif isinstance(self.load3d_special_multiply, tvm.Var):
            width_out *= self.load3d_special_multiply
            self._pad_right += self.load3d_special_multiply - 1
            new_pad = [self._pad_up, self._pad_down, self._pad_left, self._pad_right]

        return [a_batch, new_pad, height_out, width_out, a_c0]


def is_support_v200():
    """
    check if Ascend610/BS9SX1A/Ascend310P/Hi3796CV300CS version
    ----------

    Returns
    -------
    True:  Ascend610/BS9SX1A/Ascend310P/Hi3796CV300CS version
    False: Other version
    """
    soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in ("Ascend310P", "Ascend610", "BS9SX1A", "Hi3796CV300CS", "SD3403"):
        return True
    return False


def calc_info_of_iter_vars(stage):
    """
    Calcuate information of IterVar.

    Args: stage: Stage of schedule.

    Returns:
    A list of elements that are combinations of IterVar.var and information.
    For example:

    [[i0.inner, IterVar(min=0, extent=3, parent=Parent(var=i0, min=0, extent=6, factor=2, nparts=-1))],
    [i0.outer, IterVar(min=0, extent=2, parent=Parent(var=i0, min=0, extent=6, factor=2, nparts=-1))],
    [i1, (0, 16)]]
    """

    Parent = namedtuple("Parent", "var, min, extent, factor, nparts")
    IterVar = namedtuple("IterVar", "min, extent, parent")

    def calc_split_rel(rel, info_iter_var):
        val_min = (rel.parent.dom.min.value if rel.parent.dom is not None else info_iter_var[rel.parent.var][0])
        extent = (rel.parent.dom.extent.value if rel.parent.dom is not None else info_iter_var[rel.parent.var][1])
        factor = rel.factor.value if rel.factor is not None else -1
        nparts = rel.nparts.value if rel.nparts is not None else -1
        parent = Parent(rel.parent.var, val_min, extent, factor, nparts)
        if factor >= 0:
            return IterVar(val_min, math.ceil(extent / factor), parent), IterVar(val_min, factor, parent)
        if nparts >= 0:
            return IterVar(val_min, nparts, parent), IterVar(val_min, math.ceil(extent / nparts), parent)
        return rel

    def fetch_info_from_relations(info_iter_var):
        for rel in stage.relations:
            if isinstance(rel, tvm.schedule.Split):
                outer, inner = calc_split_rel(rel, info_iter_var)
                info_iter_var[rel.inner.var] = inner
                info_iter_var[rel.outer.var] = outer
            elif isinstance(rel, tvm.schedule.Fuse):
                dom_inner = (IterVar(rel.inner.dom.min.value, rel.inner.dom.extent.value, None)
                             if rel.inner.dom is not None else IterVar(
                                 info_iter_vars[rel.inner.var][0], info_iter_vars[rel.inner.var][1], None))
                dom_outer = (IterVar(rel.outer.dom.min.value, rel.outer.dom.extent.value, None)
                             if rel.outer.dom is not None else IterVar(
                                 info_iter_vars[rel.outer.var][0], info_iter_vars[rel.outer.var][1], None))
                info_iter_var[rel.fused.var] = (dom_inner.min * dom_outer.min,
                                                dom_inner.extent * dom_outer.extent)

    info_iter_vars = {
        iter_var.var: (iter_var.dom.min.value, iter_var.dom.extent.value)
        for iter_var in stage.all_iter_vars if iter_var.dom is not None
    }
    fetch_info_from_relations(info_iter_vars)
    res = [[item.var, (item.dom.min.value, item.dom.extent.value) if item.dom is not None else info_iter_vars[item.var]
            ] for item in stage.leaf_iter_vars]

    return res


def print_iter_vars(iter_vars):
    """
    Pretty print iter_vars.

    Args: iter_vars: List of iter_var.

    Returns: None.
    """

    for i, item in enumerate(iter_vars):
        print(i * 4 * ' ', item)


def is_mini_version():
    """
    check if mini version
    -------

    Returns
    -------
    True: mini version
    False: Other version
    """
    soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in [tbe_platform_info.ASCEND_310]:
        return True
    return False


def is_cloud_version():
    """
    check if cloud version
    -------

    Returns
    -------
    True: cloud version
    False: Other version
    """
    soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in [tbe_platform_info.ASCEND_910, tbe_platform_info.ASCEND_910H,
                       tbe_platform_info.ASCEND_910M, tbe_platform_info.ASCEND_910P]:
        return True
    return False


def is_ng1_version():
    """
    check if mini_ng1 version
    -------

    Returns
    -------
    True: mini_ng1 version
    False: Other version
    """
    soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in [tbe_platform_info.ASCEND_610, tbe_platform_info.BS9SX1A,
                       tbe_platform_info.ASCEND_310P]:
        return True
    return False


def is_lhisi_version():
    """
    check if shisi version
    -------

    Returns
    -------
    True: shisi version
    False: Other version
    """
    soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in [tbe_platform_info.HI3796CV300ES, tbe_platform_info.HI3796CV300CS,
                       tbe_platform_info.ASCEND_SD]:
        return True
    return False


def is_lhisi_cs_version():
    """
    check if 3796CS version
    -------

    Returns
    -------
    True: 3796CS version
    False: Other version
    """
    soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in [tbe_platform_info.HI3796CV300CS, tbe_platform_info.ASCEND_SD]:
        return True
    return False


def is_v200_version():
    """
    check if v200 version
    -------

    Returns
    -------
    True: v200 version
    False: Other version
    """
    return is_ng1_version()


def is_v200_version_new():
    """
    check if v200 new version
    -------

    Returns
    -------
    True: v200 new version
    False: Other version
    """
    return is_ng1_version() or is_lhisi_cs_version()


def is_mini_or_lhisi_version():
    """
    check if mini or lhisi version
    -------

    Returns
    -------
    True: mini or lhisi version
    False: Other version
    """
    mini_or_lhisi_version_flag = is_mini_version() or is_lhisi_version()
    return mini_or_lhisi_version_flag


def is_load3d_constraint():
    """
    check if load3d_contraint
    """
    # here get_soc_spec return type is string type. "1": true; "0" or "unknown": false
    return "1" == tbe_platform_info.get_soc_spec("load3d_constraints")


def is_exceed_l1_for_load3d(params, is_conv1d=False, is_strideh_read=False, is_c04=False, is_load3d_w_split=False):
    """
    L1 limitation, Mainly required by chip
    """
    stride_h, stride_w = params.get("stride")
    dilation_h, dilation_w = params.get("dilation")
    kernel_height, kernel_width = params.get("weight_shape")
    if is_strideh_read:
        stride_h = kernel_height
    width_grads = params.get("width_grads")
    width_fmap = params.get("width_fmap")
    input_dtype = params.get("input_dtype")

    kernel_h_dilation = (kernel_height - 1) * dilation_h + 1
    kernel_w_dilation = (kernel_width - 1) * dilation_w + 1
    # for FP16, k0 is 16, c0 is 16
    # for FP32, k0 is 16, c0 is 8, but need at least 2 c0
    k0_size = tbe_platform.CUBE_MKN.get(input_dtype).get("mac")[0]
    input_dtype_size = BIT_RATIO_DICT.get(input_dtype)
    al1_min_byte = k0_size * k0_size * input_dtype_size
    kl1_min = width_fmap
    if is_conv1d or is_load3d_w_split:
        # For conv1d/is_load3d_w_split, kl1_min is the input corresponding to K0
        kl1_min = (k0_size - 1) * stride_w + kernel_w_dilation
    kl1_min = min(kl1_min, width_fmap)

    if not is_load3d_w_split:
        # for C04, channel is 4
        row_major_c0 = 4 if is_c04 else k0_size
        if width_grads >= k0_size:
            if width_grads % k0_size == 0:
                bl1_min_byte = kernel_h_dilation * kl1_min * row_major_c0 * input_dtype_size
            else:
                bl1_min_byte = (kernel_h_dilation + stride_h) * kl1_min * row_major_c0 * input_dtype_size
        else:
            bl1_align_factor = int_ceil_div(k0_size, width_grads)
            # For example:
            # |  Wo  |  3   |  5   |  6   |  7   |  9   |  10  |  11  |  12  |  13  |  14  |  15  |
            # | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
            # |  Ho  |  6   |  4   |  4   |  4   |  3   |  3   |  3   |  2   |  3   |  2   |  2   |
            if row_major_c0 % width_grads != 0:
                bl1_align_factor += 1
            bl1_min_byte = align((kernel_h_dilation + (bl1_align_factor - 1) * stride_h) * kl1_min,
                                  k0_size) * row_major_c0 * input_dtype_size
    else:
        bl1_min_byte = kernel_h_dilation * kl1_min * k0_size * input_dtype_size
    l1_size = tbe_platform.get_soc_spec("L1_SIZE")

    return (al1_min_byte + bl1_min_byte) > l1_size


def extract_tensor_info(
    desc_tensor: Union[Dict[str, Any], tvm.te.tensor.Tensor, None]) -> Dict[str, Union[str, List[int]]]:
    if desc_tensor is None:
        return None

    if isinstance(desc_tensor, tvm.te.tensor.Tensor):
        attrs = desc_tensor.op.attrs

        for key in ('ori_shape', 'format', 'ori_format'):
            if key not in attrs:
                error_manager_cube.raise_err_message_cube(f"not found {key} in attrs of Tensor")
        return {"shape": shape_to_list(desc_tensor.shape), "ori_shape": shape_to_list(attrs["ori_shape"]),
                "dtype": desc_tensor.dtype, "format": str(attrs["format"]),
                "ori_format": str(attrs["ori_format"])}

    for key in ('shape', 'ori_shape', 'dtype', 'format', 'ori_format'):
        if key not in desc_tensor:
            error_manager_cube.raise_err_message_cube(f"not found {key} in dict of Tensor")
    return {"shape": desc_tensor["shape"], "ori_shape": desc_tensor["ori_shape"],
            "dtype": desc_tensor["dtype"], "format": desc_tensor["format"],
            "ori_format": desc_tensor["ori_format"]}


class BinaryMode(IntEnum):
    """
    Enum class of Binary Mode
    """
    NON_BINARY = 0
    NDC1HWC0 = 1
    NC1HWC0 = 1
    NCHW = 2
    NHWC = 3


class CompileMode():
    """
    Class of compile mode
    """
    def __init__(self, is_binary_flag, binary_mode=0):
        self.is_binary_flag = is_binary_flag
        if is_binary_flag:
            self.compile_mode = BinaryMode(binary_mode)


class Load3DParam:
    _filter_min = 1
    _filter_max = 255
    _stride_min = 1
    _stride_max = 63
    _pad_min = 0
    _pad_max = 255
    _dilation_min = 1
    _dilation_max = 255

    @staticmethod
    def filter_min() -> int:
        return Load3DParam._filter_min

    @staticmethod
    def filter_max() -> int:
        if not tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
            return Load3DParam._filter_max
        return 511 # for v220, filter size is 511

    @staticmethod
    def stride_min() -> int:
        return Load3DParam._stride_min

    @staticmethod
    def stride_max() -> int:
        return Load3DParam._stride_max

    @staticmethod
    def pad_min() -> int:
        return Load3DParam._pad_min

    @staticmethod
    def pad_max() -> int:
        return Load3DParam._pad_max

    @staticmethod
    def dilation_min() -> int:
        return Load3DParam._dilation_min

    @staticmethod
    def dilation_max() -> int:
        return Load3DParam._dilation_max
