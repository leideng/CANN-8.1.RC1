#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gemm_compute
"""
import json
import tbe.common.platform as tbe_platform
import tbe.common.utils as tbe_utils
from tbe import tvm
from tbe.tvm import Tensor
from tbe.common.context import op_context
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils import para_check
from tbe.common.utils import shape_util
from tbe.common.utils.const import ComputeFlow
from tbe.common.utils.errormgr import error_manager_cube
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.compute import cube_util
from tbe.dsl.compute.gemm_compute_util import FormatCompute
from tbe.dsl.compute.gemm_compute_util import ShapeAInfo
from tbe.dsl.compute.gemm_compute_util import ShapeBInfo
from tbe.dsl.compute.util import align
from tbe.dsl.compute.util import int_ceil_div
from tbe.dsl.compute.gemm_compute_util import GEMMComputeParam
from tbe.tvm import Tensor


@para_check.check_input_type(Tensor, Tensor, dict)
def gemm(tensor_a, tensor_b, para_dict):
    """
    algorithm: gemm
    calculate matrix multiplication C = alpha*(AB + bias) + beta*C

    Parameters:
    shape_a: list or tuple
            Shape of the first tensor a with rank > 1
    shape_b: list or tuple
            Shape of the second tensor b with the same type with a,
            and shape_a, shape_b must be 2 dims
    shape_bias: list or tuple
            Shape of bias, only support the input data format with ND
    trans_a: bool
            If True, shape_a is transposed before multiplication
    trans_b: bool
            If True, shape_b is transposed before multiplication
    is_fractal: bool
            If True, the input data format of a and b must be fractal format

    Returns None
    """
    # ----temp
    trans_a = para_dict.get("trans_a")
    trans_b = para_dict.get("trans_b")
    format_a = para_dict.get("format_a")
    format_b = para_dict.get("format_b")
    if format_a == "FRACTAL_NZ":
        trans_a = not trans_a
    if format_b == "FRACTAL_NZ":
        trans_b = not trans_b
    para_dict["trans_a"] = trans_a
    para_dict["trans_b"] = trans_b

    gemm_compute = GEMMCompute(tensor_a, tensor_b, para_dict)
    result = gemm_compute.compute()
    return result


class GetPerfCoreNum:
    """
    get perf core num by mte2/mte3, use in atomic add k.
    """
    BYTES_DTYPE = {
        "uint64": 8,
        "float16": 2,
        "float32": 4,
        "int32": 4,
        "int16": 2,
        "uint16": 2,
        "int8": 1,
        "uint8": 1,
        "int4": 0.5
    }
    soc_hbm_bandwidth_info = {8: 250, 32: 1100}
    soc_l2_bandwidth_info = {8: 1300, 32: 3300}
    atomic_addr_clean_cost_multi = 2

    def __init__(self):
        pass

    def get_best_perf_factor(self, shapes, blocks):
        """
        get best perf block dim factor by mte2 and mte3
        """
        m_factor = 1
        k_factor = 1
        n_factor = 1
        float16_size = 2
        m_shape, k_shape, n_shape = shapes
        block_in, block_reduce, block_out = blocks
        core_num = tbe_platform_info.get_soc_spec("CORE_NUM")
        l2_size = tbe_platform_info.get_soc_spec("L2_SIZE")
        if core_num < 8:
            return 1, 1, 1
        use_out_buffer_size = (m_shape * k_shape + k_shape * n_shape + m_shape * n_shape) * float16_size
        hbm_bandwidth, l2_bandwidth = self._get_bandwidth(core_num)
        cur_bandwidth = hbm_bandwidth
        if use_out_buffer_size < l2_size:
            cur_bandwidth = l2_bandwidth
        min_cost = core_num * (m_shape * n_shape + m_shape * k_shape + n_shape * k_shape) * float16_size / hbm_bandwidth

        m_axis_outer = int_ceil_div(m_shape, block_in)
        n_axis_outer = int_ceil_div(n_shape, block_out)
        k_axis_outer = int_ceil_div(k_shape, block_reduce)

        m_max_dim = core_num if (m_axis_outer > core_num) else m_axis_outer
        n_max_dim = core_num if (n_axis_outer > core_num) else n_axis_outer
        k_max_dim = core_num if (k_axis_outer > core_num) else k_axis_outer

        total_max_dim = m_max_dim * k_max_dim * n_max_dim
        for i in range(0, total_max_dim):
            k_dim = int(i / (m_max_dim * n_max_dim)) + 1
            n_dim = int(i / m_max_dim) % n_max_dim + 1
            m_dim = i % m_max_dim + 1
            if m_dim * k_dim * n_dim > core_num:
                continue
            if (m_dim > m_axis_outer) or (k_dim > k_axis_outer) or (n_dim > n_axis_outer):
                continue
            block_dims = (m_dim, k_dim, n_dim)
            cur_cost = self._compute_perf(shapes, block_dims, cur_bandwidth)
            if cur_cost < min_cost:
                min_cost = cur_cost
                m_factor, k_factor, n_factor = block_dims
        return m_factor, k_factor, n_factor

    def _get_bandwidth(self, core_num):
        hbm_bandwidth = self.soc_hbm_bandwidth_info.get(core_num, 0)
        l2_bandwidth = self.soc_l2_bandwidth_info.get(core_num, 0)
        if hbm_bandwidth == 0 or l2_bandwidth == 0:
            distant = abs(core_num - 8)
            core_num_best = 8
            all_corenum_value = self.soc_hbm_bandwidth_info.keys()
            for inner_core_num in all_corenum_value:
                if abs(core_num - inner_core_num) < distant:
                    distant = abs(core_num - inner_core_num)
                    core_num_best = inner_core_num
            hbm_bandwidth = self.soc_hbm_bandwidth_info.get(core_num_best, 0)
            l2_bandwidth = self.soc_l2_bandwidth_info.get(core_num_best, 0)
        return hbm_bandwidth, l2_bandwidth

    def _compute_perf(self, shapes, block_dims, cur_bandwidth):
        m_shape, k_shape, n_shape = shapes
        m_dim, k_dim, n_dim = block_dims
        m_shape_inner = int_ceil_div(m_shape, m_dim)
        k_shape_inner = int_ceil_div(k_shape, k_dim)
        n_shape_inner = int_ceil_div(n_shape, n_dim)
        out_data_size_fp32 = self.BYTES_DTYPE.get("float32", 0)
        in_data_size = self.BYTES_DTYPE.get("float16", 0)
        cast_node_cost = 0
        transdata_node_cost = 0
        atomic_add_bw_lose_radio = 1
        atomic_addr_clean_cost = 0
        if k_dim != 1:
            atomic_add_bw_lose_radio = 0.5
            atomic_addr_clean_cost = (m_shape * n_shape * out_data_size_fp32 /
                                      cur_bandwidth) * self.atomic_addr_clean_cost_multi

        mte3_cost = k_dim * (m_shape_inner * n_shape_inner * out_data_size_fp32) / (atomic_add_bw_lose_radio *
                                                                                    cur_bandwidth)
        base_load_cost = (m_shape_inner * k_shape_inner + k_shape_inner * n_shape_inner) * in_data_size / cur_bandwidth
        b_repeat_load_cost = (m_dim - 1) * k_shape_inner * n_shape_inner * in_data_size / cur_bandwidth
        a_repeat_load_cost = (n_dim - 1) * k_shape_inner * m_shape_inner * in_data_size / cur_bandwidth
        total_cost = (base_load_cost + mte3_cost + a_repeat_load_cost + b_repeat_load_cost + cast_node_cost +
                      transdata_node_cost + atomic_addr_clean_cost)
        return total_cost


class GEMMCompute(FormatCompute):
    """
    algorithm: mmad
    calculate matrix multiplication C = alpha*(AB + bias) + beta*C

    Parameters:
    tensor_a: the first tensor a

    tensor_b: the seconed tensor b with the same dtype with a

        If tensor_a/tensor_b is int8/uint8,then L0A must be 16*32,L0B
        must be 32*16.
        If A is transpose , then AShape classification matrix must be
        32*16 in gm/L1,then it is 16*32 in L0A.
        If B is transpose , then BShape classification matrix must be
        16*32 in gm/L1,then it is 32*16 in L0B.

    trans_a: if True, tensor_a needs to be transposed

    trans_b: if True, tensor_b needs to be transposed

    format_a: the format of tensor_a, support FRACTAL_NZ, FRACTAL_Z, ND
              default is "ND"

    format_b: the format of tensor_b, support FRACTAL_NZ, FRACTAL_Z, ND
              default is "ND"

    dst_dtype: output data type, support float16 float32 int32
               default is float32

    tensor_bias: the bias with used to add

    tensor_c: the c matrix with used to add

    format_out: output format, now support ND, FRACTAL_NZ

    kernel_name: kernel name, default is "gemm"

    Returns None
    """
    # if [k * k0] is equal to or larger than GEVM_MODE_K_DIM_LIMIT in gevm/gemv mode, use gemm mode.
    GEVM_MODE_K_DIM_LIMIT = 9216
    # if (K_DIM, N_DIM) in GEVM_MODE_LIMIT_LIST, use gemm mode. N_DIM is n*n0
    GEVM_MODE_LIMIT_LIST = [(4096, 4096), (4096, 1008)]
    # the c0 size of nc1hwc0 fp32
    FP32_C0_SIZE = 8
    # pad-fusion
    PAD_B = 1
    PAD_A = 2
    PAD_AB = 3
    # transdata-fusion
    NZ_VEC_B = 1
    NZ_VEC_A = 2
    NZ_VEC_AB = 3

    def __init__(self, tensor_a, tensor_b, para_dict):
        super(GEMMCompute, self).__init__()
        self.tensor_a = tensor_a
        self.tensor_b = tensor_b
        # shape
        self.shape_a = shape_util.shape_to_list(tensor_a.shape)
        self.shape_b = shape_util.shape_to_list(tensor_b.shape)
        # trans
        self.trans_a = para_dict.get("trans_a", False)
        self.trans_b = para_dict.get("trans_b", False)
        # format
        self.format_a = para_dict.get("format_a", "ND")
        self.format_b = para_dict.get("format_b", "ND")
        self.format_out = para_dict.get("format_out")
        # dtype
        self.src_dtype = tensor_a.dtype
        self.dst_dtype = para_dict.get("dst_dtype", "float16")
        # other tensor
        self.alpha = para_dict.get("alpha")
        self.beta = para_dict.get("beta")
        self.tensor_c = para_dict.get("tensor_c")
        self.compress_index = para_dict.get("compress_index")
        # batch shapes
        self.batch_shape_a = para_dict.get("batch_shape_a", [])
        self.batch_shape_b = para_dict.get("batch_shape_b", [])
        self.batch_shape_out = para_dict.get("batch_shape_out", [])
        self.ori_batch_shape_out = para_dict.get("ori_batch_shape_out", [])
        # other info from para_dict
        self.op_type = para_dict.get("op_type", "Gemm")
        self.kernel_name = para_dict.get("kernel_name", "gemm")
        self.fc_flag = para_dict.get("fc_flag", False)
        self.cache_tiling_flag = para_dict.get("cache_tiling_flag", False)
        self.is_fusion = para_dict.get("is_fusion", False)
        self.unaligned_flag = para_dict.get("unaligned_flag", False)
        self.input_range = para_dict.get("input_range")
        self.offset_a = para_dict.get("offset_a", 0)
        self.zero_flag = para_dict.get("zero_flag", False)
        self.alg = para_dict.get("alg", None)
        self.deq_vec_flag = para_dict.get("deq_vec_flag", False)
        self.compress_info = para_dict.get("compress_info", [])
        # init some params by default
        self.align_a, self.align_b = True, True
        self.ops_data_flow_mode = "fp162fp32"
        self.l0c_support_fp32 = True
        self.dtype_mmad = "float32"
        self.block_in = tbe_platform.BLOCK_IN
        self.block_out = tbe_platform.BLOCK_OUT
        self.block_reduce = tbe_platform.BLOCK_REDUCE
        self.tensor_bias = None
        self.int8_not_double_m = False
        self.need_reformat_to_nd = False
        self.mmad_mode = "gemm"
        self.only_use_gevm_gemv_flow = False
        self.split_k = False
        self.best_split_k_block_dim = []
        self.shape_a_info = None
        self.shape_b_info = None
        # milan
        self.support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        self.support_out2l1_nd2nz = tbe_platform_info.intrinsic_check_support("Intrinsic_data_move_out2l1_nd2nz")
        self.shape_m_ori = 0
        self.shape_n_ori = 0
        self.shape_k_ori = 0
        self.nd2nz_type = para_dict.get("nd2nz_type", 0)
        self.l0a_layout_nz = tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN")
        self.pad_flag = para_dict.get("pad_flag", 0)
        self.nz_fusion_flag = para_dict.get("nz_fusion_flag", 0)
        self.nz_fusion_mode = para_dict.get("nz_fusion_mode", 0)

    # ----------- main func ---------- #
    def compute(self):
        """
        the main func of gemm
        """
        # infer and update params from the origin inputs
        self._preprocess()
        tensor_a = self._compute_tensor_a()
        tensor_b = self._compute_tensor_b()
        tensor_mmad = self._compute_tensor_mmad(tensor_a, tensor_b)
        tensor_post = self._compute_tensor_post(tensor_mmad)
        return tensor_post

    # ----------- preprocess ---------- #
    def _preprocess(self):
        self.ops_data_flow_mode = self._get_ops_data_flow()
        self.l0c_support_fp32 = True if "f162f32" in tbe_platform.getValue("Intrinsic_mmad") else False
        self.dtype_mmad = self._get_dtype_mmad()
        self.block_reduce = GEMMComputeParam.get_block_reduce(self.ops_data_flow_mode)
        # reset some params
        # NOTE: tensor b info has been changed here
        self.tensor_b, self.shape_b = self._tensor_b_swap_c1_hw()
        self.tensor_bias, self.tensor_c = self._reset_bias_and_c()
        self.format_a, self.format_b, self.format_out = self._reset_format()
        self._check_format()
        self.shape_a_info = ShapeAInfo(self.shape_a, self.format_a, self.trans_a, self.pad_flag)
        self.shape_b_info = ShapeBInfo(self.shape_b, self.format_b, self.trans_b, self.pad_flag)
        # do after reset
        self.int8_not_double_m = self.tensor_a.dtype in ("int8", "uint8") and \
                                 self.format_a == "ND" and (self.alpha is None)
        self.need_reformat_to_nd = self.format_out == "ND" and self.tensor_c is None
        self._check_shape_dims()
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd") or \
            tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub"):
            self._set_shape_ori_by_a()
            self._set_shape_ori_by_b()
        self.mmad_mode, self.only_use_gevm_gemv_flow = self._process_mmad_mode()
        self.block_in, self.block_out = self._set_blocks_in_and_out()
        self.split_k, self.best_split_k_block_dim = self._process_split_k()

    def _get_ops_data_flow(self):
        src_dtype = self.src_dtype
        dst_dtype = self.dst_dtype
        type_map = {
            "float16": "fp16",
            "float32": "fp32",
            "int8": "int8",
            "int32": "int32",
            "uint8": "uint8",
            "int4": "int4",
            "bfloat16": "bf16",
        }
        connect_str = "2"
        ops_data_flow_mode = connect_str.join([type_map.get(src_dtype), type_map.get(dst_dtype)])
        merge_data_flow_dict = {
            # ---- merge to int82int32 ---- #
            "int82int32": "int82int32",
            "int82fp16": "int82int32",
            "int82int8": "int82int32",
            "uint82fp16": "int82int32",
            "int42int32": "int42int32",
            "int42int4": "int42int32",
            "int42fp16": "int42int32",
            # ---- merge to fp162fp32 ---- #
            "fp162fp32": "fp162fp32",
            # ---- merge to fp162fp16 ---- #
            "fp162fp16": "fp162fp16",
            # ---- merge to int82fp32 ---- #
            "int82fp32": "int82fp32",
            # milan support float32/bfloat16 in_dtype
            "fp322fp32": "fp322fp32",
            "bf162bf16": "bf162bf16",
            "fp162int8": "fp162int8",
        }
        ops_data_flow_mode = merge_data_flow_dict.get(ops_data_flow_mode)
        if ops_data_flow_mode is None:
            reason = ("The current input and output dtype is not supported, "
                      "input dtype is {}, output dtype is {}.".format(src_dtype, dst_dtype))
            error_manager_cube.raise_err_specific(self.op_type, reason)
        return ops_data_flow_mode

    def _get_dtype_mmad(self):
        ops_data_flow_mode = self.ops_data_flow_mode
        l0c_support_fp32 = self.l0c_support_fp32
        dict_dtype_mmad = {
            "int82int32": "int32",
            "int42int32": "int32",
            "fp162fp32": "float32",
            "fp162fp16": "float32",
            "int82fp32": "float32",
            "fp322fp32": "float32",
            "bf162bf16": "float32",
            "fp162int8": "float32",
        }
        dtype_mmad = dict_dtype_mmad.get(ops_data_flow_mode)
        if dtype_mmad == "float32" and (not l0c_support_fp32):
            dtype_mmad = "float16"
        return dtype_mmad

    def _tensor_b_swap_c1_hw(self):
        tensor_b = self.tensor_b
        shape_b = self.shape_b
        trans_b = self.trans_b
        format_b = self.format_b
        batch_shape_b = self.batch_shape_b
        op_type = self.op_type
        if op_type == "BatchMatMulV2":
            shape_b_ori = tensor_b.op.attrs["ori_shape"]
            len_shape_b_ori = len(shape_b_ori)
            len_shape_b = len(shape_b)
            # (c1hw, n1, n0, c0) -> (h, w, c1, n1, n0, c0)
            is_valid = (not in_dynamic()
                        and len_shape_b_ori in (3, 4)
                        and len_shape_b == 4
                        and tensor_b.dtype == "int8"
                        and not trans_b
                        and format_b == "FRACTAL_Z"
                        and not self.support_l0c2out)
            if is_valid:
                if len_shape_b_ori == 4:
                    height, width, _, _ = shape_b_ori
                else:
                    height = 1
                    width, _, _ = shape_b_ori

                c1hw, n1, n0, c0 = shape_b
                c1 = c1hw // (height * width)
                shape_b_swap_c1_hw = [height * width, c1, n1, n0, c0]
                # NOTE: Shape_b should be reformed by shape_to_list.
                shape_b_swap_c1_hw = shape_util.shape_to_list(shape_b_swap_c1_hw)
                tensor_b = tvm.compute(shape_b_swap_c1_hw,
                                            lambda hw_idx, c1_idx, n1_idx, n0_idx, c0_idx:
                                            tensor_b(c1_idx * height * width + hw_idx, n1_idx, n0_idx, c0_idx),
                                            name="tensor_b_swap_c1_hw",
                                            attrs={"ori_batch_shape": batch_shape_b})
                shape_b = shape_b_swap_c1_hw
        return tensor_b, shape_b

    def _reset_bias_and_c(self):
        tensor_c = self.tensor_c
        alpha = self.alpha
        beta = self.beta
        tensor_bias = None
        if (alpha is None) or (beta is None):
            # NOTE: Actually we only need to see if alpha is None because alpha and beta should be in pairs.
            # When alpha is None, the tensor_c means bias.
            tensor_bias = tensor_c
            tensor_c = None
        return tensor_bias, tensor_c

    def _reset_format(self):
        format_a = "NC1HWC0" if "NHWC_trans_5HD" in self.tensor_a.op.tag else self.format_a
        format_b = self.format_b
        format_out = self.format_out
        shape_a = self.shape_a
        # reset format_out
        if format_out is None:
            # NOTE: need to check if we set format_out correctly in impl interface
            # default format of output is FRACTAL_NZ
            # if format_a and format_b are ND and user didn't set format_out, return ND
            format_out = "ND" if (format_a == "ND" and format_b == "ND") else "FRACTAL_NZ"
        if format_out == "NC1HWC0" and \
                (not tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd")):
            # NOTE: It seems we don't need to consider trans_a here.
            m_idx = 1 if len(shape_a) in (3, 5) else 0
            format_out = "FRACTAL_NZ" if shape_a[m_idx] == 1 else "ND"
        return format_a, format_b, format_out

    def _check_format(self):
        format_a_supported = ["ND", "FRACTAL_Z", "FRACTAL_NZ", "NC1HWC0"]
        format_b_supported = ["ND", "FRACTAL_Z", "FRACTAL_NZ", "FRACTAL_ZN_RNN"]
        format_out_supported = ["ND", "NC1HWC0", "FRACTAL_NZ"]
        if self.support_l0c2out:
            format_a_supported.remove("FRACTAL_Z")
        else:
            format_a_supported.remove("NC1HWC0")

        if self.format_a not in format_a_supported:
            reason = "The supported format_a list is {}, while the current format_a is {}.".format(
                format_a_supported, self.format_a)
            error_manager_cube.raise_err_specific(self.op_type, reason)
        if self.format_b not in format_b_supported:
            reason = "The supported format_b list is {}, while the current format_b is {}.".format(
                format_b_supported, self.format_b)
            error_manager_cube.raise_err_specific(self.op_type, reason)
        if self.format_out not in format_out_supported:
            reason = "The supported format_out list is {}, while the current format_out is {}.".format(
                format_out_supported, self.format_out)
            error_manager_cube.raise_err_specific(self.op_type, reason)

    def _check_shape_dims(self):
        full_k_a = self.shape_a_info.get_full_k_dim()
        full_k_b = self.shape_b_info.get_full_k_dim()
        full_m = self.shape_a_info.get_full_m_dim()
        full_n = self.shape_b_info.get_full_n_dim()
        # check k dim
        if not in_dynamic() and full_k_a != full_k_b and not self.support_l0c2out:
            reason = "Tensor_a's k:{} should be equal to tensor_b's k:{}".format(full_k_a, full_k_b)
            error_manager_cube.raise_err_specific(self.op_type, reason)
        # check n dim if aligned
        if self.format_b == "ND" and (full_n % tbe_platform.BLOCK_OUT != 0) and self.alpha is not None:
            reason = ("In ND format, n dim must be multiple of {}.".format(tbe_platform.BLOCK_OUT))
            error_manager_cube.raise_err_specific(self.op_type, reason)
        # check gevm k1 dim
        if self.format_a in ("FRACTAL_Z", "FRACTAL_NZ"):
            if (full_m == tbe_platform.BLOCK_VECTOR) and (self.shape_a_info.k1 % tbe_platform.BLOCK_IN != 0):
                reason = "For fractal gevm input, k1 dim should be multiple of {}.".format(tbe_platform.BLOCK_IN)
                error_manager_cube.raise_err_specific(self.op_type, reason)
        # check gemv k1 dim
        if self.format_b in ("FRACTAL_Z", "FRACTAL_NZ"):
            if (full_n == tbe_platform.BLOCK_VECTOR) and (self.shape_b_info.k1 % tbe_platform.BLOCK_OUT != 0):
                reason = "For fractal gemv input, k1 dim should be multiple of {}.".format(tbe_platform.BLOCK_OUT)
                error_manager_cube.raise_err_specific(self.op_type, reason)

    def _set_shape_ori_by_a(self):
        if not in_dynamic() and "ori_shape" not in self.tensor_a.op.attrs:
            error_manager_cube.raise_err_specific(self.op_type, "tensor_a must have attr ori_shape")
        if self.format_a == "FRACTAL_NZ" and not in_dynamic():
            origin_shape = self.tensor_a.op.attrs["ori_shape"]
            self.shape_m_ori = origin_shape[-2] if not self.trans_a else origin_shape[-1]
            self.shape_k_ori = origin_shape[-1] if not self.trans_a else origin_shape[-2]
        else:
            self.shape_m_ori = self.shape_a_info.get_full_m_dim()
            self.shape_k_ori = self.shape_a_info.get_full_k_dim()

    def _set_shape_ori_by_b(self):
        if not in_dynamic() and "ori_shape" not in self.tensor_b.op.attrs:
            error_manager_cube.raise_err_specific(self.op_type, "tensor_b must have attr ori_shape")
        if in_dynamic():
            self.shape_n_ori = self.shape_b_info.get_full_n_dim()
        elif self.op_type == "FullyConnection":
            self._get_fully_connection_n_shape()
        else:
            origin_shape = self.tensor_b.op.attrs["ori_shape"]
            self.shape_n_ori = origin_shape[-2] if self.trans_b else origin_shape[-1]

    def _get_fully_connection_n_shape(self):
        """get origin n shape for fully-connection
        1) NC1HWC0 * (Cin1HW Co1 Co0 Cin0) = NC1HWC0, n shape is Co1*Co0 in Fz shape
        2) NC1HWC0/FRACTAL_NZ * (Cin1 Co1 Co0 Cin0) = FRACTAL_NZ, n shape is N dim origin_shape_b of NCHW/NHWC format
        """
        if self.format_out == "NC1HWC0":
            # B must be FZ, ["k1", "n1", "n0", "k0"]
            self.shape_n_ori = self.shape_b[-3] * self.shape_b[-2]
            if "ori_format" in self.tensor_b.op.attrs:
                ori_format_b = self.tensor_b.op.attrs["ori_format"]
                if (ori_format_b == "NCHW" and self.dst_dtype == "float32" and self.src_dtype == "float32" and
                    (self.tensor_b.op.attrs["ori_shape"][0] <= self.FP32_C0_SIZE)):
                    self.shape_n_ori = self.FP32_C0_SIZE
        else:
            # FRACTAL_NZ output
            if "ori_format" in self.tensor_b.op.attrs:
                ori_format_b = self.tensor_b.op.attrs["ori_format"]
            else:
                ori_format_b = "NCHW"
            n_index = ori_format_b.find('N')
            if n_index < 0:
                error_manager_cube.raise_err_specific(self.op_type, "origin format of input2 is illegal")
            origin_shape_b = self.tensor_b.op.attrs["ori_shape"]
            self.shape_n_ori = origin_shape_b[n_index]

    # ----------- mmad_mode calc ---------- #
    def _process_mmad_mode(self):
        """
        when m or n's length is 1 and k is align to 512Byte, can
        use gemv or gevm mode, else use gemm mode
        """
        mmad_mode = "gemm"
        only_use_gevm_gemv_flow = False
        # The op GEMM not use gevm/gemv now
        if (self.alpha is not None) or in_dynamic() or \
                (tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd")):
            return mmad_mode, only_use_gevm_gemv_flow

        full_m = self.shape_a_info.get_full_m_dim()
        full_k = self.shape_a_info.get_full_k_dim()
        full_n = self.shape_b_info.get_full_n_dim()

        if full_k % (tbe_platform.BLOCK_IN * self.block_reduce) == 0:
            # k_dim is multiple of block_in*block_reduce
            if full_m == full_n == tbe_platform.BLOCK_VECTOR:
                if self.format_a != "ND" and self.format_b != "ND":
                    reason = "Not support vector mul vector when A and B both fractal."
                    error_manager_cube.raise_err_specific(self.op_type, reason)
            elif full_m == tbe_platform.BLOCK_VECTOR:
                # The performance of gemm mode is better than gevm in some cases
                # not use gevm or gevm don't need to fill zero, keep origin flag
                if full_k < self.GEVM_MODE_K_DIM_LIMIT and (full_k, full_n) not in self.GEVM_MODE_LIMIT_LIST:
                    mmad_mode = "gevm"
            elif full_n == tbe_platform.BLOCK_VECTOR:
                mmad_mode = "gemv"
        elif full_m == tbe_platform.BLOCK_VECTOR:
            mmad_mode = "gevm"
            only_use_gevm_gemv_flow = True
        return mmad_mode, only_use_gevm_gemv_flow

    def _set_blocks_in_and_out(self):
        block_in = tbe_platform.BLOCK_VECTOR if self.mmad_mode == "gevm" else tbe_platform.BLOCK_IN
        block_out = tbe_platform.BLOCK_VECTOR if self.mmad_mode == "gemv" else tbe_platform.BLOCK_OUT
        if self.only_use_gevm_gemv_flow:
            block_in, block_out = tbe_platform.BLOCK_IN, tbe_platform.BLOCK_OUT
        return block_in, block_out

    # ----------- split k calc ---------- #
    def _get_shapes(self):
        if in_dynamic():
            if self.input_range is None:
                m_shape, shape_a_k, n_shape = 65535, 1, 65535
            else:
                m_shape = self.input_range[-3][1]
                shape_a_k = self.input_range[-2][0]
                n_shape = self.input_range[-1][1]
        else:
            m_shape = self.shape_a_info.get_full_m_dim()
            shape_a_k = self.shape_a_info.get_full_k_dim()
            n_shape = self.shape_b_info.get_full_n_dim()
        return (m_shape, shape_a_k, n_shape)

    def _get_split_k_flag(self):
        context = op_context.get_context()
        if context is None or not self.support_l0c2out:
            return False
        build_options = context.get_addition("build_options")
        if build_options:
            options = json.loads(build_options)
            return options.get("split_k_flag", False)
        return False

    def _process_split_k(self):
        # this func will replace by the info from ub fusion
        split_k = False
        best_split_k_block_dim = []
        support_split_k = (
            self.alpha is None
            and self.tensor_bias is None
            and self.compress_index is None
            and self.src_dtype == "float16"
            and self.dst_dtype == "float32"
            and (self.format_out == "FRACTAL_NZ" or (self.format_out == "ND" and self.support_l0c2out))
            and not self.is_fusion
            and not self.fc_flag
        )
        if self.cache_tiling_flag or self._get_split_k_flag():
            # NOTE: best_split_k_block_dim is still blank list in dynamic.
            split_k = support_split_k
        else:
            have_batch = self.shape_a_info.have_batch() or self.shape_b_info.have_batch()
            support_split_k = support_split_k and not have_batch
            if support_split_k:
                # get block_factors
                blocks = (self.block_in, self.block_reduce, self.block_out)
                compute_perf_core_num = GetPerfCoreNum()
                block_factors = compute_perf_core_num.get_best_perf_factor(self._get_shapes(), blocks)
                if block_factors[1] != 1:
                    best_split_k_block_dim = block_factors
                    split_k = True
        return split_k, best_split_k_block_dim

    # ----------- do align func ---------- #
    def _is_nd_int82fp32(self):
        return (self.format_b == "ND") and (self.format_a == "ND") and (self.ops_data_flow_mode == "int82fp32")

    def _check_align_a(self):
        """
        if src_dtype is fp16, m and k both aligned to 16
        if src_dtype is int8 dst_dtype is fp32, m and k both aligned to 32
        if src_dtype is int8 dst_dtype is int32, m aligned to 16 k aligned to 32
        """
        is_nd_int82fp32 = self._is_nd_int82fp32()
        if self.trans_a:
            index_m = -1
            index_km = -2
        else:
            index_m = -2
            index_km = -1
        ori_m_shape = self.shape_a[index_m]
        ori_km_shape = self.shape_a[index_km]
        if is_nd_int82fp32:
            m_shape = int_ceil_div(ori_m_shape, 32) * 32 // 16
            km_shape = int_ceil_div(ori_km_shape, 32) * 32 // 16
        else:
            if self.src_dtype in ("uint8", "int8"):
                m_shape = int_ceil_div(ori_m_shape, 32) * 32 // 16
                if self.int8_not_double_m:
                    m_shape = int_ceil_div(ori_m_shape, self.block_in)
            else:
                m_shape = int_ceil_div(ori_m_shape, self.block_in)
            km_shape = int_ceil_div(ori_km_shape, self.block_reduce)

        shape_a_aligned = [m_shape * self.block_in, km_shape * self.block_reduce]
        align_flag_a = (ori_m_shape == shape_a_aligned[-2]) and (ori_km_shape == shape_a_aligned[-1])
        shape_a_aligned = shape_a_aligned[::-1] if self.trans_a else shape_a_aligned
        if in_dynamic():
            align_flag_a = False
            if self.cache_tiling_flag:
                # current cachetiling scenes both m/k/n axis is already aligned
                align_flag_a = True
        return align_flag_a, shape_a_aligned

    def _check_align_b(self):
        """
        if src_dtype is fp16, n and k both aligned to 16
        if src_dtype is int8 dst_dtype is fp32, n and k both aligned to 32
        if src_dtype is int8 dst_dtype is int32, n aligned to 16 k aligned to 32
        """
        is_nd_int82fp32 = self._is_nd_int82fp32()
        if self.trans_b:
            index_n = -2
            index_kn = -1
        else:
            index_n = -1
            index_kn = -2
        ori_n_shape = self.shape_b[index_n]
        ori_kn_shape = self.shape_b[index_kn]
        if is_nd_int82fp32:
            n_shape = int_ceil_div(ori_n_shape, 32) * 32 // 16
            kn_shape = int_ceil_div(ori_kn_shape, 32) * 32 // 16
        else:
            if self.src_dtype in ("uint8", "int8"):
                n_shape = int_ceil_div(ori_n_shape, 32) * 32 // 16
            else:
                n_shape = int_ceil_div(ori_n_shape, self.block_out)
            kn_shape = int_ceil_div(ori_kn_shape, self.block_reduce)

        shape_b_aligned = [kn_shape * self.block_reduce, n_shape * self.block_out]
        align_flag_b = (ori_n_shape == shape_b_aligned[-1]) and (ori_kn_shape == shape_b_aligned[-2])
        shape_b_aligned = shape_b_aligned[::-1] if self.trans_b else shape_b_aligned
        if in_dynamic():
            align_flag_b = False
            if self.cache_tiling_flag:
                # current cachetiling scenes both m/k/n axis is already aligned
                align_flag_b = True
        return align_flag_b, shape_b_aligned

    def _do_align_nd_shape(self, tensor_need_align, tensor_name, in_dtype):
        """
        do align for a matrix or b matrix, pad zero along the way
        input:
            tensor_need_align: tensor, the tensor need align
            tensor_name: str, a or b
            in_dtype: str, input data type
        return:
            aligned tensor
        """

        if tensor_name == "a":
            is_align, shape_aligned = self._check_align_a()
            self.align_a = is_align
        else:
            is_align, shape_aligned = self._check_align_b()
            self.align_b = is_align

        not_need_align = is_align or (self.mmad_mode in ("gemv", "gevm"))
        if in_dynamic():
            tensor_aligned = self._do_shape_aligned_for_dynamic(
                tensor_need_align, shape_aligned, tensor_name, in_dtype, use_aligned_pattern=not_need_align)
        else:
            tensor_aligned = self._do_shape_aligned_for_static(
                tensor_need_align, shape_aligned, tensor_name, in_dtype, use_aligned_pattern=not_need_align)
        return tensor_aligned

    def _do_align_nd_shape_for_c(self, tensor_need_align, in_dtype):
        """
        do align for tensor_bias, pad zero along the way
        input:
            tensor_need_align: tensor, the tensor need align
            in_dtype: str, input data type
        return:
            aligned tensor
        """
        if len(tensor_need_align.shape) in (4, 5):
            is_align = True
            shape_aligned = tensor_need_align.shape
        else:
            ori_m_shape = cube_util.get_value(tensor_need_align.shape[-2])
            ori_n_shape = cube_util.get_value(tensor_need_align.shape[-1])
            m_shape = int_ceil_div(ori_m_shape, self.block_in) * self.block_in
            n_shape = int_ceil_div(ori_n_shape, self.block_out) * self.block_out
            is_align = (ori_m_shape == m_shape) and (ori_n_shape == n_shape)
            shape_aligned = [m_shape, n_shape]

        not_need_align = is_align or (self.mmad_mode in ("gemv", "gevm"))
        use_aligned_pattern = False if in_dynamic() else not_need_align
        tensor_aligned = self._do_shape_aligned_for_static(
            tensor_need_align, shape_aligned, "c", in_dtype, use_aligned_pattern=use_aligned_pattern)
        return tensor_aligned

    def _do_shape_aligned_for_static(self, tensor_need_align, shape_aligned, tensor_name,
                                     in_dtype, use_aligned_pattern=False):
        """
        do align for tensor_a_zz , tensor_b_zn and tensor_bias
        input:
            tensor_need_align: tensor, the tensor need align
            tensor_name: str, a , b or bias
            in_dtype: str, input data type
            use_aligned_pattern: bool, is this tensor aligned
        return:
            aligned tensor
        """
        if use_aligned_pattern:
            tensor_aligned = tvm.compute(
                tensor_need_align.shape,
                lambda *indices: tensor_need_align(*indices),
                name="tensor_{}_aligned".format(tensor_name)
            )
            return tensor_aligned

        ori_shape = shape_util.shape_to_list(tensor_need_align.shape)
        shape_aligned = ori_shape[:-2] + shape_aligned

        tensor_aligned = tvm.compute(shape_aligned,
            lambda *indices: tvm.select(indices[-2] < ori_shape[-2],
                tvm.select(
                    indices[-1] < ori_shape[-1],
                    tensor_need_align(*indices),
                    tvm.convert(0).astype(in_dtype)
                ),
                tvm.convert(0).astype(in_dtype)
            ),
            name="tensor_{}_aligned".format(tensor_name)
        )
        if self.int8_not_double_m and tensor_name == "a":
            # virtual_align means use aligned shape, but do not pad zero.
            tensor_aligned.op.attrs["virtual_align"] = True
        return tensor_aligned

    def _do_shape_aligned_for_dynamic(self, tensor_need_align, shape_aligned, tensor_name,
                                      in_dtype, use_aligned_pattern=False):
        """
        do align for tensor_a_zz or tensor_b_zn, pad zero along the way in dynamic mode
        input:
            tensor_need_align: tensor, the tensor need align
            tensor_name: str, a or b
            in_dtype: str, input data type
            use_aligned_pattern: bool is this tensor aligned
        return:
            aligned tensor
        """
        ori_shape = shape_util.shape_to_list(tensor_need_align.shape)
        # Use a virtual Node [tensor_aligned] which will be mapped as "phony_insn".
        # At schedule stage, either aligned or general pattern will be selected for execution,
        # and the remaining one will be mapped as "phony_insn".
        shape_aligned = ori_shape[:-2] + shape_aligned

        tensor_already_aligned = self.p2p_copy(tensor_need_align, shape_aligned,
                                               name="tensor_{}_already_aligned".format(tensor_name))
        tensor_do_align = tvm.compute(
            shape_aligned,
            lambda *indices: tvm.select(
                indices[-2] < ori_shape[-2],
                tvm.select(
                    indices[-1] < ori_shape[-1],
                    tensor_need_align(*indices)
                )
            ),
            name="tensor_{}_do_align".format(tensor_name)
        )
        tensor_aligned = tvm.compute(
            shape_aligned,
            lambda *indices: tensor_already_aligned(*indices)
                + tensor_do_align(*indices),
            name="tensor_{}_aligned".format(tensor_name),
            attrs={"use_aligned_pattern": use_aligned_pattern}
        )

        return tensor_aligned

    def _do_shape_a_aligned_for_cache_tiling_with_perm(self, tensor_a):
        """
        do align for tensor_a, pad zero along the way in dynamic mode
        input:
            tensor_a: tensor need align
        return:
            aligned tensor
        """
        # Use a virtual Node [tensor_aligned] which will be mapped as "phony_insn".
        # At schedule stage, either aligned or general pattern will be selected for execution,
        # and the remaining one will be mapped as "phony_insn".
        var_perm_x1 = get_te_var("perm_x1").get_tvm_var()
        var_batch = get_te_var("batch").get_tvm_var()
        var_m1 = get_te_var("m").get_tvm_var()
        var_k = get_te_var("k_ori").get_tvm_var()
        if not self.trans_a:
            shape_a_already_aligned = [var_batch, var_m1 * self.block_reduce, var_k]
            # var_perm_x1 102 means that it needs to change from [m, batch, k] to [batch, m, k]
            tensor_a_already_aligned = tvm.compute(
                shape_a_already_aligned,
                lambda batch, m, k: tvm.select(False, tensor_a(m, batch, k), tensor_a(batch, m, k)),
                name='tensor_a_already_aligned')
            shape_a_do_align = [
                var_batch, var_m1 * self.block_reduce,
                int_ceil_div(var_k, self.block_in) * self.block_in
            ]
            tensor_a_do_align = tvm.compute(shape_a_do_align,
                                            lambda batch, m, k: tvm.select(
                                                False,
                                                tvm.select(
                                                    m < tensor_a.shape[-3],
                                                    tvm.select(k < tensor_a.shape[-1], tensor_a[m, batch, k],
                                                               tvm.convert(0).astype(tensor_a.dtype))),
                                                tvm.select(
                                                    m < tensor_a.shape[-2],
                                                    tvm.select(k < tensor_a.shape[-1], tensor_a[batch, m, k],
                                                               tvm.convert(0).astype(tensor_a.dtype)))),
                                            name="tensor_a_do_align")
            tensor_a_aligned = FormatCompute.elementwise_add(tensor_lhs=tensor_a_already_aligned,
                                                             tensor_rhs=tensor_a_do_align,
                                                             name="tensor_a_aligned",
                                                             str_dst_indices=["batch", "m", "k"],
                                                             attrs={"use_aligned_pattern": True})
        else:
            shape_a_already_aligned = [var_batch, var_k, var_m1 * self.block_in]
            # var_perm_x1 120 means that it needs to change from [k, batch, m] to [batch, m, k]
            tensor_a_already_aligned = tvm.compute(shape_a_already_aligned,
                                                   lambda batch, k, m: tvm.select(
                                                       False,
                                                       tensor_a(k, batch, m),
                                                       tensor_a(batch, k, m),
                                                   ),
                                                   name='tensor_a_already_aligned')
            shape_a_do_align = [
                var_batch,
                int_ceil_div(var_k, self.block_reduce) * self.block_reduce, var_m1 * self.block_in
            ]
            tensor_a_do_align = tvm.compute(shape_a_do_align,
                                            lambda batch, k, m: tvm.select(
                                                False,
                                                tvm.select(
                                                    k < tensor_a.shape[-3],
                                                    tvm.select(m < tensor_a.shape[-1], tensor_a[k, batch, m],
                                                               tvm.convert(0).astype(tensor_a.dtype))),
                                                tvm.select(
                                                    k < tensor_a.shape[-2],
                                                    tvm.select(m < tensor_a.shape[-1], tensor_a[batch, k, m],
                                                               tvm.convert(0).astype(tensor_a.dtype)))),
                                            name="tensor_a_do_align")
            tensor_a_aligned = FormatCompute.elementwise_add(tensor_lhs=tensor_a_already_aligned,
                                                             tensor_rhs=tensor_a_do_align,
                                                             name="tensor_a_aligned",
                                                             str_dst_indices=["batch", "k", "m"],
                                                             attrs={"use_aligned_pattern": True})

        return tensor_a_aligned

    def _do_shape_b_aligned_for_cache_tiling_with_perm(self, tensor_b):
        """
        do align for tensor_b, pad zero along the way in dynamic mode
        Args:
            tensor_b: tensor need align
        Returns:
            aligned tensor
        """
        # Use a virtual Node [tensor_aligned] which will be mapped as "phony_insn".
        # At schedule stage, either aligned or general pattern will be selected for execution,
        # and the remaining one will be mapped as "phony_insn".
        var_perm_x2 = get_te_var("perm_x2").get_tvm_var()
        var_batch = get_te_var("batch").get_tvm_var()
        var_k = get_te_var("k_ori").get_tvm_var()
        var_n1 = get_te_var("n").get_tvm_var()

        if not self.trans_b:
            shape_b_already_aligned = [var_batch, var_k, var_n1 * self.block_in]
            # var_perm_x2 102 means that it needs to change from [k, batch, n] to [batch, k, n]
            tensor_b_already_aligned = tvm.compute(
                shape_b_already_aligned,
                lambda batch, k, n: tvm.select(False, tensor_b(k, batch, n), tensor_b(batch, k, n)),
                name="tensor_b_already_aligned")
            shape_b_do_align = [
                var_batch,
                int_ceil_div(var_k, self.block_reduce) * self.block_reduce,
                var_n1 * self.block_in
            ]
            tensor_b_do_align = tvm.compute(shape_b_do_align,
                                            lambda batch, k, n: tvm.select(
                                                False,
                                                tvm.select(
                                                    k < tensor_b.shape[-3],
                                                    tvm.select(n < tensor_b.shape[-1], tensor_b[k, batch, n],
                                                               tvm.convert(0).astype(tensor_b.dtype))),
                                                tvm.select(
                                                    k < tensor_b.shape[-2],
                                                    tvm.select(n < tensor_b.shape[-1], tensor_b[batch, k, n],
                                                               tvm.convert(0).astype(tensor_b.dtype))),
                                            ),
                                            name="tensor_b_do_align")
            tensor_b_aligned = FormatCompute.elementwise_add(tensor_lhs=tensor_b_do_align,
                                                             tensor_rhs=tensor_b_already_aligned,
                                                             name="tensor_b_aligned",
                                                             str_dst_indices=["batch", "k", "n"],
                                                             attrs={"use_aligned_pattern": True})
        else:
            shape_b_already_aligned = [var_batch, var_n1 * self.block_reduce, var_k]
            # var_perm_x2 102 means that it needs to change from [n, batch, k] to [batch, k, n]
            tensor_b_already_aligned = tvm.compute(
                shape_b_already_aligned,
                lambda batch, n, k: tvm.select(False, tensor_b(n, batch, k), tensor_b(batch, n, k)),
                name="tensor_b_already_aligned")
            shape_b_do_align = [
                var_batch, var_n1 * self.block_reduce,
                int_ceil_div(var_k, self.block_in) * self.block_in
            ]
            tensor_b_do_align = tvm.compute(shape_b_do_align,
                                            lambda batch, n, k: tvm.select(
                                                False,
                                                tvm.select(
                                                    n < tensor_b.shape[-3],
                                                    tvm.select(k < tensor_b.shape[-1], tensor_b[n, batch, k],
                                                               tvm.convert(0).astype(tensor_b.dtype))),
                                                tvm.select(
                                                    n < tensor_b.shape[-2],
                                                    tvm.select(k < tensor_b.shape[-1], tensor_b[batch, n, k],
                                                               tvm.convert(0).astype(tensor_b.dtype)))),
                                            name="tensor_b_do_align")
            tensor_b_aligned = FormatCompute.elementwise_add(tensor_lhs=tensor_b_do_align,
                                                             tensor_rhs=tensor_b_already_aligned,
                                                             name="tensor_b_aligned",
                                                             str_dst_indices=["batch", "n", "k"],
                                                             attrs={"use_aligned_pattern": True})

        return tensor_b_aligned

    # ----------- compute_a func ---------- #
    def _compute_tensor_a(self):
        # do compute
        if self.mmad_mode == "gemv":
            tensor_a = self._get_tensor_a_zz_gemv()
        elif self.mmad_mode == "gevm":
            tensor_a = self._get_tensor_a_zz_gevm()
        else:
            tensor_a = self._get_tensor_a_gemm()

        # add attrs info
        if "ori_batch_shape" in self.tensor_a.op.attrs:
            tensor_a.op.attrs["ori_batch_shape"] = self.tensor_a.op.attrs["ori_batch_shape"]
        return tensor_a

    def _get_tensor_a_zz_gemv(self):
        # move tensor a to L0B, dst_format is Nz in L0A and it is Zn in L0B: [k1, m1, m0, k0]
        dst_format_list = self.shape_a_info.get_base_format_list("FRACTAL_NZ")
        if self.format_a == "ND":
            # NOTE remove unnecessary compute of d2d copy
            tensor_a_aligned = self._do_align_nd_shape(self.tensor_a, "a", self.src_dtype)

            # do nd2zz
            compute_params_gemv = {"tensor_name": "tensor_a_nd2zz", "trans": False,
                "block_in": self.block_in, "block_reduce": self.block_reduce}
            tensor_a_nd2zz = self.compute_nd2zz(tensor_a_aligned, compute_params_gemv)
            # zz format no_trans: [m1, k1, m0, k0]
            src_format_list = self.shape_a_info.get_base_format_list("FRACTAL_Z")
            tensor_a_zz = self.transpose_axes(tensor_a_nd2zz, src_format_list, dst_format_list,
                                              name="tensor_a_zz", attrs={"mode": "nd_gemv"})
        else:
            # FRACTAL_Z no_trans: [m1, k1, m0, k0]
            # FRACTAL_NZ no_trans: [k1, m1, m0, k0]
            src_format_list = self.shape_a_info.format_list
            tensor_a_zz = self.transpose_axes(self.tensor_a, src_format_list, dst_format_list,
                                              name="tensor_a_zz", attrs={"mode": "fractal_gemv"})
        return tensor_a_zz

    def _get_tensor_a_zz_gevm(self):
        if self.format_a == "ND":
            # NOTE remove unnecessary compute of d2d copy
            tensor_a_aligned = self._do_align_nd_shape(self.tensor_a, "a", self.src_dtype)

            compute_params_gemv = {"tensor_name": "tensor_a_nd2zz", "trans": self.trans_a,
                                   "block_in": self.block_in, "block_reduce": self.block_reduce,
                                   "mode_info": "nd_gevm"}
            tensor_a_zz = self.compute_nd2zz_gevm(tensor_a_aligned, compute_params_gemv)
        else:
            # not support int82fp32
            tensor_a_zz = self._get_tensor_a_frac2zz(self.tensor_a)
        return tensor_a_zz

    def _get_tensor_a_vec_nd2nz(self):
        tensor_a_aligned = self.tensor_a
        if self.nz_fusion_flag in [self.NZ_VEC_A, self.NZ_VEC_AB] and self.cache_tiling_flag:
            shape_a_nz = self.shape_a_info.get_shape_nz(self.src_dtype)
            shape_a_aligned = [shape_a_nz[-3] * shape_a_nz[-2], shape_a_nz[-4] * shape_a_nz[-1]]
            tensor_a_vec_aligned = tvm.compute(
                shape_a_aligned,
                lambda *indices: tvm.select(indices[-2] < self.shape_a[-2],
                                            tvm.select(indices[-1] < self.shape_a[-1], self.tensor_a(*indices))),
                name="tensor_a_vec_aligned"
            )
            tensor_a_aligned = tvm.compute(
                shape_a_nz,
                lambda *indices: tensor_a_vec_aligned(indices[-3] * self.block_in + indices[-2],
                                                      indices[-4] * self.block_reduce + indices[-1]),
                name="tensor_a_vec_fractal",
                attrs={"ori_format": "ND", "ori_shape": shape_a_aligned, "format": "FRACTAL_NZ"},
                tag="ND_trans_NZ"
            )
            self.tensor_a = tensor_a_vec_aligned
            self.format_a = "FRACTAL_NZ"
            self.shape_a_info.set_shape_info(shape_a_nz, self.format_a, self.trans_a)
        return tensor_a_aligned

    def _get_tensor_a_gemm(self):
        tensor_a_aligned = self._get_tensor_a_vec_nd2nz()
        if self.ops_data_flow_mode == "int82fp32":
            if self.format_a == "ND":
                tensor_a_aligned = self._do_align_nd_shape(self.tensor_a, "a", self.src_dtype)

            tensor_a_aligned = FormatCompute.cast_dtype(tensor_a_aligned, "float16", name="tensor_a_s82f16")
            if self.format_a == "FRACTAL_NZ":
                compute_params_fractal = {"tensor_name": "tensor_a_zz", "trans": self.trans_a,
                                          "mode_info": "Nz2Zz_int82fp32"}
                tensor_a = self.compute_nz2zz_int82fp32(tensor_a_aligned, compute_params_fractal)
                return tensor_a
        # milan not aligned
        if self.format_a == "NC1HWC0":
            tensor_a_nz = self._get_tensor_a_5hd2nz()
            tensor_a = self._get_tensor_a_frac2zz(tensor_a_nz)
        elif self.format_a == "ND":
            if self.support_out2l1_nd2nz and self.nd2nz_type != ComputeFlow.mix_l2.value:
                tensor_a_nz = self._get_tensor_a_nd2nz()
                tensor_a = self._get_tensor_a_frac2zz(tensor_a_nz)
            else:
                if self.unaligned_flag:
                    ori_shape = shape_util.shape_to_list(self.tensor_a.shape)
                    self.shape_k_ori = ori_shape[-2:][1 - int(self.trans_a)]
                if self.ops_data_flow_mode != "int82fp32":
                    if self.cache_tiling_flag and get_te_var("batch"):
                        # NOTE only support f162f16
                        tensor_a_aligned = self._do_shape_a_aligned_for_cache_tiling_with_perm(tensor_a_aligned)
                    else:
                        tensor_a_aligned = self._do_align_nd_shape(tensor_a_aligned, "a", self.src_dtype)
                tensor_a = self._get_tensor_a_nd2zz(tensor_a_aligned)
                tensor_a.op.attrs["transpose_a"] = "true" if self.trans_a else "false"
        else:
            tensor_a = self._get_tensor_a_frac2zz(tensor_a_aligned)
        return tensor_a

    def _get_tensor_a_nd2zz(self, tensor_a_aligned):
        use_normal_func = self.int8_not_double_m
        compute_params = {
            "tensor_name": "tensor_a_zz",
            "block_in": self.block_in,
            "block_reduce": self.block_reduce,
            "data_flow": self.ops_data_flow_mode,
            "trans": self.trans_a
        }
        if use_normal_func or in_dynamic() or \
                self.ops_data_flow_mode in ("int82int32", "int4int32"):
            mode_info = "nd2Zz" if (self.ops_data_flow_mode != "int82int32") else "nd2Zz_int8"
            compute_params["mode_info"] = mode_info
            tensor_a_zz = self.compute_nd2zz(tensor_a_aligned, compute_params)
        else:
            compute_params["mode_info"] = "nd2Zz_vnchwconv"
            tensor_a_zz = self.compute_nd2zz_vnchwconv(tensor_a_aligned, compute_params)
        return tensor_a_zz

    def _get_tensor_nd2nz(self, tensor_nd, shape_nz, shape_nd, fp32_align_k, tensor_name):
        need_align_n_axis = self.unaligned_flag or (not self.cache_tiling_flag) or fp32_align_k
        a_need_k_limit = (not self.trans_a) and (not self.trans_b) and self.pad_flag > 0 and tensor_name == "b"
        b_need_k_limit = self.trans_a and self.trans_b and self.pad_flag > 0 and tensor_name == "a"
        if need_align_n_axis or a_need_k_limit or b_need_k_limit:
            tensor_nz = tvm.compute(
                shape_nz,
                lambda *indices: tvm.select(tvm.all((indices[-3] * self.block_in + indices[-2]) < shape_nd[-2]),
                    tvm.select(tvm.all((indices[-4] * self.block_reduce + indices[-1]) < shape_nd[-1]),
                    tensor_nd(*indices[:-4],
                              indices[-3] * self.block_in + indices[-2],
                              indices[-4] * self.block_reduce + indices[-1])
                )),
                name=tensor_nd.name + "_fractal",
                attrs={"ori_format": "ND", "ori_shape": shape_nd, "format": "FRACTAL_NZ"},
                tag="ND_trans_NZ"
            )
        else:
            tensor_nz_name = tensor_nd.name + "_fractal"
            if tensor_name in ["a_vec", "b_vec"]:
                tensor_nz_name = tensor_nd.name + "_vec_fractal"
            tensor_nz = tvm.compute(
                shape_nz,
                lambda *indices: tvm.select(tvm.all((indices[-4] * self.block_reduce + indices[-1]) < shape_nd[-1]),
                    tensor_nd(*indices[:-4],
                              indices[-3] * self.block_in + indices[-2],
                              indices[-4] * self.block_reduce + indices[-1])
                ),
                name=tensor_nz_name,
                attrs={"ori_format": "ND", "ori_shape": shape_nd, "format": "FRACTAL_NZ"},
                tag="ND_trans_NZ"
            )
        return tensor_nz

    def _get_tensor_a_pad(self):
        tensor_a_pad, shape_pad = self.tensor_a, self.shape_a
        if not self.trans_a:
            if self.pad_flag in [self.PAD_A, self.PAD_AB]:
                shape_pad = [self.shape_a_info.m_pad, self.shape_a_info.k_pad]
                tensor_a_pad = tvm.compute(
                    shape_pad,
                    lambda m, k: tvm.select(
                        k < self.shape_a_info.ori_k,
                        self.tensor_a(m, k)
                    ),
                    name="tensor_a_pad"
                )
        else:
            if self.pad_flag == self.PAD_A:
                shape_pad = [self.shape_a_info.k_pad, self.shape_a_info.m_pad]
                tensor_a_pad = tvm.compute(
                    shape_pad,
                    lambda k, m: tvm.select(
                        m < self.shape_a_info.ori_m,
                        self.tensor_a(k, m)
                    ),
                    name="tensor_a_pad"
                )
            elif self.pad_flag == self.PAD_AB:
                shape_pad = [self.shape_a_info.k_pad, self.shape_a_info.m_pad]
                tensor_a_pad = tvm.compute(
                    shape_pad,
                    lambda k, m: tvm.select(
                        tvm.all(k < self.shape_a_info.ori_k, m < self.shape_a_info.ori_m),
                        self.tensor_a(k, m)
                    ),
                    name="tensor_a_pad"
                )
        return tensor_a_pad, shape_pad

    def _get_tensor_a_nd2nz(self):
        """
        trans ND to FRACTAL_NZ
        for fullyconnection and binary matmul
        """
        tensor_a_pad, shape_pad = self._get_tensor_a_pad()
        shape_a_nz = self.shape_a_info.get_shape_nz(self.src_dtype)
        fp32_align_k = self.trans_a and self.src_dtype == "float32"
        return self._get_tensor_nd2nz(tensor_a_pad, shape_a_nz, shape_pad, fp32_align_k, "a")

    def _get_tensor_a_5hd2nz(self):
        """
        for 5hd input, trans 5hd to nz
        """
        _, _, src_h, src_w, _ = tuple(i.value for i in self.tensor_a.shape)
        nz_shape = self.shape_a_info.get_shape_nz(self.src_dtype)
        tensor_a_nz = tvm.compute(
            nz_shape,
            lambda * indices:
            tvm.select(tvm.all(indices[-3] * self.block_in + indices[-2] < self.tensor_a.op.attrs["ori_shape"][-4]),
                self.tensor_a(
                    indices[-3] * self.block_in + indices[-2],
                    indices[-4] // (src_h * src_w),
                    indices[-4] // src_w % src_h,
                    indices[-4] % src_w,
                    indices[-1]
                )
            ),
            name="tensor_a_5hd2nz",
            tag="5HD_trans_FZ"
        )
        return tensor_a_nz

    def _get_tensor_a_zz_mode(self, tensor_a_nz, mode=None):
        """
        get tensor_a_zz compute mode on milan
        nhwc2Zz:
            fc nhwc input
        nd2Nz_trans:
            fc 5hd input
            matmul/batchmatmul pre transdata fusion
        Nz2Nz:
            others
        """
        if not self.support_l0c2out and mode:
            return mode
        if self.format_a == "NC1HWC0":
            mode = "nhwc2Zz"
        elif self.format_a == "ND":
            mode = "nd2Zz_trans_fusion"
        else:
            if tensor_a_nz.op.input_tensors:
                mode = "nd2Zz_trans_fusion"
            else:
                mode = "Nz2Zz"
        return mode

    def _get_tensor_a_frac2zz(self, tensor_a):
        # trans nz to zz
        shape_a_zz = self.shape_a_info.get_shape_a_zz(self.src_dtype)
        if self.format_a in ["ND", "NC1HWC0"]:
            src_format_list = self.shape_a_info.get_format_list("FRACTAL_NZ")
        else:
            src_format_list = self.shape_a_info.format_list
        dst_format_list = self.shape_a_info.get_base_format_list("FRACTAL_Z")
        if src_format_list[-1] != "k0" and self.src_dtype in ("int8", "int4"):
            tensor_name = "tensor_a_zz"
            m_index, k_index = -3, -4
            if self.l0a_layout_nz:
                tensor_name = "tensor_a_nz"
                shape_a_zz[-3], shape_a_zz[-4] = shape_a_zz[-4], shape_a_zz[-3]
                m_index, k_index = k_index, m_index
            block_reduce_multiple_in = self.block_reduce // self.block_in
            tensor_a_zz = tvm.compute(
                shape_a_zz, lambda *indices:
                tensor_a(*indices[:-4],
                         indices[k_index] // block_reduce_multiple_in,
                         indices[m_index] * block_reduce_multiple_in + indices[-1] // self.block_in,
                         indices[-1] % self.block_in,
                         indices[-2] + indices[k_index] % block_reduce_multiple_in * self.block_in),
                name=tensor_name,
                attrs={"transpose_a": "true",
                       "mode": self._get_tensor_a_zz_mode(tensor_a)})
        elif src_format_list[-1] != "k0" and self.src_dtype == "float32":
            block_reduce_multiple_in = self.block_in // self.block_reduce
            tensor_a_zz = tvm.compute(
                shape_a_zz,
                lambda *indices:
                    tensor_a(*indices[:-4],
                             indices[-4] * block_reduce_multiple_in + indices[-2] // self.block_reduce,
                             (indices[-3] * self.block_reduce + indices[-1]) // self.block_in,
                             (indices[-3] * self.block_reduce + indices[-1]) % self.block_in,
                             indices[-2] % self.block_reduce),
                name="tensor_a_zz",
                attrs={"transpose_a": "true",
                       "mode": self._get_tensor_a_zz_mode(tensor_a)}
                )
        elif src_format_list == dst_format_list:
            tensor_a_zz = tensor_a
        else:
            trans_info = "true" if self.trans_a else "false"
            mode_info = "Zz_trans" if self.format_a == "FRACTAL_Z" else "Nz2Zz"
            mode_info = self._get_tensor_a_zz_mode(tensor_a, mode_info)
            tensor_name = "tensor_a_zz"
            if self.l0a_layout_nz:
                batch_prefix = ["batch"] if self.shape_a_info.have_batch() else []
                dst_format_list = batch_prefix + ["k1", "m1", "m0", "k0"]
                tensor_name = "tensor_a_nz"
            tensor_a_zz = self.transpose_axes(tensor_a, src_format_list, dst_format_list,
                                              name=tensor_name,
                                              attrs={"transpose_a": trans_info, "mode": mode_info})

        return tensor_a_zz

    # ----------- compute_b func ---------- #
    def _compute_tensor_b(self):
        # do compute
        if self.compress_index is not None and self.alg != "weight_sparse_4_2":
            # only support fp16 input
            self.format_b = "FRACTAL_Z"
            tensor_b_zn = self._get_compress_tensor_compute("tensor_b_zn")
        elif self.mmad_mode == "gemv":
            tensor_b_zn = self._get_tensor_b_zn_gemv()
        else:
            tensor_b_zn = self._get_tensor_b_zn_gemm()

        # add attrs info
        if "ori_batch_shape" in self.tensor_b.op.attrs:
            tensor_b_zn.op.attrs["ori_batch_shape"] = self.tensor_b.op.attrs["ori_batch_shape"]
        return tensor_b_zn

    def _get_tensor_b_vec_nd2nz(self):
        tensor_b_aligned = self.tensor_b
        if self.nz_fusion_flag in [self.NZ_VEC_B, self.NZ_VEC_AB] and self.cache_tiling_flag:
            shape_b_nz = self.shape_b_info.get_shape_nz(self.src_dtype)
            shape_b_aligned = [shape_b_nz[-3] * shape_b_nz[-2], shape_b_nz[-4] * shape_b_nz[-1]]
            tensor_b_vec_aligned = tvm.compute(
                shape_b_aligned,
                lambda *indices: tvm.select(indices[-2] < self.shape_b[-2],
                                            tvm.select(indices[-1] < self.shape_b[-1], self.tensor_b(*indices))),
                name="tensor_b_vec_aligned")
            tensor_b_aligned = tvm.compute(
                shape_b_nz,
                lambda *indices: tensor_b_vec_aligned(indices[-3] * self.block_in + indices[-2],
                                                      indices[-4] * self.block_reduce + indices[-1]),
                name="tensor_b_vec_fractal",
                attrs={"ori_format": "ND", "ori_shape": shape_b_aligned, "format": "FRACTAL_NZ"},
                tag="ND_trans_NZ"
            )
            self.tensor_b = tensor_b_vec_aligned
            self.format_b = "FRACTAL_NZ"
            self.shape_b_info.set_shape_info(shape_b_nz, self.format_b, self.trans_b)
        return tensor_b_aligned

    def _get_tensor_b_zn_gemm(self):
        tensor_b_aligned = self._get_tensor_b_vec_nd2nz()
        if self.ops_data_flow_mode == "int82fp32":
            if self.format_b == "ND":
                tensor_b_aligned = self._do_align_nd_shape(tensor_b_aligned, "b", self.src_dtype)

            tensor_b_aligned = FormatCompute.cast_dtype(tensor_b_aligned, "float16", name="tensor_b_s82f16")

        if self.format_b in ("FRACTAL_Z", "FRACTAL_NZ", "FRACTAL_ZN_RNN"):
            tensor_b_zn = self._get_tensor_b_frac2zn(tensor_b_aligned)
        else:
            if self.support_out2l1_nd2nz and self.nd2nz_type != ComputeFlow.mix_l2.value:
                tensor_b_nz = self._get_tensor_b_nd2nz()
                tensor_b_zn = self._get_tensor_b_frac2zn(tensor_b_nz)
            else:
                if self.ops_data_flow_mode != "int82fp32":
                    if self.cache_tiling_flag and get_te_var("batch"):
                        tensor_b_aligned = self._do_shape_b_aligned_for_cache_tiling_with_perm(tensor_b_aligned)
                    else:
                        tensor_b_aligned = self._do_align_nd_shape(tensor_b_aligned, "b", self.src_dtype)
                tensor_b_zn = self._get_tensor_b_nd2zn(tensor_b_aligned)
                tensor_b_zn.op.attrs["transpose_b"] = "true" if self.trans_b else "false"
        return tensor_b_zn

    def _get_compress_tensor_compute(self, tensor_name):
        """
        get compress tensor compute
        """
        tensor_src = self.tensor_b
        comp_index = self.compress_index
        _, _, _, compress_mode = tbe_platform_info.get_soc_spec("UNZIP")
        comp_size = 8 if compress_mode == 1 else 2

        tile_k_value = get_te_var("k_l0").get_tvm_var() if in_dynamic() else tvm.var("tile_L1_k", dtype="int32")
        tile_n_value = get_te_var("cub_n1").get_tvm_var() * get_te_var("n_ub_l0_time").get_tvm_var() \
                       if in_dynamic() else tvm.var("tile_L1_n", dtype="int32")
        if in_dynamic() and get_te_var("n_dim"):
            n_dim = get_te_var("n_dim").get_tvm_var()
        else:
            n_dim = tvm.var("block_dim_n", dtype="int32")

        shape_src = tensor_src.shape
        block_n_num = int_ceil_div(shape_src[-3], tile_n_value)
        block_k_num = int_ceil_div(shape_src[-4], tile_k_value)
        n_dim_num = int_ceil_div(block_n_num, n_dim)
        n_dim_value = n_dim_num * tile_n_value

        # tile_mode is 1 when tile_n < dim_n, or tile_mode is 0
        if len(shape_src) == 4:
            if not in_dynamic():
                tensor = tvm.compute(shape_src, lambda i, j, k, l: tvm.unzip(
                                     comp_index((j // n_dim_value * n_dim_num * block_k_num
                                                + (j % n_dim_value) // tile_n_value * block_k_num
                                                + i // tile_k_value) * comp_size), tensor_src(i, j, k, l)),
                                     name=tensor_name,
                                     attrs={"tile_L1_k": tile_k_value,
                                            "tile_L1_n": tile_n_value,
                                            "block_dim_n": n_dim}
                                    )
            else:
                block_k_num = get_te_var("kbl1_factor").get_tvm_var() * get_te_var("kbl0_factor").get_tvm_var()
                tensor = tvm.compute(shape_src, lambda i, j, k, l:
                                     tvm.unzip(comp_index((j // tile_n_value * block_k_num + i // tile_k_value)
                                                          * comp_size), tensor_src(i, j, k, l)),
                                     name=tensor_name,
                                     attrs={"tile_L1_k": tile_k_value,
                                            "tile_L1_n": tile_n_value,
                                            "block_dim_n": n_dim}
                                    )
        else:
            reason = "The compress feature only support the tensor with 4 dims, "\
                     "but the length of input tensor is {}.".format(len(shape_src))
            error_manager_cube.raise_err_specific(self.op_type, reason)
        return tensor

    def _get_tensor_b_zn_gemv(self):
        # move tensor b to L0A, dst_format is Nn in L0B and it is Zz in L0A: [n1, k1, n0, k0]
        tensor_b_aligned = self.tensor_b
        if self.format_b == "ND":
            tensor_b_aligned = self._do_align_nd_shape(self.tensor_b, "b", self.src_dtype)

        batch_prefix = ["batch"] if self.shape_b_info.have_batch() else []
        dst_format_list = batch_prefix + ["n1", "k1", "n0", "k0"]
        if self.format_b == "ND":
            compute_params_gemv = {"tensor_name": "tensor_b_nd2zz", "trans": False, "mode_info": "fractal_gemv"}
            tensor_b_nd2zz = self.compute_nd2zz(tensor_b_aligned, compute_params_gemv)
            src_format_list = batch_prefix + ["k1", "n1", "k0", "n0"]
            tensor_b_zn = self.transpose_axes(tensor_b_nd2zz, src_format_list, dst_format_list,
                                              name="tensor_b_zn", attrs={"mode": "nd_gemv"})
        else:
            # FRACTAL_Z no_trans: [k1, n1, n0, k0]
            # FRACTAL_NZ no_trans: [n1, k1, k0, n0]
            src_format_list = self.shape_b_info.format_list
            tensor_b_zn = self.transpose_axes(tensor_b_aligned, src_format_list, dst_format_list,
                                              name="tensor_b_zn", attrs={"mode": "fractal_gemv"})
        return tensor_b_zn

    def _get_tensor_b_zn_mode(self, mode=None):
        """
        get tensor_b_zn compute mode on milan
        nd2Zn_trans_fusion:
            matmul/batchmatmul pre transdata fusion
        Zn2Zn:
            fractal_z input
        Nz2Zn:
            others
        """
        if not self.support_l0c2out and mode:
            return mode
        if self.format_b == "FRACTAL_Z":
            mode = "Zn2Zn"
        elif self.format_b == "ND":
            mode = "nd2Zn_trans_fusion"
        else:
            if self.tensor_b.op.input_tensors:
                mode = "nd2Zn_trans_fusion"
            else:
                mode = "Nz2Zn"
        return mode

    def _get_tensor_b_nd2zn(self, tensor_b_aligned):
        compute_params = {
            "tensor_name": "tensor_b_zn",
            "block_in": self.block_in,
            "block_reduce": self.block_reduce,
            "block_out": self.block_out,
            "data_flow": self.ops_data_flow_mode,
            "trans": self.trans_b
        }
        if self.ops_data_flow_mode == "int82int32" or (in_dynamic() and not self.unaligned_flag):
            compute_params["mode_info"] = "nd2Zn" if self.ops_data_flow_mode != "int82int32" else "nd2Zn_int8"
            tensor_b_zn = self.compute_nd2zn(tensor_b_aligned, compute_params)
        else:
            compute_params["mode_info"] = "nd2Zn_vnchwconv"
            tensor_b_zn = self.compute_nd2zn_vnchwconv(tensor_b_aligned, compute_params)
        return tensor_b_zn

    def _get_tensor_b_pad(self):
        tensor_b_pad, shape_pad = self.tensor_b, self.shape_b
        if not self.trans_b:
            if self.pad_flag == self.PAD_B:
                shape_pad = [self.shape_b_info.k_pad, self.shape_b_info.n_pad]
                tensor_b_pad = tvm.compute(
                    shape_pad,
                    lambda k, n: tvm.select(
                        n < self.shape_b_info.ori_n,
                        self.tensor_b(k, n)
                    ),
                    name="tensor_b_pad"
                )
            elif self.pad_flag == self.PAD_AB:
                shape_pad = [self.shape_b_info.k_pad, self.shape_b_info.n_pad]
                tensor_b_pad = tvm.compute(
                    shape_pad,
                    lambda k, n: tvm.select(
                        tvm.all(k < self.shape_b_info.ori_k, n < self.shape_b_info.ori_n),
                        self.tensor_b(k, n)
                    ),
                    name="tensor_b_pad"
                )
        else:
            if self.pad_flag in [self.PAD_B, self.PAD_AB]:
                shape_pad = [self.shape_b_info.n_pad, self.shape_b_info.k_pad]
                tensor_b_pad = tvm.compute(
                    shape_pad,
                    lambda n, k: tvm.select(
                        k < self.shape_b_info.ori_k,
                        self.tensor_b(n, k)
                    ),
                    name="tensor_b_pad"
                )
        return tensor_b_pad, shape_pad

    def _get_tensor_b_nd2nz(self):
        """
        for ND input, trans ND to FRACTAL_NZ
        binary matmul
        """
        tensor_b_pad, shape_pad = self._get_tensor_b_pad()
        shape_b_nz = self.shape_b_info.get_shape_nz(self.src_dtype)
        # if trans_b is False k need align to 16 in l1
        fp32_align_k = not self.trans_b and self.src_dtype == "float32"
        return self._get_tensor_nd2nz(tensor_b_pad, shape_b_nz, shape_pad, fp32_align_k, "b")

    def _get_tensor_b_frac2zn_trans(self, tensor_b_aligned, src_format_list):
        """
        get zn tensor from nz tensor
        """
        dst_format_list = self.shape_b_info.get_base_format_list("FRACTAL_Z")
        shape_b_zn = self.shape_b_info.get_shape_b_zn(self.src_dtype)
        if self.src_dtype == "float32":
            block_reduce_multiple_in = self.block_in // self.block_reduce
            tensor_b_zn = tvm.compute(
                shape_b_zn,
                lambda *indices:
                    tensor_b_aligned(*indices[:-4],
                                        indices[-3] * block_reduce_multiple_in + indices[-2] // self.block_reduce,
                                        (indices[-4] * 8 + indices[-1]) // 16,
                                        (indices[-4] * 8 + indices[-1]) % 16,
                                        indices[-2] % 8),
                name="tensor_b_zn",
                attrs={"transpose_b": "false", "mode": self._get_tensor_b_zn_mode()}
            )
        elif (self.src_dtype in ("int8", "int4")):
            block_reduce_multiple_out = self.block_reduce // self.block_in
            tensor_b_zn = tvm.compute(
                shape_b_zn,
                lambda *indices:
                tensor_b_aligned(*indices[:-4],
                                    indices[-3] // block_reduce_multiple_out,
                                    indices[-4] * block_reduce_multiple_out + indices[-1] // self.block_out,
                                    indices[-1] % self.block_out,
                                    indices[-2] + indices[-3] % block_reduce_multiple_out * self.block_out),
                name="tensor_b_zn",
                attrs={"transpose_b": "false", "mode": self._get_tensor_b_zn_mode()})
        else:
            mode_info = "Zn_trans" if self.trans_b else "Nz2Zn"
            mode_info = self._get_tensor_b_zn_mode(mode_info)
            trans_info = "true" if self.trans_b else "false"
            if self.format_b == "FRACTAL_Z":
                trans_info = "true"
            tensor_b_zn = self.transpose_axes(tensor_b_aligned, src_format_list, dst_format_list,
                                                name="tensor_b_zn",
                                                attrs={"transpose_b": trans_info, "mode": mode_info})
        return tensor_b_zn

    def _get_tensor_b_frac2zn(self, tensor_b_aligned):
        if self.format_b == "ND":
            # only milan
            src_format_list = self.shape_b_info.get_format_list("FRACTAL_NZ")
        else:
            src_format_list = self.shape_b_info.format_list
        shape_b_zn = self.shape_b_info.get_shape_b_zn(self.src_dtype)
        if self.ops_data_flow_mode == "int82fp32" and self.format_b == "FRACTAL_Z":
            compute_params_fractal = {"tensor_name": "tensor_b_zn", "trans": self.trans_b,
                                      "mode_info": "Zn2Zn_int82fp32"}
            tensor_b_zn = self.compute_zn2zn_int82fp32(tensor_b_aligned, compute_params_fractal)
        elif src_format_list[-1] != "k0":
            tensor_b_zn = self._get_tensor_b_frac2zn_trans(tensor_b_aligned, src_format_list)
        else:
            if not self.support_l0c2out and self.format_b == "FRACTAL_Z" and not self.trans_b:
                tensor_b_zn = tensor_b_aligned
            elif self.alg == "weight_sparse_4_2":
                # 4: weight's shape is four times weight_index
                tensor_b_zn = tvm.compute(shape_b_zn,
                                          lambda k1_idx, n1_idx, n0_idx, k0_idx: tvm.load_sparse(
                                              tensor_b_aligned(
                                                  k1_idx, n1_idx, n0_idx, k0_idx),
                                              self.compress_index(k1_idx, n1_idx, n0_idx, k0_idx // 4)),
                                          name="tensor_b_zn",
                                          attrs={"transpose_b": "false", "mode": self._get_tensor_b_zn_mode(),
                                                 "alg": self.alg})
            else:
                tensor_b_zn = self.p2p_copy(tensor_b_aligned, shape_b_zn, name="tensor_b_zn",
                                            attrs={"transpose_b": "true", "mode": self._get_tensor_b_zn_mode()})

        return tensor_b_zn

    # ----------- compute_mmad func ---------- #
    def _get_mmad_kwargs(self):
        _lambda_name = lambda tensor: "none" if tensor is None else tensor.op.name
        # maybe have pre transdata
        placeholder_a = self.tensor_a.op.input_tensors[0] if self.tensor_a.op.input_tensors else self.tensor_a
        placeholder_b = self.tensor_b.op.input_tensors[0] if self.tensor_b.op.input_tensors else self.tensor_b
        if self.tensor_b.op.name == "tensor_b_swap_c1_hw":
            placeholder_b = self.tensor_b
        placeholder_name = {"a_placehold": _lambda_name(placeholder_a),
                            "b_placehold": _lambda_name(placeholder_b),
                            "bias": _lambda_name(self.tensor_bias),
                            "tensor_c": _lambda_name(self.tensor_c),
                            "alpha": _lambda_name(self.alpha),
                            "beta": _lambda_name(self.beta)}
        attrs_dict = {"format_a": self.format_a,
                      "format_b": self.format_b,
                      "ops_data_flow_mode": self.ops_data_flow_mode,
                      "kernel_name": self.kernel_name,
                      "mmad_mode": self.mmad_mode,
                      "only_use_gevm_gemv_flow": self.only_use_gevm_gemv_flow,
                      "int8_not_double_m": self.int8_not_double_m,
                      "transpose_a": self.trans_a,
                      "transpose_b": self.trans_b,
                      "align_a": self.align_a,
                      "align_b": self.align_b,
                      "format_out": self.format_out,
                      "placeholder_name": placeholder_name,
                      "compress_flag": self.compress_index is not None,
                      "split_k": int(self.split_k),
                      "cache_tiling_flag": self.cache_tiling_flag,
                      "nd2nz_type": self.nd2nz_type,
                      "sparse_4to2_flag": bool(self.alg == "weight_sparse_4_2"),
                      "batch_shape": shape_util.shape_to_list(self.ori_batch_shape_out),
                      "deq_vec_flag": self.deq_vec_flag}
        if self.best_split_k_block_dim:
            attrs_dict["custom_block_dim_m"] = self.best_split_k_block_dim[0]
            attrs_dict["custom_block_dim_k"] = self.best_split_k_block_dim[1]
            attrs_dict["custom_block_dim_n"] = self.best_split_k_block_dim[2]

        attrs_dict["fc_flag"] = self.fc_flag

        kwargs = {"name": "tensor_mmad", "attrs": attrs_dict}
        if self.support_l0c2out:
            kwargs["tag"] = "gemm"
            if self.format_b == "FRACTAL_Z":
                kwargs.get("attrs")["transpose_b"] = True
        return kwargs

    def _compute_tensor_mmad(self, tensor_a_zz, tensor_b_zn):
        # swap tensor a/b when gemv
        if self.mmad_mode == "gemv":
            tensor_a_zz, tensor_b_zn = tensor_b_zn, tensor_a_zz
        # get tensor_bias_in_mmad
        tensor_bias_in_mmad = self._get_tensor_bias_l0c2out()
        tensor_list = [tensor_a_zz, tensor_b_zn, tensor_bias_in_mmad]
        mmad_args = (self.offset_a, self.dtype_mmad, self.shape_k_ori, self.shape_m_ori, self.shape_n_ori,
                     self.format_out, self.trans_a, self.trans_b)
        kwargs = self._get_mmad_kwargs()
        tensor_mmad = self.compute_mad(tensor_list, mmad_args, **kwargs)
        return tensor_mmad

    def _get_tensor_bias_l0c2out(self):
        tensor_bias_in_mmad = None
        if self.tensor_bias is None or not self.support_l0c2out:
            return tensor_bias_in_mmad

        n_ori = self.tensor_bias.shape[-1]
        # 8 means 1 block of float32 data
        n_tail = align(n_ori, tbe_platform.BLOCK_REDUCE) - n_ori - 8
        if self.cache_tiling_flag and self.pad_flag in [self.PAD_B, self.PAD_AB]:
            shape_bias_align = (int_ceil_div(self.shape_b_info.n_pad, tbe_platform.BLOCK_REDUCE),
                                tbe_platform.BLOCK_REDUCE)
        else:
            shape_bias_align = (int_ceil_div(n_ori, tbe_platform.BLOCK_REDUCE), tbe_platform.BLOCK_REDUCE)
        # Set value zero when axis n is unaligned with 8,
        # because dma_copy with nd2nz parameter can't set value zero to the last block of float32 data
        if self.tensor_bias.dtype in ("float32", "int32") and (in_dynamic() or n_tail >= 0):
            tensor_bias_zero = tvm.compute(shape_bias_align, lambda *indices:
                               tvm.convert(0).astype(self.tensor_bias.dtype), name="tensor_bias_zero")
            tensor_bias_align = tvm.compute(shape_bias_align, lambda n1, n0:
                                tvm.select(n1 * tbe_platform.BLOCK_REDUCE + n0 < n_ori,
                                self.tensor_bias(n1 * tbe_platform.BLOCK_REDUCE + n0),
                                tvm.const(0, dtype=self.tensor_bias.dtype) if self.l0a_layout_nz else tensor_bias_zero(n1, n0)),
                                name="tensor_bias_align")
        else:
            tensor_bias_align = tvm.compute(shape_bias_align, lambda n1, n0:
                                tvm.select(n1 * tbe_platform.BLOCK_REDUCE + n0 < n_ori,
                                self.tensor_bias(n1 * tbe_platform.BLOCK_REDUCE + n0)), name="tensor_bias_align")
        tensor_bias_in_mmad = tensor_bias_align
        if self.tensor_bias.dtype == "float16":
            tensor_bias_in_mmad = self.cast_dtype(tensor_bias_align, "float32", name="tensor_bias_f162f32")
        return tensor_bias_in_mmad

    # ----------- post process after mmad ---------- #
    def _compute_tensor_post(self, tensor_mmad):
        if self.support_l0c2out:
            tensor_post = self._compute_res_milan(tensor_mmad)
        else:
            tensor_mmad = self._handle_bias_and_scale(tensor_mmad)
            tensor_alpha_mmad = self._get_tensor_alpha_mmad(tensor_mmad)
            tensor_beta_c = self._get_tensor_beta_c()
            tensor_gemm = self._get_tensor_gemm(tensor_alpha_mmad, tensor_beta_c)
            if self.cache_tiling_flag and self.compress_index is None:
                tensor_post = self._binary_post_process(tensor_gemm)
            else:
                tensor_cast = self._get_tensor_cast(tensor_gemm, self.dst_dtype, "tensor_gemm_f16")
                tensor_nz2nd = self._get_tensor_nz2nd(tensor_cast, "tensor_nz2nd")
                tensor_post = self._compute_res(tensor_nz2nd)
        return tensor_post

    # ----------- bias and scale ---------- #
    def _get_shape_bias(self):
        shape_bias_full = shape_util.shape_to_list(self.tensor_bias.shape)
        shape_bias = [1]
        for idx, val in enumerate(shape_bias_full):
            # first element value should be > 1
            # NOTE: Cannot use [val > 1] here considering dynamic shape.
            if val != 0 and val != 1:
                shape_bias = shape_bias_full[idx:]
                break
        return shape_bias

    def _get_tensor_bias_nz(self, shape_mmad):
        tensor_bias = self.tensor_bias
        shape_bias = self._get_shape_bias()
        ori_shape = tbe_utils.shape_to_list(tensor_bias.op.attrs['ori_shape'])

        # only support [n], [1,n], [1,1,n]
        def index_bias_of_ori_shape(indices):
            return [0]*(len(shape_bias) - 1) + [indices[-1]]

        def index_bias_of_fractal_nz(indices):
            return [0]*(len(shape_bias) - 1) + [indices[-4]*self.block_out + indices[-1]]
        # dynamic mode only support bias align to 16
        if in_dynamic() or ori_shape[-1] % 16 == 0:
            tensor_bias_ub = tvm.compute(
                shape_bias, lambda *indices: tensor_bias(*index_bias_of_ori_shape(indices)), name="tensor_bias_ub")
        else:
            tensor_bias_ub = tvm.compute(
                shape_bias, lambda *indices:
                tvm.select(indices[-1] < ori_shape[-1], tensor_bias(*index_bias_of_ori_shape(indices))),
                           name="tensor_bias_ub")
            tensor_init_value_of_bias_ub = tvm.compute(
                shape_bias, lambda *indices:
                tvm.select(indices[-1] >= ori_shape[-1], tvm.const(0, dtype=tensor_bias.op.dtype)),
                           name="tensor_init_value_of_bias_ub")
            tensor_virtual_add_bias = tvm.compute(
                shape_bias, lambda *indices: tensor_bias_ub[indices]+tensor_init_value_of_bias_ub[indices],
                name="tensor_virtual_add_bias")
            tensor_bias_ub = tensor_virtual_add_bias

        # shape_mmad is batch, n1, m1, m0, n0
        if tensor_bias.dtype == "float16" and self.l0c_support_fp32:
            tensor_bias_nz = tvm.compute(
                shape_mmad, lambda *indices: shape_util.cast(
                    tensor_bias_ub(*index_bias_of_fractal_nz(indices)), dtype="float32"), name="tensor_bias_nz")
        else:
            tensor_bias_nz = tvm.compute(
                shape_mmad,
                lambda *indices: tensor_bias_ub(*index_bias_of_fractal_nz(indices)), name="tensor_bias_nz")

        return tensor_bias_nz

    def _handle_bias_and_scale(self, tensor_mmad):
        if self.tensor_bias is not None and (not self.cache_tiling_flag or self.compress_index is not None):
            tensor_bias_nz = self._get_tensor_bias_nz(tensor_mmad.shape)
            tensor_mmad = tvm.compute(tensor_bias_nz.shape,
                                      lambda *indices: tensor_bias_nz[indices] + tensor_mmad[indices],
                                      name="tensor_mmad_with_bias")

        # NOTE: Return a intermediate tensor which maybe be deleted later.
        shape_mmad = shape_util.shape_to_list(tensor_mmad.shape)
        if self.mmad_mode in ("gemv", "gevm"):
            shape_mmad[-2] = 1
        tensor_mmad_with_scale = tvm.compute(
            shape_mmad,
            lambda *indices: tensor_mmad(*indices),
            name="tensor_mmad_with_scale",
            attrs={"scale_drq": "DISABLE",
                   "sqrt_out": "NON_SQRT",
                   "nz_b": self.format_b == "FRACTAL_NZ"})
        return tensor_mmad_with_scale

    # ----------- gemm attrs ---------- #
    def _get_attrs_dict(self, tensor_gemm):
        attrs_dict = {
            "shape": tensor_gemm.shape,
            "format": self.format_out,
            "dtype_out": self.dst_dtype,
            "fc_flag": self.fc_flag,
            "is_gemm_new": True,
            "unaligned_flag": self.unaligned_flag,
            "batch_shape_a": shape_util.shape_to_list(self.batch_shape_a),
            "batch_shape_b": shape_util.shape_to_list(self.batch_shape_b),
            "batch_shape_out": shape_util.shape_to_list(self.batch_shape_out),
            "batch_shape": shape_util.shape_to_list(self.ori_batch_shape_out),
            "zero_flag": self.zero_flag
        }
        return attrs_dict

    def _get_dynamic_out_shape(self):
        shape_out = []
        """Output shape from var"""
        # NOTE only cache_tiling and ND can guarantee that thre must be batch at compile time
        if self.cache_tiling_flag and self.format_out == "ND":
            if not all(len(x.shape) in (2, ) for x in (self.tensor_a, self.tensor_b)) or get_te_var("batch"):
                shape_out = [get_te_var("batch").get_tvm_var()] if get_te_var("batch") else []
        else:
            if not all(len(x.shape) in (2, 4) for x in (self.tensor_a, self.tensor_b)):
                shape_out = [get_te_var("batch").get_tvm_var()]
        if self.format_out == "ND":
            if self.cache_tiling_flag:
                if not self.support_l0c2out and self.unaligned_flag and self.dst_dtype == "float16":
                    shape_out.extend((get_te_var("m_ori").get_tvm_var(),
                                      get_te_var("n").get_tvm_var() * self.block_out))
                elif self.pad_flag or self.unaligned_flag:
                    shape_out.extend((get_te_var("m_ori").get_tvm_var(),
                                      get_te_var("n_ori").get_tvm_var()))
                else:
                    shape_out.extend((get_te_var("m").get_tvm_var() * self.block_in,
                                      get_te_var("n").get_tvm_var() * self.block_out))
        else:
            shape_out.extend((get_te_var("n").get_tvm_var(),
                              get_te_var("m").get_tvm_var(),
                              self.block_out,
                              self.block_in))
        return shape_out

    def _get_out_shape(self, tensor_gemm):
        if in_dynamic():
            return self._get_dynamic_out_shape()

        shape_gemm_origin = []
        shape_gemm = shape_util.shape_to_list(tensor_gemm.shape)
        if len(shape_gemm) in (3, 5):
            shape_gemm_origin.append(shape_gemm[0])

        if (tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd")):
            full_m = self.shape_m_ori
            full_n = self.shape_n_ori
        else:
            full_m = self.shape_a_info.get_full_m_dim()
            full_n = self.shape_b_info.get_full_n_dim()
        if self.format_out in ["NC1HWC0", "ND"]:
            shape_gemm_origin += [full_m, full_n]
        else:
            block_in = 1 if full_m == 1 else self.block_in
            block_out = 1 if full_n == 1 else self.block_out
            m1 = int_ceil_div(full_m, block_in) if self.format_a == "ND" else self.shape_a_info.m1
            n1 = int_ceil_div(full_n, block_out) if self.format_b == "ND" else self.shape_b_info.n1
            if self.mmad_mode == "gemv":
                shape_gemm_origin += [m1, n1, block_out, block_in]
            else:
                shape_gemm_origin += [n1, m1, block_in, block_out]
        return shape_gemm_origin

    def _get_ops_tag(self):
        if self.mmad_mode == "gemv":
            res_tag = "matmul_gemv"
        elif self.mmad_mode == "gevm":
            res_tag = "matmul_gevm"
        else:
            res_tag = "matmul"
        return res_tag

    # ----------- compute_gemm func ---------- #
    def _get_tensor_alpha_mmad(self, tensor_mmad):
        if self.alpha is None:
            return tensor_mmad
        tensor_alpha = self.alpha
        if tensor_alpha.dtype == "float16":
            tensor_alpha = self.cast_dtype(tensor_alpha, "float32", name="tensor_alpha_f162f32")
        tensor_alpha_mmad = tvm.compute(
            tensor_mmad.shape,
            lambda *indices: tensor_mmad(*indices) * tensor_alpha[0],
            name="tensor_alpha_mmad")
        return tensor_alpha_mmad

    def _get_tensor_beta_c(self):
        tensor_beta_c = None
        if self.beta is None or self.tensor_c is None:
            return tensor_beta_c
        # get tensor beta/c_aligned
        tensor_beta = self.beta
        tensor_c_aligned = self._do_align_nd_shape_for_c(self.tensor_c, self.dst_dtype)
        if tensor_beta.dtype == "float16":
            tensor_beta = self.cast_dtype(tensor_beta, "float32", name="tensor_beta_f162f32")
        if tensor_c_aligned.dtype == "float16":
            tensor_c_aligned = self.cast_dtype(tensor_c_aligned, "float32", name="tensor_c_f162f32")
        # swap axes when gemv
        _lambda_swap_axes = lambda *params: list(params[:-4]) + [params[-3], params[-4], params[-1], params[-2]]
        _lambda_keep_ori = lambda *params: list(params)
        _lambda_func = _lambda_swap_axes if self.mmad_mode == "gemv" else _lambda_keep_ori
        tensor_beta_c = tvm.compute(
            _lambda_func(*tensor_c_aligned.shape),
            lambda *indices: tensor_beta[0] * tensor_c_aligned(*_lambda_func(*indices)),
            name="tensor_beta_c"
        )
        return tensor_beta_c

    def _get_tensor_gemm(self, tensor_alpha_mmad, tensor_beta_c):
        if tensor_beta_c is not None:
            shape_beta_bias_aligned = shape_util.shape_to_list(tensor_beta_c.shape)
            if self.format_out == "ND":
                tensor_gemm = self.tvm_compute_nd_add_nz_to_nd(
                    tensor_beta_c,
                    tensor_alpha_mmad,
                    "tensor_gemm"
                )
            else:
                if self.ops_data_flow_mode == "int82int32":
                    tensor_gemm = self.tvm_compute_nd_add_nz_to_nz(
                        tensor_beta_c,
                        tensor_alpha_mmad,
                        "tensor_gemm"
                    )
                else:
                    tensor_gemm = tvm.compute(
                        shape_beta_bias_aligned,
                        lambda *indices: tensor_beta_c(*indices) + tensor_alpha_mmad(*indices),
                        "tensor_gemm"
                    )
        else:
            tensor_gemm = tensor_alpha_mmad

        return tensor_gemm

    def _compute_res(self, tensor_gemm):
        res_tag = self._get_ops_tag()
        attrs_dict = self._get_attrs_dict(tensor_gemm)
        shape_gemm = self._get_out_shape(tensor_gemm)
        attrs_dict["shape"] = shape_gemm
        attrs_dict["unaligned_flag"] = self.unaligned_flag
        attrs_dict["zero_flag"] = self.zero_flag
        # not_align flag is used for nd out
        not_align = (not self.align_a or not self.align_b) and self.format_out == "ND"
        # current cachetiling scenes both m/k/n axis is mutiply of 16
        not_align = not_align and not self.cache_tiling_flag

        if not_align and self.need_reformat_to_nd and attrs_dict.get("shape")[-1] % self.block_in != 0:
            tensor_res = tensor_gemm
        else:
            tensor_res = tvm.compute(
                shape_gemm,
                lambda *indices: tensor_gemm(*indices),
                    name="tensor_c_gm",
                    tag=res_tag,
                    attrs=attrs_dict
            )
        return tensor_res

    def _get_tensor_cast(self, tensor_gemm, dst_dtype, tensor_name):
        if self.src_dtype == "float16" and dst_dtype == "float16" or self.cache_tiling_flag:
            tensor_gemm = tvm.compute(
                tensor_gemm.shape,
                lambda *indices: shape_util.cast(tensor_gemm(*indices), dtype=dst_dtype),
                name=tensor_name
            )

        return tensor_gemm

    def _get_tensor_nz2nd(self, tensor_gemm, tensor_name):
        if not self.need_reformat_to_nd:
            return tensor_gemm

        res_tag = self._get_ops_tag()
        attrs_dict = self._get_attrs_dict(tensor_gemm)
        shape_gemm = self._get_out_shape(tensor_gemm)
        attrs_dict["shape"] = shape_gemm
        if not self.cache_tiling_flag and attrs_dict.get("shape")[-1] % self.block_in != 0:
            tensor_gemm_nz2nd = self.compute_nz2nd(tensor_gemm, output_shape=shape_gemm,
                                                   tensor_name="tensor_c_gm", res_tag=res_tag,
                                                   attrs_dict=attrs_dict)
        else:
            tensor_gemm_nz2nd = self.compute_nz2nd(tensor_gemm, tensor_name=tensor_name)

        return tensor_gemm_nz2nd

    def _get_tensor_res(self, shape_out, tensor_gemm, tensor_name):
        tensor_gemm = tvm.compute(
            shape_out,
            lambda *indices: tensor_gemm(*indices),
            name=tensor_name
        )

        return tensor_gemm

    def _get_bias(self, dst_dtype):
        tensor_bias_ub = None
        if self.tensor_bias is not None:
            tensor_bias = self.tensor_bias
            shape_bias = self._get_shape_bias()

            # only support [n], [1,n], [1,1,n]
            def index_bias_of_ori_shape(indices):
                return [0]*(len(shape_bias) - 1) + [indices[-1]]
            tensor_bias_ub = tvm.compute(
                shape_bias, lambda *indices: tensor_bias(*index_bias_of_ori_shape(indices)),
                name="tensor_bias_ub_" + dst_dtype)

            tensor_bias_ub = tvm.compute(shape_bias,
                lambda *indices: tensor_bias_ub(*indices).astype(dst_dtype),
                name="bias_ub_drnn_cast_" + dst_dtype,
                tag="elewise_single_cast")
        return tensor_bias_ub

    def _handle_bias_in_ub(self, tensor_cast, tensor_bias_ub, tensor_name):
        if tensor_bias_ub is not None:
            tensor_bias = self.tensor_bias
            shape_bias = self._get_shape_bias()
            ori_shape = tbe_utils.shape_to_list(tensor_bias.op.attrs['ori_shape'])

            if len(tensor_cast.shape) == 5:
                tensor_cast = tvm.compute(tensor_cast.shape,
                                          lambda b, i0, i1, i2, i3: tvm.select(i0 * 16 + i3 <= ori_shape[-1] * 16,
                                          tensor_bias_ub(i0 * 16 + i3) + tensor_cast(b, i0, i1, i2, i3),
                                          tensor_cast(b, i0, i1, i2, i3)), name=tensor_name)
            else:
                tensor_cast = tvm.compute(tensor_cast.shape,
                                          lambda i0, i1, i2, i3: tvm.select(i0 * 16 + i3 <= ori_shape[-1] * 16,
                                          tensor_bias_ub(i0 * 16 + i3) + tensor_cast(i0, i1, i2, i3),
                                          tensor_cast(i0, i1, i2, i3)), name=tensor_name)

        return tensor_cast

    def _binary_post_process(self, tensor_gemm):
        res_tag = self._get_ops_tag()
        attrs_dict = self._get_attrs_dict(tensor_gemm)
        shape_out = self._get_out_shape(tensor_gemm)

        tensor_bias_ub_fp16 = self._get_bias("float16")
        tensor_cast = self._get_tensor_cast(tensor_gemm, "float16", "tensor_gemm_f16")
        tensor_mmad_with_bias = self._handle_bias_in_ub(tensor_cast, tensor_bias_ub_fp16, "tensor_mmad_with_bias_fp16")
        tensor_nz2nd = self._get_tensor_nz2nd(tensor_mmad_with_bias, "tensor_nz2nd")
        have_batch = self.shape_a_info.have_batch() or self.shape_b_info.have_batch()
        is_binary_constant = op_context.get_context().get_addition("is_binary_constant") == 1
        need_remove_batch = is_binary_constant and not have_batch and len(shape_out) == len(tensor_nz2nd.shape) + 1
        shape_out = shape_out[1:] if need_remove_batch else shape_out
        res_fp16 = self._get_tensor_res(shape_out, tensor_nz2nd, "tensor_out_fp16")

        tensor_bias_ub_fp32 = self._get_bias("float32")
        tensor_cast_fp32 = self._get_tensor_cast(tensor_gemm, "float32", "tensor_gemm_f32")
        tensor_mmad_with_bias_fp32 = self._handle_bias_in_ub(tensor_cast_fp32, tensor_bias_ub_fp32,
            "tensor_mmad_with_bias_fp32")
        tensor_nz2nd_fp32 = self._get_tensor_nz2nd(tensor_mmad_with_bias_fp32, "tensor_nz2nd_fp32")
        res_fp32 = self._get_tensor_res(shape_out, tensor_nz2nd_fp32, "tensor_out_fp32")

        tensor_out = tvm.compute(
            shape_out,
            lambda *indices: res_fp16(*indices) + res_fp32(*indices).astype("float16"),
            name="tensor_virtual_res", tag=res_tag, attrs=attrs_dict
        )

        return tensor_out

    # ----------- milan func ---------- #
    def _handle_nd_output(self, l0c_shape, tensor_mmad):
        """handle 5hd/nd output, nz2nd must be implemented by fixpipe op
        Input:
            l0c_shape: list, shape_of l0c
            tensor_mmad: tensor, c_matrix on l0c
        ---------------------------------
        Return:
            tensor, nd tensor of fc
        """
        dtype_trans_map = {
            "int4": "S4",
            "int8": "B8",
            "float16": "F16",
            "float32": "F32",
            "int32": "S32",
            "bfloat16": "BF16"
        }
        op_dict = {
            "post_transform": "NZ2ND",
            "pre_conv": dtype_trans_map.get(tensor_mmad.dtype) + "2" + dtype_trans_map.get(self.dst_dtype)
        }
        fixpipe_tensor = tvm.compute(
            l0c_shape,
            lambda *indices: tvm.fixpipe_op(tensor_mmad(*indices), self.dst_dtype, op_dict=op_dict),
            name="fixpipe_matmul",
            attrs={
                "vector_params": [],
                "vector_tensors": [],
                "nz2nd_flag": True,
                "anti_quant_flag": False
            })

        out_shape = self._get_out_shape(tensor_mmad)
        is_nc1hwc0_fp32 = self.format_out == "NC1HWC0" and self.dst_dtype == "float32" and self.src_dtype == "float32"
        block_size = out_shape[-1] if (is_nc1hwc0_fp32 and out_shape[-1] == self.FP32_C0_SIZE) else self.block_in
        tensor_c_gm = tvm.compute(
            out_shape,
            lambda *indices: fixpipe_tensor(*indices[:-2],
                                            indices[-1] // block_size,
                                            indices[-2] // self.block_out,
                                            indices[-2] % self.block_out,
                                            indices[-1] % block_size),
            name="tensor_c_gm",
            tag="gemm",
            attrs={
                "batch_shape": shape_util.shape_to_list(self.ori_batch_shape_out),
                "unaligned_flag": self.unaligned_flag,
                "zero_flag": self.zero_flag,
                "pad_flag": self.pad_flag,
                "nz_fusion_flag": self.nz_fusion_flag,
                "nz_fusion_mode": self.nz_fusion_mode
            }
        )
        return tensor_c_gm

    # ----------- 610Lite func ---------- #
    def _handle_nd_output_ub(self, l0c_shape, tensor_mmad):
        """handle 5hd/nd output, handle 5hd/nd output ,fixpipe to ub => vmuls(nz2nd) => out
        Input:
            l0c_shape: list, shape_of l0c
            tensor_mmad: tensor, c_matrix on l0c
        ---------------------------------
        Return:
            tensor, nd tensor of fc
        """
        dtype_trans_map = {
            "int4": "S4",
            "int8": "B8",
            "float16": "F16",
            "float32": "F32",
            "int32": "S32",
            "bfloat16": "BF16"
        }
        op_dict = {
            "pre_conv": dtype_trans_map.get(tensor_mmad.dtype) + "2" + dtype_trans_map.get(self.dst_dtype)
        }
        fixpipe_tensor = tvm.compute(
            l0c_shape,
            lambda *indices: tvm.fixpipe_op(tensor_mmad(*indices), self.dst_dtype, op_dict=op_dict),
            name="fixpipe_matmul_ub",
            attrs={
                "vector_params": [],
                "vector_tensors": [],
                "nz2nd_flag": True,
                "anti_quant_flag": False
            })
        res_tag = self._get_ops_tag()
        attrs_dict = self._get_attrs_dict(fixpipe_tensor)
        shape_gemm = self._get_out_shape(fixpipe_tensor)
        attrs_dict["shape"] = shape_gemm
        if not self.cache_tiling_flag and attrs_dict.get("shape")[-1] % self.block_in != 0:
            tensor_gemm_nz2nd = self.compute_nz2nd(fixpipe_tensor, output_shape=shape_gemm, tensor_name="tensor_c_gm",
                                                   res_tag=res_tag, attrs_dict=attrs_dict)
        else:
            tensor_gemm_nz2nd = self.compute_nz2nd(fixpipe_tensor, tensor_name="tensor_nz2nd")
        tensor_c_gm = self._compute_res(tensor_gemm_nz2nd)
        return tensor_c_gm

    def _set_output_shape_for_int8(self, output_shape):
        need_fix_output_shape = (self.support_l0c2out and
                                 self.src_dtype == "int8" and self.format_out == "FRACTAL_NZ")
        if not need_fix_output_shape:
            return
        if in_dynamic():
            if self.trans_a:
                output_shape[-3] = get_te_var("m").get_tvm_var()
            if not self.trans_b:
                output_shape[-4] = get_te_var("n").get_tvm_var()
        else:
            if self.trans_a and self.shape_m_ori != 0:
                output_shape[-3] = int_ceil_div(self.shape_m_ori, self.block_in)
            if not self.trans_b and self.shape_n_ori != 0:
                output_shape[-4] = int_ceil_div(self.shape_n_ori, self.block_out)

    def _compute_res_milan(self, tensor_mmad):
        """
        compute c_gm from l0c matrix-
        """
        l0c_shape = shape_util.shape_to_list(tensor_mmad.shape)
        output_shape = l0c_shape
        self._set_output_shape_for_int8(output_shape)
        ori_shape = [self.shape_m_ori, self.shape_n_ori]
        if self.shape_a_info.have_batch() or self.shape_b_info.have_batch():
            ori_shape.insert(0, l0c_shape[0])
        attrs_dict = {
            "ori_shape": ori_shape,
            "batch_shape": shape_util.shape_to_list(self.ori_batch_shape_out),
            "format": self.format_out,
            "shape": output_shape,
            "zero_flag": self.zero_flag,
            "unaligned_flag": self.unaligned_flag
        }
        if self.format_out in ["NC1HWC0", "ND"]:
            # ND output
            if (tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd")):
                tensor_c_gm = self._handle_nd_output(l0c_shape, tensor_mmad)
            else:
                tensor_c_gm = self._handle_nd_output_ub(l0c_shape, tensor_mmad)
        elif self.dst_dtype == "float32" and self.src_dtype == "float32":
            output_shape[-4] = int_ceil_div(self.shape_n_ori, self.block_reduce)
            output_shape[-1] = self.block_reduce
            tensor_c_gm = tvm.compute(
                output_shape,
                lambda *indices: tensor_mmad(*indices[:-4],
                                                 (indices[-4] * self.block_reduce + indices[-1]) // self.block_out,
                                                 indices[-3],
                                                 indices[-2],
                                                 (indices[-4] * self.block_reduce + indices[-1]) %
                                                 self.block_out).astype(self.dst_dtype),
                tag="gemm",
                name="tensor_c_gm",
                attrs=attrs_dict)
        else:
            if self.mmad_mode == "gevm":
                # 610Lite branch, gevm mode
                output_shape = [output_shape[0], 1, 1, output_shape[3]]
            tensor_c_gm = tvm.compute(output_shape,
                                    lambda *indices: tensor_mmad(*indices).astype(self.dst_dtype),
                                    tag="gemm",
                                    name="tensor_c_gm",
                                    attrs=attrs_dict)

        return tensor_c_gm
