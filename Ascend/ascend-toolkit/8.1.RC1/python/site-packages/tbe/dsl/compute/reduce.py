#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
reduction compute
"""
import warnings
# 'pylint: disable=import-error
from functools import reduce as functools_reduce

from decorator import decorator
from tbe import tvm
from tbe.common.platform import ASCEND_310B
from tbe.common.platform import AS31XM1
from tbe.common.platform import ASCEND_610LITE
from tbe.common.platform import BS9SX2A
from tbe.common.platform import MC61AM21A
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.testing.dsl_source_info import source_info_decorator
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.base import d_format_util
from tbe.dsl.base import var_api
from tbe.dsl.base.record.decorators import reduce as reduce_decorator
from tbe.dsl.compute.constants import ComputeType

from .cast import _cast
from .math import vmuls
from .util import check_input_tensor_shape
from .util import dsl_support_dtype
from .util import dtype_check_decorator
from .util import in_dynamic_and_static_unify
from .util import is_cast_support
from .util import reduce_axis_check
from .util import refine_axis
from .util import shape_to_list
from .util import util_astype

_VALUE_MAP_IN_REDUCE_WINDOW = {
    "reduce_window_max": {
        "float16": -6.55e04,
        "float32": -3.4e38,
        "int32": -2147483648,
        "int16": -32768,
    }
}

NAME_INDEX = [0]
FRACTAL_SIZE = 16
INDICES_PAD_VALUE = 2147483647

REDUCE_FUNC_MAP = {
    "reduce_min": tvm.min,
    "reduce_max": tvm.max,
    "reduce_sum": tvm.sum,
    "reduce_all": tvm.reduce_all,
    "reduce_any": tvm.reduce_any,
    "reduce_prod": tvm.prod
}


def is_support_inf():
    return get_soc_spec(SHORT_SOC_VERSION) in (ASCEND_310B, AS31XM1, ASCEND_610LITE, ASCEND_910B, 
                                               ASCEND_910_93, BS9SX2A, MC61AM21A)


def is_true(expr, dict_args):
    """
    :param expr: condition
    :param dict_args: error message
    :return: RuntimeError
    """
    if not expr:
        raise RuntimeError(dict_args, get_error_message(dict_args))


# 'pylint: disable=too-many-branches
@decorator
def _para_check_of_reduce(func, *args, **kwargs):
    '''
    auto cast dectorator.
    Before calling elewise api, check the input tensor is supported by the intr.
    If not supported, casting the input tensor to supported dtype.
    Only static shape support auto cast.
    (On condition that the cast type is supported.
    If the cast type is not supported,raising a RuntimeError).
    '''
    intr = func.__name__

    if intr == "sum":
        intr = "reduce_sum"

    def _is_last_axis(shape, axis):
        local_axis = []
        for i in axis:
            new_axis = i
            if i < 0:
                new_axis = i + len(shape)
            local_axis.append(new_axis)

        return len(shape) - 1 in local_axis

    def _check_dynamic_dtype(raw_tensor, intr, supported_dtypes, is_last_axis):
        """
        check dtype for dynamic shape
        """
        if not is_last_axis:
            supported_dtypes.append("int32")

        dtype = raw_tensor.dtype

        is_true(dtype in supported_dtypes,
                {"errCode": "E90002",
                 "detailed_cause": "[%s] do not support [%s] in [%s] !" % (intr,
                                                                           dtype,
                                                                           get_soc_spec("SHORT_SOC_VERSION"))})

    if len(args) == 3 or len(args) == 4:
        is_true(isinstance(args[0], tvm.Tensor),
                {"errCode": "E90002",
                 "detailed_cause": "The first input type must be [%s], while type is [%s]" \
                                   % ('tvm.tensor', type(args[0]))})

        raw_tensor = args[0]
        axis = args[1]
        keepdims = args[2]
        priority_flag = False
        if len(args) == 4:
            priority_flag = args[3]

        if isinstance(axis, (tuple, list)):
            axis = axis
        else:
            axis = [axis]

        shape_len = len(raw_tensor.shape)
        axis = reduce_axis_check(shape_len, axis)

        is_last_axis = _is_last_axis(raw_tensor.shape, axis)

        supported_dtypes = dsl_support_dtype(intr)
        is_true(supported_dtypes,
                {"errCode": "E90002",
                 "detailed_cause": "[%s] is not supported!" % intr})
        # dynamic shape do not perform auto cast
        if in_dynamic_and_static_unify():
            _check_dynamic_dtype(raw_tensor, intr, supported_dtypes, is_last_axis)
            return func(raw_tensor, axis, keepdims)

        return func(raw_tensor, axis, keepdims)

    return func(*args, **kwargs)


# 'pylint: disable=redefined-builtin
@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_sum(raw_tensor, axis, keepdims=False):
    """
    calculate reduce_sum of raw_tensor, only support float16
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    keepdims : if true, retains reduced dimensions with length 1, default value is None
    Returns
    -------
    res : wrapped_tensor
    """
    return _single_reduce_op(raw_tensor, axis, "reduce_sum", keepdims)


@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_mean(raw_tensor, axis, keepdims=False):
    """
    calculate reduce_mean of raw_tensor
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    keepdims : if true, retains reduced dimensions with length 1, default value is False
    Returns
    -------
    res : wrapped_tensor
    """
    def _vdivs_compute(tmp_tensor):
        # this scalar is from shape reducing, it must be int type
        scalar_dtype = "int32"
        scalar_value = functools_reduce(lambda x, y: x * y, [shape[i] for i in axis], 1)
        lambda_func = lambda *indice: tmp_tensor(*indice) / util_astype(scalar_value, scalar_dtype)

        op_name = "elewise_single_VS_div"
        name = f"{op_name.split('_')[-1]}_{NAME_INDEX[0]}"
        NAME_INDEX[0] += 1

        with tvm.tag_scope(op_name):
            res_vdivs = tvm.compute(tmp_tensor.shape, lambda_func, name=name, attrs={"_type": ComputeType.ELEWISE})
        return res_vdivs

    is_true(axis is not None, {"errCode": "E90001", "detailed_cause": "The axis is None!"})
    check_input_tensor_shape(raw_tensor)

    shape = raw_tensor.shape

    if keepdims:
        axis = refine_axis(axis, shape)
        if not in_dynamic_and_static_unify():
            axis_for_loop = axis.copy()
            for index in axis_for_loop:
                if int(raw_tensor.shape[index]) == 1:
                    axis.remove(index)

    if not axis:
        res = _vdivs_compute(raw_tensor)
        return res

    # reduce_sum
    tmp_reduce_sum = reduce_sum(raw_tensor, axis, keepdims)
    # vdivs
    res = _vdivs_compute(tmp_reduce_sum)

    return res


@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_min(raw_tensor, axis, keepdims=False, impl_mode="high_performance"):
    """
    calculate reduce_min of raw_tensor, only support float16
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    keepdims : if true, retains reduced dimensions with length 1, default value is None
    Returns
    -------
    res : wrapped_tensor
    """
    return _single_reduce_op(raw_tensor, axis, "reduce_min", keepdims)


# 'pylint: disable=unused-argument
@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_max(raw_tensor, axis, keepdims=False, impl_mode="high_performance"):
    """
    calculate reduce_max of raw_tensor, only support float16
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    keepdims : if true, retains reduced dimensions with length 1, default value is None
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    priority_flag : supported 1(precision) and 0(performance)
    Returns
    -------
    res : wrapped_tensor
    """
    return _single_reduce_op(raw_tensor, axis, "reduce_max", keepdims)


@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_all(raw_tensor, axis, keepdims=False):
    """
    calculate reduce_all of raw_tensor
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    keepdims : if true, retains reduced dimensions with length 1, default value is False
    Returns
    -------
    res : wrapped_tensor
    """
    return _single_reduce_op(raw_tensor, axis, "reduce_all", keepdims)


@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_any(raw_tensor, axis, keepdims=False):
    """
    calculate reduce_any of raw_tensor
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    keepdims : if true, retains reduced dimensions with length 1, default value is False
    Returns
    -------
    res : wrapped_tensor
    """
    return _single_reduce_op(raw_tensor, axis, "reduce_any", keepdims)


@reduce_decorator.reduce
@source_info_decorator()
@_para_check_of_reduce
def reduce_prod(raw_tensor, axis, keepdims=False):
    """
    calculate reduce_prod of raw_tensor, only support float16
    Parameters
    ----------
    raw_tensor : wrapped_tensor or tvm.tensor
    axis : int
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    Returns
    -------
    res : wrapped_tensor
    """
    return _single_reduce_op(raw_tensor, axis, "reduce_prod", keepdims)


def _single_reduce_op(input_tensor,  # 'pylint: disable=too-many-statements
                      axis, in_op, keepdims=False):
    """
    factory method of single reduce operations
    keepdims : if true, retains reduced dimensions with length 1, default value is None
    """
    is_true(axis is not None, {"errCode": "E90001", "detailed_cause": "The axis is None!"})

    check_input_tensor_shape(input_tensor)

    def __reduce_compute(data_shape, axis, tensor, func):
        def compute_func(*indice):
            count_indice = 0
            count_reduce = 0
            res_indice = []
            for index in range(len(data_shape)):
                if index not in axis:
                    res_indice.append(indice[count_indice])
                    count_indice += 1
                else:
                    res_indice.append(reduce_axises[count_reduce])
                    count_reduce += 1
                    if keepdims:
                        count_indice += 1

            return func(tensor(*res_indice), axis=reduce_axises)

        reduce_axises = []
        for index, axis_num in enumerate(axis):
            reduce_axises.append(
                tvm.reduce_axis((0, data_shape[axis_num]),
                                name='k' + str(index + 1)))
        res_reshape = []
        for index, shape_l in enumerate(data_shape):
            if index not in axis:
                res_reshape.append(shape_l)
            else:
                if keepdims:
                    r_dim = var_api.const(1)
                    if d_format_util.in_axis_type(shape_l, ["C1", "C0"]):
                        ori_c = var_api.const(1)
                        d_format_util.set_axis_type(ori_c, "C")
                        d_format_util.set_original(r_dim, ori_c)
                    for k, v in var_api.get_annotation(shape_l).items():
                        if k not in var_api.get_attr_keys(r_dim):
                            var_api.set_attr(r_dim, k, v)
                    res_reshape.append(r_dim)

        # all axis reduce, the dim is 1
        if not res_reshape:
            res_reshape.append(1)

        name = "reduce_" + str(NAME_INDEX[0])
        NAME_INDEX[0] += 1

        reduce_res = tvm.compute(res_reshape, compute_func, name=name,
                                 attrs={"_type": ComputeType.REDUCE, "_axes": axis})
        return reduce_res

    def __get_reduce_fun(in_op):
        if in_op.lower() not in REDUCE_FUNC_MAP.keys():
            dict_args = {}
            dict_args["errCode"] = "E90003"
            dict_args["detailed_cause"] = "Not Support yet for op [%s]. " \
                                          "in_op must be reduce_min, " \
                                          "reduce_max, reduce_sum, " \
                                          "reduce_all, reduce_any or reduce_prod" % in_op
            raise RuntimeError(dict_args, get_error_message(dict_args))
        return REDUCE_FUNC_MAP.get(in_op.lower())

    reduce_func = __get_reduce_fun(in_op)

    op_tensor = input_tensor
    shape = op_tensor.shape
    res_axis = refine_axis(axis, shape)

    if keepdims:
        axis = res_axis[:]
        if not in_dynamic_and_static_unify():
            axis_for_loop = axis.copy()
            for index in axis_for_loop:
                if int(input_tensor.shape[index]) == 1:
                    axis.remove(index)
    if not axis:
        res = vmuls(input_tensor, tvm.const(1, dtype=input_tensor.dtype))
        return res

    for i in axis:
        is_last_axis = (i == (len(shape) - 1))
        if is_last_axis:
            break

    with tvm.tag_scope(in_op.lower()):
        res = __reduce_compute(shape, axis, op_tensor, reduce_func)

    return res


@decorator
def _auto_cast_of_tuple_reduce(func, *args, **kwargs):
    '''
    auto cast dectorator.
    Before calling elewise api, check the input tensor is supported by the intr.
    If not supported, casting the input tensor to supported dtype.
    (On condition that the cast type is supported.
    If the cast type is not supported,raising a RuntimeError).
    '''
    func_name = func.__name__
    supported_types = ("float16", "float32")
    is_true(func_name == "tuple_sum", {"errCode": "E90001",
                                       "detailed_cause": "function name [%s] must be tuple_sum" % func_name})

    def _is_last_axis(shape, axis):
        if isinstance(axis, (tuple, list)):
            local_axis = axis
        else:
            local_axis = [axis]
        return len(shape) - 1 in local_axis

    def _check_tensor(tensor_list):
        is_true(len(tensor_list) == 2, {"errCode": "E90001",
                                        "detailed_cause": "Tuple reduce input tensors must be 2. " \
                                                          "while is [%s]" % len(tensor_list)
                                        })
        shape1 = shape_to_list(tensor_list[0].shape)
        shape2 = shape_to_list(tensor_list[1].shape)
        is_true(shape1 == shape2, {"errCode": "E90001",
                                   "detailed_cause": "Tuple reduce input tensors must " \
                                                     "have same shape. while shape1 is [%s], " \
                                                     "shape2 is [%s]" % (shape1, shape2)})

    def _deal_tensor_dtype(raw_tensor, supported_types):
        dtype = raw_tensor.dtype
        if func_name == "tuple_sum" and not _is_last_axis(raw_tensor.shape,
                                                          axis):
            supported_types = supported_types + ("int32",)
        dealed_tensor = raw_tensor
        if dtype not in supported_types:
            if "float32" in supported_types and is_cast_support(dtype,
                                                                "float32"):
                dealed_tensor = _cast(raw_tensor, "float32")
            else:
                dealed_tensor = _cast(raw_tensor, "float16")
        return dealed_tensor

    if len(args) == 3:
        is_true(isinstance(args[0], (tuple, list)), {"errCode": "E90001",
                                                     "detailed_cause": "The first input type must be list" \
                                                                       " or tuple, while type is [%s]" % type(args[0])})
        raw_tensor_list = args[0]
        axis = args[1]
        keepdims = args[2]

        _check_tensor(raw_tensor_list)

        temp_tensor_list = []
        for raw_tensor in raw_tensor_list:
            temp_tensor = _deal_tensor_dtype(raw_tensor, supported_types)
            temp_tensor_list.append(temp_tensor)

        return func(temp_tensor_list, axis, keepdims)

    return func(*args, **kwargs)


@source_info_decorator()
@_auto_cast_of_tuple_reduce
def tuple_sum(input_tensor_list, axis, keepdims=False):
    """
    calculate sum of raw_tensor, only support float16
    Parameters
    ----------
    input_tensor_list : wrapped_tensor or tvm.tensor list that each tensor has same reduce operation
    axis : int or list
        reduce axis (range : [-len(raw_tensor.shape), len(raw_tensor.shape) - 1])
    keepdims : if true, retains reduced dimensions with length 1, default value is None
    Returns
    -------
    res : wrapped_tensor
    """
    return _tuple_reduce_op(input_tensor_list, axis, "tuple_reduce_sum",
                            keepdims)


def _tuple_reduce_op(input_tensor_list, axis, in_op, keepdims=False):
    """
    factory method of tuple reduce operations
    keepdims : if true, retains reduced dimensions with length 1, default value is None
    """
    is_true(axis is not None, {"errCode": "E90001", "detailed_cause": "The axis is None!"})

    check_input_tensor_shape(input_tensor_list[0])

    if axis in ((), []):
        res = []
        for tensor in input_tensor_list:
            temp_res = vmuls(tensor, tvm.const(1, dtype=tensor.dtype))
            res.append(temp_res)
        return res

    def __tuple_reduce_compute(data_shape, axis, tensor_list, func):
        def compute_func(*indice):
            """
            compute_func
            """
            count_indice = 0
            count_reduce = 0
            res_indice = []
            for index in range(len(data_shape)):
                if index not in axis:
                    res_indice.append(indice[count_indice])
                    count_indice += 1
                else:
                    res_indice.append(reduce_axises[count_reduce])
                    count_reduce += 1
                    if keepdims:
                        count_indice += 1

            return func(
                (tensor_list[0](*res_indice), tensor_list[1](*res_indice)),
                axis=reduce_axises)

        reduce_axises = []
        for index, axis_num in enumerate(axis):
            reduce_axises.append(
                tvm.reduce_axis((0, data_shape[axis_num]),
                                name='k' + str(index + 1)))
        res_reshape = []
        for index, shape_l in enumerate(data_shape):
            if index not in axis:
                res_reshape.append(shape_l)
            else:
                if keepdims:
                    res_reshape.append(1)

        # all axis reduce, the dim is 1
        if not res_reshape:
            res_reshape.append(1)

        name = "reduce_" + str(NAME_INDEX[0])
        NAME_INDEX[0] += 1

        reduce_res = tvm.compute(res_reshape, compute_func, name=name)
        return reduce_res

    tuple_sum_func = tvm.comm_reducer(lambda x, y: (x[0] + y[0], x[1] + y[1]),
                                      lambda t0, t1: (tvm.const(0, dtype=t0),
                                                      tvm.const(0, dtype=t1)),
                                      name="tuple_sum")

    if in_op.lower() == "tuple_reduce_sum":
        reduce_func = tuple_sum_func
    else:
        dict_args = {}
        dict_args["errCode"] = "E90003"
        dict_args["detailed_cause"] = "Not Support yet for op [%s], " \
                                      "in_op must be tuple_reduce_sum" % in_op
        raise RuntimeError(dict_args, get_error_message(dict_args))

    op_tensor = input_tensor_list[0]
    shape = op_tensor.shape
    res_axis = refine_axis(axis, shape)
    for i in res_axis:
        is_last_axis = (i == (len(shape) - 1))
        if is_last_axis:
            break

    with tvm.tag_scope(in_op.lower()):
        res = __tuple_reduce_compute(shape, res_axis, input_tensor_list,
                                     reduce_func)

    return res


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _reduce_window_param_check(input_tensor, reduction, window_axes, window_dimensions, window_strides,
                               window_dilations, padding_mode, padding_dimensions, rounding_mode, return_indices):
    """
    param check
    """
    is_true(isinstance(input_tensor, tvm.Tensor),
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, input_tensor must be a tensor."})

    is_true(reduction in ("MAX", "GMP"),
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, reduction must be in ('MAX', 'GMP')."})

    is_true(isinstance(window_axes, (list, tuple)) and isinstance(window_dimensions, (list, tuple)) and
            isinstance(window_strides, (list, tuple)) and isinstance(window_dilations, (list, tuple)),
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, window_axes/window_dimensions/window_strides/window_dilations " \
                               "must be a list or a tuple."})

    is_true(len(window_axes) == len(window_dimensions) == len(window_strides) == len(window_dilations) and \
            len(window_axes) > 0,
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, length of window_axes/window_dimensions/window_strides/" \
                               "window_dilations must be the same and larger than 0."})

    is_true(padding_mode in ("SAME", "VALID", "CALCULATED"),
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, padding_mode must be in ('SAME', 'VALID', 'CALCULATED')"})

    if padding_dimensions is not None:
        is_true(isinstance(padding_dimensions, (list, tuple)) and len(padding_dimensions) == len(window_axes),
                {"errCode": "E90001",
                 "detailed_cause": "In reduce_window, padding_dimensions if not None, it must be " \
                                   "a list[list] or a tuple[tuple] and its length is equal to window_axes."})
        for sub_dimensions in padding_dimensions:
            is_true(isinstance(sub_dimensions, (list, tuple)) and len(sub_dimensions) == 2,
                    {"errCode": "E90001",
                     "detailed_cause": "In reduce_window, element in padding_dimensions must be " \
                                       "a list or a tuple, which length must be 2."})

    is_true(rounding_mode in ("CEIL", "FLOOR"),
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, rounding_mode must be in ('CEIL', 'FLOOR')"})

    is_true(isinstance(return_indices, bool),
            {"errCode": "E90001",
             "detailed_cause": "In reduce_window, return_indices must be a bool."})


def _get_reduce_op(reduction):
    """
    get reduce op according to reduction
        MAX -> reduce_window_max
    """
    reduce_op = None
    if reduction in ("MAX", "GMP"):
        reduce_op = "reduce_window_max"

    return reduce_op


def _init_padding_dimensions(padding_dimensions, window_length):
    """
    init padding dimensions
    if padding_dimensions is None, padding_dimensions is [0, 0] * window_length
    """
    return padding_dimensions if padding_dimensions else [[0, 0] for _ in range(window_length)]


def _refine_window_axis(axes, input_shape):
    """
    convert negative value in window_axes to positive: -1 -> -1 + shape_len
    """
    res_axis = []
    shape_len = len(input_shape)
    for i in axes:
        if i < 0:
            refine_axes = shape_len + i
        else:
            refine_axes = i
        is_true(0 <= refine_axes < shape_len,
                {"errCode": "E90001",
                 "detailed_cause": f"window axis {refine_axes} must less than {shape_len} and larger than zero!"})
        res_axis.append(refine_axes)

    return res_axis


def _process_global_reduce(input_tensor, window_axes):
    """
    process global reduce
    1. padding_mode -> "VALID"
    2. window_dimensions -> input_dim
    3. window_strides -> input_dim
    4. window_dilations -> 1
    """
    local_window_dimensions = []
    local_window_strides = []
    local_window_dilations = []
    local_padding_mode = "VALID"

    for window_axis in window_axes:
        local_window_dimensions.append(input_tensor.shape[window_axis])
        local_window_strides.append(input_tensor.shape[window_axis])
        local_window_dilations.append(1)

    res = (local_window_dimensions, local_window_strides, local_window_dilations, local_padding_mode)

    return res


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _get_output(input_size, ksize, stride, dilation, padding_mode, padding, rounding_mode):
    """
    get output shape
    """
    if padding_mode == "CALCULATED":
        return _get_output_in_calculated_mode(input_size, ksize, stride, dilation, padding, rounding_mode)
    elif padding_mode == "SAME":
        return _get_output_in_same_mode(input_size, stride)
    # VALID mode
    return _get_output_in_valid_mode(input_size, ksize, stride, dilation)


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _get_output_in_calculated_mode(input_size, ksize, stride, dilation, padding, rounding_mode):
    """
    get output shape when padding_mode is calculated
    """
    if rounding_mode == "FLOOR":
        output_size = tvm.div(input_size + padding[0] + padding[1] - (dilation * (ksize - 1) + 1), stride) + 1
    else:
        output_size = tvm.div((input_size + padding[0] + padding[1] - (dilation * (ksize - 1) + 1)) + stride - 1,
                              stride) + 1
        output_size = tvm.select((output_size - 1) * stride >= input_size + padding[0], output_size - 1, output_size)

    return output_size


def _get_output_in_same_mode(input_size, stride):
    """
    get output shape when padding_mode is same
    """
    output_size = tvm.div(input_size + stride - 1, stride)

    return output_size


def _get_output_in_valid_mode(input_size, ksize, stride, dilation):
    """
    get output shape when padding_mode is valid
    """
    output_size = tvm.div(input_size - (dilation * (ksize - 1) + 1) + 1 + (stride - 1), stride)

    return output_size


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _get_padding(input_size, output_size, ksize, stride, dilation, padding_mode, padding, rounding_mode):
    """
    get paddings
    """
    if padding_mode == "CALCULATED":
        return _get_padding_in_calculated_mode(input_size, output_size, ksize, stride,
                                               dilation, padding, rounding_mode)
    elif padding_mode == "SAME":
        return _get_padding_in_same_mode(input_size, output_size, ksize, stride, dilation)
    # VALID mode
    return [0, 0]


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _get_padding_in_calculated_mode(input_size, output_size, ksize, stride, dilation, padding, rounding_mode):
    """
    get paddings when padding_mode is calculated
    """
    if rounding_mode == "FLOOR":
        return padding

    first_pad_dimension = padding[0]
    total_pad_dimension = (output_size - 1) * stride + ((ksize - 1) * dilation + 1) - input_size
    total_pad_dimension = tvm.max(total_pad_dimension, 0)
    second_pad_dimension = tvm.max(total_pad_dimension - first_pad_dimension, 0)

    return [first_pad_dimension, second_pad_dimension]


def _get_padding_in_same_mode(input_size, output_size, ksize, stride, dilation):
    """
    get paddings when padding_mode is same
    """
    total_pad_dimension = (output_size - 1) * stride + ((ksize - 1) * dilation + 1) - input_size
    total_pad_dimension = tvm.max(total_pad_dimension, 0)
    first_pad_dimension = tvm.div(total_pad_dimension, 2)
    second_pad_dimension = tvm.div(total_pad_dimension + 1, 2)

    return [first_pad_dimension, second_pad_dimension]


def _pad(reduce_op, input_tensor, after_pad_shape, window_axes, actual_padding_dimensions):
    """
    pad compute
    """

    def __pad_compute_func(indices, pad_value):
        conditions = []
        input_indices = []
        for index, dim in enumerate(indices):
            if index in window_axes:
                window_index = window_axes.index(index)
                first_pad_dimension, second_pad_dimension = actual_padding_dimensions[window_index]
                if first_pad_dimension != 0:
                    conditions.append(dim > first_pad_dimension - 1)
                if second_pad_dimension != 0:
                    conditions.append(dim < after_pad_shape[index] - second_pad_dimension)
                input_indices.append(dim - first_pad_dimension)
            else:
                input_indices.append(dim)

        if not conditions:
            return input_tensor(*input_indices)

        return tvm.select(tvm.all(*conditions), input_tensor(*input_indices), pad_value)

    dtype = input_tensor.dtype
    pad_value = float("-inf") if is_support_inf() else _VALUE_MAP_IN_REDUCE_WINDOW.get(reduce_op).get(dtype)
    name = f"pad_window_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    pad = tvm.compute(after_pad_shape, lambda *i: __pad_compute_func(i, tvm.const(pad_value, dtype=dtype)), name=name)

    return pad


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _reduce(reduce_op, input_tensor, after_reduce_shape, window_axes, window_dimensions,
            window_strides, window_dilations):
    """
    reduce window compute
    """

    def __get_reduce_func_name(reduce_op):
        reduce_func = None
        if reduce_op == "reduce_window_max":
            reduce_func = tvm.max

        return reduce_func

    def __reduce_compute_func(indices, reduce_func_name):
        reduce_axes = []
        input_indices = []
        for index, dim in enumerate(indices):
            if index in window_axes:
                window_index = window_axes.index(index)
                ksize = window_dimensions[window_index]
                stride = window_strides[window_index]
                dilation = window_dilations[window_index]
                reduce_axis = tvm.reduce_axis((0, ksize), name=f"k{index}")
                reduce_axes.append(reduce_axis)
                input_indices.append(dim * stride + dilation * reduce_axis)
            else:
                input_indices.append(dim)

        return reduce_func_name(input_tensor(*input_indices), axis=reduce_axes)

    reduce_func_name = __get_reduce_func_name(reduce_op)
    name = f"reduce_window_op_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    after_reduce = tvm.compute(after_reduce_shape,
                               lambda *i: __reduce_compute_func(i, reduce_func_name),
                               name=name,
                               attrs={"window_axes": window_axes}
                               )

    return after_reduce


def _img_pad(shape_pad, src, pads, pad_value):
    pt, pb, pl, pr = pads
    n, c1, hi, wi, c0 = src.shape

    def _pad(indices):
        _in_n, _in_c1, _in_h, _in_w, _in_c0 = indices
        condition = tvm.any(
            _in_h < pt, _in_h > hi + pt - 1,
            _in_w < pl, _in_w > wi + pl - 1,
        )
        return tvm.select(condition, pad_value, src[_in_n, _in_c1, _in_h - pt, _in_w - pl, _in_c0])

    compute_name = f"x_pad_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    return tvm.compute(shape_pad, lambda *i: _pad(i), name=compute_name)


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _img2col_fractal(src, indices, kernels, strides, dilates, out_shape, pad_value):
    kh, kw = kernels
    sh, sw = strides
    dh, dw = dilates
    ho, wo = out_shape
    hwo = ho * wo

    _n, _c1, _khw, _ceil, _block, _c0 = indices
    _real_hw = _ceil * FRACTAL_SIZE + _block

    _kh = _khw // kw
    _kw = _khw % kw
    _ho = _real_hw // wo
    _wo = _real_hw % wo

    _in_h = _ho * sh + _kh * dh
    _in_w = _wo * sw + _kw * dw

    cond = _real_hw > hwo - 1
    return tvm.select(cond, pad_value, src[_n, _c1, _in_h, _in_w, _c0])


def _reduce_max(indices, src, reduce_axis):
    _n, _c1, _k, _ceil, _block, _c0 = indices
    return tvm.max(src[_n, _c1, reduce_axis, _ceil, _block, _c0], (reduce_axis,))


def _depad(indices, src):
    _n, _c1, _k, _hwo, _c0 = indices
    _ceil = _hwo // FRACTAL_SIZE
    _block = _hwo % FRACTAL_SIZE
    return src[_n, _c1, _k, _ceil, _block, _c0]


def _is_const_value(values):
    result = True
    for _v in values:
        result = result and (isinstance(_v, (int, tvm.tir.IntImm)))
    return result


def _get_format(input_len, window_axes):
    if input_len == 5 and tuple(window_axes) == (2, 3):
        return "NC1HWC0"
    return "ND"


def _generate_shape_info(window_shape, kernels, pads, strides, ceil_mode):
    h_in, w_in = window_shape
    kh, kw = kernels
    pt, pb, pl, pr = pads
    dh, dw = 1, 1
    sh, sw = strides
    if ceil_mode == "FLOOR":
        pad_h = pt + h_in + pb
        pad_w = pl + w_in + pr
        out_h = tvm.div(pad_h - dh * (kh - 1) - 1 + sh, sh)
        out_w = tvm.div(pad_w - dw * (kw - 1) - 1 + sw, sw)
    else:
        _value_list = [h_in, pt, pb, dh, kh, sh, w_in, pl, pr, dw, kw, sw]
        if _is_const_value(_value_list):
            tmp_out_h = (pt + h_in.value + pb - 1 + sh - dh * (kh - 1) - 1 + sh) // sh
            out_h = tmp_out_h - 1 if ((tmp_out_h - 1) * sh) >= (h_in.value + pt) else tmp_out_h
            pad_h = pt + h_in.value + pb - 1 + sh
            if ((tmp_out_h - 1) * sh) >= (h_in.value + pt):
                pad_h = pt + h_in.value + pb
            tmp_out_w = (pl + w_in.value + pr - 1 + sw - dw * (kw - 1) - 1 + sw) // sw
            out_w = tmp_out_w - 1 if ((tmp_out_w - 1) * sw) >= (w_in.value + pl) else tmp_out_w
            pad_w = pl + w_in.value + pr - 1 + sw
            if ((tmp_out_w - 1) * sw) >= (w_in.value + pl):
                pad_w = pl + w_in.value + pr
        else:
            tmp_out_h = tvm.div(pt + h_in + pb - 1 + sh - dh * (kh - 1) - 1 + sh, sh)
            out_h = tvm.select(((tmp_out_h - 1) * sh) >= (h_in + pt), tmp_out_h - 1, tmp_out_h)
            pad_h = tvm.select(((tmp_out_h - 1) * sh) >= (h_in + pt),
                               pt + h_in + pb, pt + h_in + pb - 1 + sh)
            tmp_out_w = tvm.div(pl + w_in + pr - 1 + sw - dw * (kw - 1) - 1 + sw, sw)
            out_w = tvm.select(((tmp_out_w - 1) * sw) >= (w_in + pl), tmp_out_w - 1, tmp_out_w)
            pad_w = tvm.select(((tmp_out_w - 1) * sw) >= (w_in + pl),
                               pl + w_in + pr, pl + w_in + pr - 1 + sw)
    pad_window = [pad_h, pad_w]
    out_window = [out_h, out_w]
    return pad_window, out_window


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
def _reduce_window_with_indices(input_tensor, reduction, window_axes, window_dimensions, window_strides,
                                window_dilations, padding_dimensions, ceil_mode):
    pads = [padding_dimensions[0][0], padding_dimensions[0][1], padding_dimensions[1][0], padding_dimensions[1][1]]
    input_window = [input_tensor.shape[window_axes[0]], input_tensor.shape[window_axes[1]]]
    pad_window, out_window = _generate_shape_info(input_window, window_dimensions, pads, window_strides, ceil_mode)
    input_dtype = input_tensor.dtype
    _mini_value = float("-inf") if is_support_inf() else _VALUE_MAP_IN_REDUCE_WINDOW.get(reduction).get(input_dtype)
    pad_value = tvm.const(_mini_value, dtype=input_dtype)

    window_info = {
        "window_axes": window_axes,
        "window_dimensions": window_dimensions,
        "window_strides": window_strides,
        "window_dilations": window_dilations,
        "pads": pads,
        "pad_window": pad_window,
        "out_window": out_window,
        "pad_value": pad_value,
    }
    if _get_format(len(input_tensor.shape), window_axes) == "ND":
        return _reduce_with_indices_nd(input_tensor, window_info)

    return _reduce_with_indices_5hd(input_tensor, window_info)


def _reduce_with_indices_nd(input_tensor, window_info):
    input_shape = input_tensor.shape
    h_axis, w_axis = window_info.get("window_axes")
    h_in, w_in = input_shape[h_axis], input_shape[w_axis]
    kh, kw = window_info.get("window_dimensions")
    sh, sw = window_info.get("window_strides")
    dh, dw = window_info.get("window_dilations")
    pt, _, pl, _ = window_info.get("pads")
    shape_pad = input_shape[:h_axis] + window_info.get("pad_window") + input_shape[w_axis + 1:]
    shape_expand = input_shape[:h_axis] + [kh * kw] + window_info.get("out_window") + input_shape[w_axis + 1:]
    shape_reduce = input_shape[:h_axis] + [1] + window_info.get("out_window") + input_shape[w_axis + 1:]
    output_kernel_axis = h_axis
    output_h_axis = h_axis + 1
    output_w_axis = h_axis + 2

    def _pad(indices, src, pad_value):
        _in_nc, _in_h, _in_w = indices
        condition = tvm.any(_in_h < pt, _in_h > h_in + pt - 1, _in_w < pl, _in_w > w_in + pl - 1)
        return tvm.select(condition, pad_value, src[_in_nc, _in_h - pt, _in_w - pl])

    compute_name = f"pooling_with_arg_pad_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_pad"):
        x_pad = tvm.compute(shape_pad, lambda *i: _pad(i, input_tensor, window_info.get("pad_value")),
                            name=compute_name)

    compute_name = f"pooling_with_arg_expand_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_expand"):
        x_expand = tvm.compute(shape_expand,
                               lambda *i: x_pad(*i[:output_kernel_axis],
                                                i[output_h_axis] * sh + (i[output_kernel_axis] // kw) * dh,
                                                i[output_w_axis] * sw + (i[output_kernel_axis] % kw) * dw,
                                                *i[output_w_axis + 1:]),
                               name=compute_name, attrs={"workspace": "scope_gm"})

    compute_name = f"x_reduce_axis_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    x_reduce_axis = tvm.reduce_axis((0, kh * kw), name=compute_name)
    compute_name = f"pooling_with_arg_reduce_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_max"):
        x_reduce = tvm.compute(shape_reduce,
                               lambda *i: tvm.max(x_expand(*i[:output_kernel_axis],
                                                           x_reduce_axis,
                                                           i[output_h_axis],
                                                           i[output_w_axis],
                                                           *i[output_w_axis + 1:]), (x_reduce_axis,)),
                               name=compute_name)

    compute_name = f"pooling_with_arg_vcmp_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_vcmp"):
        x_vcmp = tvm.compute(shape_expand,
                             lambda *i: (tvm.expr.EQ(x_expand(*i), x_reduce(*i[:output_kernel_axis],
                                                                            0,
                                                                            i[output_h_axis],
                                                                            i[output_w_axis],
                                                                            *i[output_w_axis + 1:]))).astype("uint1"),
                             name=compute_name)

    compute_name = f"pooling_with_arg_indices_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_indices"):
        indices_x = tvm.compute(input_shape, lambda *i: (i[1] * input_shape[-1] + i[2]).astype("int32"),
                               name=compute_name)

    compute_name = f"pooling_with_arg_indices_pad_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_pad"):
        indices_pad = tvm.compute(shape_pad, lambda *i:
        _pad(i, indices_x, tvm.const(INDICES_PAD_VALUE, dtype="int32")), name=compute_name)

    compute_name = f"pooling_with_arg_indices_expand_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_expand"):
        indices_expand = tvm.compute(shape_expand,
                                    lambda *i: indices_pad(*i[:output_kernel_axis],
                                                          i[output_h_axis] * sh + (i[output_kernel_axis] // kw) * dh,
                                                          i[output_w_axis] * sw + (i[output_kernel_axis] % kw) * dw,
                                                          *i[output_w_axis + 1:]),
                                    name=compute_name)

    compute_name = f"pooling_with_arg_indices_select_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_select"):
        indices_select = tvm.compute(shape_expand,
                                     lambda *i:
                                     tvm.select(x_vcmp(*i), indices_expand(*i),
                                                tvm.const(INDICES_PAD_VALUE, dtype="int32")),
                                     name=compute_name)

    compute_name = f"x_reduce_axis_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    indices_reduce_axis = tvm.reduce_axis((0, kh * kw), name=compute_name)
    compute_name = f"pooling_with_arg_min_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_min"):
        indices_reduce = tvm.compute(shape_reduce,
                                     lambda *i: tvm.min(indices_select(*i[:output_kernel_axis],
                                                                       indices_reduce_axis,
                                                                       i[output_h_axis],
                                                                       i[output_w_axis],
                                                                       *i[output_w_axis + 1:]),
                                                        (indices_reduce_axis)),
                                     name=compute_name)
    return [x_reduce, indices_reduce]


def _reduce_with_indices_5hd(input_tensor, window_info):
    # kernels, strides, dilats, pads, pad_window, out_window, pad_value
    n, c1, h_in, w_in, c0 = input_tensor.shape
    kernels = window_info.get("window_dimensions")
    khw = kernels[0] * kernels[1]
    pad_h, pad_w = window_info.get("pad_window")
    out_h, out_w = window_info.get("out_window")
    pad_shape = [n, c1, pad_h, pad_w, c0]
    fractal_shape = [n, c1, khw, (out_h * out_w + FRACTAL_SIZE - 1) // FRACTAL_SIZE, FRACTAL_SIZE, c0]
    reduce_shape = [n, c1, 1, (out_h * out_w + FRACTAL_SIZE - 1) // FRACTAL_SIZE, FRACTAL_SIZE, c0]
    depad_shape = [n, c1, 1, out_h * out_w, c0]
    mask_shape = [(out_h * out_w + FRACTAL_SIZE - 1) // FRACTAL_SIZE, FRACTAL_SIZE, c0]

    with tvm.tag_scope("pooling_with_arg_pad"):
        x_pad = _img_pad(pad_shape, input_tensor, window_info.get("pads"), window_info.get("pad_value"))

    compute_name = f"img2col_fractal_v2_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_fractal"):
        x_fractal = tvm.compute(fractal_shape, lambda *i: _img2col_fractal(x_pad, i, kernels,
                                                                           window_info.get("window_strides"),
                                                                           window_info.get("window_dilations"),
                                                                           window_info.get("out_window"),
                                                                           window_info.get("pad_value")),
                                name=compute_name, attrs={"hwo": out_h * out_w, "wo": out_w, "ho": out_h})

    axis_name = f"reduce_axis_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    reduce_axis = tvm.reduce_axis((0, khw), name=axis_name)
    compute_name = f"reduce_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_reduce"):
        x_reduce = tvm.compute(reduce_shape, lambda *i: _reduce_max(i, x_fractal, reduce_axis), name=compute_name)

    compute_name = f"depad_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_depad"):
        x_depad = tvm.compute(depad_shape, lambda *i: _depad(i, x_reduce), name=compute_name)

    compute_name = f"vcmp_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_vcmp"):
        x_vcmp = tvm.compute(fractal_shape, lambda *i: (
            tvm.expr.EQ(x_fractal(*i), x_reduce(i[0], i[1], 0, i[3], i[4], i[5]))).astype("uint1"), name=compute_name)

    compute_name = f"mask_not_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_vdep"):
        x_mask_not = tvm.compute(mask_shape, lambda *i: (tvm.const(1, dtype="int8")).astype("uint1"), name=compute_name)

    compute_name = f"mask_out_{NAME_INDEX[0]}"
    NAME_INDEX[0] += 1
    with tvm.tag_scope("pooling_with_arg_vand"):
        x_mask_and = tvm.compute(fractal_shape, lambda _n, _c1, _khw, _f, _b, _c0: (
                x_vcmp[_n, _c1, _khw, _f, _b, _c0] & x_mask_not[_f, _b, _c0]), name=compute_name)

    return [x_depad, x_mask_and]


# 'pylint: disable=too-many-arguments,huawei-too-many-arguments
@source_info_decorator()
@dtype_check_decorator
def reduce_window(input_tensor, reduction, window_axes, window_dimensions, window_strides, window_dilations,
                  padding_mode="SAME", padding_dimensions=None, rounding_mode="FLOOR", return_indices=False):
    """
    calculate reduce_window of input

    Parameters
    ----------
    input_tensor : tvm.tensor
        Original tensor
    reduction : str
        Type of reduction, only support "MAX" and "GMP"(AVG, GAP are not supported yet)
    window_axes : list or tuple
        Indices of window, only support
        Range : [-len(input.shape), len(input.shape) - 1]
    window_dimensions : list or tuple
        Dimensions of window, length is equal to window_axes
    window_strides : list or tuple
        Strides of window, length is equal to window_axes
    window_dilations : list or tuple
        Dilations of window, length is equal to window_axes
    padding_mode : str
        Mode of padding, only support "SAME", "VALID" and "CALCULATED"
    padding_dimensions : list[list] or tuple[tuple]
        Dimensions of padding on all window sides and one window side corresponds to two dimensions
        This parameter is valid only when padding_mode is "CALCULATED"
        None means padding dimensions are all zero
    rounding_mode : str
        Mode of rounding in output shape computation, only support "FLOOR"(defalut) and "CEIL"
        This parameter is valid only when padding_mode is "CALCULATED"
    return_indices : bool
        Whether to return the max indices along with the outputs, default value is false
        This parameter is valid only when reduction is "MAX"

    Returns
    -------
    res : wrapped_tensor
    """
    _reduce_window_param_check(input_tensor, reduction, window_axes, window_dimensions, window_strides,
                               window_dilations, padding_mode, padding_dimensions, rounding_mode, return_indices)
    reduce_op = _get_reduce_op(reduction)

    if return_indices:
        return _reduce_window_with_indices(input_tensor, reduce_op, window_axes, window_dimensions, window_strides,
                                           window_dilations, padding_dimensions, rounding_mode)

    ori_padding_dimensions = _init_padding_dimensions(padding_dimensions, len(window_axes))
    window_axes = _refine_window_axis(window_axes, input_tensor.shape)

    if reduction == "GMP":
        window_dimensions, window_strides, window_dilations, padding_mode = \
            _process_global_reduce(input_tensor, window_axes)

    actual_padding_dimensions = []
    after_pad_shape = []
    after_reduce_shape = []

    for input_index, input_size in enumerate(input_tensor.shape):
        if input_index in window_axes:
            window_index = window_axes.index(input_index)
            ksize = window_dimensions[window_index]
            stride = window_strides[window_index]
            dilation = window_dilations[window_index]
            padding = ori_padding_dimensions[window_index]

            output_size = _get_output(input_size, ksize, stride, dilation, padding_mode, padding, rounding_mode)
            actual_padding = _get_padding(input_size, output_size, ksize, stride, dilation,
                                          padding_mode, padding, rounding_mode)

            actual_padding_dimensions.append(actual_padding)
            after_pad_shape.append(input_size + actual_padding[0] + actual_padding[1])
            after_reduce_shape.append(output_size)
        else:
            after_pad_shape.append(input_size)
            after_reduce_shape.append(input_size)

    with tvm.tag_scope("pad_window"):
        pad_tensor = _pad(reduce_op, input_tensor, after_pad_shape, window_axes, actual_padding_dimensions)

    with tvm.tag_scope(reduce_op.lower()):
        res = _reduce(reduce_op, pad_tensor, after_reduce_shape, window_axes, window_dimensions,
                      window_strides, window_dilations)

    return res
