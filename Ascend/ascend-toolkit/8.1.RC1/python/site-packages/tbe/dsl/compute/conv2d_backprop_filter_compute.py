#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d backprop filter DSL interface.
"""
from __future__ import absolute_import
from __future__ import print_function

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.common.utils import conv_util
from tbe.common.utils import para_check
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.base.operation import is_unify
from tbe.dsl.compute import cube_util
from tbe.dsl.compute import util
from tbe.tvm import Tensor
from tbe.tvm import Var

# fractal size, only support 16 for now
BLOCK_SIZE = 16

# channel size, support 4 channels
C04_SIZE = 4


@para_check.check_input_type(Tensor, Tensor, (list, tuple), dict)
def conv2d_backprop_filter_compute(x, out_backprop, filter_sizes, para_dict):
    """
    the DSL interface of conv2d backprop filter compute

    Parameters:
    ----------
    x : the featuremap data, tvm.placeholder, 5HD shape

    out_backprop : the grads data, tvm.placeholder, 5HD shape

    filter_sizes : 4-D shape, specifies the filter sizes

    para_dict:

    strides : 2-D shape, specifies in height and width dimension

    padding : 4-D shape, specifies in up/down/left/right dimension

    dilations : 4-D shape, specifies in batch/channel/height/width dimension

    groups : The number of filter's group. Default value is 1.

    res_dtype : the output data type

    Returns
    -------
    result tensor of conv2d_backprop_filter compute
    """
    deconv_dw_object = Conv2dBackpropFilterCompute(x, out_backprop, filter_sizes, para_dict)
    deconv_dw_object.deconv_dw_access()

    return deconv_dw_object.res_tensor


def get_input_idx_from_output_idx(out_idx, kh_idx, stride, dilation):
    """
    load3d derives the input from the output
    """
    return out_idx * stride + kh_idx * dilation


def get_binary_mode(fmap_format_in_gm, binary_flag):
    if binary_flag:
        if fmap_format_in_gm == "NC1HWC0":
            return cube_util.BinaryMode.NC1HWC0
        elif fmap_format_in_gm == "NCHW":
            return cube_util.BinaryMode.NCHW
        elif fmap_format_in_gm == "NHWC":
            return cube_util.BinaryMode.NHWC
    return cube_util.BinaryMode.NON_BINARY


class Conv2dBackpropFilterCompute:
    """
    Conv2dBackpropFilter: compute definition of conv2d_backprop_filter

    Functions
    ----------
    __init__ : initialization

    deconv_dw_input_check_1: parameters check part 1

    deconv_dw_input_check_2: parameters check part 2

    deconv_dw_compute : compute process

    grads_2_matrix : compute definition of loading grads to cbuf

    grads_2_fractal : compute definition of load_2d

    fmap_2_col_matrix : compute definition of set_fmatrix

    fmap_2_col_fractal : compute definition of load_3d

    mad : compute definition of mmad

    """

    def __init__(self,
                 input_x,
                 out_backprop,
                 filter_sizes,
                 para_dict):
        """
        initialization

        Parameters:
        ----------
        input_x : the featuremap data, tvm.placeholder, 5HD shape

        out_backprop : the grads data, tvm.placeholder, 5HD shape

        filter_sizes : 4-D shape, specifies the filter sizes

        strides : 2-D shape in height and width dimension

        padding : 4-D shape in up/down/left/right dimension

        dilations : 4-D shape in batch/channel/height/width dimension

        groups : The number of filter's group. Default value is 1.

        res_dtype : the output data type

        Returns
        -------
        None
        """
        strides = para_dict.get("strides", [1, 1])
        padding = para_dict.get("padding", [0, 0, 0, 0])
        dilations = para_dict.get("dilations", [1, 1, 1, 1])
        groups = para_dict.get("groups", 1)
        fmap_format_in_gm = para_dict.get("fmap_format_in_gm")
        res_dtype = para_dict.get("res_dtype", "float32")
        kernel_name = para_dict.get("kernel_name", "conv2d_backprop_filter_cce")
        binary_flag = para_dict.get("binary_flag", False)
        ct = para_dict.get("compute_template")  # class conv2d_bp_filter_classifier::ComputeTemplate

        # flag of special case
        self.c04_flag = ct.c04_flag
        self.l0b_dma_flag = ct.l0b_dma_flag
        self.strideh_read_flag = ct.strideh_read_flag
        self.flag_all_one_case = ct.flag_all_one_case
        self.flag_load3d_w_split_case = ct.flag_load3d_w_split_case
        self.flag_load3d_special_case = ct.flag_load3d_special_case
        self.linear_embedding_opti_flag = ct.linear_embedding_opti_flag

        DynamicConv2dBpFilterParams.binary_mode = get_binary_mode(fmap_format_in_gm, binary_flag)
        DynamicConv2dBpFilterParams.correct_range_flag = para_dict.get("correct_range_flag", False)
        DynamicConv2dBpFilterParams.ori_tensors = para_dict.get("ori_tensors")
        DynamicConv2dBpFilterParams.op_type = para_dict.get("op_type", "Conv2DBackpropFilter")
        DynamicConv2dBpFilterParams.attrs = para_dict.get("attrs", {})
        DynamicConv2dBpFilterParams.dma_c04_flag = ct.c04_flag
        DynamicConv2dBpFilterParams.strideh_read_flag = ct.strideh_read_flag
        DynamicConv2dBpFilterParams.linear_embedding_opti_flag = ct.linear_embedding_opti_flag
        DynamicConv2dBpFilterParams.l0b_dma_flag = ct.l0b_dma_flag
        DynamicConv2dBpFilterParams.load_mode = ct.load_mode
        DynamicConv2dBpFilterParams.conv1d_flag = ct.conv1d_flag
        DynamicConv2dBpFilterParams.flag_load3d_special_case = ct.flag_load3d_special_case

        self.fmap, self.grads, self.weight_shape = input_x, out_backprop, list(filter_sizes)
        self.fmap_dtype, self.grads_dtype, self.res_dtype = input_x.dtype, out_backprop.dtype, res_dtype
        self.pad, self.stride, self.dilation = cube_util.shape_to_list(padding), list(strides), list(dilations)
        self.group, self.kernel_name = groups, kernel_name
        self.shapelist, self.group_dict = {}, {}
        self.optag = "conv2d_backprop_filter"

        # 5HD shape
        self.shape_grads_5hd = cube_util.shape_to_list(self.grads.shape)
        self.shape_x_5hd = cube_util.shape_to_list(self.fmap.shape)

        self.shapelist['grads_5hd'] = self.shape_grads_5hd
        self.shapelist['fmap_5hd'] = self.shape_x_5hd

        self.dw_ddr = []
        self.res_tensor = self.dw_ddr  # return tensor of this file to topi

        self.c0_size = tbe_platform.CUBE_MKN.get(self.fmap_dtype).get("mac")[1]
        self.fp32_input = self.fmap_dtype == "float32" and self.grads_dtype == "float32"
        # for dynamic
        self.load3d_special_multiply = 1
        if is_unify():
            self.load3d_special_multiply = get_te_var("load3d_special").get_tvm_var()
        DynamicConv2dBpFilterParams.flag_all_one_case = self.flag_all_one_case
        DynamicConv2dBpFilterParams.flag_load3d_w_split_case = self.flag_load3d_w_split_case
        DynamicConv2dBpFilterParams.var_map = self._get_var_map()
        DynamicConv2dBpFilterParams.tiling_info_dict = {
            "op_type": 'conv2d_backprop_filter',
            "A_shape": cube_util.shape_to_list(self.grads.shape),
            "B_shape": cube_util.shape_to_list(self.fmap.shape),
            "C_shape": cube_util.shape_to_list([
                cube_util.ceil_div(self.weight_shape[0], BLOCK_SIZE) * BLOCK_SIZE,
                cube_util.ceil_div(self.weight_shape[1], BLOCK_SIZE),
                self.weight_shape[2],
                self.weight_shape[3],
                BLOCK_SIZE
            ]),
            "A_dtype": self.grads.dtype,
            "B_dtype": self.fmap.dtype,
            "C_dtype": res_dtype,
            "mad_dtype": 'float32',
            "padl": self.pad[2],
            "padr": self.pad[3],
            "padu": self.pad[0],
            "padd": self.pad[1],
            "strideH": self.stride[0],
            "strideW": self.stride[1],
            "strideH_expand": 1,
            "strideW_expand": 1,
            "dilationH": self.dilation[2],
            "dilationW": self.dilation[3],
            "group": 1,
            "bias_flag": 0,
            "fused_double_operand_num": 0,
            "kernel_name": kernel_name,
            "dynamic_shape_flag": True
        }

    def deconv_dw_access(self):
        """
        complete compute generation, including input check,
        compute definition and result record

        """
        self._compute_group_dict()
        self.deconv_dw_compute()
        self.res_tensor = self.dw_ddr  # return tensor of this file to topi

    def deconv_dw_compute(self):
        """
        complete compute definition

        """
        fmap_dtype = self.fmap_dtype

        batch_size, grads_channel_1, grads_height, grads_width, grads_c0 = self.shape_grads_5hd
        _, fmap_channel_1, fmap_height, fmap_width, fmap_c0 = self.shape_x_5hd
        _, _, kernel_height, kernel_width = self.weight_shape
        _, _, _, dilation_w = self.dilation

        if is_unify():
            # when wout equal 1, set stride_w be fmap_w_after_pad
            # add kernel_w_after_dilation to pad_right
            # so that grads_w be 2
            self.pad[3] += ((kernel_width - 1) * dilation_w + 1) * (self.load3d_special_multiply - 1)
            grads_width = grads_width * self.load3d_special_multiply
            self.shapelist.get('grads_5hd')[-2] = grads_width
            self.shape_grads_5hd[-2] = grads_width
        elif self.flag_load3d_special_case:
            # in this situation, stride_w do no make sense
            # set stride_w be fmap_w_after_pad
            # add kernel_w_after_dilation to pad_right
            # so that grads_w be 2
            self.stride[1] = fmap_width + self.pad[2] + self.pad[3]
            self.pad[3] += (kernel_width - 1) * dilation_w + 1
            grads_width = grads_width * 2
            self.shapelist.get('grads_5hd')[-2] = grads_width
            self.shape_grads_5hd[-2] = grads_width

        # group dict
        real_g = self.group_dict.get("real_g")
        fmap_c1_g = self.group_dict.get("cin1_g")
        grads_channel_g = self.group_dict.get("cout_g")

        # align to 16
        hw_mad = util.align(grads_height * grads_width, BLOCK_SIZE)
        hw_mad_1 = cube_util.ceil_div(grads_height * grads_width, BLOCK_SIZE)
        wo_mad_1 = cube_util.ceil_div(grads_width, BLOCK_SIZE)
        self.shapelist['hw_mad_1'] = hw_mad_1
        self.shapelist['wo_mad_1'] = wo_mad_1

        grads_c1 = real_g * grads_channel_g // grads_c0 if self.fp32_input else grads_channel_1
        # move grads to L1
        grads_shape_matrix = (batch_size, grads_c1, grads_height * grads_width, grads_c0)

        if self.flag_load3d_w_split_case:
            grads_shape_matrix = (batch_size, grads_c1, grads_height, grads_width, grads_c0)

        self.shapelist['grads_matrix'] = grads_shape_matrix

        grads_matrix = self._grads_2_matrix(grads_shape_matrix, self.grads)

        # move grads_matrix to L0A and do transpose
        grads_shape_fractal = (real_g, batch_size, grads_channel_g // grads_c0, hw_mad_1, grads_c0, BLOCK_SIZE)

        if self.fp32_input:
            # transpose to make sure k0 axis in mad is 8
            grads_shape_fractal = (real_g, batch_size, grads_channel_g // grads_c0 // 2, hw_mad_1 * 2, grads_c0 * 2,
                                   BLOCK_SIZE // 2)

        if self.flag_load3d_w_split_case:
            grads_shape_fractal = (real_g, batch_size, grads_channel_g // grads_c0, grads_height, wo_mad_1, grads_c0,
                                   BLOCK_SIZE)
            if self.fp32_input:
                grads_shape_fractal = (real_g, batch_size, grads_channel_g // grads_c0 // 2, grads_height, wo_mad_1 * 2,
                                       grads_c0 * 2, BLOCK_SIZE // 2)

        self.shapelist['grads_fractal'] = grads_shape_fractal
        grads_fractal = self._grads_2_fractal(grads_shape_fractal, grads_matrix)

        if not self.flag_all_one_case:
            if not is_unify():
                # Load 3D Data Flow:
                # fmap in DDR to fmap_matrix in L1 to fmap_fractal_nZ in L0B

                # Dma Mode Data Flow:
                # fmap in DDR to fmap_ub_pad in UB to fmap_matrix in L1 to
                # fmap_fractal_before_zZ in L1 to fmap_fractal_nZ in L0B
                fmap_ub = None
                if self.l0b_dma_flag and self.pad != [0, 0, 0, 0]:
                    pad_top, pad_bottom, pad_left, pad_right = self.pad
                    fmap_ub_shape = (batch_size, fmap_channel_1, fmap_height + pad_top + pad_bottom,
                                     fmap_width + pad_left + pad_right, fmap_c0)
                    fmap_ub = self._fmap_2_ub(fmap_ub_shape, fmap_height, fmap_width)

                fmap_c0_dma = self._get_c0_dma(fmap_c0)

                # shape of fmap_original_matrix, corresponding to set_fmatrix
                fmap_shape_original_matrix = (batch_size, grads_height * grads_width, fmap_channel_1, kernel_height,
                                              kernel_width, fmap_c0_dma)

                if self.flag_load3d_w_split_case:
                    fmap_shape_original_matrix = (batch_size, grads_height, grads_width, fmap_channel_1, kernel_height,
                                                  kernel_width, fmap_c0_dma)

                self.shapelist['fmap_original_matrix'] = fmap_shape_original_matrix

                if fmap_ub is None:
                    fmap_l1_before = self.fmap
                else:
                    fmap_l1_before = fmap_ub

                nc1hw_para = (batch_size, fmap_channel_1, fmap_height, fmap_width)
                fmap_l1_before = self._trans_nc1hwc0_to_nc1hw4(nc1hw_para, fmap_l1_before)

                if self.strideh_read_flag:
                    h_new = ((fmap_height - 1) // self.stride[0] + 1)
                    fmap_new_shape = (batch_size, fmap_channel_1, h_new, fmap_width, fmap_c0)
                    fmap_l1 = tvm.compute(fmap_new_shape,
                                          lambda n_idx, c1_idx, h_idx, w_idx, c0_idx: fmap_l1_before(
                                              n_idx, c1_idx, h_idx * self.stride[0], w_idx, c0_idx),
                                          name='fmap_l1_name',
                                          tag='fmap_l1_tag')
                    fmap_matrix = self._fmap_2_matrix(fmap_shape_original_matrix, fmap_l1, fmap_dtype)
                else:
                    fmap_matrix = self._fmap_2_matrix(fmap_shape_original_matrix, fmap_l1_before, fmap_dtype)

                # load 3d: move fmap to L0B
                # dma mode: change to zZ in L1 first

                row_major_c0 = self._get_c0_dma(fmap_c0)
                k1_size = (fmap_c1_g * kernel_height * kernel_width * row_major_c0 + BLOCK_SIZE - 1) // BLOCK_SIZE

                fmap_shape_fmap_matrix = self._get_fmap_fractal_shape(k1_size)
                self.shapelist['fmap_fmap_matrix'] = fmap_shape_fmap_matrix

                if self.l0b_dma_flag:
                    fmap_shape_fmap_matrix = (real_g, batch_size, hw_mad_1, fmap_c1_g * kernel_height * kernel_width,
                                              BLOCK_SIZE, fmap_c0)

                fmap_fractal = self._fmap_2_fractal(fmap_shape_fmap_matrix, fmap_matrix, fmap_dtype)

                if self.l0b_dma_flag:
                    fmap_shape_fmap_matrix = self.shapelist.get('fmap_fmap_matrix')
                    fmap_fractal = self._fmap_2_fractal_dma(fmap_shape_fmap_matrix, fmap_fractal)
            else:
                fmap_ub = None
                if self.l0b_dma_flag and not tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
                    pad_top, pad_bottom, pad_left, pad_right = self.pad
                    fmap_ub_shape = (batch_size, fmap_channel_1, fmap_height + pad_top + pad_bottom,
                                     fmap_width + pad_left + pad_right, fmap_c0)
                    fmap_ub = self._fmap_2_ub(fmap_ub_shape, fmap_height, fmap_width)

                fmap_l1_before = self.fmap if fmap_ub is None else fmap_ub

                fmap_l1_shape = self._get_fmap_l1_shape()
                fmap_l1 = self._fmap_2_l1_process(fmap_l1_shape, fmap_l1_before)
                fmap_shape_fmap_matrix = self._get_fmap_fractal_shape(fmap_c1_g * kernel_height * kernel_width)
                l1_2_l0b_func = self._fmap_2_fractal_dma if self.l0b_dma_flag else self._im2col_fractal_v2
                fmap_fractal = l1_2_l0b_func(fmap_shape_fmap_matrix, fmap_l1)
        # else: all_one_case, using load_2d instead of load_3d
        else:
            # shape of fmap_matrix
            fmap_shape_matrix = (batch_size, fmap_channel_1, fmap_height * fmap_width, fmap_c0)
            fmap_matrix = self._fmap_2_matrix_load2d(fmap_shape_matrix, self.fmap)

            # move fmap to L0B
            fmap_shape_fractal = (real_g, batch_size, hw_mad // BLOCK_SIZE, fmap_c1_g * kernel_height * kernel_width,
                                  fmap_c0, BLOCK_SIZE)

            fmap_fractal = self._fmap_2_fractal_load2d(fmap_shape_fractal, fmap_matrix)

        self._mad_process(grads_fractal, fmap_fractal)

        return 1

    def _get_c0_dma(self, fmap_c0):
        fmap_c0_dma = fmap_c0
        if self.c04_flag:
            fmap_c0_dma = C04_SIZE
        return fmap_c0_dma

    def _get_dw_shape(self, dw_shape_para, fmap_c0):
        real_g, fmap_c1_g, kernel_height, kernel_width, grads_channel_g = dw_shape_para
        dw_shape = (real_g, fmap_c1_g * kernel_height * kernel_width, grads_channel_g, fmap_c0)
        if self.c04_flag:
            row_major_c0 = C04_SIZE
            k1_size = cube_util.ceil_div(fmap_c1_g * kernel_height * kernel_width * row_major_c0, BLOCK_SIZE)
            dw_shape = (real_g, k1_size, grads_channel_g, fmap_c0)
        return dw_shape

    def _get_fmap_fractal_shape(self, k1_size):
        """
        fmap_fractal shape is:
        1. fp32 and w_split
           (real_g, batch, ho, ceil(wo, 16)*2, cin1_g*kh*kw//2, n0(16), k0(8))
        2. fp32 and not w_split
           (real_g, batch, ceil(howo, 16)*2, cin1_g*kh*kw//2, n0(16), k0(8))
        3. fp16 and w_split
           (real_g, batch, ho, ceil(wo, 16), cin1_g*kh*kw, n0(16), k0(16))
        4. fp16 and not w_split
           (real_g, batch, ceil(howo, 16), cin1_g*kh*kw, n0(16), k0(16))
        """

        real_g = self.group_dict.get("real_g")
        batch_size, _, grads_height, _, _ = self.shape_grads_5hd
        _, _, _, _, fmap_c0 = self.shape_x_5hd
        _, _, kernel_height, kernel_width = self.weight_shape
        wo_mad_1 = self.shapelist.get("wo_mad_1")
        hw_mad_1 = self.shapelist.get("hw_mad_1")
        fmap_c1_g = self.group_dict.get("cin1_g")
        fp32_factor = 2 if self.fp32_input else 1
        fmap_fractal_shape = (real_g, batch_size, hw_mad_1 * fp32_factor,
                              cube_util.ceil_div(k1_size, fp32_factor), fmap_c0 * fp32_factor,
                              BLOCK_SIZE // fp32_factor)
        if self.flag_load3d_w_split_case:
            fmap_fractal_shape = (real_g, batch_size, grads_height, wo_mad_1 * fp32_factor,
                                  cube_util.ceil_div(fmap_c1_g * kernel_height * kernel_width, fp32_factor),
                                  fmap_c0 * fp32_factor, BLOCK_SIZE // fp32_factor)

        return fmap_fractal_shape

    def _get_fmap_l1_shape(self):
        """
        fmap_l1 shape is:
        1. dma flag
           FP16:
           (real_g, batch, ceil(howo, k0), cin1_g*kh*kw, k0, n0)
           FP32:
           (real_g, batch, cin1_g*kh*kw, ceil(howo, k0),  k0, n0)
        2. not dma flag
           (real_g, batch, cin1_g, hi, wi, n0)
        """
        real_g = self.group_dict.get("real_g")
        fmap_c1_g = self.group_dict.get("cin1_g")
        batch_size, _, fmap_height, fmap_width, fmap_c0 = self.shape_x_5hd
        _, _, kernel_height, kernel_width = self.weight_shape
        if self.strideh_read_flag:
            fmap_height = (fmap_height - 1) // self.stride[0] + 1
        fmap_l1_shape = (real_g, batch_size, fmap_c1_g, fmap_height, fmap_width, fmap_c0)
        if self.l0b_dma_flag:
            if self.fp32_input:
                # L1 to l0b enable load2d to load3d, which needs to be typed as Nz
                fmap_l1_shape = (real_g, batch_size, fmap_c1_g * kernel_height * kernel_width,
                                 self.shapelist.get("hw_mad_1"), BLOCK_SIZE, fmap_c0)
            else:
                fmap_l1_shape = (real_g, batch_size, self.shapelist.get("hw_mad_1"),
                                 fmap_c1_g * kernel_height * kernel_width, BLOCK_SIZE, fmap_c0)

        return fmap_l1_shape

    def _trans_nc1hwc0_to_nc1hw4(self, nc1hw_para, fmap_l1_before):
        batch_size, fmap_channel_1, fmap_height, fmap_width = nc1hw_para
        if self.c04_flag:
            fmap_l1_c04_shape = batch_size, fmap_channel_1, fmap_height, fmap_width, C04_SIZE
            fmap_l1_c04 = tvm.compute(
                fmap_l1_c04_shape,
                lambda n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx: self.fmap(n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx),
                name="fmap_l1",
                tag="fmap_l1")
            fmap_l1_before = fmap_l1_c04
        return fmap_l1_before

    def _get_var_map(self):
        n_dim = 0
        h_dim = 2
        w_dim = 3
        var_map = {}

        if isinstance(self.fmap.shape[n_dim], Var):
            var_map["batch"] = get_te_var("batch").get_tvm_var()

        if isinstance(self.fmap.shape[h_dim], Var):
            var_map["fmap_h"] = get_te_var("fmap_h").get_tvm_var()
            var_map["dedy_h"] = get_te_var("dedy_h").get_tvm_var()

        if isinstance(self.fmap.shape[w_dim], Var):
            var_map["fmap_w"] = get_te_var("fmap_w").get_tvm_var()
            var_map["dedy_w"] = get_te_var("dedy_w").get_tvm_var()

        return var_map

    def _compute_group_dict(self):
        """
        calculate the params of group convlution

        """
        groups = self.group
        cout, cin, _, _ = self.weight_shape
        fmap_c = cin * groups
        c0_size = self.c0_size
        if not is_unify():
            mag_factor0 = conv_util.lcm(fmap_c // groups, self.c0_size) // (fmap_c // groups)
            mag_factor1 = conv_util.lcm(cout // groups, BLOCK_SIZE) // (cout // groups)
            mag_factor = min(conv_util.lcm(mag_factor0, mag_factor1), groups)

            cin1_g = cube_util.ceil_div(mag_factor * fmap_c // groups, self.c0_size)
            cout_g = (mag_factor * cout // groups + BLOCK_SIZE - 1) // BLOCK_SIZE * BLOCK_SIZE

            group_dict = {
                "real_g": (groups + mag_factor - 1) // mag_factor,
                "mag_factor": mag_factor,
                "cin1_g": cin1_g,
                "cout_g": cout_g,
                "cin_ori": fmap_c,
                "cout_ori": cout
            }
        else:
            group_dict = {
                "real_g": get_te_var("real_g").get_tvm_var(),
                "mag_factor": get_te_var("mag_factor").get_tvm_var(),
                "cin1_g": get_te_var("cin1_g").get_tvm_var(),
                "cout_g": get_te_var("cout1_g").get_tvm_var() * self.c0_size,
                "cin_ori": get_te_var("fmap_c").get_tvm_var(),
                "cout_ori": cout
            }
        tiling_info_dict_tmp = DynamicConv2dBpFilterParams.tiling_info_dict
        tiling_info_dict_tmp["group"] = group_dict.get("real_g")
        tiling_info_dict_tmp.get("A_shape")[1] = group_dict.get("cout_g") // c0_size
        tiling_info_dict_tmp.get("B_shape")[1] = group_dict.get("cin1_g")
        tiling_info_dict_tmp.get("C_shape")[0] = group_dict.get("cout_g")
        tiling_info_dict_tmp.get("C_shape")[1] = group_dict.get("cin1_g")
        DynamicConv2dBpFilterParams.tiling_info_dict = tiling_info_dict_tmp
        self.group_dict = group_dict

    def _grads_2_matrix(self, grads_shape_matrix, grads):
        """
        compute definiton of loading grads to L1

        Parameters:
        ----------
        grads_shape_matrix : shape of result tensor in L1

        grads : input tensor in ddr

        Returns
        -------
        None
        """

        def __grads_2_matrix_compute(indices, grads):
            """
            do coordinate calculation
            """
            if self.flag_load3d_w_split_case:
                return grads(*indices)

            grads_width = self.shapelist.get('grads_5hd')[3]
            batch_indices, grads_c1_indices, hw_mad_indices, grads_c0_indices = indices

            # calculate index of grads according to indice of grads_matrix
            batch_size_index = batch_indices
            grads_c1_index = grads_c1_indices
            grads_height_index = (hw_mad_indices // grads_width)
            grads_width_index = (hw_mad_indices % grads_width)
            grads_c0_index = grads_c0_indices

            if not is_unify() and self.flag_load3d_special_case:
                # make sure the index won't exceed real grads_w
                grads_width_index = (hw_mad_indices % (grads_width // 2))
            if is_unify():
                w_one_flag = self.load3d_special_multiply
                grads_width_index = (hw_mad_indices % (grads_width // w_one_flag))

            return grads(batch_size_index, grads_c1_index, grads_height_index, grads_width_index, grads_c0_index)

        def __grads_2_zz_matrix_compute(indices, grads):
            """
            do coordinate calculation for fp32
            """
            grads_w = self.shapelist.get('grads_5hd')[3]
            ori_grads_channel_1 = self.shape_grads_5hd[1]
            if self.flag_load3d_w_split_case:
                batch_indices, grads_c1_indices, grads_h_index, grads_w_index, grads_c0_indices = indices
            else:
                (batch_indices, grads_c1_indices, hw_mad_indices, grads_c0_indices) = indices
                grads_h_index = hw_mad_indices // grads_w
                grads_w_index = hw_mad_indices % grads_w
            return tvm.select(tvm.all(grads_c1_indices < ori_grads_channel_1),
                              grads(batch_indices, grads_c1_indices, grads_h_index, grads_w_index, grads_c0_indices),
                              tvm.const(0, dtype=self.grads_dtype))

        func_name = __grads_2_zz_matrix_compute if self.fp32_input else __grads_2_matrix_compute
        return tvm.compute(grads_shape_matrix,
                           lambda *indices: func_name(indices, grads),
                           name='grads_2_matrix',
                           tag='grads_2_matrix')

    def _grads_2_fractal(self, grads_shape_fractal, grads_2_matrix):
        """
        compute definiton of loading grads_matrix to L0A

        Parameters:
        ----------
        grads_shape_fractal : shape of result tensor in L0A

        grads_2_matrix : input tensor in L1

        Returns
        -------
        None
        """

        def __grads_2_fractal_w_split_compute(indices, grads_2_matrix):
            """
            Compute function for w-split scene.
            """
            grads_channel_1 = self.shape_grads_5hd[1]
            (group_indices, batch_indices, grads_c1_indices, ho_indices, wo_mad_1_indices, grads_c0_indices,
             wo_mad_0_indices) = indices
            if self.fp32_input:
                grads_c_index = group_indices * self.group_dict.get("cout_g") \
                                + grads_c1_indices * BLOCK_SIZE + grads_c0_indices
                grads_c1_index = grads_c_index // self.c0_size
                grads_w_index = wo_mad_1_indices * self.c0_size + wo_mad_0_indices
                grads_c0_index = grads_c_index % self.c0_size
            else:
                grads_c1_index = (group_indices * self.group_dict.get("cout_g")) // BLOCK_SIZE + grads_c1_indices
                grads_w_index = wo_mad_1_indices * BLOCK_SIZE + wo_mad_0_indices
                grads_c0_index = grads_c0_indices
            if self.fp32_input:
                return grads_2_matrix(batch_indices, grads_c1_index, ho_indices, grads_w_index, grads_c0_index)
            return tvm.select(tvm.all(grads_c1_index < grads_channel_1),
                              grads_2_matrix(batch_indices, grads_c1_index, ho_indices, grads_w_index, grads_c0_index),
                              tvm.const(0, dtype=self.grads_dtype))

        def __grads_2_fractal_compute(indices, grads_2_matrix):
            """
            do coordinate calculation
            """
            grads_channel_1 = self.shape_grads_5hd[1]
            (group_indices, batch_indices, grads_c1_indices, hw_mad_1_indices, grads_c0_indices,
             hw_mad_0_indices) = indices

            grads_hw_index = (hw_mad_1_indices * self.c0_size) + hw_mad_0_indices
            if self.fp32_input:
                # c axis in matrix is 8 aligned, while in fractal is 16 aligned
                grads_c_index = group_indices * self.group_dict.get("cout_g") \
                                + grads_c1_indices * BLOCK_SIZE + grads_c0_indices
                grads_c1_index = grads_c_index // self.c0_size
                grads_c0_index = grads_c_index % self.c0_size
                return grads_2_matrix(batch_indices, grads_c1_index, grads_hw_index, grads_c0_index)

            grads_c1_index = (group_indices * self.group_dict.get("cout_g")) // BLOCK_SIZE + grads_c1_indices
            grads_c0_index = grads_c0_indices
            return tvm.select(tvm.all(grads_c1_index < grads_channel_1),
                              grads_2_matrix(batch_indices, grads_c1_index, grads_hw_index, grads_c0_index),
                              tvm.const(0, dtype=self.grads_dtype))

        if self.flag_load3d_w_split_case:
            func_name = __grads_2_fractal_w_split_compute
        else:
            func_name = __grads_2_fractal_compute

        return tvm.compute(grads_shape_fractal,
                           lambda *indices: func_name(indices, grads_2_matrix),
                           name='grads_2_fractal',
                           tag='grads_2_fractal')

    def _fmap_2_ub(self, fmap_ub_shape, fmap_height, fmap_width):
        pad_top, _, pad_left, _ = self.pad
        return tvm.compute(
            fmap_ub_shape,
            lambda n, c1, h, w, c0: tvm.select(
                tvm.any(h < pad_top, h > fmap_height + pad_top - 1, w < pad_left, w > fmap_width + pad_left - 1),
                tvm.const(0, self.fmap_dtype), self.fmap(n, c1, h - pad_top, w - pad_left, c0)),
            name='fmap_ub_for_dma',
            tag='fmap_ub_for_dma')

    def _fmap_2_matrix(self, fmap_shape_original_matrix, fmap, fmap_dtype):
        """
        compute definiton of set_fmatrix

        Parameters:
        ----------
        fmap_shape_original_matrix : shape of result tensor in L1
        in shape (batch_size,
                  grads_height*grads_width,
                  fmap_channel_1,
                  kernel_height,
                  kernel_width,
                  fmap_c0)

        fmap : input tensor in L1

        fmap_dtype : data type of fmap
        in shape (batch_size, fmap_channel_1, fmap_height, fmap_width, C0)

        Returns
        -------
        None
        """

        def __fmap_2_matrix_w_split_compute(indices, fmap):
            """
            do coordinate calculation for w-split scene.

            """
            (batch_indices, ho_indices, wo_indices, fmap_c1_indices, kernel_height_indices, kernel_width_indices,
             fmap_c0_indices) = indices

            n_index = batch_indices
            c1_index = fmap_c1_indices
            h_index = get_input_idx_from_output_idx(ho_indices, kernel_height_indices, strideh, dilationh)
            w_index = get_input_idx_from_output_idx(wo_indices, kernel_width_indices, stridew, dilationw)
            c0_index = fmap_c0_indices

            # if index belongs to padding and 16 align, select 0
            return tvm.select(
                tvm.any(h_index < pad_top, h_index > fmap_height + pad_top - 1, w_index < pad_left,
                        w_index > fmap_width + pad_left - 1), tvm.const(0.0, fmap_dtype),
                fmap(n_index, c1_index, h_index - pad_top, w_index - pad_left, c0_index))

        def __fmap_2_matrix_compute(indices, fmap):
            """
            do coordinate calculation

            """
            (batch_indices, hw_fuse_indices, fmap_c1_indices, kernel_height_indices, kernel_width_indices,
             fmap_c0_indices) = indices

            n_index = batch_indices
            c1_index = fmap_c1_indices
            h_index = get_input_idx_from_output_idx((hw_fuse_indices // width_out),
                                                    kernel_height_indices, strideh, dilationh)
            w_index = get_input_idx_from_output_idx((hw_fuse_indices % width_out),
                                                    kernel_width_indices, stridew, dilationw)
            c0_index = fmap_c0_indices

            if self.l0b_dma_flag:
                return fmap(n_index, c1_index, h_index, w_index, c0_index)

            # if index belongs to padding and 16 align, select 0
            return tvm.select(
                tvm.any(h_index < pad_top, h_index > fmap_height + pad_top - 1, w_index < pad_left,
                        w_index > fmap_width + pad_left - 1), tvm.const(0.0, fmap_dtype),
                fmap(n_index, c1_index, h_index - pad_top, w_index - pad_left, c0_index))

        _, _, fmap_height, fmap_width, _ = self.shapelist.get('fmap_5hd')
        pad_top, _, pad_left, pad_right = self.pad
        strideh, stridew = self.stride
        _, _, dilationh, dilationw = self.dilation
        kernel_width = fmap_shape_original_matrix[4]
        dilation_kernel_width = kernel_width + (kernel_width - 1) * (dilationw - 1)
        fmap_width_after_pad = fmap_width + pad_left + pad_right
        width_out = (fmap_width_after_pad - dilation_kernel_width) // stridew + 1
        if self.strideh_read_flag:
            kernel_height = fmap_shape_original_matrix[3]
            strideh = kernel_height

        compute_func = __fmap_2_matrix_w_split_compute if self.flag_load3d_w_split_case else __fmap_2_matrix_compute
        return tvm.compute(fmap_shape_original_matrix,
                           lambda *indices: compute_func(indices, fmap),
                           name='fmap_2_col_matrix',
                           tag='fmap_2_col_matrix',
                           attrs={
                               'pad': self.pad,
                               'stride': self.stride,
                               'dilation': self.dilation,
                               'kernel_size': self.weight_shape,
                               'group_dict': self.group_dict
                           })

    def _fmap_2_matrix_load2d(self, fmap_shape_matrix, fmap):
        """
        compute definiton of loading fmap to L1

        Parameters:
        ----------
        fmap_shape_matrix : shape of result tensor in L1

        fmap : input tensor in ddr

        Returns
        -------
        None
        """

        def __fmap_2_matrix_load2d_compute(indices, fmap):
            """
            do coordinate calculation

            """
            fmap_width = self.shapelist.get('fmap_5hd')[3]
            batch_indices, fmap_c1_indices, hw_mad_indices, fmap_c0_indices \
                = indices
            batch_size_index = batch_indices
            fmap_c1_index = fmap_c1_indices
            fmap_height_index = (hw_mad_indices // fmap_width)
            fmap_width_index = (hw_mad_indices % fmap_width)
            fmap_c0_index = fmap_c0_indices
            return fmap(batch_size_index, fmap_c1_index, fmap_height_index, fmap_width_index, fmap_c0_index)

        return tvm.compute(fmap_shape_matrix,
                           lambda *indices: __fmap_2_matrix_load2d_compute(indices, fmap),
                           name='fmap_2_matrix',
                           tag='fmap_2_matrix',
                           attrs={
                               'pad': self.pad,
                               'stride': self.stride,
                               'dilation': self.dilation,
                               'kernel_size': self.weight_shape,
                               'group_dict': self.group_dict
                           })

    def _fmap_2_fractal(self, fmap_shape_fmap_matrix, fmap_2_col_matrix, fmap_dtype):
        """
        compute definiton of loading fmap to L0B

        Parameters:
        ----------
        fmap_shape_fmap_matrix : shape of result tensor in L0B
        in shape (batch_size,
                  hw_mad//block_size_K,
                  fmap_channel_1*kernel_height*kernel_width,
                  fmap_c0,
                  block_size_K)

        fmap_2_col_matrix : input tensor in L1
        in shape (batch_size,
                  grads_height*grads_width,
                  fmap_channel_1,
                  kernel_height,
                  kernel_width,
                  fmap_c0)

        fmap_dtype : data type of fmap_2_col_matrix


        Returns
        -------
        None
        """

        def __fmap_2_fractal_w_split_compute(indices, fmap_2_col_matrix):
            """
            do coordinate calculation for w-split scene.

            """
            _, _, grads_width, _, kernel_height, kernel_width, _ = self.shapelist.get('fmap_original_matrix')

            (group_index, n_vm_index, ho_indices, wo_mad_1_indices, fkk_indices, fmap_c0_indices,
             wo_mad_0_indices) = indices

            if self.fp32_input:
                wo_vm_index = wo_mad_1_indices * self.c0_size + wo_mad_0_indices
                c1_vm_index = (fkk_indices * BLOCK_SIZE +
                               fmap_c0_indices) // self.c0_size // kernel_width // kernel_height
                c1_index = group_index * self.group_dict.get("cin1_g") + c1_vm_index
                kh_vm_index = (fkk_indices * BLOCK_SIZE +
                               fmap_c0_indices) // self.c0_size // kernel_width % kernel_height
                kw_vm_index = (fkk_indices * BLOCK_SIZE + fmap_c0_indices) // self.c0_size % kernel_width
                c0_vm_index = fmap_c0_indices % self.c0_size
            else:
                wo_vm_index = wo_mad_1_indices * BLOCK_SIZE + wo_mad_0_indices
                c1_vm_index = ((fkk_indices * BLOCK_SIZE + fmap_c0_indices) // BLOCK_SIZE // kernel_width //
                               kernel_height)
                c1_index = group_index * self.group_dict.get("cin1_g") + c1_vm_index
                kh_vm_index = ((fkk_indices * BLOCK_SIZE + fmap_c0_indices) // BLOCK_SIZE // kernel_width %
                               kernel_height)
                kw_vm_index = ((fkk_indices * BLOCK_SIZE + fmap_c0_indices) // BLOCK_SIZE) % kernel_width
                c0_vm_index = (fkk_indices * BLOCK_SIZE + fmap_c0_indices) % BLOCK_SIZE

            # select padding and 16 align
            return tvm.select(
                tvm.any(wo_vm_index < 0, wo_vm_index > grads_width - 1), tvm.const(0.0, fmap_dtype),
                fmap_2_col_matrix(n_vm_index, ho_indices, wo_vm_index, c1_index, kh_vm_index, kw_vm_index, c0_vm_index))

        def __fmap_2_fractal_compute(indices, fmap_2_col_matrix):
            """
            do coordinate calculation

            """

            _, hw_fuse, fmap_c1, kernel_height, kernel_width, _ \
                = self.shapelist.get('fmap_original_matrix')

            # batch_size
            # hw_mad//block_size_K
            # fmap_channel_1*kernel_height*kernel_width
            # fmap_c0
            # block_size_K
            (group_index, n_vm_index, hw_mad_1_indices, fkk_indices, fmap_c0_indices, hw_mad_0_indices) = indices

            # Dma Mode, set fractal be zZ first
            if self.l0b_dma_flag:
                (group_index, n_vm_index, hw_mad_1_indices, fkk_indices, hw_mad_0_indices, fmap_c0_indices) = indices

            if self.fp32_input:
                # matrix: fmap_c0 aligned to 8
                # fractal: hw_mad aligned to 8, while fmap_c0 aligned to 16
                # e.g., fmap is (1, 16, 7, 7) and kernel is (16, 16, 3, 3)
                # matrix is (1, 49, 2, 3, 3, 8) to fractal is (1, 1, 8, 9, 16, 8)
                hw_vm_index = hw_mad_1_indices * self.c0_size + hw_mad_0_indices
                c0_vm_index = fmap_c0_indices % self.c0_size
                c1_vm_index = (fkk_indices * BLOCK_SIZE +
                               fmap_c0_indices) // self.c0_size // kernel_width // kernel_height
                c1_index = group_index * self.group_dict.get("cin1_g") + c1_vm_index
                kh_vm_index = (fkk_indices * BLOCK_SIZE +
                               fmap_c0_indices) // self.c0_size // kernel_width % kernel_height
                kw_vm_index = (fkk_indices * BLOCK_SIZE + fmap_c0_indices) // self.c0_size % kernel_width
            else:
                hw_vm_index = hw_mad_1_indices * BLOCK_SIZE + hw_mad_0_indices
                c1_vm_index = ((fkk_indices * BLOCK_SIZE + fmap_c0_indices) // BLOCK_SIZE // kernel_width //
                               kernel_height)

                c1_index = group_index * self.group_dict.get("cin1_g") + c1_vm_index

                kh_vm_index = ((fkk_indices * BLOCK_SIZE + fmap_c0_indices) // BLOCK_SIZE // kernel_width %
                               kernel_height)
                kw_vm_index = ((fkk_indices * BLOCK_SIZE + fmap_c0_indices) // BLOCK_SIZE) % kernel_width
                c0_vm_index = (fkk_indices * BLOCK_SIZE + fmap_c0_indices) % BLOCK_SIZE

            # 16 align
            if self.l0b_dma_flag:
                return tvm.select(
                    tvm.any(hw_vm_index < hw_fuse),
                    fmap_2_col_matrix(n_vm_index, hw_vm_index, c1_index, kh_vm_index, kw_vm_index, c0_vm_index))

            # select padding and 16 align
            return tvm.select(
                tvm.any(hw_vm_index < 0, hw_vm_index > hw_fuse - 1, c1_index >= fmap_c1), tvm.const(0.0, fmap_dtype),
                fmap_2_col_matrix(n_vm_index, hw_vm_index, c1_index, kh_vm_index, kw_vm_index, c0_vm_index))

        compute_func = __fmap_2_fractal_w_split_compute if self.flag_load3d_w_split_case else __fmap_2_fractal_compute
        return tvm.compute(fmap_shape_fmap_matrix,
                           lambda *indices: compute_func(indices, fmap_2_col_matrix),
                           name='fmap_2_col_fractal',
                           tag='fmap_2_col_fractal')

    def _fmap_2_fractal_load2d(self, fmap_shape_fractal, fmap_2_matrix):
        """
        compute definiton of loading fmap_matrix to L0B

        Parameters:
        ----------
        fmap_shape_fractal : shape of result tensor in L0B

        fmap_2_matrix : input tensor in L1

        Returns
        -------
        None
        """

        def __fmap_2_fractal_load2d_compute(indices, fmap_2_matrix):
            """
            do coordinate calculation

            """
            (group_indices, batch_indices, hw_mad_1_indices, fmap_c1_indices, fmap_c0_indices,
             hw_mad_0_indices) = indices

            batch_size_index = batch_indices
            fmap_c1_index = fmap_c1_indices
            c1_index = group_indices * self.group_dict.get("cin1_g") + fmap_c1_index
            fmap_hw_index = hw_mad_1_indices * BLOCK_SIZE + hw_mad_0_indices
            fmap_c0_index = fmap_c0_indices
            # For group scenarios, there may be misalignment of C channels
            return tvm.select(c1_index < fmap_2_matrix.shape[1],
                              fmap_2_matrix(batch_size_index, c1_index, fmap_hw_index, fmap_c0_index),
                              tvm.const(0, self.fmap_dtype))

        return tvm.compute(fmap_shape_fractal,
                           lambda *indices: __fmap_2_fractal_load2d_compute(indices, fmap_2_matrix),
                           name='famp_2_fractal',
                           tag='famp_2_fractal')

    def _mad(self, mad_shape, grads, fmap):
        """
        calculate mad result tensor
        Parameters
        ----------
        mad_shape : result shape
        (fmap_channel_1*kernel_height*kernel_width, grads_channel, fmap_c0)

        grads : tensor in L0A
        grads_shape_fractal = (batch_size,
                               grads_channel_1,
                               hw_mad//block_size_K,
                               grads_c0,
                               block_size_K)

        fmap : tensor in L0B
        fmap_shape_fmap_matrix = (batch_size,
                                  hw_mad//block_size_K,
                                  fmap_channel_1*kernel_height*kernel_width,
                                  fmap_c0,
                                  block_size_K)

        Returns
        -------
        None
        """

        batch_size, _, grads_height, grads_width, _ = self.shapelist.get('grads_5hd')

        batch_axis = tvm.reduce_axis((0, batch_size), name='axis_b')
        reduce_axes = [batch_axis]
        if self.flag_load3d_w_split_case:
            k_axis_ho = tvm.reduce_axis((0, grads_height), name='axis_k_ho')
            k_axis = tvm.reduce_axis((0, grads_width), name='axis_k_wo')
            reduce_axes.extend([k_axis_ho, k_axis])
        else:
            k_axis = tvm.reduce_axis((0, grads_height * grads_width), name='axis_k')
            reduce_axes.append(k_axis)

        k_1 = k_axis.var // self.c0_size
        k_0 = k_axis.var % self.c0_size

        mode_dict = {
            ("float16", "float16"): "f162f16",
            ("float16", "float32"): "f162f32",
            ("float32", "float32"): "fp322fp32"
        }
        mode = mode_dict.get((self.fmap_dtype, self.res_dtype), "f162f32")

        if self.flag_load3d_w_split_case:
            mad_func = lambda g, fkk, grads_c, fmap_c0: tvm.sum(
                (grads[g, batch_axis, grads_c // 16, k_axis_ho, k_1, grads_c % 16, k_0] *
                 fmap[g, batch_axis, k_axis_ho, k_1, fkk, fmap_c0, k_0]).astype(self.res_dtype),
                axis=reduce_axes)
        else:
            mad_func = lambda g, fkk, grads_c, fmap_c0: tvm.sum(
                (grads[g, batch_axis, grads_c // 16, k_1, grads_c % 16, k_0] *
                 fmap[g, batch_axis, k_1, fkk, fmap_c0, k_0]).astype(self.res_dtype),
                axis=reduce_axes)

        c_col = tvm.compute(mad_shape,
                            mad_func,
                            name='dw_ddr',
                            tag=self.optag + "dw_ddr",
                            attrs={
                                'mode': mode,
                                'kernel_name': self.kernel_name
                            })
        return c_col

    def _im2col_fractal_v2(self, shape, src_tensor):
        """
        compute definiton of loading fmap to L0B v2,
        only one-step compute description from l1 to l0b

        Parameters:
        ----------
        fmap_shape_fmap_matrix : shape of result tensor in L0B
        in shape (batch_size,
                  hw_mad//block_size_K,
                  fmap_channel_1*kernel_height*kernel_width,
                  fmap_c0,
                  block_size_K)

        fmap_2_col_matrix : input tensor in L1
        in shape (batch_size, fmap_channel_1, fmap_height, fmap_width, C0)

        Returns
        -------
        None
        """

        return tvm.compute(shape,
                           lambda *idx: self._im2col_idx(idx, src_tensor),
                           name='img2col_fractal_v2',
                           tag='im2col_fractal_v2',
                           attrs={
                               'pad': self.pad,
                               'stride': self.stride,
                               'dilation': self.dilation,
                               'kernel_size': self.weight_shape
                           })

    def _fmap_2_l1_process(self, shape, fmap_l1_before):
        """
        compute definiton of loading fmap to L1,
        Im2col description for l0b_dma_flag

        Parameters:
        ----------
        shape : shape of result tensor in L1

        Returns
        -------
        None
        """
        def __normal_idx(idx, src_tensor):
            g, n, c1, h, w, c0 = idx
            fmap_c1_g = self.group_dict.get("cin1_g")
            fmap_channel_1 = self.shape_x_5hd[1]
            stride_line = 1
            if self.strideh_read_flag:
                stride_line = self.stride[0]
                self.stride[0] = self.weight_shape[2] # kernel_h
            return tvm.select(c1 + g * fmap_c1_g < fmap_channel_1,
                              src_tensor(n, c1 + g * fmap_c1_g, h * stride_line, w, c0))

        compute_func = self._im2col_idx if self.l0b_dma_flag else __normal_idx
        return tvm.compute(shape,
                           lambda *idx: compute_func(idx, fmap_l1_before),
                           name='dw_fmap_l1',
                           tag='dw_fmap_l1',
                           attrs={'group_dict': self.group_dict}
                           )

    def _im2col_common(self, dst_info_dict):
        """
        Describes the index change of h/w/cin in im2col

        Parameters:
        ----------
        dst_info_dict : dst index and information used to infer the index of src

        Returns
        -------
        src_info_dict : src index and information
        """
        _, _, kernel_h, kernel_w = self.weight_shape
        _, _, dilation_h, dilation_w = self.dilation
        ho_idx = dst_info_dict.get("ho_idx")
        wo_idx = dst_info_dict.get("wo_idx")
        c1gkk_idx = dst_info_dict.get("c1gkk_idx")
        c0_idx = dst_info_dict.get("c0_idx")
        dst_c0_extent = dst_info_dict.get("dst_c0_extent")
        src_c0_extent = dst_info_dict.get("src_c0_extent")
        # dst_cigkk->src_c1
        col_w = (c1gkk_idx * dst_c0_extent + c0_idx) // dst_c0_extent
        virtual_w = col_w * dst_c0_extent + c0_idx
        back_c1g_idx = virtual_w // src_c0_extent // kernel_w // kernel_h
        # dst_c0 -> src_c0
        back_c0_idx = c0_idx % src_c0_extent
        # ho->hi and wo->wi
        kh_idx = col_w // kernel_w % kernel_h
        kw_idx = col_w % kernel_w
        back_h_idx = get_input_idx_from_output_idx(ho_idx, kh_idx, self.stride[0], dilation_h)
        back_w_idx = get_input_idx_from_output_idx(wo_idx, kw_idx, self.stride[1], dilation_w)
        src_info_dict = {
            "back_c1g_idx": back_c1g_idx,
            "back_h_idx": back_h_idx,
            "back_w_idx": back_w_idx,
            "back_c0_idx": back_c0_idx
        }
        return src_info_dict

    def _gen_dst_info_dict(self, idx):
        """
        Describes the generation process of dst index and dst information

        Parameters:
        ----------
        idx : dst index

        Returns
        -------
        dst_info_dict : dst index and information used to infer the index of srs
        """
        fmap_wo = self.shape_grads_5hd[-2]
        virtual_h = None

        if self.flag_load3d_w_split_case:
            # w_split, The im2col process is from 5HD to nZ
            # for fp32, frcz is from [k0(16), n0(8)] to [n0(16), k0(8)]
            g_idx, n_idx, ho_idx, wo1_idx, c1gkk_idx, c0_idx, wo0_idx = idx
            wo_idx = wo1_idx * self.c0_size + wo0_idx
            dst_c0_extent = BLOCK_SIZE
            src_c0_extent = self.c0_size
        else:
            if self.l0b_dma_flag:
                # l0b_dma_flag, The im2col process is from 5HD to zZ
                # for fp32, frcz is from [k0(16), n0(8)] to [k0(16), n0(8)]
                if self.fp32_input:
                    # L1 to l0b enable load2d to load3d, which requires L1 type Nz
                    g_idx, n_idx, c1gkk_idx, howo_1_idx, howo_0_idx, c0_idx = idx
                else:
                    g_idx, n_idx, howo_1_idx, c1gkk_idx, howo_0_idx, c0_idx = idx
                dst_c0_extent = self.c0_size
                src_c0_extent = self.c0_size
                dst_k0_extent = BLOCK_SIZE
            else:
                # normal load3d, The im2col process is from 5HD to nZ
                # for fp32, frcz is from [k0(16), n0(8)] to [n0(16), k0(8)]
                g_idx, n_idx, howo_1_idx, c1gkk_idx, c0_idx, howo_0_idx = idx
                dst_c0_extent = BLOCK_SIZE
                src_c0_extent = self.c0_size
                dst_k0_extent = self.c0_size
            virtual_h = howo_1_idx * dst_k0_extent + howo_0_idx
            ho_idx = (virtual_h // fmap_wo)
            wo_idx = (virtual_h % fmap_wo)

        dst_info_dict = {
            "ho_idx": ho_idx,
            "wo_idx": wo_idx,
            "c1gkk_idx": c1gkk_idx,
            "c0_idx": c0_idx,
            "dst_c0_extent": dst_c0_extent,
            "src_c0_extent": src_c0_extent,
            "virtual_h": virtual_h
        }
        return g_idx, n_idx, dst_info_dict

    def _im2col_idx(self, idx, src_tensor):
        """
        Describe the im2col process and obtain the tensor of the im2col output based on the tensor of the input

        Parameters:
        ----------
        idx : dst index

        src_tensor: input tensor

        Returns
        -------
        dst_tensor : output tensor
        """
        fmap_c1_g = self.group_dict.get("cin1_g")
        _, fmap_channel_1, fmap_height, fmap_width, _ = self.shape_x_5hd
        _, _, grads_height, grads_width, _ = self.shape_grads_5hd

        if self.strideh_read_flag:
            fmap_height = src_tensor.shape[3]

        g_idx, n_idx, dst_info_dict = self._gen_dst_info_dict(idx)
        src_info_dict = self._im2col_common(dst_info_dict)
        back_c1g_idx = src_info_dict.get("back_c1g_idx")
        back_h_idx = src_info_dict.get("back_h_idx")
        back_w_idx = src_info_dict.get("back_w_idx")
        if self.l0b_dma_flag:
            back_c1_idx = src_info_dict.get("back_c1g_idx") + g_idx * fmap_c1_g
            if src_tensor.op.tag == "fmap_ub_for_dma":
                src_idx = (n_idx, back_c1_idx, back_h_idx, back_w_idx,
                           src_info_dict.get("back_c0_idx"))
                return src_tensor(*src_idx)
            elif self.linear_embedding_opti_flag:
                src_idx = (n_idx, back_c1_idx, back_h_idx, back_w_idx,
                           src_info_dict.get("back_c0_idx"))
                return tvm.select(
                    dst_info_dict.get("virtual_h") < grads_height * grads_width,
                    src_tensor(*src_idx))

            src_idx = (n_idx, back_c1_idx, back_h_idx - self.pad[0],
                       back_w_idx - self.pad[2], src_info_dict.get("back_c0_idx"))
        else:
            src_idx = (g_idx, n_idx, back_c1g_idx, back_h_idx - self.pad[0],
                       back_w_idx - self.pad[2], src_info_dict.get("back_c0_idx"))

        return tvm.select(tvm.any(back_h_idx < self.pad[0],
                                  back_h_idx > fmap_height + self.pad[0] - 1,
                                  back_w_idx < self.pad[2],
                                  back_w_idx > fmap_width + self.pad[2] - 1,
                                  back_c1g_idx + g_idx * fmap_c1_g > fmap_channel_1 - 1),
                          tvm.const(0, src_tensor.dtype),
                          src_tensor(*src_idx))

    def _fmap_2_fractal_dma(self, fmap_shape_fmap_matrix, fmap_l1):
        """
        dma_mode: move fmap to L0B
        """
        def __fmap_2_fractal_dma_compute(idx):
            g_idx, n_idx, howo_1_idx, c1kk_idx, c0_idx, howo_0_idx = idx
            howo_idx = howo_1_idx * self.c0_size + howo_0_idx
            src_howo_1_idx = howo_idx // BLOCK_SIZE
            src_howo_0_idx = howo_idx % BLOCK_SIZE
            cinkk_idx = c1kk_idx * BLOCK_SIZE + c0_idx
            src_c1kk_idx = cinkk_idx // self.c0_size
            src_c0_idx = cinkk_idx % self.c0_size
            if self.fp32_input:
                # L1 to l0b enable load2d to load3d, which requires L1 type Nz
                return fmap_l1(g_idx, n_idx, src_c1kk_idx, src_howo_1_idx, src_howo_0_idx, src_c0_idx)
            return fmap_l1(g_idx, n_idx, src_howo_1_idx, src_c1kk_idx, src_howo_0_idx, src_c0_idx)

        return tvm.compute(fmap_shape_fmap_matrix,
                           lambda *idx: __fmap_2_fractal_dma_compute(idx),
                           name='fmap_2_fractal_dma',
                           tag='fmap_2_fractal_dma',
                           attrs={
                               'pad': self.pad,
                               'stride': self.stride,
                               'dilation': self.dilation,
                               'kernel_size': self.weight_shape
                           })

    def _mad_process(self, grads_fractal, fmap_fractal):
        """
        Mad process for FP16 and FP32
        Parameters
        ----------
        grads_fractal : tensor in L0A

        fmap_fractal : tensor in L0B

        Returns
        -------
        None
        """
        group_dict = self.group_dict
        real_g = group_dict.get("real_g")
        fmap_c1_g = group_dict.get("cin1_g")
        grads_channel_g = group_dict.get("cout_g")
        _, _, kernel_height, kernel_width = self.weight_shape
        _, _, _, _, fmap_c0 = self.shape_x_5hd
        # shape of result dw [group,n1,m,n0]
        if self.fp32_input:
            self.dw_ddr = self._mad_process_channel_split(grads_fractal, fmap_fractal)
        else:
            dw_shape_para = (real_g, fmap_c1_g, kernel_height, kernel_width, grads_channel_g)
            dw_shape = self._get_dw_shape(dw_shape_para, fmap_c0)
            self.shapelist['dw'] = dw_shape
            dw_cc = self._mad(dw_shape, grads_fractal, fmap_fractal)
            self.dw_ddr = dw_cc

    def _mad_process_channel_split(self, grads_fractal, fmap_fractal):
        """
        Mad process for FP32
        Parameters
        ----------
        grads_fractal : tensor in L0A

        fmap_fractal : tensor in L0B

        Returns
        -------
        None
        """
        group_dict = self.group_dict
        real_g = group_dict.get("real_g")
        fmap_c1_g = group_dict.get("cin1_g")
        grads_channel_g = group_dict.get("cout_g")
        _, _, kernel_height, kernel_width = self.weight_shape
        _, _, _, _, fmap_c0 = self.shape_x_5hd
        dw_shape = (
            real_g,
            cube_util.ceil_div(fmap_c1_g * kernel_height * kernel_width, 2),
            grads_channel_g,
            fmap_c0 * 2
        )
        dw_cc = self._mad(dw_shape, grads_fractal, fmap_fractal)
        # do channel split
        # In float32 situation, [real_cin*fmap_c0] is aligned to 8, while [fmap_c1_g*fmap_c0] is aligned to 16.
        # And we need to cut the channel_split shape by real_cin value.
        cin1_g = group_dict.get("cin1_g")
        # only supports the output of FractalZ
        # does not enable k_repeat, c1 and khkw are not separated
        dw_c_split_shape = (real_g, cin1_g * kernel_height * kernel_width, grads_channel_g, fmap_c0)
        dw_c_split = tvm.compute(
            dw_c_split_shape,
            lambda g_idx, c1_kk_idx, grads_c_idx, c0_idx:
                dw_cc(g_idx, c1_kk_idx // 2, grads_c_idx, c1_kk_idx % 2 * self.c0_size + c0_idx).astype(self.res_dtype),
            name='dw_c_split',
            tag=self.optag + "_c_split",
            attrs={'kernel_name': self.kernel_name})
        return dw_c_split


class DynamicConv2dBpFilterParams:
    """
    Class for parameters of dynamic conv2d_fp_filter.
    """

    flag_all_one_case = False
    flag_load3d_w_split_case = False
    correct_range_flag = False
    load_mode = 0
    binary_mode = 0
    op_type = ""
    ori_tensors = {}
    tiling_info_dict = {}
    var_map = {}
    attrs = {}
    dma_c04_flag = False
    strideh_read_flag = 0
    l0b_dma_flag = False
    conv1d_flag = False
    flag_load3d_special_case = False
    linear_embedding_opti_flag = False
