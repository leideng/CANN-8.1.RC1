#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
array
"""
from __future__ import absolute_import
from __future__ import print_function
from __future__ import division
import warnings
import types

from tbe import tvm
from tbe.common.context import op_context
from tbe.common.testing.dsl_source_info import source_info_decorator
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.compute.transdata import transdata_compute
from tbe.dsl.base.expr_compare import expr_equal
from .util import dtype_check_decorator
from .util import check_input_tensor_shape
from .util import shape_to_list
from .util import in_dynamic_and_static_unify

NAME_INDEX = [0]


def _index_offset(shape, axis, offset, *index):
    """Compute the offset of index along one dimension.

    Parameters
    ----------
    shape: list
        shape of tensor.
    axis: int
        the dimension along which to split.
    offset: int
        axis offset.
    index: list or tuple
        index value list.

    Returns
    -------
    output_index: list
        output index with one input index value add axis offset.
    """
    input_index = list(index)
    output_index = []
    for i, _ in enumerate(shape):
        if i == axis:
            input_index[i] = input_index[i] + offset
        output_index.append(input_index[i])

    return output_index


@source_info_decorator()
def split_compute_com(data, split_dim, size_splits):
    """
    Split a tensor into len(size_splits) tensors along one dimension
    """
    warnings.warn("split_compute_com is expired, please replace it with the func split",
                  DeprecationWarning)
    return split(data, split_dim, size_splits)


def _unify_split(input_tensor, split_dim, size_splits):
    def compute_func(*indice):
        """
        split compute expr
        """
        return input_tensor(*indice[:split_dim], indice[split_dim] + offset, *indice[split_dim + 1:])

    if split_dim < 0:
        split_dim += len(input_tensor.shape)

    res = []
    res_shape = shape_to_list(input_tensor.shape)
    offset = 0
    with tvm.tag_scope("split"):
        for index, split_len in enumerate(size_splits):
            res_shape[split_dim] = split_len
            split_n = tvm.compute(res_shape, compute_func, name=f"split_{index}")
            offset += split_len
            res.append(split_n)

    return res


@source_info_decorator()
def split(data, split_dim, size_splits):
    """Split a tensor into len(size_splits) tensors along one dimension.

    Parameters
    ----------
    data: TVM tensor
        input tensor.
    split_dim: int
        the dimension along which to split.
    size_splits: list or tuple
        a Python list containing the sizes of each output tensor along `split_dim`.

    Returns
    -------
    output_shape_list: list
        the list of output shapes.
    output_tensor_list: list
        the list of output tensors, output tensor type is TVM tensor.
    """
    if in_dynamic_and_static_unify():
        return _unify_split(data, split_dim, size_splits)

    input_shape = shape_to_list(data.shape)

    output_shape_list = []
    for size in size_splits:
        input_shape[split_dim] = size
        output_shape_list.append(list(input_shape))

    offset = 0
    output_shape = None
    output_tensor_list = []
    for i, _ in enumerate(output_shape_list):
        output_shape = output_shape_list[i]
        name = 'tensor{}'.format(str(i))
        output_tensor = tvm.compute(
            output_shape,
            lambda *index: data(
                *_index_offset(output_shape, split_dim, offset, *index)),
            name=name, tag="split_com|compute_{}".format(str(i)))
        output_tensor_list.append(output_tensor)
        offset = offset + output_shape[split_dim]

    return output_shape_list, output_tensor_list


@dtype_check_decorator
def concat(raw_tensors, axis):
    """
    concat shapes at axis,  support int8, uint8, int16, int32 float16, float32
    Parameters
    ----------
    raw_tensors : list of tensors
    axis : concat axis
    Returns
    -------
    concat tensor :
    """
    if axis < 0:
        axis = axis + len(raw_tensors[0].shape)
    _concat_para_check(raw_tensors, axis)

    if in_dynamic_and_static_unify():
        return _unify_concat(raw_tensors, axis)

    def _get_input_tensors():
        shapes = []
        for in_tensor in list(raw_tensors):
            shape = [int(in_tensor.shape[i].value) for i in range(len(in_tensor.shape))]
            shapes.append(shape)

        _shapes = list(shapes)
        return _shapes

    shapes = _get_input_tensors()

    res_shape = shapes[0][:]
    for i in range(1, len(shapes)):
        res_shape[axis] += shapes[i][axis]

    sel = []
    n_tensor = len(raw_tensors)

    def compute_func(*indice):
        """
        concat compute expr
        """
        if n_tensor > 1:
            for tensor_i in range(n_tensor - 1):
                if tensor_i == 0:
                    tensor_a = raw_tensors[0]
                    tensor_b = raw_tensors[1]
                    shape_c = shapes[0][:]
                    indice2 = list(indice[:])
                    indice2[axis] = indice[axis] - tensor_a.shape[axis]
                    sel.append(
                        tvm.select(indice[axis] < shape_c[axis],
                                   tensor_a[indice], tensor_b[tuple(indice2)]))
                    shape_c[axis] += shapes[1][axis]
                else:
                    tensor_a = sel[tensor_i - 1]
                    tensor_b = raw_tensors[tensor_i + 1]
                    indice2 = list(indice[:])
                    indice2[axis] = indice[axis] - shape_c[axis]
                    sel.append(tvm.select(indice[axis] < shape_c[axis],
                                          tensor_a, tensor_b[tuple(indice2)]))
                    shape_c[axis] += shapes[tensor_i + 1][axis]
        else:
            return raw_tensors[0][indice]

        return sel[-1]

    res = tvm.compute(res_shape, compute_func, name="concat", tag="concat")

    return res


def _unify_concat(input_tensors, axis):
    """
    concat shapes at axis,  support int8, uint8, int16, int32, float16, float32, int64, uint64
    :param input_tensors: list[tvm.tensor]
    list of input tensors
    :param axis: int
    concat axis
    :return: tvm.tensor: A concat Tensor
    """

    def concat_func(*indices):
        func = None
        concat_axis_size = sum(t.shape[axis] for t in input_tensors)
        for tensor in reversed(input_tensors):
            index = []
            for i, _ in enumerate(dst_shape):
                if i == axis:
                    index.append(indices[i] - (concat_axis_size - tensor.shape[axis]))
                else:
                    index.append(indices[i])
            if func is None:
                func = tensor(*index)
            else:
                func = tvm.select(indices[axis] < concat_axis_size, tensor(*index), func)
            concat_axis_size -= tensor.shape[axis]
        return func

    with tvm.tag_scope("concat"):
        dst_shape = list(input_tensors[0].shape)
        concat_axis_size = sum(t.shape[axis] for t in input_tensors)
        dst_shape[axis] = concat_axis_size
        concat = tvm.compute(dst_shape, concat_func, name="concat")
    return concat


def _concat_para_check(raw_tensors, axis):
    """
    concat parameter check

    Parameters
    ----------
    raw_tensors : list of tensors
    axis : concat axis

    Returns
    -------
    rasie runtime error
    """
    if not isinstance(axis, int):
        raise RuntimeError("The axis type must be int")

    # check shape
    if axis < 0 or axis >= len(raw_tensors[0].shape):
        raise RuntimeError(
            "concat axis must be in [-%d - %d), actual is %d"
            % (len(raw_tensors[0].shape), len(raw_tensors[0].shape), axis))
    check_input_tensor_shape(raw_tensors[0])
    for i in range(1, len(raw_tensors)):
        if not isinstance(raw_tensors[i], tvm.Tensor):
            raise RuntimeError("The each element of input type must be tvm.tensor")
        check_input_tensor_shape(raw_tensors[i])
        if raw_tensors[i].dtype != raw_tensors[0].dtype:
            raise RuntimeError("dtype must be the same to each other")
        if in_dynamic_and_static_unify():
            for j in range(len(raw_tensors[0].shape)):
                if (j != axis) and not expr_equal(raw_tensors[i].shape[j], raw_tensors[0].shape[j]):
                    raise RuntimeError(
                        "concat input shape value must be the same to each other except concat axis")
        else:
            for j in range(len(raw_tensors[0].shape)):
                if (j != axis) and (raw_tensors[i].shape[j].value != raw_tensors[0].shape[j].value):
                    raise RuntimeError(
                        "concat input shape len must be the same to each other except concat axis")


@source_info_decorator()
@dtype_check_decorator
def set_value(tensor, condition, value):
    """
    set specified value
    Parameters
    ----------
    tensor: tvm.tensor

    condition: lambda expr

    value: const, variable or lambda expr
    Returns
    -------
    wrapped_tensor: updated tensor
    """
    shape = tensor.shape
    if isinstance(value, types.FunctionType):
        lambda_func = lambda *indice: tvm.select(condition(*indice), value(*indice), tensor(*indice))
    else:
        lambda_func = lambda *indice: tvm.select(condition(*indice), value, tensor(*indice))

    name = "set_value_" + str(NAME_INDEX[0])
    NAME_INDEX[0] += 1

    with tvm.tag_scope("set_value"):
        out = tvm.compute(shape, lambda_func, name=name)

    return out


@source_info_decorator()
@dtype_check_decorator
def transpose(tensor, axes):
    """
    transpose a tensor by permute

    Parameters
    ----------
    tensor : tvm.tensor
        Original tensor
    axes : list[int]
        Permutes the dimensions according to the value of axes
    Returns
    -------
    tvm.tensor: A transposed Tensor
    """

    def check_input():
        input_shape = shape_to_list(tensor.shape)
        for shape in input_shape:
            if (isinstance(shape, int) and shape < 0) or not isinstance(shape, (tvm.tir.PrimExpr, int)):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": "The input shape value [%s] must be a positive integer or tvm expr"}
                raise RuntimeError(dict_args, get_error_message(dict_args))
        if not isinstance(axes, (list, tuple)):
            dict_args = {"errCode": "E90001", "detailed_cause": "The axes must be list or tuple"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        sorted_axes = sorted(axes)
        base_axes = [i for i, _ in enumerate(axes)]
        if sorted_axes != base_axes:
            dict_args = {"errCode": "E90001", "detailed_cause": "The input axes error, cannot transpose"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    check_input()
    with tvm.tag_scope("transpose"):
        src_shape = tensor.shape
        dst_shape = tuple(src_shape[i] for i in axes)
        attrs = {"permute": axes}
        transpose = tvm.compute(dst_shape,
                                lambda *index: tensor(*(x for _, x in sorted(zip(axes, index)))),
                                attrs=attrs, name="transpose")
    return transpose


# 'pylint: disable=too-many-arguments
@source_info_decorator()
@dtype_check_decorator
def gather(params, indices, axis=None, batch_dims=0, negative_index_support=False, support_out_of_bound_index=True):
    """
    :param params:
    :param indices:
    :param axis:
    :param batch_dims:
    :param support_out_of_bound_index:
    :return:
    """
    p_shape = params.shape
    p_dtype = params.dtype
    i_shape = indices.shape
    p_name = params.name
    i_name = indices.name
    i_dtype = indices.dtype

    i_bound_min = 0
    i_bound_max = p_shape[axis]

    # check indices dytpe
    _check_gather_indices_dtype(i_dtype)

    g_shape = p_shape[:axis] + i_shape[batch_dims:] + p_shape[axis + 1:]

    def index(i, negative_index_support):
        end = axis + len(i_shape) - batch_dims
        idx_pos = i[:batch_dims] + i[axis:end]
        if negative_index_support:
            gather_size = params.shape[axis] if params.shape[axis] != 0 else 1
            real_idx = indices[idx_pos] - tvm.floordiv(indices[idx_pos], gather_size) * gather_size
            return list(i[:axis]) + [real_idx] + list(i[end:])
        return list(i[:axis]) + [indices[idx_pos]] + list(i[end:])

    def get_index_data(i, negative_index_support):
        end = axis + len(i_shape) - batch_dims
        idx_pos = i[:batch_dims] + i[axis:end]
        if negative_index_support:
            gather_size = params.shape[axis] if params.shape[axis] != 0 else 1
            real_idx = indices[idx_pos] - tvm.floordiv(indices[idx_pos], gather_size) * gather_size
            return real_idx
        return indices[idx_pos]

    op_name = "gather"
    compute_name = op_name + str(_get_next_name_index())

    if support_out_of_bound_index:
        with tvm.tag_scope(op_name):
            g_tensor = tvm.compute(g_shape, lambda *i: tvm.select(
                tvm.all(get_index_data(i, negative_index_support) < i_bound_max,
                        get_index_data(i, negative_index_support) >= i_bound_min),
                params(*index(i, negative_index_support)),
                tvm.const(0, p_dtype)), name=compute_name, attrs={"params_name": p_name, "indices_name": i_name})
    else:
        with tvm.tag_scope(op_name):
            g_tensor = tvm.compute(g_shape, lambda *i: params(*index(i, negative_index_support)), name=compute_name,
                                   attrs={"params_name": p_name, "indices_name": i_name})

    return g_tensor


@source_info_decorator()
@dtype_check_decorator
def gather_nd(params, indices, batch_dims=0, negative_index_support=False, support_out_of_bound_index=True):
    """
    :param params:
    :param indices:
    :param batch_dims:
    :negative_index_support:
    :support_out_of_bound_index:
    :return:
    """
    p_shape = params.shape
    p_dtype = params.dtype
    i_shape = indices.shape
    p_name = params.name
    i_name = indices.name
    i_dtype = indices.dtype
 
    # check indices dytpe
    _check_gather_indices_dtype(i_dtype)

    rank_value = i_shape[-1].value if isinstance(i_shape[-1], tvm.expr.ConstExpr) else i_shape[-1]

    if rank_value > len(p_shape):
        dict_args = {}
        dict_args["errCode"] = "E90001"
        dict_args["detailed_cause"] = "indices last dim value [%u] > params shape length [%u]! " % (
            rank_value, len(p_shape))
        raise RuntimeError(dict_args, get_error_message(dict_args))

    g_shape = i_shape[:-1] + p_shape[batch_dims + rank_value:]

    def index(i, negative_index_support):
        p_pos = []
        p_pos.extend(i[:batch_dims])
        axis = len(i_shape) - 1
        for j in range(rank_value):
            if not negative_index_support:
                p_pos.append(indices(*i[:axis], j))
            else:
                gather_size = p_shape[batch_dims + j] if p_shape[batch_dims + j] != 0 else 1
                real_idx = indices(*i[:axis], j) - tvm.floordiv(indices(*i[:axis], j), gather_size) * gather_size
                p_pos.append(real_idx)
        p_pos.extend(i[axis:])
        return p_pos

    def get_index_condition(i, negative_index_support):
        axis = len(i_shape) - 1
        condition = tvm.const(1, dtype="bool")
        for j in range(rank_value):
            if negative_index_support:
                gather_size = p_shape[batch_dims + j] if p_shape[batch_dims + j] != 0 else 1
                real_idx = indices(*i[:axis], j) - tvm.floordiv(indices(*i[:axis], j), gather_size) * gather_size
            else:
                real_idx = indices(*i[:axis], j)
            for k in range(1, rank_value + 1):
                condition = tvm.all(condition, real_idx >= 0)
                if j == k - 1:
                    condition = tvm.all(condition, real_idx < p_shape[k])
        return condition

    op_name = "gather_nd"
    compute_name = op_name + str(_get_next_name_index())
    if support_out_of_bound_index:
        with tvm.tag_scope(op_name):
            g_tensor = tvm.compute(g_shape, lambda *i: tvm.select(get_index_condition(i, negative_index_support),
                                                                  params(*index(i, negative_index_support)),
                                                                  tvm.const(0, p_dtype)), name=compute_name,
                                   attrs={"params_name": p_name, "indices_name": i_name})
    else:
        with tvm.tag_scope(op_name):
            g_tensor = tvm.compute(g_shape, lambda *i: params(*index(i, negative_index_support)), name=compute_name,
                                   attrs={"params_name": p_name, "indices_name": i_name})

    return g_tensor


def _check_gather_indices_dtype(i_dtype):
    """
    :param i_dtype: indices dtype str
    :return:
    """
    if i_dtype not in ("int32, int64"):
        dict_args = {}
        dict_args["errCode"] = "E90001"
        dict_args["detailed_cause"] = "indices tensors dtype must be int32 or int64! " \
                                      "while dtype is [%s], " % (i_dtype,)
        raise RuntimeError(dict_args, get_error_message(dict_args))


def _get_next_name_index():
    """
    :return: next name index as string
    """
    NAME_INDEX[0] += 1
    return NAME_INDEX[0]


@source_info_decorator()
@dtype_check_decorator
def slice(tensor, begin, end, stride=None):
    """
    :param tensor: The tensor from which to slice values.
    :param begin: The begin indexes to slice for each dimension.
    :param end: The end indexes to slice for each dimension and end point not include.
    :param stride: The stride means slice continues by stride length. now stride is not support.
    :return:
    """

    def _get_resust_idx(idxs, begin):
        return [i + b for i, b in zip(idxs, begin)]

    def _check_stride(stride):
        if stride is not None:
            dict_args = {}
            dict_args["errCode"] = "E90001"
            dict_args["detailed_cause"] = "slice stride input must be None"
            raise RuntimeError(dict_args, get_error_message(dict_args))

    def _check_list_input(list_input, input_len):
        if not isinstance(list_input, list):
            dict_args = {}
            dict_args["errCode"] = "E90001"
            dict_args["detailed_cause"] = "input dtype must be list"
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if input_len != len(list_input):
            dict_args = {}
            dict_args["errCode"] = "E90001"
            dict_args["detailed_cause"] = "input lenght must be [%u] " \
                                          "while real is [%u]" % (input_len, len(list_input))
            raise RuntimeError(dict_args, get_error_message(dict_args))

    def _check_begin_end_value(tensor_shape, begin_list, end_list):
        for _tensor_value, _begin, _end in zip(tensor_shape, begin_list, end_list):
            if isinstance(_tensor_value, tvm.tir.IntImm):
                if isinstance(_begin, int):
                    if _begin < 0 or _begin > _tensor_value.value:
                        dict_args = {}
                        dict_args["errCode"] = "E90001"
                        dict_args["detailed_cause"] = "begin [%u] out of range [0, %u]" % (_begin, _tensor_value.value)
                        raise RuntimeError(dict_args, get_error_message(dict_args))

                if isinstance(_end, int):
                    if _end < 0 or _end > _tensor_value.value:
                        dict_args = {}
                        dict_args["errCode"] = "E90001"
                        dict_args["detailed_cause"] = "_end [%u] out of range [0, %u]" \
                                                      % (_end, _tensor_value.value)
                        raise RuntimeError(dict_args, get_error_message(dict_args))

                if isinstance(_begin, int) and isinstance(_end, int):
                    if _end < _begin:
                        dict_args = {}
                        dict_args["errCode"] = "E90001"
                        dict_args["detailed_cause"] = "_begin [%u] must small or equal than _end %u" \
                                                      % (_begin, _end)
                        raise RuntimeError(dict_args, get_error_message(dict_args))

    _check_stride(stride)
    tensor_shape_len = len(tensor.shape)
    _check_list_input(begin, tensor_shape_len)
    _check_list_input(end, tensor_shape_len)
    _check_begin_end_value(tensor.shape, begin, end)

    t_name = tensor.name
    output_shape = [_end - _begin for _begin, _end in zip(begin, end)]
    op_name = "slice"
    compute_name = op_name + str(_get_next_name_index())
    with tvm.tag_scope(op_name):
        slice_res = tvm.compute(output_shape, lambda *i: tensor(*_get_resust_idx(i, begin)), name=compute_name,
                                attrs={"tensor_name": t_name})
    return slice_res


@source_info_decorator()
@dtype_check_decorator
def transdata(tensor, dst_shape, axes_map, pad_value=0):
    """
    transdata a tensor by axes_map and dst_shape

    Parameters
    ----------
    tensor : tvm.tensor
        Original tensor
    dst_shape : list[int]
        Shape of dst_tensor after transdata
    axes_map : dict
        Permutes the dimensions according to the axes_map
    pad_value : int
        Determine the padding value when padding is required
    Returns
    -------
    tvm.tensor: A transdata tensor that shape is dst_shape
    """
    def forward_get_permute(_axes_map):
        permute = []
        for _, v in _axes_map.items():
            if isinstance(v, int):
                permute.append(v)
            elif isinstance(v, (tuple, list)):
                permute.extend(v)
        return [x for _, x in sorted(zip(permute, range(0, len(permute))))]

    def backward_get_permute(_axes_map):
        permute = []
        for k, _ in _axes_map.items():
            if isinstance(k, int):
                permute.append(k)
            elif isinstance(k, (tuple, list)):
                permute.extend(k)
        return permute

    def check():
        if axes_map == "c04Forward" or axes_map == "c04Backward":  # C04
            return
        input_shape = shape_to_list(tensor.shape)
        for shape in input_shape:
            if (isinstance(shape, int) and shape < 0) or not isinstance(shape, (tvm.tir.PrimExpr, int)):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": "The input shape value [%s] must be a positive integer or tvm expr"}
                raise RuntimeError(dict_args, get_error_message(dict_args))
        if not isinstance(axes_map, dict):
            dict_args = {"errCode": "E90001", "detailed_cause": "The axes_map must dict"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        is_forward = False
        is_backward = False
        for k, v in axes_map.items():
            if isinstance(k, (list, tuple)) and isinstance(v, int):
                is_backward = True
            if isinstance(k, int) and isinstance(v, (list, tuple)):
                is_forward = True

        if is_forward and is_backward:
            # Pad and DePad existed together
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "Pad and DePad existed together is invalid"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if not is_forward and not is_backward:
            # Pad and DePad not existed
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "Pad or DePad is required in transdata, "
                                           "but axes_map not exist Pad or DePad"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if is_forward:
            _axes_map = dict(sorted(axes_map.items()))
            permute = forward_get_permute(_axes_map)
            # check ele of axes_map
            for k, v in _axes_map.items():
                if not isinstance(k, int):
                    dict_args = {"errCode": "E90001", "detailed_cause": "In forward, key of axes_map must be int"}
                    raise RuntimeError(dict_args, get_error_message(dict_args))
                if not isinstance(v, (int, tuple, list)):
                    dict_args = {"errCode": "E90001", "detailed_cause": "In forward, value of axes_map "
                                                                        "should in [int, tuple, list]"}
                    raise RuntimeError(dict_args, get_error_message(dict_args))
                if isinstance(v, (list, tuple)) and len(v) > 2:
                    dict_args = {"errCode": "E90001", "detailed_cause":
                        "In forward, only support split DimX to [DimX0, DimX1],"
                        f"but value of axes_map is {v}"}
                    raise RuntimeError(dict_args, get_error_message(dict_args))
        else:
            _axes_map = dict(sorted(axes_map.items(), key=lambda x: x[1]))
            permute = backward_get_permute(_axes_map)
            # check ele of axes_map
            for k, v in _axes_map.items():
                if not isinstance(v, int):
                    dict_args = {"errCode": "E90001", "detailed_cause": "In backward, value of axes_map must be int"}
                    raise RuntimeError(dict_args, get_error_message(dict_args))
                if not isinstance(k, (int, tuple, list)):
                    dict_args = {"errCode": "E90001", "detailed_cause": "In backward, key of axes_map "
                                                                        "should in [int, tuple, list]"}
                    raise RuntimeError(dict_args, get_error_message(dict_args))
                if isinstance(k, (list, tuple)) and len(k) > 2:
                    dict_args = {"errCode": "E90001", "detailed_cause":
                        "In forward, only support fuse [DimX0, DimX1] to DimX,"
                        f"but key of axes_map is {k}"}
                    raise RuntimeError(dict_args, get_error_message(dict_args))

        sorted_axes = sorted(permute)
        base_axes = [i for i, _ in enumerate(permute)]
        if sorted_axes != base_axes:
            dict_args = {"errCode": "E90001", "detailed_cause": "The input axes_map error, cannot do transdata"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    check()
    return transdata_compute.transdata(tensor, dst_shape, axes_map, pad_value)


@source_info_decorator()
@dtype_check_decorator
def sort(tensor, sort_axis=-1, direction='ascend', return_type='value', indices_dtype=None, need_cast=False):
    """
    :param tensor: tvm.tensor
        The input tensor to sort
    :param sort_axis: int
        The axis along which to sort
    :param direction: str
        The sorting order('ascend' or 'descend')
    :param return_type: str
        Return output condition('value', 'indices', 'both')
    :param indices_dtype: str
        Return indices dtype('int32', 'int64')
    """
    def check_input():
        input_shape = shape_to_list(tensor.shape)
        for shape in input_shape:
            if (isinstance(shape, int) and shape < 0) or not isinstance(shape, (tvm.tir.PrimExpr, int)):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": "The input dim value must be positive integer or tvm expr"}
                raise RuntimeError(dict_args, get_error_message(dict_args))

        if not isinstance(sort_axis, int):
            dict_args = {"errCode": "E90001", "detailed_cause": "The sort axis must be int"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        axis = sort_axis
        if axis < 0:
            axis += len(input_shape)
        if axis >= len(input_shape):
            dict_args = {"errCode": "E90001", "detailed_cause": "The sort axis is out of range"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    check_input()
    op_name = "sort"
    compute_name = op_name + str(_get_next_name_index())
    with tvm.tag_scope(op_name):
        s_axis = tvm.sort_axis((0, tensor.shape[sort_axis]), name="s")
        if return_type == "both":
            if need_cast:
                res_data, res_idx = \
                    tvm.compute(tensor.shape,
                                lambda i, j: (tvm.sort(tensor[i, s_axis], axis=s_axis, index=j, order=direction,
                                                    output="value").astype("bfloat16"),
                                            tvm.sort(tensor[i, s_axis], axis=s_axis, index=j, order=direction,
                                                    output="index").astype(indices_dtype)),
                                name=compute_name)
                return res_data, res_idx
            else:
                res_data, res_idx = \
                    tvm.compute(tensor.shape,
                                lambda i, j: (tvm.sort(tensor[i, s_axis], axis=s_axis, index=j, order=direction,
                                                    output="value"),
                                            tvm.sort(tensor[i, s_axis], axis=s_axis, index=j, order=direction,
                                                    output="index").astype(indices_dtype)),
                                name=compute_name)
                return res_data, res_idx
        elif return_type == "value":
            if need_cast:
                res_data = tvm.compute(tensor.shape,
                                   lambda i, j: tvm.sort(tensor[i, s_axis], axis=s_axis, index=j,
                                                          order=direction).astype("bfloat16"),
                                   name=compute_name)
            else:
                res_data = tvm.compute(tensor.shape,
                                    lambda i, j: tvm.sort(tensor[i, s_axis], axis=s_axis, index=j, order=direction),
                                    name=compute_name)
            return res_data
        else:
            dict_args = {"errCode": "E90001", "detailed_cause": "Sort not support only return indices now"}
            raise RuntimeError(dict_args, get_error_message(dict_args))


def _scatter_compute(raw_tensors, reduction, scatter_axis, is_ref, scatter_op, support_out_of_bound_index):
    def _check_params():
        check_input_tensor_shape(var)
        check_input_tensor_shape(indices)
        check_input_tensor_shape(update)

        if not isinstance(reduction, str):
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "Type of reduction must be str"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if not isinstance(scatter_axis, int) or scatter_axis != 0:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "Type of scatter_axis must be int and the value only supports 0 temporarily"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if not isinstance(is_ref, bool) or not is_ref:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "Type of is_ref must be bool and the value only supports True temporarily"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        indices_rank = len(indices_shape)
        update_rank = len(update_shape)

        if update_rank < indices_rank:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "rank of update should be equal or more than the rank of indices"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    def _get_sparse_cond(*idx):
        sparse_idx = []
        indices_idx = []
        for i in range(sparse_rank):
            sparse_idx.append(sparse_axes[i])

        for i in range(scatter_rank):
            tmp = sparse_idx[:]
            tmp.append(i)
            indices_idx.append(tmp)

        condition = tvm.const(1, dtype="bool")
        context = op_context.get_context()
        inplace = context.get_addition("build_options")
        is_inplace = inplace and isinstance(inplace, str) and "is_inplace" in inplace
        if is_inplace and var_shape[0] != 0:
            size = var_shape[0]
            condition = tvm.all(condition,
                                (indices(*indices_idx[0]) - tvm.floordiv(indices(*indices_idx[0]), size) * size) == idx[
                                    scatter_axis])
        else:
            for i in range(scatter_rank):
                condition = tvm.all(condition, indices(*indices_idx[i]) == idx[scatter_axis + i])

        return condition

    def _get_update_idx(*idx):
        update_data_idx = []
        for i in range(sparse_rank):
            update_data_idx.append(sparse_axes[i])
        update_data_idx.extend(idx[scatter_axis + scatter_rank:])
        return update_data_idx

    def _get_scatter_func(*i):
        if reduction.lower() == "add" or reduction.lower() == "":
            scatter_func = var(*i) + update(*_get_update_idx(*i))
        elif reduction.lower() == "div":
            scatter_func = var(*i) / update(*_get_update_idx(*i))
        elif reduction.lower() == "sub":
            scatter_func = var(*i) - update(*_get_update_idx(*i))
        elif reduction.lower() == "mul":
            scatter_func = update(*_get_update_idx(*i)) * var(*i)
        elif reduction.lower() == "max":
            scatter_func = tvm.max(update(*_get_update_idx(*i)), var(*i))
        elif reduction.lower() == "min":
            scatter_func = tvm.min(update(*_get_update_idx(*i)), var(*i))
        elif reduction.lower() == "update":
            scatter_func = update(*_get_update_idx(*i))
        else:
            dict_args = {"errCode": "E90003", "detailed_cause": "Not Support yet for func [%s]. "
                                                                "func must be add, "
                                                                "div, sub mul, max, min or update" % reduction}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        return scatter_func

    var = raw_tensors[0]
    indices = raw_tensors[1]
    update = raw_tensors[2]
    var_shape = var.shape
    indices_shape = indices.shape
    update_shape = update.shape
    scatter_rank = indices_shape[-1].value if scatter_op == "scatter_nd" else 1
    _check_params()

    sparse_axes = []
    actual_indices_dims = indices_shape[:-1]
    for dim, dim_size in enumerate(actual_indices_dims):
        sparse_axes.append(tvm.sparse_axis((0, dim_size), name='k' + str(dim + 1)))
    sparse_rank = len(sparse_axes)

    if reduction.lower() != "update":
        _compute_func = tvm.compute(var_shape,
                                    lambda *i: tvm.sparse(
                                        tvm.select(_get_sparse_cond(*i), _get_scatter_func(*i)),
                                        axis=[*sparse_axes], out_of_bound_check=support_out_of_bound_index),
                                    name=scatter_op + '_' + reduction + str(_get_next_name_index()),
                                    attrs={"var_name": var.name, "indices_name": indices.name,
                                           "update_name": update.name})
    else:
        _compute_func = tvm.compute(var_shape,
                                    lambda *i: tvm.sparse(
                                        tvm.select(_get_sparse_cond(*i), _get_scatter_func(*i), var(*i)),
                                        axis=[*sparse_axes], out_of_bound_check=support_out_of_bound_index),
                                    name=scatter_op + '_' + reduction + str(_get_next_name_index()),
                                    attrs={"var_name": var.name, "indices_name": indices.name,
                                           "update_name": update.name})
    return _compute_func


@source_info_decorator()
@dtype_check_decorator
def scatter(var, indices, update, reduction, scatter_axis=0, is_ref=True, support_out_of_bound_index=True):
    """
    :param var:
    :param indices:
    :param update:
    :param reduction:
    :param scatter_axis:
    :param is_ref:
    :support_out_of_bound_index:
    :return:
    """
    scatter_op = "scatter_" + reduction.lower()
    with tvm.tag_scope(scatter_op):
        res = _scatter_compute([var, indices, update], reduction, scatter_axis, is_ref, "scatter",
                               support_out_of_bound_index)

    return res


@source_info_decorator()
@dtype_check_decorator
def scatter_nd(var, indices, update, reduction, scatter_axis=0, is_ref=True, support_out_of_bound_index=True):
    """
    :param var:
    :param indices:
    :param update:
    :param reduction:
    :param scatter_axis:
    :param is_ref:
    :support_out_of_bound_index:
    :return:
    """
    scatter_op = "scatter_nd_" + reduction.lower() if reduction != "" else "scatter_nd"
    with tvm.tag_scope(scatter_op):
        res = _scatter_compute([var, indices, update], reduction, scatter_axis, is_ref, "scatter_nd",
                               support_out_of_bound_index)

    return res


# 'pylint: disable=too-many-arguments
@source_info_decorator()
@dtype_check_decorator
def segment(tensor, segment_ids, num_segments, init_value, segment_op="segmentensor_sum",
            support_out_of_bound_index=True):
    x_name = tensor.name
    id_name = segment_ids.name
    shape_tensor = shape_to_list(tensor.shape)
    shape_ids = shape_to_list(segment_ids.shape)
    reduce_k = tvm.reduce_axis((0, shape_ids[0]), "reduce_k")
    check_ids = support_out_of_bound_index
    def __segment_select(indices):
        tmp = tvm.select(indices[0] == segment_ids[reduce_k], tensor[(reduce_k,) + indices[1:]],
                         tvm.const(init_value, tensor.dtype))
        return tmp

    if segment_op == "segmentensor_min":
        lambda_func = lambda *indices: tvm.min(__segment_select(indices), axis=[reduce_k], out_of_bound_check=check_ids)
    elif segment_op == "segmentensor_max":
        lambda_func = lambda *indices: tvm.max(__segment_select(indices), axis=[reduce_k], out_of_bound_check=check_ids)
    elif segment_op == "segmentensor_prod":
        lambda_func = lambda *indices: tvm.prod(__segment_select(indices), axis=[reduce_k],
                                                out_of_bound_check=check_ids)
    elif segment_op == "segmentensor_sum":
        lambda_func = lambda *indices: tvm.sum(__segment_select(indices), axis=[reduce_k], out_of_bound_check=check_ids)
    else:
        raise RuntimeError("operation %s not support yet" % segment_op)

    name = "data_" + segment_op.split("_")[-2] + '_' + tensor.name.split("_")[-1]
    shape_tensor[0] = num_segments
    with tvm.tag_scope(
            segment_op + "|" + str("num_segments") + "|" + str(init_value)):
        tmp = tvm.compute(shape_tensor, lambda_func, name=name, attrs={"var_name":x_name, "id_name": id_name})
    return tmp


@source_info_decorator()
@dtype_check_decorator
def topk(tensor, k, sort_axis=-1, direction='ascend', return_type='value', indices_dtype=None, need_cast=False):
    """
    :param tensor: tvm.tensor
        The input tensor to sort
    :param k: int/Expr
        Number of top elements to look for along the sort axis
    :param sort_axis: int
        The axis along which to sort
    :param direction: str
        The sorting order('ascend' or 'descend')
    :param return_type: str
        Return output condition('value', 'indices', 'both')
    :param indices_dtype: str
        Return indices dtype('int32', 'int64')
    """
    def check_input():
        input_shape = shape_to_list(tensor.shape)
        for shape in input_shape:
            if (isinstance(shape, int) and shape < 0) or not isinstance(shape, (tvm.tir.PrimExpr, int)):
                dict_args = {"errCode": "E90001",
                             "detailed_cause": "The input shape must be positive integer or tvm expr"}
                raise RuntimeError(dict_args, get_error_message(dict_args))

        if not isinstance(sort_axis, int):
            dict_args = {"errCode": "E90001", "detailed_cause": "The sort axis must be int"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        axis = sort_axis
        if axis < 0:
            axis += len(input_shape)
        if axis >= len(input_shape):
            dict_args = {"errCode": "E90001", "detailed_cause": "The sort axis is invalid, out of range"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if not isinstance(k, (int, tvm.tir.PrimExpr)):
            dict_args = {"errCode": "E90001", "detailed_cause": "The k value must be integer or tvm expr"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
        if isinstance(k, int) and k < 1:
            dict_args = {"errCode": "E90001", "detailed_cause": "The k value is out of range"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    check_input()
    op_name = "topk"
    compute_name = op_name + str(_get_next_name_index())
    with tvm.tag_scope(op_name):
        s_axis = tvm.sort_axis((0, tensor.shape[sort_axis]), name="s")
        output_shape = list(tensor.shape)
        output_shape[sort_axis] = k
        if return_type == "both":
            if need_cast:
                res_data, res_idx = \
                    tvm.compute(output_shape,
                                lambda i, j: (tvm.topk(tensor[i, s_axis], axis=s_axis, index=j, k=k, order=direction,
                                                       output="value").astype("bfloat16"),
                                              tvm.topk(tensor[i, s_axis], axis=s_axis, index=j, k=k, order=direction,
                                                       output="index").astype(indices_dtype)),
                                name=compute_name)
                return res_data, res_idx
            else:
                res_data, res_idx = \
                    tvm.compute(output_shape,
                                lambda i, j: (tvm.topk(tensor[i, s_axis], axis=s_axis, index=j, k=k, order=direction,
                                                       output="value"),
                                              tvm.topk(tensor[i, s_axis], axis=s_axis, index=j, k=k, order=direction,
                                                       output="index").astype(indices_dtype)),
                                name=compute_name)
                return res_data, res_idx
        elif return_type == "value":
            if need_cast:
                res_data = tvm.compute(output_shape,
                                       lambda i, j: tvm.topk(
                                           tensor[i, s_axis],
                                           axis=s_axis, index=j, k=k, order=direction).astype("bfloat16"),
                                       name=compute_name)
                return res_data
            else:
                res_data = tvm.compute(output_shape,
                                       lambda i, j: tvm.topk(
                                           tensor[i, s_axis], axis=s_axis, index=j, k=k, order=direction),
                                       name=compute_name)
                return res_data
        else:
            dict_args = {"errCode": "E90001",
                         "detailed_cause": "Topk not support only return indices now"}
            raise RuntimeError(dict_args, get_error_message(dict_args))
