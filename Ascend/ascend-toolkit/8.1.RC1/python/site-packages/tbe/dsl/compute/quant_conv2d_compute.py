#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

from tbe.tvm import Tensor
from tbe.dsl.compute.conv_compute import conv
from impl.util import util_conv2d
from impl.util.util_conv2d_dynamic import Conv2dParaProcess
from impl.util.platform_adapter import tvm
from impl.dynamic.ascend_dequant import ascend_dequant_compute
from impl.util.platform_adapter import error_manager_cube as err_man
from impl.dynamic.fix_pipe import fixpipe_compute
from tbe.common.platform import CUBE_MKN
from tbe.common.utils.op_util import op_util_conv2d
from tbe.common.utils import log
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe.dsl.compute.conv_compute import ConvParam
from tbe.common.register import register_pass_for_fusion
from te.utils import shape_util

N_DIM = 0
H_DIM = 2
W_DIM = 3
MKN_N_INDEX = 2

QUANT_CONV2D_OUTPUT_DTYPE_TO_CONV2D_OUTPUT_DTYPE = {
    "float16": "int32"
}


def _collect_org_tensors(ori_paras):
    """
    get valid tensors
    """
    ori_tensors = {}
    for key, value in ori_paras.items():
        if isinstance(value, dict):
            valid_tensor = isinstance(value.get("ori_shape"), (list, tuple)) \
                           and len(value.get("ori_shape")) > 0
            if valid_tensor:
                ori_tensors[key] = value
        elif isinstance(value, tvm.Tensor):
            para_dict = {'shape': shape_util.shape_to_list(value.shape),
                         'dtype': value.dtype, 'ori_shape': shape_util.shape_to_list(value.op.attrs.get("ori_shape")),
                         'format': value.op.attrs.get("format"), 'ori_format': value.op.attrs.get("ori_format")}
            ori_tensors[key] = para_dict
    return ori_tensors


def process_scale_tensor_for_v220(scale_tensor: Tensor, cout1_opt: int):
    co0 = CUBE_MKN[ConvParam.para_dict.get("res_dtype")].get('mac')[MKN_N_INDEX]
    scale_real_dim_len = scale_tensor.shape[0]
    # ND -> NC1HWC0
    shape_5hd = (1, cout1_opt, 1, 1, co0)
    scale_l1 = tvm.compute(shape_5hd,
                           lambda batch_idx, co1_idx, ho_idx, wo_idx, co0_idx:
                           tvm.select(
                               (co1_idx * co0 + co0_idx) < scale_real_dim_len,
                               scale_tensor(co1_idx * co0 + co0_idx)),
                           name='scale_l1')

    scale_l1_zero = tvm.compute(shape_5hd,
                                lambda *indice: tvm.const(0, scale_tensor.dtype),
                                name="scale_l1_zero")

    scale_l1_vir_add = tvm.compute(shape_5hd,
                                   lambda *indice:
                                   scale_l1_zero(*indice) + scale_l1(*indice),
                                   name="scale_l1_vir_add",
                                   attrs={"ori_shape": [scale_real_dim_len]})

    ConvParam.tensor_map.update({
        "scale_l1": scale_l1,
        "scale_l1_zero": scale_l1_zero,
        "scale_l1_vir_add": scale_l1_vir_add
    })

    return scale_l1_vir_add


def process_scale_tensor_for_v200(scale_tensor: Tensor, cout1_opt: int):
    co0 = CUBE_MKN[ConvParam.para_dict.get("res_dtype")].get('mac')[MKN_N_INDEX]
    scale_real_dim_len = scale_tensor.shape[0]
    # ND -> NC1HWC0
    shape_5hd = (1, cout1_opt, 1, 1, co0)
    scale_ub = tvm.compute(shape_5hd,
                           lambda batch_idx, co1_idx, ho_idx, wo_idx, co0_idx:
                           tvm.select(
                               co1_idx * co0 + co0_idx < scale_real_dim_len,
                               scale_tensor(co1_idx * co0 + co0_idx)),
                           name='scale_ub')

    scale_ub_zero = tvm.compute(shape_5hd,
                                lambda batch_idx, co1_idx, ho_idx, wo_idx, co0_idx:
                                tvm.select(co1_idx * co0 + co0_idx >= scale_real_dim_len,
                                           tvm.const(0, dtype=scale_tensor.dtype)),
                                name="scale_ub_zero")

    scale_5hd_ub = tvm.compute(shape_5hd,
                               lambda *indice:
                               scale_ub(*indice) + scale_ub_zero(*indice),
                               name="scale_5hd_ub",
                               attrs={"ori_shape": [scale_real_dim_len]})

    ConvParam.tensor_map.update({
        "scale_ub": scale_ub,
        "scale_ub_zero": scale_ub_zero,
        "scale_5hd_ub": scale_5hd_ub
    })

    return scale_5hd_ub



def quant_conv2d_forward_compute(op_type, inputs, weights, scale, bias,
                                 outputs, dtype, strides, pads, dilations, groups=1,
                                 data_format='NHWC', offset_x=0, kernel_name="quant_conv2d",
                                 dsl_flag=True, default_para=None, binary_static_flag=None, options=None):
    """
    QuantConv2d compute

    Notice
    ------
    only used by framework combine with IR

    Parameters
    ----------
    inputs: dict with keys(shape and dtype and range)
        input 4d feature map tensor
    weights: dict with keys(shape and dtype)
        input 4d weight tensor
    scale: dict with keys(shape and dtype) or None
        input scale tensor
    bias: dict with keys(shape and dtype) or None
        input bias tensor
    outputs: dict with keys(shape and dtype)
        output tensor, dtype must be assigned
    strides: tuple/list of 4 integers
        stride on H/W, format sensitive
    pads: tuple/list of 4 integers
        [pad_top, pad_bottom, pad_left, pad_right]
    dilations: tuple/list of 4 integers
        dilation on H/W, format sensitive
    groups: int
        param for group covolution
    data_format: string
        input data format
    offset_x: int
        offset of fmap
    kernel_name: str
        kernel name, default value is "quant_conv2d"

    Returns
    -------
    tvm compute
    """
    if isinstance(options, dict) and options.get("load3d_flag") == 1:
        log.debug("QuantConv2d enter load3d processing!")
    else:
        err_man.raise_err_message_cube("Classify is error, QuantConv2d only supports load3d now!")

    if not outputs.get("ori_shape"):
        outputs["ori_shape"] = default_para.get("ori_shape")
    ori_paras = {
        "op_type": op_type, "inputs": inputs, "weights": weights, "bias": bias,
        "outputs": outputs, "dtype": dtype, "strides": strides, "pads": pads,
        "dilations": dilations, "groups": groups, "data_format": data_format,
        "offset_x": offset_x, "kernel_name": kernel_name, "optim_dict": default_para.get("optim_dict"),
        "binary_static_flag": binary_static_flag
    }
    impl_mode = util_conv2d.get_op_precision_mode(op_type)
    conv_para = Conv2dParaProcess(ori_paras)
    paras = conv_para.config_paras(options)

    pad_t, pad_b, pad_l, pad_r = conv_para.pads
    res_dtype = QUANT_CONV2D_OUTPUT_DTYPE_TO_CONV2D_OUTPUT_DTYPE.get(outputs.get("dtype"), "float16")
    co0 = CUBE_MKN[res_dtype].get('mac')[op_util_conv2d.CUBE_MKN_IDX_N]
    weight_format_str = conv_para.weights.get("ori_format")
    if weight_format_str is None:
        err_man.raise_err_message_cube("Weight is illegal, ori_format of weight is None!")
    pos_cout = weight_format_str.find('N')
    cout1_opt = (conv_para.weights.get("ori_shape")[pos_cout] + co0 - 1) // co0
    conv_res = conv(paras.get("input_tensor"), paras.get("weight_tensor"),
                    {"bias_tensor": paras.get("bias_tensor"),
                     "pad_h": [pad_t, pad_b], "pad_w": [pad_l, pad_r],
                     "stride_h": conv_para.strides[H_DIM], "stride_w": conv_para.strides[W_DIM],
                     "dilate_h": conv_para.dilations[H_DIM], "dilate_w": conv_para.dilations[W_DIM],
                     "filter_h": paras.get("w_shape")[H_DIM],
                     "filter_w": paras.get("w_shape")[W_DIM],
                     "offset_x": default_para.get("offset_x"),
                     "res_dtype": res_dtype,
                     "fusion_para": default_para.get("fusion_para"),
                     "kernel_name": kernel_name,
                     "impl_mode": impl_mode,
                     "group": conv_para.groups,
                     "enlarge": paras.get("group_para").get("enlarge"),
                     "c1_opt": paras.get("group_para").get("c1_opt"),
                     "cout1_opt": cout1_opt,
                     "group_opt": groups,
                     "a_shape": paras.get("in_shape_nc1hwc0"),
                     "weight_fracz_shape": paras.get("w_shape_frac_z"),
                     "weight_ori_shape_nchw": paras.get("w_shape"),
                     "padding_mode": paras.get("padding_mode"),
                     "pooling_mode": paras.get("pooling_mode"),
                     "correct_range_flag": paras.get("correct_range_flag", False),
                     "new_in_range": paras.get("new_in_range"),
                     "ori_tensors": _collect_org_tensors(ori_paras),
                     "cache_tiling_flag": paras.get("cache_tiling_flag"),
                     "ori_shape_attr": paras.get("ori_shape_attr"),
                     "binary_static_flag": binary_static_flag},
                    optim_dict=paras.get("optim_dict"),
                    dsl_flag=dsl_flag)

    if not default_para.get("fusion_op_flag"):
        scale_tensor = tvm.placeholder((paras.get("w_shape")[N_DIM],), name="scale_tensor",
                                       dtype=scale.get("dtype"))
    else:
        scale_tensor = scale

    ConvParam.para_dict["quant_conv2d_paras"] = {
        "strides": default_para.get("strides"),
        "pads": default_para.get("pads"),
        "dilations": default_para.get("dilations"),
        "scale": scale,
        "groups": groups,
    }

    if is_support_fixpipe():
        scale_res = process_scale_tensor_for_v220(scale_tensor, cout1_opt)
        op_res = fixpipe_compute(conv_res, None, scale_res, None, None, None,
                                 None, None, None, None, outputs, [], [], "")
    else:
        scale_res = process_scale_tensor_for_v200(scale_tensor, cout1_opt)
        op_res = ascend_dequant_compute(conv_res, scale_res, None)

    if bias is not None:
        return {"op_placeholder": [paras.get("input_tensor"), paras.get("weight_tensor"),
                                   scale_tensor, paras.get("bias_tensor")],
                "op_res": [op_res]}
    return {"op_placeholder": [paras.get("input_tensor"), paras.get("weight_tensor"),
                               scale_tensor],
            "op_res": [op_res]}
