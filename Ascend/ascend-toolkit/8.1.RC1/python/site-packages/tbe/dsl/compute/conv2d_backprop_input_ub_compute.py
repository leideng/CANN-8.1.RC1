#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d backprop input general compute.
"""
from tbe.common import platform as tbe_platform
from tbe.common.utils.errormgr import error_manager_cube
from tbe.dsl.compute import cube_util
from tbe import tvm
from tbe.common.platform import get_cube_mkn


PAD_LIMIT = 63


class DeConvUbPattern(cube_util.CubeDslPattern):
    """
    class of convolution back propagation

    Parameters
    ----------
    kernel_sizes : shape of weight, [N, C, H, W]

    strides : list of strides, [strideh, stridew]

    pad: list of padding, [forward_padu, forward_padd, forward_padl, forward_padr]

    output_shape : shape of dE/dX, [N, C, H, W]

    dilations: list of dilations, [dilation_n, dilation_c, dilation_h, dilation_w]

    offset_x : offset of x

    kernel_name : cce kernel name

    group_dict : The params of group convolution.

    Returns
    -------
    deconv_pattern_instance : instance of deconv pattern
    """

    def __init__(
        self,
        kernel_sizes,
        strides,
        pad,
        output_shape,
        output_dtype,
        dilations,
        offset_x,
        fusion_para,
        kernel_name,
        group_dict,
    ):
        self._co, self._ci, self._kernel_h, self._kernel_w = kernel_sizes
        self._stride_h, self._stride_w = strides
        self._forward_padu, self._forward_padd, self._forward_padl, self._forward_padr = pad
        self._output_shape = output_shape
        self._ori_output_shape = [output_shape[0], self._co, *output_shape[2:4]]
        self._kernel_name = kernel_name
        _, _, self._dilation_h, self._dilation_w = dilations
        self._offset_x = offset_x
        self._fusion_para = fusion_para
        self._group_dict = group_dict
        self._need_prepad = False
    
    def generate_a(self, dy_ddr):
        """
        generate dy_filling tensor for conv

        Parameters
        ----------
        dy_ddr: 5D dE/dY tensor in ddr

        Returns
        ----------
        dy_filling: 5D dE/dY tensor of in ub
        """
        def get_dy_filling():
            if self._need_prepad:
                shape_dy_filling = (
                    dy_batch, kernel_cout1,
                    (dy_h - 1) * self._stride_h + 1 + pad_top + pad_bottom,
                    (dy_w - 1) * self._stride_w + 1 + pad_left + pad_right,
                    kernel_cout0
                )
                offset_tensor = tvm.compute(shape_dy_filling, lambda *indices: tvm.const(self._offset_x, dy_ddr.dtype),
                    name="offset_tensor")
                dy_filling = tvm.compute(
                                shape_dy_filling,
                                lambda idx_batch, idx_co1, idx_ho_expand, idx_wo_expand, idx_co0: tvm.select(
                                    tvm.all(
                                        (idx_ho_expand - pad_top) % self._stride_h == 0, (idx_wo_expand - pad_left) %
                                            self._stride_w == 0,
                                        idx_ho_expand < pad_top + dy_h * self._stride_h, idx_wo_expand < pad_left +
                                            dy_w * self._stride_w,
                                        idx_ho_expand >= pad_top, idx_wo_expand >= pad_left
                                    ),
                                    dy_ddr[idx_batch,
                                            idx_co1,
                                            (idx_ho_expand - pad_top) // self._stride_h,
                                            (idx_wo_expand - pad_left) // self._stride_w,
                                            idx_co0],
                                    offset_tensor[idx_batch, idx_co1, idx_ho_expand, idx_wo_expand, idx_co0]
                                ),
                                name="dy_filling",
                                tag="stride_filling"
                            )
            else:
                shape_dy_filling = (dy_batch, kernel_cout1, (dy_h - 1) * self._stride_h + 1,
                                    (dy_w - 1) * self._stride_w + 1, kernel_cout0)
                dy_filling = tvm.compute(
                                shape_dy_filling,
                                lambda idx_batch, idx_co1, idx_ho_expand, idx_wo_expand, idx_co0: tvm.select(
                                    tvm.all(idx_ho_expand % self._stride_h == 0, idx_wo_expand % self._stride_w == 0),
                                    dy_ddr[idx_batch,
                                            idx_co1,
                                            idx_ho_expand // self._stride_h,
                                            idx_wo_expand // self._stride_w,
                                            idx_co0],
                                    tvm.const(self._offset_x, dy_ddr.dtype)
                                ),
                                name="dy_filling",
                                tag="stride_filling"
                            )
            return dy_filling
        if self._stride_h == 1 and self._stride_w == 1:
            dy_filling = tvm.compute(dy_ddr.shape, lambda *indices: dy_ddr(*indices), name="dy_filling")
        else:
            dy_batch, kernel_cout1, dy_h, dy_w, kernel_cout0 = cube_util.shape_to_list(dy_ddr.shape)
            pad_top = self._dilation_h * (self._kernel_h - 1) - self._forward_padu
            pad_bottom = self._dilation_h * (self._kernel_h - 1) - self._forward_padd
            pad_left = self._dilation_w * (self._kernel_w - 1) - self._forward_padl
            pad_right = self._dilation_w * (self._kernel_w - 1) - self._forward_padr
            pad_list = [pad_top, pad_bottom, pad_left, pad_right]
            k_w_dila = self._kernel_w + (self._kernel_w - 1) * (self._dilation_w - 1)
            k_h_dila = self._kernel_h + (self._kernel_h - 1) * (self._dilation_h - 1)
            pad_w_flag = True if pad_left + pad_right > k_w_dila else False
            pad_h_flag = True if pad_top + pad_bottom > k_h_dila else False
            if (not all(0 <= pad_value <= PAD_LIMIT for pad_value in pad_list)) or pad_w_flag or pad_h_flag:
                self._need_prepad = True
            dy_filling = get_dy_filling()
        return dy_filling
    
    def generate_b(self, kernels):
        """
        generate weight tensor for conv

        Parameters
        ----------
        kernels: fractal_z tensor in ddr

        Returns
        ----------
        weight: fractal_z tensor in ub
        """
        return kernels
    
    def generate_c(self, dy_filling, weight, bias=None):
        """
        generate conv result

        Parameters
        ----------
        dy_filling: 5D dy tensor after filling process
        weight: fractal_z weight tensor 
        bias: ND bias tensor

        Returns
        ----------
        weight: fractal_z tensor in ub
        """

        def create_reduce_axis():
            cin1_reduce_axis = tvm.reduce_axis((0, dy_filling_ci1), name="cin1_data")
            kh_reduce_axis = tvm.reduce_axis((0, self._kernel_h), name="weight_h")
            kw_reduce_axis = tvm.reduce_axis((0, self._kernel_w), name="weight_w")
            cin0_reduce_axis = tvm.reduce_axis((0, dy_filling_ci0), name="cin0_data")

            return [cin1_reduce_axis, kh_reduce_axis, kw_reduce_axis, cin0_reduce_axis]

        def get_conv_res():
            conv_res = tvm.compute(self._output_shape,
                                   lambda batch_idx, cout1_idx, ho_idx, wo_idx, cout0_idx: tvm.sum(
                                   tvm.conv_op(
                                       dy_filling(batch_idx, cin1_reduce_axis,
                                                  ho_idx * stride_h + kh_reduce_axis * self._dilation_h
                                                  - pad_top,
                                                  wo_idx * stride_w + kw_reduce_axis * self._dilation_w
                                                  - pad_left,
                                                  cin0_reduce_axis),
                                       # block_size_k_data is different in int16 with int8
                                       weight(((cin1_reduce_axis * block_size_k_data + cin0_reduce_axis) //
                                              block_size_k_weight) * self._kernel_h * self._kernel_w +
                                              kh_reduce_axis * self._kernel_w + kw_reduce_axis,
                                              cout1_idx,
                                              cout0_idx,
                                              (cin1_reduce_axis * block_size_k_data + cin0_reduce_axis) %
                                              block_size_k_weight),
                                       bias[cout1_idx * block_size_n + cout0_idx] if bias_flag else None,
                                       dy_filling_w,            # w_in
                                       dy_filling_h,            # h_in
                                       self._ci,                # cin_ori
                                       self._kernel_w,          # k_w
                                       self._kernel_h,          # k_h
                                       self._co,                # cout_ori
                                       stride_w,                # stride_w
                                       stride_h,                # stride_h
                                       self._dilation_w,        # dilate_w
                                       self._dilation_h,        # dilate_h
                                       pad_top,                 # pad_top
                                       pad_bottom,              # pad_bottom
                                       pad_left,                # pad_left
                                       pad_right,               # pad_right
                                       self._offset_x,          # pad_value (default = 0)
                                       "int32",
                                       op_dict={
                                           "enable_depthwise": 0,
                                           "use_bias": int(bias_flag)
                                       }),
                                   axis=[cin1_reduce_axis, kh_reduce_axis, kw_reduce_axis, cin0_reduce_axis]),
                               name="conv_res",
                               tag="conv2d_backprop_input_ub",
                               attrs=c_res_attrs
                               )
            return conv_res

        if bias is not None:
            bias_flag = True
        else:
            bias_flag = False

        block_size_k_data = get_cube_mkn(dy_filling.dtype)[1]
        _, block_size_k_weight, block_size_n = get_cube_mkn(weight.dtype)
        batch_dy, dy_filling_ci1, dy_filling_h, dy_filling_w, dy_filling_ci0 = cube_util.shape_to_list(dy_filling.shape)
        cin1_reduce_axis, kh_reduce_axis, kw_reduce_axis, cin0_reduce_axis = create_reduce_axis()
        
        pad_top = self._dilation_h * (self._kernel_h - 1) - self._forward_padu
        pad_bottom = self._dilation_h * (self._kernel_h - 1) - self._forward_padd
        pad_left = self._dilation_w * (self._kernel_w - 1) - self._forward_padl
        pad_right = self._dilation_w * (self._kernel_w - 1) - self._forward_padr
        stride_h, stride_w = [1, 1]

        c_res_attrs = {
            "ori_cout": self._co,
            "forward_dilation_h": self._dilation_h,
            "forward_dilation_w": self._dilation_w,
            "offset_x": self._offset_x,
            "fm_c0": block_size_k_data,
            "w_k0": block_size_k_weight,
            "w_n0": block_size_n,
            "res_c0": block_size_k_data,
            "ori_format": "NCHW",
            "ori_shape": self._ori_output_shape,
            "need_prepad": self._need_prepad
        }
        if self._need_prepad:
            pad_top = 0
            pad_bottom = 0
            pad_left = 0
            pad_right = 0

        conv_res = get_conv_res()
        return conv_res
