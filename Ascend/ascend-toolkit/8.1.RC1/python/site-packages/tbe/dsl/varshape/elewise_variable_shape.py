#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
elewise variable shape
"""
from tbe.common.utils.errormgr import get_error_message
from tbe.common.utils.varshape.variable_shape import register_variable
from tbe.dsl.base import expr_compare
from tbe.dsl.base import operation
from tbe.dsl.base import var_api

from tbe.dsl.classifier.elewise_classifier import ElewiseMode
from tbe.dsl.classifier.elewise_classifier import ElewisePattern

LEAST_DIFF_NUM = 2
CONST_SHAPE_DIFF = 2
UNKNOWN_DIM_VALUE = -1
ELEWISE_CONST_SHAPE = "_elewise_const_shape"


@register_variable("elewise")
def variable_shape(ins):
    """
    elewise variable shape
    :param ins: each classify res
    :return: list of all input shapes after variable
    """
    # variable_shape input num must be greater than zero
    expr_compare.is_true(len(ins) >= 1,
                         {"errCode": "E90001",
                          "detailed_cause": "elewise variable shape not support zero input num!"})

    # all classify res contains attr mode and pattern
    mode = ins[0].get("mode")
    pattern = ins[0].get("pattern")
    operation.get_context().get_current_compute().add("_mode", mode)
    operation.get_context().get_current_compute().add("_pattern", pattern)
    enable_fractal_format = all(_ins.get("mode_5hd") for _ins in ins)
    operation.get_context().get_current_compute().add("_is_fractal_format", enable_fractal_format)
    if mode == ElewiseMode.CONST:
        elewise_const_shape = ConstVariable.get_elewise_const_shape(ins)
        operation.get_context().get_current_compute().add(ELEWISE_CONST_SHAPE, elewise_const_shape)

    # variable shape dispatch
    if enable_fractal_format:
        return DisableFuseVariable.fratcal_format_variable(ins)
    if mode == ElewiseMode.CONST:
        return ConstVariable.const_variable(ins)
    if mode == ElewiseMode.DISABLE_FUSE:
        return DisableFuseVariable.disable_fuse_variable(ins)
    if mode == ElewiseMode.ALL_FUSE:
        return AllFuseVariable.all_fuse_variable(ins)
    if mode == ElewiseMode.EMPTY:
        return EmptyVariable.empty_variable(ins)

    dict_args = {"errCode": "E90001",
                 "detailed_cause": f"elewise variable shape only support all_fuse, const, empty and partial_fuse mode, "
                                   f"but now classify mode is {mode}."}
    raise RuntimeError(dict_args, get_error_message(dict_args))


class AllFuseVariable:
    """
    AllFuseVariable
    """

    @classmethod
    def all_fuse_variable(cls, ins):
        """
        all fuse variable shape dispatch
        """
        funcs = {ElewisePattern.PURE_ELEWISE: AllFuseVariable.pure_elewise_variable_shape,
                 ElewisePattern.BROADCAST_SCALAR: AllFuseVariable.broadcast_scalar_variable_shape,
                 ElewisePattern.SCALAR_BROADCAST: AllFuseVariable.scalar_broadcast_variable_shape,
                 ElewisePattern.ONE_RANK: AllFuseVariable.one_rank_variable_shape
                 }
        return funcs.get(ins[0].get('pattern'))(ins)

    @classmethod
    def pure_elewise_variable_shape(cls, ins):
        """
        pure_elewise_variable_shape, all ins generate same var
        """
        all_same_var = operation.var_inner_adaptive("_dim_0_0", ins[0].get('range')[0])
        return [[all_same_var] for _ in range(len(ins))]

    @classmethod
    def broadcast_scalar_variable_shape(cls, ins):
        """
        broadcast_scalar_variable_shape, only first input generate one var
        """
        expr_compare.is_true(len(ins) == LEAST_DIFF_NUM,
                             {"errCode": "E90001",
                              "detailed_cause": f"broadcast scalar variable_shape only support 2 inputs, but now "
                                                f"input num is {len(ins)}!"})

        left_var = operation.var_inner_adaptive("_dim_0_0", ins[0].get("range")[0])
        right_const_shape = [1]
        return [[left_var], right_const_shape]

    @classmethod
    def scalar_broadcast_variable_shape(cls, ins):
        """
        scalar_broadcast_variable_shape, only second input generate one var
        """
        expr_compare.is_true(len(ins) == LEAST_DIFF_NUM,
                             {"errCode": "E90001",
                              "detailed_cause": f"scalar broadcast variable_shape only support 2 inputs, but now "
                                                f"input num is {len(ins)}!"})

        left_const_shape = [1]
        right_var = operation.var_inner_adaptive("_dim_0_1", ins[1].get("range")[0])
        return [left_const_shape, [right_var]]

    @classmethod
    def one_rank_variable_shape(cls, ins):
        """
        one_rank_variable_shape, same dim with all ins will generate same var
        """
        expr_compare.is_true(len(ins) > LEAST_DIFF_NUM,
                             {"errCode": "E90001",
                              "detailed_cause": f"one rank variable_shape only support input num bigger than 2, but "
                                                f"now input num is {len(ins)}!"})

        res = [[] for _ in range(len(ins))]
        for index, (_res, _ins) in enumerate(zip(res, ins)):
            # unknown broadcast shape must be one dim
            current_shape = _ins.get('shape')[0]
            if current_shape > 0:
                _res.append(current_shape)
            else:
                _res.append(operation.var_inner_adaptive(f"_dim_0_{index}", _ins.get('range')[0]))

        return res


class ConstVariable:
    """
    ConstVariable
    """

    @classmethod
    def get_elewise_const_shape(cls, ins):
        """
        get elewise const shape
        """
        diff_const_shapes = list(set(tuple(_ins.get("const_shape")) for _ins in ins))
        if len(diff_const_shapes) == 1:
            elewise_const_shape = list(diff_const_shapes[0])
        if len(diff_const_shapes) == CONST_SHAPE_DIFF:
            # only broadcast const shape, with shape size 1 and another equals to output will enter into this branch
            elewise_const_shape = \
                [dim_l & dim_r for dim_l, dim_r in zip(diff_const_shapes[0], diff_const_shapes[1])]
        return elewise_const_shape

    @classmethod
    def const_variable(cls, ins):
        """
        const mode variable shape
        """
        return [list(_ins.get("shape")) for _ins in ins]


class DisableFuseVariable:
    """
    DisableFuseVariable
    """

    @classmethod
    def fratcal_format_variable(cls, ins):
        """
        fractal format variable shape, only satisfy elewise 5hd scene will enter into this branch
        """
        def _ori_c_variable(ins):
            # elewise only support same C value
            pad_axis = ins[0].get("pad_axes")
            if pad_axis == UNKNOWN_DIM_VALUE:
                c_value = operation.var_inner_adaptive("_ori_dim_0", [1, None],
                                                        addition={"annotation": {"axis_type": "C"}})
            else:
                c_value = var_api.const(pad_axis, annotation={"axis_type": "C"})

            return c_value

        def _shape_variable(c_value, ins):
            res = [[] for _ in range(len(ins))]
            for dim_index in range(len(ins[0].get('shape'))):
                shape_var = None
                for input_index, (_res, _ins) in enumerate(zip(res, ins)):
                    annot = {}
                    axis_type = _ins.get("s_format")[dim_index]
                    annot["axis_type"] = axis_type
                    if axis_type in (["C1"], ["C0"]):
                        annot["original"] = c_value
                    if _ins.get('shape')[dim_index] == UNKNOWN_DIM_VALUE:
                        if shape_var is None:
                            shape_var = operation.var_inner_adaptive(f"_dim_{dim_index}_{input_index}",
                                                            _ins.get("range")[dim_index],
                                                            addition={"annotation": annot})
                        _res.append(shape_var)
                    else:
                        shape_const = var_api.const(_ins.get('shape')[dim_index], annotation=annot)
                        _res.append(shape_const)

            return res

        # ori_c value varibale
        c_value = _ori_c_variable(ins)
        # shape variable
        return _shape_variable(c_value, ins)

    @classmethod
    def disable_fuse_variable(cls, ins):
        """
        disable fuse variable shape, each unknown dim will be variable encoding with its dim index and input index
        """
        res = [[] for _ in range(len(ins))]
        for dim_index in range(len(ins[0].get("shape"))):
            for input_index, (_ins, _res) in enumerate(zip(ins, res)):
                cur_shape = _ins.get("shape")
                cur_range = _ins.get("range")
                if cur_shape[dim_index] == -1 and cur_range[dim_index][0] == cur_range[dim_index][1]:
                    _res.append(cur_range[dim_index][0])
                elif cur_shape[dim_index] == -1:
                    is_pure_elewise = operation.get_context().get("_is_pure_elewise")
                    if is_pure_elewise:
                        # pure elewise same dim value must be equal
                        _res.append(operation.var_inner_adaptive(f"_dim_{dim_index}_0", cur_range[dim_index]))
                    else:
                        _res.append(operation.var_inner_adaptive(f"_dim_{dim_index}_{input_index}",
                                                                 cur_range[dim_index]))
                else:
                    _res.append(cur_shape[dim_index])
        return res


class EmptyVariable:
    """
    EmptyVariable
    """

    @classmethod
    def empty_variable(cls, ins):
        """
        empty variable shape
        """
        return [[0] for _ in range(len(ins))]
