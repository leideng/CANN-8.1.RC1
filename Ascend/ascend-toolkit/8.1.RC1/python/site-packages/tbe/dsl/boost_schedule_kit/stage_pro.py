#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================


from tbe import tvm
from tbe.dsl.compute.constants import ComputeType
from tbe.dsl.unify_schedule.constants import INSN_MAPPING
from tbe.dsl.utils import lists


class StagePro:
    def __init__(self, sch, stage):
        self._sch = sch
        self._tvm_stage = stage
        self._emited = False
        self._tag = self.op.tag or self.op.attrs.get("_tag")
        self._compute_type = self.op.attrs.get("_type")

    def __getattr__(self, name):
        attr = getattr(self._tvm_stage, name)

        if not callable(attr):
            return attr

        def wrapper(*args, **kwargs):
            t_args = [x.get() if isinstance(x, StagePro) else x for x in args]
            return attr(*t_args, **kwargs)

        return wrapper

    def __repr__(self):
        return repr(self._tvm_stage)

    @property
    def shape(self):
        return self.tensors[0].shape

    @property
    def dtype(self):
        return self.tensors[0].dtype

    @property
    def tag(self):
        return self._tag

    @tag.setter
    def tag(self, tag_):
        self._tag = tag_

    @property
    def insn(self):
        if self.tag.find("|") != -1:
            return self.tag.split("|")[0]
        return self.tag

    @property
    def compute_type(self):
        return self._compute_type

    @compute_type.setter
    def compute_type(self, type_):
        self._compute_type = type_

    @property
    def scope(self):
        return self._tvm_stage.scope

    @property
    def tensors(self):
        return self._sch.to_tensors([self])

    @property
    def tensor(self):
        return self.tensors[0]

    @property
    def leaf_axes(self):
        return self.leaf_iter_vars

    @property
    def ancestors(self):
        stages = []
        for x in self.producers:
            stages.append(x)
            stages.extend(x.ancestors)

        return lists.unique(stages)

    @property
    def descendants(self):
        stages = []
        for x in self.consumers:
            stages.append(x)
            stages.extend(x.descendants)

        return lists.unique(stages)

    @property
    def producers(self):
        stages = []
        for x in self.op.input_tensors:
            lists.append_only(stages, self._sch[x])
        return stages

    @property
    def consumers(self):
        stages = []
        for x in self._sch.stages:
            if self in x.producers:
                lists.append_only(stages, x)
        return stages

    def get(self):
        return self._tvm_stage

    def transpose(self, *axes, mode="cache_write", inline=True):
        self.reorder(*axes)

        if mode == "cache_read":
            x_t = self._sch.cache_read(self.tensor, "local.UB", self._sch.to_tensors(self.consumers))
        else:
            x_t = self._sch.cache_write(self.tensor, "local.UB")

        if inline:
            self._sch[x_t].compute_type = self.compute_type
            self._sch[x_t].tag = self.tag
            self.compute_inline()
        else:
            self._sch[x_t].compute_type = ComputeType.TRANSPOSE
            self._sch[x_t].tag = "transpose"

        return x_t

    def is_compute_inline(self):
        return self.attach_type == AttachType.COMPUTE_INLINE

    def is_placeholder(self):
        return isinstance(self.op, tvm.PlaceholderOp)

    def is_scan_init(self):
        for x in self._sch.stages:
            if isinstance(x.op, tvm.ScanOp):
                if self.tensors == self._sch.get_origin_tensors(x.op.init):
                    return True
        return False

    def is_scan_state(self):
        for x in self._sch.stages:
            if isinstance(x.op, tvm.ScanOp):
                if self.tensors == self._sch.get_origin_tensors(x.op.state_placeholder):
                    return True
        return False

    def is_scan_update(self):
        for x in self._sch.stages:
            if isinstance(x.op, tvm.ScanOp):
                if self.tensors == self._sch.get_origin_tensors(x.op.update):
                    return True
        return False

    def is_scan(self):
        return isinstance(self.op, tvm.ScanOp)

    def is_scan_component(self):
        return self.is_scan_init() or self.is_scan_state() or self.is_scan_update()

    def is_broadcast(self):
        return self.compute_type in (ComputeType.BRC_SCALAR, ComputeType.BRC_TENSOR)

    def is_elewise(self):
        return self.compute_type == ComputeType.ELEWISE

    def is_dma_in(self):
        return self.compute_type == ComputeType.DMA_IN

    def is_reshape(self):
        return self.compute_type == ComputeType.RESHAPE

    def is_cast(self):
        return self.compute_type == ComputeType.CAST

    def is_emited(self):
        return self._emited

    def emit_phony(self):
        self.emit(axis=self.leaf_axes[-1], insn="phony_insn")

    def emit(self, axis=None, insn=None, attrs=None):
        if axis is None:
            axis = self.leaf_axes[0]

        if insn is None:
            insn = INSN_MAPPING[self.insn]

        if attrs is None:
            attrs = {"auto_movedown_pragma": 1}

        if "auto_movedown_pragma" not in attrs:
            attrs["auto_movedown_pragma"] = 1

        self._tvm_stage.emit_insn(axis, insn, attrs=attrs)

        self._emited = True


class AttachType:
    COMPUTE_INLINE = 2
    COMPUTE_AT = 4
