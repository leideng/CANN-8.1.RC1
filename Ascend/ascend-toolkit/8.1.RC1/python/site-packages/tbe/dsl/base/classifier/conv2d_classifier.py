#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
classifier of shape in conv2d
"""

import copy
from tbe.common.utils.errormgr import get_error_message
from tbe.common.context import get_context
from tbe.common.utils.para_check import TensorFormat
from tbe.common.utils.op_util.op_util_conv2d import BinaryInfoKey
from tbe.common.utils.op_util.op_util_conv2d import check_load3d_w_out_1_support
from tbe.common.utils.op_util.op_util_conv2d import is_conv2d_binary
from tbe.common.utils.op_util.op_util_conv2d import is_pooling_binary
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe.common.utils.op_util.op_util_conv2d import is_v300_soc
from tbe.common.utils.op_util.op_util_conv2d import BINARY_SRC_SHAPE_ATTR, L0A_DMA_SCENE, C0_OPTIM_FLG
from tbe.common.utils.op_util.op_util_conv2d import TilingDataKey
from tbe.common.utils.op_util.op_util_conv2d import Conv2DL0aDmaScene
from tbe.common.utils.op_util.op_util_conv2d import CONV_FILTER_IDX
from tbe.common.context import op_info

KERNEL_H_INDEX = 0
KERNEL_W_INDEX = 1
ZERO_INS_INDEX = 0

ATTR_STRIDE_INDEX = 0
ATTR_PAD_INDEX = 1
ATTR_DILATION_INDEX = 2
ATTR_GROUP_INDEX = 3
ATTR_FORMAT_INDEX = 4
ATTR_OFFSETX_INDEX = 5

QUANT_CONV_ATTR_DTYPE_INDEX = 0
QUANT_CONV_ATTR_STRIDE_INDEX = 1
QUANT_CONV_ATTR_PAD_INDEX = 2
QUANT_CONV_ATTR_DILATION_INDEX = 3
QUANT_CONV_ATTR_GROUP_INDEX = 4
QUANT_CONV_ATTR_FORMAT_INDEX = 5
QUANT_CONV_ATTR_OFFSETX_INDEX = 6
QUANT_CONV_ATTR_ROUND_MODE_INDEX = 7

GROUP_FEATURE_BRANCHES = [{BinaryInfoKey.GROUPOPT_FLAG: 0},
                          {BinaryInfoKey.GROUPOPT_FLAG: 1}]

# All possible branch flags
COMPUTE_MULTI_BRANCH_FLAGS = [BinaryInfoKey.LOAD2D_FLAG,
                              BinaryInfoKey.LOAD3D_FLAG,
                              BinaryInfoKey.DMA_FLAG,
                              BinaryInfoKey.CONV1D_FLAG,
                              BinaryInfoKey.Nx1_FLAG,
                              BinaryInfoKey.AL0BoundCheck_Flag]

# Flags that needs to be set for each branch, if not specified, default to 0
BRANCH_TO_FLAG_MAP = {
    "load2d": {BinaryInfoKey.LOAD2D_FLAG: 1},
    "load3d": {BinaryInfoKey.LOAD3D_FLAG: 1},
    "dma": {BinaryInfoKey.DMA_FLAG: 1},
    "conv1d": {BinaryInfoKey.CONV1D_FLAG: 1, BinaryInfoKey.LOAD3D_FLAG: 1},
    "nx1": {BinaryInfoKey.LOAD3D_FLAG: 1, BinaryInfoKey.Nx1_FLAG: 1},
    "al0boundcheck_load3d": {BinaryInfoKey.AL0BoundCheck_Flag: 1, BinaryInfoKey.LOAD3D_FLAG: 1},
    "al0boundcheck_conv1d": {BinaryInfoKey.AL0BoundCheck_Flag: 1, BinaryInfoKey.LOAD3D_FLAG: 1,
                             BinaryInfoKey.CONV1D_FLAG: 1},
}


CONV_INS_ATTR = {ATTR_STRIDE_INDEX: "strides",
                 ATTR_PAD_INDEX: "pads",
                 ATTR_DILATION_INDEX: "dilation",
                 ATTR_GROUP_INDEX: "groups",
                 ATTR_OFFSETX_INDEX: "offset_x"}


QUANT_CONV_INS_ATTR = {QUANT_CONV_ATTR_STRIDE_INDEX: "strides",
                       QUANT_CONV_ATTR_PAD_INDEX: "pads",
                       QUANT_CONV_ATTR_DILATION_INDEX: "dilation",
                       QUANT_CONV_ATTR_GROUP_INDEX: "groups",
                       QUANT_CONV_ATTR_OFFSETX_INDEX: "offset_x"}


OP_ATTR_IDX_MAP = {"QuantConv2D": QUANT_CONV_INS_ATTR,
                   "Conv2D": CONV_INS_ATTR,
                   "Pooling": CONV_INS_ATTR}


FUSION_FEATURE_LIST = [BinaryInfoKey.BROADCAST_FLAG]


def valid_binary_context(op_type):
    """
    Checks if the current context supports binary mode
    Returns:
        True - the current context supports binary mode
        False - the current context does not supports binary mode
    """
    if op_type == "Conv2D":
        return is_conv2d_binary()
    if op_type == "Pooling":
        return is_pooling_binary()
    return False


def get_context_mode():
    op_mode = get_context().get_op_mode()
    return op_mode


class BaseComputeMode(object):
    """
    compute classify public func
    """
    # flag, decide whether to generate compile param
    task_ready = False

    # an option, used to consider whether to generate compile param.
    fmap_dtype = None
    weight_format = None

    def __init__(self, conv_ins: list or tuple, op_type):
        self.input_list, self.conv_attr, self.conv_option = conv_ins
        self.op_type = op_type
        self.flags_to_set = {}
        self.attrs_to_set = {}
        self.options_to_set = {}
        self.set_attr_none()

    @staticmethod
    def set_fmap_dtype(fmap_dtype: str):
        BaseComputeMode.fmap_dtype = fmap_dtype

    @staticmethod
    def set_weight_format(weight_format: str):
        BaseComputeMode.weight_format = weight_format

    @classmethod
    def set_task_ready(cls):
        cls.task_ready = False

    def set_attr_none(self):
        "attr is None for binary mode"
        if not get_context_mode() == "static" and self.op_type != "QuantConv2D":
            # conv binary use conv_ins_attr
            for idx, _ in CONV_INS_ATTR.items():
                if idx != ATTR_GROUP_INDEX and self.conv_attr[idx] is not None:
                    self.conv_attr[idx] = None

    def update_attrs(self):
        for idx, attr in OP_ATTR_IDX_MAP.get(self.op_type).items():
            if self.attrs_to_set.get(attr):
                self.conv_attr[idx] = self.attrs_to_set.get(attr)

    def update_branch_flags(self, feature):
        for flag in COMPUTE_MULTI_BRANCH_FLAGS:
            self.conv_option[flag] = self.flags_to_set.get(flag, 0)
        for feature_flag, feature_value in feature.items():
            self.conv_option[feature_flag] = feature_value

    def update_options(self):
        for option in self.options_to_set:
            self.conv_option[option] = self.options_to_set.get(option)

    def mode_update(self, feature):
        self.update_attrs()
        self.update_branch_flags(feature)
        self.update_options()


class Load2dComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get('load2d'))
        self.attrs_to_set.update({"pads": [0, 0, 0, 0], "strides": [1, 1, 1, 1]})
        self.options_to_set.update({BINARY_SRC_SHAPE_ATTR: {TilingDataKey.K_H: 1,
                                                            TilingDataKey.K_W: 1}
                                    })

    @classmethod
    def set_task_ready(cls):
        format_limit = (cls.weight_format == "FRACTAL_Z")
        dtype_limit = cls.fmap_dtype == "float16" or (cls.fmap_dtype == "bfloat16" and is_support_fixpipe())
        cls.task_ready = format_limit and dtype_limit


class Load3dComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get('load3d'))
        self.options_to_set.update({C0_OPTIM_FLG: self.weight_format == TensorFormat.FRACTAL_Z_C04.value})

    @classmethod
    def set_task_ready(cls):
        cls.task_ready = True


class DmaComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get('dma'))
        self.options_to_set.update({L0A_DMA_SCENE: Conv2DL0aDmaScene.BASIC})

    @classmethod
    def set_task_ready(cls):
        format_limit = (cls.weight_format == "FRACTAL_Z")
        dtype_limit = cls.fmap_dtype == "float16" or is_support_fixpipe()
        cls.task_ready = format_limit and dtype_limit


class DmaAubLoadK0ComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get('dma'))
        self.options_to_set.update({L0A_DMA_SCENE: Conv2DL0aDmaScene.AUB_ONLY_LOAD_K0})

    @classmethod
    def set_task_ready(cls):
        format_limit = (cls.weight_format == "FRACTAL_Z")
        dtype_limit = cls.fmap_dtype == "float16" and (not is_support_fixpipe())
        cls.task_ready = format_limit and dtype_limit


class DmaConv1dWithoutPadComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get('dma'))
        # BINARY_SRC_SHAPE_ATTR value will be fix in schedule
        # L0A_DMA_SCENE to distinguish dmaconv1d scene
        self.options_to_set.update({BINARY_SRC_SHAPE_ATTR: {TilingDataKey.K_H: 1,
                                                            TilingDataKey.FMAP_H: 1,
                                                            TilingDataKey.HO: 1,
                                                            TilingDataKey.PAD_TOP: 0,
                                                            TilingDataKey.PAD_BOTTOM: 0,
                                                            TilingDataKey.PAD_LEFT: 0,
                                                            TilingDataKey.PAD_RIGHT: 0
                                                            },
                                    L0A_DMA_SCENE: Conv2DL0aDmaScene.DMA_CONV1D_WITHOUT_PAD,
                                    })

    @classmethod
    def set_task_ready(cls):
        cls.task_ready = (cls.weight_format == "FRACTAL_Z") and is_support_fixpipe()


class Conv1dComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get('conv1d'))
        self.options_to_set.update({BINARY_SRC_SHAPE_ATTR: {TilingDataKey.K_H: 1,
                                                            TilingDataKey.FMAP_H: 1,
                                                            TilingDataKey.HO: 1,
                                                            TilingDataKey.PAD_TOP: 0,
                                                            TilingDataKey.PAD_BOTTOM: 0},
                                    C0_OPTIM_FLG: self.weight_format == TensorFormat.FRACTAL_Z_C04.value,
                                    })

    @classmethod
    def set_task_ready(cls):
        cls.task_ready = True


class Nx1ComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get("nx1"))

    @classmethod
    def set_task_ready(cls):
        if not is_support_fixpipe():
            cls.task_ready = not check_load3d_w_out_1_support()


class AL0BoundCheckWithLoad3dComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get("al0boundcheck_load3d"))

    @classmethod
    def set_task_ready(cls):
        cls.task_ready = is_support_fixpipe() and not is_v300_soc()


class AL0BoundCheckWithConv1dComputeMode(BaseComputeMode):
    def __init__(self, conv_ins: list or tuple, op_type):
        super().__init__(conv_ins, op_type)
        self.flags_to_set.update(BRANCH_TO_FLAG_MAP.get("al0boundcheck_conv1d"))

    @classmethod
    def set_task_ready(cls):
        cls.task_ready = is_support_fixpipe() and not is_v300_soc()


class Classifier(object):
    """
    Classifier checks if the input {ins} is in binary mode, if so, it
    creates multiple modified duplicates of ins corrsponds to different
    binary compute branchs.

    If the input is not binary, classifier will just return [ins]
    """

    def __init__(self, op_type, ins: list or tuple, extra_params=None):
        self.ins = ins
        self.op_type = op_type
        self.set_base_compute_mode()
        if extra_params is None:
            extra_params = dict()
        self.binary_static_flag = extra_params.get("binary_static_flag", False) or get_context_mode() == "static"

    def get_all_compute_modes(self):
        if self.binary_static_flag:
            return [Load3dComputeMode, AL0BoundCheckWithLoad3dComputeMode]
        return BaseComputeMode.__subclasses__()

    def get_input_list(self):
        return self.ins[0]

    def get_fmap(self):
        input_list = self.ins[0]
        fmap = input_list[0]
        return fmap

    def get_weight(self):
        input_list, *_ = self.get_conv_ins(self.ins)
        weight = input_list[CONV_FILTER_IDX]
        return weight

    def get_groups(self):
        _, conv_attr, *_ = self.get_conv_ins(self.ins)
        # conv and quantconv had diff attr idx
        groups = conv_attr[QUANT_CONV_ATTR_GROUP_INDEX] if self.op_type == "QuantConv2D" \
            else conv_attr[ATTR_GROUP_INDEX]
        return groups

    def get_group_branch(self):
        # reduce online compilation time
        groups = self.get_groups()
        weight_format = self.get_input_format(self.get_weight(), TensorFormat.FRACTAL_Z.value)
        group_branch = []
        if groups == 1 or weight_format == TensorFormat.FRACTAL_Z_C04.value:
            group_branch.append(GROUP_FEATURE_BRANCHES[0])
        else:
            group_branch = GROUP_FEATURE_BRANCHES
        return group_branch

    def get_all_features(self):
        # get all features that need compile
        all_features = []
        group_feature = self.get_group_branch()
        all_features.extend(group_feature)
        return all_features

    def get_input_dtype(self, fmap):
        # fmap data type should be float16
        in_dtype = None
        if "data_type" in fmap:
            in_dtype = fmap.get("data_type")
        elif "dtype" in fmap:
            in_dtype = fmap.get("dtype")
        return in_dtype

    def get_input_format(self, input_info: dict, default_value=None):
        input_format = input_info.get("format", default_value)
        return input_format

    def is_binary_shape(self):
        fmap = self.get_fmap()
        fmap_ori_shape = fmap.get("ori_shape")
        if not fmap_ori_shape:
            return False
        # We consider an operator to be binary
        # if ANY axis of fmap have shape = -1
        for idx, _ in enumerate(fmap_ori_shape):
            if fmap_ori_shape[idx] in [-1]:
                return True
        return False

    def get_conv_ins(self, ins):
        pass

    def generate_classified_ins(self, compute_mode: BaseComputeMode, feature):
        new_ins = copy.deepcopy(self.ins)
        conv_ins = self.get_conv_ins(new_ins)
        compute_mode(conv_ins, self.op_type).mode_update(feature)
        return new_ins

    def generate_classified_ins_list(self):
        classified_ins_list = []
        if (valid_binary_context(self.op_type) and self.is_binary_shape()) or \
                self.binary_static_flag:
            self._generate_classified_ins_list(classified_ins_list)
        else:
            classified_ins_list.append(self.ins)

        return classified_ins_list

    def set_base_compute_mode(self):
        fmap_dtype = self.get_input_dtype(self.get_fmap())
        BaseComputeMode.set_fmap_dtype(fmap_dtype)

        weight_format = self.get_input_format(self.get_weight(), TensorFormat.FRACTAL_Z.value)
        BaseComputeMode.set_weight_format(weight_format)

    def update_classified_ins_list(self, classified_ins_list):
        return classified_ins_list
    
    def _generate_classified_ins_list(self, classified_ins_list):
        all_features = self.get_all_features()
        compute_mode_list = self.get_all_compute_modes()
        for compute_mode in compute_mode_list:
            compute_mode.set_task_ready()
            for feature in all_features:
                if compute_mode.task_ready:
                    classified_ins = self.generate_classified_ins(compute_mode, feature)
                    classified_ins_list.append(classified_ins)


def get_braodcast_fusion_flag():
    op_info_list = get_context().get_op_info()
    for info in op_info_list:
        if info.pattern == "Broadcast":
            return True

    return False


def get_broadcast_feafure_value():
    broad_cast_flag_value = [False]
    if get_braodcast_fusion_flag():
        broad_cast_flag_value.append(True)
    if get_context_mode() == "static" and get_braodcast_fusion_flag():
        broad_cast_flag_value = [True]

    return broad_cast_flag_value


FUSION_FEATURE_FLAG_VALUE_GET_FUNC = {BinaryInfoKey.BROADCAST_FLAG: get_broadcast_feafure_value}


class FusionClassifier(Classifier):
    def __init__(self, op_type, ins: list or tuple):
        super().__init__(op_type, ins)

    def get_conv_ins(self, ins):
        """
        In Fusion mode, ins may contain infomation about other operators.

        This function extracts conv related infomation, and reformat all required
        infomation into one tuple
        """
        input_list, attr_list, option_list, *_ = ins
        conv_attr = self.extract_conv_info(attr_list, 'val')
        conv_option = self.extract_conv_info(option_list, 'options')
        conv_ins = (input_list, conv_attr, conv_option)
        return conv_ins

    def extract_conv_info(self, info_list, key):
        def is_conv_dict(element):
            if isinstance(element, dict) and "conv" in element.get("name", '').lower():
                return True
            else:
                return False

        conv_info_dicts = list(filter(is_conv_dict, info_list))

        if not conv_info_dicts:
            error_msg = f"get conv info failed, no dict with name = 'conv' was found. list: {info_list}"
            dict_args = {"errCode": "E90001", "detailed_cause": error_msg}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        for info_dict in conv_info_dicts:
            if info_dict.get(key):
                return info_dict.get(key)

        error_msg = f"get conv info failed, {len(conv_info_dicts)} conv related dict(s) found," \
                    f" non contains the required key '{key}'."
        dict_args = {"errCode": "E90001", "detailed_cause": error_msg}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    def update_fusion_feature_flag(self, classified_ins_list):
        res_ins_list = []
        for fusion_feature_flag in FUSION_FEATURE_LIST:
            fusion_flag_value = FUSION_FEATURE_FLAG_VALUE_GET_FUNC.get(fusion_feature_flag)()
            for value in fusion_flag_value:
                for res_ins in classified_ins_list:
                    res_ins_new = copy.deepcopy(res_ins)
                    conv_ins = self.get_conv_ins(res_ins_new)
                    _, _, conv_option = conv_ins
                    conv_option.update({fusion_feature_flag: value})
                    res_ins_list.append(res_ins_new)
        return res_ins_list

    def update_classified_ins_list(self, classified_ins_list):
        classified_ins_list_tmp = self.update_fusion_feature_flag(classified_ins_list)
        return classified_ins_list_tmp


class SingleOpClassifier(Classifier):
    def __init__(self, op_type, ins: list or tuple, extra_params=None):
        super().__init__(op_type, ins, extra_params)
        self.set_context()

    def set_context(self):
        context = get_context()
        op_info_list = context.get_op_info()
        if not op_info_list:
            # In DSL mode, op_info has not been set yet.
            current_op_info = op_info.OpInfo(self.op_type, self.op_type)
            context.add_op_info(current_op_info)
        else:
            current_op_info = op_info_list[0]
        if not current_op_info.inputs:
            current_op_info.inputs = self.get_input_list()
        return

    def get_conv_ins(self, ins):
        input_list, conv_attr, conv_option, output, *_ = ins
        conv_ins = (input_list, conv_attr, conv_option)
        return conv_ins


# classify for tefusion
def classify(ins: list or tuple, extra_params: dict):
    op_pattern = extra_params.get("op_pattern")
    op_type = "QuantConv2D" if op_pattern == "QuantConvolution" else "Conv2D"
    classifier = FusionClassifier(op_type, ins)
    classified_ins_list = classifier.generate_classified_ins_list()
    classified_ins_list = classifier.update_classified_ins_list(classified_ins_list)
    return classified_ins_list


# classify for single op
def cube_forward_op_classify(op_type, ins: list or tuple, extra_params: dict):
    classifier = SingleOpClassifier(op_type, ins, extra_params)
    classified_ins_list = classifier.generate_classified_ins_list()
    return classified_ins_list
