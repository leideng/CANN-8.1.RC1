#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gemm schedule
"""

from tbe import tvm
from tbe.common.buildcfg import build_config
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.dsl.base.operation import in_dynamic
from . import gemm_schedule_util as util

K_AXIS_ALIGN_FACTOR = 2


def update_gemm_multiout_list(outs):
    """
    remove virtual res from out_list in multi_out scene
    """
    if not isinstance(outs, (list, tuple)):
        return outs
    for tensor in outs:
        if tensor.op.tag == "gemm_virtual_res":
            outs.remove(tensor)
    return outs


def print_ir_matmul(process, sch):
    """
    print ir for input sch
    :param process: tag
    :param sch: schedule
    :return: IR process
    """
    if process == "debug":
        with build_config():
            start = process + " IR start"
            end = process + " IR end\n"
            sch = sch.normalize()
            print(start)
            bounds = tvm.schedule.InferBound(sch)
            stmt = tvm.ScheduleOps(sch, bounds, True)
            print(stmt)
            print(end)


class GemmScheduleV2:
    """
    class of gemm schedule
    """
    def __init__(self, res, sch_list, tiling_case_para):
        self.res = res
        self.sch = sch_list[0]
        self.tiling_case_para = tiling_case_para
        self.tensor_list = self.tiling_case_para["tensor_list"]

    @staticmethod
    def get_al1_bound(tiling, tensor_map):
        """
        cal the l1 bound of al1
        """
        shape_al0 = util.get_value(tensor_map.get("a_l0a").shape)
        # get parts from gm to l0c every core of m direction: ceil(m1, m_l0c)
        # the index -4 in shape_al0 is m1
        l0c_parts = util.int_ceil_div(shape_al0[-4], tiling.get("CL0_matrix")[1])
        # the index -3 in shape_al0 is k1
        k_full_load_bound = shape_al0[-3] * shape_al0[-1]
        if tiling.get("AL1_shape"):
            # get parts from gm to L1: l0c_parts // multiple_of_L1_and_L0
            al1_parts = util.int_ceil_div(l0c_parts, tiling.get("AL1_shape")[1])
            # k on Al1
            if tiling.get("attach_at_flag").get("akl1_fully_load"):
                k_bound = k_full_load_bound
            else:
                k_bound = tiling.get("AL1_shape")[0]
        else:
            # Al1 full load, move all data in one time
            al1_parts = 1
            # total K of input: k0 * k1
            k_bound = k_full_load_bound
        # parts from from gm to l0c ignore multi core
        l0c_bounds = util.int_ceil_div(shape_al0[-4], tiling.get("CL0_matrix")[1])
        # multiple of al1 and l0c space
        al1_bounds = util.int_ceil_div(l0c_bounds, al1_parts)
        # divide blockdim since l0c_bounds ignored multicore to get m on L1
        # the index 2 in CL0_matrix is tiling_cub_m
        m_bound = (util.int_ceil_div(al1_bounds, tiling.get("block_dim")[2]) * tiling.get("CL0_matrix")[1] *
                   tiling.get("CL0_matrix")[2])
        return k_bound * m_bound

    @staticmethod
    def get_bl1_bound(tiling, tensor_map):
        """
        cal the l1 bound of bl1
        """
        shape_bl0 = util.get_value(tensor_map.get("b_l0b").shape)
        # the index -3 in shape_bl0 is n1
        l0c_parts = util.int_ceil_div(shape_bl0[-3], tiling.get("CL0_matrix")[0])
        # the index -4 in shape_bl0 is k1
        k_full_load_bound = shape_bl0[-4] * shape_bl0[-1]
        if tiling.get("BL1_shape"):
            bl1_parts = util.int_ceil_div(l0c_parts, tiling.get("BL1_shape")[1])
            if tiling.get("attach_at_flag").get("bkl1_fully_load"):
                k_bound = k_full_load_bound
            else:
                k_bound = tiling.get("BL1_shape")[0]
        else:
            bl1_parts = 1
            k_bound = k_full_load_bound
        # the index -3 in shape_bl0 is n1
        l0c_bounds = util.int_ceil_div(shape_bl0[-3], tiling.get("CL0_matrix")[0])
        bl1_bounds = util.int_ceil_div(l0c_bounds, bl1_parts)
        # the index 3 in CL0_matrix is tiling_cub_n
        n_bound = (util.int_ceil_div(bl1_bounds, tiling.get("block_dim")[1]) * tiling.get("CL0_matrix")[0] *
                   tiling.get("CL0_matrix")[3])
        return k_bound * n_bound, n_bound

    
    def gemm_schedule(self):
        """
        schedule enter
        param:
        res: tensor
        sch_list: list of schedule
        """
        sch = self.sch
        tensor_map = self.tensor_list[3]
        tiling = self.tiling_case_para["tiling_strategy"]

        # set scope for matmul
        util.set_matmul_scope(sch, tensor_map)
        util.set_out_scope(sch, self.tensor_list)
        print_ir_matmul("after set scope", sch)

        # get factor and parts from tiling
        l0c_factor, ub_factor_parts, al1_parts, bl1_parts = util.get_aicore_factor(tiling, tensor_map)
        if len(self.res.shape) in (util.MATMUL_LEN_ND, util.BATCH_MATMUL_LEN_ND):
            l0c_factor = [
                tiling["CL0_matrix"][0]*tiling["CL0_matrix"][3],
                tiling["CL0_matrix"][1]*tiling["CL0_matrix"][2]
            ]
        # split l0c, al1, bl1
        l1_m_axis, l1_n_axis = util.split_mn_l0c_l1(self.res, sch, l0c_factor, al1_parts, bl1_parts)
        # split upon block
        batch_inner = util.split_mn_block(self.res, sch, tiling, l1_m_axis, l1_n_axis)
        # reorder m and n of l1 upon minimun memory
        if tiling["attach_at_flag"]["abl1_reorder_flag"] == 1:
            sch[self.res].reorder(l1_m_axis[0], l1_n_axis[0])

        # split when need hanble ub
        if len(self.res.shape) in (util.MATMUL_LEN_ND, util.BATCH_MATMUL_LEN_ND):
            ub_factor_parts = [
                tiling["CUB_matrix"][0]*tiling["CUB_matrix"][3],
                tiling["CUB_matrix"][1]*tiling["CUB_matrix"][2]
            ]
        if tensor_map.get("ub_eltwise") is None:
            ub_factor_parts = None
        c_gm_emit_axis = util.split_ub(self.res, sch, l1_m_axis, l1_n_axis, ub_factor_parts)
        # attach tensor of l0c and bias, c_slice_axis is l1_m_axis[1]
        sch[tensor_map.get("c_l0c")].compute_at(sch[self.res], l1_m_axis[1])
        util.do_buffer_align(sch, tensor_map)
        util.attach_of_ub(sch, tensor_map, c_gm_emit_axis[2])
        util.attach_of_bias_table(sch, tensor_map, tiling, l1_m_axis[1], batch_inner)
        util.attach_of_fixpipe(sch, tensor_map, tiling, c_gm_emit_axis[2], batch_inner)
        k_axis = util.split_k(tensor_map.get("c_l0c"), sch, tiling["AL0_matrix"][1],
                              [al1_parts[0], bl1_parts[0]], tiling)
        # attch of l1 tensor and l0 tensor, l1_attch_axis = l1a_k_axis, l1b_k_axis, l0_k_axis, l1a_m_axis, l1b_n_axis
        util.attach_of_l1_l0(
            sch,
            tensor_map,
            [*k_axis, l1_m_axis[0], l1_n_axis[0]],
            tiling
        )
        # double buffer function
        util.double_buffer_func(sch, tensor_map, tiling)
        # emit_insn function, emit_axis is for l0c and res emit_insn
        util.emit_insn_func(sch, tensor_map, k_axis, c_gm_emit_axis)
        print_ir_matmul("final IR", sch)
        # dynamic tensor
        self._mem_process(tiling, tensor_map)
        # get spec_mid_list
        spec_mid_list = tensor_map.get("spec_mid_list", [])
        # clear global cache
        tiling.clear()
        return spec_mid_list

    def _mem_process(self, tiling, tensor_map):
        al1_bound = self.get_al1_bound(tiling, tensor_map)
        bl1_bound, n_bound = self.get_bl1_bound(tiling, tensor_map)
        a_l1, b_l1, a_l0a, b_l0b, c_l0c = (
            tensor_map["a_l1"],
            tensor_map["b_l1"],
            tensor_map["a_l0a"],
            tensor_map["b_l0b"],
            tensor_map["c_l0c"]
        )
        if in_dynamic():
            self.sch.sequential_malloc(tbe_platform_info.scope_cbuf)
            self.sch.sequential_malloc(tbe_platform_info.scope_ca)
            self.sch.sequential_malloc(tbe_platform_info.scope_cb)
            self.sch.sequential_malloc(tbe_platform_info.scope_cc)
            self.sch.sequential_malloc(tbe_platform_info.scope_ubuf)
            self.sch.sequential_malloc(tbe_platform_info.scope_bt)
            # get l1 bound
            self.sch[a_l1].set_buffer_size(al1_bound)
            self.sch[b_l1].set_buffer_size(bl1_bound)
            # mem_unique
            mem_unique_list = [a_l1, b_l1, a_l0a, b_l0b, c_l0c]
            if tensor_map.get("bias_l1") is not None:
                self.sch[tensor_map.get("bias_l1")].set_buffer_size(n_bound)
                self.sch[tensor_map.get("bias_bt")].set_buffer_size(n_bound)
                mem_unique_list += [tensor_map.get("bias_l1"), tensor_map.get("bias_bt")]
            for mem_unique_mem in mem_unique_list:
                self.sch[mem_unique_mem].mem_unique()



def gemm_schedule(res, sch_list, tiling_case_para=None):
    """
    schedule enter
    param:
    res: list of tensor
    sch_list: list of schedule
    """
    gemm_sch = GemmScheduleV2(res, sch_list, tiling_case_para)
    return gemm_sch.gemm_schedule()
