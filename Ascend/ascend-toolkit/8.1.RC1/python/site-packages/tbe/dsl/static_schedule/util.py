#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
common function
"""
import math
import collections
from functools import update_wrapper
from functools import reduce as functools_reduce
from queue import Queue
from types import MappingProxyType
from typing import Hashable
from typing import Callable
from typing import Union

from tbe import tvm
from tbe.common.context import op_context
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.platform import ASCEND_910
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.utils.errormgr import raise_err_message_cube


from . import pattern


# fake node label
FAKE_NODE_TAG = "elewise_empty_intrin"
SET_GM_SCOPE_TAG = "elewise_set_gm_scope"

REDUCE_OP_TAG_LABEL = "reduce_"
BROADCAST_TAG_LABEL = "broadcast_"

FAKE_NODE_PRAGMA = "phony_insn"
BROADCAST_ALIGN_PRAGMA = "vector_dup"
BROADCAST_TRANSPOSE = "vector_broadcast_transpose"
VECTOR_AUTO_PRAGMA = "vector_auto"
DMA_COPY_PRAGMA = "dma_copy"

REDUCE_MULTI_PRIME_KEY = "_unique_name_reduce_multi"

FAKE_NODE_FUSE_FAILED = -1

MULTI_CORE_UNIT = 1024
REDUCE_AXIS_SIZE = 1

VECTOR_ONE_REPEAT_UNIT = 128
VECTOR_ONE_BLOCK_UNIT = 16
VECTOR_ONE_REPEAT_BLOCK = 8
MAX_TYPE_SIZE_UNIT = 2
MIN_TYPE_SIZE_UNIT = 0.5

LOG_SOFTMAX_LIMIT = 80000
LOG_SOFTMAX_MATCH = 30528

MULTI_REDUCE = True
MULTI_ELEMWISE = True
MULTI_WORKSPACE = True
MULTI_WORKSPACE_ALL = False

PATTERN_OPTIMAZE = True
PATTERN_LIMIT = False

# the bit of dtype/16 map
DTYPE_WIDTH_MAP = {"float16": 1,
                   "float32": 2,
                   "int32": 2,
                   "int16": 1,
                   "uint16": 1,
                   "int8": 0.5,
                   "uint8": 0.5,
                   "bool": 0.5}

DTYPE_BYTE_MAPPING = {
    "uint1": 0.125,
    "bool": 1,
    "uint4": 0.5,
    "int4": 0.5,
    "int8": 1,
    "uint8": 1,
    "float16": 2,
    "int16": 2,
    "uint16": 2,
    "float32": 4,
    "int32": 4,
    "uint32": 4,
    "int64": 8,
    "uint64": 8,
    "bfloat16": 2,
}

FIXPIPE_SCOPE_MAP = {
    "quant_scale_0": tbe_platform_info.scope_fb0,
    "relu_weight_0": tbe_platform_info.scope_fb1,
    "relu_weight_1": tbe_platform_info.scope_fb2,
    "quant_scale_1": tbe_platform_info.scope_fb3
}

REDUCE_ATOMIC_SUPPORT = {ASCEND_910: {"tag": "reduce_sum",
                                         "dtype": "float32"},
                         ASCEND_910B: {"tag": "reduce_sum",
                                         "dtype": "float32"},
                         ASCEND_910_93: {"tag": "reduce_sum",
                                       "dtype": "float32"}}

DEFAULT_INDEX = -1
INIT_COUNT = 0
INIT_SIZE = 1

TILING_RADICAL = 0
TILING_CONSERVATIVE = 1

INTRINSIC_FIXPIPE_UNIT_LIST = "Intrinsic_fix_pipe_unit_list"

OP_TYPE_MAP = {
    "MatMul": ["MatMul", "MatMulV2", "BatchMatMul", "BatchMatMulV2", "FullyConnection"],
    "Conv2DBackpropInput": ["Deconvolution", "Conv2DBackpropInputD", "Conv2DTransposeD"],
    "Conv2DBackpropFilter": ["Conv2DBackpropFilterD"],
    "Conv3D": ["Conv3D"],
}


class L1CommonParam:
    """"L1 common parameter"""
    l1_fusion_tensors_map = None

    def __init__(self):
        pass


def get_fixpipe_emit_str():
    """
    get emit tag for fixpipe data flow
    :return: str
    """
    return "fixpipe_op" if tbe_platform_info.intrinsic_check_support(INTRINSIC_FIXPIPE_UNIT_LIST) else "dma_copy"


def parse_tbe_compile_para(compile_para):
    """
    get tbe compile parameter form compile parameter
    :param compile_para:
    :return:
    """
    tbe_compile_para = {}
    tbe_sch_control_para = {}
    # Current usage of compile_para:
    # preload_l1/reverse_load|preload|out_of_order|read_write_bank_conflict|pipeline_opt
    if compile_para is None:
        tbe_compile_para = None
        tbe_sch_control_para["preload"] = 0
        tbe_sch_control_para["preload_l1"] = 0
        tbe_sch_control_para["reverse_load"] = 0
    else:
        tbe_compile_para["pipeline_opt"] = (compile_para >> 0) & 1
        tbe_compile_para["read_write_bank_conflict"] = (compile_para >> 1) & 1
        tbe_compile_para["out_of_order"] = (compile_para >> 2) & 1
        tbe_sch_control_para["preload"] = (compile_para >> 3) & 1
        tbe_sch_control_para["preload_l1"] = (compile_para >> 4) & 1
        tbe_sch_control_para["reverse_load"] = (compile_para >> 4) & 1
    return tbe_compile_para, tbe_sch_control_para


def get_split_axis(shape, max_ub_count):
    """
    obtains the axis and factor of segmentation based on the number of data
    records that can be stored at a time in UB.

    Parameters
    ----------
    shape : tensor shape
    max_ub_count : data count

    Returns
    -------
    int : split rfactor
    int : split axis
    """
    # find the split axis, shape = (shape[0], ..., shape[split_axis], shape[-1])
    # so that shape[split_axis]*shape[split_axis + 1]*...*shape[-1] < max_ub_count
    # and shape[split_axis - 1]**shape[split_axis + 1]*...*shape[-1] > max_ub_count
    rfactor = max_ub_count
    axis = len(shape) - 1

    for i, it_n in enumerate(reversed(shape)):
        if max_ub_count < it_n:
            break
        rfactor = it_n
        max_ub_count = max_ub_count // it_n
        # exactly divided by the current axis
        if max_ub_count == 1 or i == len(shape) - 1:
            return rfactor, axis

        # obtains the currently traversed axis.
        if i != len(shape) - 1:
            axis -= 1

    # if the calculated value is less than the current axis.
    # calculate the factor corresponding to the maximum common value of the
    # current axis.
    for i in range(max_ub_count, 1, -1):
        if shape[axis] % i == 0:
            rfactor = i
            return rfactor, axis

    # last axis prime number
    if axis == len(shape) - 1:
        rfactor = 1
        return rfactor, axis

    # the segmentation factor is not found. The split axis is the next axis.
    return rfactor, axis + 1


def get_align_factor(dtype):
    """
    get_align_factor
    """
    # base on the diff data type, get the align_factor
    align_factor = 16
    dtype_bytes = 2
    if dtype in ('int8', 'uint8'):
        align_factor = 32
        dtype_bytes = 1
    elif dtype in ('float16', 'int16', 'uint16'):
        align_factor = 16
        dtype_bytes = 2
    else:
        align_factor = 8
        dtype_bytes = 4
    return align_factor, dtype_bytes


def get_dst_tensor_map(reslist, tensor_map):
    """
    get the dst_tensor list of the tensor with more than one dst_tensor
    tensor_map = {input: outputlist}
    """
    for out_tensor in reslist:
        for in_tensor in list(out_tensor.op.input_tensors):
            if in_tensor in tensor_map:
                if out_tensor not in tensor_map[in_tensor]:
                    tensor_map[in_tensor].append(out_tensor)
            else:
                tensor_map[in_tensor] = [out_tensor]
                get_dst_tensor_map([in_tensor], tensor_map)


def shape_to_list(shape):
    """
    translate tvm.shape to list type in python
    """
    tmp = []
    for i in shape:
        if isinstance(i, tvm.Var):
            tmp.append(i)
        else:
            tmp.append(i.value)
    return tmp


def is_prime_number(value):
    """
    check the value is prime number or not.
    """
    if value < 2:
        return False
    for i in range(2, int(math.sqrt(value)) + 1):
        if value % i == 0:
            return False
    return True


def get_reduce_axis_num(reduce_tensor):
    """
    get reduce axis num
    """
    data_axis_var = reduce_tensor.op.body[0].source[0].indices
    reduce_axis_var = []
    for i in reduce_tensor.op.reduce_axis:
        reduce_axis_var.append(i.var)

    axis_num = []
    for ax_var in reduce_axis_var:
        num = 0
        for i in data_axis_var:
            if i.same_as(ax_var):
                axis_num.append(num)
            num += 1

    return axis_num


def get_bits_of(dtype):
    """
    calculate bits of dtype of TVM
    Parameters
    ----------
    dtype : string
        dtype of TVM

    Returns
    -------
    ret : int
        bit length of dtype.
    """
    index = 0
    for i in dtype:
        if i.isdigit():
            break
        index += 1
    return int(dtype[index:])


def get_max_divisor(num, x_var=None):
    """
    find the maximum divisor of a number (not counting itself, and after x)
    """
    if x_var is None:
        divisor = num // 2
    else:
        divisor = min(num // 2, x_var)

    while divisor > 1:
        if num % divisor == 0:
            break
        else:
            divisor = divisor - 1
    return divisor


def get_greatest_common_divisor(m_var, n_var):
    """
    greatest common divisor
    """
    if m_var % n_var == 0:
        return n_var
    while m_var % n_var != 0:
        m_var, n_var = n_var, m_var % n_var
    return n_var


def get_least_common_multiple(m_var, n_var):
    """
    least common multiple
    product of two numbers = least common multiple * greatest common divisor
    lcm = m * n // gcd
    """
    return (m_var * n_var) // get_greatest_common_divisor(m_var, n_var)


def get_mod2_count(num):
    """
    The count of remainders for 2
    """
    count = 0
    while num % 2 == 0:
        num = num // 2
        count = count + 1
    return count


def get_shape_size_ext(shape, value=None):
    """
    shape size
    """
    if not shape:
        shape_size = 0
    else:
        shape_size = functools_reduce(lambda i, j: i * j, shape)

    if isinstance(value, int) and shape_size == 0:
        shape_size = value
    elif isinstance(value, int) and shape_size != 0:
        shape_size = shape_size * value

    return shape_size


# 'pylint: disable=too-many-branches, too-many-return-statements
def tiling_from_front_to_back(shape, max_size, align_factor=None,
                              is_divisible=False):
    """
    tiling_from_front_to_back, using for block_tiling
    shape:
    max_size:
    is_divisible: if true, do force divisibility.
    align_factor: if not None, try alignment, but do not force alignment.
                If the data does not match the alignment, it can also be misaligned.
    """
    bound_size = 1
    split_axis = 0
    # 'pylint: disable=consider-using-enumerate
    for i in range(len(shape)):
        bound_size = shape[i] * bound_size
        split_axis = i
        if bound_size >= max_size:
            break
    # 1. bound_size <= max_size
    if bound_size <= max_size:
        outer = shape[split_axis]
        inner = 1
        return outer, inner, split_axis

    # 2. bound_size > max_size
    outer = max_size * shape[split_axis] // bound_size
    inner = (shape[split_axis] + outer - 1) // outer

    # 2.1 do force divisibility.
    if is_divisible:
        outer = get_max_divisor(shape[split_axis], outer)
        inner = shape[split_axis] // outer

    # 2.2 try 256 or 32 alignment, but do not force alignment.
    # If the data does not match the alignment, it can also be misaligned.
    if align_factor and align_factor is not None:
        mod2count_align = get_mod2_count(align_factor)
        mod2count_reduceshape = get_mod2_count(bound_size // shape[split_axis])
        mod2count_splitaxis = get_mod2_count(shape[split_axis])
        mod2count_all = mod2count_reduceshape + mod2count_splitaxis

        # 2.2.1 if: mod2count_reduceshape + mod2count_splitaxis < mod2count_all
        if mod2count_align > mod2count_all:
            return outer, inner, split_axis
        # 2.2.2 elif: mod2count_reduceshape >= mod2count_all
        if mod2count_align <= mod2count_reduceshape:
            return outer, inner, split_axis
        # 2.2.3 else: mod2count_reduceshape + mod2count_splitaxis >= mod2count_all,
        # and mod2count_reduceshape < mod2count_all
        factor = 2 ** (mod2count_align - mod2count_reduceshape)
        # 2.2.3.1 if: is_divisible
        if is_divisible:
            outer_update = outer
            while outer_update >= factor:
                if outer_update % factor == 0:
                    outer = outer_update
                    inner = (shape[split_axis] + outer - 1) // outer
                    break
                outer_update = get_max_divisor(shape[split_axis],
                                               outer_update - 1)

            return outer, inner, split_axis

        # 2.2.3.2 else: not is_divisible
        if outer < factor:
            return outer, inner, split_axis
        outer = outer // factor * factor
        inner = (shape[split_axis] + outer - 1) // outer
        return outer, inner, split_axis

    return outer, inner, split_axis


# 'pylint: disable=too-many-branches, too-many-return-statements
def tiling_from_back_to_front(shape, max_size, align_factor=None,
                              is_divisible=False):
    """
    tiling_from_back_to_front, using for ub_tiling
    shape:
    max_size:
    is_divisible: if true, do force divisibility.
    align_factor: if not None, try alignment, but do not force alignment.
                If the data does not match the alignment, it can also be misaligned.
    """
    bound_size = 1
    split_axis = len(shape) - 1
    for i in reversed(range(len(shape))):
        bound_size = shape[i] * bound_size
        split_axis = i
        if bound_size >= max_size:
            break
    # 1. bound_size <= max_size
    if bound_size <= max_size:
        inner = shape[split_axis]
        outer = 1
        return outer, inner, split_axis

    # 2. bound_size > max_size
    inner = max_size * shape[split_axis] // bound_size
    outer = (shape[split_axis] + inner - 1) // inner

    # 2.1 do force divisibility.
    if is_divisible:
        inner = get_max_divisor(shape[split_axis], inner)
        outer = shape[split_axis] // inner

    # 2.2 try 256 or 32 alignment, but do not force alignment.
    # If the data does not match the alignment, it can also be misaligned.
    if align_factor and align_factor is not None:
        mod2count_align = get_mod2_count(align_factor)
        mod2count_reduceshape = get_mod2_count(bound_size // shape[split_axis])
        mod2count_splitaxis = get_mod2_count(shape[split_axis])
        mod2count_all = mod2count_reduceshape + mod2count_splitaxis

        # 'pylint: disable=no-else-return
        # 2.2.1 if: mod2count_reduceshape + mod2count_splitaxis < mod2count_all
        if mod2count_align > mod2count_all:
            return outer, inner, split_axis
        # 2.2.2 elif: mod2count_reduceshape >= mod2count_all
        elif mod2count_align <= mod2count_reduceshape:
            return outer, inner, split_axis
        # 2.2.3 else: mod2count_reduceshape + mod2count_splitaxis >= mod2count_all,
        # and mod2count_reduceshape < mod2count_all
        else:
            factor = 2 ** (mod2count_align - mod2count_reduceshape)
            # 2.2.3.1 if: is_divisible
            if is_divisible:
                inner_update = inner
                while inner_update >= factor:
                    if inner_update % factor == 0:
                        inner = inner_update
                        outer = (shape[split_axis] + inner - 1) // inner
                        break
                    inner_update = get_max_divisor(shape[split_axis],
                                                   inner_update - 1)
                return outer, inner, split_axis

            # 'pylint: disable=no-else-return
            # 2.2.3.2 else: not is_divisible
            if inner < factor:
                return outer, inner, split_axis
            else:
                inner = inner // factor * factor
                outer = (shape[split_axis] + inner - 1) // inner
                return outer, inner, split_axis

    return outer, inner, split_axis


def fake_node_fuse_fun(tensors):
    """
    fuse tensors into a fake node by mul compute with 0.
    """
    dtype = tensors[0].dtype
    shape = shape_to_list(tensors[0].shape)
    dim = len(shape)

    for tensor in tensors:
        if len(tensor.shape) != dim:
            return FAKE_NODE_FUSE_FAILED
        for i in range(dim):
            if shape[i] < tensor.shape[i].value:
                shape[i] = tensor.shape[i].value

    def phony_insn_fuse(*indice):
        res = tvm.const(1, dtype)
        for tensor in tensors:
            # get full indice order
            cur_index = []
            for i in range(dim):
                if tensor.shape[i].value == shape[i]:
                    cur_index.append(indice[i])
                else:
                    cur_index.append(indice[i] % tensor.shape[i].value)
            res *= tvm.expr.Cast(dtype, tensor(*cur_index))
        return res

    with tvm.tag_scope(FAKE_NODE_TAG):
        res = tvm.compute(shape, phony_insn_fuse, name="fake_node")
    return res


def ceil(value_a, value_b):
    """
    get up multi value_b
    """
    if value_b == 0:
        raise_err_message_cube("Division by zero")
    return (value_a + value_b - 1) // value_b


def align(value_a, value_b):
    """
    get up align value_b
    """
    return ceil(value_a, value_b) * value_b


def gcd(value_a, value_b):
    """
    get gcd value
    """
    if value_a < value_b:
        value_a, value_b = value_b, value_a
    while value_a % value_b != 0:
        value_a, value_b = value_b, value_a % value_b
    return value_b


def get_limit_coef(value_a, value_b):
    """
    get limit coef value
    """
    coef = []
    cur_num = 1
    while cur_num <= value_b:
        if value_a % cur_num == 0:
            coef.append(cur_num)
        cur_num += 1
    return coef


def get_shape_size(shape):
    """
    get shape size
    """
    size = INIT_SIZE
    for dim_v in shape:
        size *= dim_v
    return size


def get_block_factor_conservative(shape, barrier, factor):
    """
    get block factor conservative
    """
    visit = [[], ]

    res_idx = []
    res_factor = []
    res_efficiency = DEFAULT_INDEX

    # updata result
    def compare_cut_result(cur_size, cur_idxs, cur_factor):
        """
        compare cut result
        """
        nonlocal res_idx, res_factor, res_efficiency
        complete_size = align(cur_size, factor)
        cur_efficiency = cur_size / complete_size
        # equal case, use front index
        priority1 = cur_efficiency > res_efficiency
        priority2 = cur_efficiency == res_efficiency and \
            cur_idxs[0] < res_idx[0]
        priority3 = cur_efficiency == res_efficiency and \
            cur_idxs[0] == res_idx[0] and cur_factor[0] > res_factor[0]
        if priority1 or priority2 or priority3:
            res_idx = cur_idxs[:]
            res_factor = cur_factor[:]
            res_efficiency = cur_efficiency

    # dsf for factor
    def calcu_factor_from_series_axes(shape, idxs):
        """
        calculate factor from series axes
        """
        if idxs in visit:
            return
        visit.append(idxs)

        # single pattern
        if len(shape) == 1:
            coef = ceil(shape[0], factor)
            compare_cut_result(shape[0], idxs, [coef])
            return

        # multi pattern
        inner_size = get_shape_size(shape[1:-1])
        # inner_size too large
        if factor < inner_size:
            calcu_factor_from_series_axes(shape[1:-1], idxs[1:-1])
        # inner_size and factor have no common side
        elif int(factor // inner_size) < 1:
            compare_cut_result(inner_size, shape[1:-1], idxs[1:-1])
        # inner_size less than factor and have space for coef
        else:
            pre_dim = shape[0]
            suf_dim = shape[-1]
            pre_coefss = get_limit_coef(pre_dim, factor)
            suf_coefss = get_limit_coef(suf_dim, factor)
            for coef in suf_coefss:
                coef = int(suf_dim / coef)
            for pre_coef in pre_coefss:
                for suf_coef in suf_coefss:
                    cur_size = inner_size * pre_coef * int(suf_dim / suf_coef)
                    if cur_size <= factor:
                        compare_cut_result(cur_size, idxs, [pre_coef, suf_coef])

        # dsf part
        calcu_factor_from_series_axes(shape[1:], idxs[1:])
        calcu_factor_from_series_axes(shape[:-1], idxs[:-1])

    barrier = list(barrier)
    barrier.append(len(shape))
    shape = list(shape)
    shape.append(DEFAULT_INDEX)
    temp_shape = []
    temp_idxs = []
    # 'pylint: disable=consider-using-enumerate
    for idx in range(len(shape)):
        if idx in barrier:
            calcu_factor_from_series_axes(temp_shape, temp_idxs)
            temp_shape = []
            temp_idxs = []
        else:
            temp_shape.append(shape[idx])
            temp_idxs.append(idx)

    return res_idx, res_factor


def get_block_factor_radical(shape, barrier, factor):
    '''
    make block dim axes, and the result axis are series
    :param shape: input shape
    :param barrier: axis that can't be cut
    :param factor: block dim size, axes outer
    :return: 1 : axes index
             2 : cut coef as fuse all axes in `1`
    '''

    def get_factor_from_series_axes(shape):
        total_size = get_shape_size(shape)
        align_size = align(total_size, factor)
        return align_size // factor, total_size / align_size

    def compare_cut_result(ori_res, new_res):
        return new_res > ori_res

    temp_shape = []
    temp_idx = []
    res_idx = []
    res_efficiency = DEFAULT_INDEX
    res_coef = INIT_SIZE
    for d_var, d_shape in enumerate(shape):
        if d_var in barrier:
            temp_coef, temp_efficiency = get_factor_from_series_axes(temp_shape)
            if compare_cut_result(res_efficiency, temp_efficiency) and len(
                    temp_shape) > 0:
                res_idx = temp_idx[:]
                res_efficiency = temp_efficiency
                res_coef = temp_coef
            temp_shape = []
            temp_idx = []
        else:
            temp_shape.append(d_shape)
            temp_idx.append(d_var)

    return res_idx, res_coef


def get_ub_factor(shape, barrier, rest_size):
    '''
    make ub as full as possiable, axes are series
    :param shape: input shape
    :param barrier: axis that can't be cut
    :param factor: block dim size, axes inner
    :return: 1 : cut index, until to last, except barrier axes
             2 : cut factor for axes in `1`
    '''

    # modify ub factor
    # case like, shape is (10) and the rest size is 6
    # calculat cut by 6, modify to 5
    def modify_d(axis_size, rest_size):
        cur_res = ceil(axis_size, rest_size)
        if axis_size % cur_res == 0:
            return int(axis_size // cur_res)
        return int(rest_size)

    legal_dim = DEFAULT_INDEX
    for idx in reversed(range(len(shape))):
        if idx not in barrier:
            if rest_size < shape[idx]:
                return idx, modify_d(shape[idx], rest_size)
            rest_size = int(rest_size // shape[idx])
            legal_dim = idx
    return legal_dim, shape[legal_dim]


def get_atomic_reduce_info():
    """
    get atomic reduce info
    """
    version_code = get_soc_spec(SHORT_SOC_VERSION)
    if version_code in REDUCE_ATOMIC_SUPPORT.keys():
        return REDUCE_ATOMIC_SUPPORT[version_code]
    return {}


def is_support_atomic_reduce(tensor, info):
    """
    check support atomic reduce or not
    """
    if not info:
        return False
    if tensor.op.tag == info["tag"] and tensor.dtype == info["dtype"]:
        return True
    return False


def pattern_identify(tensor_list):
    """
    pattern identify
    """
    tag_list = []
    former_broadcast = False
    for tensor in tensor_list:
        if BROADCAST_TAG_LABEL in tensor.op.tag:
            if former_broadcast:
                continue
            former_broadcast = True
        else:
            former_broadcast = False
        tag_list.append(tensor.op.tag)
    for cur_pattern in pattern.data:
        if pattern.data[cur_pattern] == tag_list:
            return cur_pattern
    return pattern.P_NONE


def _map_apend(input_map, key, value):
    if input_map.get(key):
        if isinstance(value, list):
            for tmp_v in value:
                if not tmp_v in input_map[key]:
                    input_map[key].append(tmp_v)
        else:
            if not value in input_map[key]:
                input_map[key].append(value)
    else:
        if isinstance(value, list):
            input_map[key] = value
        else:
            input_map[key] = [value]


def gen_reversed_subgraph_list(out_tensor, tensor_list_map,
                               tensor_list_dst_tensor_map):
    """traverse tensors by Depth-First-Search

    Parameters
    ----------
    out_tensor : tensor
        traverse tensors from this tensor,
        traversing its input tensors recursively.

    tensor_list : list
        record tensors in the order of Depth-First-Search.

    """
    if out_tensor is None:
        return
    stack = [out_tensor]
    visited_list = []
    while stack:
        cur_tensor = stack.pop()
        visited_list.append(cur_tensor)
        for in_tensor in cur_tensor.op.input_tensors:
            if in_tensor not in visited_list:
                stack.append(in_tensor)
                tensor_list_map[in_tensor.name] = in_tensor
            _map_apend(tensor_list_dst_tensor_map, in_tensor, cur_tensor)


def get_emit_insn_map(tensor):
    """
    get tensor's emit_insn key
    """
    insn_map = {"elewise_single_cast": "vector_conv",
                "elewise_single_VS_max": "vector_maxs",
                "elewise_single_VS_min": "vector_mins",
                "elewise_single_log": "vector_ln",
                "elewise_single_exp": "vector_exp",
                "elewise_single_rec": "vector_rec",
                "elewise_single_relu": "vector_relu",
                "elewise_single_abs": "vector_abs",
                "elewise_single_not": "vector_not",
                "elewise_single_sqrt": "vector_sqrt",
                "elewise_single_rsqrt": "vector_rsqrt",
                "elewise_binary_mul": "vector_mul",
                "elewise_single_VS_mul": "vector_muls",
                "elewise_binary_div": "vector_div",
                "elewise_binary_add": "vector_add",
                "elewise_single_VS_add": "vector_adds",
                "elewise_binary_min": "vector_min",
                "elewise_binary_max": "vector_max",
                "elewise_binary_vcmpv_gt": "vector_gt",
                "elewise_binary_vcmpv_ge": "vector_ge",
                "elewise_binary_vcmpv_lt": "vector_lt",
                "elewise_binary_vcmpv_le": "vector_le",
                "elewise_binary_vcmpv_eq": "vector_eq",
                "elewise_binary_vcmpv_ne": "vector_ne",
                "elewise_binary_cmpsel_gt": "vector_select_gt",
                "elewise_binary_cmpsel_ge": "vector_select_ge",
                "elewise_binary_cmpsel_lt": "vector_select_lt",
                "elewise_binary_cmpsel_le": "vector_select_le",
                "elewise_binary_cmpsel_eq": "vector_select_eq",
                "elewise_binary_cmpsel_ne": "vector_select_ne",
                "elewise_binary_or": "vector_or",
                "elewise_binary_and": "vector_and",
                "elewise_multiple_mla": "vector_multiple",
                "elewise_multiple_madd": "vector_multiple",
                "elewise_multiple_maddrelu": "vector_multiple",
                "elewise_multiple_sel": "vector_select_bool",
                "broadcast_for_tensor": "unified_broadcast",
                "elewise_binary_sub": "vector_sub",
                "elewise_binary_cmpsel": "vector_cmpsel",
                "emit_insn_elewise_binary_cmp": "elewise_binary_cmp",
                "reduce_sum": "vector_reduce_sum"}
    if tensor.op.tag.find("|") != -1:
        str_list = tensor.op.tag.split("|")
        insn = insn_map.get(str_list[0])
    else:
        insn = insn_map.get(tensor.op.tag)
    return insn


def is_bert_bn_target(tensor_list):
    """
    bert_bn_training_reduce_grad_fp32_1980 identify
    """
    tag_list = []
    for tensor in tensor_list:
        tag_list.append(tensor.op.tag)
    if tag_list in pattern.bn_update_bert_target:
        return True
    return False


def get_nearest_factor(dim, split_size):
    """
    find the exact division factor small than split_size as nearest_factor,
    if distance of nearest_factor and split_size is small, will use the
    nearest_factor as factor, otherwise use the split_size
    """
    nearest_factor = split_size
    while dim % nearest_factor != 0:
        nearest_factor -= 1
    if split_size / nearest_factor < 2:
        split_size = nearest_factor
    return split_size


def dfs_tensor_graph(tensor,  # 'pylint: disable=R0912, R0915, R0913
                     is_out=True, visited=None, input_tensors=None,
                     mid_tensors=None, tensor_map=None):
    """
    Based on the output tensor, use dfs(Depth First Search)
    algorithm to construct the graph.
    """
    # First trial, initialize results
    if visited is None:
        visited = []
        input_tensors = []
        mid_tensors = []
        tensor_map = collections.OrderedDict()
    # Record current tensor
    if tensor not in visited:
        visited.append(tensor)
    else:
        return visited, input_tensors, mid_tensors, tensor_map
    # Input tensor is a tensor without any input_tensor
    if not tuple(tensor.op.input_tensors) and \
            isinstance(tensor.op, tvm.PlaceholderOp):
        input_tensors.insert(0, tensor)
    elif not is_out:
        mid_tensors.append(tensor)
    # Iterate through all of current_tensor's input tensors
    for input_tensor in list(tensor.op.input_tensors):
        # Add input -> output mapping to tensor_map
        if input_tensor not in tensor_map:
            tensor_map[input_tensor] = [tensor]
        else:
            tensor_map[input_tensor].insert(0, tensor)
        dfs_tensor_graph(input_tensor, False, visited,
                         input_tensors, mid_tensors, tensor_map)
    return visited, input_tensors, mid_tensors, tensor_map


def gen_dfs_tensor_map(outs):
    """
    Based on the output tensor, use dfs(Depth First Search)
    algorithm to construct the graph.
    """
    visited, input_tensors, mid_tensors, tensor_map = dfs_tensor_graph(outs[0])
    for out in outs[1:]:
        dfs_results = dfs_tensor_graph(out, True,
                                       visited, input_tensors,
                                       mid_tensors, tensor_map)
        visited, input_tensors, mid_tensors, tensor_map = dfs_results
    return visited, input_tensors, mid_tensors, tensor_map


def _check_pattern_matched(
        dfs_tensor_list,
        expected_dfs_tag_list,
        ignore_tags=None):
    """
    check pattern matched
    """
    exct_idx = 0
    for i, _ in enumerate(dfs_tensor_list):
        if exct_idx == len(expected_dfs_tag_list):
            return True

        if ignore_tags:
            if dfs_tensor_list[i].op.tag in ignore_tags:
                continue

        if dfs_tensor_list[i].op.tag == expected_dfs_tag_list[exct_idx] or \
                (dfs_tensor_list[i].op.tag == "" and
                 expected_dfs_tag_list[exct_idx] == "placeholder"):
            exct_idx = exct_idx + 1
        elif dfs_tensor_list[i].op.tag.find("elewise") == -1 and \
                dfs_tensor_list[i].op.tag != "" and \
                dfs_tensor_list[i].op.tag != "broadcast_for_tensor":
            return False
    if exct_idx == len(expected_dfs_tag_list):
        return True
    return False


def get_reduce_axes(reduce_tensor):
    """Get reduce axes var"""
    reduce_tensor_body = reduce_tensor.op.body
    reduce_tensor_axes = list(reduce_tensor_body[0].axis)
    for idx, axis in enumerate(reduce_tensor_axes):
        reduce_tensor_axes[idx] = axis.var
    return reduce_tensor_axes


def get_all_axes(reduce_tensor):
    """Get all axes"""
    reduce_tensor_body = reduce_tensor.op.body
    return list(reduce_tensor_body[0].source[0].indices)


def generic_dispatch(key: Union[int, str] = 0) -> Callable:
    """generic function decorator.

    Transforms a function into a generic function, which can have different
    behaviours depending upon the value of its key of arguments
    or key of keyword arguments.
    The decorated function acts as the default implementation, and additional
    implementations can be registered using the register() attribute of the
    generic function.
    """

    def decorate(func: Callable) -> Callable:
        registry = {}

        def dispatch(key: Hashable) -> Callable:
            """
            Runs the dispatch algorithm to return the best available
            implementation for the given *key* registered on *generic_func*.
            """
            try:
                impl = registry[key]
            except KeyError:
                impl = registry[object]
            return impl

        def register(key: Hashable, func: Callable = None) -> Callable:
            """
            Registers a new implementation for the given key on a generic_func
            """
            if func is None:
                return lambda f: register(key, f)

            registry[key] = func
            return func

        def wrapper_index(*args, **kw):
            return dispatch(args[key])(*args, **kw)

        def wrapper_keyword(*args, **kw):
            return dispatch(kw[key])(*args, **kw)

        registry[object] = func
        if isinstance(key, int):
            wrapper = wrapper_index
        elif isinstance(key, str):
            wrapper = wrapper_keyword
        else:
            raise KeyError('The key must be int or str')
        wrapper.register = register
        wrapper.dispatch = dispatch
        wrapper.registry = MappingProxyType(registry)
        update_wrapper(wrapper, func)

        return wrapper

    return decorate


def check_support_fixpipe_l0c2ub():
    """
    check if the soc support moving data from l0c to ub
    """
    return tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub")


def get_value(shape_object):
    """
    get the value of shape_object when having attr "value"
    """
    return shape_object.value if hasattr(shape_object, "value") else shape_object


class CalculateMultiUB:
    """
    calculate cub multi_fused_num
    """
    BYTES_DTYPE = {"uint64": 8, "float16": 2, "float32": 4, "int32": 4,
                    "int16": 2, "uint16": 2, "int8": 1, "uint8": 1,
                    "int4": 0.5, "bool": 1}
    ALIGN_BYTE = 32
    MASK_SIZE_RATIO = 100
    DEFAULT_SIZE = 2

    def __init__(self, start_tensor, end_tensor, not_count_list):
        self.start_tensor = start_tensor
        self.end_tensor = end_tensor
        self.not_count_list = not_count_list
        self.tensor_occur_times = {}
        self.ub_res = 0
        self.scalar_size = 0

    @staticmethod
    def _get_shape_value(shape_object):
        shape_object_value = int(functools_reduce(lambda x, y: x*y, shape_object))
        return shape_object.value if hasattr(shape_object, "value") else shape_object_value

    def calculate_start(self):
        """
        the enter to calculate the need of multi ub
        """
        self._calculate_multi_ub_auto()
        end_tensor_shape = self._get_shape_value(self.end_tensor.shape)
        self.scalar_size = (self.scalar_size + self.ALIGN_BYTE - 1) // self.ALIGN_BYTE * self.ALIGN_BYTE * \
                           self.MASK_SIZE_RATIO
        self.scalar_size = math.ceil(self.scalar_size / \
                                     (end_tensor_shape * self.BYTES_DTYPE.get(self.end_tensor.dtype, 2)))
        return self.ub_res, self.scalar_size

    def _calculate_multi_ub_auto(self):
        tensor_q = Queue()
        tensor_q.put(self.end_tensor)
        while not tensor_q.empty():
            tensor_out = tensor_q.get()
            if tensor_out == self.start_tensor:
                self._compute_result(tensor_out)
                continue
            merge_flag = False
            input_tensors = list(tensor_out.op.input_tensors)
            for tensor_in in input_tensors:
                if tensor_in in self.tensor_occur_times.keys():
                    continue
                if tensor_in in self.not_count_list:
                    if tensor_in != self.start_tensor:
                        self._merge_compute_inline(tensor_in, input_tensors)
                    continue
                tensor_q.put(tensor_in)
                self.tensor_occur_times[tensor_in] = 1
                if merge_flag:
                    continue
                if self._can_merge(tensor_out, tensor_in):
                    merge_flag = True
            if not merge_flag:
                self._compute_result(tensor_out)
        return True

    def _merge_compute_inline(self, tensor, input_tensors):
        tensor_not_count_q = Queue()
        tensor_not_count_q.put(tensor)
        while not tensor_not_count_q.empty():
            cur_tensor = tensor_not_count_q.get()
            for next_tensor in list(cur_tensor.op.input_tensors):
                if next_tensor in self.not_count_list:
                    if next_tensor != self.start_tensor:
                        tensor_not_count_q.put(next_tensor)
                else:
                    input_tensors.append(next_tensor)
        return True

    def _can_merge(self, tensor_out, tensor_in):
        if tensor_out.op.attrs is not None and "not_reuse_pre_tensors" in tensor_out.op.attrs:
            if tensor_out.op.attrs["not_reuse_pre_tensors"].value:
                return False

        tensor_out_dtype = tensor_out.dtype
        tensor_out_shape = self._get_shape_value(tensor_out.shape)
        tensor_in_dtype = tensor_in.dtype
        tensor_in_shape = self._get_shape_value(tensor_in.shape)
        if self._not_count(tensor_in):
            return False
        can_merge = (tensor_out_dtype == tensor_in_dtype) and (tensor_out_shape == tensor_in_shape)
        return can_merge

    def _compute_result(self, tensor):
        if self._not_count(tensor):
            return
        self.ub_res += self.BYTES_DTYPE.get(tensor.dtype, self.DEFAULT_SIZE)
        return

    def _not_count(self, tensor):
        if tensor in self.not_count_list:
            return True
        shape_size = self._get_shape_value(tensor.shape)
        if shape_size == 1:
            self.scalar_size += self.BYTES_DTYPE.get(tensor.dtype, 2)
            return True
        return False


def get_precision_mode(op_name):
    """
    get mad calculation mode from op_context

    :param op_name: op_name
    :return: str
    """
    context = op_context.get_context()
    if context is None:
        return ""
    op_infos = context.get_op_info()
    if op_infos is None:
        return ""
    for op_info in op_infos:
        if op_info.op_type in OP_TYPE_MAP.get(op_name, []):
            return op_info.precision_mode
    return ""


def get_op_impl_mode_enum(op_name):
    """
    get op_impl_mode_enum from op_name

    :param op_name
    :return: int
    """
    mapping = {'default': 0x1,
               'high_performance': 0x2,
               'high_precision': 0x4,
               'super_performance': 0x8,
               'support_of_bound_index': 0x10,
               'enable_float_32_execution': 0x20,
               'enable_hi_float_32_execution': 0x40}

    op_impl_mode = get_precision_mode(op_name)
    if not op_impl_mode:
        op_impl_mode = 'default'

    if op_impl_mode not in mapping:
        raise_err_message_cube(f'not support convert op_impl_mode {op_impl_mode} to int')
    return mapping[op_impl_mode]


def get_all_tensor(last_tensor, stop_tensor=None):
    """
    find compute_tensor, placeholder_tensors, output_tensors
    :param tensor: last_tensor  before it, all tensors will be browsed
    :param tensor: stop_tensor, before it, the tensors will be discarded
        O  --------->  O
        |              |
        X              |
        |              |
        X(stop_tensor) |
        |              |
        O  <---------  O
        |
        O(last_tensor)
    O will be recorded, x will be discarded
    """
    compute_tensors = {}
    placeholder_tensors = {}
    output_tensors = {}

    compute_tensors[last_tensor.op.name] = last_tensor

    def get(cur_tensor):
        for input_tensor in cur_tensor.op.input_tensors:
            # stop browsed
            if stop_tensor is not None and input_tensor.op.name == stop_tensor.op.name:
                continue
            if isinstance(input_tensor.op, tvm.PlaceholderOp):
                if input_tensor.op.name not in placeholder_tensors:
                    placeholder_tensors[input_tensor.op.name] = input_tensor
            else:
                if input_tensor.op.name not in compute_tensors:
                    compute_tensors[input_tensor.op.name] = input_tensor
                    get(input_tensor)
            if input_tensor in output_tensors:
                if cur_tensor not in output_tensors.get(input_tensor):
                    output_tensors.get(input_tensor).append(cur_tensor)
            else:
                output_tensors[input_tensor] = [cur_tensor]
    get(last_tensor)
    return compute_tensors, placeholder_tensors, output_tensors


def get_load3d_special_factor(flag_load3d_special):
    """
    get factor for load3d special case.
    The condition of load3d special case:
       1. Wo == 1
       2. The chip load3d instruction is defective and cannot handle this scenario
    In this scenario, we need to amplify Wo by 2 times so that Wo equals 2 to complete load3D,
    But the output is still 1, so the tiling of the output needs to be divided by 2

    :param flag_load3d_special: Whether it's a scene load3d_special
    :return: load3d_special_factor
    """
    load3d_special_factor = 2 if flag_load3d_special else 1

    return load3d_special_factor
