#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Schedule of conv2d in v220/v300.
"""
import sys
import os
import inspect
from concurrent.futures import process
from functools import reduce
from collections import deque
import tbe
from tbe import tvm
from tbe.tvm import deepcopy
from tbe.common import platform as tbe_platform
from tbe.common.utils import log
from tbe.common.platform import CUBE_MKN
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.tiling import tiling_api
from tbe.common.register import set_fusion_buildcfg
from tbe.common.context import get_context
from tbe.dsl.base.operation import get_compile_info
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.common.utils.op_util import op_util_conv2d
from tbe.common.utils.op_util.op_util_conv2d import AttachMode
from tbe.common.utils.op_util.op_util_conv2d import BinaryTilingKey
from tbe.common.utils.op_util.op_util_conv2d import ConstValue
from tbe.common.utils.op_util.op_util_conv2d import BIT_RATIO_MAP
from tbe.common.utils.op_util.op_util_conv2d import Conv2dTensorName
from tbe.common.utils.op_util.op_util_conv2d import get_min_al1_ci1
from tbe.common.utils.op_util.op_util_conv2d import TilingDataKey
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe, is_support_v300, is_support_v220, is_v300_soc
from tbe.common.utils.op_util.op_util_conv2d import is_v310_soc, is_l0a_layout_zn
from tbe.common.utils.op_util.op_util_conv2d import UB_COEFF_CONVERT
from tbe.common.utils.op_util.op_util_conv2d import Conv2DL0aDmaScene
from tbe.dsl.compute.conv_compute import is_support_v200
from tbe.common.utils.op_util.op_util_conv2d import check_support_db_fold
from tbe.common.utils.op_util.op_util_conv2d import add_tiling_compile_info
from tbe.common.utils.op_util.op_util_conv2d import WINO_OUT_TILE_HW
from tbe.dsl.compute.max_pool2d_3_2_fusion_compute import MaxPoolParam
from tbe.dsl.static_schedule import util
from tbe.dsl.static_schedule import conv_schedule_log_util
from tbe.dsl.static_schedule.conv_fixpipefusion_schedule import FixpipeFusion
from tbe.dsl.static_schedule.conv_schedule_util import ceil, ceil_div, get_src_tensor
from tbe.dsl.static_schedule.conv_schedule_util import delete_op, is_wino_res_tensor
from tbe.dsl.static_schedule.conv_ubfusion_schedule import EltwiseUBFusion
from tbe.dsl.static_schedule.conv_ubfusion_schedule import QuantFusion
from tbe.dsl.static_schedule.conv_maxpoolfusion_schedule import MaxpoolFusion
from tbe.dsl.static_schedule.conv_bn1_fusion_schedule import Conv2dBN1Fusion
from tbe.dsl.static_schedule.quant_conv2d_schedule import QuantConv2dSchedule
from tbe.dsl.static_schedule.conv_dynamic_util import DynamicShape
from tbe.dsl.static_schedule.conv_dynamic_util import set_al1_buffer_size_common
from tbe.dsl.static_schedule.conv_dynamic_util import get_al1_bound_common
from tbe.dsl.static_schedule.conv_schedule_winograd import WinogradConv
from te.platform import cce_params

DDR_SCOPE = 0
L1_SCOPE = 1
L2_SCOPE = 2

INTEGER_BITS_SIZE = 32
K0_ALIGN_BYTES = 32
BINARY_HEAD_0B = 2
C04_C0 = 4

NON_L1_FUSION = -1
DEPTH_L1_FUSION = 0
BREADTH_L1_FUSION = 1

DOUBLE_BUFFER_DISABLED = 1
DOUBLE_BUFFER_ENABLED = 2
AIPP_DST_ADDR_ALIGN = 32

# conv2d attr num (except private attr)
ATTRLEN_CONV2D = 6

STATIC_FAST_TILING_WHITE_LIST = [
    ([1500, 1, 64, 128], [1500, 1, 30, 64]),
    ([1500, 1, 128, 64], [1500, 1, 62, 32]),
    ([1500, 1, 30, 64], [1500, 1, 13, 128]),
    ([1500, 1, 62, 32], [1500, 1, 29, 64]),
    ([1500, 1, 60, 88], [1500, 1, 28, 44]),
    ([1500, 1, 88, 60], [1500, 1, 42, 30]),
    ([1500, 1, 28, 44], [1500, 1, 12, 88]),
    ([1500, 1, 42, 30], [1500, 1, 19, 60]),
    ([1500, 1, 50, 124], [1500, 1, 23, 62]),
    ([1500, 1, 124, 50], [1500, 1, 60, 25]),
    ([1500, 1, 23, 62], [1500, 1, 10, 124]),
    ([1500, 1, 60, 25], [1500, 1, 28, 50]),
    ([1500, 1, 30, 124], [1500, 1, 13, 62]),
    ([1500, 1, 124, 30], [1500, 1, 60, 15]),
    ([1500, 1, 13, 62], [1500, 1, 5, 124]),
    ([1500, 1, 60, 15], [1500, 1, 28, 30]),
    ([1500, 1, 50, 116], [1500, 1, 23, 58]),
    ([1500, 1, 116, 50], [1500, 1, 56, 25]),
    ([1500, 1, 23, 58], [1500, 1, 10, 116]),
    ([1500, 1, 56, 25], [1500, 1, 26, 50]),
]


def show_ir(sch):
    file_path = os.path.dirname(os.path.realpath(__file__))
    line_no = inspect.currentframe().f_back.f_lineno
    funcname = inspect.currentframe().f_back.f_code.co_name
    co_filename = inspect.currentframe().f_back.f_code.co_filename
    filename = os.path.relpath(co_filename, file_path)
    log_str = '[%s:%d][%s] ' % (filename, line_no, funcname)
    sch1 = sch.normalize()
    bounds = tvm.schedule.InferBound(sch1)
    stmt = tvm.ScheduleOps(sch1, bounds, True)
    print("[show_ir]" + log_str)
    print(stmt)


class InputNd2Nz:
    """
    Class of input nd2nz.
    Transform the fmap of ND format in ddr to the fmap_l1 of 5hd format in L1.
    Only support NHWC in v2; only support NCHW in binary
    """
    def __init__(self, conv_param):
        self.flag = conv_param.input_nd_flag
        self.mode = conv_param.input_nd_mode
        self._fmap_in_ub = {}
        self.aub_at_al1_axis = None

    @staticmethod
    def al1_nd2nz_emit_insn(sch, al1):
        """
        Dma for the al1 tensor in nd2nz situation.
        """
        sch[al1].emit_insn(al1.op.axis[1],
                           "dma_copy",
                           {"layout_transform": "nd2nz"})

    def inline_input_nd(self, sch, tensor_map, dynamic_flag, l0a_load2d_flag):
        """
        inline al1 in nd2nz dynamic situation (group = 1).
        """
        input_nd = tensor_map.get("fmap")

        if self.flag and dynamic_flag:
            sch[input_nd].compute_inline()

    def input_nd_binary_set_scope(self, sch, _op_graph, fmap):
        """
        set scope in binary input_nd
        """
        if self.mode == "NCHW":
            nchw_in_gm = None
            for lop in _op_graph.input_ops:
                next_op = lop["next_op"][0]["dst_buffer"]
                if next_op.op.name == "input_ub_td":
                    nchw_in_gm = lop["dst_buffer"]
                    break

            tensor_queue = deque()
            tensor_queue.append(fmap)
            while tensor_queue:
                process_tensor = tensor_queue.popleft()
                if process_tensor.op.name == "reshape_c":
                    sch[process_tensor].compute_inline()
                elif process_tensor != fmap:
                    sch[process_tensor].set_scope(cce_params.scope_ubuf)
                    self._fmap_in_ub[process_tensor.op.name] = process_tensor

                if process_tensor.op.input_tensors:
                    append_list = list(i for i in process_tensor.op.input_tensors if i != nchw_in_gm)
                    tensor_queue.extend(append_list)

            sch[fmap].compute_inline()

            sch[self._fmap_in_ub.get("input_ub_vn")].reused_by(
                self._fmap_in_ub.get("input_ub_td"), self._fmap_in_ub.get("input_ub_pad"))

            sch[self._fmap_in_ub.get("transpose_hw_c0")].mem_unique()

    def input_nd_binary_compute_at(self, sch, al1, l0a_load2d_flag, aub_nparts):
        """
        compute at in binary input_nd
        """
        if self.mode == "NCHW":
            if l0a_load2d_flag:
                # al1 shape [n, ci1, m, ci0]
                c1_outer, c1_inner = sch[al1].split(sch[al1].op.axis[1], nparts=aub_nparts[0])
                h_outer, h_inner = sch[al1].split(sch[al1].op.axis[2], nparts=aub_nparts[1])
                sch[al1].reorder(c1_outer, h_outer, sch[al1].op.axis[0],
                                 c1_inner, h_inner)
            else:
                # al1 shape [group_opt, n, ci1, hi, wi, ci0]
                c1_outer, c1_inner = sch[al1].split(sch[al1].op.axis[2], nparts=aub_nparts[0])
                h_outer, h_inner = sch[al1].split(sch[al1].op.axis[3], nparts=aub_nparts[1])
                sch[al1].reorder(c1_outer, h_outer, sch[al1].op.axis[0],
                                 sch[al1].op.axis[1], c1_inner, h_inner)
            for tensor in self._fmap_in_ub.values():
                sch[tensor].compute_at(sch[al1], h_outer)
            self.aub_at_al1_axis = h_outer

    def input_nd_binary_process_post(self, sch, conv_param, tiling_param,
                                     emit_insn_dict, attach_axis_dict, al1_bound_list):
        """
        binary input_nd process post
        """
        if self.mode == "NCHW":
            def get_buffer_tile_m_aub_offset_split_w():
                # split_w(current is only conv1d scene supported)
                if al1_tiling:
                    # only split_w_out2cl0_loopm_axis can be not 1, other axis's bound related to ho and wo is [0,1]
                    split_w_out2cl0_loopm_axis = attach_axis_dict.get("split_w_out2cl0_loopm_axis")
                    m_offset = split_w_out2cl0_loopm_axis * m_al1
                    wi_offset = m_offset * conv_param.stride_w - conv_param.padding[2]  # 2: pad_left index
                else:
                    # splitw al1 full load, wi is full load both in l1 and ub, start offset is 0
                    wi_offset = tvm.const(0)
                return wi_offset

            def get_buffer_tile_m_aub_offset():
                # calc aub m axis offset
                # typical IR be like （only m axis）
                # nums for bindcore
                #     times for singlecore m out to l1
                #         times for singlecore m ub to l1
                #             m_aub_size
                #         m_al1_size
                res_m_dim_axis = attach_axis_dict["singlecore_out2al1_loopm_axis"]
                bindcore_axis = emit_insn_dict["bindcore_axis"]
                if m_dim != 1:
                    res_m_dim_axis = res_m_dim_axis + bindcore_axis % m_dim * ceil_div(al1_nparts[1], m_dim)

                m_offset = tvm.min(res_m_dim_axis * m_al1, out_width * out_height - 1)

                if not conv_param.l0a_load2d_flag:
                    ho_offset = tvm.floordiv(m_offset, out_width)
                    hi_offset = ho_offset * conv_param.stride_h - conv_param.padding[0]
                    m_offset = hi_offset * in_width

                m_aub_offset = m_offset + self.aub_at_al1_axis * m_aub_bound
                m_aub_offset = tvm.min(m_aub_offset + m_aub_bound, in_height * in_width) - m_aub_bound
                m_aub_offset = tvm.max(m_aub_offset, tvm.const(0))

                return m_aub_offset

            m_al1_bound, k1_al1, in_c0 = al1_bound_list
            k_aub_nparts, m_aub_nparts = 1, 1
            if "aub_nparts" in tiling_param:
                k_aub_nparts, m_aub_nparts = tiling_param["aub_nparts"]

            m_aub_bound = ceil(m_al1_bound//m_aub_nparts, in_c0)
            aub_buffer_size = m_aub_bound * (k1_al1//k_aub_nparts) * in_c0

            _, _, in_height, in_width, _ = tiling_param["fmap_5hd_shape"]
            out_width = conv_param.w_out
            out_height = conv_param.h_out

            al1_tiling = tiling_param["al1_tiling"]
            al1_nparts = tiling_param["al1_nparts"]
            m_dim = tiling_param["block_dim"][2]

            if al1_tiling and not conv_param.l0a_load2d_flag:
                multi_m_al1 = tiling_param["multi_m_al1"]
                m_cl0 = tiling_param["m_cl0"]
                m_al1 = multi_m_al1 * m_cl0
            else:
                m_al1 = m_al1_bound

            if conv_param.split_w_flag:
                m_aub_offset = get_buffer_tile_m_aub_offset_split_w()
            else:
                m_aub_offset = get_buffer_tile_m_aub_offset()

            for tensor in self._fmap_in_ub.values():
                sch[tensor].set_buffer_size(aub_buffer_size)
                if tensor.op.name == "transpose_hw_c0":
                    sch[tensor].storage_align(tensor.op.axis[-3], 256, 0)
                    sch[tensor].buffer_tile((None, None), (None, None),
                                            (m_aub_offset, m_aub_bound), (None, None))
                else:
                    sch[tensor].storage_align(tensor.op.axis[-2], 16, 0)
                    sch[tensor].buffer_tile((None, None), (None, None),
                                            (m_aub_offset, m_aub_bound))

    def input_nd_binary_double_buffer(self, sch):
        """
        binary input_nd enable AUB double buffer
        """
        if self.mode == "NCHW":
            for tensor in self._fmap_in_ub.values():
                sch[tensor].double_buffer()

    def input_nd_binary_emit_insn(self, sch):
        """
        binary input_nd emit_insn
        """
        if self.mode == "NCHW":
            binary_schedule_emit_insn_dict = {
                "input_ub_td": "dma_copy",  # N C_align HiWi
                "input_ub_pad": "vector_dup",
                "input_ub_vn": "phony_insn",
            }
            for tensor_name in binary_schedule_emit_insn_dict.keys():
                tensor = self._fmap_in_ub.get(tensor_name)
                sch[tensor].emit_insn(tensor.op.axis[0], binary_schedule_emit_insn_dict.get(tensor_name))

            # transpose (N C_align HiWi) to (N Ci1 HiWi Ci0)
            transpose_hw_c0 = self._fmap_in_ub.get("transpose_hw_c0")
            src_in_dst_order = tvm.call_intrin('handle', 'tir.tvm_tuple', 1, 0)
            sch[transpose_hw_c0].emit_insn(transpose_hw_c0.op.axis[2], 'vector_transpose',
                                           attrs={'src_in_dst_order': src_in_dst_order})


class WeightNd2Nz:
    """
    Class of weight nd2nz.
    Transform the weight of NHWC format in ddr to the weight_l1 of fracZ format in L1.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.weight_nd_flag

    @staticmethod
    def bl1_nd2nz_emit_insn(sch, bl1):
        """
        Dma for the bl1 tensor in weight nd2nz situation.
        """
        sch[bl1].emit_insn(bl1.op.axis[0],
                           "dma_copy",
                           {"layout_transform": "nd2nz"})

    def check_bl1_nd2nz_tiling(self, tiling):
        """
        Check whether the bl1 tiling is None in weight nd2nz situation.
        """
        if self.flag and tiling["BL1_shape"] is None:
            err_man.raise_err_specific(
                "conv2d", "BL1 tiling cannot be None when weight nd2nz.")


class OutputNz2Nd:
    """
    Class of output nz2nd.
    Transform the output of 5hd format in L0C to the output of ND format in DDR.
    """
    def __init__(self, res):
        self.flag = False
        self.mode = None
        self.c_ub_transpose = None
        self.output_nchw_tensor = None
        self._set_output_nd_flag_mode(res)

    def output_nd_binary_set_scope(self, sch):
        """
        set scope in binary output_nd
        """
        if self.mode == "NCHW":
            self.c_ub_transpose = self.output_nchw_tensor.op.input_tensors[0]
            sch[self.c_ub_transpose].set_scope(cce_params.scope_ubuf)
            sch[self.output_nchw_tensor].compute_inline()

    def output_nd_binary_compute_at(self, sch, res, cub_at_res_axis):
        """
        compute at in binary output_nd
        """
        if self.mode == "NCHW":
            sch[self.c_ub_transpose].compute_at(sch[res], cub_at_res_axis)

    def output_nd_binary_process_post(self, sch, cub, tiling_param):
        """
        binary output_nd process post
        """
        if self.mode == "NCHW":
            cub_buffer_size = reduce((lambda x, y: x*y), tiling_param.get("cub_tiling"))
            sch[self.c_ub_transpose].set_buffer_size(cub_buffer_size)
            sch[self.c_ub_transpose].storage_align(self.c_ub_transpose.op.axis[-2], 16, 0)
            process_tensor = self.c_ub_transpose
            while True:
                process_tensor = process_tensor.op.input_tensors[0]
                if process_tensor == cub:
                    break
                sch[process_tensor].set_buffer_size(cub_buffer_size)
                sch[process_tensor].storage_align(sch[process_tensor].op.axis[-3], 256, 0)
                if not process_tensor.op.input_tensors:
                    break

    def res_nz2nd_emit_insn(self, sch, res):
        """
        Emit insn for res in output nz2nd situation.
        """
        if self.mode == "NCHW":
            # transpose (N, Co1, HoWo_mad, Co0) -> (N, Co1, Co0, HoWo_mad)
            src_in_dst_order = tvm.call_intrin('handle', 'tir.tvm_tuple', 1, 0)
            sch[self.c_ub_transpose].emit_insn(self.c_ub_transpose.op.axis[2], 'vector_transpose',
                                               attrs={'src_in_dst_order': src_in_dst_order})

            leaf_ivars = list(sch[res].leaf_iter_vars)
            res_emit_insn_axis = leaf_ivars[-2::][0]
            sch[res].emit_insn(res_emit_insn_axis,
                               "dma_copy",
                               {'no_overlap': "process_data_smaller_than_one_block"})

    def cal_nz2nd_cub_coeff(self, coeff_unit):
        """
        Get ub coefficient (of cc_to_ub mad tensor's succeeded tensor(s) on UB) in nz2nd situation.
        """
        cub_coeff_nz2nd = 0
        if not self.flag:
            return cub_coeff_nz2nd
        if is_support_fixpipe():  # There is no cc_to_ub mad result tensor.
            return cub_coeff_nz2nd

        if self.mode == "NCHW":
            cub_coeff_nz2nd += coeff_unit  # Add tensor "c_ub_transpose"

        return cub_coeff_nz2nd

    def _set_output_nd_flag_mode(self, res):
        """
        set output_nd_flag and mode, support NCHW, NHWC
        """
        if res.op.tag == "5HD_trans_NHWC":
            self.flag = True
            self.mode = "NHWC"
        elif res.op.input_tensors[0].op.tag == "5HD_TRANS_NCHW":
            self.flag = True
            self.mode = "NCHW"
            self.output_nchw_tensor = res.op.input_tensors[0]


class LxFusion:
    """
    Class of L1 fusion and L2 fusion.
    """
    def __init__(self, conv_param):
        # lxfusion params
        self.fusion_para = conv_param.fusion_para
        self.l1_fusion_type = self.fusion_para.get("l1_fusion_type")
        self.fmap_l1_addr_flag = self.fusion_para.get("fmap_l1_addr_flag")
        self.fmap_l1_valid_size = self.fusion_para.get("fmap_l1_valid_size")
        self.input_memory_type = self.fusion_para.get("input_memory_type")
        self.output_memory_type = self.fusion_para.get("output_memory_type")

        # combined parameters
        self.l1_fusion_flag = self.l1_fusion_type in (DEPTH_L1_FUSION, BREADTH_L1_FUSION) # l1fusion enable

    @staticmethod
    def align_al1_lxfusion(sch, al1):
        """
        AL1 buffer_align in l1fusion breadth fusion.
        """
        sch[al1].buffer_align(
            (1, 1),
            (1, 1),
            (al1.shape[2], al1.shape[2]),
            (al1.shape[3], al1.shape[3]),
            (1, 1))

    def config_default_tiling(self, default_tiling):
        """
        Config default tiling in l1fusion.
        """
        if self.l1_fusion_flag:
            default_tiling["block_dim"] = [1, 1, 1, 1]
        if self.l1_fusion_type == BREADTH_L1_FUSION:
            default_tiling["AL1_shape"] = []
        return default_tiling

    def check_l1fusion_tiling(self, tiling):
        """
        Check the tiling of l1fusion.
        """
        if self.l1_fusion_type == 1 and tiling["AL1_shape"] != []:
            err_man.raise_err_value_or_format_invalid(
                "conv2d", "tiling[\"AL1_shape\"]", "[]",
                "when l1_fusion_type is breadth fusion.")

        if self.l1_fusion_flag:
            if tiling["A_overhead_opt_flag"]:
                err_man.raise_err_value_or_format_invalid(
                    "conv2d", 'tiling["A_overhead_opt_flag"]', "False", "when l1_fusion.")

            if tiling["B_overhead_opt_flag"]:
                err_man.raise_err_value_or_format_invalid(
                    "conv2d", 'tiling["B_overhead_opt_flag"]', "False", "when l1_fusion.")

            if tiling["block_dim"] != [1, 1, 1, 1]:
                err_man.raise_err_specific(
                    "conv2d", "only support one core tiling in L1 Fusion.")

    def config_al1_scope(self):
        """
        Config L1 scope to scope_cbuf_fusion when l1fusion is enabled.
        """
        if self.l1_fusion_flag:
            return cce_params.scope_cbuf_fusion
        return cce_params.scope_cbuf

    def config_l1_tensormap(self, sch, fmap, al1, op_graph):
        """
        Append L1 tensor in cce kernel function.
        """
        l1_tensor_map = {}
        if self.fmap_l1_addr_flag == "nothing":
            l1_tensor_map = None
        else:
            if self.l1_fusion_flag and self.input_memory_type[0] in (DDR_SCOPE, L2_SCOPE):
                for input_op in op_graph.input_ops:
                    l1_tensor_map[input_op["dst_buffer"]] = tvm.var("dummy")
                l1_tensor_map[fmap] = al1
                if self.fmap_l1_valid_size > 0:
                    sch[al1].set_buffer_size(self.fmap_l1_valid_size)
            else:
                l1_tensor_map = None

        util.L1CommonParam.l1_fusion_tensors_map = l1_tensor_map

    def al1_l1fusion_pragma(self, sch, al1):
        """
        Pragma on AL1 when l1fusion and fmap ddr in.
        """
        if self.input_memory_type[0] != L1_SCOPE and self.l1_fusion_flag:
            sch[al1].pragma(al1.op.axis[0], 'jump_data', 1)


class AippFusion:
    """
    Class of Aipp + conv2d fusion.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.aipp_fuse_flag

    @staticmethod
    def al1_aipp_emit_insn(sch, al1):
        """
        Emit insn for al1 in aipp fusion.
        """
        aipp_map = al1.op.attrs
        aipp_map['spr_0'] = al1.op.axis[0]
        aipp_map_res = {"spr_0": al1.op.axis[0],
                        "spr_1": aipp_map["spr_1"],
                        "spr_2": aipp_map["spr_2"],
                        "spr_3": aipp_map["spr_3"],
                        "spr_4": aipp_map["spr_4"],
                        "spr_8": aipp_map["spr_8"],
                        "spr_9": aipp_map["spr_9"],
                        "src_image_h": aipp_map["src_image_h"],
                        "src_image_w": aipp_map["src_image_w"],
                        "input_format": aipp_map["input_format"],
                        "load_start_pos_h": aipp_map["load_start_pos_h"],
                        "load_start_pos_w": aipp_map["load_start_pos_w"],
                        "crop_size_h": aipp_map["crop_size_h"],
                        "crop_size_w": aipp_map["crop_size_w"]}
        # v300 spr5-7 are deleted
        if not (is_v300_soc() or is_v310_soc()):
            aipp_map_res["spr_5"] = aipp_map["spr_5"]
            aipp_map_res["spr_6"] = aipp_map["spr_6"]
            aipp_map_res["spr_7"] = aipp_map["spr_7"]

        if is_v300_soc() or is_v310_soc():
            # v300 spr18-spr21 are added
            aipp_map_res["spr_18"] = aipp_map["spr_18"]
            aipp_map_res["spr_19"] = aipp_map["spr_19"]
            aipp_map_res["spr_20"] = aipp_map["spr_20"]
            aipp_map_res["spr_21"] = aipp_map["spr_21"]
            # padding size for 4 directions
            aipp_map_res["padding_left"] = aipp_map.get("padding_left", 0)
            aipp_map_res["padding_right"] = aipp_map.get("padding_right", 0)
            aipp_map_res["padding_top"] = aipp_map.get("padding_top", 0)
            aipp_map_res["padding_bottom"] = aipp_map.get("padding_bottom", 0)

        sch[al1].emit_insn(al1.op.axis[1], "load_image_to_cbuf", aipp_map_res)

    @staticmethod
    def align_al1_aipp(sch, al1):
        """
        align aipp dst al1
        """
        if is_v300_soc() or is_v310_soc():
            # intrinsic constraints：load out to l1 image dst addr must be 32-Byte aligned
            sch[al1].storage_align(al1.op.axis[0], AIPP_DST_ADDR_ALIGN // BIT_RATIO_MAP.get(al1.dtype), 0)


class StridedRead:
    """
    Class of StridedRead + Conv2d fusion.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.strided_read_flag

    def process_strided_read(self, sch, al1, strideh_opti_flag, l0a_load2d_flag):
        """
        Inline the output tensor of strided read when strideh_opti or l0a_load2d is enabled.
        """
        def inline_fmap_strided_read(fusion_flag, al1):
            if fusion_flag and self.flag:
                fmap_strided_read = get_src_tensor(al1)
                sch[fmap_strided_read].compute_inline()

        inline_fmap_strided_read(strideh_opti_flag, al1)
        inline_fmap_strided_read(l0a_load2d_flag, al1)


class StridedWrite:
    """
    Class of Conv2d + StridedWrite fusion.
    """
    def __init__(self, res):
        self.flag = res.op.tag == "strided_write"

    def process_strided_write(self, sch, res):
        """
        Inline the output tensor of strided write.
        """
        if self.flag:
            strided_write_src = get_src_tensor(res)
            res_hw, res_c0 = res.shape[-2:]
            sch[strided_write_src].compute_inline()
            sch[res].bind_buffer(res.op.axis[0], res_hw * res_c0 * res.op.attrs['stride'], 0)


class Im2colDma:
    """
    Class of Im2col dma when load3d exceeds L1 size.
    """
    def __init__(self, conv_param, block_k0):
        self.flag = conv_param.l0a_dma_flag
        self.split_k1_flag = conv_param.l0a_dma_split_k1_flag
        self.special_scene = conv_param.get_l0a_dma_scene()
        self._block_k0 = block_k0

    @staticmethod
    def config_al0_im2coldma(sch, al1_im2col, cl0):
        """
        Cache read al1_im2col into L0.
        """
        al0 = sch.cache_read(al1_im2col, cce_params.scope_ca, [cl0])
        return al0

    @staticmethod
    def im2col_dma_emit_insn(sch, al1_im2col, al0, al0_axis_list):
        """
        Emit insn for al1_im2col and al0.
        """
        sch[al0].emit_insn(al0_axis_list[0], "dma_copy")

    def dma_im2col_emit_insn(self, sch, tensor_param, tiling_param,
                             conv_param, _l0a_layout_zn_flag):
        """
        Buffer align al1_im2col.
        """
        def dma_im2col_emit_insn_static_scene():
            k1_outer, k1_inner = sch[al1_im2col].split(al1_im2col.op.axis[al1_k1_idx], factor=conv_param.filter_w)
            k1_outer_outer, k1_outer_inner = sch[al1_im2col].split(k1_outer, factor=conv_param.filter_h)
            sch[al1_im2col].reorder(al1_im2col.op.axis[al1_g_idx],
                                    al1_im2col.op.axis[al1_batch_idx],
                                    al1_im2col.op.axis[al1_m1_idx],
                                    k1_outer_outer,
                                    al1_im2col.op.axis[al1_mo],
                                    k1_outer_inner,
                                    k1_inner,
                                    al1_im2col.op.axis[al1_k0])
            sch[al1_im2col].emit_insn(k1_outer_inner, "dma_copy")

        def dma_im2col_emit_insn_binary_scene():
            # emit insn for al1_im2col
            # Binary mode will trigger EliminateBranch in [Pass]. Static mode will not.
            # EliminateBranch does not support non-linear transform, such as floordiv/floormod, min/max.
            # EliminateBranch does not support more than 1 axis below emit insn in IF conditon.
            # Axis below emit insn takes part in eliminate branch and has non-linear will report compile error.
            # Tuscany L1 buffer can not enable pad, while Milan L1 buffer can enable pad.
            # dma in Tuscany, al1_im2col is im2col_fractal_v2, while in Milan, is fmap_l1_dma_im2col.
            # im2col_fractal_v2 has no pad select process, aub_dma has. Emit insn also is in c0_idx.
            # fmap_l1_dma_im2col has pad select process, which using k axis, trigger eliminate branch.
            # As a result, put this emit insn just above last axis, that is al1_k0 axis.
            al1_tiling = tiling_param.get('al1_tiling')
            kh_x_kw = conv_param.filter_h * conv_param.filter_w
            if al1_tiling:  # Split according to al1_tiling[0], which is kal1_k0.
                k1_outer, k1_inner = sch[al1_im2col].split(al1_im2col.op.axis[al1_k1_idx], factor=tvm.min(al1_tiling[0],
                                                                                                          kh_x_kw))
                k1_inner_outer, k1_inner_inner = sch[al1_im2col].split(k1_inner, factor=tvm.min(al1_tiling[0],
                                                                                                conv_param.filter_w))
            else:
                k1_outer, k1_inner = sch[al1_im2col].split(al1_im2col.op.axis[al1_k1_idx], factor=kh_x_kw)
                k1_inner_outer, k1_inner_inner = sch[al1_im2col].split(k1_inner, factor=conv_param.filter_w)

            if self.special_scene == Conv2DL0aDmaScene.DMA_CONV1D_WITHOUT_PAD:
                sch[al1_im2col].reorder(al1_im2col.op.axis[al1_g_idx], # group
                                        al1_im2col.op.axis[al1_batch_idx], # batch
                                        al1_im2col.op.axis[al1_m1_idx], # m1
                                        al1_im2col.op.axis[al1_mo], # m0
                                        k1_outer, # --> emit_insn axis
                                        k1_inner_outer,
                                        k1_inner_inner,
                                        al1_im2col.op.axis[al1_k0]) # k0
            else:
                sch[al1_im2col].reorder(al1_im2col.op.axis[al1_g_idx],
                                        al1_im2col.op.axis[al1_batch_idx],
                                        al1_im2col.op.axis[al1_m1_idx],
                                        k1_outer,
                                        al1_im2col.op.axis[al1_mo],
                                        k1_inner_outer,
                                        k1_inner_inner,
                                        al1_im2col.op.axis[al1_k0])
            if is_support_fixpipe():
                if self.special_scene == Conv2DL0aDmaScene.DMA_CONV1D_WITHOUT_PAD:
                    sch[al1_im2col].emit_insn(k1_outer, "dma_copy")
                else:
                    sch[al1_im2col].emit_insn(al1_im2col.op.axis[al1_k0], "dma_copy")
            elif self.special_scene == Conv2DL0aDmaScene.AUB_ONLY_LOAD_K0:
                sch[tensor_param["aub_dma"]].compute_at(sch[al1_im2col], k1_inner_inner)
                sch[al1_im2col].emit_insn(al1_im2col.op.axis[al1_k0], "dma_copy")
            else:
                sch[tensor_param["aub_dma"]].compute_at(sch[al1_im2col], al1_im2col.op.axis[al1_mo])
                sch[al1_im2col].emit_insn(k1_inner_outer, "dma_copy")

        if is_support_fixpipe():
            al1_zero = tensor_param["al1_zero"]
            al1_im2col = tensor_param["al1_dma_im2col"]
            al1_virtual_add = tensor_param["al1_im2col"]
        else:
            al1_im2col = tensor_param["al1_im2col"]

        if _l0a_layout_zn_flag:
            al1_g_idx, al1_batch_idx, al1_k1_idx, al1_m1_idx, al1_mo, al1_k0 = range(len(al1_im2col.shape))
        else:
            al1_g_idx, al1_batch_idx, al1_m1_idx, al1_k1_idx, al1_mo, al1_k0 = range(len(al1_im2col.shape))
        if conv_param.binary_mode:
            dma_im2col_emit_insn_binary_scene()
        else:
            dma_im2col_emit_insn_static_scene()

        # emit insn for al0
        al0 = tensor_param["al0"]
        al0_g_idx, _, _, _, _, _ = range(len(al0.shape))
        sch[al0].emit_insn(al0.op.axis[al0_g_idx], "dma_copy")
        if is_support_fixpipe():
            # emit insn for dma tensors
            sch[al1_zero].emit_insn(al1_zero.op.axis[0], "set_2d")
            sch[al1_virtual_add].emit_insn(al1_virtual_add.op.axis[0], "phony_insn")
            # reuse dma tensor memory
            sch[al1_virtual_add].reused_by(al1_zero, al1_im2col)
        else:
            # emit insn fo aub_dma
            aub_dma_size = \
                {Conv2DL0aDmaScene.BASIC: conv_param.filter_w_dilation * conv_param.filter_h_dilation * self._block_k0,
                 Conv2DL0aDmaScene.AUB_ONLY_LOAD_K0: self._block_k0}
            aub_dma = tensor_param["aub_dma"]
            _, _, _, h_idx, w_idx, c0_idx = range(len(aub_dma.shape))
            sch[aub_dma].emit_insn(aub_dma.op.axis[h_idx], "dma_copy")
            sch[aub_dma].set_buffer_size(aub_dma_size.get(self.special_scene,
                                                          aub_dma_size.get(Conv2DL0aDmaScene.BASIC)))

    def config_al1_im2col(self, sch, tensor_map):
        """
        Get the im2col_fractal tensor in L1.
        """
        if self.flag:
            al1_im2col = tensor_map["fmap_im2col"]
            sch[al1_im2col].set_scope(cce_params.scope_cbuf)
            if is_support_fixpipe():
                al1_zero = tensor_map["fmap_l1_dma_zero"]
                al1_dma_im2col = tensor_map["fmap_l1_dma_im2col"]
                sch[al1_zero].set_scope(cce_params.scope_cbuf)
                sch[al1_dma_im2col].set_scope(cce_params.scope_cbuf)
        else:
            al1_im2col = None
        return al1_im2col

    def config_aub(self, sch, tensor_map):
        """
        Get the fmap tensor in ub.
        """
        if self.flag and not is_support_fixpipe():
            aub_dma = tensor_map[Conv2dTensorName.FMAP_L1]
            sch[aub_dma].set_scope(cce_params.scope_ubuf)
        else:
            aub_dma = None
        return aub_dma

    def align_al1_im2col(self, sch, al1_im2col, block_k0):
        """
        Buffer align al1_im2col.
        """
        if self.flag:
            sch[al1_im2col].buffer_align(
                (1, 1),
                (1, 1),
                (1, 1),
                (1, 1),
                (1, 1),
                (1, block_k0))


class InnerBatch:
    """
    Class of l0b innerbatch.
    """
    def __init__(self):
        self.flag = False # inner_batch of sharing L0B weight

    def set_l0b_innerbatch_flag(self, tiling, batch_cl0):
        """
        Set l0b innerbatch flag by tiling.
        """
        if batch_cl0 > 1:
            self.flag = True
            log.debug("enable innerbatch")

    def config_innerbatch_axis(self, batch_al1, batch_cl0):
        """
        Config innerbatch case batch inner split axis.
        """
        if self.flag:
            return batch_cl0
        return batch_al1

    def check_innerbatch_tiling(self, tiling, batch, out_hw):
        """
        Check the tiling of l0b innerbatch.
        """
        if self.flag:
            _, mc_cl0, m0_cl0, _, batch_cl0, _ = tiling["CL0_matrix"]
            batch_dim, _, _, _ = tiling["block_dim"]
            if mc_cl0 * m0_cl0 != ceil(out_hw, m0_cl0):
                log.warn("conv2d innerbatch case not full load M.")
            if batch_cl0 * batch_dim > batch:
                err_man.raise_err_specific("conv2d", "innerbatch batch_cl0*batch_dim cannot be greater than batch.")


class L0aLoad2d:
    """
    class of fmap load2d optimization.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.l0a_load2d_flag

    @staticmethod
    def load2d_emit_insn(sch, al1, al0, input_nd_mode):
        """
        Emit insn for al1 and al0 when al1_load2d is enabled.
        For nd_mode, nd2nz insn need the last axis to be c0 size, so emit "dma_copy",
        and add "split_select" attr to handle two scenarios of c-axis alignment and non-alignment.
        For 5hd_mode, emit "dma_padding" to avoid using set_value.
        """
        sch[al0].emit_insn(al0.op.axis[0], "dma_copy")

        if input_nd_mode == "NHWC" and is_support_fixpipe():
            sch[al1].emit_insn(al1.op.axis[0], "phony_insn")
        else:
            sch[al1].emit_insn(al1.op.axis[0], "dma_padding")

    @staticmethod
    def set_zero_value_on_k_axis(sch, al0, ci1_opt, in_c1, _l0a_layout_zn_flag):
        """
        set redundant points to ZERO on k-axis
        """
        if _l0a_layout_zn_flag:
            al0_group_idx, _, al0_k1_idx, _, _, _ = range(len(al0.shape))
        else:
            al0_group_idx, _, _, al0_k1_idx, _, _ = range(len(al0.shape))

        sch[al0].set_value(lambda *i: i[al0_group_idx] * ci1_opt + i[al0_k1_idx] >= in_c1, tvm.const(0, al0.dtype))

    def special_process_pre(self, sch, param_dict):
        if self.flag:
            # for multi-group
            if param_dict.get("group_opt") > 1:
                al0 = param_dict.get("al0")
                ci1_opt = param_dict.get("ci1_opt")
                in_c1 = param_dict.get("in_c1")
                _l0a_layout_zn_flag = param_dict.get("_l0a_layout_zn_flag")
                self.set_zero_value_on_k_axis(sch, al0, ci1_opt, in_c1, _l0a_layout_zn_flag)

            # when load2d and al1_howo is not aligned by 16, there might be some L1 dirty data
            # if case is nd_input, use set_value to set 0 in l1 to fix the issue
            input_nd_flag = param_dict.get("input_nd_flag")
            if input_nd_flag:
                al1 = param_dict.get("al1")
                al1_howo = param_dict.get("al1_howo")
                # emit al1 load2d zero with set_2d, pad zero in m
                al1_load2d_zero = param_dict.get("al1_load2d_zero")
                _, _, al1_zero_m_idx, _ = range(len(al1_load2d_zero.shape))
                al1_zero_m_select_condition = tvm.any(tvm.all(al1_load2d_zero.op.axis[al1_zero_m_idx].var >= al1_howo))
                sch[al1_load2d_zero].set_store_predicate(al1_zero_m_select_condition, partition=True)
                sch[al1_load2d_zero].emit_insn(al1_load2d_zero.op.axis[0], "set_2d")
                # emit al1 load2d real with nd2nz, pad zero in k
                al1_load2d_real = param_dict.get("al1_load2d_real")
                _, _, al1_real_m_idx, _ = range(len(al1_load2d_real.shape))
                al1_real_m_select_condition = tvm.any(tvm.all(al1_load2d_real.op.axis[al1_real_m_idx].var < al1_howo))
                sch[al1_load2d_real].set_store_predicate(al1_real_m_select_condition, partition=True)
                sch[al1_load2d_real].emit_insn(al1_load2d_real.op.axis[1], "dma_copy",
                    {"layout_transform": "nd2nz", "split_select": 1, "no_select2if": 1, "force_nd2nz": 1})
                # al1 reuse buffer with al1_load2d_zero and al1_load2d_real
                sch[al1].reused_by(al1_load2d_real, al1_load2d_zero)

    def align_al1_load2d(self, sch, al1):
        """
        al1 M align.
        """
        if self.flag:
            sch[al1].storage_align(al1.op.axis[1], 256, 0)


class C04Opti:
    """
    Class of C04 optimization.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.c04_flag
        self.dma_c04_flag = conv_param.dma_c04_flag
        self.conv1d_c04_flag = self.flag and conv_param.conv1d_split_w_flag
        self.dynamic_flag = conv_param.dynamic_flag

    def align_al1_c04(self, sch, al1):
        if self.conv1d_c04_flag:
            return

        if self.dynamic_flag:
            # hi and wi are grouped in dynamic c04 scene.
            # wi axis needs load enough data(size of fmap_width) from out to l1 to keep continuous.
            # compute_align keeps wi axis data in L1 fmap_width-aligned.
            group_idx, n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx = range(len(al1.shape))
            sch[al1].compute_align(al1.op.axis[wi_idx], al1.shape[wi_idx])
            return

        # width direction buffer align
        al1_c04_buffer_align_list = []
        for _ in range(len(al1.shape)):
            al1_c04_buffer_align_list.append((1, 1))
        n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx = range(len(al1.shape))
        al1_c04_buffer_align_list[wi_idx] = (al1.shape[wi_idx], al1.shape[wi_idx])
        sch[al1].buffer_align(*al1_c04_buffer_align_list)

    def pragma_al1_axis_group_c04(self, sch, al1):
        if (not self.dma_c04_flag) or (not self.dynamic_flag):
            return

        # tbe pass infer hi and wi axis are not continuous.
        # Apply axis_group to fix out->l1 destination address not aligned.
        merge_axis_group_id = tvm.call_extern("int32", "axis_group", 0, "overwrite")
        group_idx, n_idx, ci1_idx, hi_idx, wi_idx, ci0_idx = range(len(al1.shape))
        sch[al1].pragma(al1.op.axis[hi_idx], "axis_group", merge_axis_group_id)
        sch[al1].pragma(al1.op.axis[wi_idx], "axis_group", merge_axis_group_id)


class StridehOpti:
    """
    class of stride_h optimization when kernel_h = 1 and stride_h > 1.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.strideh_opti_flag
        self.stride_h_update = 1 if self.flag else conv_param.stride_h


class Conv1dSplitw:
    """
    Class of Conv1d.
    """
    def __init__(self, conv_param):
        self.flag = conv_param.conv1d_split_w_flag or conv_param.split_w_flag
        self.conv_param = conv_param

    def align_row_major_conv1d(self, sch, fmap_row_major, block_k0):
        """
        Buffer align row major in Conv1d.
        """
        axis_buffer_align_list = []
        for i in range(len(fmap_row_major.shape)):
            axis_buffer_align_list.append((1, 1))
        k0_align_value = C04_C0 if self.conv_param.c04_flag else block_k0
        axis_buffer_align_list[-1] = (k0_align_value, k0_align_value)
        if self.conv_param.split_w_flag:
            fmap_row_major_buffer_align_list = []
            _, _, ho_idx, _, _, kernel_h_idx, kernel_w_idx, _ = range(len(fmap_row_major.shape))
            for i in range(len(fmap_row_major.shape)):
                fmap_row_major_buffer_align_list.append((1, 1))
            fmap_row_major_buffer_align_list[kernel_h_idx] = (1, self.conv_param.filter_h)
            fmap_row_major_buffer_align_list[kernel_w_idx] = (1, self.conv_param.filter_w)
        sch[fmap_row_major].buffer_align(*axis_buffer_align_list)


class Conv2dSchedule:
    """
    Class of Conv2d Schedule.
    """
    def __init__(self, sch, res, spec_node_list, conv_param, op_graph, tiling_dict_flag, tiling_case, var_range):
        self._sch = sch
        self._res = res
        self._conv_param = conv_param
        self._op_graph = op_graph

        self._tensor_map = conv_param.tensor_map
        self._dim_map = conv_param.dim_map
        self._para_dict = conv_param.para_dict

        # dtype
        self._fmap_dtype = self._tensor_map["fmap"].dtype
        self._weight_dtype = self._tensor_map["filter"].dtype
        self._res_dtype = res.dtype

        # frac unit size
        self._block_m0, self._block_k0, self._block_n0 = CUBE_MKN[self._weight_dtype]["mac"]

        # conv2d params
        self._out_height = conv_param.h_out
        self._out_width = conv_param.w_out
        self._out_hw = self._out_height*self._out_width
        self._out_with_aligned = conv_param.out_width_aligned

        self._group_opt = self._para_dict["group_opt"]
        self._ci1_opt = self._para_dict["c1_opt"]
        self._co1_opt = self._para_dict["cout1_opt"]
        self._batch, self._in_c1, self._in_height, self._in_width, self._in_c0 = self._para_dict["a_shape"]
        self._quant_fusion_muti_groups_in_cl0 = self._co1_opt % 2 == 1 and self._group_opt > 1 and res.dtype == "int8"
        self._bias_tensor = self._para_dict["bias_tensor"]

        self._kernel_h = conv_param.filter_h
        self._kernel_w = conv_param.filter_w
        self._stride_h = conv_param.stride_h
        self._stride_w = conv_param.stride_w
        self._dilate_h = conv_param.dilate_h
        self._dilate_w = conv_param.dilate_w
        self._filter_w_dilation = conv_param.filter_w_dilation
        self._filter_h_dilation = conv_param.filter_h_dilation

        # flags
        self._bias_flag = self._bias_tensor is not None
        self._sparse_4to2_flag = conv_param.sparse_4to2_flag
        self._split_w_flag = conv_param.split_w_flag
        self._tiling_splitw_flag_for_conv1d = conv_param.tiling_splitw_flag_for_conv1d
        self._v200_width_out_1_flag = conv_param.v200_width_out_1_flag
        self._groupopt_flag = conv_param.groupopt_flag
        self._tiny_weight_fractal_flag = conv_param.get_tiny_weight_fractal_flag()
        self._l0a_layout_zn_flag = is_l0a_layout_zn()
        self._bias_set_zero_in_ub_flag = conv_param.tiling_query_param.get("bias_not_support_nd2nz_set_zero_flag")

        # bias dtype
        if self._bias_flag:
            self._bias_dtype = self._para_dict["bias_tensor"].dtype

        # self._tiling params
        self._tiling_query_param = conv_param.tiling_query_param
        self._tiling = {}
        self._fusion_type = op_graph.fusion_type
        self._fusion_type_str = op_graph.fusion_type_str

        # device params
        self._core_num = get_soc_spec("CORE_NUM")

        # mixl2 workspace_tensor
        self.workspace_tensor = []

        #====================create feature instance=============================
        self._fixpipe_fusion = FixpipeFusion(sch, res, op_graph, conv_param)
        self._mixl2_flag = self._fixpipe_fusion.get_mix_l2_flag()

        # ub fusion
        self._fixpipe_res_list, self._fixpipe_res_gm2ub_list = self._fixpipe_fusion.get_fixpipe_res_list()
        self._pre_op_list, self._next_op_list = self._fixpipe_fusion.get_pre_next_op_list()
        self._eltwise_ub_fusion = EltwiseUBFusion(self._sch, res, conv_param, self._mixl2_flag,
                                                  self._fixpipe_res_list, self._fixpipe_res_gm2ub_list,
                                                  self._pre_op_list, self._next_op_list)
        self._quant_fusion = QuantFusion(res, op_graph)
        self._input_nd2nz = InputNd2Nz(conv_param)
        self._weight_nd2nz = WeightNd2Nz(conv_param)
        self._output_nz2nd = OutputNz2Nd(res)
        self._lx_fusion = LxFusion(conv_param)
        self._aipp_fusion = AippFusion(conv_param)
        self._dynamic_shape = DynamicShape(conv_param, self._weight_dtype, tiling_dict_flag,
                                           tiling_case, var_range)
        self._strided_read = StridedRead(conv_param)
        self._strided_write = StridedWrite(res)
        self._im2col_dma = Im2colDma(conv_param, self._block_k0)
        self._inner_batch = InnerBatch()
        self._l0a_load2d = L0aLoad2d(conv_param)
        self._strideh_opti = StridehOpti(conv_param)
        self._c04 = C04Opti(conv_param)
        self._conv1d = Conv1dSplitw(conv_param)
        self._pooling_fusion = MaxpoolFusion(res, MaxPoolParam)
        self._convbn1 = Conv2dBN1Fusion(conv_param, self._fmap_dtype, op_graph,
                                        self._fixpipe_res_list[0])
        self._support_l0c_to_ub_flag = (tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub") or
                                        tbe_platform.intrinsic_check_support("Intrinsic_data_move_l0c2ub"))
        self._quant_conv2d_schedule = QuantConv2dSchedule(self._sch, self._tensor_map)
        self._wino_conv = WinogradConv(conv_param, res)

        # multi_out specify
        if len(spec_node_list) > 1:
            self._multi_out = spec_node_list[:-1]
        else:
            self._multi_out = None

        if self._aipp_fusion.flag:
            self._fmap_dtype = self._weight_dtype

        #====================special flag on specific soc========================
        # Ascend310P3 and Ascend910A and Ascend910B soc, random point is not zero, cause mad overflow.
        # We need to select the real points on k axis which join the calculation.
        # Otherwise, random data will be involved.
        self._al0_cl0_k_axis_select_flag = self._groupopt_flag and \
            not self._conv_param.l0a_load2d_flag and \
            not self._conv_param.l0a_dma_flag and \
            self._conv_param.binary_mode
        
    def fetch_info_dict(self, tiling_case):
        """
        Fetch the info_dict to get tiling.
        """
        def cal_single_op_cub_coeff():
            """
            Get ub coefficient (of cc_to_ub mad tensor “cub” and its succeeded tensor(s) on UB) in single op situation.
            """
            cub_coeff_single_op = 0
            coeff_unit = 0
            if is_support_fixpipe():  # There is no cc_to_ub mad result tensor.
                return cub_coeff_single_op, coeff_unit

            cub_tensor_dtype = self._tensor_map.get(Conv2dTensorName.CUB, "float16")
            coeff_unit = UB_COEFF_CONVERT.get(cub_tensor_dtype, 1)
            cub_coeff_single_op += coeff_unit  # Add tensor "cub"
            if self._v200_width_out_1_flag:
                # Tensor remove_padded_column and its succeeded tensor size should be half of "cub" size.
                coeff_unit *= ConstValue.ONE_HALF
                cub_coeff_single_op += coeff_unit  # Add tensor "remove_padded_column"
            if self._bias_flag:
                cub_coeff_single_op += coeff_unit  # Add tensor "bias_add"

            return cub_coeff_single_op, coeff_unit

        def cal_cub_coeff():
            """
            get cub space coefficient to get tiling.
            """
            cub_coeff, coeff_unit = cal_single_op_cub_coeff()
            cub_coeff += self._output_nz2nd.cal_nz2nd_cub_coeff(coeff_unit)
            # Calculate post elementwise fusion coefficient
            cub_eltwise_coeff, channelwise_coeff, scalar_num = self._eltwise_ub_fusion.coeff_eltwise_cal()
            cub_eltwise_coeff += self._quant_fusion.cal_quant_coeff()
            cub_eltwise_coeff += self._pooling_fusion.cal_pooling_coeff()

            cub_coeff += cub_eltwise_coeff

            return cub_coeff, channelwise_coeff, scalar_num

        def cal_aub_coeff():
            """
            get aub space coefficient for get tiling.
            """
            aub_coeff = 0
            if self._input_nd2nz.mode == "NCHW":
                # keep same with static. [Tiling Team] do not add one.
                aub_coeff += ConstValue.PRE_TRANSDATA_TENSORS_SPACE_OCCUPY_NUM
            return aub_coeff

        def cal_fusion_vector_utilize():
            """
            get pre_vec_util, post_vec_util for fast tiling.
            """
            pre_vec_util, post_vec_util = 0, 0
            post_vec_util += self._eltwise_ub_fusion.calc_ub_vector_utilize(self._res)
            if self._input_nd2nz.mode == "NCHW":
                pre_vec_util += 2
            if self._output_nz2nd.mode == "NCHW":
                post_vec_util += 1
            return pre_vec_util, post_vec_util

        def get_info_dict_cdtype(c_dtype):
            """
            get info_dict cdtype
            """
            if self._convbn1.flag:
                return self._convbn1.get_info_dict_cdtype()
            return c_dtype

        def get_info_dict_fusion_type(fusion_type_str):
            """
            get info_dict fusion_type_str
            """
            if self._convbn1.flag:
                return self._convbn1.fusion_type_str
            return fusion_type_str

        def get_out_c_memory_type():
            """
            get Cout destination
            """
            out_c_memory_type = ["UB"] if self._eltwise_ub_fusion.flag else ["OUT"]
            return out_c_memory_type
        
        def get_fixpipe_info():
            """
            get fixpipe_type_list and fixpipe_fused_coeff to get tiling.
            """
            if not is_support_fixpipe():
                return [{}], [0, 0]

            fixpipe_type_list = self._fixpipe_fusion.fixpipe_fused_type
            fixpipe_fused_coeff = [self._fixpipe_fusion.fixpipe_channel_coeff,
                                   self._fixpipe_fusion.fixpipe_eltwise_coeff]

            return fixpipe_type_list, fixpipe_fused_coeff

        def get_performance_mode():
            """
            get performance mode
            """
            # fp16/int8 mode. default is high_precision
            if self._fmap_dtype != "float32":
                return False, True

            # fp32 mode. default is high_performance
            enable_hi_float_32_execution = True
            high_precision_mode = False
            if conv_param.impl_mode == "enable_float_32_execution":
                enable_hi_float_32_execution = False
                high_precision_mode = True
            return enable_hi_float_32_execution, high_precision_mode

        if self._dynamic_shape.flag and tiling_case: # pass when tiling_case
            return None

        tiling_query_param = self._tiling_query_param
        conv_param = self._conv_param

        fmap_shape_nc1hwc0 = list(tiling_query_param["fmap_shape_nc1hwc0"])
        shape_w_nc1hwc0 = list(tiling_query_param["shape_w_nc1hwc0"])
        result_shape = tiling_query_param["c_shape"]
        mad_dtype = tiling_query_param["mad_dtype"]
        bias_flag = tiling_query_param["bias_flag"]
        bias_dtype = self._bias_dtype if bias_flag else ""

        cub_eltwise_coeff, channelwise_coeff, scalar_num = cal_cub_coeff()

        aub_coeff = cal_aub_coeff()
        pooling_shape, pooling_stride = self._pooling_fusion.config_window_stride()
        result_dtype = get_info_dict_cdtype(self._res_dtype)
        fusion_type_str = get_info_dict_fusion_type(self._fusion_type_str)

        # ============fusion_type_str============
        if self._fusion_type_str == "0":
            log.warn("WANRNING: Conv2d fusion_type is 0!")

        out_c_memory_type = get_out_c_memory_type()
        enable_hi_float_32_execution, high_precision_mode = get_performance_mode()
        fixpipe_type_list, fixpipe_fused_coeff = get_fixpipe_info()

        # group conv, send one group_opt a, b, c shape to tiling
        info_dict = {"op_type": 'conv2d',
                     "fm_shape": fmap_shape_nc1hwc0,
                     "filter_shape": shape_w_nc1hwc0,
                     "result_shape": result_shape,
                     "output_num": 1,
                     "fm_dtype": self._fmap_dtype,
                     "filter_dtype": self._weight_dtype,
                     "result_dtype": result_dtype,
                     "mad_dtype": mad_dtype,
                     "pad": [conv_param.pad_w[0], conv_param.pad_w[1],
                             conv_param.pad_h[0], conv_param.pad_h[1]],
                     "stride": [self._stride_h, self._stride_w],
                     "dilation": [self._dilate_h, self._dilate_w],
                     "group": self._group_opt,
                     "bias_flag": bias_flag,
                     "bias_dtype": bias_dtype,
                     "in_fm_memory_type": self._lx_fusion.input_memory_type,
                     "out_fm_memory_type": self._lx_fusion.output_memory_type,
                     "out_c_memory_type": out_c_memory_type,
                     "l1_fusion_type": self._lx_fusion.l1_fusion_type,
                     "fm_l1_valid_size": self._lx_fusion.fmap_l1_valid_size,
                     "fusion_type": fusion_type_str,
                     "kernel_name": conv_param.kernel_name,
                     "special_mode": {"use_c04_mode": 1 if self._c04.flag else 0,
                                      # disable strideh opti when input nd2nz.
                                      "input_nd_flag": self._input_nd2nz.flag,
                                      "scalar_num": scalar_num,
                                      "high_performance_mode": enable_hi_float_32_execution,
                                      "high_precision_mode": high_precision_mode,
                                      "l0a_dma_flag": self._im2col_dma.flag,
                                      "split_w_flag": self._split_w_flag or self._tiling_splitw_flag_for_conv1d,
                                      "4To2_structured_sparsity": self._sparse_4to2_flag,
                                      "winograd_conv_flag": self._wino_conv.flag,
                                      "winograd_mc_multi_wo_flag": self._wino_conv.mc_multi_wo_flag
                                     },
                     "fixpipe_fused_type": fixpipe_type_list,
                     "fixpipe_fused_coefficient": fixpipe_fused_coeff,
                     "fused_coefficient": [aub_coeff, 0, cub_eltwise_coeff],
                     "fused_channel_wise": [0, 0, channelwise_coeff],
                     "pooling_shape": pooling_shape,
                     "pooling_stride": pooling_stride,
                    }
        if self._dynamic_shape.flag:
            pre_vec_util, post_vec_util = cal_fusion_vector_utilize()
            info_dict.update({"fusion_vector_utilize": [pre_vec_util, post_vec_util]})
        else:
            log.debug("[{}][OriginDtype]fmap_dtype:{} filter_dtype:{} bias_dtype:{} output_dtype:{}".format(
                self._conv_param.kernel_name, info_dict.get("fm_dtype"), info_dict.get("filter_dtype"),
                info_dict.get("bias_dtype"), info_dict.get("result_dtype")))
            # bfloat16 reuse float16 repo, both use float16 for tiling
            for key in ('fm_dtype', 'filter_dtype', 'result_dtype'):
                if info_dict.get(key) == "bfloat16":
                    info_dict[key] = "float16"

        if self._bias_set_zero_in_ub_flag:
            info_dict.get("special_mode").update({"bias_set_zero_in_ub_flag": True})

        return info_dict

    def update_fusion_buildcfg(self):
        """
        handle speciel build config requirements for fusion
        """
        build_config = dict()
        # handel split_w buffer tile speical buildcfg
        if self._split_w_flag:
            build_config["constant_realize_extent_in_infer_bound"] = False

        if self._dynamic_shape.flag and check_support_db_fold():
            build_config["enable_db_fold"] = True

        set_fusion_buildcfg("conv2d", build_config)

    def fetch_tiling(self, info_dict, tiling_case):
        """
        Fetch tiling info.
        """
        def check_support_op_tiling_for_static_op():
            """
            check whether support using op tiling for static operation
            """
            # op_tiling not support depthwise
            special_scene_flag = self._im2col_dma.flag or self._l0a_load2d.flag or self._strideh_opti.flag or \
                self._c04.flag or self._sparse_4to2_flag or self._split_w_flag 
            # op_tiling not support int8 int4 dtype
            special_dtype_flag = (self._fmap_dtype in ("int8", "int4"))
            if self._bias_flag:
                special_dtype_flag = special_dtype_flag or \
                                     (self._fmap_dtype == "float16" and self._bias_dtype == "float32")
            # op tiling only normal single operator for load3d secne on platform of support fixpipe
            support_op_tiling_for_static_op = False if special_scene_flag else True
            support_op_tiling_for_static_op = support_op_tiling_for_static_op and (not special_dtype_flag)
            support_op_tiling_for_static_op = support_op_tiling_for_static_op and is_support_fixpipe()

            return support_op_tiling_for_static_op

        def get_default_tiling():
            """
            Set default tiling when fetch tiling failed.
            """
            default_tiling = {'AL0_matrix': [1, 1, 1, 1, 1, 1, 0], 'AL1_shape': [1, 1, 1, 1, 1, 1, 0],
                              'AUB_shape': None,
                              'BL0_matrix': [1, 1, 1, 1, 1, 1, 0], 'BL1_shape': [1, 1, 1, 1, 1, 1, 0],
                              'BUB_shape': None,
                              'CL0_matrix': [1, 1, 1, 1, 1, 1, 0], 'INPUT_L1_sparse_index': None,
                              'L0C_OUTPUT_matrix': [1, 1, 1, 1, 1, 1], 'UB_channel_wise_input': [True, True, True],
                              'block_dim': [1, 1, 1, 1, 0], "control_reorder_flag": 0,
                              'manual_pingpong_buffer': {'AL0_pbuffer': 1, 'AL1_pbuffer': 1, 'AUB_pbuffer': 1,
                                                         'BL0_pbuffer': 1, 'BL1_pbuffer': 1, 'BUB_pbuffer': 1,
                                                         'CL0_pbuffer': 1, 'INPUT_L1_BT_pbuffer': 1,
                                                         'INPUT_L1_FB_pbuffer': 1, 'INPUT_L1_eltwise_pbuffer': 1,
                                                         'L0C_OUTPUT_pbuffer': 1, 'UBG_pbuffer': 1},
                              'INPUT_L1_BT_param': "slice", 'INPUT_L1_FB_param': "slice",
                              'INPUT_L1_eltwise_param': "slice", 'special_optimize_flag': 0,
                              'tbe_compile_para': 0, 'vector_block_num': 1}
            tiling_m = 1
            if self._wino_conv.mc_multi_wo_flag:
                tiling_m = ceil_div(self._wino_conv.wino_res_wo, 16)
            tiling_ka = 1
            tiling_kb = tiling_ka
            if self._sparse_4to2_flag:
                if self._kernel_h * self._kernel_w * self._in_c1 > 1:
                    tiling_ka = 2
                    tiling_kb = 1

            tiling_n = 1
            if self._res_dtype == "int8":
                tiling_n = 2  # load 2 * INT8 to fit the size of c0
            elif self._res_dtype == "int4":
                tiling_n = 4  # load 4 * INT4 to fit the size of c0
            if self._fixpipe_fusion.get_antiquant_flag():
                fixpipe_info = self._fixpipe_fusion.fixpipe_info_dict.get(self._fixpipe_res_list[0])
                fixpipe_x2 = fixpipe_info.get("cache_read_tensors_elewise")[0]
                if fixpipe_x2.dtype == "int8":
                    tiling_n = 2
                elif fixpipe_x2.dtype == "int4":
                    tiling_n = 4

            group_cl0 = 1
            if self._quant_fusion_muti_groups_in_cl0:
                tiling_n = self._co1_opt
                group_cl0 = 2

            default_tiling["AL0_matrix"] = [tiling_m, tiling_ka, 16, self._in_c0, 1, 1, 0]
            default_tiling["BL0_matrix"] = [tiling_kb, tiling_n, 16, self._in_c0, 1, 1, 0]
            default_tiling["CL0_matrix"] = [tiling_n, tiling_m, 16, 16, 1, group_cl0, 0]
            default_tiling["L0C_OUTPUT_matrix"] = default_tiling.get("CL0_matrix")[:-1]

            min_al1_ci1 = get_min_al1_ci1(self._sparse_4to2_flag, self._in_c1, self._kernel_h, self._kernel_w)
            kal1 = max(min_al1_ci1, ceil_div(tiling_ka, self._kernel_h * self._kernel_w)) * \
                   self._filter_h_dilation * self._filter_w_dilation * self._in_c0
            default_tiling["AL1_shape"] = [kal1, 1, 1, 1, 0]
            default_tiling["BL1_shape"] = None

            _fused_coeff = info_dict.get("fused_coefficient")
            if _fused_coeff[0] > 0:
                default_tiling["AUB_shape"] = default_tiling.get("AL1_shape")
            else:
                default_tiling["AUB_shape"] = None

            if self._sparse_4to2_flag or self._wino_conv.flag:
                kbl1 = self._kernel_h * self._kernel_w * self._in_c0
                default_tiling["BL1_shape"] = [kbl1, 1, 1, 1, 0]

            if self._batch > 1 and self._core_num > 1:
                if self._batch <= self._core_num:
                    default_tiling.get("block_dim")[0] = self._batch
                else:
                    for i in range(self._core_num, 0, -1):
                        if self._batch % i == 0:
                            break
                    default_tiling.get("block_dim")[0] = i

            default_tiling = self._lx_fusion.config_default_tiling(default_tiling)

            return default_tiling

        def get_v220_tiling(info_dict):
            """
            Get tiling in v220 situation.
            """
            def change_single_op(op_tiling_param_dict, info_dict, para_dict):
                kernel = op_tiling_param_dict["inputs"][1]
                fmap = op_tiling_param_dict["inputs"][0]
                fmap_ori_shape = fmap["ori_shape"]
                fmap_nchw_shape = [fmap_ori_shape[0], fmap_ori_shape[3], fmap_ori_shape[1], fmap_ori_shape[2]]
                fmap["format"] = "NC1HWC0"
                fmap["shape"] = info_dict["fm_shape"]
                fmap["ori_format"] = "NCHW"
                fmap["ori_shape"] = fmap_nchw_shape
                kernel["ori_format"] = "NCHW"
                kernel["ori_shape"] = para_dict["weight_ori_shape_nchw"]
                strides = op_tiling_param_dict["attrs"][0]["value"]
                dilation = op_tiling_param_dict["attrs"][2]["value"]
                # pad 
                strides = (strides[0], strides[1], strides[3], strides[2])
                dilation = (dilation[0], dilation[1], dilation[3], dilation[2])

            def rebuild_op_tiling_param_dict(op_type, op_tiling_param_dict,
                                             info_dict, support_op_tiling_for_static_op):
                para_dict = self._conv_param.para_dict
                dim_map = self._conv_param.dim_map
                fmap_nchw_shape = dim_map["fmap_ori_nchw_shape"]
                fmap_ori_shape = [fmap_nchw_shape[0], fmap_nchw_shape[2],
                                  fmap_nchw_shape[3], fmap_nchw_shape[1]]
                output = op_tiling_param_dict["outputs"][0]
                if op_type == "Conv2D" and (fmap_ori_shape, output["ori_shape"]) in \
                        STATIC_FAST_TILING_WHITE_LIST:
                    change_single_op(op_tiling_param_dict, info_dict, self._conv_param.para_dict)
                elif op_type == "FixPipe" and op_tiling_param_dict["attrs"][0]["value"] == ["TransData"] \
                    and (fmap_ori_shape, output["ori_shape"]) in \
                        STATIC_FAST_TILING_WHITE_LIST:

                    strides = (1, 1, para_dict["stride_h"], para_dict["stride_w"])
                    pads = (*para_dict["pad_h"], *para_dict["pad_w"])
                    dilations = (1, 1, para_dict["dilate_h"], para_dict["dilate_w"])
                    new_fmap = {"shape": info_dict["fm_shape"], "ori_shape": fmap_nchw_shape, "format": "NC1HWC0",
                            "sub_format": output["sub_format"], "ori_format": "NCHW", "dtype": output["dtype"],
                            "addr_type": output["addr_type"], "total_shape": output["total_shape"],
                            "slice_offset": output["slice_offset"], "L1_addr_offset": output["L1_addr_offset"],
                            "L1_fusion_type": output["L1_fusion_type"],
                            "L1_workspace_size": output["L1_workspace_size"],
                            "valid_shape": output["valid_shape"], "split_index": output["split_index"],
                            "atomic_type": output["atomic_type"], "input_c_values": 50}
                    new_weight = {"shape": para_dict["weight_fracz_shape"],
                            "ori_shape": para_dict["weight_ori_shape_nchw"], "format": "FRACTAL_Z",
                            "sub_format": output["sub_format"], "ori_format": "NCHW", "dtype": output["dtype"],
                            "addr_type": output["addr_type"], "total_shape": output["total_shape"],
                            "slice_offset": output["slice_offset"],
                            "L1_addr_offset": output["L1_addr_offset"],
                            "L1_fusion_type": output["L1_fusion_type"],
                            "L1_workspace_size": output["L1_workspace_size"],
                            "valid_shape": output["valid_shape"], "split_index": output["split_index"],
                            "atomic_type": output["atomic_type"], "input_c_values": 50}
                    new_outputs = {"shape": para_dict["output_shape"], "ori_shape": para_dict["output_shape"],
                            "format": "NHWC", "sub_format": output["sub_format"], "ori_format": output["ori_format"],
                            "dtype": output["dtype"], "addr_type": output["addr_type"],
                            "total_shape": para_dict["output_shape"], "slice_offset": output["slice_offset"],
                            "L1_addr_offset": output["L1_addr_offset"], "L1_fusion_type": output["L1_fusion_type"],
                            "L1_workspace_size": output["L1_workspace_size"], "valid_shape": output["valid_shape"],
                            "split_index": output["split_index"], "atomic_type": output["atomic_type"],
                            "input_c_values": output["input_c_values"]}
                    attrs = [{"name": "strides", "dtype": "list_int64", "value": strides},
                            {"name": "pads", "dtype": "list_int64", "value": pads},
                            {"name": "dilations", "dtype": "list_int64", "value": dilations},
                            {"name": "groups", "dtype": "int64", "value": info_dict["group"], "is_default_value": True},
                            {"name": "data_format", "dtype": "str", "value": output["ori_format"]},
                            {"name": "offset_x", "dtype": "int64", "value": 0, "is_default_value": True}, ]
                    op_tiling_param_dict["fast_tiling"] = (new_fmap, new_weight, new_outputs, attrs)
                else:
                    support_op_tiling_for_static_op[0] = False

            add_tiling_compile_info(info_dict, self._conv_param, static_op_flag=True)
            support_op_tiling_for_static_op = check_support_op_tiling_for_static_op()
            support_op_tiling_for_static_op_dic = [support_op_tiling_for_static_op]

            op_tiling_param_dict = dict()
            info_dict["extra_param"] = dict()
            graph_op_info = get_context().get_graph_op_info()
            if graph_op_info is not None:
                compile_info = get_compile_info()
                # graph_op_info's op_type record original op type, include DepthWise, SpaceToDepth eg
                op_tiling_param_dict["op_type"] = graph_op_info.op_type
                op_tiling_param_dict["inputs"] = graph_op_info.inputs
                op_tiling_param_dict["outputs"] = graph_op_info.outputs
                op_tiling_param_dict["attrs"] = graph_op_info.attrs
                # static op tiling cannot contain private attr, which will cause padding update(should delete)
                if op_tiling_param_dict["op_type"] == "Conv2D" and len(op_tiling_param_dict["attrs"]) > ATTRLEN_CONV2D:
                    op_tiling_param_dict["attrs"] = op_tiling_param_dict["attrs"][0 : ATTRLEN_CONV2D]

                fm_ori_format = None
                if isinstance(op_tiling_param_dict.get("inputs")[0], dict):
                    fm_ori_format = op_tiling_param_dict.get("inputs")[0].get('ori_format')
                # when fm format is nhwc change optiling_info to use fast tiling
                if support_op_tiling_for_static_op_dic[0] and fm_ori_format == "NHWC":
                    rebuild_op_tiling_param_dict(op_tiling_param_dict["op_type"], op_tiling_param_dict, info_dict, 
                                                support_op_tiling_for_static_op_dic)
                op_tiling_param_dict["compile_info"] = compile_info
                info_dict["extra_param"]["optiling_info"] = op_tiling_param_dict
                support_op_tiling_for_static_op_dic[0] = support_op_tiling_for_static_op_dic[0] and \
                (fm_ori_format == "NHWC" or self._conv_param.single_op_flag) and \
                op_tiling_param_dict["compile_info"] is not None
            else:
                info_dict["extra_param"]["optiling_info"] = None

            if support_op_tiling_for_static_op_dic[0]:
                info_dict["support_op_tiling_flag"] = True

            tiling = tiling_api.get_tiling(info_dict)
            if tiling is None or tiling["AL0_matrix"][2] == 32:
                log.warn("get invalid tiling, default tiling will be used")
                tiling = get_default_tiling()
            return tiling

        def tiling_adapter(tiling):
            """
            tiling_adapter for v220/v300 new tiling.
            """
            tiling_new = tiling.copy()

            remove_d_items = ["AL1_shape", "BL1_shape", "AL0_matrix", "BL0_matrix", "CL0_matrix", "block_dim"]
            for item in remove_d_items:
                if tiling_new[item]:
                    tiling_new[item] = tiling[item][: -1]

            tiling_new["AUB_channel_wise_flag"] = tiling["UB_channel_wise_input"][0]
            tiling_new["BUB_channel_wise_flag"] = tiling["UB_channel_wise_input"][1]
            tiling_new["CUB_channel_wise_flag"] = tiling["UB_channel_wise_input"][2]
            tiling_new["CUB_matrix"] = tiling["L0C_OUTPUT_matrix"]
            tiling_new["A_overhead_opt_flag"] = 0
            tiling_new["B_overhead_opt_flag"] = 0

            # change special_opt[2^n] to overhead_opt[0 or 1]
            special_opt_map = {0: "A_overhead_opt_flag",
                               1: "B_overhead_opt_flag"}

            special_opt = tiling["special_optimize_flag"]
            # 32 bits for integer
            special_opt_bit_len = INTEGER_BITS_SIZE
            bin_special_opt = (bin(special_opt)[BINARY_HEAD_0B:].rjust(special_opt_bit_len, '0'))[::-1]
            bin_mask = bin(0)[BINARY_HEAD_0B:].rjust(special_opt_bit_len, '0')
            special_opt_list = [ord(opt) ^ ord(mask) for opt, mask in zip(bin_special_opt, bin_mask)]

            for key in special_opt_map:
                tiling_new[special_opt_map.get(key)] = special_opt_list[key]

            return tiling_new

        if self._dynamic_shape.flag and tiling_case:
            self._tiling = self._dynamic_shape.fetch_tiling_case()
            self._tiling = self._tiling if self._dynamic_shape.flag else tiling_adapter(self._tiling)
        else:
            self._tiling = get_v220_tiling(info_dict)
            self._tiling = tiling_adapter(self._tiling)

    def verify_tiling(self):
        """
        Verify whether the tiling returned is legal.
        """
        def check_l0_tiling():
            """
            Check al0 tiling and bl0 tiling.
            """
            if ma_al0 != mc_cl0:
                err_man.raise_err_equal_invalid("conv2d", "ma", "mc")

            if tiling["BL0_matrix"]:
                kb_bl0, nb_bl0, _, _, _, _ = tiling["BL0_matrix"]
                if not self._sparse_4to2_flag and ka_al0 != kb_bl0:
                    err_man.raise_err_equal_invalid("conv2d", "ka", "kb")
                if nb_bl0 != nc_cl0:
                    err_man.raise_err_equal_invalid("conv2d", "nb", "nc")

        def check_overhead_opt_flag():
            """
            Check the overhead_opt_flag.
            """
            if group_cl0 > 1 and tiling["A_overhead_opt_flag"]:
                err_man.raise_err_value_or_format_invalid(
                    "conv2d", 'tiling["A_overhead_opt_flag"]', "False", "when multi_cl0_group.")

        def modify_bl0_tiling():
            """
            Modify bl0 tiling in certain circumstances.
            """
            filter_matrix = list(self._dim_map["filter_matrix_dim"]) # [k1, n1, n0, k0]
            filter_matrix[1] = (filter_matrix[1] + n_dim - 1) // n_dim
            if self._tiling["BL0_matrix"]:
                if self._tiling["BL0_matrix"][0: 4] == filter_matrix and group_bl0 == 1:
                    self._tiling["BL0_matrix"] = []

        def modify_bl1_tiling():
            """
            Modify bl1 tiling in certain circumstances.
            """
            if self._tiling["BL0_matrix"] == []:
                if self._sparse_4to2_flag or self._wino_conv.flag:
                    self._tiling["BL1_shape"] = []
                    return
                if ceil_div(self._group_opt, group_dim) == 1:
                    self._tiling["BL1_shape"] = None
                elif ceil_div(self._group_opt, group_dim) > 1 and self._tiling["BL1_shape"] is not None:
                    self._tiling["BL1_shape"] = []
                return

        def check_cub_tiling():
            """
            check cub tiling in no ub fusion situation.
            """
            if not self._eltwise_ub_fusion.flag and tiling["CUB_matrix"] != tiling["CL0_matrix"]:
                err_man.raise_err_specific("conv2d", "CUB_matrix must be equal to CL0_matrix in no ub fusion cases!")

        def check_sparse_4to2_tiling():
            if not self._sparse_4to2_flag:
                return

            if self._tiling["BL1_shape"] is None:
                err_man.raise_err_message_cube("BL1_matrix cannot be None in sparse 4to2 scene.")

            ka = self._tiling["AL0_matrix"][1]
            n, c1, kh, kw, c0 = self._conv_param.dim_map["weight_tiling_b_shape"]
            kb_full = c1 * kh * kw
            kb = kb_full if self._tiling["BL0_matrix"] == [] else self._tiling["BL0_matrix"][0]
            if ka == kb and kb == kb_full:
                return

            if self._ci1_opt * self._kernel_h * self._kernel_w != 1 and ka != 2 * kb:
                err_man.raise_err_message_cube("ka should be 2kb in sparse 4to2.")

        if self._dynamic_shape.flag:
            return

        tiling = self._tiling
        bl0_tiling = tiling["BL0_matrix"]
        ma_al0, ka_al0, _, _, _, _ = tiling["AL0_matrix"]
        nc_cl0, mc_cl0, _, _, batch_cl0, group_cl0 = tiling["CL0_matrix"]
        if bl0_tiling:
            _, _, _, _, _, group_bl0 = bl0_tiling

        _, n_dim, _, group_dim = tiling["block_dim"]

        self._lx_fusion.check_l1fusion_tiling(tiling)

        self._weight_nd2nz.check_bl1_nd2nz_tiling(tiling)

        self._dynamic_shape.check_dynamic_overhead_opt_flag(tiling, self._dynamic_shape.flag)

        self._inner_batch.set_l0b_innerbatch_flag(tiling, batch_cl0)
        self._inner_batch.check_innerbatch_tiling(tiling, self._batch, self._out_hw)

        check_l0_tiling()
        check_overhead_opt_flag()
        check_cub_tiling()
        check_sparse_4to2_tiling()

        #==================modify tiling to be deleted=========================
        modify_bl0_tiling()
        modify_bl1_tiling()

        tiling = self._pooling_fusion.modify_tiling(
            tiling, self._dim_map["filter_matrix_dim"], self._out_width, self._conv_param, self._block_k0)

    def config_scope(self):
        """
        Config tensor scope.

        Returns
        -------
        tensor_param: dict
            Tensors those set scope.
        """
        def config_cl0():
            """
            Config cl0 scope.
            """
            cl0 = tensor_map[Conv2dTensorName.CL0]
            sch[cl0].set_scope(cce_params.scope_cc)
            return cl0

        def config_fmap_row_major():
            """
            Config row major scope.
            """
            if self._dynamic_shape.flag or self._l0a_load2d.flag or self._im2col_dma.flag or self._wino_conv.flag:
                return None
            fmap_row_major = tensor_map["fmap_row_major"]
            sch[fmap_row_major].set_scope(cce_params.scope_cbuf)
            return fmap_row_major

        def config_al1():
            """
            Config al1 scope.
            """
            scope_al1 = self._lx_fusion.config_al1_scope()
            al1_already_exist_flags = (self._dynamic_shape.flag,
                                       self._l0a_load2d.flag,
                                       self._strideh_opti.flag,
                                       self._input_nd2nz.flag,
                                       self._strided_read.flag,
                                       self._aipp_fusion.flag,
                                       self._c04.dma_c04_flag,
                                       self._im2col_dma.flag)
            for flag in al1_already_exist_flags:
                if flag:
                    if self._im2col_dma.flag:
                        al1 = tensor_map["fmap_im2col"]
                    else:
                        al1 = tensor_map["fmap_l1"]
                    sch[al1].set_scope(scope_al1)
                    return al1

            if self._wino_conv.flag:
                al1 = sch.cache_read(fmap, scope_al1, [al0])
            else:
                al1 = sch.cache_read(fmap, scope_al1, [fmap_row_major])
            return al1

        def config_al1_nd_input():
            """
            Config al1 scope: nd scene has two additional tensor
            """
            if self._input_nd2nz.flag and self._l0a_load2d.flag:
                al1_load2d_real = tensor_map.get("al1_load2d_real")
                al1_load2d_zero = tensor_map.get("al1_load2d_zero")
                if al1_load2d_real is None or al1_load2d_zero is None:
                    err_man.raise_err_specific("conv2d", "load2d nd input tensor not exist.")
                sch[al1_load2d_real].set_scope(cce_params.scope_cbuf)
                sch[al1_load2d_zero].set_scope(cce_params.scope_cbuf)
                return [al1_load2d_real, al1_load2d_zero]
            return [None, None]

        def config_dma():
            """
            Config al1 scope
            """
            al1_zero, al1_dma_im2col, al1_virtual_add = None, None, None
            if self._im2col_dma.flag and is_support_fixpipe():
                al1_zero = tensor_map["fmap_l1_dma_zero"]
                al1_dma_im2col = tensor_map["fmap_l1_dma_im2col"]
                al1_virtual_add = tensor_map["fmap_im2col"]
            return al1_zero, al1_dma_im2col, al1_virtual_add

        def config_al0():
            """
            Config al0 scope.
            """
            if self._im2col_dma.flag:
                return self._im2col_dma.config_al0_im2coldma(sch, al1_im2col, cl0)

            al0 = tensor_map["fmap_im2col"]
            sch[al0].set_scope(cce_params.scope_ca)
            return al0

        def config_bl1():
            """
            Config bl1 scope.
            """
            weight_index_l1 = None
            bl0 = self._conv_param.tensor_map.get(Conv2dTensorName.BL0)

            if self._sparse_4to2_flag:
                weight_index = self._conv_param.tensor_map.get(Conv2dTensorName.WEIGHT_INDEX)
                weight_index_l1 = sch.cache_read(weight_index, cce_params.scope_cbuf, [bl0])

            if self._tiling["BL1_shape"] is None:
                bl1 = None
            elif self._weight_nd2nz.flag:
                bl1 = weight
                sch[bl1].set_scope(cce_params.scope_cbuf)
            else:
                bl1 = sch.cache_read(weight, cce_params.scope_cbuf, [bl0])

            return bl1, weight_index_l1

        def config_bias_l1_bt():
            """
            Config bias scope.
            """
            if self._bias_flag and is_support_fixpipe():
                bias_l1 = tensor_map["bias_l1"]
                sch[bias_l1].set_scope(cce_params.scope_cbuf)
                bias_bt = tensor_map["bias_bt"]
                sch[bias_bt].set_scope(cce_params.scope_bt)
                bias_l1_real = tensor_map.get("bias_l1_real")
                bias_l1_zero = tensor_map.get("bias_l1_zero")
                if self._conv_param.bias_init_align_dim_flag:
                    sch[bias_l1_real].set_scope(cce_params.scope_cbuf)
                    sch[bias_l1_zero].set_scope(cce_params.scope_cbuf)
                return [bias_l1, bias_bt, bias_l1_real, bias_l1_zero]

            return [None, None, None, None]

        def config_bl0():
            """
            Config bl0 scope.
            """
            bl0 = tensor_map[Conv2dTensorName.BL0]
            sch[bl0].set_scope(cce_params.scope_cb)
            return bl0

        def config_bias_ub_l1_bt():
            """
            Config bias scope and bias ub scope.
            """
            if self._bias_flag:
                bias_l1 = tensor_map["bias_l1"]
                sch[bias_l1].set_scope(cce_params.scope_cbuf)
                bias_bt = tensor_map["bias_bt"]
                sch[bias_bt].set_scope(cce_params.scope_bt)
                bias_ub = tensor_map.get("bias_ub")
                sch[bias_ub].set_scope(cce_params.scope_ubuf)
                return [bias_l1, bias_bt, bias_ub]
            return [None, None, None]

        def config_bias_ub():
            """
            Config bias ub scope.
            """
            if self._bias_flag and not is_support_fixpipe():
                bias_ub = tensor_map.get(Conv2dTensorName.BIAS_UB)
                if bias_ub is not None:
                    sch[bias_ub].set_scope(cce_params.scope_ubuf)
                cub_bias_add = tensor_map.get(Conv2dTensorName.CUB_BIAS_ADD)
                if cub_bias_add is not None:
                    sch[cub_bias_add].set_scope(cce_params.scope_ubuf)
                return bias_ub, cub_bias_add
 
            return None, None

        def config_cub():
            """
            Config cub scope.
            """
            cub = None
            if not is_support_fixpipe():
                cub = tensor_map.get(Conv2dTensorName.CUB)
                if cub is not None:
                    sch[cub].set_scope(cce_params.scope_ubuf)
                return cub
            if self._mixl2_flag:
                return self._fixpipe_res_gm2ub_list[0]
            return self._fixpipe_res_list[0]

        def config_cub_remove_padded_column():
            cub_remove_padded_column = None
            if self._v200_width_out_1_flag:
                cub_remove_padded_column = tensor_map[Conv2dTensorName.REMOVE_PADDED_COLUMN]
                sch[cub_remove_padded_column].set_scope(cce_params.scope_ubuf)
            return cub_remove_padded_column

        #========set scope && cache_read && cache_write==========
        tensor_map = self._tensor_map
        sch = self._sch
        res = self._res

        fmap = tensor_map["fmap"]
        weight = tensor_map["filter"]
        fmap_row_major_reshape = tensor_map.get("fmap_row_major_reshape", None)
        al1_im2col = self._im2col_dma.config_al1_im2col(sch, tensor_map)

        cl0 = config_cl0()
        fmap_row_major = config_fmap_row_major()
        al0 = config_al0()
        al1 = config_al1()
        al1_load2d_real, al1_load2d_zero = config_al1_nd_input()
        al1_zero, al1_dma_im2col, al1_virtual_add = config_dma()
        bl1, weight_index_l1 = config_bl1()
        bl0 = config_bl0()
        cub = config_cub()
        cub_remove_padded_column = config_cub_remove_padded_column()
        bias_l1, bias_bt, bias_l1_real, bias_l1_zero = config_bias_l1_bt()
        bias_ub, cub_bias_add = config_bias_ub()
        if self._bias_set_zero_in_ub_flag:
            bias_l1, bias_bt, bias_ub = config_bias_ub_l1_bt()

        aub_dma = self._im2col_dma.config_aub(sch, tensor_map)

        self._eltwise_ub_fusion.cub_set_scope(sch)
        self._eltwise_ub_fusion.inputs_cache_read(sch, self._op_graph)
        self._eltwise_ub_fusion.res_cache_write(sch, self._res)

        self._quant_fusion.quant_tensors_set_scope(sch)

        self._pooling_fusion.set_maxpool_ub_scope(sch, self._op_graph.body_ops)

        self._quant_conv2d_schedule.config_scope()

        if self._dynamic_shape.flag and not is_support_fixpipe():
            self._input_nd2nz.input_nd_binary_set_scope(sch, self._op_graph, fmap)
            self._output_nz2nd.output_nd_binary_set_scope(sch)

        tensor_param = {"al1": al1, "bl1": bl1,
                        "al1_zero": al1_zero, "al1_dma_im2col": al1_dma_im2col, "al1_virtual_add": al1_virtual_add,
                        "fmap": fmap, "weight": weight,
                        "fmap_row_major": fmap_row_major, "fmap_row_major_reshape": fmap_row_major_reshape,
                        "al1_im2col": al1_im2col, "aub_dma": aub_dma,
                        "al0": al0, "bl0": bl0, "cl0": cl0,
                        "bias_l1": bias_l1, "bias_bt": bias_bt,
                        "bias_l1_real": bias_l1_real, "bias_l1_zero": bias_l1_zero,
                        "cub": cub, "bias_ub": bias_ub, "cub_bias_add": cub_bias_add,
                        "weight_index_l1": weight_index_l1,
                        "cub_remove_padded_column": cub_remove_padded_column,
                        "al1_load2d_real": al1_load2d_real, "al1_load2d_zero": al1_load2d_zero}

        return tensor_param

    def special_process_pre(self, res, tensor_param):
        """
        Special process before tiling is parsed.
        """
        def align_al1():
            """
            Align al1 in various situation.
            """
            if self._c04.dma_c04_flag or self._conv_param.aipp_fuse_flag:
                self._c04.align_al1_c04(sch, al1)
            if self._conv_param.aipp_fuse_flag:
                self._aipp_fusion.align_al1_aipp(sch, al1)
            if self._lx_fusion.l1_fusion_type == BREADTH_L1_FUSION:
                return self._lx_fusion.align_al1_lxfusion(sch, al1)
            if self._l0a_load2d.flag:
                return self._l0a_load2d.align_al1_load2d(sch, al1)
            if self._pooling_fusion.flag:
                return self._pooling_fusion.align_al1_pooling(sch, al1)
            return None

        def align_row_major():
            """
            Align row major in various situation.
            """
            if self._dynamic_shape.flag or self._l0a_load2d.flag or self._im2col_dma.flag or self._wino_conv.flag:
                return None

            if self._conv1d.flag:
                return self._conv1d.align_row_major_conv1d(sch, fmap_row_major, self._block_k0)

            if self._split_w_flag:
                _, _, _, m_idx, _, _, _, k0_idx = range(len(fmap_row_major.shape))
            else:
                _, _, m_idx, _, _, _, k0_idx = range(len(fmap_row_major.shape))

            fmap_row_major_buffer_align_list = []
            for i in range(len(fmap_row_major.shape)):
                fmap_row_major_buffer_align_list.append((1, 1))
            fmap_row_major_buffer_align_list[m_idx] = (self._out_width, self._out_width)
            fmap_row_major_buffer_align_list[k0_idx] = (1, 4 if self._c04.flag else self._block_k0)
            sch[fmap_row_major].buffer_align(*fmap_row_major_buffer_align_list)
            return None

        def align_cl0():
            def get_cl0_axis_buffer_align_list():
                cl0_axis_buffer_align_list_ = []
                k_axis_num = len(cl0.op.reduce_axis)
                cl0_total_axis = len(cl0.shape) + k_axis_num
                for i in range(cl0_total_axis):
                    cl0_axis_buffer_align_list_.append((1, 1))

                if self._wino_conv.flag:
                    _, _, _, _, m_idx, co0_idx, _, _, k0_idx = range(cl0_total_axis)
                elif self._split_w_flag:
                    _, _, _, _, m_idx, co0_idx, _, k0_idx = range(cl0_total_axis)
                else:
                    _, _, _, m_idx, co0_idx, _, k0_idx = range(cl0_total_axis)

                cl0_axis_buffer_align_list_[m_idx] = (1, self._block_m0)
                cl0_axis_buffer_align_list_[co0_idx] = (1, self._block_n0)
                cl0_axis_buffer_align_list_[k0_idx] = (1, self._block_k0)

                return cl0_axis_buffer_align_list_

            cl0_axis_buffer_align_list = get_cl0_axis_buffer_align_list()
            sch[cl0].buffer_align(*cl0_axis_buffer_align_list)

        def align_cub():
            cub = tensor_param.get("cub")
            if cub is None or is_support_fixpipe():
                return

            cub_axis_buffer_aligns = list()
            cub_total_axis = len(cub.shape)
            for i in range(cub_total_axis):
                cub_axis_buffer_aligns.append((1, 1))
            _, _, m_idx, _ = range(cub_total_axis)
            cub_axis_buffer_aligns[m_idx] = (1, self._block_m0)
            sch[cub].buffer_align(*cub_axis_buffer_aligns)

        def align_bias():
            """
            Align and add zeros to the bias.
            """
            cout = self._para_dict.get("weight_ori_shape_nchw")[0]
            # k0 32 bytes align
            align_sizes = ceil(cout, K0_ALIGN_BYTES // BIT_RATIO_MAP.get(self._bias_dtype))
            bias_ub = tensor_param.get("bias_ub")
            sch[bias_ub].set_value(
                lambda group_idx, n_idx, co1_idx, h_idx, w_idx, co0_idx:
                tvm.call_intrin(
                    self._bias_dtype,
                    "tir.likely",
                    tvm.all(
                        group_idx * self._co1_opt * self._block_n0 + co1_idx * self._block_n0 + co0_idx >= align_sizes,
                        group_idx * self._co1_opt * self._block_n0 + co1_idx * self._block_n0 + co0_idx < \
                        self._group_opt * self._co1_opt * self._block_n0
                        )
                    ),
                tvm.const(0, dtype=self._bias_dtype)
                )

        def process_data_rm():
            """
            process remove pad M
            """
            if self._conv_param.invalid_data_rm_flag:
                delete_op("invalid_conv2d_rmpad", self._op_graph.body_ops, sch)

        sch = self._sch
        al1 = tensor_param["al1"]
        al1_load2d_real = tensor_param["al1_load2d_real"]
        al1_load2d_zero = tensor_param["al1_load2d_zero"]
        al0 = tensor_param["al0"]
        fmap = tensor_param["fmap"]
        fmap_row_major = tensor_param["fmap_row_major"]
        fmap_row_major_reshape = tensor_param["fmap_row_major_reshape"]
        al1_im2col = tensor_param["al1_im2col"]
        cl0 = tensor_param["cl0"]

        align_cl0()
        align_al1()
        align_cub()
        if self._conv_param.binary_static_flag:
            self._eltwise_ub_fusion.align_cub(sch, self._block_m0)
        align_row_major()
        process_data_rm()

        self._quant_conv2d_schedule.l0c_bias_add_align(self._block_m0)

        if is_support_fixpipe() and self._bias_set_zero_in_ub_flag:
            align_bias()

        # for multi-group load2d
        special_process_dict = {
            "group_opt": self._group_opt, "al0": al0, "ci1_opt": self._ci1_opt,
            "in_c1": self._in_c1, "_l0a_layout_zn_flag": self._l0a_layout_zn_flag,
            "al1": al1, "al1_howo": self._out_hw, "input_nd_flag": self._input_nd2nz.flag,
            "al1_load2d_real": al1_load2d_real, "al1_load2d_zero": al1_load2d_zero,
        }
        if not self._dynamic_shape.flag:
            self._l0a_load2d.special_process_pre(sch, special_process_dict)

        # fixpipe fusion
        if not self._dynamic_shape.flag:
            self._fixpipe_fusion.special_process_pre(sch, res)

        # bn1 fusion
        self._convbn1.bn1fusion_special_process_pre(sch)

        # inline row_major_reshape
        if fmap_row_major_reshape is not None:
            sch[fmap_row_major_reshape].compute_inline()

        self._im2col_dma.align_al1_im2col(sch, al1_im2col, self._block_k0)

        self._strided_read.process_strided_read(sch, al1, self._strideh_opti.flag, self._l0a_load2d.flag)
        self._strided_write.process_strided_write(sch, self._res)

        # inline input_nd
        self._input_nd2nz.inline_input_nd(sch, self._tensor_map, self._dynamic_shape.flag, self._l0a_load2d.flag)

        # quant fusion
        self._quant_fusion.inline_input_ub(sch)

        # ub fusion
        self._eltwise_ub_fusion.ub_tensors_inline(sch)
        self._eltwise_ub_fusion.ub_tensors_reuse(sch, self._res)

        # dynamic shape
        self._dynamic_shape.handle_var_range(sch)
        self._dynamic_shape.disable_memory_reuse(sch, tensor_param)

    def tile_attach_tensor(self, res, sch_list, tensor_param):
        """
        Split tensor axis and attach tensors.
        """
        def tile_tensor_al0():
            """
            tile al0 for load3d emit insn
            """
            if self._wino_conv.flag:
                return None, None
            if self._l0a_layout_zn_flag:
                if self._split_w_flag:
                    group_opt_idx, batch_idx, ho_idx, k1_idx, m1_idx, m0_idx, k0_idx = range(len(al0.shape))
                else:
                    group_opt_idx, batch_idx, k1_idx, m1_idx, m0_idx, k0_idx = range(len(al0.shape))
            else:
                if self._split_w_flag:
                    group_opt_idx, batch_idx, ho_idx, m1_idx, k1_idx, m0_idx, k0_idx = range(len(al0.shape))
                else:
                    group_opt_idx, batch_idx, m1_idx, k1_idx, m0_idx, k0_idx = range(len(al0.shape))

            if self._dynamic_shape.flag and not self._conv_param.al0boundcheck_flag:
                al0_mo, al0_mi = sch[al0].split(al0.op.axis[m1_idx], ma_al0, tail_strategy='shift_inwards')
            else:
                al0_mo, al0_mi = sch[al0].split(al0.op.axis[m1_idx], ma_al0)

            al0_ko, al0_ki = sch[al0].split(al0.op.axis[k1_idx], ka_al0)
            al0_no, al0_ni = sch[al0].split(al0.op.axis[batch_idx], 1)

            if self._l0a_layout_zn_flag:
                if self._split_w_flag:
                    sch[al0].reorder(al0.op.axis[group_opt_idx],  # group
                                     al0_no,  # batch.outer
                                     al0_ko,  # k_1.outer
                                     al0_mo,  # m_1.outer  out2cl0_loopm
                                     al0.op.axis[ho_idx],  # out2al1_loopm + al12al0_loopm
                                     al0_ni,  # batch.inner = 1
                                     al0_ki,  # k_1.inner
                                     al0_mi,  # m_1.inner
                                     al0.op.axis[m0_idx],  # m_0
                                     al0.op.axis[k0_idx])  # k_0
                else:
                    sch[al0].reorder(al0.op.axis[group_opt_idx],  # group
                                     al0_no,  # batch.outer
                                     al0_ko,  # k_1.outer
                                     al0_mo,  # m_1.outer
                                     al0_ni,  # batch.inner = 1
                                     al0_ki,  # k_1.inner
                                     al0_mi,  # m_1.inner
                                     al0.op.axis[m0_idx],  # m_0
                                     al0.op.axis[k0_idx])  # k_0
            else:
                if self._split_w_flag:
                    sch[al0].reorder(al0.op.axis[group_opt_idx],  # group
                                     al0_no,  # batch.outer
                                     al0_mo,  # m_1.outer
                                     al0.op.axis[ho_idx],
                                     al0_ko,  # k_1.outer
                                     al0_ni,  # batch.inner = 1
                                     al0_mi,  # m_1.inner
                                     al0_ki,  # k_1.inner
                                     al0.op.axis[m0_idx],  # m_0
                                     al0.op.axis[k0_idx])  # k_0
                else:
                    sch[al0].reorder(al0.op.axis[group_opt_idx],  # group
                                     al0_no,  # batch.outer
                                     al0_mo,  # m_1.outer
                                     al0_ko,  # k_1.outer
                                     al0_ni,  # batch.inner = 1
                                     al0_mi,  # m_1.inner
                                     al0_ki,  # k_1.inner
                                     al0.op.axis[m0_idx],  # m_0
                                     al0.op.axis[k0_idx])  # k_0

            al0_axis_list = [al0_no, al0_mo, al0_ko,
                             al0_ni, al0_mi, al0_ki,
                             al0.op.axis[m0_idx], al0.op.axis[k0_idx]]  # axis for im2col
            dynamic_al0_pragma_axis = al0_ni

            return al0_axis_list, dynamic_al0_pragma_axis

        def get_reorder_mn_flag():
            """
            get_reorder_mn_flag. return True for "MN", return False for "NM".
            """
            if not self._dynamic_shape.flag:
                if control_reorder_flag == 1:
                    return True
                if control_reorder_flag == 0:
                    return False
                err_man.raise_err_specific("conv2d", "Wrong control_reorder_flag!")
                return False
            else:
                if not bl1_tiling:
                    return True
                if pingpong_buffer["AL1_pbuffer"] == pingpong_buffer["BL1_pbuffer"]:
                    if not self._dynamic_shape.flag and bl1_nparts[1] >= al1_nparts[1]:
                        return True
                    return False
                if pingpong_buffer["BL1_pbuffer"] == 2:
                    return True
                return False

        def get_1_n_ratio():
            """
            get vector core / cube core ratio
            """
            cube_core = get_soc_spec("CUBE_CORE_CNT")
            vector_core = get_soc_spec("VECTOR_CORE_CNT")
            return int(vector_core) // int(cube_core)

        def process_1_n(sch, res, cl02cub_loopn_axis):
            """
            process 1_N in v220
            """
            if not self._support_l0c_to_ub_flag and self._eltwise_ub_fusion.flag:
                ratio = get_1_n_ratio()
                sch[res].bind_sub_block(tvm.thread_axis("subBlockIdx.x"), cl02cub_loopn_axis, nparts=ratio)

        def tile_tensor_res():
            """
            tile tensor res
            """
            def set_reorder_mn_flag():
                """
                Reorder axis m and n to achieve better performance.
                """
                def reorder_res_mn_axis():
                    """
                    Reorder axis m and n according to various flags.
                    """
                    if self._inner_batch.flag:
                        if reorder_mn_flag:
                            sch[res].reorder(
                                out2al1_loopbatch_axis,
                                singlecore_out2al1_loopm_axis,
                                res_batch_1_axis,
                                singlecore_out2bl1_loopn_axis
                                )
                        else:
                            sch[res].reorder(
                                out2al1_loopbatch_axis,
                                singlecore_out2bl1_loopn_axis,
                                singlecore_out2al1_loopm_axis,
                                res_batch_1_axis
                                )
                    else:
                        # singlecore_out2bl1_loopn_axis means nparts of co1 axis loading into L1 in single core.
                        # (N axis)
                        # singlecore_out2al1_loopm_axis means nparts of howo axis loading into L1 in single core.
                        # (M axis)
                        first_part_axis_list = [out2al1_loopbatch_axis]
                        if self._split_w_flag:
                            first_part_axis_list = [out2al1_loopbatch_axis, out2cl0_loopm_axis]

                        if reorder_mn_flag:
                            sch[res].reorder(
                                *first_part_axis_list,
                                singlecore_out2al1_loopm_axis,
                                res_batch_1_axis,
                                singlecore_out2bl1_loopn_axis,
                                bl12bl0_loopn_axis,
                                res_al1_batch_axis
                                )
                        else:
                            sch[res].reorder(
                                *first_part_axis_list,
                                singlecore_out2bl1_loopn_axis,
                                singlecore_out2al1_loopm_axis,
                                bl12bl0_loopn_axis,
                                res_batch_1_axis,
                                res_al1_batch_axis
                                )

                reorder_mn_flag = get_reorder_mn_flag()
                reorder_res_mn_axis()

                return reorder_mn_flag


            # Only fixpipe fusion. The special axis split operation works on res.
            fixpipe_nz2nd_flag = self._fixpipe_fusion.get_nz2nd_flag()
            fixpipe_channelsplit_flag = self._fmap_dtype == "float32" and not self._eltwise_ub_fusion.flag
            fixpipe_channelmerge_flag = res.dtype in ("int4", "int8") and not self._eltwise_ub_fusion.flag
            fixpipe_antiquant_flag = self._fixpipe_fusion.get_antiquant_flag()

            if fixpipe_nz2nd_flag:
                fixpipe_channelsplit_flag = False
                fixpipe_channelmerge_flag = False

            special_axis_dict = {}
            dtype_coeff = {
                # channel merge/split coeff of c0 from L0C to UB/OUT.
                "int4": 4,
                "int8": 2,
                "float32": 0.5
            }

            def fetch_base_axis():
                """
                Fetch axes of the res tensor.
                """
                if self._output_nz2nd.mode == "NCHW":
                    res_n_axis, res_c_axis, res_hw_axis = res.op.axis  # [n, co, howo]
                    res_c1_axis, res_c0_axis = sch[res].split(res_c_axis, 16)  # [n, co1, co0, howo]
                elif fixpipe_nz2nd_flag:
                    res_n_axis, res_hw_axis, res_c_axis = res.op.axis  # [n, howo, co]
                    # split c axis into c1 and c0 to avoid nonlinear ir
                    res_c1_axis, res_c0_axis = sch[res].split(res_c_axis, 16)  # [n, howo, co1, co0]
                else:
                    res_n_axis, res_c1_axis, res_hw_axis, res_c0_axis = res.op.axis  # [n, co1, howo, co0]

                return res_n_axis, res_c1_axis, res_hw_axis, res_c0_axis

            def cal_co1_opt_factor():
                """
                Calculate the co1_opt factor to split out group axis.
                """
                if fixpipe_nz2nd_flag:
                    co1_opt_factor = co1_opt
                elif res.dtype in ("int4", "int8"):
                    if multi_cl0_group:  # group_cl0 is even while co1_opt is odd.
                        co1_opt_factor = ceil_div(co1_opt * group_cl0, dtype_coeff.get(res.dtype))
                    else:
                        co1_opt_factor = ceil_div(co1_opt, dtype_coeff.get(res.dtype))
                elif self._fmap_dtype == "float32":
                    co1_opt_factor = co1_opt * 2
                elif res.dtype in ("float16", "bfloat16", "int32", "float32"):
                    co1_opt_factor = co1_opt
                else:
                    err_man.raise_err_specific("conv2d", "res dtype is not supported!")

                return co1_opt_factor

            def cal_nc_cl0_factor():
                """
                Fetch nc_cl0 for various tiling situation.
                """
                nc_cl0_factor = nc_cl0
                if res.dtype in ("int4", "int8") and not fixpipe_nz2nd_flag:
                    if multi_cl0_group:
                        nc_cl0_factor = co1_opt * group_cl0 // dtype_coeff.get(res.dtype)
                    else:
                        nc_cl0_factor = nc_cl0 // dtype_coeff.get(res.dtype)

                return nc_cl0_factor

            def split_group_opt_axis(co1_opt_factor):
                """
                Split out group_opt axis and co1_opt axis.
                """
                res_group_opt_axis, res_co1_opt_axis = sch[res].split(res_c1_axis, factor=co1_opt_factor)

                if fixpipe_channelsplit_flag:
                    res_co1_opt_axis_ori = res_co1_opt_axis
                    res_co1_opt_axis, res_c0_npart_axis = sch[res].split(res_co1_opt_axis_ori, 2)
                    special_axis_dict["fixpipe_channelsplit_res_c0_npart_axis"] = res_c0_npart_axis

                return res_group_opt_axis, res_co1_opt_axis

            def split_nc_cl0_axis(nc_cl0_factor):
                """
                Split out nc_cl0_axis.
                """
                out2cl0_loopn_axis, res_nc_cl0_axis = sch[res].split(res_co1_opt_axis, factor=nc_cl0_factor)

                if fixpipe_antiquant_flag:
                    # fp16 and not nd2nz and anti_quant, split 2 for pass calculate c0 stride.
                    res_nc_cl0_axis_ori = res_nc_cl0_axis
                    res_nc_cl0_axis, res_c0_npart_axis = sch[res].split(res_nc_cl0_axis_ori, 2)
                    special_axis_dict["fixpipe_antiquant_res_c0_npart_axis"] = res_c0_npart_axis

                return out2cl0_loopn_axis, res_nc_cl0_axis

            def special_axis_process(res_c0_axis):
                """
                Process special axes in fixpipe fusion situation.
                """
                if fixpipe_channelmerge_flag:
                    # split c0=16 in channel merging to avoid nonlinear ir
                    _, _ = sch[res].split(res_c0_axis, factor=16)

            res_n_axis, res_c1_axis, res_hw_axis, res_c0_axis = fetch_base_axis()
            if self._split_w_flag:
                ho_axis, wo_axis = sch[res].split(res_hw_axis, nparts=self._out_height)
                res_hw_axis = wo_axis

            co1_opt_factor = cal_co1_opt_factor()
            res_group_opt_axis, res_co1_opt_axis = split_group_opt_axis(co1_opt_factor)

            nc_cl0_factor = cal_nc_cl0_factor()
            out2cl0_loopn_axis, res_nc_cl0_axis = split_nc_cl0_axis(nc_cl0_factor)

            special_axis_process(res_c0_axis)

            # split cl0 tiling m axis
            res_m_cl0_factor = mc_cl0 * m0_cl0
            if self._v200_width_out_1_flag:
                res_m_cl0_factor = res_m_cl0_factor // 2
            res_m_cl0_factor = self._pooling_fusion.modify_res_m_cl0_factor(res_m_cl0_factor)
            wino_conv_res_flag = is_wino_res_tensor(res)
            if self._wino_conv.flag:
                if wino_conv_res_flag:
                    res_m_hp1_axis, res_m_wp2_axis = sch[res].split(res_hw_axis, self._out_width)
                    res_m_h_axis, res_m_p1_axis = sch[res].split(res_m_hp1_axis, WINO_OUT_TILE_HW)
                    res_m_w_axis, res_m_p2_axis = sch[res].split(res_m_wp2_axis, WINO_OUT_TILE_HW)
                    sch[res].reorder(res_m_p1_axis, res_m_h_axis, res_m_w_axis, res_m_p2_axis)
                    res_hw_axis = sch[res].fuse(res_m_h_axis, res_m_w_axis)
                else:
                    res_m_cl0_factor = mc_cl0 * m0_cl0 * WINO_OUT_TILE_HW * WINO_OUT_TILE_HW
            out2cl0_loopm_axis, res_m_cl0_axis = sch[res].split(res_hw_axis, res_m_cl0_factor)

            # split cub tiling axis
            cl02cub_loopn_axis, res_nc_factor_axis = sch[res].split(res_nc_cl0_axis,
                                                                    nparts=ceil_div(nc_cl0, nc_factor_cub))
            cl02cub_loopm_axis, res_m_factor_axis = sch[res].split(res_m_cl0_axis, nparts=1)
            out2cl0_loopm_axis_list = [out2cl0_loopm_axis]
            if self._split_w_flag:
                out2cl0_loopm_axis_list = [out2cl0_loopm_axis, ho_axis]

            if wino_conv_res_flag:
                sch[res].reorder(out2cl0_loopn_axis, # co1_opt // nc
                                 *out2cl0_loopm_axis_list,  # wino_res_howo // (mc*m0) 
                                 #========cl0 tiling=================
                                 cl02cub_loopn_axis, # nc_cl0 // nc_factor_cub
                                 cl02cub_loopm_axis, # 1
                                 #========cub tiling=================
                                 res_nc_factor_axis, # nc_factor_cub
                                 res_m_p1_axis, # p1
                                 res_m_factor_axis, # mc_factor_cub*m0_cl0
                                 res_m_p2_axis) # p2
            elif fixpipe_nz2nd_flag:
                sch[res].reorder(res_group_opt_axis,
                                 out2cl0_loopn_axis,
                                 *out2cl0_loopm_axis_list,
                                 #========cl0 tiling=================
                                 cl02cub_loopn_axis, # nc_cl0 // nc_factor_cub
                                 cl02cub_loopm_axis, # 1
                                 #========cub tiling=================
                                 res_m_factor_axis, # mc_factor_cub*m0_cl0
                                 res_nc_factor_axis) # nc_factor_cub
                # [n, group_opt, co1_opt // nc, howo // (mc*m0),
                # ||| nc // nc_factor, 1,
                # ||| mc_factor*m0, nc_factor, co0]
            elif fixpipe_channelsplit_flag:
                sch[res].reorder(res_group_opt_axis,
                                 out2cl0_loopn_axis,
                                 *out2cl0_loopm_axis_list,
                                 #========cl0 tiling=================
                                 cl02cub_loopn_axis, # nc_cl0 // nc_factor_cub
                                 cl02cub_loopm_axis, # 1
                                 #========cub tiling=================
                                 res_nc_factor_axis, # nc_factor_cub
                                 special_axis_dict.get("fixpipe_channelsplit_res_c0_npart_axis") ,
                                 res_m_factor_axis) # mc_factor_cub*m0_cl0
                # [n, group_opt, co1_opt // 2*nc, howo // (mc*m0),
                # ||| nc // nc_factor, 1,
                # ||| nc_factor, 2, mc_factor*m0, co0]
            elif fixpipe_antiquant_flag:
                sch[res].reorder(out2cl0_loopn_axis, # co1_opt // nc
                                 *out2cl0_loopm_axis_list,  # [howo // (mc*m0)] || [wo_ailgn // (mc*m0), ho]
                                 #========cl0 tiling=================
                                 cl02cub_loopn_axis, # nc_cl0 // nc_factor_cub
                                 cl02cub_loopm_axis, # 1
                                 #========cub tiling=================
                                 res_nc_factor_axis, # nc_factor_cub // 2
                                 special_axis_dict.get("fixpipe_antiquant_res_c0_npart_axis"),
                                 res_m_factor_axis) # mc_factor_cub*m0_cl0
            else:
                sch[res].reorder(out2cl0_loopn_axis, # co1_opt // nc
                                 *out2cl0_loopm_axis_list,  # [howo // (mc*m0)] || [wo_ailgn // (mc*m0), ho]
                                 #========cl0 tiling=================
                                 cl02cub_loopn_axis, # nc_cl0 // nc_factor_cub
                                 cl02cub_loopm_axis, # 1
                                 #========cub tiling=================
                                 res_nc_factor_axis, # nc_factor_cub
                                 res_m_factor_axis) # mc_factor_cub*m0_cl0
                # [n, group_opt, co1_opt // nc, howo // (mc*m0),
                # ||| nc // nc_factor, 1,
                # ||| nc_factor, mc_factor*m0, co0]

                if self._output_nz2nd.mode == "NCHW":
                    sch[res].reorder(cl02cub_loopn_axis,
                                     cl02cub_loopm_axis,
                                     res_nc_factor_axis,
                                     res_c0_axis,
                                     res_m_factor_axis)

            mal1_split_axis = out2cl0_loopm_axis
            if self._split_w_flag:
                mal1_split_axis = ho_axis
            out2al1_loopm_axis, al12al0_loopm_axis = sch[res].split(mal1_split_axis, nparts=al1_nparts[1])
            # when multi_cl0_group, cl0_factor[0] is 1 and bl1_nparts[1] is 1
            out2bl1_loopn_axis, bl12bl0_loopn_axis = sch[res].split(out2cl0_loopn_axis, nparts=bl1_nparts[1])

            # split batch of res
            if self._dynamic_shape.n_dynamic:
                batch_dim_factor = tvm.max(1, ceil_div(batch, batch_dim))
                if self._conv_param.al0boundcheck_flag:
                    res_batch_dim_axis, res_singlecore_batch_axis = sch[res].split(res_n_axis, batch_dim_factor)
                else:
                    res_batch_dim_axis, res_singlecore_batch_axis = sch[res].split(res_n_axis,
                                                                                   batch_dim_factor,
                                                                                   tail_strategy='shift_inwards')
            else:
                res_batch_dim_axis, res_singlecore_batch_axis = sch[res].split(res_n_axis, nparts=batch_dim)

            res_group_dim_axis, res_singlecore_group_opt_axis = sch[res].split(res_group_opt_axis, nparts=group_dim)

            batch_factor = self._inner_batch.config_innerbatch_axis(batch_al1, batch_cl0)
            out2al1_loopbatch_axis, res_al1_batch_axis = sch[res].split(res_singlecore_batch_axis, batch_factor)

            if group_cl0 == 1 and multi_bl0_group:
                singlecore_out2bl0_loopg_axis, res_bl0_group_opt_axis = sch[res].split(res_singlecore_group_opt_axis,
                                                                                       factor=group_bl0)
            else:
                singlecore_out2bl0_loopg_axis, res_bl0_group_opt_axis = sch[res].split(res_singlecore_group_opt_axis, 1)

            # split cout of res
            res_n_dim_axis, singlecore_out2bl1_loopn_axis = sch[res].split(out2bl1_loopn_axis, nparts=n_dim)
            res_m_dim_axis, singlecore_out2al1_loopm_axis = sch[res].split(out2al1_loopm_axis, nparts=m_dim)

            if self._inner_batch.flag:
                if self._pooling_fusion.flag:
                    err_man.raise_err_specific("conv2d", "Inner-batch is not comparible with pooling fusion.")
                if self._im2col_dma.flag:
                    err_man.raise_err_specific("conv2d", "Inner-batch is not comparible with dma.")
                log.debug("Enter inner-batch branch, axes may be reordered.")
                conv_schedule_log_util.record_n_optimization_log_for_inner_batch_fix_pipe(self._fixpipe_fusion)
                sch[res].reorder(res_batch_dim_axis,
                                 res_group_dim_axis,
                                 res_n_dim_axis,
                                 res_m_dim_axis,
                                 singlecore_out2bl0_loopg_axis,
                                 res_bl0_group_opt_axis,
                                 out2al1_loopbatch_axis,
                                 singlecore_out2bl1_loopn_axis,
                                 singlecore_out2al1_loopm_axis,
                                 bl12bl0_loopn_axis,
                                 al12al0_loopm_axis,
                                 # ===============cl0 tiling========================
                                 cl02cub_loopn_axis,
                                 cl02cub_loopm_axis,
                                 # ===============cub tiling========================
                                 res_al1_batch_axis)
            else:
                res_reorder_axis_list = [res_batch_dim_axis,
                                         res_group_dim_axis,
                                         res_n_dim_axis,
                                         res_m_dim_axis,
                                         singlecore_out2bl0_loopg_axis,
                                         res_bl0_group_opt_axis,
                                         out2al1_loopbatch_axis,
                                         singlecore_out2bl1_loopn_axis,
                                         singlecore_out2al1_loopm_axis,
                                         res_al1_batch_axis,
                                         bl12bl0_loopn_axis]
                if self._split_w_flag:
                    res_ho_dim_axis = res_m_dim_axis
                    singlecore_out2al1_loop_ho_axis = singlecore_out2al1_loopm_axis
                    res_reorder_axis_list = [res_batch_dim_axis,
                                             res_group_dim_axis,
                                             res_n_dim_axis,
                                             res_ho_dim_axis,
                                             singlecore_out2bl0_loopg_axis,
                                             res_bl0_group_opt_axis,
                                             out2al1_loopbatch_axis,
                                             out2cl0_loopm_axis,  # wo // (mc * m0)
                                             singlecore_out2bl1_loopn_axis,
                                             singlecore_out2al1_loop_ho_axis,
                                             res_al1_batch_axis,  # 1, inner batch is not supported for split_w
                                             bl12bl0_loopn_axis]

                sch[res].reorder(*res_reorder_axis_list)

            if fixpipe_nz2nd_flag:
                res_pragma_axis = res_m_factor_axis
            elif self._dynamic_shape.flag and self._fmap_dtype == "float32" and self._res_dtype == "float32":
                res_pragma_axis = special_axis_dict.get("fixpipe_channelsplit_res_c0_npart_axis")
            else:
                res_pragma_axis = res_nc_factor_axis

            blocks = batch_dim*n_dim*m_dim*group_dim

            if blocks != 1:
                if self._dynamic_shape.flag:
                    multicore_axis_o, batchbindonly_pragma_axis = sch[res].split(res_m_dim_axis, factor=1)
                    bind_axis_list = [res_batch_dim_axis, res_group_dim_axis, res_n_dim_axis, multicore_axis_o]
                    block = tvm.thread_axis("blockIdx.x")
                    sch.bind_axes(bind_axis_list, block)
                    bindcore_axis = multicore_axis_o
                else:
                    multicore_axis = sch[res].fuse(
                        res_batch_dim_axis,
                        res_group_dim_axis,
                        res_n_dim_axis,
                        res_m_dim_axis)
                    if self._dynamic_shape.flag:
                        multicore_axis_o, _ = sch[res].split(multicore_axis, factor=1)
                    else:
                        multicore_axis_o, _ = sch[res].split(multicore_axis, nparts=blocks)

                    bindcore_axis, batchbindonly_pragma_axis = sch[res].split(multicore_axis_o, 1)
                    sch[res].bind(bindcore_axis, tvm.thread_axis("blockIdx.x"))

                    if blocks == batch_dim:
                        sch[res].pragma(batchbindonly_pragma_axis, 'json_info_batchBindOnly', 1)
            else:
                bindcore_axis = res_batch_dim_axis

            process_1_n(sch, res, cl02cub_loopn_axis)

            out2al1_loopbatch_axis_ori = out2al1_loopbatch_axis
            out2al1_loopbatch_axis, res_batch_1_axis = sch[res].split(out2al1_loopbatch_axis_ori, factor=1)

            reorder_mn_flag = set_reorder_mn_flag()

            if not self._dynamic_shape.flag:
                if self._tiling["control_reorder_flag"] == 0 and not reorder_mn_flag and not self._inner_batch.flag:
                    sch[res].reorder(singlecore_out2bl1_loopn_axis, out2al1_loopbatch_axis)
            else:
                if self._tiling["n_bef_batch_flag"] and not reorder_mn_flag and not self._inner_batch.flag:
                    sch[res].reorder(singlecore_out2bl1_loopn_axis, out2al1_loopbatch_axis)

            def get_res_attach_axis():
                """
                prepare res attach axis for compute_at
                """
                # get cub_at_res_axis
                cub_at_res_axis = cl02cub_loopm_axis

                # get cl0_at_res_axis
                cl0_at_res_axis = al12al0_loopm_axis

                # get bl0_at_res_axis
                bl0_at_res_axis = singlecore_out2bl0_loopg_axis

                # get al1_at_res_axis
                al1_at_res_axis = out2al1_loopbatch_axis
                if self._lx_fusion.l1_fusion_type == DEPTH_L1_FUSION:
                    al1_at_res_axis = singlecore_out2bl0_loopg_axis
                if al1_tiling or self._conv1d.flag:
                    al1_at_res_axis = singlecore_out2al1_loopm_axis

                # get bl1_at_res_axis
                bl1_at_res_axis = singlecore_out2bl0_loopg_axis
                if bl1_tiling:
                    bl1_at_res_axis = singlecore_out2bl1_loopn_axis
                    if group_cl0 == 1 and multi_bl0_group:
                        bl1_at_res_axis = singlecore_out2bl0_loopg_axis
                if self._inner_batch.flag:
                    bl1_at_res_axis = singlecore_out2bl1_loopn_axis

                return [cub_at_res_axis, cl0_at_res_axis, bl0_at_res_axis,
                        al1_at_res_axis, bl1_at_res_axis]

            res_axis_list = get_res_attach_axis()

            attach_axis_dict.update(
                {
                    "cub_at_res_axis": res_axis_list[0],
                    "singlecore_out2al1_loopm_axis": singlecore_out2al1_loopm_axis,
                    "al12al0_loopm_axis": al12al0_loopm_axis,
                    "batchbindonly_pragma_axis": batchbindonly_pragma_axis if blocks != 1 else None,
                    "res_m_dim_axis": res_m_dim_axis,
                    "split_w_out2cl0_loopm_axis": out2cl0_loopm_axis,
                    "bl1_at_res_axis": res_axis_list[4],
                    "res_n_dim_axis": res_n_dim_axis,
                    "singlecore_out2bl1_loopn_axis": singlecore_out2bl1_loopn_axis,
                    "bl12bl0_loopn_axis": bl12bl0_loopn_axis,
                    "singlecore_out2bl0_loopg_axis": singlecore_out2bl0_loopg_axis,
                    "res_group_dim_axis": res_group_dim_axis,
                    "res_m_factor_axis": res_m_factor_axis,
                    "res_bl0_group_opt_axis": res_bl0_group_opt_axis,
                }
                )

            return res_axis_list, bindcore_axis, res_pragma_axis

        def bn1fusion_tile_tensor_res():
            """
            bn1fusion special tile_tensor_res
            """
            # k_0, k_1
            res_n_axis, res_hw_axis = res.op.reduce_axis
            out2cl0_loopm_axis, res_m_cl0_axis = sch[res].split(res_hw_axis, factor=mc_cl0 * m0_cl0)
            res_batch_dim_axis, res_singlecore_batch_axis = sch[res].split(res_n_axis, nparts=batch_dim)

            out2al1_loopm_axis, al12al0_loopm_axis = sch[res].split(out2cl0_loopm_axis, nparts=al1_nparts[1])
            res_m_dim_axis, singlecore_out2al1_loopm_axis = sch[res].split(out2al1_loopm_axis, nparts=m_dim)

            sch[res].reorder(
                res_batch_dim_axis,
                res_m_dim_axis,
                res_singlecore_batch_axis,
                singlecore_out2al1_loopm_axis)
            res_batch_m_dim_fused = sch[res].fuse(res_batch_dim_axis, res_m_dim_axis)

            # res_batch_dim_axis and res_m_dim_axis are usually bindcore axis
            # in this scenario, these two are reduction axes
            # TBE don't support bindcore to a reduction axis
            # use rfactor to factor a reduction axis in tensor's schedule to be an explicit axis
            # which create a new stage that generated the new tensor with axis as the first dimension
            # The tensor's body will be rewritten as a reduction over the factored tensor
            res_ub_rf, _ = sch.rfactor(res, res_batch_m_dim_fused)
            sch[res_ub_rf].set_scope(cce_params.scope_ubuf)

            res_batch_m_dim_fused, res_c1_axis, res_c0_axis = res_ub_rf.op.axis
            res_singlecore_batch_axis, singlecore_out2al1_loopm_axis, al12al0_loopm_axis, res_m_cl0_axis \
                                                                                = res_ub_rf.op.reduce_axis

            sum_x_global, square_sum_x_global = sch.cache_write([res, res], "global")

            sch[res].emit_insn(sch[res].op.axis[0], "phony_insn")

            bn_c1_axis, bn_c0_axis = sum_x_global.op.axis
            bn_batch_m_dim_fused = sum_x_global.op.reduce_axis[0]

            sch[sum_x_global].reorder(
                bn_batch_m_dim_fused,
                bn_c1_axis,
                bn_c0_axis)

            # use sch_list[0] to return conv schedule
            # use sch_list[1:] to indicate real_outs
            sch_list.append(self._multi_out[0])
            sch_list.append(sum_x_global)
            sch_list.append(square_sum_x_global)

            # split sum_x_global
            if self._convbn1.fp32_bn1_flag:
                bn_group_opt_axis, bn_co1_opt_axis_ori = sch[sum_x_global].split(bn_c1_axis, factor=co1_opt*2)
                bn_co1_opt_axis, bn_c0_npart_axis = sch[sum_x_global].split(bn_co1_opt_axis_ori, factor=2)
            else:
                bn_group_opt_axis, bn_co1_opt_axis = sch[sum_x_global].split(bn_c1_axis, factor=co1_opt)

            bn_group_dim_axis, bn_singlecore_group_opt_axis = sch[sum_x_global].split(bn_group_opt_axis,
                                                                                      nparts=group_dim)
            bn_out2cl0_loopn_axis, bn_nc_cl0_axis = sch[sum_x_global].split(bn_co1_opt_axis, factor=nc_cl0)

            bn_out2bl1_loopn_axis, bn_bl12bl0_loopn_axis = sch[sum_x_global].split(bn_out2cl0_loopn_axis,
                                                                                   nparts=bl1_nparts[1])
            bn_n_dim_axis, bn_singlecore_out2bl1_loopn_axis = sch[sum_x_global].split(bn_out2bl1_loopn_axis,
                                                                                      nparts=n_dim)
            if self._convbn1.fp32_bn1_flag:
                sch[sum_x_global].reorder(
                    bn_batch_m_dim_fused,
                    bn_group_dim_axis,
                    bn_n_dim_axis,
                    bn_singlecore_group_opt_axis,
                    bn_bl12bl0_loopn_axis,
                    bn_singlecore_out2bl1_loopn_axis,
                    bn_nc_cl0_axis,
                    bn_c0_npart_axis,  # 2
                    bn_c0_axis)
            else:
                sch[sum_x_global].reorder(
                    bn_batch_m_dim_fused,
                    bn_group_dim_axis,
                    bn_n_dim_axis,
                    bn_singlecore_group_opt_axis,
                    bn_bl12bl0_loopn_axis,
                    bn_singlecore_out2bl1_loopn_axis,
                    bn_nc_cl0_axis,
                    bn_c0_axis)

            multicore_axis = sch[sum_x_global].fuse(bn_batch_m_dim_fused, bn_group_dim_axis, bn_n_dim_axis)

            # split res_ub_rf
            if self._convbn1.fp32_bn1_flag:
                res_group_opt_axis, res_co1_opt_axis_ori = sch[res_ub_rf].split(res_c1_axis, factor=co1_opt*2)
                res_co1_opt_axis, res_c0_npart_axis = sch[res_ub_rf].split(res_co1_opt_axis_ori, factor=2)
            else:
                res_group_opt_axis, res_co1_opt_axis = sch[res_ub_rf].split(res_c1_axis, factor=co1_opt)

            out2cl0_loopn_axis, res_nc_cl0_axis = sch[res_ub_rf].split(res_co1_opt_axis, factor=nc_cl0)

            cl02cub_loopm_axis, res_m_factor_axis = sch[res_ub_rf].split(res_m_cl0_axis, nparts=1)

            # when multi_cl0_group, cl0_factor[0] is 1 and bl1_nparts[1] is 1
            out2bl1_loopn_axis, bl12bl0_loopn_axis = sch[res_ub_rf].split(out2cl0_loopn_axis, nparts=bl1_nparts[1])

            res_group_dim_axis, res_singlecore_group_opt_axis = sch[res_ub_rf].split(res_group_opt_axis,
                                                                                     nparts=group_dim)

            batch_factor = self._inner_batch.config_innerbatch_axis(batch_al1, batch_cl0)

            out2al1_loopbatch_axis, res_al1_batch_axis = sch[res_ub_rf].split(res_singlecore_batch_axis,
                                                                              factor=batch_factor)

            if group_cl0 == 1 and multi_bl0_group:
                singlecore_out2bl0_loopg_axis, res_bl0_group_opt_axis = \
                sch[res_ub_rf].split(res_singlecore_group_opt_axis, factor=group_bl0)
            else:
                singlecore_out2bl0_loopg_axis, res_bl0_group_opt_axis = \
                sch[res_ub_rf].split(res_singlecore_group_opt_axis, factor=1)

            res_n_dim_axis, singlecore_out2bl1_loopn_axis = sch[res_ub_rf].split(out2bl1_loopn_axis, nparts=n_dim)

            cl02cub_loopn_axis, res_nc_factor_axis = sch[res_ub_rf].split(res_nc_cl0_axis, factor=nc_factor_cub)

            if self._convbn1.fp32_bn1_flag:
                sch[res_ub_rf].reorder(
                    res_batch_m_dim_fused,
                    res_group_dim_axis,
                    res_n_dim_axis,
                    singlecore_out2bl0_loopg_axis,
                    out2al1_loopbatch_axis,
                    singlecore_out2bl1_loopn_axis,
                    singlecore_out2al1_loopm_axis,
                    res_bl0_group_opt_axis,
                    res_al1_batch_axis,
                    bl12bl0_loopn_axis,
                    al12al0_loopm_axis,
                    cl02cub_loopn_axis,
                    cl02cub_loopm_axis,
                    res_nc_factor_axis,
                    res_c0_npart_axis,  # 2
                    res_m_factor_axis,
                    res_c0_axis)
            else:
                sch[res_ub_rf].reorder(
                    res_batch_m_dim_fused,
                    res_group_dim_axis,
                    res_n_dim_axis,
                    singlecore_out2bl0_loopg_axis,
                    out2al1_loopbatch_axis,
                    singlecore_out2bl1_loopn_axis,
                    singlecore_out2al1_loopm_axis,
                    res_bl0_group_opt_axis,
                    res_al1_batch_axis,
                    bl12bl0_loopn_axis,
                    al12al0_loopm_axis,
                    cl02cub_loopn_axis,
                    cl02cub_loopm_axis,
                    res_nc_factor_axis,
                    res_m_factor_axis,
                    res_c0_axis)

            if self._inner_batch.flag:
                sch[res_ub_rf].reorder(
                    bl12bl0_loopn_axis,
                    al12al0_loopm_axis,
                    cl02cub_loopn_axis,
                    cl02cub_loopm_axis,
                    res_nc_factor_axis,
                    res_al1_batch_axis)

            blocks = batch_dim*n_dim*m_dim*group_dim
            sch[sum_x_global].bind(multicore_axis, tvm.thread_axis("blockIdx.x"))
            if blocks == batch_dim:
                sch[sum_x_global].pragma(bn_bl12bl0_loopn_axis, 'json_info_batchBindOnly', 1)

            reorder_mn_flag = get_reorder_mn_flag()
            if not self._inner_batch.flag:
                if reorder_mn_flag:
                    sch[res_ub_rf].reorder(
                        out2al1_loopbatch_axis,
                        singlecore_out2al1_loopm_axis,
                        bl12bl0_loopn_axis,
                        singlecore_out2bl1_loopn_axis,
                        res_bl0_group_opt_axis,
                        res_al1_batch_axis)
                else:
                    sch[res_ub_rf].reorder(
                        out2al1_loopbatch_axis,
                        singlecore_out2bl1_loopn_axis,
                        singlecore_out2al1_loopm_axis,
                        bl12bl0_loopn_axis,
                        res_bl0_group_opt_axis,
                        res_al1_batch_axis)

            res_pragma_axis = res_m_factor_axis
            bindcore_axis = out2al1_loopbatch_axis

            if not self._support_l0c_to_ub_flag:
                vc_ratio = get_1_n_ratio()
                if vc_ratio > 1:
                    sum_x_global_loopn_axis, _ = sch[sum_x_global].split(bn_nc_cl0_axis, factor=nc_factor_cub)
                    sum_x_global_subblock_outer, sum_x_global_subblock_inner = \
                    sch[sum_x_global].split(sum_x_global_loopn_axis, nparts=vc_ratio)

                    sub_block_var = tvm.thread_axis("subBlockIdx.x")
                    sch[res_ub_rf].bind_sub_block(sub_block_var, cl02cub_loopn_axis, nparts=vc_ratio)
                    sch[sum_x_global].set_store_predicate(tvm.all(sum_x_global_subblock_outer.var == sub_block_var.var),
                                                          partition=True)
                    if ceil_div(nc_cl0, nc_factor_cub) > 1:
                        log.debug("Enable 1:N mode.")

            def get_bn1_res_attach_axis():
                """
                conv2d+bn1 fusion prepare res attach axis for compute_at
                """
                # get cub_at_res_axis
                cub_at_res_axis = cl02cub_loopm_axis

                # get cl0_at_res_axis
                cl0_at_res_axis = al12al0_loopm_axis

                # get bl0_at_res_axis
                bl0_at_res_axis = singlecore_out2bl0_loopg_axis

                # get al1_at_res_axis
                al1_at_res_axis = out2al1_loopbatch_axis
                if self._lx_fusion.l1_fusion_type == DEPTH_L1_FUSION:
                    al1_at_res_axis = singlecore_out2bl0_loopg_axis
                if al1_tiling or self._conv1d.flag:
                    al1_at_res_axis = singlecore_out2al1_loopm_axis

                # get bl1_at_res_axis
                bl1_at_res_axis = singlecore_out2bl0_loopg_axis
                if bl1_tiling:
                    bl1_at_res_axis = singlecore_out2bl1_loopn_axis
                    if group_cl0 == 1 and multi_bl0_group:
                        bl1_at_res_axis = singlecore_out2bl0_loopg_axis
                if self._inner_batch.flag:
                    bl1_at_res_axis = singlecore_out2bl1_loopn_axis

                # get sum_x_global_pragma_axis
                if not self._support_l0c_to_ub_flag and vc_ratio > 1:
                    sum_x_global_pragma_axis = sum_x_global_subblock_inner
                elif cub_channel_wise_flag:
                    sum_x_global_pragma_axis = bn_nc_cl0_axis
                else:
                    sum_x_global_pragma_axis = bn_bl12bl0_loopn_axis

                # get res_ub_rf_at_sum_x_axis
                res_ub_rf_at_sum_x_axis = multicore_axis
                if group_opt > 1:
                    res_ub_rf_at_sum_x_axis = bn_singlecore_group_opt_axis
                if cub_channel_wise_flag:
                    res_ub_rf_at_sum_x_axis = bn_singlecore_out2bl1_loopn_axis

                self._convbn1.set_sum_x_global(sum_x_global, sum_x_global_pragma_axis, res_ub_rf_at_sum_x_axis)

                return [cub_at_res_axis, cl0_at_res_axis, bl0_at_res_axis,
                        al1_at_res_axis, bl1_at_res_axis]

            res_axis_list = get_bn1_res_attach_axis()

            attach_axis_dict.update(
                {
                    "singlecore_out2al1_loopm_axis": singlecore_out2al1_loopm_axis,
                    "al12al0_loopm_axis": al12al0_loopm_axis,
                    "res_m_dim_axis": bn_batch_m_dim_fused % m_dim,
                    "singlecore_out2bl0_loopg_axis": singlecore_out2bl0_loopg_axis,
                    "res_group_dim_axis": bn_group_dim_axis
                }
                )

            return res_ub_rf, res_axis_list, bindcore_axis, res_pragma_axis

        def enable_unit_flag_pragma(cl0_ko, cl0_ki):
            # set condition to mark the last k loop for unit flag enable
            not_mark = (not self._dynamic_shape.flag) or self._groupopt_flag or (not is_support_fixpipe())
            if not_mark:
                return

            # the reduce aix reaches the last loop at cl0_k0 == K // ka_l0 - 1
            total_k1 = self._ci1_opt * self._kernel_h * self._kernel_w
            if self._c04.flag:
                total_k1 = ceil_div(total_k1, ConstValue.C04_CONST)
            unit_flag_condition = (cl0_ko.var == total_k1 // ka_al0 - 1)
            sch[cl0].pragma(cl0_ki, "unit_flag_condition", unit_flag_condition)

        def tile_tensor_cl0():
            """
            tile tensor cl0
            """
            if self._wino_conv.flag:
                cl0_reduce_wino, cl0_k1, cl0_k0 = cl0.op.reduce_axis
            else:
                cl0_k1, cl0_k0 = cl0.op.reduce_axis

            # split ma*m0
            cl0_ma_factor = ma_al0
            cl0_ma_factor = self._pooling_fusion.modify_cl0_m_factor(cl0_ma_factor)

            if self._wino_conv.flag:
                batch_idx, p1_idx, p2_idx, co1_opt_idx, m_align_idx, block_n0_idx = range(len(cl0.shape))
            elif self._split_w_flag:
                group_opt_idx, batch_idx, hout_idx, co1_opt_idx, m_align_idx, block_n0_idx = range(len(cl0.shape))
            else:
                group_opt_idx, batch_idx, co1_opt_idx, m_align_idx, block_n0_idx = range(len(cl0.shape))

            mc_split_axis_idx = m_align_idx
            cl0_mo, cl0_mi = sch[cl0].split(sch[cl0].op.axis[mc_split_axis_idx], cl0_ma_factor * self._block_m0)

            if bl0_tiling == []:
                cl0_co, cl0_ci = sch[cl0].split(sch[cl0].op.axis[co1_opt_idx], nparts=1)
            else:
                cl0_co, cl0_ci = sch[cl0].split(sch[cl0].op.axis[co1_opt_idx], nb_bl0)

            if multi_cl0_group and multi_bl0_group:
                cl0_go, cl0_gi = sch[cl0].split(cl0.op.axis[group_opt_idx], factor=group_bl0)

            # for reduce axis, al0 and bl0 should be the same
            cl0_ko, cl0_ki = sch[cl0].split(cl0_k1, ka_al0)
            enable_unit_flag_pragma(cl0_ko, cl0_ki)

            cl0_no, cl0_ni = sch[cl0].split(cl0.op.axis[batch_idx], 1)
            cl0_co0 = cl0.op.axis[block_n0_idx]
            if self._split_w_flag:
                cl0_ho, cl0_hi = sch[cl0].split(cl0.op.axis[hout_idx], nparts=al1_nparts[1])

            al0_at_cl0_axis_tmp = cl0_mo
            if self._wino_conv.flag:
                sch[cl0].reorder(cl0_no, # batch.outer
                                 cl0_ko, # k_1.outer
                                 cl0_co, # co1.outer
                                 cl0_mo,
                                 # =======L0C tiling=========
                                 cl0_ni,  # batch.inner = 1
                                 cl0.op.axis[p1_idx],  # p1_idx = 2
                                 cl0.op.axis[p2_idx],  # p2_idx = 2
                                 cl0_ci, # co1.inner = nb
                                 cl0_mi,  # m.inner = ma*m0
                                 cl0_co0,  # n0
                                 cl0_reduce_wino, # reduce wino
                                 cl0_ki,  # reduce k1 (ka = kb)
                                 cl0_k0)  # reduce k0
            elif self._inner_batch.flag:
                sch[cl0].reorder(cl0_ko,
                                 cl0_co,
                                 cl0_no,
                                 cl0_mo,
                                 cl0_ni,
                                 cl0_ci,
                                 cl0_mi,
                                 cl0_co0,
                                 cl0_ki,
                                 cl0_k0)
            elif self._pooling_fusion.flag:
                cl0_cio, cl0_cii = sch[cl0].split(cl0_ci, 1)
                sch[cl0].reorder(cl0_ko,
                                 cl0_co,
                                 cl0_mo,
                                 cl0_cio,
                                 cl0_ni, # 1
                                 cl0_cii,
                                 cl0_mi, # ma*m0
                                 cl0_co0, # co0
                                 cl0_ki, # ka = kb
                                 cl0_k0) # k0
            else:
                reorder_list = [cl0_no,
                                cl0_ko,
                                cl0_co,
                                cl0_mo,
                                #=======L0C tiling=========
                                cl0_ni,  # 1
                                cl0_ci, # nb
                                cl0_mi,  # ma*m0
                                cl0_co0,  # co0
                                cl0_ki,  # ka = kb
                                cl0_k0]  # k0
                if self._split_w_flag:
                    al0_at_cl0_axis_tmp = cl0_hi
                    reorder_list = [cl0_no,
                                    cl0_mo,
                                    cl0_ho,
                                    cl0_ko,  # al1 or bl1
                                    cl0_co,
                                    cl0_hi,
                                    # =======L0C tiling=========
                                    cl0_ni,  # 1
                                    cl0_ci,  # nb
                                    cl0_mi,  # ma*m0
                                    cl0_co0,  # co0
                                    cl0_ki,  # ka = kb
                                    cl0_k0]  # k0
                sch[cl0].reorder(*reorder_list)

            if multi_cl0_group and multi_bl0_group:
                if self._inner_batch.flag:
                    sch[cl0].reorder(cl0_go,
                                     cl0_co,
                                     cl0_gi,
                                     cl0_ko,
                                     cl0_no,
                                     cl0_mo)
                else:
                    reorder_list = [cl0_go,
                                    cl0_no,
                                    cl0_co,
                                    cl0_gi,
                                    cl0_ko,
                                    cl0_mo]
                    if self._split_w_flag:
                        reorder_list = [cl0_go,
                                        cl0_no,
                                        cl0_mo,
                                        cl0_ho,
                                        cl0_co,
                                        cl0_gi,
                                        cl0_ko,
                                        cl0_hi]
                    sch[cl0].reorder(*reorder_list)

            bl1_nparts_tmp = bl1_nparts[0]
            if self._dynamic_shape.flag:
                binary_attach_flag = self._dynamic_shape.tiling_case.get('attach_at_flag')
                bl1_attach_flag = binary_attach_flag.get("bl1_attach_flag")
                reuse_al1_flag = self._dynamic_shape.cal_reuse_al1(binary_attach_flag)
                if bl1_attach_flag == AttachMode.ATTACH_PASS:
                    bl1_nparts_tmp = 1 if not reuse_al1_flag else al1_nparts[0]
            else:
                outer_factor = max(al1_nparts[0], bl1_nparts[0])
                inner_factor = min(al1_nparts[0], bl1_nparts[0])
                if outer_factor % inner_factor != 0 and not self._sparse_4to2_flag:
                    err_man.raise_err_specific("conv2d", "illegal value of AL1_shape & BL1_shape")
                reuse_al1_flag = al1_nparts[0] <= bl1_nparts[0]

            # al1 not fullload, k1_al1 not factor of ci1, only happends in sparse_4to2 scenario now
            k1_al1_not_factor_flag = False
            if self._sparse_4to2_flag and not self._dynamic_shape.flag and \
                al1_tiling and ceil(ci1_opt, k1_al1) != ci1_opt:
                if ceil(k1_al1 * kernel_h * kernel_w, ka_al0) != k1_al1 * kernel_h * kernel_w:
                    log.warn("k_l0 must be factor of k_l1, check tiling please")
                k1_al1_not_factor_flag = True
                cl0_al1_factor = k1_al1 * kernel_h * kernel_w // ka_al0

            if not reuse_al1_flag:
                # for cl0_l1_loopk_outer_axis / cl0_bl1_loopk
                #     allocate bl1
                #     for cl0_l1_loopk_inner_axis / cl0_al1_loopk
                #         allocate al1
                #         for cl0_l0_loopk_axis
                if k1_al1_not_factor_flag:
                    cl0_l1_loopk_axis, cl0_l0_loopk_axis = sch[cl0].split(cl0_ko, cl0_al1_factor)
                else:
                    cl0_l1_loopk_axis, cl0_l0_loopk_axis = sch[cl0].split(cl0_ko, nparts=al1_nparts[0])
                cl0_l1_loopk_outer_axis, cl0_l1_loopk_inner_axis = sch[cl0].split(cl0_l1_loopk_axis,
                                                                                  nparts=bl1_nparts_tmp)
            else:
                # for cl0_l1_loopk_outer_axis / cl0_al1_loopk
                #     allocate al1
                #     for cl0_l1_loopk_inner_axis / cl0_bl1_loopk
                #         allocate bl1
                #         for cl0_l0_loopk_axis
                if k1_al1_not_factor_flag:
                    cl0_l1_loopk_outer_axis, cl0_l1_loopk_axis = sch[cl0].split(cl0_ko, cl0_al1_factor)
                    cl0_l1_loopk_inner_axis, cl0_l0_loopk_axis = \
                        sch[cl0].split(cl0_l1_loopk_axis, k1_bl1 * kernel_h * kernel_w // kb_bl0)
                else:
                    cl0_l1_loopk_axis, cl0_l0_loopk_axis = sch[cl0].split(cl0_ko, nparts=bl1_nparts_tmp)
                    cl0_l1_loopk_outer_axis, cl0_l1_loopk_inner_axis = sch[cl0].split(cl0_l1_loopk_axis,
                                                                                      nparts=al1_nparts[0])
            k_outer_list = [cl0_l1_loopk_outer_axis, cl0_l1_loopk_inner_axis, cl0_l0_loopk_axis]

            def get_cl0_attach_axis():
                """
                prepare cl0 attach axis for compute_at
                """
                # get al0_at_cl0_axis
                al0_at_cl0_axis = al0_at_cl0_axis_tmp

                # get bl0_at_cl0_axis
                bl0_at_cl0_axis = cl0_co
                if self._pooling_fusion.flag:
                    bl0_at_cl0_axis = cl0_cio

                # get al1_at_cl0_axis and bl1_at_cl0_axis
                if not reuse_al1_flag:
                    al1_at_cl0_axis = cl0_l1_loopk_inner_axis
                    bl1_at_cl0_axis = cl0_l1_loopk_outer_axis
                else:
                    al1_at_cl0_axis = cl0_l1_loopk_outer_axis
                    bl1_at_cl0_axis = cl0_l1_loopk_inner_axis

                if bl1_tiling:
                    if bl1_nparts[0] == 1 and multi_cl0_group:
                        bl1_at_cl0_axis = cl0_co

                if bl1_tiling == [] and multi_cl0_group:
                    bl1_at_cl0_axis = cl0_co

                return [al0_at_cl0_axis, bl0_at_cl0_axis,
                        al1_at_cl0_axis, bl1_at_cl0_axis]

            cl0_axis_list = get_cl0_attach_axis()
            cl0_pragma_axis = cl0_ni

            attach_axis_dict.update(
                {
                    "cl0_mo": cl0_mo
                }
                )

            return k_outer_list, cl0_axis_list, cl0_pragma_axis

        def config_fixpipe_attach_axis():
            def get_fixpipe_fb_attach_axis():
                fixpipe_fullload_flag = self._fixpipe_fusion.get_fixpipe_fullload_flag(group_opt * co1_opt)
                if self._tiling.get("INPUT_L1_FB_param") == "all" and fixpipe_fullload_flag:
                    self._fixpipe_fusion.fbparams_fullload_flag = True
                    return bindcore_axis
                return fixpipe_slice_axis

            def get_eltwise_attach_axis():
                if self._tiling.get("INPUT_L1_eltwise_param") == "slice":
                    return cub_at_res_axis
                return bindcore_axis

            fixpipe_slice_axis = cl0_at_res_axis if self._mixl2_flag else cub_at_res_axis
            fixpipe_fb_attach_axis = get_fixpipe_fb_attach_axis()
            fixpipe_l1_attach_axis = \
                bindcore_axis if self._tiling.get("INPUT_L1_FB_param") == "all" else fixpipe_slice_axis
            eltwise_attach_axis = get_eltwise_attach_axis()

            return fixpipe_fb_attach_axis, fixpipe_l1_attach_axis, eltwise_attach_axis

        def cl0_compute_at():
            sch[cl0].compute_at(sch[res], cl0_at_res_axis)

        def cl0_set_store_predicate():
            if self._al0_cl0_k_axis_select_flag:
                # splitw feature's compute is not same with common compute
                # im2col fractal compute and l0c compute mad
                # we should guarantee to get the right k axis based on compute logic
                if self._split_w_flag:
                    cl0_group_idx, cl0_batch_idx, cl0_ho_idx, cl0_co1_idx, cl0_wo_aligned_idx, cl0_co0_idx = \
                    range(len(cl0.shape))
                else:
                    cl0_group_idx, cl0_batch_idx, cl0_co1_idx, cl0_howo_idx, cl0_co0_idx = range(len(cl0.shape))

                # while cl0 reduce are the same in splitw feature or others.
                cl0_reduce_axis_k1_idx, cl0_reduce_axis_k0_idx = range(len(cl0.op.reduce_axis))
                # select the real k value to join the calculation. Otherwise, random data will be involved.
                reduce_k1 = ci1_opt * kernel_w * kernel_h
                reduce_c1hw = self._in_c1 * kernel_w * kernel_h
                mad_select_condition = tvm.any(
                    tvm.all(cl0.op.axis[cl0_group_idx].var * reduce_k1 + \
                    cl0.op.reduce_axis[cl0_reduce_axis_k1_idx].var < reduce_c1hw)
                )
                sch[cl0].set_store_predicate(mad_select_condition, partition=True)

        def cl0_buffer_tile():
            def group_doubleout_nznd_buffer_tile():
                # group doubleout(which contains nz2nd) case, need buffer tile to make sure infer bound correct
                group_opt_idx, _, co1_opt_idx, _, _ = range(len(cl0.shape))
                cl0_buffer_tile_list = []
                for _ in range(len(cl0.shape)):
                    cl0_buffer_tile_list.append((None, None))
                log.debug('axes required to calculate co1_opt buffer tile:'
                          'res_n_dim_axis: {}, singlecore_out2bl1_loopn_axis: {}, bl12bl0_loopn_axis: {}'.
                          format(attach_axis_dict.get("res_n_dim_axis"),
                                 attach_axis_dict.get("singlecore_out2bl1_loopn_axis"),
                                 attach_axis_dict.get("bl12bl0_loopn_axis")))
                base_co1_idx_ioi = nc_cl0  # base of axis co1_idx_inner_outer_inner
                base_co1_idx_iooi = base_co1_idx_ioi * ceil_div(ceil_div(co1_opt, nc_cl0), bl1_nparts[1])
                base_co1_idx_iooo = base_co1_idx_iooi * ceil_div(bl1_nparts[1], n_dim)
                co1_opt_offset = attach_axis_dict.get("res_n_dim_axis") * base_co1_idx_iooo + \
                                 attach_axis_dict.get("singlecore_out2bl1_loopn_axis") * base_co1_idx_iooi + \
                                 attach_axis_dict.get("bl12bl0_loopn_axis") * base_co1_idx_ioi
                cl0_buffer_tile_list[co1_opt_idx] = (co1_opt_offset, nc_cl0)

                log.debug('axes required to calculate group_opt buffer tile:'
                          'res_group_dim_axis: {}, singlecore_out2bl0_loopg_axis: {}, res_bl0_group_opt_axis: {}'.
                          format(attach_axis_dict.get("res_group_dim_axis"),
                                 attach_axis_dict.get("singlecore_out2bl0_loopg_axis"),
                                 attach_axis_dict.get("res_bl0_group_opt_axis")))
                base_group_idx_ioi = group_cl0  # base of axis co1_idx_inner_outer_inner
                base_group_idx_iooi = base_group_idx_ioi
                base_group_idx_iooo = base_group_idx_iooi * ceil_div(ceil_div(group_opt, group_cl0), group_dim)
                group_opt_offset = attach_axis_dict.get("res_group_dim_axis") * base_group_idx_iooo + \
                                   attach_axis_dict.get("singlecore_out2bl0_loopg_axis") * base_group_idx_iooi + \
                                   attach_axis_dict.get("res_bl0_group_opt_axis") * base_group_idx_ioi
                cl0_buffer_tile_list[group_opt_idx] = (group_opt_offset, group_cl0)

                for _ in range(len(cl0.op.reduce_axis)):
                    cl0_buffer_tile_list.append((None, None))

                sch[cl0].buffer_tile(*cl0_buffer_tile_list)

            nc_not_divisible_flag = not self._dynamic_shape.flag and nc_cl0 % nc_factor_cub
            group_doubleout_nz2nd_flag = not self._dynamic_shape.flag and self._fixpipe_fusion.multiout_flag and \
                self._fixpipe_fusion.fixpipe_nz2nd_flag and self._groupopt_flag
            if (self._dynamic_shape.flag or self._split_w_flag) or nc_not_divisible_flag:
                if self._wino_conv.flag:
                    batch_idx, p1_idx, p2_idx, co1_opt_idx, m_align_idx, block_n0_idx = range(len(cl0.shape))
                elif self._split_w_flag:
                    group_idx, batch_idx, hout_idx, co1_opt_idx, m_align_idx, block_n0_idx = range(len(cl0.shape))
                else:
                    group_idx, batch_idx, co1_opt_idx, m_align_idx, block_n0_idx = range(len(cl0.shape))
                cl0_buffer_tile_list = []
                for i in range(len(cl0.shape)):
                    cl0_buffer_tile_list.append((None, None))

                if self._split_w_flag:
                    split_w_out2cl0_loopm_axis = attach_axis_dict.get("split_w_out2cl0_loopm_axis")
                    cl0_m_bound = ma_al0 * self._block_m0
                    offset = split_w_out2cl0_loopm_axis.var * cl0_m_bound
                    cl0_buffer_tile_list[m_align_idx] = (offset, cl0_m_bound)
                    cl0_buffer_tile_list[hout_idx] = (None, 1)

                if (self._groupopt_flag and self._dynamic_shape.flag) or nc_not_divisible_flag:
                    # fix wrong inferbound because of cub_compute_floormod
                    log.debug('axes required to calculate co1_opt buffer tile min:'
                              'res_n_dim_axis: {}, singlecore_out2bl1_loopn_axis: {}, bl12bl0_loopn_axis: {}'.
                              format(attach_axis_dict.get("res_n_dim_axis"),
                                     attach_axis_dict.get("singlecore_out2bl1_loopn_axis"),
                                     attach_axis_dict.get("bl12bl0_loopn_axis")))
                    base_co1_idx_ioi = nc_cl0  # base of axis co1_idx_inner_outer_inner
                    base_co1_idx_iooi = base_co1_idx_ioi * ceil_div(ceil_div(co1_opt, nc_cl0), bl1_nparts[1])
                    base_co1_idx_iooo = base_co1_idx_iooi * ceil_div(bl1_nparts[1], n_dim)
                    co1_opt_offset = attach_axis_dict.get("res_n_dim_axis") * base_co1_idx_iooo +\
                        attach_axis_dict.get("singlecore_out2bl1_loopn_axis") * base_co1_idx_iooi +\
                        attach_axis_dict.get("bl12bl0_loopn_axis") * base_co1_idx_ioi
                    cl0_buffer_tile_list[co1_opt_idx] = (co1_opt_offset, nc_cl0)

                for i in range(len(cl0.op.reduce_axis)):
                    cl0_buffer_tile_list.append((None, None))

                sch[cl0].buffer_tile(*cl0_buffer_tile_list)

            elif group_doubleout_nz2nd_flag:
                group_doubleout_nznd_buffer_tile()

        def al0_buffer_tile():
            def gconv_innerbatch_buffer_tile():
                """
                ONLY in gconv + innerbatch
                buffer_tile for row_major cin_1 axis to fix tail block load3d batchidx offset
                """
                fm_ci1 = self._in_c1
                ci1_opt_per_group = self._para_dict.get("c1_opt")
                # singlecore group_opt axis
                ci1_extent = attach_axis_dict.get("singlecore_out2bl0_loopg_axis").var * ci1_opt_per_group
                al1_ci1_max = ci1_opt_per_group

                # singlecore out2al1 k axis when al1 k not full load
                al1_tiling = self._tiling.get("AL1_shape")
                if al1_tiling and al1_nparts[0] != 1:
                    al1_ci1_max = k1_al1
                    ci1_extent += al1_at_cl0_axis.var * al1_ci1_max

                # multi core group_opt axis
                group_dim = self._tiling.get("block_dim")[3]
                if group_dim != 1:
                    fm_g = ceil_div(fm_ci1, ci1_opt_per_group)
                    ci1_per_core = ceil_div(fm_g, group_dim) * ci1_opt_per_group
                    ci1_extent += attach_axis_dict.get("res_group_dim_axis").var * ci1_per_core

                sch[fmap_row_major].buffer_tile(
                    (None, None),
                    (None, None),
                    (None, None),
                    (None, tvm.min(fm_ci1 - ci1_extent, al1_ci1_max)),
                    (None, None),
                    (None, None),
                    (None, None)
                )

            if not self._dynamic_shape.flag and not self._l0a_load2d.flag and \
                    self._inner_batch.flag and group_opt > 1 and ci1_opt * group_opt != self._in_c1:
                # when fm cin can not divided by group_opt, and innerbatch enable,
                # the extent of tail block of row_major can not cal correct, need buffer_tile to cal extent
                gconv_innerbatch_buffer_tile()

        def al1_buffer_tile():
            def conv1d_split_w_al1_buffer_tile():
                # FULL LOAD situation do not need buffer tile.
                if al1_tiling and al0_tiling:
                    _, multi_m_al1, _, _ = al1_tiling
                    ma_al0, _, _, _, _, _ = al0_tiling
                else:
                    return

                if not self._conv_param.binary_mode:
                    if self._convbn1.flag or self._fixpipe_fusion.get_nz2nd_flag():
                        howo = res.op.input_tensors[0].shape[2].value
                    else:
                        howo = res.shape[2].value
                if self._conv_param.binary_mode:
                    howo = self._out_width * self._out_height
                multi_m_al1 = ceil_div(ceil_div(ceil(howo, self._block_m0),
                                                ma_al0 * self._block_m0),
                                       al1_nparts[1])

                # get al1 shape and for easy expansion
                if self._conv_param.binary_mode:
                    group_idx, batch_idx, ci1_opt_idx, in_height_idx, in_width_idx, in_c0 = range(len(al1.shape))
                else:
                    batch_idx, ci1_opt_idx, in_height_idx, in_width_idx, in_c0 = range(len(al1.shape))
                al1_buffer_tile_list = []
                for i in range(len(al1.shape)):
                    al1_buffer_tile_list.append((None, None))

                binary_al1_attach_flag = self._dynamic_shape.attach_flags.get("al1_attach_flag")
                _, _, al1_attach_res_flag = get_al1_attach_info()
                static_al1_attach_res_flag = (not self._conv_param.binary_mode) and al1_attach_res_flag
                # two axis before al1 if al1_attach_res
                if static_al1_attach_res_flag or binary_al1_attach_flag == AttachMode.ATTACH_RES:
                    # for [res_m_dim_axis] howo_idx_outer_outer_outer_outer
                    #     for [singlecore_out2al1_loopm_axis] howo_idx_outer_outer_inner
                    #         allocate al1
                    # get al1 axis before al1
                    howo = multi_m_al1 * ma_al0 * self._block_m0
                    temp_axis = ceil_div(al1_nparts[1], m_dim) * attach_axis_dict.get("res_m_dim_axis") + \
                                attach_axis_dict.get("singlecore_out2al1_loopm_axis")
                    temp_offset = tvm.min(temp_axis * howo, (self._out_width - 1))
                    # calculate buffer_tile extend
                    howo = tvm.min(howo, self._out_width - temp_axis * howo)
                    w_in = (howo - 1) * self._stride_w + self._filter_w_dilation
                    # get w_in_offset from howo_offset
                    al1_offset = temp_offset * self._stride_w - self._conv_param.padding[2]
                    al1_buffer_tile_list[in_width_idx] = (al1_offset, w_in)

                # three axis before al1 if al1_attach_cl0
                elif (not self._conv_param.binary_mode) or binary_al1_attach_flag == AttachMode.ATTACH_CL0:
                    # for [res_m_dim_axis] howo_idx_outer_outer_outer_outer
                    #     for [singlecore_out2al1_loopm_axis] howo_idx_outer_outer_inner
                    #         for [al12al0_loopm_axis] howo_idx_outer_inner
                    #              allocate cl0
                    #                  allocate al1
                    # get al1 axis before al1
                    howo = ma_al0 * self._block_m0
                    # when al1 compute at cl0, multi m can not load in L1, multi_m_al1 = 1
                    temp_axis = (ceil_div(al1_nparts[1], m_dim) * attach_axis_dict.get("res_m_dim_axis") + \
                                attach_axis_dict.get("singlecore_out2al1_loopm_axis")) * multi_m_al1 + \
                                attach_axis_dict.get("al12al0_loopm_axis")

                    temp_offset = tvm.min(temp_axis * howo, self._out_width - 1)
                    # calculate buffer_tile extend
                    howo = tvm.min(howo, self._out_width - temp_axis * howo)
                    w_in = (howo - 1) * self._stride_w + self._filter_w_dilation
                    # get w_in_offset from howo_offset
                    al1_offset = temp_offset * self._stride_w - self._conv_param.padding[2]
                    al1_buffer_tile_list[in_width_idx] = (al1_offset, w_in)

                sch[al1].buffer_tile(*al1_buffer_tile_list)

            # conv1d_split_w feature, al1 buffer tile.
            if self._conv_param.conv1d_split_w_flag:
                conv1d_split_w_al1_buffer_tile()

        def al0_compute_at():
            sch[al0].compute_at(sch[cl0], al0_at_cl0_axis)

        def al0_set_store_predicate():
            if self._al0_cl0_k_axis_select_flag:
                # splitw feature's compute is not same with common compute
                # im2col fractal compute and l0c compute mad
                # we should guarantee to get the right k axis based on compute logic
                if self._l0a_layout_zn_flag:
                    if self._split_w_flag:
                        al0_group_idx, al0_n_idx, al0_ho_idx, al0_k1_idx, al0_wo_align_idx, al0_m0_idx, al0_k0_idx = \
                        range(len(al0.shape))
                    else:
                        al0_group_idx, al0_n_idx, al0_k1_idx, al0_m1_idx, al0_m0_idx, al0_k0_idx = range(len(al0.shape))
                else:
                    if self._split_w_flag:
                        al0_group_idx, al0_n_idx, al0_ho_idx, al0_wo_align_idx, al0_k1_idx, al0_m0_idx, al0_k0_idx = \
                        range(len(al0.shape))
                    else:
                        al0_group_idx, al0_n_idx, al0_m1_idx, al0_k1_idx, al0_m0_idx, al0_k0_idx = range(len(al0.shape))
                # select the real k value to join the calculation. Otherwise, random data will be involved.
                reduce_k1 = ci1_opt * kernel_w * kernel_h
                reduce_c1hw = self._in_c1 * kernel_w * kernel_h
                mad_select_condition = tvm.any(
                    tvm.all(al0.op.axis[al0_group_idx].var * reduce_k1 + al0.op.axis[al0_k1_idx].var < reduce_c1hw)
                )
                sch[al0].set_store_predicate(mad_select_condition, partition=True)

            # when splitw conv1d case(binary mode), make sure load3d m equals to fmap_w in tail block
            if self._conv_param.conv1d_split_w_flag and \
                self._dynamic_shape.attach_flags.get("al1_attach_flag") == AttachMode.ATTACH_RES:
                al0_group_idx, _, al0_m1_idx, al0_k1_idx, _, _ = range(len(al0.shape))
                # total m not larger than ori width of conv1d case
                load3d_w_tail_condition = tvm.any(
                    tvm.all(al0.op.axis[al0_m1_idx].var * self._block_m0 < ceil(self._out_width, self._block_m0))
                )
                sch[al0].set_store_predicate(load3d_w_tail_condition, partition=True)

        def select_weight_full_load_attach_axis(at_res_axis, bind_core_axis):
            if self._tiny_weight_fractal_flag:
                return bind_core_axis

            return at_res_axis

        def bl0_compute_at():
            """
            Handle bl0 attach.
            """
            if self._dynamic_shape.flag:
                binary_bl0_attach_flag = self._dynamic_shape.attach_flags.get(BinaryTilingKey.BL0_ATTACH_FLAG)
                if binary_bl0_attach_flag == AttachMode.ATTACH_FULL_LOAD:
                    bl0_attach_axis = select_weight_full_load_attach_axis(bl0_at_res_axis, bindcore_axis)
                    sch[bl0].compute_at(sch[res], bl0_attach_axis)
                elif binary_bl0_attach_flag == AttachMode.ATTACH_CL0_FOR_BL0:
                    sch[bl0].compute_at(sch[cl0], bl0_at_cl0_axis)
            else:
                if self._pooling_fusion.flag:
                    sch[bl0].compute_at(sch[cl0], bl0_at_cl0_axis)
                elif bl0_tiling or (bl0_tiling == [] and multi_cl0_group):
                    if multi_bl0_group and group_cl0 == 1:
                        sch[bl0].compute_at(sch[res], bl0_at_res_axis)
                    else:
                        sch[bl0].compute_at(sch[cl0], bl0_at_cl0_axis)
                else:
                    sch[bl0].compute_at(sch[res], bl0_at_res_axis)

        def get_al1_attach_info():
            """
            Get the consumer tensor of al1 and the target axis to be attached.
            """
            al1_attach_res_flag = False
            if multi_cl0_group:
                return cl0, al1_at_cl0_axis, al1_attach_res_flag

            if self._split_w_flag and al1_tiling:
                return cl0, al1_at_cl0_axis, al1_attach_res_flag

            if al1_tiling:
                if al1_nparts[0] != 1:
                    return cl0, al1_at_cl0_axis, al1_attach_res_flag
            al1_attach_res_flag = True
            return res, al1_at_res_axis, al1_attach_res_flag

        def al1_compute_at():
            """
            Handle al1 attach.
            """
            if self._dynamic_shape.flag:
                binary_al1_attach_flag = self._dynamic_shape.attach_flags.get("al1_attach_flag")
                if self._split_w_flag and binary_al1_attach_flag != AttachMode.ATTACH_FULL_LOAD:
                    binary_al1_attach_flag = AttachMode.ATTACH_CL0
                if binary_al1_attach_flag in [AttachMode.ATTACH_FULL_LOAD, AttachMode.ATTACH_RES]:
                    sch[al1].compute_at(sch[res], al1_at_res_axis)
                elif binary_al1_attach_flag == AttachMode.ATTACH_CL0:
                    sch[al1].compute_at(sch[cl0], al1_at_cl0_axis)
                else:
                    err_man.raise_err_specific("conv2d",
                                               ("binary bl1 compute val not support[%s]" % binary_al1_attach_flag))
            else:
                consumer, target_axis, _ = get_al1_attach_info()
                sch[al1].compute_at(sch[consumer], target_axis)
                if fmap_row_major is not None:
                    sch[fmap_row_major].compute_at(sch[consumer], target_axis)
                if al1_load2d_real is not None:
                    sch[al1_load2d_real].compute_at(sch[consumer], target_axis)
                if al1_load2d_zero is not None:
                    sch[al1_load2d_zero].compute_at(sch[consumer], target_axis)

            if self._im2col_dma.flag:
                if is_support_fixpipe():
                    consumer, target_axis, _ = get_al1_attach_info()
                    sch[al1_zero].compute_at(sch[consumer], target_axis)
                    sch[al1_dma_im2col].compute_at(sch[consumer], target_axis)
                    sch[al1_virtual_add].compute_at(sch[consumer], target_axis)

        def bl1_compute_at():
            """
            Handle bl1 attach.
            """
            def get_bl1_attach_info():
                """
                Get the consumer tensor of bl1 and the target axis to be attached.
                """
                if self._inner_batch.flag:
                    return res, bl1_at_res_axis

                if bl1_tiling:
                    if bl1_nparts[0] != 1:
                        # cl0_gi is under cl0_ko
                        return cl0, bl1_at_cl0_axis
                    if bl1_nparts[0] == 1 and multi_cl0_group:
                        return cl0, bl1_at_cl0_axis
                    return res, bl1_at_res_axis

                if bl1_tiling == [] and multi_cl0_group:
                    return cl0, bl1_at_cl0_axis

                return res, bl1_at_res_axis

            if self._dynamic_shape.flag:
                binary_bl1_attach_flag = self._dynamic_shape.attach_flags.get(BinaryTilingKey.BL1_ATTACH_FLAG)
                if binary_bl1_attach_flag in [AttachMode.ATTACH_FULL_LOAD, AttachMode.ATTACH_RES]:
                    bl1_attach_axis = select_weight_full_load_attach_axis(bl1_at_res_axis, bindcore_axis)
                    sch[bl1].compute_at(sch[res], bl1_attach_axis)
                elif binary_bl1_attach_flag == AttachMode.ATTACH_CL0:
                    sch[bl1].compute_at(sch[cl0], bl1_at_cl0_axis)
                elif binary_bl1_attach_flag == AttachMode.ATTACH_PASS:
                    pass
                else:
                    err_man.raise_err_specific("conv2d",
                                               ("binary bl1 compute val not support[%s]" % binary_bl1_attach_flag))
            else:
                if bl1_tiling is not None:
                    consumer, target_axis = get_bl1_attach_info()
                    sch[bl1].compute_at(sch[consumer], target_axis)
                    if self._sparse_4to2_flag:
                        weight_index_l1 = tensor_param.get("weight_index_l1")
                        sch[weight_index_l1].compute_at(sch[consumer], target_axis)

        def cub_compute_at():
            """
            Handle cub attach.
            """
            if not is_support_fixpipe() and cub is not None:
                sch[cub].compute_at(sch[res], cub_at_res_axis)

        def cub_buffer_tile():
            if self._split_w_flag and self._dynamic_shape.flag and cub is not None:
                split_w_out2cl0_loopm_axis = attach_axis_dict.get("split_w_out2cl0_loopm_axis")
                _, _, m_idx, _ = range(len(cub.shape))
                buffer_tile_list = []
                for i in range(len(cub.shape)):
                    buffer_tile_list.append((None, None))

                bound = ma_al0 * self._block_m0
                offset = split_w_out2cl0_loopm_axis.var * bound
                buffer_tile_list[m_idx] = (offset, bound)
                sch[cub].buffer_tile(*buffer_tile_list)

        def cub_remove_padded_column_compute_at():
            if self._v200_width_out_1_flag:
                sch[self._tensor_map[Conv2dTensorName.REMOVE_PADDED_COLUMN]].compute_at(sch[res], cub_at_res_axis)

        def bias_compute_at():
            """
            Handle bias attach.
            """
            def bias_l1_bt_compute_at():
                """
                Handle bias on L1 and BT
                """
                bias_l1_attach_axis = cl0_at_res_axis if bias_slice_flag else bindcore_axis
                bias_bt_attach_axis = get_bias_bt_attach_axis()

                sch[bias_l1].compute_at(sch[res], bias_l1_attach_axis)
                if self._conv_param.bias_init_align_dim_flag:
                    bias_l1_real = tensor_param["bias_l1_real"]
                    bias_l1_zero = tensor_param["bias_l1_zero"]
                    sch[bias_l1_real].compute_at(sch[res], bias_l1_attach_axis)
                    sch[bias_l1_zero].compute_at(sch[res], bias_l1_attach_axis)
                sch[bias_bt].compute_at(sch[res], bias_bt_attach_axis)

            def bias_ub_compute_at():
                """
                Handle bias on UB
                """
                if cub_bias_add is not None:
                    sch[cub_bias_add].compute_at(sch[res], cub_at_res_axis)
                # bias_slice_flag: 0 --> bias full load, 1 --> bias non-full load
                if bias_slice_flag:
                    sch[bias_ub].compute_at(sch[res], cl0_at_res_axis)
                else:
                    sch[bias_ub].compute_at(sch[res], bindcore_axis)

            def get_bias_bt_attach_axis():
                # the size of bias table is 1KB, and it can store up tp 256 biases for B32 type
                if self._dynamic_shape.flag or bias_slice_flag or group_opt * co1_opt > 16:
                    return cl0_at_res_axis
                return bindcore_axis

            def bias_ub_l1_bt_compute_at():
                """
                Perform the compute at on bias when nd2nz instruction not supported and need set zero in ub.
                """
                bias_ub = tensor_param.get("bias_ub")
                bias_ub_attach_axis = cl0_at_res_axis if bias_slice_flag else bindcore_axis
                bias_l1_attach_axis = bias_ub_attach_axis
                bias_bt_attach_axis = get_bias_bt_attach_axis()
                sch[bias_ub].compute_at(sch[res], bias_ub_attach_axis)
                sch[bias_l1].compute_at(sch[res], bias_l1_attach_axis)
                sch[bias_bt].compute_at(sch[res], bias_bt_attach_axis)

            if not self._bias_flag:
                return
            if is_support_fixpipe():
                if self._bias_set_zero_in_ub_flag:
                    bias_ub_l1_bt_compute_at()
                else:
                    bias_l1_bt_compute_at()
            else:
                bias_ub_compute_at()

        def bias_buffer_tile():
            if self._split_w_flag and self._dynamic_shape.flag and cub_bias_add is not None:
                split_w_out2cl0_loopm_axis = attach_axis_dict.get("split_w_out2cl0_loopm_axis")
                _, _, m_idx, _ = range(len(cub_bias_add.shape))
                buffer_tile_list = []
                for i in range(len(cub_bias_add.shape)):
                    buffer_tile_list.append((None, None))

                bound = ma_al0 * self._block_m0
                offset = split_w_out2cl0_loopm_axis.var * bound
                buffer_tile_list[m_idx] = (offset, bound)
                sch[cub_bias_add].buffer_tile(*buffer_tile_list)

        def get_eltwise_ub_fusion_buffer_tile_tiling_info():
            buffer_tile_info = {
                "attach_axis_dict" : attach_axis_dict,
                "al1_nparts" : al1_nparts,
                "m_dim" : m_dim,
                "ma_al0" : ma_al0,
                "block_m0" : self._block_m0,
            }
            if al1_tiling:
                buffer_tile_info["multi_m_al1"] = multi_m_al1
            else:
                # split_w multi_m_al1 when al1 full load
                buffer_tile_info["multi_m_al1"] = ceil_div(self._out_height, m_dim)
            return buffer_tile_info

        def eltwise_ub_fusion_buffer_tile():
            if self._split_w_flag and not self._dynamic_shape.flag:
                buffer_tile_tiling_info = get_eltwise_ub_fusion_buffer_tile_tiling_info()
                self._eltwise_ub_fusion.res_buffer_tile(sch, res, buffer_tile_tiling_info)

        def check_weight_attach_flag():
            """
            Check bl1, bl0 attach flag combination in tiny_weight_fractal scene.
            """
            if not self._tiny_weight_fractal_flag:
                return

            if not self._dynamic_shape.flag:
                return

            binary_bl1_attach_flag = self._dynamic_shape.attach_flags.get(BinaryTilingKey.BL1_ATTACH_FLAG)
            binary_bl0_attach_flag = self._dynamic_shape.attach_flags.get(BinaryTilingKey.BL0_ATTACH_FLAG)

            valid_bl1_attach = (AttachMode.ATTACH_PASS, AttachMode.ATTACH_FULL_LOAD)
            if binary_bl1_attach_flag not in valid_bl1_attach:
                err_man.raise_err_scene_equal_limitation("pooling", "bl1_attach_flag",
                                                         "ATTACH_PASS or ATTACH_FULL_LOAD")

            if binary_bl0_attach_flag == AttachMode.ATTACH_FULL_LOAD and \
                    not binary_bl1_attach_flag == AttachMode.ATTACH_PASS:
                err_man.raise_err_specific("pooling", "bl1 attach needs be skipped when bl0 full loads.")

            if binary_bl1_attach_flag == AttachMode.ATTACH_CL0_FOR_BL0 and \
                    not binary_bl1_attach_flag == AttachMode.ATTACH_FULL_LOAD:
                err_man.raise_err_specific("pooling", "bl1 attach needs be full loaded when bl0 non-full loads.")

        def check_fully_spilt_flag():
            # 1971 static scene will occur not_fully_spilt condition.
            # When the pass derives the formula, the if likely condition is lost.
            # As a result, the L1 space calculation exceeds the tiling expectation.
            cout = self._para_dict.get("weight_ori_shape_nchw")[0]
            co1_per_core = ((cout + n_dim - 1) // n_dim)
            nc = cl0.shape[3]  # 3：nc index in cl0 shape
            # condition 0: conv+bn
            # condition 1: Cout % ndim != 0
            # condition 2: co1_per_core % nc != 0
            return (cout % n_dim != 0) and (co1_per_core % nc != 0)

        #==========================parse tiling==================================
        al1_tiling = self._tiling["AL1_shape"]
        bl1_tiling = self._tiling["BL1_shape"]
        al0_tiling = self._tiling["AL0_matrix"]
        bl0_tiling = self._tiling["BL0_matrix"]
        cl0_tiling = self._tiling["CL0_matrix"]
        cub_tiling = self._tiling["CUB_matrix"]
        aub_tiling = self._tiling["AUB_shape"]
        pingpong_buffer = self._tiling["manual_pingpong_buffer"]
        cub_channel_wise_flag = False
        if self._dynamic_shape.flag:
            bias_slice_flag = self._tiling["BIAS_channel_wise_flag"]
        else:
            cub_channel_wise_flag = self._tiling["CUB_channel_wise_flag"]
            bias_slice_flag = self._tiling["INPUT_L1_BT_param"] == "slice"
            control_reorder_flag = self._tiling["control_reorder_flag"]
        batch = self._batch
        kernel_h = self._kernel_h
        kernel_w = self._kernel_w
        dilate_h = self._dilate_h
        dilate_w = self._dilate_w
        block_k0 = self._block_k0
        group_opt = self._group_opt
        ci1_opt = self._ci1_opt
        co1_opt = self._co1_opt
        out_hw = self._out_hw

        # only bl1_tiling can be None, only al0_tiling cannot be []
        if al0_tiling:
            ma_al0, ka_al0, _, _, _, _ = al0_tiling

        if bl0_tiling:
            kb_bl0, nb_bl0, _, _, _, group_bl0 = bl0_tiling

        if cl0_tiling:
            nc_cl0, mc_cl0, m0_cl0, _, batch_cl0, group_cl0 = cl0_tiling

        if cub_tiling:
            nc_factor_cub, mc_factor_cub, m0_cub, _, _, _ = cub_tiling

        batch_al1 = 1

        if al1_tiling:
            k_al1, multi_m_al1, batch_al1, _ = al1_tiling
            if not self._dynamic_shape.flag:
                k1_al1 = k_al1 // (((kernel_h - 1)*dilate_h + 1)*((kernel_w - 1)*dilate_w + 1)*block_k0)
            else:
                k1_al1 = k_al1
            if k1_al1 == 0:
                k1_al1 = 1
        if bl1_tiling:
            k_bl1, multi_n_bl1, _, _ = bl1_tiling
            if not self._dynamic_shape.flag:
                k1_bl1 = k_bl1 // (kernel_h*kernel_w*block_k0)
            else:
                k1_bl1 = k_bl1

        batch_dim, n_dim, m_dim, group_dim = self._tiling["block_dim"]

        #==========================calculate various coefficients==================================
        multi_bl0_group = bl0_tiling and group_bl0 > 1
        multi_cl0_group = group_cl0 > 1

        if multi_bl0_group or multi_cl0_group:
            cl0_factor = [1, ceil_div(out_hw, mc_cl0*m0_cl0)]
        else:
            cl0_factor = [ceil_div(co1_opt, nc_cl0), ceil_div(out_hw, mc_cl0*m0_cl0)]
            if self._wino_conv.flag:
                cl0_factor[-1] = ceil_div(self._wino_conv.wino_res_howo, mc_cl0 * m0_cl0)
            elif self._split_w_flag:
                cl0_factor[-1] = ceil_div(ceil(self._out_width, 16), mc_cl0*m0_cl0)

        cl0_factor = self._pooling_fusion.modify_cl0_factor(cl0_factor)

        if al1_tiling:
            # Tuscany & Milan both support split k axis in binary mode, dma scene.
            if self._conv_param.binary_mode:
                if self._im2col_dma.flag:
                    al1_nparts = [ci1_opt * kernel_h * kernel_w // k1_al1, ceil_div(cl0_factor[1], multi_m_al1)]
                else:
                    al1_nparts = [self._dynamic_shape.cache_tiling[TilingDataKey.KAL1_FACTOR],
                              ceil_div(cl0_factor[1], multi_m_al1)]
            else:
                al1_nparts = [ceil_div(ci1_opt, k1_al1), ceil_div(cl0_factor[1], multi_m_al1)]

            if self._split_w_flag:
                al1_nparts[-1] = ceil_div(self._out_height, multi_m_al1)
            elif self._im2col_dma.split_k1_flag:
                min_al1_ci1 = get_min_al1_ci1(self._sparse_4to2_flag, self._in_c1, self._kernel_h, self._kernel_w)
                al1_nparts[0] = ceil_div(ci1_opt, min_al1_ci1) * kernel_h * kernel_w
        else: # al1 full load
            al1_nparts = [1, 1]

        if bl1_tiling:
            if not self._dynamic_shape.flag:
                if cl0_factor[0] % multi_n_bl1 != 0:
                    err_man.raise_err_specific("conv2d", "second value of BL1_shape should be factor of n block num")

            if self._conv_param.binary_mode:
                bl1_nparts = [self._dynamic_shape.cache_tiling[TilingDataKey.KBL1_FACTOR],
                          (cl0_factor[0] + multi_n_bl1 - 1) // multi_n_bl1]
            else:
                bl1_nparts = [(ci1_opt + k1_bl1 - 1) // k1_bl1,
                          (cl0_factor[0] + multi_n_bl1 - 1) // multi_n_bl1]
                if self._sparse_4to2_flag:
                    ci1_sparse = ceil_div(ci1_opt, 2)  # 2: sparse bl1 ci1 is ceil of origin ci1/2
                    bl1_nparts[0] = (ci1_sparse + k1_bl1 - 1) // k1_bl1
                    if self._im2col_dma.split_k1_flag:
                        bl1_nparts[0] = ci1_sparse * kernel_h * kernel_w
        else:
            bl1_nparts = [1, n_dim]

        if aub_tiling:
            k_aub, m_aub, *_ = aub_tiling
            k1_aub = k_aub // (((kernel_h - 1)*dilate_h + 1)*((kernel_w - 1)*dilate_w + 1)*block_k0)
            k1_aub = 1 if k1_aub == 0 else k1_aub
            if al1_tiling:
                aub_nparts = [k1_al1 // k1_aub, multi_m_al1 // m_aub]
            else:  # al1 full load
                aub_nparts = [ci1_opt // k1_aub, ceil_div(cl0_factor[1], m_aub)]
                if self._split_w_flag:
                    aub_nparts[-1] = ceil_div(self._out_height, m_aub)

        #===========================split and compute at=========================================
        sch = self._sch
        al1 = tensor_param["al1"]
        al1_zero, al1_dma_im2col, al1_virtual_add = \
            tensor_param["al1_zero"], tensor_param["al1_dma_im2col"], tensor_param["al1_virtual_add"]
        al1_load2d_real, al1_load2d_zero = tensor_param["al1_load2d_real"], tensor_param["al1_load2d_zero"]
        bl1 = tensor_param["bl1"]
        al0 = tensor_param["al0"]
        bl0 = tensor_param["bl0"]
        cl0 = tensor_param["cl0"]
        aub_dma = tensor_param["aub_dma"]
        al1_im2col = tensor_param["al1_im2col"]
        fmap_row_major = tensor_param["fmap_row_major"]
        bias_l1 = tensor_param["bias_l1"]
        bias_bt = tensor_param["bias_bt"]
        if not is_support_fixpipe():
            cub = tensor_param["cub"]
            bias_ub = tensor_param["bias_ub"]
            cub_bias_add = tensor_param["cub_bias_add"]

        attach_axis_dict = {}
        tiling_param = {}

        # tile
        #===================================tile al0============================================
        al0_axis_list, dynamic_al0_pragma_axis = tile_tensor_al0()

        #===================================tile res============================================
        if self._convbn1.flag:
            res, res_axis_list, bindcore_axis, res_pragma_axis = bn1fusion_tile_tensor_res()
        else:
            res_axis_list, bindcore_axis, res_pragma_axis = tile_tensor_res()
        cub_at_res_axis, cl0_at_res_axis, bl0_at_res_axis, al1_at_res_axis, bl1_at_res_axis = res_axis_list

        #=====================================tile cl0==============================================
        k_outer_list, cl0_axis_list, cl0_pragma_axis = tile_tensor_cl0()
        al0_at_cl0_axis, bl0_at_cl0_axis, al1_at_cl0_axis, bl1_at_cl0_axis = cl0_axis_list

        #=======================tile fixpipe_res==============================
        cub_pragma_axis_dict = \
            self._fixpipe_fusion.tile_tensor_fixpipe_res(sch, self._block_n0, nc_factor_cub, self._fmap_dtype)

        #=================tile reform_by_vmuls/reform_by_vadds in quant op==================

        self._quant_fusion.split_reform_axis(sch)

        # =============================== attach check =======================================
        check_weight_attach_flag()

        #===============================attach=======================================
        if not self._dynamic_shape.flag or self._conv_param.binary_static_flag:
            # fixpipe
            fixpipe_fb_attach_axis, fixpipe_l1_attach_axis, eltwise_attach_axis = config_fixpipe_attach_axis()

            self._fixpipe_fusion.fixpipe_inputs_compute_at(
                sch, res, fixpipe_fb_attach_axis, fixpipe_l1_attach_axis, eltwise_attach_axis)

        # ub fusion
        self._eltwise_ub_fusion.ub_tensors_attach(sch, res, cl0_at_res_axis, cub_at_res_axis)

        # quant fusion
        self._quant_fusion.quant_tensors_attach(sch, res, cub_at_res_axis)

        # bn1 fusion
        if self._convbn1.flag:
            not_fully_spilt_flag = check_fully_spilt_flag()
            self._convbn1.bn1fusion_compute_at(sch, res, cl0_at_res_axis, not_fully_spilt_flag, nc_cl0)

        if self._dynamic_shape.flag:
            self._input_nd2nz.input_nd_binary_compute_at(sch, al1, self._l0a_load2d.flag, aub_nparts)
            self._output_nz2nd.output_nd_binary_compute_at(sch, res, cub_at_res_axis)


        eltwise_ub_fusion_buffer_tile()
        cub_remove_padded_column_compute_at()
        cub_compute_at()
        cub_buffer_tile()
        cl0_compute_at()
        cl0_set_store_predicate()
        cl0_buffer_tile()
        bl0_compute_at()
        al0_compute_at()
        al0_buffer_tile()
        al0_set_store_predicate()
        al1_compute_at()
        al1_buffer_tile()
        bl1_compute_at()
        bias_compute_at()
        bias_buffer_tile()
        self._quant_conv2d_schedule.compute_at(res, cl0_at_res_axis,
                                               cub_at_res_axis if bias_slice_flag else bindcore_axis,
                                               cl0_at_res_axis if bias_slice_flag else bindcore_axis)

        tiling_param.update(
            {
                "al1_tiling": al1_tiling,
                "al0_tiling": al0_tiling,
                "bl1_tiling": bl1_tiling,
                "bl0_tiling": bl0_tiling,
                "cl0_tiling": cl0_tiling,
                "cub_tiling": cub_tiling,
                "aub_tiling": aub_tiling,
                "al1_nparts": al1_nparts,
                "stride_h_update": self._strideh_opti.stride_h_update,
                "fmap_5hd_shape": self._para_dict["a_shape"],
                "weight_fracz_shape": self._para_dict["weight_fracz_shape"],
                "blocks": batch_dim*n_dim*m_dim*group_dim,
                "block_dim": self._tiling["block_dim"],
                "manual_pingpong_buffer": self._tiling.get("manual_pingpong_buffer"),
                "m_cl0": mc_cl0*m0_cl0,
                "out_hw": self._out_hw
            }
            )
        if al1_tiling:
            tiling_param.update({"multi_m_al1": multi_m_al1,
                                 "k_al1": k_al1,
                                 "k1_al1": k1_al1,
                                 "batch_al1": batch_al1})
        if self._inner_batch.flag:
            tiling_param.update({"batch_al1": batch_cl0})
        if aub_tiling:
            tiling_param.update({"aub_nparts": aub_nparts})

        emit_insn_dict = {"al0_axis_list": al0_axis_list,
                          "bindcore_axis": bindcore_axis,
                          "k_outer": k_outer_list,
                          "cl0_pragma_axis": cl0_pragma_axis,
                          "res_pragma_axis": res_pragma_axis,
                          "cub_pragma_axis_dict": cub_pragma_axis_dict,
                          #================dynamic shape======================
                          "dynamic_al0_pragma_axis": dynamic_al0_pragma_axis,
                         }

        return res, tiling_param, emit_insn_dict, attach_axis_dict

    def special_process_post(self, res, conv_param, tensor_param, tiling_param, emit_insn_dict, attach_axis_dict):
        """
        Special process after tiling is parsed.
        """
        def get_workspace(sch):
            """
            return workspace_tensor for mixl2 in v220
            """
            if is_support_v220() and hasattr(sch, "cce_special"):
                self.workspace_tensor = sch.cce_special["tensor_list"]

        def get_bias_load_tensor():
            bias_ub = tensor_param.get("bias_ub")  # bias load to ub on v100/v200 soc
            bias_l1 = tensor_param.get("bias_l1")  # bias load to l1 on v220/v300 soc
            bias_load_tensor = bias_l1 if is_support_fixpipe() else bias_ub
            return bias_load_tensor

        def group_conv_set_al1_buffer_size():
            """
            set buffer_size for al1 to allocate the extra space
            when group conv enlarge Ci1 to Ci1_opt*group_opt in load3d/load2d.
            """
            if not self._dynamic_shape.flag and (self._in_c1 < self._group_opt * self._ci1_opt):
                case_info = {"static_shape": True}
                al1_bound_list = get_al1_bound_common(case_info, conv_param, tiling_param)
                set_al1_buffer_size_common(sch, al1, al1_bound_list)

        #===========================prepare params================================================
        fmap = tensor_param["fmap"]
        al1 = tensor_param["al1"]
        bl1 = tensor_param["bl1"]
        cl0 = tensor_param["cl0"]
        bl0 = tensor_param["bl0"]
        cub = tensor_param["cub"]
        bias_load_tensor = get_bias_load_tensor()
        bias_l1_real = tensor_param.get("bias_l1_real")
        bias_l1_zero = tensor_param.get("bias_l1_zero")
        bias_bt = tensor_param.get("bias_bt")

        pingpong_buffer = self._tiling["manual_pingpong_buffer"]
        cl0_tiling = tiling_param["cl0_tiling"]
        cub_tiling = tiling_param.get("cub_tiling")
        res_pragma_axis = emit_insn_dict["res_pragma_axis"]
        sch = self._sch

        # parse the tbe compile parameter
        sch.tbe_compile_para, tbe_sch_control_para = util.parse_tbe_compile_para(self._tiling.get("compile_para"))
        if pingpong_buffer["CL0_pbuffer"] == 2 and tbe_sch_control_para.get("preload"):
            sch[cl0].preload()

        # align cl0 memory allocation when channel merging
        for fixpipe_res in self._fixpipe_res_list:
            if fixpipe_res.dtype in ("int4", "int8"):
                l0c_buffer_size = reduce((lambda x, y: x * y), cl0_tiling)
                if self._wino_conv.flag:
                    l0c_buffer_size = l0c_buffer_size * WINO_OUT_TILE_HW * WINO_OUT_TILE_HW
                sch[cl0].set_buffer_size(l0c_buffer_size)
                break

        #=================================lxfusion======================================
        self._lx_fusion.config_l1_tensormap(sch, fmap, al1, self._op_graph)

        #=================================dynamic shape process=====================================
        al1_bound_list = self._dynamic_shape.get_al1_bound(conv_param, tiling_param)
        self._dynamic_shape.set_al1_bound(sch, al1, al1_bound_list)
        if self._dynamic_shape.flag:
            self._dynamic_shape.set_bl0_bound(sch, bl0, tiling_param, self._group_opt)
            self._dynamic_shape.set_bl1_bound(sch, bl1, tiling_param, self._group_opt)
            self._dynamic_shape.set_bias_bound(sch, bias_load_tensor, bias_bt, [bias_l1_real, bias_l1_zero])
            self._input_nd2nz.input_nd_binary_process_post(sch, conv_param, tiling_param,
                                                           emit_insn_dict, attach_axis_dict, al1_bound_list)
            self._output_nz2nd.output_nd_binary_process_post(sch, cub, tiling_param)

        self._dynamic_shape.set_cl0_bound(sch, cl0, cl0_tiling)
        self._dynamic_shape.res_hw_dynamic_pragma(sch, res, res_pragma_axis)

        #=================================group conv==============================================
        group_conv_set_al1_buffer_size()

        #=================================maxpool fusion=====================================
        self._pooling_fusion.special_process(sch, res, cub, conv_param, tensor_param, tiling_param,
                                             emit_insn_dict, attach_axis_dict)
        
        #=================================winograd===========================================
        cache_read_elewise_list = self._fixpipe_fusion.get_fixpipe_cache_read_elewise()
        self._wino_conv.winograd_special_process(sch, res, tensor_param, tiling_param, attach_axis_dict,
                                                 self._fixpipe_res_list, cache_read_elewise_list)

        #=================================fixpipe fusion=====================================
        nc_factor_cub, _, _, _, _, group_cub = cub_tiling
        buffer_size = nc_factor_cub * group_cub * self._block_n0
        self._fixpipe_fusion.set_fixpipe_buffer_size(sch, buffer_size)

        if not self._dynamic_shape.flag:
            if self._split_w_flag:
                # splitw case set_store_predict to wout direction
                _, mc_cl0, _, _, _, _ = cl0_tiling
                cl0_m_bound = mc_cl0 * self._block_m0
                self._fixpipe_fusion.fixpipe_res_store_predict(sch, attach_axis_dict, self._out_width, cl0_m_bound)

        get_workspace(sch)

    def double_buffer(self, tensor_param):
        """
        Enable pingpong buffer.
        """

        if not self._dynamic_shape.flag and is_support_fixpipe() and self._im2col_dma.flag:
            return

        pingpong_buffer = self._tiling["manual_pingpong_buffer"]

        al1 = tensor_param.get("al1")
        bl1 = tensor_param.get("bl1")
        al0 = tensor_param.get("al0")
        bl0 = tensor_param.get("bl0")
        cl0 = tensor_param.get("cl0")
        aub = tensor_param.get("aub")
        bub = tensor_param.get("bub")
        cub = tensor_param.get("cub")
        ubg = tensor_param.get("ubg")
        al1_dma_im2col = tensor_param.get("al1_dma_im2col")
        al1_zero = tensor_param.get("al1_zero")
        aub_dma = tensor_param.get("aub_dma")
        al1_load2d_zero = tensor_param.get("al1_load2d_zero")
        al1_load2d_real = tensor_param.get("al1_load2d_real")
        # bias may involve more than 1 tensor (in condition where C0 != N0 or in binary mode)
        bias_l1 = [value for key, value in tensor_param.items() if "bias_l1" in key and value is not None]

        pingpong_map = {
            "AL1_pbuffer": al1,
            "BL1_pbuffer": bl1,
            "AL0_pbuffer": al0,
            "BL0_pbuffer": bl0,
            "CL0_pbuffer": cl0,
            "AUB_pbuffer": aub,
            "BUB_pbuffer": bub,
            "UBG_pbuffer": ubg,
        }
        if cub is not None:
            pingpong_map["CUB_pbuffer"] = cub

        self._quant_conv2d_schedule.transform_pingpong_buffer_to_static_format(pingpong_buffer, self._conv_param)
        self._quant_conv2d_schedule.update_pingpong_map_for_bias_l0c(pingpong_map)

        if is_support_fixpipe():
            pingpong_buffer.pop("AUB_pbuffer", None)
            pingpong_buffer.pop("BUB_pbuffer", None)
            pingpong_buffer.pop("CUB_pbuffer", None)
            pingpong_map.update({"INPUT_L1_BT_pbuffer": bias_l1})
            pingpong_map.update({"AL1_IM2COL_pbuffer": al1_dma_im2col})
            pingpong_map.update({"AL1_ZERO_pbuffer": al1_zero})

            self._eltwise_ub_fusion.double_buffer(self._sch, pingpong_buffer.get("L0C_OUTPUT_pbuffer"))
            self._fixpipe_fusion.double_buffer(self._sch, pingpong_buffer)

            pingpong_buffer.pop("UBG_pbuffer", None)
            pingpong_buffer.pop("L0C_OUTPUT_pbuffer", None)
            pingpong_buffer.pop("INPUT_L1_FB_pbuffer", None)
            pingpong_buffer.pop("INPUT_L1_eltwise_pbuffer", None)
        else:
            self._eltwise_ub_fusion.double_buffer(self._sch, pingpong_buffer.get("CUB_pbuffer"))
            self._input_nd2nz.input_nd_binary_double_buffer(self._sch)

        if self._dynamic_shape.flag and not self._conv_param.binary_static_flag:
            self._dynamic_shape.double_buffer_dynamic(self._sch, pingpong_buffer, pingpong_map)
        else:
            for key, value in pingpong_buffer.items():
                if value == DOUBLE_BUFFER_DISABLED:
                    continue
                if value != DOUBLE_BUFFER_ENABLED:
                    log.debug("Unsupported input of double_buffer(). value %s is valid" % value)

                tensor_object_list = pingpong_map.get(key)
                if tensor_object_list is None:
                    continue
                if not isinstance(tensor_object_list, list):
                    tensor_object_list = [tensor_object_list]
                for tensor in tensor_object_list:
                    self._sch[tensor].double_buffer()

        if self._input_nd2nz.flag and self._l0a_load2d.flag:
            if pingpong_buffer.get("AL1_pbuffer") == DOUBLE_BUFFER_ENABLED:
                self._sch[al1_load2d_zero].double_buffer()
                self._sch[al1_load2d_real].double_buffer()

        self._pooling_fusion.maxpool_al1_preload(self._sch, pingpong_buffer, al1)

    def map_insn(self, res, tensor_param, tiling_param, emit_insn_dict):
        """
        Emit insn for each tensor.
        """
        def im2col_emit_insn():
            """
            Emit insn for AL1/Row_major/AL0.
            """
            def config_setfmatrix():
                """
                Emit insn for row_major tensor.
                """
                if self._im2col_dma.flag:
                    return None

                setfmatrix_dict = {
                    "conv_kernel_h": conv_param.filter_h,
                    "conv_kernel_w": conv_param.filter_w,
                    "conv_padding_top": conv_param.padding[0],
                    "conv_padding_bottom": conv_param.padding[1],
                    "conv_padding_left": conv_param.padding[2],
                    "conv_padding_right": conv_param.padding[3],
                    "conv_stride_h": stride_h_update,
                    "conv_stride_w": conv_param.stride_w,
                    "conv_dilation_h": conv_param.dilate_h,
                    "conv_dilation_w": conv_param.dilate_w
                }

                setfmatrix_dict["conv_fm_c"] = al1.shape[1]*al1.shape[4]
                setfmatrix_dict["conv_fm_h"] = al1.shape[2]
                setfmatrix_dict["conv_fm_w"] = al1.shape[3]

                if self._split_w_flag:
                    setfmatrix_dict["conv_w_split"] = True

                return setfmatrix_dict

            def al1_emit_insn():
                """
                Emit insn for al1.
                """
                def al1_common_emit_insn(sch, al1):
                    """
                    Emit insn for al1 in common usage.
                    """
                    sch[al1].emit_insn(al1.op.axis[0],
                                       "dma_copy",
                                       {"mem_align": 1})

                if self._input_nd2nz.flag:
                    return self._input_nd2nz.al1_nd2nz_emit_insn(sch, al1)
                if self._aipp_fusion.flag:
                    return self._aipp_fusion.al1_aipp_emit_insn(sch, al1)

                return al1_common_emit_insn(sch, al1)

            def al0_emit_insn():
                """
                Emit insn for al0.
                """
                def al0_common_emit_insn(sch, fmap_row_major, al0, setfmatrix_dict, al0_axis_list):
                    """
                    Emit insn for al0 and row major tensor in common usage.
                    """
                    fmap_row_major_emit_insn_axis = fmap_row_major.op.axis[1]
                    if self._split_w_flag:
                        _, batch_idx, out_height_idx, out_width_idx, _, _, _, _ = range(len(fmap_row_major.shape))
                        fmap_row_major_emit_insn_axis = fmap_row_major.op.axis[out_height_idx]
                    sch[fmap_row_major].emit_insn(fmap_row_major_emit_insn_axis, 'set_fmatrix', setfmatrix_dict)
                    sch[al0].emit_insn(al0_axis_list[3], 'im2col')

                if self._im2col_dma.flag:
                    return self._im2col_dma.im2col_dma_emit_insn(sch, al1_im2col, al0, al0_axis_list)

                return al0_common_emit_insn(sch, fmap_row_major, al0, setfmatrix_dict, al0_axis_list)

            if self._wino_conv.flag:
                self._wino_conv.fm_transform_emit_insn(sch, al1, al0, tiling_param)
            elif self._im2col_dma.flag:
                self._im2col_dma.dma_im2col_emit_insn(sch, tensor_param, tiling_param,
                                                      conv_param, self._l0a_layout_zn_flag)
            elif self._dynamic_shape.flag:
                self._dynamic_shape.dynamic_mode_im2col_v2(
                    sch, conv_param, tensor_param, tiling_param,
                    emit_insn_dict, self._input_nd2nz.mode == "NHWC", self._l0a_load2d.flag)
                self._c04.pragma_al1_axis_group_c04(sch, tensor_param.get("al1"))
            elif self._l0a_load2d.flag:
                self._l0a_load2d.load2d_emit_insn(sch, al1, al0, self._input_nd2nz.mode)
            else:
                setfmatrix_dict = config_setfmatrix()
                al1_emit_insn()
                self._lx_fusion.al1_l1fusion_pragma(sch, al1)
                al0_emit_insn()

            if self._dynamic_shape.flag:
                self._input_nd2nz.input_nd_binary_emit_insn(sch)

            if not self._dynamic_shape.flag:
                get_weight_repeat_number()

        def bl0_emit_insn():
            """
            Emit insn for bl0.
            """
            def get_sparse_4to2_emit_insn_axis():
                # load_cbuf_to_cb_sp insn only has repeat param.
                # [Pass Team] only support continuous 512B data MTE.
                # [Schedule Team] need to consider tiling whether data is continuous or not.
                # k1_BL1 range is [0, x1]
                #     n1_BL1 range is [0, x2]
                #         n0 range is [0, 15]
                #             k0 range is [0, 31]
                #                 weight: gm -> cbuf
                #                     k1_nparts range is [0, x3]
                #                         n1_nparts range is [0, x4]
                #                             k1_BL0 range is [0, x5] - emit insn may be here.
                #                                 n1_BL0 range is [0, x6] - emit insn may be here.
                #                                     n0 range is [0, 15]
                #                                         k0 range is [0, 31]
                #                                             weight: cbuf -> L0B
                # emit insn at k1_BL0 example diagram as following:
                # vertical is k, and horizontal is n
                # if k1_BL0 == 1, data is continuous in one Line.
                # |xxxx|xxxx|xxxx|xxxx|----|----|
                # |----|----|----|----|----|----|
                # |----|----|----|----|----|----|
                # |----|----|----|----|----|----|
                # |----|----|----|----|----|----|
                # if k1_BL0 != 1, but n1_nparts == 1, data is also continuous in one Line.
                # |xxxx|xxxx|xxxx|xxxx|xxxx|xxxx|
                # |xxxx|xxxx|xxxx|xxxx|xxxx|xxxx|
                # |xxxx|xxxx|xxxx|xxxx|xxxx|xxxx|
                # |----|----|----|----|----|----|
                # |----|----|----|----|----|----|
                # otherwise, if k1_BL0 != 1 and n1_nparts != 1, diagram as following:
                # |---nparts 0---|---nparts 1---|
                # |xxxx|xxxx|xxxx|----|----|----|
                # |xxxx|xxxx|xxxx|----|----|----|
                # |xxxx|xxxx|xxxx|----|----|----|
                # |----|----|----|----|----|----|
                # |----|----|----|----|----|----|
                # data is not continuous in one Line to another Line. NOT MATCH [PASS].

                # sparse 4to2 scene, bl1_tiling will never be None.
                # k1_bl0 in bl0_tiling index is 0.
                # multi_n_bl1 in bl1_tiling index is 1.
                if bl0_tiling != [] and bl0_tiling[0] > 1:
                    multi_n_bl1 = bl1_tiling[1] if bl1_tiling else ceil_div(ceil_div(bl1.shape[1], block_dim[1]), bl0_tiling[1])
                    if multi_n_bl1 != 1:
                        return bl0.op.axis[1]

                return bl0.op.axis[0]

            if self._wino_conv.flag:
                return self._wino_conv.filter_transform_emit_insn(sch, bl0)

            emit_dict = {}
            if self._sparse_4to2_flag:
                emit_dict["enable_sparse"] = 1

            # force to emit in 1d format in dynamic case to reduce scalar
            if self._dynamic_shape.flag:
                emit_dict["map_policy"] = "1d"

            bl0_emit_insn_axis = bl0.op.axis[0]
            if self._sparse_4to2_flag:
                bl0_emit_insn_axis = get_sparse_4to2_emit_insn_axis()

            sch[bl0].emit_insn(bl0_emit_insn_axis, "dma_copy", emit_dict)

        def bl1_emit_insn():
            """
            Emit insn for bl1.
            """
            def bl1_common_emit_insn():
                """
                Emit insn for bl1 in common usage.
                """
                if bl1_tiling is not None:
                    sch[bl1].emit_insn(bl1.op.axis[0], "dma_copy")

                if weight_index_l1 is not None:
                    sch[weight_index_l1].emit_insn(weight_index_l1.op.axis[0], "dma_copy")

            if self._weight_nd2nz.flag:
                return self._weight_nd2nz.bl1_nd2nz_emit_insn(sch, bl1)

            return bl1_common_emit_insn()

        def cl0_emit_insn():
            """
            Emit insn for cl0.
            """
            mad_dict = {"mad_pattern": 2, "k_outer": k_outer}
            if self._wino_conv.flag:
                mad_dict = {"k_outer": k_outer}
                self._wino_conv.wino_conv_emit_insn(sch, cl0, cl0_pragma_axis, mad_dict)
                return

            if self._sparse_4to2_flag:
                mad_dict["mad_type"] = 1
            self._quant_conv2d_schedule.update_mad_dict_for_bias_l0c(mad_dict)

            need_check_hf32 = (self._fmap_dtype == "float32") and \
                tbe.common.platform.platform_info.intrinsic_check_support("Intrinsic_mmad", "h322f32")
            if need_check_hf32:
                if self._dynamic_shape.flag:
                    mad_dict["hf32"] = self._dynamic_shape.cache_tiling[TilingDataKey.HF32_MODE]
                    log.debug("Use variable to set HF32 mode.")
                elif conv_param.impl_mode == "enable_hi_float_32_execution" or conv_param.impl_mode == "":
                    mad_dict["hf32"] = 1
                    log.debug("Enable HF32 mode.")
                else:
                    pass

            sch[cl0].emit_insn(cl0_pragma_axis, 'mad', mad_dict)

        def get_weight_repeat_number():
            """
            Get load repeat number of weight tensor.
            """
            if bl1_tiling is None:
                if bl0_tiling == []:
                    weight_repeat_load_num = 1
                else:
                    weight_repeat_load_num = al1_nparts[1]
            elif bl1_tiling == []:
                weight_repeat_load_num = 1
            else:
                weight_repeat_load_num = al1_nparts[1]
            sch[res].pragma(bindcore_axis, "json_info_weight_repeat", weight_repeat_load_num)

        def fixpipe_res_emit_insn():
            """
            Emit insn for fixpipe_res tensor.
            """
            def get_fixpipe_res_pragma_axis(fixpipe_res):
                if res_pragma_flag:
                    return cub_pragma_axis_dict.get(fixpipe_res)

                return res_pragma_axis

            def fixpipe_res_common_emit_insn():
                """
                Emit insn for fixpipe_res tensor in common usage.
                """
                emit_insn_pragma = "fixpipe_op" if is_support_fixpipe() else "dma_copy"
                for fixpipe_res in self._fixpipe_res_list:
                    fixpipe_res_pragma_axis = get_fixpipe_res_pragma_axis(fixpipe_res)
                    sch[fixpipe_res].emit_insn(fixpipe_res_pragma_axis, emit_insn_pragma)

            if self._convbn1.flag:
                return self._convbn1.bn1fusion_cub_emit_insn(sch)

            if self._output_nz2nd.flag:
                return self._output_nz2nd.res_nz2nd_emit_insn(sch, res)

            return fixpipe_res_common_emit_insn()

        def res_emit_insn():
            """
            Emit insn for res tensor when ub fusion.
            """
            if self._convbn1.flag:
                self._convbn1.bn1fusion_res_emit_insn(sch, res, res_pragma_axis, cub_tiling)
            elif res_pragma_flag:
                res_pragma = "phony_insn" if self._eltwise_ub_fusion.multiout_flag else "dma_copy"
                sch[res].emit_insn(res_pragma_axis, res_pragma)

        def bias_emit_insn():
            """
            Emit insn for bias.
            """
            def get_bias_l1_insn_attrs():
                """
                get bias_l1 emit insn attrs for arch v200 plus
                """
                if is_v310_soc():
                    return {"pad": 1, "pad_value": tvm.const(0)}
                else:
                    return {"layout_transform": "nd2nz"}

            def get_bias_l1_insn_idx():
                """
                get bias_l1 emit insn idx.
                when bound block dim in cout or group,
                emit insn at 0 axis, pass infer larger than ori shape,
                change emit insn axis to 1 axis to fix.
                """
                if self._dynamic_shape.flag:
                    return 1
                if self._tiling.get("block_dim") is not None:
                    _, n_dim, _, group_dim = self._tiling.get("block_dim")
                    if n_dim != 1 or group_dim != 1:
                        return 1
                return 0

            if not self._bias_flag:
                return

            if not is_support_fixpipe():
                if bias_ub is not None:
                    sch[bias_ub].emit_insn(bias_ub.op.axis[0], "dma_copy")
                if cub_bias_add is not None:
                    sch[cub_bias_add].emit_insn(cub_bias_add.op.axis[0], "vector_add")
                return

            bias_l1_insn_attrs = get_bias_l1_insn_attrs()
            bias_insn_idx = get_bias_l1_insn_idx()

            if self._bias_set_zero_in_ub_flag:
                bias_ub_stage = tensor_param.get("bias_ub")
                dma_dict = {"pad_value": 0}
                sch[bias_ub_stage].emit_insn(bias_ub_stage.op.axis[0], "dma_copy", dma_dict)
                sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy", attrs=bias_l1_insn_attrs)
                sch[bias_bt].emit_insn(bias_bt.op.axis[0], "dma_copy")
                return

            if self._conv_param.bias_init_align_dim_flag:
                bias_l1_real = tensor_param["bias_l1_real"]
                bias_l1_zero = tensor_param["bias_l1_zero"]
                sch[bias_l1_real].emit_insn(bias_l1_real.op.axis[bias_insn_idx], "dma_copy", attrs=bias_l1_insn_attrs)
                sch[bias_l1_zero].emit_insn(bias_l1_zero.op.axis[0], "set_2d")
                sch[bias_l1].reused_by(bias_l1_real, bias_l1_zero)
                sch[bias_l1].emit_insn(bias_l1.op.axis[0], "phony_insn")
                sch[bias_bt].emit_insn(bias_bt.op.axis[0], "dma_copy")
            else:
                sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy", attrs=bias_l1_insn_attrs)
                sch[bias_bt].emit_insn(bias_bt.op.axis[0], "dma_copy")

        def cub_emit_insn():
            """
            Emit insn for cub.
            """
            if not is_support_fixpipe() and cub is not None:
                emit_dict = {}
                if not self._v200_width_out_1_flag:
                    emit_dict["map_policy"] = "2d"
                sch[cub].emit_insn(cub.op.axis[0], 'dma_copy', emit_dict)
                sch.sequential_malloc(cce_params.scope_ubuf)
                sch[cub].mem_unique()
                if self._split_w_flag:
                    # pass infer c1 and m axis are not continuous, emit c1 times cub dma_copy insn
                    # aplly axis_group to fix bug
                    merge_axis_group_id = tvm.call_extern("int32", "axis_group", 0, "overwrite")
                    _, cout_c1_idx, m_idx, block_n0_idx = range(len(cub.shape))
                    sch[cub].pragma(cub.op.axis[block_n0_idx], "axis_group", merge_axis_group_id)
                    sch[cub].pragma(cub.op.axis[m_idx], "axis_group", merge_axis_group_id)
                    sch[cub].pragma(cub.op.axis[cout_c1_idx], "axis_group", merge_axis_group_id)

            if self._mixl2_flag:
                for tensor in self._fixpipe_res_gm2ub_list:
                    # dma from gm to ub
                    sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")

        def cub_remove_padded_column_emit_insn():
            if self._v200_width_out_1_flag:
                sch[cub_remove_padded_column].emit_insn(cub_remove_padded_column.op.axis[0], "vector_adds")

        #=============================prepare params=========================================
        conv_param = self._conv_param
        sch = self._sch
        al1 = tensor_param["al1"]
        bl1 = tensor_param["bl1"]
        weight_index_l1 = tensor_param["weight_index_l1"]
        al0 = tensor_param["al0"]
        bl0 = tensor_param["bl0"]
        cl0 = tensor_param["cl0"]
        al1_im2col = tensor_param["al1_im2col"]
        fmap_row_major = tensor_param["fmap_row_major"]
        bias_l1 = tensor_param["bias_l1"]
        bias_bt = tensor_param["bias_bt"]
        if not is_support_fixpipe():
            cub = tensor_param.get("cub")
            bias_ub = tensor_param.get("bias_ub")
            cub_bias_add = tensor_param.get("cub_bias_add")
        if self._v200_width_out_1_flag:
            cub_remove_padded_column = tensor_param["cub_remove_padded_column"]

        block_dim = tiling_param["block_dim"]
        bl1_tiling = tiling_param["bl1_tiling"]
        bl0_tiling = tiling_param["bl0_tiling"]
        cub_tiling = tiling_param["cub_tiling"]
        al1_nparts = tiling_param["al1_nparts"]
        stride_h_update = tiling_param["stride_h_update"]

        al0_axis_list = emit_insn_dict.get("al0_axis_list")
        bindcore_axis = emit_insn_dict.get("bindcore_axis")
        k_outer = emit_insn_dict.get("k_outer")
        cl0_pragma_axis = emit_insn_dict.get("cl0_pragma_axis")
        res_pragma_axis = emit_insn_dict.get("res_pragma_axis")
        cub_pragma_axis_dict = emit_insn_dict.get("cub_pragma_axis_dict")

        # res_pragma_flag means res is not fixpipe_res, which can be ub tensor or conv_virtual_res.
        res_pragma_flag = self._eltwise_ub_fusion.flag or self._eltwise_ub_fusion.multiout_flag
        #=============================emit insn=========================================
        im2col_emit_insn()

        bl1_emit_insn()

        bl0_emit_insn()

        cl0_emit_insn()

        fixpipe_res_emit_insn()

        res_emit_insn()

        bias_emit_insn()

        cub_emit_insn()

        cub_remove_padded_column_emit_insn()

        self._fixpipe_fusion.fixpipe_inputs_emit_insn(sch)

        self._eltwise_ub_fusion.ub_tensors_emit_insn(sch)

        self._quant_fusion.quant_tensors_emit_insn(sch)

        self._quant_conv2d_schedule.emit_insn(cl0)

        # ir simplification
        self._dynamic_shape.ir_simplify(sch, tensor_param)


def conv_v220_schedule(sch, res, spec_node_list, sch_list, conv_param, op_graph, tiling_dict_flag,
                       tiling_case=None, var_range=None):
    """
    Schedule for Conv2d v220.
    """

    schedule = Conv2dSchedule(sch, res, spec_node_list, conv_param, op_graph, tiling_dict_flag, tiling_case, var_range)

    schedule.update_fusion_buildcfg()

    info_dict = schedule.fetch_info_dict(tiling_case)

    if conv_param.dynamic_flag and tiling_dict_flag:
        return info_dict

    schedule.fetch_tiling(info_dict, tiling_case)

    schedule.verify_tiling()

    tensor_param = schedule.config_scope()

    schedule.special_process_pre(res, tensor_param)

    res, tiling_param, emit_insn_dict, attach_axis_dict = schedule.tile_attach_tensor(res, sch_list, tensor_param)

    schedule.special_process_post(res, conv_param, tensor_param, tiling_param, emit_insn_dict, attach_axis_dict)

    schedule.double_buffer(tensor_param)

    schedule.map_insn(res, tensor_param, tiling_param, emit_insn_dict)

    return schedule.workspace_tensor
