#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d multi output fusion.
"""
from tbe.dsl.compute.conv_compute import shape_to_list
from tbe.dsl.static_schedule.conv_schedule_util import clear_suffix
from tbe.dsl.static_schedule.conv_schedule_util import get_fixpipe_tag_list
from tbe.dsl.static_schedule.conv_schedule_util import is_elewise
from tbe.dsl.static_schedule.conv_schedule_util import is_shape_equal
from tbe.dsl.static_schedule.conv_schedule_util import search_op
from tbe.dsl.static_schedule.conv_schedule_util import is_wino_res_tensor
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe import tvm


def is_fixpipe_output(tensor):
    """
    Check whether tensor is a fixpipe res tensor.
    """
    if clear_suffix(tensor.op.tag) in get_fixpipe_tag_list():
        return True
    return False


def check_fixpipe_version_dualoutput(outs):
    """
    Check whether it is dual output situation in v220/v300.
    """
    if is_support_fixpipe():
        if not isinstance(outs, list) or len(outs) != 2:
            return False

        conv_flag = False

        for tensor in outs:
            if search_op(tensor, "convolution_C") is not None:
                conv_flag = True
                break

        if not conv_flag:
            return False

        for tensor in outs:
            if not is_fixpipe_output(tensor) and \
                not is_wino_res_tensor(tensor) and \
                not is_elewise(tensor) and \
                    tensor.op.tag != "quant":
                return False
        return True
    return False


def common_dualout_compute(outs):
    """
    Common dualoutput compute to generate fake virtual_res node.
    """
    def check_nd_5hd_add(res1, res2):
        """
        Check whether it is nd tensor + 5hd tensor situation.
        """
        res_nd, res_5hd = (res1, res2) if len(res1.shape) == 3 else (res2, res1)

        if len(res_nd.shape) != 3 or len(res_5hd.shape) != 4:
            err_man.raise_err_specific(
                "conv2d",
                f"check nd_5hd_add shape dim error, res1.shape is {res1.shape}, res2.shape is {res2.shape}")

        batch_nd, hw_nd, co_nd = shape_to_list(res_nd.shape)
        batch_5hd, co1_5hd, hw_5hd, co0_5hd = shape_to_list(res_5hd.shape)

        if (batch_nd, hw_nd) != (batch_5hd, hw_5hd) or co_nd > co1_5hd * co0_5hd:
            err_man.raise_err_specific(
                "conv2d",
                f"check nd_5hd_add shape value error, res1.shape is {res1.shape}, res2.shape is {res2.shape}")
        return res_nd, res_5hd

    def check_channelreform_add(res1, res2):
        """
        Check whether it is channelreform tensor + tensor of other dtype situation.
        """
        res_long, res_short = (res1, res2) if res1.shape[-1].value > res2.shape[-1].value else (res2, res1)
        if len(res_long.shape) != 4 or len(res_short.shape) != 4:
            err_man.raise_err_specific(
                "conv2d",
                f"check channelreform_add shape dim error, res1.shape is {res1.shape}, res2.shape is {res2.shape}")

        batch_long, co1_long, hw_long, co0_long = shape_to_list(res_long.shape)
        batch_short, co1_short, hw_short, co0_short = shape_to_list(res_short.shape)

        if (batch_long, hw_long) != (batch_short, hw_short) or \
            co1_long * co0_long < co1_short * co0_short or \
            bool(co0_long % co0_short != 0):
            err_man.raise_err_specific(
                "conv2d",
                f"check channelreform_add shape value error, res1.shape is {res1.shape}, res2.shape is {res2.shape}"
                )
        return res_long, res_short

    def res_euqal_add_compute(res1, res2):
        """
        Dualoutput compute when res1.shape equals to res2.shape.
        """
        res = tvm.compute(
            res1.shape,
            lambda *indices: res1(*indices).astype(res2.dtype) + res2(*indices),
            name="conv_virtual_res",
            tag="euqal_add_compute")
        return res

    def res_nd_5hd_add_compute(res_nd, res_5hd):
        """
        Dualoutput compute for [N, HW, C] + [N, C1, HW, C0].
        """
        src_c0 = res_5hd.shape[-1]
        res = tvm.compute(
            res_nd.shape,
            lambda batch_idx, howo_idx, co_idx:
            res_nd(batch_idx,
                   howo_idx,
                   co_idx).astype(res_5hd.dtype) + \
            res_5hd(batch_idx,
                    co_idx // src_c0,
                    howo_idx,
                    co_idx % src_c0),
            name="conv_virtual_res",
            tag="nd_5hd_add_compute"
            )
        return res

    def res_channelreform_add_compute(res_long, res_short):
        """
        Dualoutput compute for [N, C1, HW, C0=32 or 64] + [N, C1, HW, C0=8 or 16].
        """
        long_c0 = res_long.shape[-1]
        short_c0 = res_short.shape[-1]
        res = tvm.compute(
            res_long.shape,
            lambda batch_idx, co1_idx, howo_idx, co0_idx:
            res_long(batch_idx,
                     co1_idx,
                     howo_idx,
                     co0_idx) + \
            res_short(batch_idx,
                      (co1_idx * long_c0 + co0_idx) // short_c0,
                      howo_idx,
                      (co1_idx * long_c0 + co0_idx) % short_c0).astype(res_long.dtype),
            name="conv_virtual_res",
            tag="channelreform_add_compute"
            )
        return res

    res1, res2 = outs

    if res1.op.tag == "quant" and res2.op.tag == "quant":
        err_man.raise_err_specific(
            "conv2d",
            "both output tensors are Ascend_quant output tensor, which is not supported."
            )

    if is_shape_equal(res1, res2):
        res = res_euqal_add_compute(res1, res2)
    elif len(res1.shape) == 3 or len(res2.shape) == 3:
        res_nd, res_5hd = check_nd_5hd_add(res1, res2)
        res = res_nd_5hd_add_compute(res_nd, res_5hd)
    else:
        res_long, res_short = check_channelreform_add(res1, res2)
        res = res_channelreform_add_compute(res_long, res_short)

    outs = [res, res1, res2]
    return outs
