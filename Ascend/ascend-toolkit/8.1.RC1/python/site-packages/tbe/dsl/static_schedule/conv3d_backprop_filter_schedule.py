#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv3d backprop filter schudule.
"""
from functools import reduce
from tbe import tvm
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.dsl.compute import util as compute_util

from tbe.common.tiling import tiling_api
from tbe.common import platform as tbe_platform
from tbe.dsl.compute import cube_util
from tbe.common.utils.errormgr import error_manager_util
from tbe.common.utils.errormgr import error_manager_cube as err_man_cube
from tbe.common.utils.op_util.op_util_cube import decode_tiling_v1_to_v2
from tbe.dsl.base.operation import get_op_context
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.compute.conv3d_backprop_filter_compute import DynamicConv3dBpFilterParams as DynamicParams
from tbe.dsl.static_schedule.conv_util import ConvBpFilterBinaryDynamic
from tbe.dsl.static_schedule.conv2d_backprop_filter_schedule_util import ScheduleAgentReverse

_CUBE_DIM = 16
_FLOAT16_SIZE = 2
_CUBE_MUL_SHAPE = 256
_OPEN_DOUBLE_BUFFER = 2
_LOOSE_LINE_CONDITION = 2

_DYNAMIC_BATCH = 0X0001
_DYNAMIC_DEPTH = 0X0002
_DYNAMIC_HEIGHT = 0X0004
_DYNAMIC_WIDTH = 0X0008


class CceConv3dBackpropFilterOp:
    """
    CceConv3dBackpropFilterOp: schedule definition of conv3d_backprop_filter

    Functions
    ----------
    __init__ : initialization

    schedule : schedule definition of conv3d_backprop_filter

    """

    def __init__(self, scope, need_tensorize=True, need_pragma=True):
        """
        initialization

        Parameters:
        ----------
        scope : scope definition

        need_tensorize : whether needs tensorize

        need_pragma : whether needs pragma

        Returns
        -------
        None
        """
        self.scope = scope
        self.need_tensorize = need_tensorize
        self.need_pragma = need_pragma
        self.spec_node_list = []
        self.dynamic_mode = None
        self.binary_schedule = None
        self.l0c_attach_axis = None
        self.binary_mode = DynamicParams.binary_mode
        self.support_l0c2out = DynamicParams.support_l0c2out
        self.attr_dict = DynamicParams.attr_dict
        self.var_map = DynamicParams.var_map
        self.tiling_para_dict = {}
        self.tensor_map = {}
        self.axis_dict = {}
        self.axis_size = {}
        self.flag_load3d_w_split_case = False
        self.flag_all_one_case = DynamicParams.flag_all_one_case
        self.sch = None
        self.sch_agent = None
        self.c0_size = tbe_platform.C0_SIZE

    @staticmethod
    def _ceil_div_tvm(num_a, num_b):
        """
        tvm.floordiv result
        """
        return tvm.floordiv((num_a + num_b - 1), num_b)

    def schedule(
        self,
        res,
        spec_node_list,
        sch_list,
        dynamic_para=None):
        """
        schedule definition of conv3d_backprop_filter

        Parameters:
        ----------
        res :

        spec_node_list :

        sch_list:

        Returns
        -------
        None
        """
        self.spec_node_list = spec_node_list

        def _get_tiling():
            if self.dynamic_mode:
                tiling = dynamic_para.get("tiling_strategy")
                decode_tiling_v1_to_v2(tiling)
            else:
                tiling = dynamic_para.get("tiling")
            if self.binary_mode:
                self.binary_schedule.config_cache_tiling(tiling)
            return tiling

        def _get_block_dim():
            batch_num = batch_fmap * depth_grads
            if self.binary_mode:
                block_dim_k = binary_schedule.cache_tiling.get("k_dim")
                block_dim_batch = tiling.get("block_dim")[0]
                block_dim_cout = tiling.get("block_dim")[2]
                block_dim_cin = tiling.get("block_dim")[1]
                block_dim_g = tiling.get("block_dim")[3]
                block_dim_list = [block_dim_k, block_dim_batch, block_dim_cout, block_dim_cin, block_dim_g]
                return block_dim_list
            if tiling.get("AUB_shape"):
                block_dim_k = tiling.get("AUB_shape")[0]
            else:
                block_dim_k = 1

            block_dim_batch = tiling.get("block_dim")[0]
            if not isinstance(batch_fmap, (tvm.tir.IntImm, int)) or not isinstance(depth_fmap, (tvm.tir.IntImm, int)):
                block_dim_batch = tvm.min(block_dim_batch, batch_num)
                batch_dim_factor = compute_util.int_ceil_div(batch_num, block_dim_batch)
                batch_dim_npart = compute_util.int_ceil_div(batch_num, batch_dim_factor)
                block_dim_batch = batch_dim_npart

            block_dim_cout = tiling.get("block_dim")[2]
            block_dim_cin = tiling.get("block_dim")[1]
            if tiling.get("BUB_shape") and tiling.get("BUB_shape")[0]:
                block_dim_g = tiling.get("BUB_shape")[0]
            else:
                block_dim_g = 1
            block_dim_list = [block_dim_k, block_dim_batch, block_dim_cout, block_dim_cin, block_dim_g]
            return block_dim_list

        def _load3d_fmap_l1_process():
            if not load2d_flag and not self.binary_mode:
                sch[self.tensor_map.get("fmap_l1")].set_scope(tbe_platform_info.scope_cbuf)
                sch[self.tensor_map.get("fmap_matrix")].buffer_align(
                    (1, 1), (1, 1), (width_grads, width_grads), (1, 1),
                    (kernel_h, kernel_h), (kernel_w, kernel_w), (1, _CUBE_DIM))
            elif self.binary_mode:
                self.tensor_map["fmap_l1"] = self.tensor_map.get("fmap_matrix")
            else:
                sch[self.tensor_map.get("fmap_matrix")].storage_align(
                    sch[self.tensor_map.get("fmap_matrix")].op.axis[1], _CUBE_MUL_SHAPE, 0)

        def _tiling_shape_check():
            """
            do tiling shape paramters general check

            """
            if self.binary_mode:
                return
            al1_shape = tiling.get("AL1_shape")
            bl1_shape = tiling.get("BL1_shape")
            al0_matrix = tiling.get("AL0_matrix")
            bl0_matrix = tiling.get("BL0_matrix")
            cl0_matrix = tiling.get("CL0_matrix")
            if al1_shape:
                if al1_shape[0] % al0_matrix[1] != 0:
                    err_man_cube.raise_err_specific("conv3d",
                        "k of AL1_shape should be integral multiple of AL0_matrix")

                if al1_shape[1] < 1:
                    err_man_cube.raise_err_specific("conv3d",
                        "m of AL1_shape should be integral multiple of AL0_matrix")

            if bl1_shape:
                if (bl1_shape[0] // _CUBE_DIM) % bl0_matrix[0] != 0:
                    err_man_cube.raise_err_specific("conv3d",
                        "k of BL1_shape should be integral multiple of BL0_matrix")

                if bl1_shape[1] < 1:
                    err_man_cube.raise_err_specific("conv3d",
                        "n of BL1_shape should be integral multiple of BL0_matrix")

            if al0_matrix:
                if al0_matrix[0] != cl0_matrix[1]:
                    err_man_cube.raise_err_specific("conv3d",
                        "mc of AL0_matrix and CL0_matrix should be same")

            if bl0_matrix:
                if bl0_matrix[1] != cl0_matrix[0]:
                    err_man_cube.raise_err_specific("conv3d",
                        "nc of BL0_matrix and CL0_matrix should be same")

            if al0_matrix and bl0_matrix:
                if al0_matrix[1] != bl0_matrix[0]:
                    err_man_cube.raise_err_specific("conv3d",
                        "k of AL0_matrix and BL0_matrix should be same")

        def _tiling_buffer_check():
            """
            Do buffer paramters general check

            """
            if self.binary_mode:
                return
            block_cout = tiling.get("block_dim")[2]

            al1_pbuff = tiling.get("manual_pingpong_buffer").get("AL1_pbuffer")
            bl1_pbuff = tiling.get("manual_pingpong_buffer").get("BL1_pbuffer")
            al0_pbuff = tiling.get("manual_pingpong_buffer").get("AL0_pbuffer")
            bl0_pbuff = tiling.get("manual_pingpong_buffer").get("BL0_pbuffer")
            l0c_pbuff = tiling.get("manual_pingpong_buffer").get("CL0_pbuffer")
            cub_pbuff = tiling.get("manual_pingpong_buffer").get("L0C_OUTPUT_pbuffer")
            cl0_matrix = tiling.get("CL0_matrix")
            cub_matrix = tiling.get("L0C_OUTPUT_matrix")
            if not self.support_l0c2out and (cl0_matrix[0] % cub_matrix[0] != 0 or cl0_matrix[1] != cub_matrix[1]):
                err_man_cube.raise_err_specific("conv3d", "invalid CUB_matrix value")

            # blockIdx must be positive int
            if block_cout < 1:
                err_man_cube.raise_err_specific("conv3d", "blockIdx must be positive int")

            # only support no dbuffer/ dbuffer
            if al1_pbuff not in (1, 2):
                dict_args = {
                    'errCode': 'E62305',
                    'param_name': 'AL1_pbuffer',
                    'expect_value': '1 or 2',
                    'value': str(al1_pbuff)
                }
                raise RuntimeError(dict_args,
                    error_manager_util.get_error_message(dict_args))

            if bl1_pbuff not in (1, 2):
                dict_args = {
                    'errCode': 'E62305',
                    'param_name': 'BL1_pbuffer',
                    'expect_value': '1 or 2',
                    'value': str(bl1_pbuff)
                }
                raise RuntimeError(dict_args,
                    error_manager_util.get_error_message(dict_args))

            if al0_pbuff not in (1, 2):
                dict_args = {
                    'errCode': 'E62305',
                    'param_name': 'AL0_pbuffer',
                    'expect_value': '1 or 2',
                    'value': str(al0_pbuff)
                }
                raise RuntimeError(dict_args,
                    error_manager_util.get_error_message(dict_args))

            if bl0_pbuff not in (1, 2):
                dict_args = {
                    'errCode': 'E62305',
                    'param_name': 'BL0_pbuffer',
                    'expect_value': '1 or 2',
                    'value': str(bl0_pbuff)
                }
                raise RuntimeError(dict_args,
                    error_manager_util.get_error_message(dict_args))

            if l0c_pbuff not in (1, 2):
                dict_args = {
                    'errCode': 'E62305',
                    'param_name': 'L0C_pbuffer',
                    'expect_value': '1 or 2',
                    'value': str(l0c_pbuff)
                }
                raise RuntimeError(dict_args,
                    error_manager_util.get_error_message(dict_args))

            if cub_pbuff not in (1, 2):
                dict_args = {
                    'errCode': 'E62305',
                    'param_name': 'CUB_pbuffer',
                    'expect_value': '1 or 2',
                    'value': str(cub_pbuff)
                }
                raise RuntimeError(dict_args,
                    error_manager_util.get_error_message(dict_args))

        def _atomic_add(sch, res_cc):
            """
            achieve atomic add according to refactor dw_cc

            """

            # redefine dw_ddr, dw_ub, dw_cc to achieve atomic write
            batch, real_k = sch[res_cc].op.reduce_axis
            batch_core, batch_in = sch[res_cc].split(batch, self.tiling_para_dict.get("batch_dim_factor"))

            if self.binary_mode:
                hw_single_core_factor = binary_schedule.k_expr_single_core * self.c0_size
                k_1_multicore, real_k = sch[res_cc].split(real_k, hw_single_core_factor)
                sch[res_cc].reorder(k_1_multicore, batch_core, batch_in, real_k)
            elif (self.dynamic_mode and not self.flag_all_one_case):
                # for dynamic hw, the reduce axis of res_cc dose not cut k0
                hw_single_core_factor = compute_util.int_ceil_div(hw_pad_1 * _CUBE_DIM, block_dim_k)
                hw_single_core_factor = compute_util.align(hw_single_core_factor, dw_k * _CUBE_DIM)
                k_1_multicore, real_k = sch[res_cc].split(real_k, hw_single_core_factor)
                sch[res_cc].reorder(k_1_multicore, batch_core, batch_in, real_k)
            else:
                real_k, k_in = sch[res_cc].split(real_k, _CUBE_DIM)
                k_1_multicore, real_k = sch[res_cc].split(real_k, nparts=block_dim_k)
                sch[res_cc].reorder(k_1_multicore, batch_core, batch_in, real_k, k_in)

            fused_atomic_write = sch[res_cc].fuse(k_1_multicore, batch_core)

            # after rfactor op, dw_cc becomes dw_ddr, original dw_ub and dw_ddr
            # will be dropped
            res_ddr = res_cc
            res_cc = sch.rfactor(res_ddr, fused_atomic_write)
            sch[res_cc].set_scope(tbe_platform_info.scope_cc)
            res_ub = None
            if not self.support_l0c2out:
                res_ub = sch.cache_read(res_cc, tbe_platform_info.scope_ubuf, [res_ddr])
            return res_cc, res_ub, res_ddr

        def _full_k_check():
            """
            set flag whether axis K is fully loaded in L0A and L0B
            return:
            -------
            full_k_l0a: 1 or 0,
                        1 means K is fully loaded in L0A
            full_k_l0b: 1 or 0,
                        1 means K is fully loaded in L0B
            """
            if self.binary_mode:
                return 0, 0
            # if k is fully load in BL1 and
            # there is multi load in N1 and N1 in BL1
            # isn't aligned to kernel_h*kernel_w, then align to it
            if (tiling.get("BL1_shape") and tiling.get("BL1_shape")[1] > 1 and
                    tiling.get("BL1_shape")[1] * tiling.get("BL0_matrix")[1]
                    % (kernel_h * kernel_w) != 0):
                tiling.get("BL1_shape")[1] = compute_util.align(
                    tiling.get("BL1_shape")[1] * tiling.get("BL0_matrix")[1],
                    kernel_h * kernel_w) // tiling.get("BL0_matrix")[1]

            # whether axis K is fully loaded in L0A and L0B
            # excluding axis batch
            if not tiling.get("AL0_matrix"):
                full_k_l0a = 1
            else:
                full_k_l0a = tiling.get("AL0_matrix")[1] // compute_util.int_ceil_div(hw_pad_1, block_dim_k)

            if not tiling.get("BL0_matrix"):
                full_k_l0b = 1
            else:
                full_k_l0b = tiling.get("BL0_matrix")[0] // compute_util.int_ceil_div(hw_pad_1, block_dim_k)

            return full_k_l0a, full_k_l0b

        def _compute_tiling_parts():
            """
            compute the parts or the factors of tensors

            """
            if self.binary_mode:
                self.tiling_para_dict.update(binary_schedule.update_tiling_nparts())
                self.tiling_para_dict["batch_dim_npart"] = \
                    self._ceil_div_tvm(batch_fmap * depth_grads, self.tiling_para_dict.get('batch_dim_factor'))
            else:
                if not tiling.get("AL0_matrix"):  # if grads no tiling in L0A
                    tiling["AL1_shape"] = []  # then no tiling in L1

                # dw_cc is (fmap_channel_1*kernel_h*kernel_w,
                #          grads_channel_1, C0_grads, C0_fmap)
                dw_tiling_factor = [
                    tiling.get("CL0_matrix")[0], tiling.get("CL0_matrix")[1]
                ]
                # nparts N, nparts M
                # dw_tiling_nparts only describe the nparts from single core to L0
                dw_tiling_nparts = [
                    compute_util.int_ceil_div(fkk // block_dim_cin, dw_tiling_factor[0]),
                    compute_util.int_ceil_div(compute_util.int_ceil_div(c1_grads, dw_tiling_factor[1]),
                                        block_dim_cout)
                    ]

                # tiling parameters of dw_ub
                dw_ub_tiling_factor = [
                    tiling.get("L0C_OUTPUT_matrix")[0], tiling.get("L0C_OUTPUT_matrix")[1]
                ]
                dw_ub_tiling_nparts = [
                    compute_util.int_ceil_div(dw_tiling_factor[0], dw_ub_tiling_factor[0]),
                    compute_util.int_ceil_div(dw_tiling_factor[1], dw_ub_tiling_factor[1])
                ]

                # only support loading one batch to L1 at a time for now
                # cout:out->single core(sc)->L1
                if tiling.get("AL1_shape"):  # if grads needs tiling in L1
                    if len(tiling.get("AL1_shape")) == 1:  # but no C_1 tiling info
                        tiling["AL1_shape"] = tiling.get("AL1_shape") + [1]
                    # nparts K1 in L1, nparts M1 in L1
                    grads_l1_tiling_nparts = [
                        compute_util.int_ceil_div(
                            compute_util.int_ceil_div(hw_pad_1, block_dim_k),
                            (tiling.get("AL1_shape")[0] // _CUBE_DIM)),
                        dw_tiling_nparts[1] // tiling.get("AL1_shape")[1]
                    ]
                else:
                    grads_l1_tiling_nparts = [1, 1]

                if tiling.get("BL1_shape"):  # if fmap needs tiling in L1
                    if len(tiling.get("BL1_shape")) == 1:  # but no fkk tiling info
                        tiling["BL1_shape"] = tiling.get("BL1_shape") + [1]  # tiling fkk=1
                    # DDR to L1 [nparts K1, nparts N1]
                    fmap_l1_tiling_nparts = [
                        compute_util.int_ceil_div(
                            compute_util.int_ceil_div(hw_pad_1, block_dim_k),
                        (tiling.get("BL1_shape")[0] // _CUBE_DIM)),
                        dw_tiling_nparts[0] // tiling.get("BL1_shape")[1]
                    ]
                else:
                    fmap_l1_tiling_nparts = [1, 1]

                # during L1 to L0 [nparts N1, nparts M1]
                l1_2_l0_tiling_nparts = [
                    dw_tiling_nparts[0] // fmap_l1_tiling_nparts[1],
                    dw_tiling_nparts[1] // grads_l1_tiling_nparts[1]
                ]
                # ka and kb may be different,
                # the min value corresponds to one MMAD,
                # the larger one is []
                if tiling.get("AL0_matrix"):  # dw_k equals to ka if L0A needs tiling
                    dw_k = tiling.get("AL0_matrix")[1]
                elif tiling.get("BL0_matrix"):
                    dw_k = tiling.get("BL0_matrix")[0]
                else:  # both fully loaded
                    dw_k = compute_util.int_ceil_div(hw_pad_1, block_dim_k)

                batch_dim_factor = compute_util.int_ceil_div(batch_fmap * depth_grads, block_dim_batch)
                batch_dim_factor = tvm.max(1, batch_dim_factor)
                batch_dim_npart = compute_util.int_ceil_div(batch_fmap * depth_grads, batch_dim_factor)

                self.tiling_para_dict["batch_dim_factor"] = batch_dim_factor
                self.tiling_para_dict["batch_dim_npart"] = batch_dim_npart
                self.tiling_para_dict["dw_tiling_factor"] = dw_tiling_factor
                self.tiling_para_dict["dw_tiling_nparts"] = dw_tiling_nparts
                self.tiling_para_dict["dw_ub_tiling_factor"] = dw_ub_tiling_factor
                self.tiling_para_dict["dw_ub_tiling_nparts"] = dw_ub_tiling_nparts
                self.tiling_para_dict["grads_l1_tiling_nparts"] = grads_l1_tiling_nparts
                self.tiling_para_dict["fmap_l1_tiling_nparts"] = fmap_l1_tiling_nparts
                self.tiling_para_dict["l1_2_l0_tiling_nparts"] = l1_2_l0_tiling_nparts
                self.tiling_para_dict["dw_k"] = dw_k

        def _get_n_factor():
            # for N axis, if Hk and Wk needs split, do explict split
            if not self.flag_all_one_case:
                if tiling.get("BL1_shape"):
                    # n1 in L1
                    nc_cc = tiling.get("CL0_matrix")[0] * tiling.get("BL1_shape")[1]
                else:
                    # BL1 is full load
                    nc_cc = kernel_d * cin1_g * kernel_w * kernel_h // block_dim_cin
                factor_kw = compute_util.int_ceil_div(kernel_w, nc_cc)
                factor_kh = compute_util.int_ceil_div(kernel_w*kernel_h, nc_cc) // factor_kw
            else:
                factor_kw = 1
                factor_kh = 1
            return factor_kw, factor_kh

        def _l0_attach():
            """
            achieve Al0 and Bl0 compute at loc or ddr

            """
            if self.dynamic_mode and not load2d_flag:
                l0a_attach_mode = (dynamic_l0a_attach == "dw_ddr")
                l0b_attach_mode = (dynamic_l0b_attach == "dw_ddr")
            else:
                l0b_attach_mode = ((batch_num_sc == 1) and (full_k_in_l0b == 1))
                l0a_attach_mode = ((batch_num_sc == 1) and (full_k_in_l0a == 1))

            if tiling.get("AL0_matrix"):
                if l0a_attach_mode:
                    # L0A data is more than that L0C needed, attach to dw_ddr
                    sch[self.tensor_map.get("grads_fractal")].compute_at(
                        sch[dw_ddr], self.axis_dict.get("c_grads_mad_at"))
                else:
                    sch[self.tensor_map.get("grads_fractal")].compute_at(
                        sch[dw_cc], self.axis_dict.get("hw_mad_1_mad_at"))
            else:  # else: fully load, attach to thread_axis
                sch[self.tensor_map.get("grads_fractal")].compute_at(sch[dw_ddr], fused_multi_core)

            if tiling.get("BL0_matrix"):
                if l0b_attach_mode:
                    sch[self.tensor_map.get("fmap_fractal")].compute_at(
                        sch[dw_ddr], self.axis_dict.get("c_fmap_mad_at"))
                else:
                    sch[self.tensor_map.get("fmap_fractal")].compute_at(
                        sch[dw_cc], self.axis_dict.get("hw_mad_1_mad_at"))
            else:  # else: fully load, attach to thread_axis
                sch[self.tensor_map.get("fmap_fractal")].compute_at(sch[dw_ddr], fused_multi_core)

        def _al1_attach():
            """
            achieve Al1 compute at l0c or ddr

            """
            if self.dynamic_mode and not load2d_flag:
                al1_attach_mode = (dynamic_al1_attach == "dw_cc")
            else:
                al1_attach_mode = (grads_l1_tiling_nparts[0] != 1 or batch_num_sc != 1)

            if tiling.get("AL1_shape"):
                # if axis K needs split, then attach to dw_cc
                if al1_attach_mode:
                    sch[self.tensor_map.get("grads_matrix")].compute_at(sch[dw_cc], self.axis_dict.get("al1_at_axis"))
                else:  # if axis K fully load in L1, attach to dw_ddr
                    sch[self.tensor_map.get("grads_matrix")].compute_at(
                        sch[dw_ddr], self.axis_dict.get("c_grads_l1_at"))
            else:  # else: fully load, attach to thread_axis
                sch[self.tensor_map.get("grads_matrix")].compute_at(sch[dw_ddr], fused_multi_core)

        def _bl1_attach():
            """
            achieve Bl1 compute at l0c or ddr

            """
            if self.dynamic_mode and not load2d_flag:
                bl1_attach_mode = (dynamic_bl1_attach == "dw_cc")
            else:
                bl1_attach_mode = (fmap_l1_tiling_nparts[0] != 1 or batch_num_sc != 1)

            if tiling.get("BL1_shape"):
                # if axis K needs split, then attach to dw_cc
                if bl1_attach_mode:
                    sch[self.tensor_map.get("fmap_matrix")].compute_at(sch[dw_cc], self.axis_dict.get("bl1_at_axis"))
                    if not load2d_flag and not self.binary_mode:
                        sch[self.tensor_map.get("fmap_l1")].compute_at(sch[dw_cc], self.axis_dict.get("bl1_at_axis"))
                else:  # if axis K fully load in L1, attach to dw_ddr
                    sch[self.tensor_map.get("fmap_matrix")].compute_at(sch[dw_ddr], self.axis_dict.get("c_fmap_l1_at"))
                    if not load2d_flag:
                        sch[self.tensor_map.get("fmap_l1")].compute_at(sch[dw_ddr], self.axis_dict.get("c_fmap_l1_at"))

            else:  # else: fully load, attach to thread_axis
                sch[self.tensor_map.get("fmap_matrix")].compute_at(sch[dw_ddr], fused_multi_core)
                if not load2d_flag:
                    sch[self.tensor_map.get("fmap_l1")].compute_at(sch[dw_ddr], fused_multi_core)

        def _double_buffer():
            """
            achieve double_buffer

            """
            if tiling.get("manual_pingpong_buffer").get("AL1_pbuffer") == _OPEN_DOUBLE_BUFFER:
                sch[self.tensor_map.get("grads_matrix")].double_buffer()

            if tiling.get("manual_pingpong_buffer").get("BL1_pbuffer") == _OPEN_DOUBLE_BUFFER:
                if not load2d_flag and not self.binary_mode:
                    sch[self.tensor_map.get("fmap_l1")].double_buffer()
                else:
                    sch[self.tensor_map.get("fmap_matrix")].double_buffer()

            if tiling.get("manual_pingpong_buffer").get("AL0_pbuffer") == _OPEN_DOUBLE_BUFFER:
                sch[self.tensor_map.get("grads_fractal")].double_buffer()

            if tiling.get("manual_pingpong_buffer").get("BL0_pbuffer") == _OPEN_DOUBLE_BUFFER:
                sch[self.tensor_map.get("fmap_fractal")].double_buffer()

            if tiling.get("manual_pingpong_buffer").get("CL0_pbuffer") == _OPEN_DOUBLE_BUFFER:
                sch[dw_cc].double_buffer()

            if not self.support_l0c2out and \
                tiling.get("manual_pingpong_buffer").get("L0C_OUTPUT_pbuffer") == _OPEN_DOUBLE_BUFFER:
                sch[dw_ub].double_buffer()

        def _get_mad_dict():
            mad_dict = {
                "mad_pattern":
                3,
                "k_outer": [
                    self.axis_dict.get("batch_insn_o"), self.axis_dict.get("hw_mad_1_l1_out_at"),
                    self.axis_dict.get("hw_mad_1_l1_in_at"), self.axis_dict.get("hw_mad_1_mad_at")
                ]
            }
            dk_c1_axis = 0
            if self.binary_mode or pad_front != 0 or pad_back != 0:
                batch_do_axis_outer = (
                    (block.var) // block_dim_cout // block_dim_cin // block_dim_group %
                    self.tiling_para_dict.get("batch_dim_npart")) * self.tiling_para_dict.get("batch_dim_factor")
                batch_do_axis = batch_do_axis_outer + self.axis_dict.get("batch_insn_o")

                c_fmap_l1_at_extend = self.axis_dict.get("c_fmap_l1_at")
                n_single_core_extend = self.tiling_para_dict.get("dw_tiling_nparts")[0] * \
                    self.tiling_para_dict.get("dw_tiling_factor")[0]
                block_dim_cin_non_factor = tvm.min(block_dim_cin, self._ceil_div_tvm(fkk, n_single_core_extend))
                if not self.binary_mode:
                    c_fmap_l1_at_extend = ((self.axis_dict.get("c_fmap_l1_c1") * factor_kh +
                                            self.axis_dict.get("c_fmap_l1_kh")) * factor_kw +
                                           self.axis_dict.get("c_fmap_l1_at"))

                dk_c1_axis = (
                    (block % (block_dim_cin_non_factor)) * n_single_core_extend +
                    (c_fmap_l1_at_extend * (dw_tiling_nparts[0] // fmap_l1_tiling_nparts[1]) +
                     self.axis_dict.get("c_fmap_mad_at")) * dw_tiling_factor[0]) // (kernel_h * kernel_w)

                c1_fmap_info = group_dict.get('cin1_g')
                axis_k_reduce_for_mad = self.axis_size.get("axis_k_reduce_for_mad")
                kernel_d_dil_index = dk_c1_axis // c1_fmap_info % kernel_d * dilation_d
                fmap_d_index = (batch_do_axis % depth_grads) * stride_d + kernel_d_dil_index
                mad_dict.update({
                    "k_coeff":
                    tvm.all(fmap_d_index >= pad_front,
                            fmap_d_index < pad_front + depth_fmap),
                    "k_cond":
                    tvm.any(
                        tvm.all((batch_do_axis % depth_grads - 1) * stride_d + kernel_d_dil_index < pad_front,
                                axis_k_reduce_for_mad <= 0,
                                batch_do_axis % self.tiling_para_dict.get("batch_dim_factor") < depth_grads,
                                batch_do_axis // depth_grads == batch_do_axis_outer // depth_grads),
                        self.axis_dict.get("batch_insn_o") + axis_k_reduce_for_mad <= 0,
                        tvm.all((batch_do_axis_outer % depth_grads) * stride_d +
                                kernel_d_dil_index >= pad_front + depth_fmap,
                                (batch_do_axis % depth_grads) * stride_d + kernel_d_dil_index >= pad_front,
                                (batch_do_axis - 1) % depth_grads * stride_d +
                                kernel_d_dil_index >= pad_front + depth_fmap,
                                self.axis_dict.get("batch_insn_o") < depth_grads,
                                axis_k_reduce_for_mad <= 0),
                    )
                })
            if not self.dynamic_mode or load2d_flag:
                mad_dict["mad_pattern"] = 2

            if (tbe_platform.intrinsic_check_support("Intrinsic_vconv", "bf162f32")
                    and get_te_var("is_bf16") is not None):
                mad_dict["datatype_bf16"] = get_te_var("is_bf16").get_tvm_var()

            return mad_dict, dk_c1_axis

        def _calc_load3d_channel_size():
            """
            Calculate the channel value of load3d
            """
            # in fp32 scene, c1 in L1 aligned to 16, so we use cin1_g(aligned to 16) here
            conv_fm_c = kernel_d * c1_fmap * c0_fmap
            conv_fm_c1 = kernel_d * c1_fmap
            # for dynamic scense, load3d channel_size should be actual channel
            if self.var_map:
                cache_tiling = self.binary_schedule.cache_tiling
                n_l0 = cache_tiling.get("n_ub_l0_time") * cache_tiling.get("cub_n1")
                n_l1 = cache_tiling.get("n_bl1") * n_l0
                kernel_hw = kernel_w * kernel_h
                c1 = compute_util.int_ceil_div(n_l1, kernel_hw)
                # if divisible, there is no need to overload
                # if doubling is divisible, load three C1's twice
                # in other cases, the worst scenario is to load 3 C1 at once
                extern_c1 = tvm.select(kernel_hw > n_l1,
                                       tvm.select(kernel_hw % n_l1 != 0, 1, 0),
                                       tvm.select(n_l1 % kernel_hw == 0, 0,
                                                  tvm.select(n_l1 * 2 %
                                                             kernel_hw == 0, 1, 2)
                                                  )
                                       )
                conv_fm_c1 = c1 + extern_c1
                conv_fm_c = conv_fm_c1 * c0_fmap
            return conv_fm_c1, conv_fm_c

        def _get_setfmatrix_dict():
            setfmatrix_dict = {}
            setfmatrix_dict["conv_kernel_h"] = kernel_h
            setfmatrix_dict["conv_kernel_w"] = kernel_w
            setfmatrix_dict["conv_padding_top"] = pad_up
            setfmatrix_dict["conv_padding_bottom"] = pad_down
            setfmatrix_dict["conv_padding_right"] = pad_right
            setfmatrix_dict["conv_padding_left"] = pad_left
            setfmatrix_dict["conv_stride_h"] = stride_h
            setfmatrix_dict["conv_stride_w"] = stride_w
            setfmatrix_dict["conv_fm_c"] = fmap_c
            setfmatrix_dict["conv_fm_h"] = fmap_h
            setfmatrix_dict["conv_fm_w"] = fmap_w
            setfmatrix_dict["conv_dilation_h"] = dilation_h
            setfmatrix_dict["conv_dilation_w"] = dilation_w

            setfmatrix_dict_0 = {}
            setfmatrix_dict_0["set_fmatrix"] = 0
            setfmatrix_dict_0["enable_row_major_vm_desc"] = 1
            setfmatrix_dict_0["conv_kernel_h"] = kernel_h
            setfmatrix_dict_0["conv_kernel_w"] = kernel_w
            setfmatrix_dict_0["conv_padding_top"] = pad_up
            setfmatrix_dict_0["conv_padding_bottom"] = pad_down
            setfmatrix_dict_0["conv_padding_right"] = pad_right
            setfmatrix_dict_0["conv_padding_left"] = pad_left
            setfmatrix_dict_0["conv_stride_h"] = stride_h
            setfmatrix_dict_0["conv_stride_w"] = stride_w
            setfmatrix_dict_0["conv_fm_c"] = fmap_c
            setfmatrix_dict_0["conv_fm_h"] = fmap_h
            setfmatrix_dict_0["conv_fm_w"] = fmap_w
            setfmatrix_dict_0["conv_dilation_h"] = dilation_h
            setfmatrix_dict_0["conv_dilation_w"] = dilation_w

            if self.binary_mode:
                conv_fm_c1, conv_fm_c = _calc_load3d_channel_size()
                setfmatrix_dict["set_fmatrix"] = 1
                setfmatrix_dict["conv_fm_c1"] = conv_fm_c1
                setfmatrix_dict["conv_fm_c"] = conv_fm_c
                setfmatrix_dict["group_flag"] = 1
                setfmatrix_dict["l1_group_flag"] = 1
                setfmatrix_dict["conv_fm_c0"] = c0_fmap
                setfmatrix_dict_0["conv_fm_c1"] = conv_fm_c1
                setfmatrix_dict_0["conv_fm_c"] = conv_fm_c
            elif self.dynamic_mode and not load2d_flag:
                setfmatrix_dict["set_fmatrix"] = 1
                setfmatrix_dict["enable_row_major_vm_desc"] = 1
                setfmatrix_dict["conv_fm_c1"] = kernel_d * cin1_g
                setfmatrix_dict["conv_fm_c0"] = c0_fmap
                setfmatrix_dict["group_flag"] = 1

            return setfmatrix_dict, setfmatrix_dict_0

        def _emit_insn_grads():
            if flag_load3d_special_case:
                sch[self.tensor_map.get("grads_matrix")].emit_insn(
                    self.tensor_map.get("grads_matrix").op.axis[3], 'dma_copy')
            else:
                sch[self.tensor_map.get("grads_matrix")].emit_insn(
                    self.tensor_map.get("grads_matrix").op.axis[0], 'dma_copy')
            # move grads from L1 to L0A
            sch[self.tensor_map.get("grads_fractal")].emit_insn(
                self.tensor_map.get("grads_fractal").op.axis[0], 'dma_padding')

        def _emit_insn_fmap():
            # move fmap from ddr to L1
            setfmatrix_dict, setfmatrix_dict_0 = _get_setfmatrix_dict()
            if not load2d_flag:
                if self.dynamic_mode:
                    in_dtype = self.tensor_map.get("fmap").dtype
                    fmap_l1_emit_insn_axis = 3 if self.binary_mode else 2
                    sch[self.tensor_map.get("fmap_l1")].emit_insn(
                        self.tensor_map.get("fmap_l1").op.axis[fmap_l1_emit_insn_axis], 'dma_copy', setfmatrix_dict_0)
                    if not self.binary_mode:
                        sch[self.tensor_map.get("fmap_matrix")].emit_insn(
                            self.tensor_map.get("fmap_matrix").op.axis[1], 'row_major_vm', setfmatrix_dict)
                    fmap_fractal_axis = self.sch[self.tensor_map.get("fmap_fractal")].leaf_iter_vars
                    cond = fmap_fractal_axis[0] > 0
                    self.sch[self.tensor_map.get("fmap_fractal")].set_value(cond, tvm.const(0.0, dtype=in_dtype), True)
                    sch[self.tensor_map.get("fmap_fractal")].emit_insn(
                        self.tensor_map.get("fmap_fractal").op.axis[1], 'im2col_v2', setfmatrix_dict)
                else:
                    # The group scene encounters nonlinear issues, so the instruction mapping axis is 2
                    l1_emit_insn_idx = 0 if real_g == 1 else 2
                    sch[self.tensor_map.get("fmap_l1")].emit_insn(
                        self.tensor_map.get("fmap_l1").op.axis[l1_emit_insn_idx], 'dma_copy')
                    sch[self.tensor_map.get("fmap_matrix")].emit_insn(
                        self.tensor_map.get("fmap_matrix").op.axis[1], 'set_fmatrix', setfmatrix_dict)
                    sch[self.tensor_map.get("fmap_fractal")].emit_insn(
                        self.tensor_map.get("fmap_fractal").op.axis[1], 'im2col')
            else:
                if self.dynamic_mode and (pad_front != 0 or pad_back != 0):
                    sch[self.tensor_map.get("fmap_matrix")].emit_insn(
                        self.tensor_map.get("fmap_matrix").op.axis[2], 'dma_copy')
                else:
                    sch[self.tensor_map.get("fmap_matrix")].emit_insn(
                        self.tensor_map.get("fmap_matrix").op.axis[0], 'dma_copy')
                sch[self.tensor_map.get("fmap_fractal")].emit_insn(
                    self.tensor_map.get("fmap_fractal").op.axis[0], 'dma_copy')

        def _dw_ddr_set_store_predicate(dk_c1_axis):
            # add condition for pad of D dimension
            batch_insn_o_size = self.axis_size.get("batch_insn_o_size")
            if not self.binary_mode and pad_front == 0 and pad_back == 0:
                return
            batch_insn_o_size1 = tvm.select((block.var // block_dim_cout // block_dim_cin // block_dim_group %
                self.tiling_para_dict.get("batch_dim_npart")) == (self.tiling_para_dict.get("batch_dim_npart") - 1),
                (batch_grads * depth_grads - (self.tiling_para_dict.get("batch_dim_npart") - 1) * batch_insn_o_size),
                batch_insn_o_size)
            mid = tvm.floordiv(batch_insn_o_size1, 2)
            mid2 = tvm.floordiv(mid, 2)
            mid3 = mid + mid2
            mid4 = tvm.floordiv(mid, 3)
            ddr_condition_right = (
                (block.var) // block_dim_cout // block_dim_cin // block_dim_group %
                self.tiling_para_dict.get("batch_dim_npart")) * self.tiling_para_dict.get("batch_dim_factor")
            ddr_condition_left = ddr_condition_right + batch_insn_o_size1 - 1
            ddr_condition_mid = ddr_condition_right + mid
            ddr_condition_mid2 = ddr_condition_right + mid2
            ddr_condition_mid3 = ddr_condition_right + mid3
            ddr_condition_mid4 = ddr_condition_right + mid4
            c1_fmap_info = group_dict.get('cin1_g')
            kernel_d_dil_index = dk_c1_axis // c1_fmap_info % kernel_d * dilation_d

            sch[dw_ddr].set_store_predicate(
                tvm.all(tvm.any((ddr_condition_left % depth_grads) * stride_d + kernel_d_dil_index >= pad_front,
                                (ddr_condition_right % depth_grads) * stride_d + kernel_d_dil_index >= pad_front,
                                (ddr_condition_mid % depth_grads) * stride_d + kernel_d_dil_index >= pad_front,
                                (ddr_condition_mid2 % depth_grads) * stride_d + kernel_d_dil_index >= pad_front,
                                (ddr_condition_mid3 % depth_grads) * stride_d + kernel_d_dil_index >= pad_front,
                                (ddr_condition_mid4 % depth_grads) * stride_d + kernel_d_dil_index >= pad_front),
                        tvm.any((ddr_condition_right % depth_grads) * stride_d +
                                kernel_d_dil_index < pad_front + depth_fmap,
                                (ddr_condition_left % depth_grads) * stride_d +
                                kernel_d_dil_index < pad_front + depth_fmap,
                                (ddr_condition_mid % depth_grads) * stride_d +
                                kernel_d_dil_index < pad_front + depth_fmap,
                                (ddr_condition_mid2 % depth_grads) * stride_d +
                                kernel_d_dil_index < pad_front + depth_fmap,
                                (ddr_condition_mid3 % depth_grads) * stride_d +
                                kernel_d_dil_index < pad_front + depth_fmap,
                                (ddr_condition_mid4 % depth_grads) * stride_d +
                                kernel_d_dil_index < pad_front + depth_fmap))
            )

        def _emit_insn():
            """
            achieve emit_insn

            """
            mad_dict, dk_c1_axis = _get_mad_dict()
            _emit_insn_grads()
            _emit_insn_fmap()

            # move dw from L0C to UB
            if not self.support_l0c2out:
                sch[dw_ub].emit_insn(dw_ub.op.axis[0], 'dma_copy')
            sch[dw_cc].emit_insn(self.axis_dict.get("batch_insn"), 'mad', mad_dict)

            # move dw form UB to ddr
            _dw_ddr_set_store_predicate(dk_c1_axis)
            sch[dw_ddr].emit_insn(c_fmap_2_ub_insn, 'dma_copy')

            sch_list.append(dw_ddr)

        def _set_var_range():
            if self.binary_mode:
                binary_schedule.set_shape_var_range()
                binary_schedule.set_tiling_var_range()
                return
            var_range = dynamic_para.get("var_range")

            if not isinstance(batch_fmap, (tvm.tir.IntImm, int)):
                sch.set_var_range(batch_fmap, *var_range.get('batch_n'))
                sch.set_var_range(batch_grads, *var_range.get('batch_n'))
            if not isinstance(depth_fmap, (tvm.tir.IntImm, int)):
                sch.set_var_range(depth_fmap, *var_range.get('fmap_d'))
                sch.set_var_range(depth_grads, *var_range.get('dedy_d'))
            if not isinstance(height_fmap, (tvm.tir.IntImm, int)):
                sch.set_var_range(height_fmap, *var_range.get('fmap_h'))
                sch.set_var_range(height_grads, *var_range.get('dedy_h'))
            if not isinstance(width_fmap, (tvm.tir.IntImm, int)):
                sch.set_var_range(width_fmap, *var_range.get('fmap_w'))
                sch.set_var_range(width_grads, *var_range.get('dedy_w'))

        def _get_attach_flag():
            attach_list = [None, None, None, None]
            if self.dynamic_mode and not DynamicParams.flag_all_one_case and tiling:
                dynamic_l0a_attach = tiling.get('dynamic_l0a_attach')
                dynamic_l0b_attach = tiling.get('dynamic_l0b_attach')
                dynamic_al1_attach = tiling.get('dynamic_al1_attach')
                dynamic_bl1_attach = tiling.get('dynamic_bl1_attach')
                attach_list = [dynamic_l0a_attach, dynamic_l0b_attach, dynamic_al1_attach, dynamic_bl1_attach]
                if self.binary_mode:
                    #  true or false
                    dynamic_al1_attach = tiling.get("attach_at_flag").get("al1_attach_flag") in \
                        binary_schedule.k_full_load_list
                    dynamic_bl1_attach = tiling.get("attach_at_flag").get("bl1_attach_flag") in \
                        binary_schedule.k_full_load_list
                    dynamic_l0a_attach = dynamic_al1_attach and dynamic_bl1_attach and \
                        tiling.get("attach_at_flag").get("min_kl1_cmp_kl0") == 0
                    dynamic_l0b_attach = dynamic_l0a_attach
                    attach_list = [dynamic_l0a_attach, dynamic_l0b_attach, dynamic_al1_attach, dynamic_bl1_attach]
                    for item, val in enumerate(attach_list):
                        attach_list[item] = "dw_ddr" if val else "dw_cc"

            return attach_list

        def _reduce_split_mode():
            reduce_split_mode = True
            if self.binary_mode:
                reduce_split_mode = (tiling.get("attach_at_flag").get("abkl1_attach_flag") ==
                                     binary_schedule.tiling_utils.get("attach_less"))
            elif not isinstance(height_fmap, (tvm.tir.IntImm, int)) or \
                 not isinstance(width_fmap, (tvm.tir.IntImm, int)):
                if tiling.get("BL1_shape") and tiling.get("AL1_shape"):
                    # grads and fmap need tiling in L1
                    reduce_split_mode = \
                              tiling.get("AL1_shape")[0] < tiling.get("BL1_shape")[0]
                elif tiling.get("BL1_shape"):
                    # only fmap needs tiling in L1
                    reduce_split_mode = False
                elif tiling.get("AL1_shape"):
                    # only grads needs tiling in L1
                    reduce_split_mode = True
                else:
                    # Neither grads nor fmap need tiling in L1
                    reduce_split_mode = False
            else:
                reduce_split_mode = \
                    grads_l1_tiling_nparts[0] > fmap_l1_tiling_nparts[0]
            return reduce_split_mode

        def _compute_tiling_factors():
            fmap_l1_tiling_factor_k, grads_l1_tiling_factor_k = None, None
            if self.dynamic_mode and not load2d_flag:
                if reduce_split_mode:
                    if tiling.get("AL1_shape"):
                        grads_l1_tiling_factor_k = \
                            tiling.get("AL1_shape")[0] // (dw_k * _CUBE_DIM)
                    if tiling.get("BL1_shape") and tiling.get("AL1_shape"):
                        fmap_l1_tiling_factor_k = \
                            tiling.get("BL1_shape")[0] // tiling.get("AL1_shape")[0]
                else:
                    if tiling.get("BL1_shape"):
                        fmap_l1_tiling_factor_k = \
                            tiling.get("BL1_shape")[0] // (dw_k * _CUBE_DIM)
                    if tiling.get("BL1_shape") and tiling.get("AL1_shape"):
                        grads_l1_tiling_factor_k = \
                            tiling.get("AL1_shape")[0] // tiling.get("BL1_shape")[0]
            return grads_l1_tiling_factor_k, fmap_l1_tiling_factor_k

        sch = sch_list[0]
        self.sch = sch
        # use ScheduleAgentReverse for split dw_cc and dw_ddr's axes, record split factor while split axis
        sch_agent = ScheduleAgentReverse(sch)
        self.sch_agent = sch_agent
        binary_schedule = Conv3DBpFilterBinaryDynamic(sch, self.binary_mode, self.flag_load3d_w_split_case,
                                                      self.flag_all_one_case)
        self.binary_schedule = binary_schedule
        # ####################### get computing graph #######################
        self.dynamic_mode = DynamicParams.dynamic_mode
        load2d_flag = self.attr_dict.get('load2d_flag')
        group_dict = self.attr_dict.get('group_dict')
        flag_load3d_special_case = self.attr_dict.get('flag_load3d_special_case')
        kernel_name = self.attr_dict.get('kernel_name')

        self.tensor_map["dw_ddr"] = res
        dw_cc = self.tensor_map.get("dw_ddr")
        self.tensor_map["dw_cc"] = dw_cc
        self.tensor_map["grads_fractal"] = dw_cc.op.input_tensors[0]
        self.tensor_map["grads_matrix"] = self.tensor_map.get("grads_fractal").op.input_tensors[0]
        self.tensor_map["grads"] = self.tensor_map.get("grads_matrix").op.input_tensors[0]
        self.tensor_map["fmap_fractal"] = dw_cc.op.input_tensors[1]
        self.tensor_map["fmap_matrix"] = self.tensor_map.get("fmap_fractal").op.input_tensors[0]
        if load2d_flag or self.binary_mode:
            self.tensor_map["fmap"] = self.tensor_map.get("fmap_matrix").op.input_tensors[0]
            self.tensor_map["fmap_l1"] = self.tensor_map.get("fmap_matrix")
        else:
            self.tensor_map["fmap_l1"] = self.tensor_map.get("fmap_matrix").op.input_tensors[0]
            self.tensor_map["fmap"] = self.tensor_map.get("fmap_l1").op.input_tensors[0]

        # BF16 only supports dynamic constantization and binary, static compilation fails
        if (self.tensor_map.get("fmap").dtype == "bfloat16" and not
            (get_op_context().get_addition("is_dynamic_constantization") or self.binary_mode)):
            return True

        # ########################extract parameters##########################
        cin1_g = group_dict.get('cin1_g')
        cout_g = group_dict.get('cout_g')
        real_g = group_dict.get('real_g')

        batch_grads, depth_grads, c1_grads, height_grads, \
            width_grads, c0_grads = cube_util.shape_to_list(self.tensor_map.get("grads").shape)
        _, grads_matrix_c1, grads_matrix_howo, grads_matrix_c0 = \
            cube_util.shape_to_list(self.tensor_map.get("grads_matrix").shape)

        batch_fmap, depth_fmap, c1_fmap, height_fmap, width_fmap, c0_fmap \
            = cube_util.shape_to_list(self.tensor_map.get("fmap").shape)
        _, fkk, _, _ = cube_util.shape_to_list(dw_cc.shape)
        _, _, hw_pad_1, _, _, _ = cube_util.shape_to_list(self.tensor_map.get("fmap_fractal").shape)

        # load_3d parameters
        stride_d = self.attr_dict.get('stride')[0]
        stride_h = self.attr_dict.get('stride')[1]
        stride_w = self.attr_dict.get('stride')[2]
        pad_front = self.attr_dict.get('pad')[0]
        pad_back = self.attr_dict.get('pad')[1]
        pad_up = self.attr_dict.get('pad')[2]
        pad_down = self.attr_dict.get('pad')[3]
        pad_left = self.attr_dict.get('pad')[4]
        pad_right = self.attr_dict.get('pad')[5]
        kernel_d = self.attr_dict.get('kernel_size')[2]
        kernel_h = self.attr_dict.get('kernel_size')[3]
        kernel_w = self.attr_dict.get('kernel_size')[4]
        dilation_d = self.attr_dict.get('dilation')[2]
        dilation_h = self.attr_dict.get('dilation')[3]
        dilation_w = self.attr_dict.get('dilation')[4]
        fmap_c = kernel_d * cin1_g * c0_fmap
        fmap_h = height_fmap
        fmap_w = width_fmap

        _set_var_range()

        tiling = _get_tiling()

        _tiling_shape_check()
        _tiling_buffer_check()
        dynamic_l0a_attach, dynamic_l0b_attach, dynamic_al1_attach, dynamic_bl1_attach = _get_attach_flag()

        batch_num = batch_fmap * depth_fmap
        block_dim_k, block_dim_batch, block_dim_cout, block_dim_cin, block_dim_group = _get_block_dim()
        self.tiling_para_dict["block_dim"] = [block_dim_k, block_dim_batch,
                                              block_dim_cout, block_dim_cin, block_dim_group]

        sch[self.tensor_map.get("grads_matrix")].set_scope(tbe_platform_info.scope_cbuf)
        sch[self.tensor_map.get("grads_fractal")].set_scope(tbe_platform_info.scope_ca)
        sch[self.tensor_map.get("fmap_matrix")].set_scope(tbe_platform_info.scope_cbuf)

        sch[self.tensor_map.get("fmap_fractal")].set_scope(tbe_platform_info.scope_cb)

        sch[self.tensor_map.get("grads_matrix")].storage_align(sch[self.tensor_map.get("grads_matrix")].op.axis[1],
                                                               _CUBE_MUL_SHAPE, 0)
        sch[self.tensor_map.get("grads_fractal")].buffer_align((1, 1), (1, 1), (1, 1), (1, 1),
                                                               (1, _CUBE_DIM), (1, _CUBE_DIM))
        sch[self.tensor_map.get("fmap_fractal")].buffer_align((1, 1), (1, 1), (1, 1), (1, 1),
                                                              (1, _CUBE_DIM), (1, _CUBE_DIM))

        _load3d_fmap_l1_process()
        # #######################tiling parameters analyze####################

        full_k_in_l0a, full_k_in_l0b = _full_k_check()

        _compute_tiling_parts()
        dw_tiling_factor = self.tiling_para_dict.get("dw_tiling_factor")
        dw_tiling_nparts = self.tiling_para_dict.get("dw_tiling_nparts")
        dw_ub_tiling_factor = self.tiling_para_dict.get("dw_ub_tiling_factor")
        grads_l1_tiling_nparts = self.tiling_para_dict.get("grads_l1_tiling_nparts")
        fmap_l1_tiling_nparts = self.tiling_para_dict.get("fmap_l1_tiling_nparts")
        l1_2_l0_tiling_nparts = self.tiling_para_dict.get("l1_2_l0_tiling_nparts")
        dw_k = self.tiling_para_dict.get("dw_k")

        reduce_split_mode = _reduce_split_mode()
        grads_l1_tiling_factor_k, fmap_l1_tiling_factor_k = _compute_tiling_factors()
        self.tiling_para_dict["grads_l1_tiling_factor_k"] = grads_l1_tiling_factor_k
        self.tiling_para_dict["fmap_l1_tiling_factor_k"] = fmap_l1_tiling_factor_k

        dw_cc, dw_ub, dw_ddr = _atomic_add(sch, dw_cc)
        self.tensor_map["dw_ddr"] = dw_ddr
        self.tensor_map["dw_cc"] = dw_cc

        batch_num_sc = compute_util.int_ceil_div(batch_num, block_dim_batch)

        # #############################split axis N##########################
        factor_kw, factor_kh = _get_n_factor()
        self._dw_ddr_split(factor_kh, factor_kw)

        def _ub_and_cc_attach():
            # optimization by move small loops to outer
            if not self.binary_mode:
                reorder_flag = l1_2_l0_tiling_nparts[0] > l1_2_l0_tiling_nparts[1]
                reorder_l1_mn = fmap_l1_tiling_nparts[1] > grads_l1_tiling_nparts[1]
            else:
                reorder_flag = False
                reorder_l1_mn = False
            # during L1 to L0, if M loop is smaller, then move to outer
            if reorder_flag:
                sch[dw_ddr].reorder(self.axis_dict.get("c_grads_mad_at"), self.axis_dict.get("c_fmap_mad_at"))
            # during sc to L1, if M loop is smaller, then move to outer
            if reorder_l1_mn:
                sch[dw_ddr].reorder(self.axis_dict.get("c_grads_l1_at"), self.axis_dict.get("c_fmap_l1_c1"),
                                    self.axis_dict.get("c_fmap_l1_kh"), self.axis_dict.get("c_fmap_l1_at"))

            c_fmap_2_ub_insn = self.axis_dict.get("c_fmap_mad_insn")
            if not self.support_l0c2out:
                # dw_ub attach
                # dw_ub split
                c_fmap_2_ub_at, c_fmap_2_ub_insn = sch[dw_ddr].split(self.axis_dict.get("c_fmap_mad_insn"),
                                                                     dw_ub_tiling_factor[0])
                # dw_ub attach
                sch[dw_ub].compute_at(sch[dw_ddr], c_fmap_2_ub_at)

            # dw attach
            if reorder_flag:
                sch[dw_cc].compute_at(sch[dw_ddr], self.axis_dict.get("c_fmap_mad_at"))
                self.l0c_attach_axis = self.axis_dict.get("c_fmap_mad_at")
            else:
                sch[dw_cc].compute_at(sch[dw_ddr], self.axis_dict.get("c_grads_mad_at"))
                self.l0c_attach_axis = self.axis_dict.get("c_grads_mad_at")
            return c_fmap_2_ub_insn

        c_fmap_2_ub_insn = _ub_and_cc_attach()

        self._dw_cc_split(reduce_split_mode, tiling)

        # #############################multi core#############################
        blocks = (block_dim_batch * block_dim_cin * block_dim_cout * block_dim_k * block_dim_group)
        fused_multi_core, block = self._bind_core(blocks, block_dim_batch)

        def _do_buffer_tile():
            if not self.dynamic_mode and not load2d_flag and tiling.get("BL1_shape"):
                k_bl1 = tiling.get("BL1_shape")[0]
                if width_grads % k_bl1 == 0:
                    step = 1
                else:
                    if k_bl1 % width_grads == 0:
                        step = k_bl1 // width_grads
                    else:
                        step = compute_util.int_ceil_div(k_bl1, width_grads) + 1
                extent_h = (step - 1) * stride_h + (kernel_h - 1) * dilation_h + 1
                if extent_h < height_fmap:
                    sch[self.tensor_map.get("fmap_l1")].buffer_tile(
                        (None, None),
                        (None, None),
                        (None, extent_h),
                        (None, None),
                        (None, None)
                    )

        _do_buffer_tile()
        _l0_attach()
        _al1_attach()
        _bl1_attach()
        _double_buffer()
        _emit_insn()
        binary_schedule._binary_constant()

        def _get_al1_bound():
            # al1 set storage bound
            if tiling.get("AL1_shape"):
                al1_m = tiling.get("AL1_shape")[1] * tiling.get("AL0_matrix")[0] * _CUBE_DIM
                al1_k = tiling.get("AL1_shape")[0]
                al1_bound = al1_k * al1_m
            else:
                al1_m = grads_matrix_c1 * grads_matrix_c0
                al1_bound = grads_matrix_howo * al1_m
            return al1_bound

        def _get_bl1_bound():
            """
            for bl1_bound set for dynamic

            Returns :
            bl1_bound: dynamic_mode param for storage_bound

            additional_rows: int
                param for buffer_tile in multi-core cases
                -1 indicates not used

            ho_len: int
                actually number of lines loaded in dynamic_batch
                -1 indicates not used
            """

            def _set_additional_rows(bl1_k, width_grads):
                """
                additional rows set for load3d

                Returns : 1. Exp for dynamic_hw
                    2. int for dynamic_batch
                """

                if (self.dynamic_mode & _DYNAMIC_HEIGHT != 0 or
                    self.dynamic_mode & _DYNAMIC_WIDTH != 0):
                    # dynamic_hw returns exp
                    # generally fix shape:
                    # 1. kbl1 small than wo: ho is 1 if wo % kbl1 is 0 else ho is 2
                    # 2. kbl1 bigger wo: ho is ceilDiv(kbl1, wo) if kbl1%wo is 0 else ho is ceilDiv(kbl1,wo) + 1
                    # dynamic_hw need to consider multi_core tail blocks: hw_single_core%wo == 0
                    return tvm.select(
                               bl1_k < width_grads,
                               tvm.select(
                                   tvm.all(
                                       tvm.floormod(width_grads, bl1_k) == 0,
                                       tvm.floormod(hw_single_core_factor,
                                                    width_grads) == 0),
                                   1,
                                   _LOOSE_LINE_CONDITION),
                               tvm.select(
                                   tvm.all(
                                       tvm.floormod(bl1_k, width_grads) == 0,
                                       tvm.floormod(hw_single_core_factor,
                                                    width_grads) == 0),
                                   0,
                                   tvm.select(
                                       tvm.all(tvm.floormod(bl1_k*2, width_grads) == 0,
                                               tvm.floormod(hw_single_core_factor,
                                                    width_grads) == 0),
                                       1,
                                       _LOOSE_LINE_CONDITION)))

                if  bl1_k % width_grads == 0:
                    # fully loaded dont extra line
                    return 0

                if (bl1_k * 2 % width_grads == 0
                    or bl1_k % width_grads == 1):
                    # special cases need only 1 extra line
                    return 1

                # other situations need 2 extra lines
                return 2

            if tiling.get("BL1_shape"):
                bl1_k = tiling.get("BL1_shape")[0]
                bl1_k_full = bl1_k
                if load2d_flag:
                    additional_rows = -1
                    ho_len = -1
                elif (self.dynamic_mode == _DYNAMIC_BATCH
                      and bl1_k < width_grads):
                    additional_rows = -1
                    ho_len = 2
                    # check if only need to load int times of bl0
                    ho_len = 1 if (width_grads % bl1_k == 0) \
                        else _LOOSE_LINE_CONDITION
                    hi_max = (kernel_h - 1) * dilation_h + 1 + (ho_len - 1) * stride_h
                    bl1_k_full = width_fmap * hi_max
                else:
                    # load3d can not split width_grads
                    additional_rows = _set_additional_rows(bl1_k, width_grads)

                    ho_len = tvm.floordiv(bl1_k, width_grads) + additional_rows
                    hi_max = (kernel_h - 1) * dilation_h + 1 + (ho_len - 1) * stride_h
                    bl1_k_full = hi_max * width_fmap

                bl1_bound = bl1_k_full * tiling.get("BL1_shape")[1] * \
                            tiling.get("BL0_matrix")[1] // \
                            (kernel_h * kernel_w) * c0_fmap
            else:
                bl1_k_full = compute_util.int_ceil_div(width_fmap * height_fmap, block_dim_k)
                bl1_k_full = compute_util.align(bl1_k_full, _CUBE_DIM)
                bl1_bound = bl1_k_full * c1_fmap * c0_fmap
                ho_len = height_fmap
                additional_rows = -1

            return bl1_bound, additional_rows, ho_len

        def _set_tile_mode():
            # set buffer tile mode for dynamic
            if dynamic_bl1_attach == "dw_cc":
                # hw need both multi-core offset and k_axis offset
                return "tile_h_dw_cc"

            elif dynamic_bl1_attach == "dw_ddr":
                # hw only need multi_core offset
                return "tile_h_dw_ddr"

            return "None"

        def _set_tile_params(ho_len, tile_mode):
            ho_min = 0

            # axis_k offset
            wi_min = -pad_left
            wi_extent = width_fmap + pad_left + pad_right

            bl1_k = tiling.get("BL1_shape")[0]
            al1_k = grads_matrix_howo
            if tiling.get("AL1_shape"):
                al1_k = tiling.get("AL1_shape")[0]

            if self.axis_dict.get("bl1_at_axis") == self.axis_dict.get("hw_mad_1_l1_in_at"):
                # hw splited two times before BL1 attach
                axis_k_var = (self.axis_dict.get("hw_mad_1_l1_out_at").var * al1_k +
                              self.axis_dict.get("hw_mad_1_l1_in_at").var * bl1_k)
            else:
                # hw splited one time before BL1 attach
                axis_k_var = self.axis_dict.get("hw_mad_1_l1_out_at").var * bl1_k

            if self.dynamic_mode == _DYNAMIC_BATCH:
                # multi_core offset
                multi_core_offset = tvm.floordiv(
                                        tvm.floordiv(fused_multi_core,
                                                     block_dim_cout * block_dim_cin * block_dim_group),
                                        tvm.floordiv(batch_fmap * depth_grads - 1,
                                                        self.tiling_para_dict.get("batch_dim_factor")) +
                                        1) * hw_single_core_factor

                if tile_mode == "tile_h_dw_cc":
                    ho_min = tvm.floordiv(multi_core_offset + axis_k_var, width_grads)
                elif tile_mode == "tile_h_dw_ddr":
                    ho_min = tvm.floordiv(multi_core_offset, width_grads)
            else:
                # multi_core offset
                block_div = (self.tiling_para_dict.get("batch_dim_npart") *
                             block_dim_cout * block_dim_cin * block_dim_group)
                multi_core_offset = fused_multi_core // block_div * hw_single_core_factor

                if tile_mode == "tile_h_dw_cc":
                    hw_block_offset = multi_core_offset + axis_k_var
                    ho_min = tvm.floordiv(hw_block_offset, width_grads)

                elif tile_mode == "tile_h_dw_ddr":
                    hw_block_offset = multi_core_offset
                    ho_min = tvm.floordiv(hw_block_offset, width_grads)

                # dynamic_hw need process multi_core tail blocks
                ho_len = compute_util.int_ceil_div(tvm.floormod(hw_block_offset, width_grads) + bl1_k, width_grads)

            hi_min = ho_min * stride_h - pad_up

            # Calculate the min and extent of the h dimension bound
            hi_extent = (kernel_h - 1) * dilation_h + 1 + (ho_len - 1) * stride_h

            return hi_min, hi_extent, wi_min, wi_extent

        def _dynamic_bl1_buffer_tile(ho_len):
            # buffer_tile for dynamic mode
            if not self.binary_mode and not load2d_flag:
                tile_mode = _set_tile_mode()

                hi_min, hi_extent, wi_min, wi_extent = _set_tile_params(ho_len, tile_mode)

                if "tile_h" not in tile_mode:
                    hi_min, hi_extent = None, None

                sch[self.tensor_map.get("fmap_l1")].buffer_tile((None, None), (None, None),
                                                                (hi_min, hi_extent),
                                                                (wi_min, wi_extent),
                                                                (None, None))

        def _dynamic_memory_management():
            # sequential_malloc
            sch.sequential_malloc(tbe_platform_info.scope_cbuf)
            sch.sequential_malloc(tbe_platform_info.scope_ca)
            sch.sequential_malloc(tbe_platform_info.scope_cb)
            sch.sequential_malloc(tbe_platform_info.scope_cc)
            if not self.support_l0c2out:
                sch.sequential_malloc(tbe_platform_info.scope_ubuf)
                sch[dw_ub].mem_unique()

            # mem_unique
            if not load2d_flag:
                sch[self.tensor_map.get("fmap_l1")].mem_unique()
            sch[self.tensor_map.get("grads_matrix")].mem_unique()
            sch[self.tensor_map.get("fmap_matrix")].mem_unique()
            sch[self.tensor_map.get("grads_fractal")].mem_unique()
            sch[self.tensor_map.get("fmap_fractal")].mem_unique()
            sch[dw_cc].mem_unique()

        def _manage_memory_process(bl1_bound):
            _dynamic_memory_management()
            bl1_bound = binary_schedule.cache_tiling.get("bl1_bound") if self.binary_mode else bl1_bound
            al0_bound = reduce(lambda x, y: x * y, tiling.get("AL0_matrix")[:-1])
            bl0_bound = reduce(lambda x, y: x * y, tiling.get("BL0_matrix")[:-1])
            cl0_bound = reduce(lambda x, y: x * y, tiling.get("CL0_matrix")[:-1])
            sch[self.tensor_map.get("grads_matrix")].set_buffer_size(al1_bound)
            if load2d_flag:
                sch[self.tensor_map.get("fmap_matrix")].set_buffer_size(bl1_bound)
            if self.binary_mode:
                sch[self.tensor_map.get("fmap_matrix")].set_buffer_size(bl1_bound)
                sch[self.tensor_map.get("grads_fractal")].set_buffer_size(al0_bound)
                sch[self.tensor_map.get("fmap_fractal")].set_buffer_size(bl0_bound)
                sch[dw_cc].set_buffer_size(cl0_bound)

        if self.dynamic_mode:
            hw_single_core_factor = compute_util.int_ceil_div(hw_pad_1, block_dim_k) * _CUBE_DIM
            hw_single_core_factor = compute_util.align(hw_single_core_factor, dw_k * _CUBE_DIM)
            al1_bound = _get_al1_bound()
            bl1_bound, _, ho_len = _get_bl1_bound()
            _dynamic_bl1_buffer_tile(ho_len)
            _manage_memory_process(bl1_bound)

        return True

    def _dw_ddr_split(self, factor_kh, factor_kw):
        """
        Split dw_ddr according to tiling
        """
        if self.binary_mode:
            self._dw_ddr_binary_split()
        else:
            self._dw_ddr_normal_split(factor_kh, factor_kw)

    def _dw_ddr_binary_split(self):
        kernel_h = self.attr_dict.get('kernel_size'[3])
        kernel_w = self.attr_dict.get('kernel_size'[4])
        dw_ddr = self.tensor_map.get("dw_ddr")
        _, _, block_dim_cout, block_dim_cin, block_dim_g = self.tiling_para_dict.get("block_dim")
        # dw_shape is (real_g, fmap_channel_1*kernel_height*kernel_width,
        #              grads_channel_1, C0_grads, C0_fmap)
        g_multicore, g_axis = self.sch_agent[dw_ddr].split(self.sch[dw_ddr].op.axis[0],
                                                           nparts=block_dim_g,
                                                           split_params=self.binary_schedule.split_params)

        nparts_n_block_dim, nparts_n_l0, nparts_n_l1 = [None, None, None]
        factor_n_block_dim, factor_n_l0, factor_n_l1 = [None, None, None]
        nparts_m_block_dim, nparts_m_l0, nparts_m_l1 = [None, None, None]
        factor_m_block_dim, factor_m_l0, factor_m_l1 = [None, None, None]
        cache_tiling = self.binary_schedule.cache_tiling
        n_l0 = cache_tiling.get("n_ub_l0_time") * cache_tiling.get("cub_n1")
        factor_n_block_dim = cache_tiling.get("n_single_core") * cache_tiling.get("n_bl1") * n_l0
        factor_n_l0 = n_l0
        factor_n_l1 = cache_tiling.get("n_bl1")

        factor_m_l0 = cache_tiling.get("m_l0") * _CUBE_DIM
        factor_m_block_dim = cache_tiling.get("m_single_core") * cache_tiling.get("m_al1") * factor_m_l0
        factor_m_l1 = cache_tiling.get("m_al1")
        split_param0 = self.binary_schedule.split_nofactor_params

        c_fmap_multicore, c_fmap_mad_at \
            = self.sch_agent[dw_ddr].split(self.sch[dw_ddr].op.axis[1], factor=factor_n_block_dim,
                                           nparts=nparts_n_block_dim, split_params=split_param0)
        # factorization in ecah single core
        c_fmap_mad_at, c_fmap_mad_insn \
            = self.sch_agent[dw_ddr].split(c_fmap_mad_at, factor=factor_n_l0, nparts=nparts_n_l0,
                                           split_params=self.binary_schedule.split_params)
        c_fmap_axis_list = [c_fmap_mad_insn, ]
        c_fmap_l1_at, c_fmap_mad_at \
            = self.sch_agent[dw_ddr].split(c_fmap_mad_at, factor=factor_n_l1, nparts=nparts_n_l1,
                                           split_params=self.binary_schedule.split_params)

        # split axis M
        c_grads_multicore, c_grads_mad_at \
            = self.sch_agent[dw_ddr].split(self.sch[dw_ddr].op.axis[2], factor=factor_m_block_dim,
                                           nparts=nparts_m_block_dim, split_params=split_param0)
        c_grads_mad_at, c_grads_mad_insn \
            = self.sch_agent[dw_ddr].split(c_grads_mad_at, factor=factor_m_l0, nparts=nparts_m_l0,
                                           split_params=self.binary_schedule.split_params)
        c_grads_l1_at, c_grads_mad_at \
            = self.sch_agent[dw_ddr].split(c_grads_mad_at, factor=factor_m_l1, nparts=nparts_m_l1,
                                           split_params=self.binary_schedule.split_params)

        self.sch[dw_ddr].reorder(self.sch[dw_ddr].op.reduce_axis[0], g_multicore, c_grads_multicore,
                                 c_fmap_multicore, g_axis, c_fmap_l1_at, c_grads_l1_at, c_fmap_mad_at,
                                 c_grads_mad_at, *c_fmap_axis_list, c_grads_mad_insn, self.sch[dw_ddr].op.axis[3])
        self.axis_dict.update({
            "c_grads_mad_at": c_grads_mad_at, "c_grads_l1_at": c_grads_l1_at, "g_multicore": g_multicore,
            "c_grads_multicore": c_grads_multicore, "c_fmap_multicore": c_fmap_multicore, "c_fmap_l1_at": c_fmap_l1_at,
            "c_fmap_mad_at": c_fmap_mad_at, "c_fmap_mad_insn": c_fmap_mad_insn, "c_grads_mad_insn": c_grads_mad_insn})

    def _dw_ddr_normal_split(self, factor_kh, factor_kw):
        """
        Split dw_ddr of common scenes
        """
        dw_ddr = self.tensor_map.get("dw_ddr")
        _, _, block_dim_cout, block_dim_cin, block_dim_g = self.tiling_para_dict.get("block_dim")

        g_multicore, g_axis = self.sch[dw_ddr].split(self.sch[dw_ddr].op.axis[0], nparts=block_dim_g)
        c_fmap_multicore, c_fmap_mad_at = self.sch[dw_ddr].split(self.sch[dw_ddr].op.axis[1], nparts=block_dim_cin)

        c_fmap_mad_at, c_fmap_mad_insn = self.sch[dw_ddr].split(
            c_fmap_mad_at, nparts=self.tiling_para_dict.get("dw_tiling_nparts")[0])

        c_fmap_l1_ori, c_fmap_mad_at = self.sch[dw_ddr].split(
            c_fmap_mad_at, nparts=self.tiling_para_dict.get("fmap_l1_tiling_nparts")[1])
        # split axis N
        c_fmap_l1_out, c_fmap_l1_at = self.sch[dw_ddr].split(c_fmap_l1_ori, factor_kw)

        c_fmap_l1_c1, c_fmap_l1_kh = self.sch[dw_ddr].split(c_fmap_l1_out, factor_kh)

        # split axis M
        c_grads_mad_at, c_grads_mad_insn = self.sch[dw_ddr].split(
            self.sch[dw_ddr].op.axis[2], self.tiling_para_dict.get("dw_tiling_factor")[1]*_CUBE_DIM)

        c_grads_multicore, c_grads_mad_at = self.sch[dw_ddr].split(
            c_grads_mad_at, nparts=block_dim_cout)

        c_grads_l1_at, c_grads_mad_at = self.sch[dw_ddr].split(
            c_grads_mad_at, nparts=self.tiling_para_dict.get("grads_l1_tiling_nparts")[1])

        # reorder according to requirments of mmad EmitInsn
        self.sch[dw_ddr].reorder(self.sch[dw_ddr].op.reduce_axis[0], g_multicore,
                            c_grads_multicore, c_fmap_multicore,
                            g_axis, c_fmap_l1_c1, c_fmap_l1_kh,
                            c_fmap_l1_at, c_grads_l1_at, c_fmap_mad_at,
                            c_grads_mad_at, c_fmap_mad_insn, c_grads_mad_insn, self.sch[dw_ddr].op.axis[3])
        self.axis_dict["c_grads_mad_at"] = c_grads_mad_at
        self.axis_dict["c_grads_l1_at"] = c_grads_l1_at
        self.axis_dict["g_multicore"] = g_multicore
        self.axis_dict["c_grads_multicore"] = c_grads_multicore
        self.axis_dict["c_fmap_multicore"] = c_fmap_multicore
        self.axis_dict["c_fmap_l1_c1"] = c_fmap_l1_c1
        self.axis_dict["c_fmap_l1_kh"] = c_fmap_l1_kh
        self.axis_dict["c_fmap_l1_at"] = c_fmap_l1_at
        self.axis_dict["c_fmap_mad_at"] = c_fmap_mad_at
        self.axis_dict["c_fmap_mad_insn"] = c_fmap_mad_insn
        self.axis_dict["c_grads_mad_insn"] = c_grads_mad_insn

    def _dw_cc_dynamic(self, reduce_split_mode, tiling):
        # get the 2 reduce axis of dw_cc
        dw_cc = self.tensor_map.get("dw_cc")
        dw_k = self.tiling_para_dict.get("dw_k")
        batch_axis_sc, k_1_axis_sc = self.sch[dw_cc].op.reduce_axis
        k_1_axis_sc_out_size = k_1_axis_sc.dom.extent // (dw_k * _CUBE_DIM)
        batch_insn_o_size = batch_axis_sc.dom.extent
        grads_l1_tiling_factor_k = self.tiling_para_dict.get("grads_l1_tiling_factor_k")
        fmap_l1_tiling_factor_k = self.tiling_para_dict.get("fmap_l1_tiling_factor_k")
        fmap_l1_tiling_nparts = self.tiling_para_dict.get("fmap_l1_tiling_nparts")
        grads_l1_tiling_nparts = self.tiling_para_dict.get("grads_l1_tiling_nparts")
        dw_tiling_factor = self.tiling_para_dict.get("dw_tiling_factor")

        # dw_k is the part for one MMAD
        hw_mad_1_mad_at, hw_mad_1_mad_insn = self.sch[dw_cc].split(k_1_axis_sc, dw_k * _CUBE_DIM)
        # mad_pattern :2 , the 1st axis should be 1, so do a fake split
        batch_insn_o, batch_insn = self.sch[dw_cc].split(batch_axis_sc, 1)
        self.axis_dict.update({"batch_insn_o": batch_insn_o, "batch_insn": batch_insn})
        if reduce_split_mode:
            # the factor of grads_l1 is smaller than fmap_l1
            if tiling.get("AL1_shape") and tiling.get("BL1_shape"):
                # grads and fmap need tiling in L1
                hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch[dw_cc].split(hw_mad_1_mad_at, grads_l1_tiling_factor_k)
                hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch[dw_cc].split(hw_mad_1_l1_at, fmap_l1_tiling_factor_k)
            elif tiling.get("AL1_shape"):
                # only grads needs tiling in L1
                hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch[dw_cc].split(hw_mad_1_mad_at, grads_l1_tiling_factor_k)
                hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch[dw_cc].split(
                    hw_mad_1_l1_at, nparts=fmap_l1_tiling_nparts[0])
            bl1_at_axis = hw_mad_1_l1_out_at
            al1_at_axis = hw_mad_1_l1_in_at
            axis_k_reduce_for_mad = (hw_mad_1_l1_out_at * (grads_l1_tiling_nparts[0] // fmap_l1_tiling_nparts[0]) +
                hw_mad_1_l1_in_at) * grads_l1_tiling_factor_k + hw_mad_1_mad_at

        else:
            # the factor of fmap_l1 is smaller than grads_l1
            if tiling.get("AL1_shape") and tiling.get("BL1_shape"):
                # grads and fmap need tiling in L1
                hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch[dw_cc].split(hw_mad_1_mad_at, fmap_l1_tiling_factor_k)
                hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch[dw_cc].split(hw_mad_1_l1_at, grads_l1_tiling_factor_k)
            elif tiling.get("BL1_shape"):
                # only fmap needs tiling in L1
                hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch[dw_cc].split(hw_mad_1_mad_at, fmap_l1_tiling_factor_k)
                hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch[dw_cc].split(
                    hw_mad_1_l1_at, nparts=grads_l1_tiling_nparts[0])
            else:
                # Neither grads nor fmap need tiling in L1
                hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch[dw_cc].split(
                    hw_mad_1_mad_at, nparts=fmap_l1_tiling_nparts[0])
                hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch[dw_cc].split(
                    hw_mad_1_l1_at, nparts=grads_l1_tiling_nparts[0])
            al1_at_axis = hw_mad_1_l1_out_at
            bl1_at_axis = hw_mad_1_l1_in_at
            axis_k_reduce_for_mad = (hw_mad_1_l1_out_at * (fmap_l1_tiling_nparts[0] // grads_l1_tiling_nparts[0]) +
                hw_mad_1_l1_in_at) * (k_1_axis_sc_out_size // fmap_l1_tiling_nparts[0]) + hw_mad_1_mad_at

        # split dw_cc.op.axis[0](N1), factor is one MMAD
        fkk_mad_at, fkk_mad_insn = self.sch[dw_cc].split(self.sch[dw_cc].op.axis[2], dw_tiling_factor[0])

        # split dw_cc.op.axis[1](M1*M0), factor is one MMAD
        lc_mad_at, lc_mad_insn = self.sch[dw_cc].split(self.sch[dw_cc].op.axis[3], dw_tiling_factor[1] * _CUBE_DIM)
        self.sch[dw_cc].reorder(fkk_mad_at, lc_mad_at, self.sch[dw_cc].op.axis[0],
                                batch_insn_o, hw_mad_1_l1_out_at, hw_mad_1_l1_in_at, hw_mad_1_mad_at,
                                batch_insn, fkk_mad_insn, lc_mad_insn, self.sch[dw_cc].op.axis[4], hw_mad_1_mad_insn)
        self.axis_dict.update({"al1_at_axis": al1_at_axis, "bl1_at_axis": bl1_at_axis,
                                "hw_mad_1_mad_at": hw_mad_1_mad_at, "batch_insn_o": batch_insn_o,
                                "hw_mad_1_l1_out_at": hw_mad_1_l1_out_at, "hw_mad_1_l1_in_at": hw_mad_1_l1_in_at,
                                "batch_insn": batch_insn})
        self.axis_size = {"axis_k_reduce_for_mad": axis_k_reduce_for_mad, "batch_insn_o_size": batch_insn_o_size}

    def _dw_cc_split_l0(self, dw_cc):
        """
        Split dw_cc according to L0 tiling
        """
        dw_k = self.tiling_para_dict.get("dw_k")
        axis_cc_outer, axis_cc_inner = None, None
        if self.binary_mode:
            # get the 2 reduce axis of dw_cc
            batch_axis_single_core, k_1_axis_single_core = self.sch[dw_cc].op.reduce_axis
            k_l0_factor = dw_k * self.c0_size
        else:
            # get the 3 reduce axis of dw_cc
            batch_axis_single_core, k_1_axis_single_core, k_0 = self.sch[dw_cc].op.reduce_axis
            k_l0_factor = dw_k
        # dw_k is the part for one MMAD
        hw_mad_1_mad_at, hw_mad_1_mad_insn = self.sch_agent[dw_cc].split(k_1_axis_single_core, k_l0_factor)
        # mad_pattern :2 , the 1st axis should be 1, so do a fake split
        batch_insn_o, batch_insn = self.sch_agent[dw_cc].split(batch_axis_single_core, 1)
        axis_cc_outer = [batch_insn_o, ]
        axis_cc_inner = [hw_mad_1_mad_insn, ] if self.var_map else [hw_mad_1_mad_insn, k_0]
        self.axis_dict.update({"batch_insn_o": batch_insn_o, "batch_insn": batch_insn,
                               "k_1_axis_single_core": k_1_axis_single_core,
                               "batch_axis_single_core": batch_axis_single_core})
        return axis_cc_outer, axis_cc_inner, hw_mad_1_mad_at

    def _dw_cc_normal(self, reduce_split_mode):
        dw_cc = self.tensor_map.get("dw_cc")
        axis_cc_outer, axis_cc_inner, hw_mad_1_mad_at = self._dw_cc_split_l0(dw_cc)
        dw_k = self.tiling_para_dict.get("dw_k")
        fmap_l1_tiling_nparts = self.tiling_para_dict.get("fmap_l1_tiling_nparts")
        grads_l1_tiling_nparts = self.tiling_para_dict.get("grads_l1_tiling_nparts")
        dw_tiling_factor = self.tiling_para_dict.get("dw_tiling_factor")
        # dw_cc split
        # K of AL1 and BL1 can be different, there are 2 split methods
        # on which one is larger
        k_1_axis_sc_out_size = compute_util.int_ceil_div(self.axis_dict.get("k_1_axis_single_core").dom.extent, dw_k)
        batch_insn_o_size = self.axis_dict.get("batch_axis_single_core").dom.extent

        if reduce_split_mode:
            hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch_agent[dw_cc].split(hw_mad_1_mad_at,
                                                                          nparts=grads_l1_tiling_nparts[0])
            hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch_agent[dw_cc].split(hw_mad_1_l1_at,
                                                                                nparts=fmap_l1_tiling_nparts[0])
            al1_at_axis = hw_mad_1_l1_in_at
            bl1_at_axis = hw_mad_1_l1_out_at
            axis_k_reduce_for_mad = (hw_mad_1_l1_out_at * (grads_l1_tiling_nparts[0] // fmap_l1_tiling_nparts[0]) +
                hw_mad_1_l1_in_at) * (k_1_axis_sc_out_size // grads_l1_tiling_nparts[0]) + hw_mad_1_mad_at
        else:
            hw_mad_1_l1_at, hw_mad_1_mad_at = self.sch_agent[dw_cc].split(
                hw_mad_1_mad_at, nparts=fmap_l1_tiling_nparts[0])
            hw_mad_1_l1_out_at, hw_mad_1_l1_in_at = self.sch_agent[dw_cc].split(
                hw_mad_1_l1_at, nparts=grads_l1_tiling_nparts[0])
            bl1_at_axis = hw_mad_1_l1_in_at
            al1_at_axis = hw_mad_1_l1_out_at
            axis_k_reduce_for_mad = (hw_mad_1_l1_out_at * (fmap_l1_tiling_nparts[0] // grads_l1_tiling_nparts[0]) +
                hw_mad_1_l1_in_at) * (k_1_axis_sc_out_size // fmap_l1_tiling_nparts[0]) + hw_mad_1_mad_at

        # split dw_cc.op.axis[0](N1), factor is one MMAD
        fkk_mad_at, fkk_mad_insn  = self.sch_agent[dw_cc].split(self.sch[dw_cc].op.axis[2], dw_tiling_factor[0])

        # split dw_cc.op.axis[1](M1*M0), factor is one MMAD
        lc_mad_at, lc_mad_insn = self.sch_agent[dw_cc].split(
            self.sch[dw_cc].op.axis[3], dw_tiling_factor[1] * _CUBE_DIM)
        batch_insn = self.axis_dict.get("batch_insn")
        self.sch[dw_cc].reorder(fkk_mad_at, lc_mad_at, self.sch[dw_cc].op.axis[0],
                                *axis_cc_outer, hw_mad_1_l1_out_at, hw_mad_1_l1_in_at,
                                hw_mad_1_mad_at, batch_insn, fkk_mad_insn,
                                lc_mad_insn, self.sch[dw_cc].op.axis[4],
                                *axis_cc_inner)
        self.axis_dict.update({"al1_at_axis": al1_at_axis, "bl1_at_axis": bl1_at_axis,
                                "hw_mad_1_mad_at": hw_mad_1_mad_at,
                                "hw_mad_1_l1_out_at": hw_mad_1_l1_out_at, "hw_mad_1_l1_in_at": hw_mad_1_l1_in_at,
                                "batch_insn": batch_insn})
        self.axis_size = {"axis_k_reduce_for_mad": axis_k_reduce_for_mad, "batch_insn_o_size": batch_insn_o_size}

    def _dw_cc_split(self, reduce_split_mode, tiling):
        if not self.binary_mode and self.dynamic_mode and not self.flag_all_one_case:
            self._dw_cc_dynamic(reduce_split_mode, tiling)
        else:
            self._dw_cc_normal(reduce_split_mode)

    def _bind_core_fuse_axis(self, blocks, block_dim_batch):
        """
        bind core by the fused axis
        """
        dw_ddr = self.tensor_map.get("dw_ddr")
        fused_multi_core = \
            self.sch[dw_ddr].fuse(self.sch[dw_ddr].op.reduce_axis[0], self.axis_dict.get("g_multicore"),
                                    self.axis_dict.get("c_grads_multicore"), self.axis_dict.get("c_fmap_multicore"))
        fused_multi_core, pragma_at = self.sch[dw_ddr].split(fused_multi_core, 1)
        block = tvm.thread_axis("blockIdx.x")
        self.sch[dw_ddr].bind(fused_multi_core, block)

        if blocks == block_dim_batch:
            self.sch[dw_ddr].pragma(pragma_at, 'json_info_batchBindOnly')
        return fused_multi_core, block

    def _bind_core_axis_list(self):
        """
        bind core by the axis list
        """
        axis_list = [self.sch[self.tensor_map.get("dw_ddr")].op.reduce_axis[0], self.axis_dict.get("g_multicore"),
                     self.axis_dict.get("c_grads_multicore"), self.axis_dict.get("c_fmap_multicore")]
        block = tvm.thread_axis("blockIdx.x")
        self.sch.bind_axes(axis_list, block)
        self.sch[self.tensor_map.get("dw_ddr")].remove_init()
        return self.axis_dict.get("c_fmap_multicore"), block

    def _bind_core(self, blocks, block_dim_batch):
        """
        bind core according to tiling
        """
        if self.binary_mode:
            return self._bind_core_axis_list()

        return self._bind_core_fuse_axis(blocks, block_dim_batch)


class Conv3DBpFilterBinaryDynamic(ConvBpFilterBinaryDynamic):
    """
    special for dynamic binary
    """
    def __init__(self, sch, binary_mode, flag_load3d_w_split_case, flag_all_one_case):
        super().__init__(sch, binary_mode, flag_load3d_w_split_case, flag_all_one_case)

    def set_shape_var_range(self):
        """
        set var range for cache tiling vars and shape vars
        """
        range_shape = self._tiling_range.get("range_shape")
        range_pad = self._tiling_range.get("range_pad")
        range_stride = self._tiling_range.get("range_stride")
        range_kernel = self._tiling_range.get("range_kernel")
        range_dilation = self._tiling_range.get("range_dilation")
        range_c = self._tiling_range.get("range_c")
        range_c1 = self._tiling_range.get("range_c1")
        range_max = self._tiling_range.get("range_max")
        shape_var_range = {
            "batch_n": range_shape,
            "dedy_d": range_max,
            "dedy_h": range_shape,
            "dedy_w": range_shape,
            "fmap_d": range_max,
            "fmap_h": range_shape,
            "fmap_w": range_shape,
            "padf": range_pad,
            "padb": range_pad,
            "padu": range_pad,
            "padd": range_pad,
            "padl": range_pad,
            "padr": range_pad,
            "stride_d": range_stride,
            "stride_h": range_stride,
            "stride_w": range_stride,
            "kernel_d": range_kernel,
            "kernel_h": range_kernel,
            "kernel_w": range_kernel,
            "dilation_d": range_dilation,
            "dilation_h": range_dilation,
            "dilation_w": range_dilation,
            "fmap_c": range_c,
            "dedy_c": range_c,
            "fmap_c1": range_c1,
            "dedy_c1": range_c1,
            "cout1_g": range_c1,
            "cin1_g": range_c1,
            "real_g": range_shape,
            "mag_factor": range_shape
        }
        for var, var_range in shape_var_range.items():
            self.sch.set_var_range(get_te_var(var).get_tvm_var(), *var_range)

    def set_tiling_var_range(self):
        """set tiling var range"""
        range_shape = self._tiling_range.get("range_shape")
        range_outer_axis = self._tiling_range.get("range_outer_axis")
        range_l0_ub_param = self._tiling_range.get("range_l0_ub_param")
        range_l1 = self._tiling_range.get("range_l1")
        range_block_dim = self._tiling_range.get("range_block_dim")
        tiling_var_range = {
            "group_dim": range_block_dim,
            "batch_dim": range_block_dim,
            "n_dim": range_block_dim,
            "m_dim": range_block_dim,
            "k_dim": range_block_dim,
            "batch_single_core": range_shape,
            "m_single_core": range_shape,
            "n_single_core": range_outer_axis,
            "m_al1": range_outer_axis,
            "n_bl1": range_outer_axis,
            "cub_n1": range_l0_ub_param,
            "m_l0": range_l0_ub_param,
            "k_l0": range_l0_ub_param,
            "n_ub_l0_time": range_l0_ub_param,
            "kal1_factor": range_outer_axis,
            "kbl1_factor": range_outer_axis,
            "kal0_factor": range_outer_axis,
            "kbl0_factor": range_outer_axis,
            "bl1_bound": range_l1,
            "kl1_times": self._tiling_range.get("range_kl1_times"),
            "ho_bL1": range_shape,
            "load3d_special": self._tiling_range.get("range_load3d_special")
            }

        for var, var_range in tiling_var_range.items():
            self.sch.set_var_range(get_te_var(var).get_tvm_var(), *var_range)