#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Sub-function of gemm_integrated_schedule
"""
from tbe import tvm
from tbe.common.platform import platform_info as tbe_platform
from tbe.common.buildcfg import build_config
from tbe.common.utils.errormgr import error_manager_util
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.boost_schedule_kit import SplitParam
from tbe.dsl.compute.util import get_value
from tbe.dsl.compute.util import int_ceil_div
from tbe.dsl.compute.gemm_compute_util import FormatCompute


def copy_attrs(src_tensor, dst_tensor):
    for attr_key, value in src_tensor.op.attrs.items():
        dst_tensor.op.attrs[attr_key] = value


def print_ir_matmul(debug_ir_flag, process, sch):
    """
    print ir for input sch
    :param process: tag
    :param sch: schedule
    :return: IR process
    """
    if debug_ir_flag:
        with build_config():
            start = process + " IR start"
            end = process + " IR end\n"
            sch = sch.normalize()
            print(start)
            bounds = tvm.schedule.InferBound(sch)
            stmt = tvm.ScheduleOps(sch, bounds, True)
            print(stmt)
            print(end)


def debug(debug_param_flag, info, tag=""):
    """
    print log if debug
    :param info:
    :return:
    """
    if debug_param_flag:
        print("----")
        print(tag, info)


def get_optional_te_var(var_name):
    return None if not get_te_var(var_name) else get_te_var(var_name).get_tvm_var()


def get_al1_m_fix_value(m_shape):
    fix_each_time = 2
    fix_value = 1
    if not in_dynamic():
        # 65536 is 2Byte's max number + 1
        while m_shape >= 65536:
            m_shape //= fix_each_time
            fix_value *= fix_each_time
    return fix_value


class GemmScheduleContainer:
    """
    This is a container Class used to store all containers,
    """
    def __init__(self, para_map):
        """
        fuse_num_group in para_map
        """
        self.tensor_map = {}
        self.buffer_reuse_dict = {}
        self.elemwise_tensors = []
        self.matmul_dequant_tensor = []
        self.ele_header_ub_tensors = []
        self.dequant_activation_tensor = []
        self.header_ub_tensors = []
        self.tensors_in_aub = []
        self.tensors_in_bub = []
        self.tensors_in_cub = []
        self.tensors_in_l0c = []
        self.matmul_tensors = []
        self.fusion_list = []
        self.fusion_ele = []
        self.tensor_fusion_list = []
        self.compute_inline_list = []
        self.elewise_compute_inline_list = []
        self.fusion_tensor_cub = []
        self.fuse_num_group = para_map.get("fuse_num_group")
        self.vector_muls_attr = {}
        self.axis_core = None
        self.k_axis_core = None
        self.double_out_tensor = []
        self.mix_cache_read_list = []
        self.nd2nz_dict = {"layout_transform": "nd2nz", "split_select": 1}
        if in_dynamic():
            self.nd2nz_dict = {"layout_transform": "nd2nz", "no_select2if": 1}
        self.mix_workspace_tensor = {}


class GemmScheduleStatusController:
    """
    This is a controller used to control flags like "attach_status"
    """
    def __init__(self, para_map):
        self.gm_ub = None
        self.have_batch_a, self.have_batch_b, self.have_batch = [para_map.get("have_batch_a", False),
                                                                 para_map.get("have_batch_b", False),
                                                                 para_map.get("have_batch", False)]
        self.need_init_bias = False
        self.b_l1_inline_flag = False
        self.reduce_fusion = False
        self.l1_fusion_and_l1_size_0 = False
        self.input_l1_flag = False
        self.aub_attach_status = "full_load"
        self.bub_attach_status = "full_load"
        self.al1_attach_status = "full_load"
        self.bl1_attach_status = "full_load"
        self.c_l0c_attach_status = "full_load"
        self.al0_attach_status = "full_load"
        self.c_ub_attach_status = "full_load"
        self.ops_data_flow_mode = para_map.get("ops_data_flow_mode")
        self.transpose_a, self.transpose_b = para_map.get("trans_a"), para_map.get("trans_b")
        self.a_use_aligned_pattern, self.b_use_aligned_pattern = False, False
        self.storage_m_bound_change = False
        self.storage_ka_bound_change = False
        self.storage_kb_bound_change = False
        self.storage_n_bound_change = False
        self.cgm_ka_storage_change = False
        self.cgm_kb_storage_change = False
        self.attach_at_flag = None
        self.split_k_axis_by_tiling = False
        self.split_k_with_nd_out = False
        self.split_k = para_map.get("split_k")
        self.over_head_flag = False
        self.requant_n_odd_flag = para_map.get("requant_n_odd_flag", False)
        self.batch_broadcast_flag = para_map.get("batch_broadcast_flag", False)
        self.batch_broadcast_change_attach = para_map.get("batch_broadcast_change_attach", False)
        self.support_fix_pipe_l0c2out = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        self.support_fix_pipe = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_unit_list")
        self.support_bias_table = tbe_platform.intrinsic_check_support("Intrinsic_data_move_l12bt")
        self.support_fix_pipe_l0c2ub = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub")
        self.support_out2l1_nd2nz = tbe_platform.intrinsic_check_support("Intrinsic_data_move_out2l1_nd2nz")
        self.support_l0c2out_nz2nd = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd")
        self.support_mix_l2_fusion = self.support_fix_pipe_l0c2out and not self.support_fix_pipe_l0c2ub
        self.support_bf16 = tbe_platform.intrinsic_check_support("Intrinsic_vconv", "bf162f32")
        self.unaligned_flag = para_map.get("unaligned_flag")
        self.enable_ub_resuse = False
        self.performance_flag = 0
        self.enable_k_alignment = self.ops_data_flow_mode == "fp322fp32" and self.transpose_a
        self.pad_flag = para_map.get("pad_flag", 0)
        self.nz_fusion_flag = para_map.get("nz_fusion_flag", 0)
        self.nz_fusion_mode = para_map.get("nz_fusion_mode", 0)


class BufferChecker:
    """ Check whether buffer size exceed bound of memory
    """
    FP16_DTYPE = 2
    FP32_DTYPE = 4
    INPUT_SIZE = {"fp162fp16": 2, "fp162fp32": 2, "int82int32": 1,
                  "int82fp32": 1, "int42int32": 0.5, "fp322fp32": 4,
                  "fp162int8": 2}
    OUTPUT_SIZE = {"fp162fp16": 2, "fp162fp32": 4, "int82int32": 4,
                   "int82fp32": 4, "int42int32": 4, "fp322fp32": 4,
                   "bf162bf16": 4, "fp162int8": 2}
    DTYPE_WIDTH_MAP = {"uint64": 4, "float16": 1, "float32": 2, "int32": 2, "int16": 1, "uint16": 1,
                       "int8": 0.5, "uint8": 0.5, "int4": 0.25, "bool": 0.5}
    DOUBLE_MULTI = 2
    PRE_UB_MULTIPLIER = 10.0
    THRESHOLD_DATA_NUM = 64
    UB_SIZE = tbe_platform.get_soc_spec("UB_SIZE")
    AUB_M_INDEX = 1
    BUB_N_INDEX = 1
    CUB_M_INDEX = 1

    def __init__(self):
        self.tiling = None
        self.container = None
        self.status_controller = None
        self.block_in = tbe_platform.BLOCK_IN
        self.block_out = tbe_platform.BLOCK_OUT
        self.block_reduce = tbe_platform.BLOCK_REDUCE
        self.format_a = None
        self.format_b = None
        self.format_out = None
        self.a_fused_num = 0
        self.b_fused_num = 0
        self.c_fused_num = 0

    def check_aub_preload(self, tiling, params):
        """ check whether total_ub_size exceed ub_buffer_size when aub preload
        """
        self.tiling = tiling
        self.container = params.get("container")
        self.status_controller = params.get("status_controller")
        if params.get("cache_tiling"):
            return True

        total_ub_size = 0
        total_ub_size += self._aub_size_preload()
        total_ub_size += self._cub_size_preload()
        return total_ub_size <= self.UB_SIZE

    def check_bub_preload(self, tiling, params):
        """ check whether total_ub_size exceed ub_buffer_size when bub preload
        """
        self.tiling = tiling
        self.container = params.get("container")
        self.status_controller = params.get("status_controller")
        if params.get("cache_tiling"):
            return True

        total_ub_size = 0
        total_ub_size += self._bub_size_preload()
        total_ub_size += self._cub_size_preload()
        return total_ub_size <= self.UB_SIZE

    def check_bias_preload(self, tiling, params):
        """ check whether total_ub_size exceed ub_buffer_size when bias preload
        """
        self.tiling = tiling
        self.container = params.get("container")
        self.status_controller = params.get("status_controller")
        if params.get("cache_tiling"):
            return True
        if self.container.fuse_num_group:
            _, _, self.c_fused_num = self.container.fuse_num_group
        self.c_fused_num += 1

        total_ub_size = 0
        total_ub_size += self._aub_size_preload()
        total_ub_size += self._bub_size_preload()
        total_ub_size += self._cub_size_preload(bias_preload=True)
        return total_ub_size <= self.UB_SIZE

    def check_exceed_ub(self, tiling, params):
        """ If storage_align is used, more UB space is used.
            Therefore, check the UB space usage after storage_align is used.
        """
        self.tiling = tiling
        self.container = params.get("container")
        self.status_controller = params.get("status_controller")
        self.format_out = params.get("format_out")
        self.format_a = params.get("format_a")
        self.format_b = params.get("format_b")

        if self.container.fuse_num_group:
            self.a_fused_num, self.b_fused_num, self.c_fused_num = self.container.fuse_num_group
        self.a_fused_num = self.a_fused_num / self.PRE_UB_MULTIPLIER + 1
        self.b_fused_num = self.b_fused_num / self.PRE_UB_MULTIPLIER + 1
        self.c_fused_num += 1

        need_aub_storage_align = (self.container.tensor_map.get("a_ub") is not None) and (self.format_a == "ND")
        need_bub_storage_align = (self.container.tensor_map.get("b_ub") is not None) and (self.format_b == "ND")
        need_cub_storage_align = (((self.container.tensor_map.get("c_add_bias_ub") is not None) or
                                  (self.container.tensor_map.get("before_c_gm") is not None)) and
                                  (not params.get("cache_tiling") and self.format_out == "ND"))

        # compute before storage_align used UB size
        base_buffer_size = 0
        base_buffer_size, a_add_size = self._get_a_ub_storage_align_buffer_size(base_buffer_size,
            need_aub_storage_align, self.status_controller.transpose_a)
        base_buffer_size, b_add_size = self._get_b_ub_storage_align_buffer_size(base_buffer_size,
            need_bub_storage_align, self.status_controller.transpose_b)
        base_buffer_size, c_add_size = self._get_c_ub_storage_align_buffer_size(base_buffer_size,
            need_cub_storage_align)

        base_buffer_size, c_ub_storage_align = self._check_cub_gap(base_buffer_size,
            c_add_size, need_cub_storage_align)
        base_buffer_size, a_ub_storage_align = self._check_aub_gap(base_buffer_size,
            a_add_size, need_aub_storage_align, self.status_controller.transpose_a)
        base_buffer_size, b_ub_storage_align = self._check_bub_gap(base_buffer_size,
            b_add_size, need_bub_storage_align, self.status_controller.transpose_b)

        return a_ub_storage_align, b_ub_storage_align, c_ub_storage_align

    def _aub_size_preload(self):
        """ calculate aub_size when aub preload
        """
        tiling = self.tiling
        aub_size = 0
        if self.container.tensor_map.get("a_ub") is not None:
            aub_k, aub_m = tiling.get("AUB_shape")[0 : self.AUB_M_INDEX + 1]
            a_db = tiling.get("manual_pingpong_buffer").get("AUB_pbuffer")
            aub_size += (aub_m * self.block_in * aub_k *
                         self.INPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * a_db)
        return aub_size

    def _bub_size_preload(self):
        """ calculate bub_size when bub preload
        """
        tiling = self.tiling
        bub_size = 0
        if self.container.tensor_map.get("b_ub") is not None:
            bub_k, bub_n = tiling.get("BUB_shape")[0: self.BUB_N_INDEX + 1]
            b_db = tiling.get("manual_pingpong_buffer").get("BUB_pbuffer")
            bub_size += (bub_k * bub_n * self.block_out *
                         self.INPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * b_db)
        return bub_size

    def _cub_size_preload(self, bias_preload=False):
        """ calculate cub_size when aub/bub/bias preload
        """
        tiling = self.tiling
        cub_n, cub_m = tiling.get("CUB_matrix")[0: self.CUB_M_INDEX + 1]
        cub_size = (cub_n * self.block_out * cub_m * self.block_in * self.c_fused_num *
                    self.OUTPUT_SIZE.get(self.status_controller.ops_data_flow_mode))
        if bias_preload:
            cub_size += (cub_n * self.block_out *
                         self.OUTPUT_SIZE.get(self.status_controller.ops_data_flow_mode))
        return cub_size

    def _get_a_ub_storage_align_buffer_size(self, base_buffer_size, need_aub_storage_align, a_trans):
        """
        calculate extra aub buffer size

        Parameters:
        ------------
        base_buffer_size: int, base ub buffer size
        need_aub_storage_align: bool, whether need aub storage align
        a_trans: bool, transpose of matrix A

        Returns:
        ------------
        base_buffer_size: int, base ub buffer size
        a_add_size: int, extra buffer size
        """
        tiling = self.tiling
        gap_value = self.block_reduce
        a_add_size = 0
        if self.container.tensor_map.get("a_ub") is not None:
            aub_k, aub_m = tiling.get("AUB_shape")[0: self.AUB_M_INDEX + 1]
            aub_m *= self.block_in
            a_db = tiling.get("manual_pingpong_buffer").get("AUB_pbuffer")
            base_buffer_size += (aub_m * aub_k * self.a_fused_num *
                                 self.INPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * a_db)

        if need_aub_storage_align:
            # if use storage_align, need UB size
            a_add_size = (gap_value * (aub_k if a_trans else aub_m) *
                          self.INPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * a_db)
        return base_buffer_size, a_add_size

    def _get_b_ub_storage_align_buffer_size(self, base_buffer_size, need_bub_storage_align, b_trans):
        """
        calculate extra bub buffer size

        Parameters:
        ------------
        base_buffer_size: int, base ub buffer size
        need_bub_storage_align: bool, whether need bub storage align
        b_trans: bool, transpose of matrix B

        Returns:
        ------------
        base_buffer_size: int, base ub buffer size
        b_add_size: int, extra buffer size
        """
        tiling = self.tiling
        gap_value = self.block_reduce
        b_add_size = 0
        if self.container.tensor_map.get("b_ub") is not None:
            bub_k, bub_n = tiling.get("BUB_shape")[0: self.BUB_N_INDEX + 1]
            bub_n *= self.block_out
            b_db = tiling.get("manual_pingpong_buffer").get("BUB_pbuffer")
            base_buffer_size += (bub_k * bub_n * self.b_fused_num *
                                 self.INPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * b_db)

        if need_bub_storage_align:
            # if use storage_align, need UB size
            b_add_size = (gap_value * (bub_n if b_trans else bub_k) *
                          self.INPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * b_db)
        return base_buffer_size, b_add_size

    def _get_c_ub_storage_align_buffer_size(self, base_buffer_size, need_cub_storage_align):
        """
        calculate extra cub buffer size

        Parameters:
        ------------
        base_buffer_size: int, base ub buffer size
        need_bub_storage_align: bool, whether need cub storage align

        Returns:
        ------------
        base_buffer_size: int, base ub buffer size
        c_add_size: int, extra buffer size
        """
        tiling = self.tiling
        c_add_size = 0
        cub_n, cub_m = tiling.get("CUB_matrix")[0: self.CUB_M_INDEX + 1]
        c_db = tiling.get("manual_pingpong_buffer").get("CUB_pbuffer")
        base_buffer_size += (cub_n * cub_m * self.block_in * self.block_out * self.c_fused_num *
                             self.OUTPUT_SIZE.get(self.status_controller.ops_data_flow_mode) * c_db)
        if need_cub_storage_align:
            # if use storage_align, need UB size
            if self.container.tensor_map.get("before_c_gm") is not None:
                before_c_gm = self.container.tensor_map.get("before_c_gm")
                data_size = self.DTYPE_WIDTH_MAP.get(before_c_gm.dtype) * self.DOUBLE_MULTI
                c_add_size = cub_n * self.block_out * data_size * c_db
            else:
                data_size = self.FP32_DTYPE
                c_ub_cast_to_fp16 = self.container.tensor_map.get("cast_to_fp16")
                tensor_alpha = self.container.tensor_map.get("alpha")
                if (c_ub_cast_to_fp16 is not None) and (tensor_alpha is None):
                    data_size = self.FP16_DTYPE
                c_add_size = self.block_out * cub_n * cub_m * data_size * c_db
        return base_buffer_size, c_add_size

    def _check_aub_gap(self, base_buffer_size, a_add_size, need_aub_storage_align, a_trans):
        """
        check whether to do aub storage align

        Parameters:
        ------------
        base_buffer_size: int, base ub buffer size
        a_add_size: int, extra buffer size
        need_aub_storage_align: bool, whether need aub storage align
        a_trans: bool, transpose of matrix A

        Returns:
        ------------
        base_buffer_size: int, base ub buffer size
        a_ub_storage_align: bool, whether aub to do storage align
        """
        a_ub_storage_align = False
        if need_aub_storage_align:
            aub_k, aub_m = self.tiling.get("AUB_shape")[0: self.AUB_M_INDEX + 1]
            aub_m *= self.block_in
            judge_value = aub_m if a_trans else aub_k
            a_ub_storage_align = ((judge_value % self.THRESHOLD_DATA_NUM == 0)
                and ((base_buffer_size + a_add_size) <= self.UB_SIZE))
            if a_ub_storage_align:
                base_buffer_size += a_add_size
        return base_buffer_size, a_ub_storage_align

    def _check_bub_gap(self, base_buffer_size, b_add_size, need_bub_storage_align, b_trans):
        """
        check whether to do bub storage align

        Parameters:
        ------------
        base_buffer_size: int, base ub buffer size
        b_add_size: int, extra buffer size
        need_bub_storage_align: bool, whether need bub storage align
        b_trans: bool, transpose of matrix B

        Returns:
        ------------
        base_buffer_size: int, base ub buffer size
        b_ub_storage_align: bool, whether bub to do storage align
        """
        b_ub_storage_align = False
        if need_bub_storage_align:
            bub_k, bub_n = self.tiling.get("BUB_shape")[0: self.BUB_N_INDEX + 1]
            bub_n *= self.block_out
            judge_value = bub_k if b_trans else bub_n
            b_ub_storage_align = ((judge_value % self.THRESHOLD_DATA_NUM == 0)
                and ((base_buffer_size + b_add_size) <= self.UB_SIZE))
            if b_ub_storage_align:
                base_buffer_size += b_add_size
        return base_buffer_size, b_ub_storage_align

    def _check_cub_gap(self, base_buffer_size, c_add_size, need_cub_storage_align):
        """
        check whether to do aub storage align

        Parameters:
        ------------
        base_buffer_size: int, base ub buffer size
        c_add_size: int, extra buffer size
        need_cub_storage_align: bool, whether need cub storage align

        Returns:
        ------------
        base_buffer_size: int, base ub buffer size
        c_ub_storage_align: bool, whether cub to do storage align
        """
        c_ub_storage_align = False
        if need_cub_storage_align:
            c_ub_storage_align = (base_buffer_size + c_add_size <= self.UB_SIZE)
            if c_ub_storage_align:
                base_buffer_size += c_add_size
        return base_buffer_size, c_ub_storage_align


class UbBufferReuser:
    UB_ND2NZ_TENSOR_NUM = 2

    # This Class is used to set CUB to Reuse AUB/BUB in cache Tiling Mode.
    def __init__(self, tiling, tensor_map, buffer_reuse_dict):
        self.tiling = tiling
        self.tensor_map = tensor_map
        self.buffer_reuse_dict = buffer_reuse_dict
        # Store Reused Ub tensors
        self.a_ub = None
        self.a_ub_fract = None
        self.b_ub = None
        self.b_ub_fract = None
        self.c_ub = None
        self.cub_nz_to_nd = None
        self.cast_to_fp32 = None
        self.nz_to_nd_fp32 = None
        self.c_add_bias_ub_fp16 = None
        self.c_add_bias_ub_fp32 = None
        self.bias_ub_fp16 = None
        self.bias_ub_fp32 = None

    def set_ub_reuse_process(self, cache_tiling, status_controller, format_info):
        if not status_controller.enable_ub_resuse:
            return
        aub_vacant_flag, bub_vacant_flag, flag_pre_ub_not_reused = self._get_reused_flag(status_controller, format_info)
        # Only reusing nz_to_nd if it is not split K scene.
        if (aub_vacant_flag and bub_vacant_flag):
            # cub reused aub/bub and bub and reused aub
            if not status_controller.split_k:
                c_ub_reused_tensor = [self.a_ub, self.b_ub]
                cub_nz_to_nd_reused_tensor = [self.a_ub_fract, self.b_ub_fract]
                if status_controller.have_batch:
                    if self.cast_to_fp32 is not None:
                        c_ub_reused_tensor.append(self.cast_to_fp32)
                    if self.nz_to_nd_fp32 is not None:
                        cub_nz_to_nd_reused_tensor.append(self.nz_to_nd_fp32)
                    self._add_to_reused_dict(self.c_ub, c_ub_reused_tensor)
                    self._add_to_reused_dict(self.cub_nz_to_nd, cub_nz_to_nd_reused_tensor)
                else:
                    if self.cast_to_fp32 is not None:
                        cub_nz_to_nd_reused_tensor.append(self.cast_to_fp32)
                    if self.nz_to_nd_fp32 is not None:
                        c_ub_reused_tensor.append(self.nz_to_nd_fp32)
                    self._add_to_reused_dict(self.cub_nz_to_nd, c_ub_reused_tensor)
                    self._add_to_reused_dict(self.c_ub, cub_nz_to_nd_reused_tensor)
            else:
                self._add_to_reused_dict(self.c_ub, [self.a_ub, self.b_ub])
                self._add_to_reused_dict(self.a_ub_fract, self.b_ub_fract)
        elif not flag_pre_ub_not_reused and not (aub_vacant_flag or bub_vacant_flag):
            # bub can used AUB/BUB and cub cannot reuse aub/bub
            self._add_to_reused_dict(self.a_ub, self.b_ub)
            self._add_to_reused_dict(self.a_ub_fract, self.b_ub_fract)
        elif flag_pre_ub_not_reused:
            self._set_reused_for_no_pre_reused(aub_vacant_flag, bub_vacant_flag, status_controller)

    def _set_reused_for_no_pre_reused(self, aub_vacant_flag, bub_vacant_flag, status_controller):
        fract_tensor = []
        cub_to_reuse_aligned_pre_ub = self.c_ub
        if not (status_controller.split_k or status_controller.have_batch):
            cub_to_reuse_aligned_pre_ub = self.cub_nz_to_nd
        c_ub_reused_tensor = []
        if not status_controller.split_k and (self.cast_to_fp32 is not None or self.nz_to_nd_fp32 is not None):
            c_ub_reused_tensor = [self.cast_to_fp32]
            if not (status_controller.have_batch and self.cast_to_fp32 is not None):
                c_ub_reused_tensor = [self.nz_to_nd_fp32]
        if aub_vacant_flag and not bub_vacant_flag:
            c_ub_reused_tensor.append(self.a_ub)
            self._add_to_reused_dict(cub_to_reuse_aligned_pre_ub, c_ub_reused_tensor)
            fract_tensor.append(self.a_ub_fract)
        elif bub_vacant_flag and not aub_vacant_flag:
            c_ub_reused_tensor.append(self.b_ub)
            self._add_to_reused_dict(cub_to_reuse_aligned_pre_ub, c_ub_reused_tensor)
            fract_tensor.append(self.b_ub_fract)
        if not status_controller.split_k:
            if status_controller.have_batch:
                cub_reused_fract_tensor = self.cub_nz_to_nd
                if self.nz_to_nd_fp32 is not None:
                    fract_tensor.append(self.nz_to_nd_fp32)
            else:
                cub_reused_fract_tensor = self.c_ub
                if self.cast_to_fp32 is not None:
                    fract_tensor.append(self.cast_to_fp32)
            self._add_to_reused_dict(cub_reused_fract_tensor, fract_tensor)

    def _cal_aub_vacant_flag(self, al1_full_load, aub_full_load, cub_can_enable_db):
        binary_aub_vacant_flag = al1_full_load and not aub_full_load
        tiling_data = self.tiling.get("binary_tiling_data", None)
        if tiling_data is None:
            return binary_aub_vacant_flag
        ub_size = tbe_platform.get_soc_spec("UB_SIZE")
        aub_db = self.tiling.get("manual_pingpong_buffer").get("AUB_pbuffer")
        cub_db = self.tiling.get("manual_pingpong_buffer").get("CUB_pbuffer")

        const_aub_vacant_flag = al1_full_load and not aub_full_load and (
                          (aub_db == cub_db) and (cub_can_enable_db or aub_db == 1))
        cub_n1_init = tiling_data.get("cub_n1")
        # cub can not enable cub double buffer, binary_const different from binary
        if binary_aub_vacant_flag != const_aub_vacant_flag:
            aub_db = self.tiling.get("manual_pingpong_buffer").get("AUB_pbuffer")
            cub_db = self.tiling.get("manual_pingpong_buffer").get("CUB_pbuffer")
            cub_db = cub_db if cub_can_enable_db else 1
            aub_used_space = tiling_data.get("aub_align_bound") * self.UB_ND2NZ_TENSOR_NUM * aub_db
            cub_used_space = cub_n1_init * tiling_data.get("m_l0") * tbe_platform.BLOCK_IN * \
                             tbe_platform.BLOCK_OUT * self.UB_ND2NZ_TENSOR_NUM * cub_db
            bias_space = cub_n1_init * tbe_platform.BLOCK_IN * cub_db if tiling_data.get("bias_flag") else 0
            # number 2 means fp16 is 2byte
            # ubsize is enouth to close aub and cub reuse buffer
            if (aub_used_space + cub_used_space + bias_space) * 2 <= ub_size:
                return const_aub_vacant_flag
        else:
            return const_aub_vacant_flag
        # ubsize is  not enouth to close aub and cub reuse buffer, rewrite cub_n1
        for cub_factor in range(cub_n1_init - 1, 0, -1):
            if cub_n1_init % cub_factor == 0:
                tiling_data["cub_n1"] = cub_factor
                tiling_data["n_ub_l0_time"] = tiling_data["n_ub_l0_time"] * (cub_n1_init // cub_factor)
                break
        # if cub_n1_init is 1, need to reduce aub tiling, raise error now.
        if cub_n1_init == tiling_data["cub_n1"]:
            args = {"errCode": "E60114", "reason": "Cache_tiling isillegal which large than ub_size", "value": "None"}
            raise RuntimeError(args, error_manager_util.get_error_message(args))
        # can enable cub double buffer, binary_const same as binary
        return binary_aub_vacant_flag

    def _get_reused_flag(self, status_controller, format_info):
        self.a_ub = self.tensor_map.get("a_ub")
        self.a_ub_fract = self.tensor_map.get("a_ub_fract")
        self.b_ub = self.tensor_map.get("b_ub")
        self.b_ub_fract = self.tensor_map.get("b_ub_fract")
        if status_controller.split_k:
            self.c_ub = self.tensor_map.get("c_ub_fract")
        else:
            self.c_ub = self.tensor_map.get("cast_to_fp16")
            self.cub_nz_to_nd = self.tensor_map.get("nz_to_nd")
            self.cast_to_fp32 = self.tensor_map.get("cast_to_fp32")
            self.nz_to_nd_fp32 = self.tensor_map.get("nz_to_nd_fp32")
            self.c_add_bias_ub_fp16 = self.tensor_map.get("c_add_bias_ub_fp16")
            self.c_add_bias_ub_fp32 = self.tensor_map.get("c_add_bias_ub_fp32")
            self.bias_ub_fp16 = self.tensor_map.get("bias_ub_fp16")
            self.bias_ub_fp32 = self.tensor_map.get("bias_ub_fp32")

        # The following attach flag can only be enabled in cacheTiling mode.
        al1_attach_flag = self.tiling.get("attach_at_flag").get("al1_attach_flag")
        bl1_attach_flag = self.tiling.get("attach_at_flag").get("bl1_attach_flag")
        aub_attach_flag = self.tiling.get("attach_at_flag").get("aub_multi_flag")
        bub_attach_flag = self.tiling.get("attach_at_flag").get("bub_multi_flag")
        al1_full_load = al1_attach_flag == 0
        bl1_full_load = bl1_attach_flag == 0
        aub_full_load = aub_attach_flag == 1
        bub_full_load = bub_attach_flag == 1
        # To avoid Preload Precision Problem or Double Buffer Failure Problem,
        # reused is disable when one of pre_ub 's L1 and UB are full load and the other is not.
        # reused is disable when aub's double buffer is different from cub's double buffer,
        # binary const not support batch_matmul no need check batch dim.
        tiling_data = self.tiling.get("binary_tiling_data", None)
        cub_can_enable_db = tiling_data is None or (tiling_data is not None and (
                            tiling_data.get("cub_n1") * tiling_data.get("n_dim") < tiling_data.get("n") or
                            tiling_data.get("m_l0") * tiling_data.get("m_dim") < tiling_data.get("m")))
        bub_db = self.tiling.get("manual_pingpong_buffer").get("BUB_pbuffer")
        cub_db = self.tiling.get("manual_pingpong_buffer").get("CUB_pbuffer")

        aub_vacant_flag = self._cal_aub_vacant_flag(al1_full_load, aub_full_load, cub_can_enable_db)
        bub_vacant_flag = bl1_full_load and not bub_full_load and (
                          (bub_db == cub_db) and (cub_can_enable_db or bub_db == 1))
        flag_aub_pb_fail = al1_full_load and aub_full_load
        flag_bub_pb_fail = bl1_full_load and bub_full_load
        flag_pre_ub_not_reused = ((flag_aub_pb_fail != flag_bub_pb_fail) or (not al1_full_load and not bl1_full_load) or
                                  (flag_aub_pb_fail and flag_bub_pb_fail and status_controller.have_batch))
        if status_controller.have_batch and not flag_pre_ub_not_reused:
            # Disable cub reused as posible in BMM
            aub_vacant_flag = False
            bub_vacant_flag = False
        if format_info.get("b") != "ND":
            bub_vacant_flag = False
        # Avoid Sync Problem between PreUb and PostUb
        flag_pre_ub_not_reused = flag_pre_ub_not_reused or (aub_vacant_flag and not bub_vacant_flag) or (
            bub_vacant_flag and not aub_vacant_flag)
        return aub_vacant_flag, bub_vacant_flag, flag_pre_ub_not_reused

    def _add_to_reused_dict(self, src_tensor, reuse_tensor):
        # Allowing reuse_tensor(or reuse_tensors) to reuse the spaces of src_tensor.
        if (src_tensor is not None) and (reuse_tensor is not None):
            self.buffer_reuse_dict[src_tensor] = reuse_tensor


class GemmTilingWork:
    """ There are tiling parameters in matmul
    """
    IDX_K_DIM = 3
    block_in = tbe_platform.BLOCK_IN
    block_out = tbe_platform.BLOCK_OUT
    block_reduce = tbe_platform.BLOCK_REDUCE
    block_size = tbe_platform.BLOCK_IN
    default_l0_ub_tiling = (1, 1, 1, block_size, block_size)
    l1_attach_at_cl0_flag = 2
    l1_attach_at_ddr_flag = 1
    l1_full_load_flag = 0
    kal1_eq_kbl1_flag = 0
    kal1_gt_kbl1_flag = 1
    kal1_lt_kbl1_flag = 2
    k_multi_int8 = 2

    def __init__(self):
        self.tiling = None
        (self.al0_tiling_batch, self.al0_tiling_ma,
         self.al0_tiling_ka, self.al0_tiling_m0, self.al0_tiling_k0) = self.default_l0_ub_tiling
        (self.bl0_tiling_batch, self.bl0_tiling_nb,
         self.bl0_tiling_kb, self.bl0_tiling_n0, self.bl0_tiling_k0) = self.default_l0_ub_tiling
        (self.cl0_tiling_batch, self.cl0_tiling_nc,
         self.cl0_tiling_mc, self.cl0_tiling_n0, self.cl0_tiling_m0) = self.default_l0_ub_tiling
        (self.aub_tiling_batch, self.aub_tiling_m,
         self.aub_tiling_k, self.aub_tiling_m0, self.aub_tiling_k0) = self.default_l0_ub_tiling
        (self.bub_tiling_batch, self.bub_tiling_n,
         self.bub_tiling_k, self.bub_tiling_n0, self.bub_tiling_k0) = self.default_l0_ub_tiling
        self.al1_tiling_batch, self.al1_tiling_m, self.al1_tiling_k = 1, 1, 1
        self.bl1_tiling_batch, self.bl1_tiling_n, self.bl1_tiling_k = 1, 1, 1
        self.cub_tiling_batch = 1
        self.factor_shape = {"aub": [], "bub": [], "cub": [], "al0": [], "bl0": [], "cl0": [], "al1": [], "bl1": []}
        self.c_col_k0, self.c_col_k1 = 1, 1
        self.bind_core_when_full_load_bl1 = False
        self.attach_tensor_map = {}
        self.sc_non_factor_tail_strategy = "guard_with_if"

    @staticmethod
    def get_split_param(cache_tiling_mgr, tail_strategy=None):
        """
        get param for attach at, ceildiv by default, use floordiv to aid simplification in binary scene
        -----------------------
        Return:
            split_param: dict, include factor_ceil_mode, split_ceil_mode, tail_strategy and activate_scope.
        """
        factor_ceil_mode = True
        split_ceil_mode = True
        if cache_tiling_mgr.cache_tiling and not cache_tiling_mgr.unaligned_flag:
            factor_ceil_mode = False
            split_ceil_mode = False
            tail_strategy = "round_up" if tail_strategy is None else tail_strategy
        else:
            tail_strategy = "guard_with_if"
        return {"split_ceil_mode": split_ceil_mode, "factor_ceil_mode": factor_ceil_mode,
                "tail_strategy": tail_strategy, "active_scope": "outer"}

    def config_tiling(self, tiling, cache_tiling, para_map):
        """
        config tiling variable for cache tiling
        """
        self.sc_non_factor_tail_strategy = "round_up"
        support_fix_pipe_l0c2out = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        if support_fix_pipe_l0c2out:
            self.sc_non_factor_tail_strategy = "shift_inwards"
        tiling['block_dim'] = [cache_tiling.get('batch_dim'),
                               cache_tiling.get("n_dim"),
                               cache_tiling.get("m_dim"),
                               cache_tiling.get('k_dim')]
        attach_at_flag = tiling.get("attach_at_flag")
        if (para_map.get("a_dtype") == "int8" and (attach_at_flag.get("al1_attach_flag") in (0, 1)) and
            (attach_at_flag.get("abkl1_attach_flag") != 0) and support_fix_pipe_l0c2out):
            tiling.get('AL1_shape')[0] = (int_ceil_div(cache_tiling.get("kal1_16"), self.k_multi_int8) *
                self.block_reduce)
        else:
            tiling.get('AL1_shape')[0] = cache_tiling.get("kal1_16") * self.block_reduce
        if tiling.get('AL1_shape')[1] == -1:
            tiling.get('AL1_shape')[1] = cache_tiling.get("m_al1")
        if (para_map.get("b_dtype") == "int8" and (attach_at_flag.get("bl1_attach_flag") in (0, 1)) and
            (attach_at_flag.get("abkl1_attach_flag") != 0) and support_fix_pipe_l0c2out):
            tiling.get('BL1_shape')[0] = (int_ceil_div(cache_tiling.get("kbl1_16"), self.k_multi_int8) *
                self.block_reduce)
        else:
            tiling.get('BL1_shape')[0] = cache_tiling.get("kbl1_16") * self.block_reduce
        if tiling.get('BL1_shape')[1] == -1:
            tiling.get('BL1_shape')[1] = cache_tiling.get("n_bl1")
        tiling.get('AL0_matrix')[0] = cache_tiling.get("m_l0")
        tiling.get('CL0_matrix')[1] = cache_tiling.get("m_l0")
        tiling.get('CUB_matrix')[1] = cache_tiling.get("m_l0")
        tiling.get('CUB_matrix')[0] = cache_tiling.get("cub_n1")
        tiling.get('AL0_matrix')[1] = cache_tiling.get("k_al0")
        tiling.get('BL0_matrix')[0] = cache_tiling.get("k_bl0")
        tiling.get('BL0_matrix')[1] = cache_tiling.get("n_ub_l0_time") * cache_tiling.get("cub_n1")
        tiling.get('CL0_matrix')[0] = tiling.get('BL0_matrix')[1]
        tiling.get('manual_pingpong_buffer')["AL1_pbuffer"] = cache_tiling.get('al1_db')
        tiling.get('manual_pingpong_buffer')["BL1_pbuffer"] = cache_tiling.get('bl1_db')
        if para_map.get('have_batch'):
            tiling.get('AL1_shape')[2] = cache_tiling.get("batch_l0")
            tiling.get('BL1_shape')[2] = cache_tiling.get("batch_l0")
            tiling.get('AL0_matrix')[4] = cache_tiling.get("batch_l0")
            tiling.get('BL0_matrix')[4] = cache_tiling.get("batch_l0")
            tiling.get('CL0_matrix')[4] = cache_tiling.get("batch_l0")
            tiling.get('CUB_matrix')[4] = cache_tiling.get("batch_cub")
        if para_map.get("need_aub"):
            tiling.get('AUB_shape')[0] = cache_tiling.get('k_aub') * self.block_reduce
            tiling.get('AUB_shape')[1] = cache_tiling.get('m_aub')
            if para_map.get('have_batch'):
                tiling.get('AUB_shape')[2] = cache_tiling.get('batch_aub')
            self.sc_non_factor_tail_strategy = "shift_inwards"
        if para_map.get("need_bub"):
            tiling.get('BUB_shape')[0] = cache_tiling.get('k_bub') * self.block_reduce
            tiling.get('BUB_shape')[1] = cache_tiling.get('n_bub')
            if para_map.get('have_batch'):
                tiling.get('BUB_shape')[2] = cache_tiling.get('batch_bub')
        if para_map.get("nz_fusion_flag"):
            tiling.get("AUB_shape")[0] = cache_tiling.get("k1_aub")
            tiling.get("AUB_shape")[1] = cache_tiling.get("m1_aub")
            tiling.get("BUB_shape")[0] = cache_tiling.get("k1_bub")
            tiling.get("BUB_shape")[1] = cache_tiling.get("n1_bub")
            tiling["mix_ub_dim"] = [cache_tiling.get("m_aub_dim"), cache_tiling.get("n_bub_dim"),
                                    cache_tiling.get("k_aub_dim"), cache_tiling.get("k_bub_dim")]
        elif para_map.get("pad_flag"):
            tiling.get("AUB_shape")[0] = cache_tiling.get("k_aub")
            tiling.get("AUB_shape")[1] = cache_tiling.get("m_aub")
            tiling.get("BUB_shape")[0] = cache_tiling.get("k_bub")
            tiling.get("BUB_shape")[1] = cache_tiling.get("n_bub")
            tiling["mix_ub_dim"] = [cache_tiling.get("aub_dim"), cache_tiling.get("bub_dim")]

        return tiling

    def set_factor_shape(self, cache_tiling_mgr, format_info, status_controller, para_map):
        """
        define the tiling factor for attach at. only used in binary scene
        we split root scope from small to large, the basic sequence is UB->L0->L1.
        the factor is TILING_L0/TILING_UB when split TILING_L0.
        the factor is TILING_L1/TILING_L0 when split TILING_L1.
        """
        if not cache_tiling_mgr.cache_tiling:
            return

        cache_tiling = cache_tiling_mgr.cache_tiling
        attach_at_flag = self.tiling.get("attach_at_flag")
        cub_tiling = self.tiling.get("CUB_matrix")

        ub_ka = int_ceil_div(self.aub_tiling_k, self.aub_tiling_k0)
        ub_kb = int_ceil_div(self.bub_tiling_k, self.bub_tiling_k0)

        if para_map.get("need_aub"):
            self.factor_shape["aub"] = [self.aub_tiling_m, ub_ka, self.aub_tiling_m0, self.aub_tiling_k0]
            if cache_tiling_mgr.unaligned_flag:
                self.factor_shape["aub"] = [self.aub_tiling_m, ub_ka, self.aub_tiling_k0, self.aub_tiling_m0]
                if status_controller.transpose_a:
                    self.factor_shape["aub"] = [ub_ka, self.aub_tiling_m, self.aub_tiling_m0, self.aub_tiling_k0]
        if para_map.get("need_bub"):
            self.factor_shape["bub"] = [ub_kb, self.bub_tiling_n, self.bub_tiling_n0, self.bub_tiling_k0]
            if cache_tiling_mgr.unaligned_flag:
                self.factor_shape["bub"] = [ub_kb, self.bub_tiling_n, self.bub_tiling_n0, self.bub_tiling_k0]
                if status_controller.transpose_b:
                    self.factor_shape["bub"] = [self.bub_tiling_n, ub_kb, self.bub_tiling_k0, self.bub_tiling_n0]

        self._set_factor_shape_to_l0c(cache_tiling, attach_at_flag)
        if format_info.get("out") == "ND":
            self._set_factor_shape_to_out_nd(cache_tiling, cub_tiling)
        else:
            self._set_factor_shape_to_out_nz(cache_tiling, cub_tiling)

        if status_controller.have_batch_a:
            self.factor_shape.get("aub").insert(0, self.aub_tiling_batch)
        if status_controller.have_batch_b:
            self.factor_shape.get("bub").insert(0, self.bub_tiling_batch)

        if status_controller.have_batch:
            self.cub_tiling_batch = cub_tiling[-2]
            self.factor_shape.get("cub").insert(0, self.cub_tiling_batch)
            if not status_controller.support_fix_pipe_l0c2out:
                self.cl0_tiling_batch = cache_tiling.get("batch_ub_l0_time")
            self.factor_shape.get("cl0").insert(0, self.cl0_tiling_batch)
            self.factor_shape.get("al0").insert(0, self.al0_tiling_batch)
            self.factor_shape.get("al12ddr").insert(0, self.al1_tiling_batch)
            self.bl0_tiling_batch = 1
            self.factor_shape.get("bl0").insert(0, self.bl0_tiling_batch)
            self.factor_shape.get("bl12ddr").insert(0, self.bl1_tiling_batch)
            self.factor_shape.get("al12cl0").insert(0, 1)
            self.factor_shape.get("bl12cl0").insert(0, 1)

        if status_controller.split_k_axis_by_tiling:
            self.factor_shape.get("cub").insert(0, 1)
            self.factor_shape.get("cl0").insert(0, 1)
            self.factor_shape.get("al0").insert(0, 1)
            self.factor_shape.get("bl0").insert(0, 1)
            self.factor_shape.get("al12ddr").insert(0, 1)
            self.factor_shape.get("bl12ddr").insert(0, 1)
            self.factor_shape.get("al12cl0").insert(0, 1)
            self.factor_shape.get("bl12cl0").insert(0, 1)

    def get_a_max_k_bound(self, a_l0a_shape):
        """
        This function is used to get the maximum k bound, which will be used in the
        following calculation to solve bank conflict and to set storage bound.
        """
        a_matrix_dim = [get_value(i) for i in a_l0a_shape]
        k1_dim = a_matrix_dim[-4] if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN") else a_matrix_dim[-3]
        k_bound_tiling = (int_ceil_div(k1_dim,
                                       self.tiling.get("AL0_matrix")[1]) * self.tiling.get("AL0_matrix")[1] *
                          self.block_reduce)
        k_bound_tiling = int_ceil_div(k_bound_tiling, self.tiling.get("block_dim")[self.IDX_K_DIM])
        return k_bound_tiling

    def get_b_max_k_bound(self, b_l0b, is_dynamic, dynamic_k):
        """
        This function is used to get the maximum k bound, which will be used in the
        following calculation to solve bank conflict and to set storage bound.
        """
        b_matrix_dim = [get_value(i) for i in b_l0b.shape]
        if self.tiling.get("BL0_matrix"):
            k_bound_tiling = (int_ceil_div(b_matrix_dim[-4],
                                           self.tiling.get("BL0_matrix")[0]) *
                              self.tiling.get("BL0_matrix")[0] * self.block_reduce)
            return int_ceil_div(k_bound_tiling, self.tiling.get("block_dim")[-1])
        elif is_dynamic:
            return int_ceil_div(dynamic_k, self.tiling.get("block_dim")[-1]) * self.block_reduce
        else:
            return int_ceil_div(b_matrix_dim[-4], self.tiling.get("block_dim")[-1]) * self.block_reduce

    def _set_factor_shape_to_l0c(self, cache_tiling, attach_at_flag):
        """
        define the tiling factor for tensor attach at cl0
        parent of al0/bl0 is cl0 by default, parent of al1/bl1 is cl0 in this func
        factor corresponding to [n1, m1, m0, n0, k1, k0]
        """
        self.factor_shape["al0"] = [
            None, self.al0_tiling_ma, None, self.al0_tiling_m0, self.al0_tiling_ka, self.al0_tiling_k0
        ]
        self.factor_shape["bl0"] = [self.bl0_tiling_nb, None, None, 1, 1, 1]
        self.factor_shape["al12cl0"] = [None, 1, None, None, 1, None]
        self.factor_shape["bl12cl0"] = [1, None, None, None, 1, None]
        # only split by kl0_factor when k_al1/k_bl1 larger than kl0
        if attach_at_flag.get("min_kl1_cmp_kl0"):
            if attach_at_flag.get("abkl1_attach_flag") in (self.kal1_eq_kbl1_flag, self.kal1_gt_kbl1_flag):
                self.factor_shape.get("bl12cl0")[-2] = cache_tiling.get("kbl0_factor")
            else:
                self.factor_shape.get("al12cl0")[-2] = cache_tiling.get("kal0_factor")

        # only split by kl1_times when al1/bl1 both attach at cl0
        if (attach_at_flag.get("al1_attach_flag") == self.l1_attach_at_cl0_flag and
            attach_at_flag.get("bl1_attach_flag") == self.l1_attach_at_cl0_flag):
            if attach_at_flag.get("abkl1_attach_flag") == self.kal1_gt_kbl1_flag:
                self.factor_shape.get('al12cl0')[-2] = cache_tiling.get("kl1_times")
            elif attach_at_flag.get("abkl1_attach_flag") == self.kal1_lt_kbl1_flag:
                self.factor_shape.get('bl12cl0')[-2] = cache_tiling.get("kl1_times")

    def _set_factor_shape_to_out_nd(self, cache_tiling, cub_tiling):
        """
        define the tiling factor for tensor attach at ddr when output format is ND.
        parent of cub/cl0 is cddr by default, parent of al1/bl1 is cddr in this function.
        factor corresponding to [m, n]
        """
        cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0, _, _ = cub_tiling
        self.factor_shape["cub"] = [cub_tiling_mc_factor * cub_tiling_m0, cub_tiling_nc_factor * cub_tiling_n0]
        self.factor_shape["cl0"] = [1, cache_tiling.get("n_ub_l0_time")]
        # al1 attach at ddr, bl1 not full load
        self.factor_shape["al12ddr"] = [cache_tiling.get("m_al1"), cache_tiling.get("n_single_core")]
        # al1 attach at ddr, bl1 full load
        if self.tiling.get("attach_at_flag").get("bl1_attach_flag") == self.l1_full_load_flag:
            self.factor_shape.get("al12ddr")[1] = cache_tiling.get("n_single_core")

        # al1/bl1 both attach at ddr
        self.factor_shape["bl12ddr"] = [1, cache_tiling.get("n_bl1")]
        # bl1 attach at ddr, al1 full load
        if self.tiling.get("attach_at_flag").get("al1_attach_flag") == self.l1_full_load_flag:
            self.factor_shape.get("bl12ddr")[0] = cache_tiling.get("m_single_core") * cache_tiling.get("m_al1")
        elif self.tiling.get("attach_at_flag").get("al1_attach_flag") == self.l1_attach_at_cl0_flag:
            self.factor_shape.get("bl12ddr")[0] = cache_tiling.get("m_single_core")

    def _set_factor_shape_to_out_nz(self, cache_tiling, cub_tiling):
        """
        define the tiling factor for tensor attach at ddr when output format is NZ.
        parent of cub/cl0 is cddr by default, parent of al1/bl1 is cddr in this function.
        factor corresponding to [n1, m1, m0, n0]
        """
        cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0, _, _ = cub_tiling
        self.factor_shape["cub"] = [cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0]
        self.factor_shape["cl0"] = [cache_tiling.get("n_ub_l0_time"), 1, 1, 1]

        # al1/bl1 both attach at ddr
        self.factor_shape["al12ddr"] = [1, 1, 1, None]
        # al1 attach at ddr, bl1 full load
        if self.tiling.get("attach_at_flag").get("bl1_attach_flag") == self.l1_full_load_flag:
            self.factor_shape.get("al12ddr")[:2] = [cache_tiling.get("n_bl1"), cache_tiling.get("m_al1")]
        # al1 attach at ddr, bl1 attach at cl0
        elif self.tiling.get("attach_at_flag").get("bl1_attach_flag") == self.l1_attach_at_cl0_flag:
            self.factor_shape.get("al12ddr")[1] = cache_tiling.get("m_al1")

        # al1/bl1 both attach at ddr
        self.factor_shape["bl12ddr"] = [cache_tiling.get("n_bl1"), cache_tiling.get("m_al1"), None, 1]
        # bl1 attach at ddr, al1 not attach at ddr
        if self.tiling.get("attach_at_flag").get("al1_attach_flag") != self.l1_attach_at_ddr_flag:
            self.factor_shape.get("bl12ddr")[1] = cache_tiling.get("m_single_core") * cache_tiling.get("m_al1")


class CceSimplification:
    """
    schedule simplification which can significantly reduce emitted cce code size.
    """
    MAX_ORI_SHAPE_TEMP = 1048560
    MAX_UB_SHAPE = 4096

    def __init__(self, sch, dynamic_para):
        self.tiling = dynamic_para.get("tiling_strategy")
        self.sch = sch
        self.cache_tiling = None
        self.tensor_map = None
        self.status_controller = None
        self.is_align_mode = True if (self.tiling and self.tiling.get("schedule_pattern") == "Aligned") else False
        self.var_manager = VarManage(sch, dynamic_para.get("var_range"))
        self.block_in = tbe_platform.BLOCK_IN
        self.block_out = tbe_platform.BLOCK_OUT
        self.block_reduce = tbe_platform.BLOCK_REDUCE
        self.support_l0c2out_nz2nd = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd")

    def set_kaub_simplification_l1_fullload(self, single_core_k, compute_param):
        """
        config kaub when al1 and aub are full load.
        """
        if not compute_param.need_aub:
            return
        aub_full_load = self.tiling.get("attach_at_flag").get("aub_multi_flag") == 1
        k_aub = self.cache_tiling.get('k_aub')
        multi_k_aub_k1 = self.cache_tiling.get("multi_k_aub_l1")
        if aub_full_load and self.tiling["attach_at_flag"].get("min_kl1_cmp_kl0") == 0:
            # When AL1 full load and Aub full load, the reduced axis K is identical.
            self.sch.set_var_value(k_aub, single_core_k)
        elif self.tiling["attach_at_flag"].get("min_kl1_cmp_kl0") == 0:
            self.sch.set_constraint((multi_k_aub_k1 * k_aub == single_core_k).asobject())

    def cce_simplify(self, compute_param, cache_tiling_mgr, sch_container, para_map):
        """
        handles cce simplification for binary, include set_var_range, pragma, skip_bound_check, etc.
        """
        self.tensor_map = sch_container.tensor_map
        self.var_manager.binary_tiling_data = self.tiling.get("binary_tiling_data")
        self.var_manager.set_var_range_for_dynamic_scene(compute_param, cache_tiling_mgr)
        if cache_tiling_mgr.cache_tiling:
            self._enable_skip_bound_check(cache_tiling_mgr, sch_container, para_map)
            self._buffer_tile_for_simplify(compute_param, cache_tiling_mgr.sch_agent, cache_tiling_mgr.unaligned_flag)
            self._emit_insn_simplyfy_c_gm(compute_param)
            if compute_param.need_aub:
                self._emit_insn_simplify_aub()
                self._emit_insn_simplify_bub()
                self._fuse_l1_axis(compute_param)
                self._emit_insn_simplify_al1(compute_param)
                self._emit_insn_simplify_bl1()
        elif self.is_align_mode:
            self.sch.set_constraint((self.var_manager.m_var % tbe_platform.BLOCK_IN == 0).asobject())
            self.sch.set_constraint((self.var_manager.k_var % tbe_platform.BLOCK_REDUCE == 0).asobject())
            self.sch.set_constraint((self.var_manager.n_var % tbe_platform.BLOCK_OUT == 0).asobject())

    def _emit_insn_simplyfy_c_gm(self, compute_param):
        """
        add pragma on c_gm for cachetiling
        """
        # This template does not support overlarge imput dimension
        if not self.status_controller.support_fix_pipe_l0c2out:
            self.sch.set_constraint(self.var_manager.m_var * self.block_in < self.MAX_ORI_SHAPE_TEMP)
            if compute_param.format_a == "ND":
                self.sch.set_constraint(self.var_manager.k_var < self.MAX_ORI_SHAPE_TEMP)
            else:
                self.sch.set_constraint(self.var_manager.k_var * self.block_in < self.MAX_ORI_SHAPE_TEMP)
            self.sch.set_constraint(self.var_manager.n_var * self.block_out < self.MAX_ORI_SHAPE_TEMP)
        c_gm = self.tensor_map.get("c_gm")
        m_l0 = self.cache_tiling.get("m_l0")
        cub_n1 = self.cache_tiling.get("cub_n1")

        self.sch[c_gm].pragma(self.sch[c_gm].leaf_iter_vars[0], "constraint", self.var_manager.n_var - cub_n1 >= 0)
        self.sch[c_gm].pragma(self.sch[c_gm].leaf_iter_vars[0], "constraint",
            tvm.truncmod(((m_l0 * self.block_in) * (cub_n1 * self.block_out)), self.MAX_ORI_SHAPE_TEMP) > 0)
        self.sch[c_gm].pragma(self.sch[c_gm].leaf_iter_vars[0], "constraint",
            ((m_l0 * self.block_in) * (cub_n1 * self.block_out)) < self.MAX_ORI_SHAPE_TEMP)


    def _emit_insn_simplify_aub(self):
        """
        add pragma on aub for ND_in_ND_out cachetiling
        """
        # Set constraints for aub
        multi_k_aub_l1 = self.cache_tiling.get("multi_k_aub_l1")
        multi_m_ub_l1 = self.cache_tiling.get("multi_m_ub_l1")
        k_aub = self.cache_tiling.get("k_aub")
        m_aub = self.cache_tiling.get("m_aub")
        aub_var = [self.cache_tiling.get("k_aub"), self.cache_tiling.get("m_aub")]
        a_align_value = self.cache_tiling.get("a_align_value")
        trans_a = int(self.status_controller.transpose_a)
        # Constraint in m dimension: m_1 is not smaller than m1_single_core and
        # m1_single_core is not smaller than multi_m_ub_l1 * m_aub

        # Constraint in k dimension: k_ori is not smaller than k_single_core and
        # k_single_core is not smaller than multi_k_aub_l1 * k_aub * self.block_reduce
        self.sch.set_constraint(self.var_manager.m_var - multi_m_ub_l1 * m_aub >= 0)
        # aligned condition
        constraint_aub = aub_var[trans_a] * self.block_in + tvm.floormod(a_align_value -
            tvm.floormod(aub_var[trans_a] * self.block_in, a_align_value), a_align_value)
        constraint_aub_multi = aub_var[trans_a] * self.block_in * self.block_reduce + tvm.floormod(a_align_value -
            tvm.floormod(aub_var[trans_a] * self.block_in, a_align_value), a_align_value) * self.block_reduce
        self.sch.set_constraint((constraint_aub % self.block_in == 0).asobject())
        self.sch.set_constraint((constraint_aub_multi % (self.block_in * self.block_reduce) == 0).asobject())

    def _emit_insn_simplify_bub(self):
        """
        add pragma on bub for ND_in_ND_out cachetiling
        """
        # Set constraints for bub
        multi_k_bub_l1 = self.cache_tiling.get("multi_k_bub_l1")
        multi_n_ub_l1 = self.cache_tiling.get("multi_n_ub_l1")
        k_bub = self.cache_tiling.get("k_bub")
        n_bub = self.cache_tiling.get("n_bub")
        b_align_value = self.cache_tiling.get("b_align_value")
        trans_b = int(self.status_controller.transpose_b)
        bub_var = [self.cache_tiling.get("n_bub"), self.cache_tiling.get("k_bub")]
        # Constraint: n_1 is not smaller than n1_single_core and
        # n1_single_core is not smaller than n1_single_core multi_n_ub_l1 * n_bub;

        # Constraint: k_ori is not smaller than k_single_core and
        # k_single_core is not smaller than multi_k_bub_l1 * k_bub * self.block_reduce
        self.sch.set_constraint(self.var_manager.n_var - multi_n_ub_l1 * n_bub >= 0)
        # aligned condition
        constraint_bub = bub_var[trans_b] * self.block_out + tvm.floormod(
            b_align_value - tvm.floormod(bub_var[trans_b] * self.block_out, b_align_value), b_align_value)
        constraint_bub_multi = bub_var[trans_b] * self.block_out * self.block_reduce + tvm.floormod(
            b_align_value - tvm.floormod(bub_var[trans_b] * self.block_out, b_align_value),
            b_align_value) * self.block_reduce
        self.sch.set_constraint((constraint_bub % self.block_out == 0).asobject())
        self.sch.set_constraint((constraint_bub_multi % (self.block_out * self.block_reduce) == 0).asobject())

    def _emit_insn_simplify_al1(self, compute_param):
        """
        add pragma on al1 for ND_in_ND_out cachetiling
        """
        a_l1 = self.tensor_map.get("a_l1")
        a_align_value = self.cache_tiling.get("a_align_value")
        a_ori = [self.var_manager.k_var, self.var_manager.m_var]
        aub_var = [self.cache_tiling.get("k_aub"), self.cache_tiling.get("m_aub")]
        al1_k_ext = self.cache_tiling.get("kal1_16")
        multi_aub_var = [self.cache_tiling.get("multi_k_aub_l1"), self.cache_tiling.get("multi_m_ub_l1")]
        host_var_a = [aub_var[1] * self.block_in, aub_var[0] * self.block_reduce]
        trans_a = int(self.status_controller.transpose_a)

        cons1 = aub_var[trans_a] * self.block_in + tvm.floormod(a_align_value -
            tvm.floormod(aub_var[trans_a] * self.block_in, a_align_value), a_align_value)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint",
            tvm.div(cons1, self.block_in) - aub_var[trans_a] >= 0)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint", a_ori[trans_a] - aub_var[trans_a] >= 0)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint",
            tvm.truncmod((host_var_a[trans_a] * host_var_a[1 - trans_a]), self.MAX_ORI_SHAPE_TEMP) > 0)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint",
            (host_var_a[trans_a] * host_var_a[1 - trans_a]) < self.MAX_ORI_SHAPE_TEMP)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint",
            multi_aub_var[0] * aub_var[0] < self.MAX_UB_SHAPE)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint", al1_k_ext - aub_var[0] >= 0)
        self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint",
            tvm.truncmod(aub_var[1] * aub_var[0] * self.block_in * self.block_reduce, self.MAX_ORI_SHAPE_TEMP) > 0)
        if (self.status_controller.al1_attach_status == "c_l0c" and
            not self.status_controller.attach_at_flag.get("min_kl1_cmp_kl0") and compute_param.need_aub):
            self.sch[a_l1].pragma(self.sch[a_l1].leaf_iter_vars[0], "constraint",
                                  (self.cache_tiling.get("k_l0") - self.cache_tiling.get("k_aub") >= 0))

    def _emit_insn_simplify_bl1(self):
        """
        add pragma on bl1 for ND_in_ND_out cachetiling
        """
        b_l1 = self.tensor_map.get("b_l1")
        b_align_value = self.cache_tiling.get("b_align_value")
        b_ori = [self.var_manager.n_var, self.var_manager.k_var]
        bub_var = [self.cache_tiling.get("n_bub"), self.cache_tiling.get("k_bub")]
        multi_bub_var = [self.cache_tiling.get("multi_n_ub_l1"), self.cache_tiling.get("multi_k_bub_l1")]
        bl1_n_ext = (self.cache_tiling.get("n_ub_l0_time") *
                     self.cache_tiling.get("cub_n1") * self.cache_tiling.get("n_bl1"))
        host_var_b = [bub_var[1] * self.block_reduce, bub_var[0] * self.block_out]
        trans_b = int(self.status_controller.transpose_b)

        cons2 = bub_var[trans_b] * self.block_out + tvm.floormod(b_align_value -
            tvm.floormod(bub_var[trans_b] * self.block_out, b_align_value), b_align_value)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint",
            tvm.div(cons2, self.block_out) - bub_var[trans_b] >= 0)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint",
            b_ori[trans_b] < self.MAX_ORI_SHAPE_TEMP)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint",
            tvm.div(b_ori[trans_b], self.block_out) - bub_var[trans_b] >= 0)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint",
            tvm.truncmod((host_var_b[trans_b] * host_var_b[1 - trans_b]), self.MAX_ORI_SHAPE_TEMP) > 0)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint",
            (host_var_b[trans_b] * host_var_b[1 - trans_b]) < self.MAX_ORI_SHAPE_TEMP)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint",
            multi_bub_var[0] * bub_var[0] < self.MAX_UB_SHAPE)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint", bl1_n_ext - bub_var[0] >= 0)
        self.sch[b_l1].pragma(self.sch[b_l1].leaf_iter_vars[0], "constraint", tvm.truncmod(
            bub_var[1] * bub_var[0] * self.block_out * self.block_reduce, self.MAX_ORI_SHAPE_TEMP) > 0)

    def _enable_skip_bound_check(self, cache_tiling_mgr, sch_container, para_map):
        """
        use skip_bound_check to ignore iflikely restraint when k is splited by factor
        """
        no_support_skip_scene = (self.support_l0c2out_nz2nd or para_map.get("dtype_out") != "float16" or
                                 self.tiling.get("binary_tiling_data", None) is not None)
        if cache_tiling_mgr.unaligned_flag and no_support_skip_scene:
            get_context().get_current_compute().get_current_schedule().add(
                "_build_config", {"predicate_realize_bound": True})
        else:
            skip_bound_check_list = [self.tensor_map.get("a_l0a"), self.tensor_map.get("b_l0b"),
                                         self.tensor_map.get("c_l0c")]
            if not cache_tiling_mgr.unaligned_flag:
                skip_bound_check_list.append(self.tensor_map.get("a_l1"))
                skip_bound_check_list.append(self.tensor_map.get("b_l1"))
            if para_map.get("compress_flag", False):
                skip_bound_check_list.remove(self.tensor_map.get("c_l0c"))
            skip_bound_check_list += sch_container.tensors_in_aub
            skip_bound_check_list += sch_container.tensors_in_bub
            skip_bound_check_list += sch_container.tensors_in_cub
            for tensor in skip_bound_check_list:
                self.sch[tensor].skip_bound_check()

    def _buffer_tile_for_simplify(self, compute_param, sch_agent, unaligned_flag):
        """
        align m/k/n axis for tensor in ub and l1
        """
        aub_multi_flag = self.tiling.get("attach_at_flag").get("aub_multi_flag")
        bub_multi_flag = self.tiling.get("attach_at_flag").get("bub_multi_flag")
        al1_batch_ext = None
        if compute_param.need_aub:
            if aub_multi_flag == 1:
                # UB full load
                al1_k_ext = self.cache_tiling.get("k_aub")
                al1_m_ext = self.cache_tiling.get("m_aub")
            else:
                al1_k_ext = self.cache_tiling.get("kal1_16")
                al1_m_ext = self.cache_tiling.get("m_l0") * self.cache_tiling.get("m_al1")
            al1_batch_ext = self.cache_tiling.get("multi_batch_aub_l1") * self.cache_tiling.get("batch_aub")
            al1_buffer_tile_list = [(None, al1_m_ext), (None, al1_k_ext), (None, None), (None, None)]
            if self.status_controller.unaligned_flag and self.status_controller.transpose_a:
                al1_buffer_tile_list = [(None, al1_k_ext), (None, al1_m_ext), (None, None), (None, None)]
            if compute_param.batch_a:
                al1_buffer_tile_list.insert(0, (None, al1_batch_ext))
            sch_agent[self.tensor_map.get("a_l1")].buffer_tile(*al1_buffer_tile_list)
        bl1_batch_ext = None
        if compute_param.need_bub:
            if bub_multi_flag == 1:
                bl1_k_ext = self.cache_tiling.get("k_bub")
                bl1_n_ext = self.cache_tiling.get("n_bub")
            else:
                bl1_k_ext = self.cache_tiling.get("kbl1_16")
                bl1_n_ext = (self.cache_tiling.get("n_ub_l0_time") *
                             self.cache_tiling.get("cub_n1") * self.cache_tiling.get("n_bl1"))
            bl1_batch_ext = self.cache_tiling.get("multi_batch_bub_l1") * self.cache_tiling.get("batch_bub")
            bl1_buffer_tile_list = [(None, bl1_k_ext), (None, bl1_n_ext), (None, None), (None, None)]
            if self.status_controller.unaligned_flag and self.status_controller.transpose_b:
                bl1_buffer_tile_list = [(None, bl1_n_ext), (None, bl1_k_ext), (None, None), (None, None)]
            if compute_param.batch_b:
                bl1_buffer_tile_list.insert(0, (None, bl1_batch_ext))
            sch_agent[self.tensor_map.get("b_l1")].buffer_tile(*bl1_buffer_tile_list)
        l1_batch_ext = [al1_batch_ext, bl1_batch_ext]
        self._l0_buffer_tile(compute_param, sch_agent, l1_batch_ext, unaligned_flag)
        self._transdata_fusion_buffer_tile(sch_agent)

    def _transdata_fusion_buffer_tile(self, sch_agent):
        if not self.status_controller.nz_fusion_flag:
            return
        tensor_a_ub_nz = self.tensor_map.get("a_ub_nz")
        tensor_b_ub_nz = self.tensor_map.get("b_ub_nz")
        if tensor_a_ub_nz is not None:
            m1_aub_ext = self.cache_tiling.get("m1_aub")
            k1_aub_ext = self.cache_tiling.get("k1_aub")
            a_ub_nz_buffer_tile_list = [(None, k1_aub_ext), (None, m1_aub_ext), (None, None), (None, None)]
            if self.status_controller.transpose_a:
                a_ub_nz_buffer_tile_list = [(None, m1_aub_ext), (None, k1_aub_ext), (None, None), (None, None)]
            sch_agent[tensor_a_ub_nz].buffer_tile(*a_ub_nz_buffer_tile_list)
        if tensor_b_ub_nz is not None:
            k1_bub_ext = self.cache_tiling.get("k1_bub")
            n1_bub_ext = self.cache_tiling.get("n1_bub")
            b_ub_nz_buffer_tile_list = [(None, n1_bub_ext), (None, k1_bub_ext), (None, None), (None, None)]
            if self.status_controller.transpose_b:
                b_ub_nz_buffer_tile_list = [(None, k1_bub_ext), (None, n1_bub_ext), (None, None), (None, None)]
            sch_agent[tensor_b_ub_nz].buffer_tile(*b_ub_nz_buffer_tile_list)

    def _l0_buffer_tile(self, compute_param, sch_agent, l1_batch_ext, unaligned_flag):
        al1_batch_ext, bl1_batch_ext = l1_batch_ext
        batch_axes = self.sch[self.tensor_map.get("c_gm")].leaf_iter_vars
        batch_l0_extent = self.cache_tiling.get("batch_ub_l0_time") * self.cache_tiling.get("batch_cub")
        # the 3rd axis in batch_axes is the first axis below block axis; if split_k, 2 axes will be added
        batch_inner_idx = 3 + 2 * compute_param.split_k_flag
        batch_var = get_te_var("batch").get_tvm_var() if get_te_var("batch") else 1
        batch_single_core = get_te_var("batch_single_core").get_tvm_var() if get_te_var("batch_single_core") else 1
        batch_offset = tvm.min(batch_axes[compute_param.split_k_flag] * batch_single_core,
                               batch_var - batch_single_core)
        list_batch_broad = [get_optional_te_var("batch_c1"), get_optional_te_var("batch_c2"),
                            get_optional_te_var("batch_c3"), get_optional_te_var("batch_c4")]
        if unaligned_flag:
            batch_offset = batch_axes[compute_param.split_k_flag] * batch_single_core
        al0 = self.tensor_map.get("a_l0a")
        bl0 = self.tensor_map.get("b_l0b")
        c_l0c = self.tensor_map.get("c_l0c")
        zero_flag = self.cache_tiling.get("zero_flag")
        if compute_param.batch_a and "ori_batch_shape" in al0.op.attrs:
            al0_batch_ext = al1_batch_ext if compute_param.need_aub else batch_l0_extent
            al0_batch_offset = batch_offset + batch_axes[batch_inner_idx] * al0_batch_ext
            ori_batch_shape_a = al0.op.attrs["ori_batch_shape"]
            # the max batch_dim is 4
            batch_index_a = FormatCompute.get_batch_index(al0_batch_offset,
                                                          list_batch_broad[4 - len(ori_batch_shape_a):],
                                                          ori_batch_shape_a)
            self.sch[self.tensor_map.get("a_l1")].set_store_predicate([al0_batch_offset < batch_var, zero_flag == 0])
            sch_agent[al0].buffer_tile((batch_index_a, al0_batch_ext), (None, None),
                                       (None, None), (None, None), (None, None))
        if compute_param.batch_b and "ori_batch_shape" in bl0.op.attrs:
            bl0_batch_ext = bl1_batch_ext if compute_param.need_bub else batch_l0_extent
            bl0_batch_offset = batch_offset + batch_axes[batch_inner_idx] * bl0_batch_ext
            ori_batch_shape_b = bl0.op.attrs["ori_batch_shape"]
            # the max batch_dim is 4
            batch_index_b = FormatCompute.get_batch_index(bl0_batch_offset,
                                                          list_batch_broad[4 - len(ori_batch_shape_b):],
                                                          ori_batch_shape_b)
            self.sch[self.tensor_map.get("b_l1")].set_store_predicate([bl0_batch_offset < batch_var, zero_flag == 0])
            sch_agent[bl0].buffer_tile((batch_index_b, bl0_batch_ext), (None, None),
                                       (None, None), (None, None), (None, None))

    def _fuse_l1_axis(self, compute_param):
        """
        fuse axis for tensor in l1
        """
        if self.status_controller.have_batch_a:
            self.sch[self.tensor_map.get("a_l1")].fuse(
                self.sch[self.tensor_map.get("a_l1")].leaf_iter_vars[1],
                self.sch[self.tensor_map.get("a_l1")].leaf_iter_vars[2])
        else:
            self.sch[self.tensor_map.get("a_l1")].fuse(
                self.sch[self.tensor_map.get("a_l1")].leaf_iter_vars[0],
                self.sch[self.tensor_map.get("a_l1")].leaf_iter_vars[1])
        if compute_param.need_bub:
            if self.status_controller.have_batch_b:
                self.sch[self.tensor_map.get("b_l1")].fuse(
                    self.sch[self.tensor_map.get("b_l1")].leaf_iter_vars[1],
                    self.sch[self.tensor_map.get("b_l1")].leaf_iter_vars[2])
            else:
                self.sch[self.tensor_map.get("b_l1")].fuse(
                    self.sch[self.tensor_map.get("b_l1")].leaf_iter_vars[0],
                    self.sch[self.tensor_map.get("b_l1")].leaf_iter_vars[1])


class CacheTilingManager:
    """
    manager tiling vars and cache tiling flags in bianry mode
    """

    def __init__(self, sch, dynamic_para):
        self.sch = sch
        self.sch_agent = None
        self.tiling_strategy = dynamic_para.get("tiling_strategy")
        self.attach_at_flag = None
        self.non_factor_k_flag = None
        self.non_factor_bmn_flag = None
        self.performance_flag = None
        self.unaligned_flag = None
        self.reorder_flag = None
        self.cache_tiling = None
        self.batch_expr = None
        self.k_expr = None
        self.m_expr = None
        self.n_expr = None
        self.flag_l0c_preload = False
        self.flag_aub_preload = True
        self.flag_bub_preload = True
        self.block_in = tbe_platform.BLOCK_IN
        self.block_out = tbe_platform.BLOCK_OUT
        self.mata_fuzzy_scenario = False

    @staticmethod
    def get_res_tensor_emit_axis(format_info, status_controller):
        # if NZ format there are 2 axis, if ND format there are 4 axis, if have batch plus one batch axis
        res_axis = -2 if format_info.get("out") == "ND" else -4
        if status_controller.have_batch:
            res_axis -= 1
        return res_axis

    def simplify_cache_tiling(self, cce_simplification_obj, compute_param, sch_container):
        """
        simplify cache tiling variables in binary mode and process axis expression
        """
        if not self.cache_tiling:
            return
        sch_container.vector_muls_attr = {'axis_dynamic_shift': 1}
        cce_simplification_obj.cache_tiling = self.cache_tiling
        aub_multi_flag = self.attach_at_flag.get("aub_multi_flag")
        bub_multi_flag = self.attach_at_flag.get("bub_multi_flag")
        if self.performance_flag == 1:
            self.sch.set_var_value(self.cache_tiling.get("out_branch_flag"), 1)
            self.sch.set_var_value(self.cache_tiling.get("bias_flag"), 0)
        if aub_multi_flag == 1:
            self.sch.set_var_value(self.cache_tiling.get("multi_k_aub_l1"), 1)
            self.sch.set_var_value(self.cache_tiling.get("multi_m_ub_l1"), 1)
        elif aub_multi_flag == 0:
            self.sch.set_var_value(self.cache_tiling.get("multi_batch_aub_l1"), 1)
        if bub_multi_flag == 1:
            self.sch.set_var_value(self.cache_tiling.get("multi_n_ub_l1"), 1)
            self.sch.set_var_value(self.cache_tiling.get("multi_k_bub_l1"), 1)
        elif bub_multi_flag == 0:
            self.sch.set_var_value(self.cache_tiling.get("multi_batch_bub_l1"), 1)
        self._simplify_k_axis(cce_simplification_obj, compute_param)
        self._set_preload_flag()
        self._set_multi_batch_flag(cce_simplification_obj, compute_param)

    def config_cache_tiling(self, compute_param, ops_data_flow_mode):
        """
        config cache tiling variables in binary mode
        """
        self.attach_at_flag = self.tiling_strategy.get("attach_at_flag")
        self.non_factor_k_flag = self.tiling_strategy.get("non_factor_k_flag")
        self.non_factor_bmn_flag = self.tiling_strategy.get("non_factor_bmn_flag")
        self.unaligned_flag = self.tiling_strategy.get("unaligned_flag")
        self.non_factor_bmn_flag = False if self.unaligned_flag else self.non_factor_bmn_flag
        self._get_cache_tiling(compute_param, ops_data_flow_mode)
        # 1: high-performance template; 0: general template
        self.performance_flag = self.tiling_strategy.get("performance_flag")
        # 0: not changed; 1: reorder m_single_core and n_single_core
        self.reorder_flag = self.tiling_strategy.get("reorder_flag", 0)

    def multi_batch_process(self, container, status_controller, sch_agent):
        """
        handles the multi batch scene for binary mode
        """
        if self.cache_tiling:
            c_gm = container.tensor_map.get("c_gm")
            # batch_matmul support multi_batch
            if status_controller.have_batch:
                c_gm_axis_len = len(c_gm.op.axis)
                iter_axis = 3
                if status_controller.split_k_axis_by_tiling:
                    c_gm_axis_len += 1
                    iter_axis += 2
                batch_buffer_tile_list = [(None, None)] * c_gm_axis_len
                batch_buffer_tile_list[0] = (None, self.batch_expr)
                sch_agent[c_gm].buffer_tile(*batch_buffer_tile_list)
                batch_single_core = self.cache_tiling.get("batch_single_core")
                batch_cub = self.cache_tiling.get("batch_cub")
                batch_l1_factor = self.cache_tiling.get("batch_l1_factor")
                batch_l1_outer, batch_l1_inner = self.sch[c_gm].split(self.sch[c_gm].leaf_iter_vars[iter_axis],
                                                                      nparts=self.cache_tiling.get("batch_l1_factor"),
                                                                      tail_strategy="round_up")
                extent = tvm.floordiv((tvm.floordiv((batch_single_core - 1), batch_cub) - 1), batch_l1_factor)
                # Do not support different multi batch between L1 and L0
                self.sch[c_gm].pragma(self.sch[c_gm].leaf_iter_vars[0], "constraint", extent == 1)

    def cache_tiling_full_load(self, container, status_controller, sch_agent, res):
        """
        handles the full load scene for binary mode
        """
        if self.cache_tiling:
            c_gm = res
            iter_axis = 0 if (status_controller.pad_flag or status_controller.nz_fusion_flag) else 1
            if status_controller.have_batch:
                iter_axis += 2
            # split k_axis by k_dim will create one axis to bind multi core and one axis equal to 1
            if status_controller.split_k_axis_by_tiling:
                iter_axis += 2
            if self.attach_at_flag.get("bl1_attach_flag") == 0:
                self.sch.set_var_value(self.cache_tiling.get("n_single_core"), 1)
                bl1 = container.tensor_map.get("b_l1")
                bias_l1 = container.tensor_map.get("bias_l1")
                bias_zero = container.tensor_map.get("bias_zero")
                self.sch[bl1].compute_at(self.sch[c_gm], self.sch[c_gm].leaf_iter_vars[iter_axis])
                if bias_l1 is not None:
                    self.sch[bias_l1].compute_at(self.sch[c_gm], self.sch[c_gm].leaf_iter_vars[iter_axis])
                if bias_zero is not None:
                    self.sch[bias_zero].compute_at(self.sch[c_gm], self.sch[c_gm].leaf_iter_vars[iter_axis])
                fixpipe_l1 = container.tensor_map.get("fixpipe_l1", [])
                for tensor in fixpipe_l1:
                    self.sch[tensor].compute_at(self.sch[c_gm], self.sch[c_gm].leaf_iter_vars[iter_axis])
            if self.attach_at_flag.get("al1_attach_flag") == 0:
                self.sch.set_var_value(self.cache_tiling.get("m_single_core"), 1)
                al1 = container.tensor_map.get("a_l1")
                self.sch[al1].compute_at(self.sch[c_gm], self.sch[c_gm].leaf_iter_vars[iter_axis])

    def bind_multi_core_cache_tiling(self, root_tensor, status_controller, dtype_info, format_info, splited_flag):
        """
        bind multi-core for cache tiling
        multi-core is binded before split in non-factor scene, and binded after splited in factor scene

        Parameters
        ----------
        root_tensor: output tensor
        status_controller: object of GemmScheduleStatusController, control flags like "attach_status"
        dtype_info: data type information of input and output tensor
        format_info: format information of input and output tensor
        splited_flag: flag for whether the root tensor is splited

        Returns
        -------
        the total length of the multi-core axis
        """
        ax_result = [None, 1, 1, 1]
        if not self.non_factor_bmn_flag and not splited_flag:
            return ax_result
        if self.non_factor_bmn_flag and splited_flag:
            return ax_result

        self.mata_fuzzy_scenario = self._check_mata_fuzzy_scenerio(status_controller, dtype_info, format_info)
        format_out = format_info.get("out")
        ax_reduce, ax_batch, ax_m, ax_n = self._get_multi_core_axes(status_controller, format_out, root_tensor)
        splited_result = self._split_core_sub_func(root_tensor, ax_m, ax_n, format_out, self.mata_fuzzy_scenario)

        ax_result = self._bind_core_sub_func(root_tensor, [ax_reduce, ax_batch], splited_result,
                                             status_controller, self.mata_fuzzy_scenario)
        return ax_result

    def set_l0c_cub_buffer_size(self, container, format_info, tiling_work):
        """
        set buffer size for binary
        """
        if self.cache_tiling:
            cub_bound = self.cache_tiling.get("cub_n1") * self.cache_tiling.get(
                "m_l0") * self.block_in * self.block_out * tiling_work.cub_tiling_batch
            cl0_bound = cub_bound * self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("batch_ub_l0_time")
            bias_bound = self.cache_tiling.get("cub_n1") * self.block_out
            cub_fp32_bound = self.cache_tiling.get("out_branch_flag") * tvm.floordiv(cub_bound, 2)
            # the number 2 means fp32_data_size / fp16_data_size
            bias_fp32_bound = tvm.floordiv(self.cache_tiling.get("out_branch_flag") * bias_bound, 2)
            self._set_buffer_size(container.tensor_map.get("c_l0c"), cl0_bound)
            self._set_buffer_size(container.tensor_map.get("c_ub_fract"), cub_bound)
            self._set_buffer_size(container.tensor_map.get("cast_to_fp16"), cub_bound)
            if container.tensor_map.get("cast_to_fp32") is not None:
                self._set_buffer_size(container.tensor_map.get("cast_to_fp32"), cub_fp32_bound)

            if container.tensor_map.get("bias") is not None:
                self._set_buffer_size(container.tensor_map.get("bias_ub_fp16"), bias_bound)
                self._set_buffer_size(container.tensor_map.get("bias_ub_fp32"), bias_fp32_bound)
                self._set_buffer_size(container.tensor_map.get("bias_ub_drnn_cast_fp16"), bias_bound)
                self._set_buffer_size(container.tensor_map.get("bias_ub_drnn_cast_fp32"), bias_fp32_bound)
                self._set_buffer_size(container.tensor_map.get("c_add_bias_ub_fp16"), cub_bound)
                self._set_buffer_size(container.tensor_map.get("c_add_bias_ub_fp32"), cub_fp32_bound)
            if format_info.get("out") == "ND":
                extra_size_solving_bank_conflict = (self.cache_tiling.get("m_l0") *
                    self.block_in * self.block_out * tiling_work.cub_tiling_batch)
                nz_to_nd_bound = tvm.select(self.cache_tiling.get("flag_cub_solving_bank_conflict") == 1,
                                            cub_bound + extra_size_solving_bank_conflict, cub_bound)
                self._set_buffer_size(container.tensor_map.get("nz_to_nd"), nz_to_nd_bound)
                if container.tensor_map.get("nz_to_nd_fp32") is not None:
                    self._set_buffer_size(container.tensor_map.get("nz_to_nd_fp32"), cub_fp32_bound)

    def _set_buffer_size(self, tensor, bound):
        if tensor is None:
            return
        self.sch[tensor].set_buffer_size(bound)

    def _get_multi_core_axes(self, status_controller, format_out, root_tensor):
        """"
        get the multi-core bonding axis
        """
        # in split_k_nd_out scene, schedule_agent not need init
        self.sch_agent.update_ignore_init(status_controller.split_k_with_nd_out)
        axis_outer = self.sch_agent[root_tensor].get_active_scopes()
        self.sch_agent.update_ignore_init(False)
        start_index = 0
        ax_reduce = None
        ax_batch = None
        if status_controller.split_k_axis_by_tiling:
            ax_reduce = axis_outer[start_index]
            start_index += 1
        if status_controller.have_batch:
            ax_batch = axis_outer[start_index]
            start_index += 1
        if format_out == "ND":
            ax_m, ax_n = axis_outer[start_index:]
        else:
            ax_n, ax_m, *_ = axis_outer[start_index:]
        core_axes = (ax_reduce, ax_batch, ax_m, ax_n)
        return core_axes

    def _check_mata_fuzzy_scenerio(self, status_controller, dtype_info, format_info):
        binary_tiling_data = self.tiling_strategy.get("binary_tiling_data")
        if not binary_tiling_data:
            return False
        is_nd_in_out = format_info.get("a") == "ND" and format_info.get("b") == "ND" and format_info.get("out") == "ND"
        dtype_support_list = [("float16", "float16", "float16"), ("bfloat16", "bfloat16", "bfloat16")]
        is_dtype_support = (dtype_info.get("a"), dtype_info.get("b"), dtype_info.get("out")) in dtype_support_list
        mata_fuzzy_scenario = (is_nd_in_out and is_dtype_support and status_controller.support_fix_pipe_l0c2out and
                               not status_controller.have_batch and binary_tiling_data.get("bias_flag") == 0)
        mata_fuzzy_scenario &= self.non_factor_bmn_flag == 1
        mata_fuzzy_scenario &= (not status_controller.pad_flag and not status_controller.nz_fusion_flag)
        mata_fuzzy_scenario &= (not self.cache_tiling.get("zero_flag"))
        al1_attach_flag = self.attach_at_flag.get("al1_attach_flag")
        bl1_attach_flag = self.attach_at_flag.get("bl1_attach_flag")
        mata_fuzzy_scenario &= (al1_attach_flag == 2 and bl1_attach_flag == 2)
        mata_fuzzy_scenario &= (status_controller.transpose_a and not status_controller.transpose_b)
        m_dim = binary_tiling_data.get("m_dim")
        n_dim = binary_tiling_data.get("n_dim")
        mata_fuzzy_scenario &= (m_dim * n_dim <= int(tbe_platform.get_soc_spec("CUBE_CORE_CNT")))
        m_single_core = binary_tiling_data.get("m_single_core")
        n_single_core = binary_tiling_data.get("n_single_core")
        m_l0 = binary_tiling_data.get("m_l0")
        n_l0 = binary_tiling_data.get("n_ub_l0_time") * binary_tiling_data.get("cub_n1")
        basic_block = [[8, 16], [16, 8]]
        mata_fuzzy_scenario &= (
            (m_l0 == basic_block[0][0] and n_l0 == basic_block[0][1] and m_single_core % 2 == 0) or
            (m_l0 == basic_block[1][0] and n_l0 == basic_block[1][1] and n_single_core % 2 == 0)
        )
        return mata_fuzzy_scenario

    def _split_core_sub_func(self, root_tensor, ax_m, ax_n, format_out, mata_fuzzy_scenario):
        m_dim = self.cache_tiling.get("m_dim")
        n_dim = self.cache_tiling.get("n_dim")
        if self.non_factor_bmn_flag:
            m_factor = self.cache_tiling.get("m_al1") * self.cache_tiling.get("m_l0")
            n_factor = (self.cache_tiling.get("n_bl1") * self.cache_tiling.get("n_ub_l0_time") *
                        self.cache_tiling.get("cub_n1"))
            if mata_fuzzy_scenario:
                binary_tiling_data = self.tiling_strategy.get("binary_tiling_data")
                m_l0 = binary_tiling_data.get("m_l0")
                n_l0 = binary_tiling_data.get("n_ub_l0_time") * binary_tiling_data.get("cub_n1")
                # the 1st 8 blocks are loaded from HBM and the 2nd 8 blocks will hit L2 cache
                m_factor *= 2 if m_l0 == 8 else 1
                n_factor *= 2 if n_l0 == 8 else 1
            else:
                m_factor = self.cache_tiling.get("m_single_core") * m_factor
                n_factor = self.cache_tiling.get("n_single_core") * n_factor
            if format_out == "ND":
                m_factor *= tbe_platform.BLOCK_IN
                n_factor *= tbe_platform.BLOCK_OUT
            if tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
                # if extent smaller than factor, shift_inwards will fail
                m_factor = tvm.min(ax_m.dom.extent, m_factor)
                n_factor = tvm.min(ax_n.dom.extent, n_factor)
            params = SplitParam(tail_strategy="shift_inwards", active_scope="inner")
            ax_m_out, ax_m_in = self.sch_agent[root_tensor].split(ax_m, factor=m_factor, split_params=params)
            ax_n_out, ax_n_in = self.sch_agent[root_tensor].split(ax_n, factor=n_factor, split_params=params)

            ax_m_oo, ax_n_oo = None, None
            if mata_fuzzy_scenario:
                ax_m_oo, ax_m_out = self.sch_agent[root_tensor].split(ax_m_out, factor=m_dim, split_params=params)
                ax_n_oo, ax_n_out = self.sch_agent[root_tensor].split(ax_n_out, factor=n_dim, split_params=params)
            splited_axis = [ax_n_out, ax_m_out, ax_n_in, ax_m_in, ax_n_oo, ax_m_oo]
        else:
            params = SplitParam(tail_strategy="guard_with_if", active_scope="outer")
            ax_m_out, ax_m_in = self.sch_agent[root_tensor].split(ax_m, nparts=m_dim, split_params=params)
            ax_n_out, ax_n_in = self.sch_agent[root_tensor].split(ax_n, nparts=n_dim, split_params=params)
            splited_axis = [ax_n_out, ax_m_out, ax_n_in, ax_m_in, None, None]
        return [splited_axis, params]

    def _bind_core_sub_func(self, root_tensor, ori_axis, splited_result, status_controller, mata_fuzzy_scenario):
        batch_dim = self.cache_tiling.get("batch_dim")
        m_dim = self.cache_tiling.get("m_dim")
        n_dim = self.cache_tiling.get("n_dim")
        ax_reduce, ax_batch = ori_axis
        splited_axis, params = splited_result
        ax_n_out, ax_m_out, ax_n_inner, ax_m_inner, ax_n_oo2, ax_m_oo2 = splited_axis

        multi_core_axes_list = [ax_n_out, ax_m_out]
        single_core_axes_list = [ax_n_inner, ax_m_inner]
        if mata_fuzzy_scenario:
            single_core_axes_list = [ax_n_oo2, ax_m_oo2, ax_n_inner, ax_m_inner]
            binary_tiling_data = self.tiling_strategy.get("binary_tiling_data")
            m_extent = binary_tiling_data.get("m")
            n_extent = binary_tiling_data.get("n")
            if m_extent >= n_extent:
                multi_core_axes_list = [ax_m_out, ax_n_out]
                single_core_axes_list = [ax_m_oo2, ax_n_oo2, ax_m_inner, ax_n_inner]
        axis_core = ax_n_out * m_dim + ax_m_out
        if status_controller.have_batch:
            if self.non_factor_bmn_flag:
                ax_batch_out, ax_batch_inner = self.sch_agent[root_tensor].split(ax_batch,
                    factor=self.cache_tiling.get("batch_single_core"), split_params=params)
            else:
                ax_batch_out, ax_batch_inner = self.sch_agent[root_tensor].split(ax_batch,
                    nparts=batch_dim, split_params=params)
            multi_core_axes_list.insert(0, ax_batch_out)
            single_core_axes_list.insert(0, ax_batch_inner)
            axis_core += ax_batch_out * (n_dim * m_dim)
        ax_k_out = 1
        if status_controller.split_k_axis_by_tiling:
            is_fp32_padfusion = status_controller.pad_flag and status_controller.ops_data_flow_mode == "fp322fp32"
            params = params if is_fp32_padfusion else None
            ax_k_out, ax_k_inner = self.sch_agent[root_tensor].split(ax_reduce, factor=1, split_params=params)
            multi_core_axes_list.insert(0, ax_k_out)
            single_core_axes_list.insert(0, ax_k_inner)
            axis_core += ax_k_out * (batch_dim * n_dim * m_dim)
        self.sch[root_tensor].reorder(*(multi_core_axes_list + single_core_axes_list))
        if status_controller.pad_flag or status_controller.nz_fusion_flag:
            block_fused = self.sch[root_tensor].fuse(*multi_core_axes_list)
            self.sch[root_tensor].bind(block_fused, tvm.thread_axis("blockIdx.x"))
        else:
            self.sch.bind_axes(multi_core_axes_list, tvm.thread_axis("blockIdx.x"))
        return [axis_core, ax_k_out, ax_m_out, ax_n_out]

    def _get_cache_tiling(self, compute_param, ops_data_flow_mode):
        """
        get basic tiling variables in binary mode
        """
        self.cache_tiling = {
            "m": get_te_var("m").get_tvm_var(),
            "k": get_te_var("k").get_tvm_var(),
            "n": get_te_var("n").get_tvm_var(),
            "batch_single_core": get_te_var("batch_single_core").get_tvm_var(),
            "k_ori": get_te_var("k_ori").get_tvm_var(),
            "batch_dim": get_te_var("batch_dim").get_tvm_var(),
            "n_single_core": get_te_var("n_single_core").get_tvm_var(),
            "n_dim": get_te_var("n_dim").get_tvm_var(),
            "n_bl1": get_te_var("n_bl1").get_tvm_var(),
            "n_ub_l0_time": get_te_var("n_ub_l0_time").get_tvm_var(),
            "cub_n1": get_te_var("cub_n1").get_tvm_var(),
            "m_dim": get_te_var("m_dim").get_tvm_var(),
            "m_single_core": get_te_var("m_single_core").get_tvm_var(),
            "m_al1": get_te_var("m_al1").get_tvm_var(),
            "m_l0": get_te_var("m_l0").get_tvm_var(),
            "k_dim": get_te_var("k_dim").get_tvm_var(),
            "k_l0": get_te_var("k_l0").get_tvm_var(),
            "k_al0": get_te_var("k_l0").get_tvm_var(),
            "k_bl0": get_te_var("k_l0").get_tvm_var(),
            "kal1_factor": get_te_var("kal1_factor").get_tvm_var(),
            "kbl1_factor": get_te_var("kbl1_factor").get_tvm_var(),
            "kal0_factor": get_te_var("kal0_factor").get_tvm_var(),
            "kbl0_factor": get_te_var("kbl0_factor").get_tvm_var(),
            "kal1_16": get_te_var("kal1_16").get_tvm_var(),
            "kbl1_16": get_te_var("kbl1_16").get_tvm_var(),
            "kl1_times": get_te_var("kl1_times").get_tvm_var(),
            "batch_l1_factor": get_te_var("batch_l1_factor").get_tvm_var(),
            "batch_ub_l0_time": get_te_var("batch_ub_l0_time").get_tvm_var(),
            "batch_cub": get_te_var("batch_cub").get_tvm_var(),
            "out_branch_flag": get_te_var("out_branch_flag").get_tvm_var(),
            "bias_flag": get_te_var("bias_flag").get_tvm_var(),
            "hf32_flag": get_te_var("hf32_flag").get_tvm_var(),
            "m_aub": get_optional_te_var("m_aub"),
            "n_bub": get_optional_te_var("n_bub"),
            "k_aub": get_optional_te_var("k_aub"),
            "k_bub": get_optional_te_var("k_bub"),
            "batch_aub": get_optional_te_var("batch_aub"),
            "batch_bub": get_optional_te_var("batch_bub"),
            "multi_n_ub_l1": get_optional_te_var("multi_n_ub_l1"),
            "multi_m_ub_l1": get_optional_te_var("multi_m_ub_l1"),
            "multi_k_aub_l1": get_optional_te_var("multi_k_aub_l1"),
            "multi_k_bub_l1": get_optional_te_var("multi_k_bub_l1"),
            "multi_batch_aub_l1": get_optional_te_var("multi_batch_aub_l1"),
            "multi_batch_bub_l1": get_optional_te_var("multi_batch_bub_l1"),
            "a_align_value": get_optional_te_var("a_align_value"),
            "b_align_value": get_optional_te_var("b_align_value"),
            "aub_align_bound": get_optional_te_var("aub_align_bound"),
            "bub_align_bound": get_optional_te_var("bub_align_bound"),
            "flag_cub_solving_bank_conflict": get_optional_te_var("flag_cub_solving_bank_conflict"),
            "unaligned_flag": self.unaligned_flag,
            "datatype_bf16": get_optional_te_var("datatype_bf16"),
            "zero_flag": self.tiling_strategy.get("zero_flag", 0),
            "aub_dim": get_optional_te_var("aub_dim"),
            "bub_dim": get_optional_te_var("bub_dim"),
            "m_pad": get_optional_te_var("m_pad"),
            "k_pad": get_optional_te_var("k_pad"),
            "n_pad": get_optional_te_var("n_pad"),
            "m1_aub": get_optional_te_var("m1_aub"),
            "n1_bub": get_optional_te_var("n1_bub"),
            "k1_aub": get_optional_te_var("k1_aub"),
            "k1_bub": get_optional_te_var("k1_bub"),
            "m_aub_dim": get_optional_te_var("m_aub_dim"),
            "n_bub_dim": get_optional_te_var("n_bub_dim"),
            "k_aub_dim": get_optional_te_var("k_aub_dim"),
            "k_bub_dim": get_optional_te_var("k_bub_dim"),
            "al1_db": get_optional_te_var("al1_db"),
            "bl1_db": get_optional_te_var("bl1_db"),
            "l0c_db": get_optional_te_var("l0c_db"),
            "l2_cache_flag": get_optional_te_var("l2_cache_flag"),
            "close_k_shift": get_optional_te_var("close_k_shift")
        }
        self.cache_tiling["kal1_16"] = self.cache_tiling.get("kal0_factor") * self.cache_tiling.get("k_l0")
        self.cache_tiling["kbl1_16"] = self.cache_tiling.get("kbl0_factor") * self.cache_tiling.get("k_l0")
        self.cache_tiling["batch_l0"] = self.cache_tiling.get("batch_ub_l0_time") * self.cache_tiling.get("batch_cub")
        if ((not compute_param.split_k_flag and (not ops_data_flow_mode == "fp322fp32")) or
           (not self.tiling_strategy.get("performance_flag") and ops_data_flow_mode == "fp322fp32")):
            self.cache_tiling["k_dim"] = 1
        abkl1_attach_flag = self.attach_at_flag.get("abkl1_attach_flag")
        if abkl1_attach_flag == 0:
            self._norange_kal1_kbl1_equal()
        elif abkl1_attach_flag == 1:
            self._norange_kal1(compute_param.pad_flag)
        else:
            self._norange_kbl1(compute_param.pad_flag)
        self.m_expr = (self.cache_tiling.get("m_dim") * self.cache_tiling.get("m_single_core") *
                       self.cache_tiling.get("m_al1") * self.cache_tiling.get("m_l0"))
        self.n_expr = (self.cache_tiling.get("n_dim") * self.cache_tiling.get("n_single_core") *
                       self.cache_tiling.get("n_bl1") * self.cache_tiling.get("n_ub_l0_time") *
                       self.cache_tiling.get("cub_n1"))
        support_fix_pipe_l0c2out = tbe_platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        if not support_fix_pipe_l0c2out:
            self.batch_expr = (self.cache_tiling.get("batch_dim") * self.cache_tiling.get("batch_l0") *
                               self.cache_tiling.get("batch_l1_factor"))

    def _simplify_k_axis(self, cce_simplification_obj, compute_param):
        kal0_factor = self.cache_tiling.get("kal0_factor")
        kbl0_factor = self.cache_tiling.get("kbl0_factor")
        kal1_factor = self.cache_tiling.get("kal1_factor")
        k_dim = self.cache_tiling.get("k_dim")
        k_l0 = self.cache_tiling.get("k_l0")
        abkl1_attach_flag = self.attach_at_flag.get("abkl1_attach_flag")
        if abkl1_attach_flag == 0:
            self.sch.set_var_value(self.cache_tiling.get("kl1_times"), 1)
            if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
                self.sch.set_var_value(self.cache_tiling.get("kal0_factor"), 1)
            if self.attach_at_flag.get("al1_attach_flag") == 0:
                self.sch.set_var_value(self.cache_tiling.get("kal1_factor"), 1)
                if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
                    cce_simplification_obj.set_kaub_simplification_l1_fullload(
                        kal1_factor * kal0_factor * k_l0, compute_param)
                    if not self.unaligned_flag and compute_param.need_aub:
                        self.sch.set_var_value(self.cache_tiling.get("k_ori"), k_dim * k_l0 * tbe_platform.BLOCK_REDUCE)
        elif abkl1_attach_flag == 1:
            if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
                self.sch.set_var_value(kbl0_factor, 1)
            if self.attach_at_flag.get("al1_attach_flag") == 0:
                self.sch.set_var_value(self.cache_tiling.get("kal1_factor"), 1)
                cce_simplification_obj.set_kaub_simplification_l1_fullload(
                    self.cache_tiling.get("kbl1_factor") * kbl0_factor * k_l0, compute_param)
        else:
            if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
                self.sch.set_var_value(kal0_factor, 1)
            if self.attach_at_flag.get("bl1_attach_flag") == 0:
                self.sch.set_var_value(self.cache_tiling.get("kbl1_factor"), 1)

    def _norange_kal1_kbl1_equal(self):
        """
        config k related tiling variable when kal1 equals kbl1
        """
        kal0_factor = self.cache_tiling.get("kal0_factor")
        kal1_factor = self.cache_tiling.get("kal1_factor")
        k_l0 = self.cache_tiling.get("k_l0")
        k_dim = self.cache_tiling.get("k_dim")
        if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
            self.cache_tiling["k_al0"] = kal0_factor * k_l0
            self.cache_tiling["k_bl0"] = kal0_factor * k_l0
            if self.attach_at_flag.get("al1_attach_flag") == 0:
                self.cache_tiling["k_al0"] = kal1_factor * kal0_factor * k_l0
                self.cache_tiling["k_bl0"] = self.cache_tiling.get("k_al0")
        self.cache_tiling["kal1_16"] = kal0_factor * k_l0
        self.cache_tiling["kbl1_16"] = kal0_factor * k_l0
        self.k_expr = kal1_factor * kal0_factor * k_l0 * k_dim

    def _norange_kal1(self, pad_flag):
        """
        config k related tiling variable when kal1 large than kbl1
        """
        kbl0_factor = self.cache_tiling.get("kbl0_factor")
        k_l0 = self.cache_tiling.get("k_l0")
        kl1_times = self.cache_tiling.get("kl1_times")
        k_dim = self.cache_tiling.get("k_dim")
        if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
            self.cache_tiling["k_al0"] = kbl0_factor * k_l0
            self.cache_tiling["k_bl0"] = kbl0_factor * k_l0
        if self.attach_at_flag.get("al1_attach_flag") in (0, 1):
            # Only Valid in NZ Mode
            self.k_expr = self.cache_tiling.get("kbl1_factor") * kbl0_factor * k_l0 * k_dim
            if k_dim == 1 and not pad_flag:
                self.cache_tiling["kal1_16"] = self.cache_tiling.get("k")
        else:
            self.cache_tiling["kal1_16"] = kl1_times * kbl0_factor * k_l0
            self.cache_tiling["kbl1_16"] = kbl0_factor * k_l0
            self.k_expr = self.cache_tiling.get("kal1_factor") * kl1_times * kbl0_factor * k_l0 * k_dim

    def _norange_kbl1(self, pad_flag):
        """
        config k related tiling variable when kal1 smaller than kbl1
        """
        kal0_factor = self.cache_tiling.get("kal0_factor")
        k_l0 = self.cache_tiling.get("k_l0")
        kl1_times = self.cache_tiling.get("kl1_times")
        k_dim = self.cache_tiling.get("k_dim")
        if not self.attach_at_flag.get("min_kl1_cmp_kl0"):
            self.cache_tiling["k_al0"] = kal0_factor * k_l0
            self.cache_tiling["k_bl0"] = kal0_factor * k_l0
        if self.attach_at_flag.get("bl1_attach_flag") in (0, 1):
            self.k_expr = self.cache_tiling.get("kal1_factor") * kal0_factor * k_l0 * k_dim
            if k_dim == 1 and not pad_flag:
                self.cache_tiling["kbl1_16"] = self.cache_tiling.get("k")
        else:
            self.cache_tiling["kbl1_16"] = kl1_times * kal0_factor * k_l0
            self.k_expr = self.cache_tiling.get("kbl1_factor") * kl1_times * kal0_factor * k_l0 * k_dim

    def _set_preload_flag(self):
        l0c_pb = self.tiling_strategy.get("manual_pingpong_buffer").get("CL0_pbuffer")
        al1_attach_flag = self.attach_at_flag.get("al1_attach_flag")
        bl1_attach_flag = self.attach_at_flag.get("bl1_attach_flag")
        # enable l0c_preload for template_5 because only template_5's performance is improved for now
        # 2,2,0 means that l0c_double_buffer, no_k_al1_full_load and bl1_full_load
        self.flag_l0c_preload = (l0c_pb == 2) and ((al1_attach_flag != 0 and bl1_attach_flag == 0) or
                                                   (al1_attach_flag == 0 and bl1_attach_flag != 0))
        self.flag_aub_preload = self.flag_aub_preload and (not self.flag_l0c_preload)
        self.flag_bub_preload = self.flag_bub_preload and (not self.flag_l0c_preload)

    def _set_multi_batch_flag(self, cce_simplification_obj, compute_param):
        # support multi_batch
        if cce_simplification_obj.status_controller.have_batch:
            # not full load no multi_batch
            # l0c_multi_batch == 0 means no multi_batch
            # l0c_multi_batch > 0 means multi_batch on
            # and equals batch_aub_axis_flag * 2^2 + batch_bub_axis_flag * 2^1 + batch_cub_axis_flag * 2^0 + 1
            l0c_multi_batch = self.attach_at_flag.get("l0c_multi_batch")
            if l0c_multi_batch == 0:
                self.sch.set_var_value(self.cache_tiling.get("batch_cub"), 1)
                self.sch.set_var_value(self.cache_tiling.get("batch_ub_l0_time"), 1)
                if compute_param.need_aub:
                    self.sch.set_var_value(self.cache_tiling.get("batch_aub"), 1)
                    self.sch.set_var_value(self.cache_tiling.get("batch_bub"), 1)
                    self.sch.set_var_value(self.cache_tiling.get("multi_batch_aub_l1"), 1)
                    self.sch.set_var_value(self.cache_tiling.get("multi_batch_bub_l1"), 1)
            else:
                l0c_multi_batch -= 1
                batch_aub_axis_flag = l0c_multi_batch // 4
                batch_bub_axis_flag = l0c_multi_batch % 4 // 2
                batch_cub_axis_flag = l0c_multi_batch % 4 % 2
                if not batch_cub_axis_flag:
                    self.sch.set_var_value(self.cache_tiling.get("batch_ub_l0_time"), 1)
                if compute_param.need_aub:
                    aub_multi_flag = self.attach_at_flag.get("aub_multi_flag")
                    bub_multi_flag = self.attach_at_flag.get("bub_multi_flag")
                    # if aub/ bub full load, then aub/bub preload is disabled.
                    if not batch_aub_axis_flag:
                        self.sch.set_var_value(self.cache_tiling.get("multi_batch_aub_l1"), 1)
                        self.flag_aub_preload = self.flag_aub_preload and (not aub_multi_flag)
                    if not batch_bub_axis_flag:
                        self.sch.set_var_value(self.cache_tiling.get("multi_batch_bub_l1"), 1)
                        self.flag_bub_preload = self.flag_bub_preload and (not bub_multi_flag)


class VarManage:
    """
    manage variable and range in shape and tiling
    """
    def __init__(self, sch, var_range):
        self.sch = sch
        self.var_range = var_range
        self.batch_var = None
        self.m_var = None
        self.k_var = None
        self.n_var = None
        self.k_ori_var = None
        self.binary_tiling_data = None
        self.commom_var_range = {
            "range_block_dim": (1, 32),
            "range_64": (1, 64),
            "range_128": (1, 128),
            "range_1024": (1, 1024),
            "range_ub": (256, 262144),
            "range_2": (1, 2),
            "range_1": (0, 1),
            "range_none": (1, None),
            "range_48": (1, 48),
            "range_16_65280": (16, 65280),
            "range_65280": (1, 65280),
            "range_384": (1, 384),
            "range_l2_cache_type": (1, 30)
        }
        self.binary_shape_name = ("m", "k", "n")

    def set_var_range_for_dynamic_scene(self, compute_param, cache_tiling_mgr):
        """
        set range for vars in shape and tiling
        """
        self.batch_var = get_optional_te_var(compute_param.batch_var_name)
        self.m_var = get_optional_te_var(compute_param.m_var_name)
        self.k_var = get_optional_te_var(compute_param.k_var_name)
        self.n_var = get_optional_te_var(compute_param.n_var_name)
        self.k_ori_var = get_optional_te_var(compute_param.k_ori_var_name)
        self._set_var_range_for_cache_tiling(compute_param, cache_tiling_mgr)

    def _init_var_range_dict(self, compute_param):
        range_1024 = self.commom_var_range.get("range_1024")
        range_64 = self.commom_var_range.get("range_64")
        range_128 = self.commom_var_range.get("range_128")
        range_ub = self.commom_var_range.get("range_ub")
        range_block_dim = self.commom_var_range.get("range_block_dim")
        range_2 = self.commom_var_range.get("range_2")
        range_1 = self.commom_var_range.get("range_1")
        range_none = self.commom_var_range.get("range_none")
        range_48 = self.commom_var_range.get("range_48")
        range_65280 = self.commom_var_range.get("range_65280")
        range_16_65280 = self.commom_var_range.get("range_16_65280")
        range_384 = self.commom_var_range.get("range_384")
        range_l2_cache_type = self.commom_var_range.get("range_l2_cache_type")
        var_range_dict = {"batch_dim": range_block_dim, "n_dim": range_block_dim, "m_dim": range_block_dim,
                          "m_single_core": range_1024, "n_single_core": range_1024, "m_al1": range_1024,
                          "n_bl1": range_1024, "cub_n1": range_64, "m_l0": range_64, "k_l0": range_64,
                          "n_ub_l0_time": range_64, "kal0_factor": range_64, "kbl0_factor": range_64,
                          "kal1_factor": range_64, "kbl1_factor": range_64, "kl1_times": range_64,
                          "batch_ub_l0_time": range_64, "batch_l1_factor": range_64, "batch_cub": range_64,
                          "out_branch_flag": range_2, "bias_flag": range_1, "batch_single_core": range_none,
                          "hf32_flag": range_2, "datatype_bf16": range_1, "al1_db": range_1,
                          "bl1_db": range_1, "l0c_db": range_1, "l2_cache_flag": range_l2_cache_type,
                          "close_k_shift": range_1}
        if compute_param.need_aub:
            value_range_append = {"m_aub": range_128, "k_aub": range_128, "k_bub": range_128, "n_bub": range_128,
                                  "multi_n_ub_l1": range_64, "multi_m_ub_l1": range_64, "multi_k_aub_l1": range_64,
                                  "multi_k_bub_l1": range_64, "a_align_value":range_1024, "b_align_value":range_1024,
                                  "aub_align_bound": range_ub, "bub_align_bound":range_ub, "batch_aub": range_64,
                                  "batch_bub": range_64, "multi_batch_aub_l1": range_64,
                                  "multi_batch_bub_l1": range_64}
            var_range_dict.update(value_range_append)
        if compute_param.split_k_flag:
            value_range_append_dim = {"k_dim": range_block_dim}
            var_range_dict.update(value_range_append_dim)
        if compute_param.pad_flag:
            pad_range = {"m_aub": range_65280, "k_aub": range_65280, "k_bub": range_65280, "n_bub": range_65280,
                         "aub_dim": range_48, "bub_dim": range_48,
                         "m_pad": range_16_65280, "k_pad": range_16_65280, "n_pad": range_16_65280}
            var_range_dict.update(pad_range)
        if compute_param.nz_fusion_flag:
            nz_fusion_range = {"m1_aub": range_384, "k1_aub": range_384, "k1_bub": range_384,
                               "n1_bub": range_384, "k_aub_dim": range_48, "k_bub_dim": range_48,
                               "m_aub_dim": range_48, "n_bub_dim": range_48}
            var_range_dict.update(nz_fusion_range)
        return var_range_dict

    def _static_init_var_range_dict(self, compute_param):
        var_range_dict = {}
        binary_tiling_data_list = {"batch_dim", "n_dim", "m_dim", "batch_single_core", "m_single_core", "n_single_core",
                                   "m_al1", "n_bl1", "cub_n1", "m_l0", "k_l0", "n_ub_l0_time", "kal0_factor",
                                   "kbl0_factor", "kal1_factor", "kbl1_factor", "kl1_times",
                                   "batch_ub_l0_time", "batch_l1_factor", "batch_cub", "out_branch_flag", "bias_flag",
                                   "hf32_flag", "datatype_bf16", "al1_db", "bl1_db", "l0c_db", "l2_cache_flag",
                                   "close_k_shift"}
        if compute_param.pad_flag:
            binary_tiling_data_list.update({"m_aub", "n_bub", "k_aub", "k_bub", "aub_dim", "bub_dim"})
        if compute_param.nz_fusion_flag:
            binary_tiling_data_list.update({
                "m1_aub", "n1_bub", "k1_aub", "k1_bub", "k_aub_dim", "k_bub_dim", "m_aub_dim", "n_bub_dim"})
        for _, value in enumerate(binary_tiling_data_list):
            if self.binary_tiling_data.get(value) is not None:
                var_range_dict[value] = self.binary_tiling_data.get(value)
        if compute_param.need_aub:
            binary_tiling_data_nd_list = {"m_aub", "k_aub", "k_bub", "n_bub", "multi_n_ub_l1", "multi_m_ub_l1",
                                          "multi_k_aub_l1", "multi_k_bub_l1", "a_align_value", "b_align_value",
                                          "aub_align_bound", "bub_align_bound", "batch_aub", "batch_bub",
                                          "multi_batch_aub_l1", "multi_batch_bub_l1"}
            for _, value in enumerate(binary_tiling_data_nd_list):
                if self.binary_tiling_data.get(value) is not None:
                    var_range_dict[value] = self.binary_tiling_data.get(value)
        if compute_param.split_k_flag:
            var_range_dict["k_dim"] = self.binary_tiling_data.get("k_dim")
        return var_range_dict

    def _set_var_range_for_cache_tiling(self, compute_param, cache_tiling_mgr):
        """
        set var range for cache tiling
        """
        if self.binary_tiling_data:
            static_range_dict = self._static_init_var_range_dict(compute_param)
            for var, static_range in static_range_dict.items():
                self.sch.set_var_range(cache_tiling_mgr.cache_tiling.get(var), static_range, static_range)
            compute = get_context().get_current_compute().get_operator_context().get_current_compute()
            compute.get_var("m").set_bound([self.binary_tiling_data.get("m"), self.binary_tiling_data.get("m")])
            compute.get_var("n").set_bound([self.binary_tiling_data.get("n"), self.binary_tiling_data.get("n")])
            compute.get_var("k").set_bound([self.binary_tiling_data.get("k"), self.binary_tiling_data.get("k")])
            compute.get_var("k_ori").set_bound([self.binary_tiling_data.get("k_ori"),
                                                self.binary_tiling_data.get("k_ori")])
            compute.get_var("m_ori").set_bound([self.binary_tiling_data.get("m_ori"),
                                                self.binary_tiling_data.get("m_ori")])
            compute.get_var("n_ori").set_bound([self.binary_tiling_data.get("n_ori"),
                                                self.binary_tiling_data.get("n_ori")])
            if compute_param.pad_flag:
                compute.get_var("m_pad").set_bound([self.binary_tiling_data.get("m_pad"),
                                                    self.binary_tiling_data.get("m_pad")])
                compute.get_var("n_pad").set_bound([self.binary_tiling_data.get("n_pad"),
                                                    self.binary_tiling_data.get("n_pad")])
                compute.get_var("k_pad").set_bound([self.binary_tiling_data.get("k_pad"),
                                                    self.binary_tiling_data.get("k_pad")])
            if self.batch_var is not None:
                compute.get_var("batch").set_bound([self.binary_tiling_data.get("batch"),
                                                    self.binary_tiling_data.get("batch")])
                batch_broadcast_var_list = ["batch_a1", "batch_a2", "batch_a3", "batch_a4",
                                            "batch_b1", "batch_b2", "batch_b3", "batch_b4",
                                            "batch_c1", "batch_c2", "batch_c3", "batch_c4"]
                for var_name in batch_broadcast_var_list:
                    batch_broadcast_var = get_optional_te_var(var_name)
                    var_value = self.binary_tiling_data.get(var_name)
                    compute.get_var(var_name).set_bound([var_value, var_value])
        else:
            var_range_dict = self._init_var_range_dict(compute_param)
            # set constant
            for var, var_range in var_range_dict.items():
                self.sch.set_var_range(cache_tiling_mgr.cache_tiling.get(var), *var_range)
            if not cache_tiling_mgr.non_factor_bmn_flag and not cache_tiling_mgr.unaligned_flag:
                if self.batch_var is not None and cache_tiling_mgr.batch_expr is not None:
                    self.sch.set_var_value(self.batch_var, cache_tiling_mgr.batch_expr)
                self.sch.set_var_value(self.m_var, cache_tiling_mgr.m_expr)
                self.sch.set_var_value(self.n_var, cache_tiling_mgr.n_expr)
