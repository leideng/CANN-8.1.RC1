#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d backprop input general schedule.
"""
from enum import Enum
from functools import reduce
from itertools import product
from typing import Iterable
import collections

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.common.context import op_context
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.tiling import tiling_api
from tbe.common.utils import log
from tbe.common.utils.const import SPLIT_AXIS_MODE_STR
from tbe.common.utils.const import SplitAxisMode
from tbe.common.utils.const import IS_CONV1D_SITUATION_STR
from tbe.common.utils.const import QUANT_DTYPES
from tbe.common.utils.errormgr import error_manager_cube
from tbe.common.utils.errormgr import error_manager_util
from tbe.common.utils.op_util.op_util_cube import decode_tiling
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.boost_schedule_kit import Compare
from tbe.dsl.boost_schedule_kit import ScheduleAgent
from tbe.dsl.boost_schedule_kit import SplitParam
from tbe.dsl.compute import cube_util
from tbe.dsl.compute.cube_util import BIT_RATIO_DICT
from tbe.dsl.compute.conv2d_backprop_input_compute import DeconvParam
from tbe.dsl.compute.conv2d_backprop_input_general_compute import DeConvPattern
from tbe.dsl.static_schedule.conv_schedule_util import ceil_div
from tbe.dsl.static_schedule.conv_schedule_util import clear_suffix
from tbe.dsl.static_schedule.conv_schedule_util import search_op
from tbe.dsl.static_schedule.conv_util import DxDynamicUtil
from tbe.dsl.static_schedule.conv_util import FIXPIPE_SCOPE_MAP
from tbe.dsl.static_schedule.conv_util import fetch_elewise_fusion_ub_info
from tbe.dsl.static_schedule.conv_util import fetch_fixpipe_tensor
from tbe.dsl.static_schedule.conv_util import fetch_relugard_fusion_ub_info
from tbe.dsl.static_schedule.conv_util import fetch_requant_fusion_ub_info
from tbe.dsl.static_schedule.conv_util import get_all_tensors
from tbe.dsl.static_schedule.conv_util import get_c_add_bias_tensor
from tbe.dsl.static_schedule.conv_util import get_inout_dtype
from tbe.dsl.static_schedule.conv_util import get_mix_ratio
from tbe.dsl.static_schedule.conv_util import get_tensor_workspace
from tbe.dsl.static_schedule.conv_util import print_ir_conv
from tbe.dsl.static_schedule.conv_util import set_intrinsic_support
from tbe.dsl.static_schedule.conv_util import update_info_dict
from tbe.dsl.static_schedule.util import L1CommonParam
from tbe.dsl.static_schedule.util import align
from tbe.dsl.static_schedule.util import ceil
from tbe.dsl.static_schedule.util import get_fixpipe_emit_str
from tbe.dsl.static_schedule.util import INTRINSIC_FIXPIPE_UNIT_LIST
from tbe.dsl.static_schedule.util import parse_tbe_compile_para
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import OriNode
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import collect_attrs_from_compute_graph
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import calc_padding_for_auto_tiling
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import is_valid_tensor_attr
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import is_valid_tensor_map
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import is_valid_tiling
from tbe.dsl.unify_schedule.conv2d_bp_input_tilingcase import post_process_get_tiling


class ScopeDDR(Enum):
    DDR = 0
    L1 = 1


class L1FusionType(Enum):
    DISABLE = -1
    L1_WIDTH_FUSION = 0
    L1_BREADTH_FUSION = 1


# Don't modify, used in log_util
DX_SUPPORT_TAG_LOG_PREFIX = "#Conv2DBackpropInput only support#"
UINT32_MAX = 2 ^ 32 - 1
OUTPUT_WITH_4D_SHAPE = ('5HD_TO_4D_DYN', '5HD_TO_NHWC_FP')

FUSION_SUPPORT_LIST = ["dx + requant", "dx + (dequant) + elemwise + (quant)"]

SPARSE_4TO2_K_RATIO = 2

isa_support_l0c_to_ub = {
    tbe_platform_info.scope_gm: [tbe_platform_info.scope_cbuf,
                                 tbe_platform_info.scope_ubuf],
    tbe_platform_info.scope_cbuf: [tbe_platform_info.scope_ca,
                                   tbe_platform_info.scope_cb,
                                   tbe_platform_info.scope_ubuf],
    tbe_platform_info.scope_cb: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_ca: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_cc: [tbe_platform_info.scope_ubuf],
    tbe_platform_info.scope_ubuf: [tbe_platform_info.scope_ubuf,
                                   tbe_platform_info.scope_cbuf,
                                   tbe_platform_info.scope_gm,
                                   tbe_platform_info.scope_cc]}

isa_support_l0c_to_out = {
    tbe_platform_info.scope_gm: [tbe_platform_info.scope_cbuf],
    tbe_platform_info.scope_cbuf: [tbe_platform_info.scope_ca,
                                   tbe_platform_info.scope_cb,
                                   tbe_platform_info.scope_bt],
    tbe_platform_info.scope_cb: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_ca: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_bt: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_cc: [tbe_platform_info.scope_gm]}


isa_support_l0c_to_out_with_ub = {
    tbe_platform_info.scope_gm: [tbe_platform_info.scope_cbuf,
                                 tbe_platform_info.scope_ubuf],
    tbe_platform_info.scope_cbuf: [tbe_platform_info.scope_ca,
                                   tbe_platform_info.scope_cb,
                                   tbe_platform_info.scope_bt],
    tbe_platform_info.scope_cb: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_ca: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_bt: [tbe_platform_info.scope_cc],
    tbe_platform_info.scope_cc: [tbe_platform_info.scope_gm,
                                 tbe_platform_info.scope_ubuf],
    tbe_platform_info.scope_ubuf: [tbe_platform_info.scope_ubuf,
                                   tbe_platform_info.scope_cbuf,
                                   tbe_platform_info.scope_gm]}


def bfs_path(inputs, start, end):
    # constraint: inputs has cycles
    prevs = {}

    def bfs_valid_path():
        searched = []
        deque = collections.deque([start])

        layer = 0
        while len(deque) != 0:
            size = len(deque)
            layer += 1
            for _ in range(size):
                curr_node = deque.popleft()
                if curr_node not in searched:
                    searched.append(curr_node)

                    for next_node in inputs[curr_node]:
                        prevs[(next_node, layer+1)] = (curr_node, layer)
                        if next_node == end:
                            return (next_node, layer+1)
                        deque.append(next_node)
        return None

    def backtrace_path():
        path = []
        curr_node = last_node_layer
        while True:
            if curr_node not in prevs:
                error_manager_cube.raise_err_message_cube(
                    f"failed to backtrace path because curr_node({curr_node}) has no previous node")
            prev_node = prevs[curr_node]
            node, layer = prev_node
            if node != start:
                path.insert(0, node)
            else:
                return path
            curr_node = prev_node
    last_node_layer = bfs_valid_path()
    if last_node_layer:
        return backtrace_path()
    return []


def set_scope(og, sch, tensor_map, tensor_attr):
    def short_scope(scope):
        return {
            tbe_platform_info.scope_cbuf: "l1",
            tbe_platform_info.scope_ubuf: "ub",
            tbe_platform_info.scope_gm: "gm",
        }.get(scope, "")

    if tensor_attr['support_l0c_to_out']:
        if tensor_attr['support_ub_to_l1']:
            isa = isa_support_l0c_to_out_with_ub
        else:
            isa = isa_support_l0c_to_out
    else:
        isa = isa_support_l0c_to_ub

    if og.root.scope != tbe_platform_info.scope_gm:
        log.debug(f'{og.root.scope} -> {tbe_platform_info.scope_gm}')
        if tbe_platform_info.scope_gm not in isa[og.root.scope]:
            error_manager_cube.raise_err_message_cube(
                f"Not support scope from {og.root.scope} -> {tbe_platform_info.scope_gm}")
        tensor = sch.cache_write(og.root.tensor, og.root.scope)
        # TAKS alias 会不会有用到的场景
        node = OriNode(tensor.name, tensor.op.tag, tensor)
        node.scope = og.root.scope
        node.input_in_same_scope = og.root.input_in_same_scope
        og.root.scope = tbe_platform_info.scope_gm
        og.root.input_in_same_scope = False
        og.insert_before(node, og.root)

    record_insert_after = []
    for dst, srcs in og.inputs.items():
        for src in srcs:
            if src in dst.not_verify_correctness_of_path:
                continue

            log.debug(f'verify connection {repr(src)} -> {repr(dst)}')
            if dst.input_in_same_scope:
                if src.scope != dst.scope:
                    if dst.scope in isa[src.scope]:
                        tensor = sch.cache_read(src.tensor, dst.scope, [dst.tensor])
                        if src.alias is not None:
                            idx_underscore = src.alias.find('_')
                            name = src.alias[:idx_underscore] if idx_underscore != -1 else src.alias
                        else:
                            name = src.name
                        key = f"{name}_{short_scope(dst.scope)}"
                        node = OriNode(key, tensor.op.tag, tensor)
                        node.scope = dst.scope
                        record_insert_after.append((node, src))
                        tensor_map[key] = tensor
                    else:
                        error_manager_cube.raise_err_message_cube(f"current isa path cannot support scope({src.scope})")
            else:
                dst_scope = dst.scope
                src_scope = dst.scope if dst.input_in_same_scope else src.scope

                # compute_inline tensor set scope to None
                if dst_scope is None or src_scope is None:
                    continue

                if dst_scope not in isa[src_scope]:
                    log.debug(f'create edge {src}, {src_scope} -> {dst} {dst_scope}')
                    path = bfs_path(isa, src_scope, dst_scope)
                    if len(path) != 1:
                        error_manager_cube.raise_err_message_cube(
                            f"Not support {src}({src_scope}) -> {dst}({dst_scope}) yet {str(og)}")
                    # NOTE 不支持多路的
                    tensor = sch.cache_read(src.tensor, path[0], [dst.tensor])
                    # NOTE 需要确保alias是name+scope形式
                    name = src.alias[:src.alias.find('_')]
                    scope = path[0]
                    key = f"{name}_{short_scope(scope)}"
                    node = OriNode(key, tensor.op.tag, tensor)
                    node.scope = path[0]
                    record_insert_after.append((node, src))
                    tensor_map[key] = tensor
                    log.debug(f'add {tensor} to tensor_map with key {key}')
    for node, dst in record_insert_after:
        log.debug(f'insert {node} after {dst}')
        og.insert_after(node, dst)

    for node in og.inputs.keys():
        # compute_inline tensor set scope to None
        if node.scope is None:
            continue
        log.debug(f'set scope of {node.tensor} to {node.scope}')
        sch[node.tensor].set_scope(node.scope)


def _process_ub_reuse(ub_tensor_list, base_ub, dx_res, sch, len_align):
    """
    reuse the ub tensor in ub fusion
    """
    reuse_tensor_list = []
    # the output of dx_res is upper 1, which can not reuse
    next_tensor = [ub_tensor for ub_tensor in ub_tensor_list if dx_res in ub_tensor.op.input_tensors]
    # only the ub tensor has one output can be reused
    while len(next_tensor) == 1:
        reuse_tensor_list.append(next_tensor[0])
        next_tensor = [ub_tensor for ub_tensor in ub_tensor_list if next_tensor[0] in ub_tensor.op.input_tensors]
    for reuse_tensor in reuse_tensor_list:
        sch[base_ub].reused_by(reuse_tensor)
        if len_align is not None:
            sch[reuse_tensor].bind_buffer(reuse_tensor.op.axis[1], len_align, 0)


def _raise_dx_general_err(msg):
    """
    In op Conv2DBackpropInput_general, [%s] % (msg)
    msg for discribe the error info
    the error info only for Conv2DBackpropInput_general's developers
    """
    args_dict = {"errCode": "E60108", "reason": msg}
    msg = error_manager_util.get_error_message(args_dict)
    raise RuntimeError(args_dict, msg)


def _lcm(param1, param2):
    """
    calculate least common multiple
    """
    temp = param1 * param2
    while param1 % param2 != 0:
        param1, param2 = param2, param1 % param2
    return temp // param2


def _do_preload(sch, tensors, tiling, preload):
    al1_pbuffer = tiling.get("manual_pingpong_buffer").get("AL1_pbuffer")
    l0c_pbuffer = tiling.get("manual_pingpong_buffer").get("CL0_pbuffer")

    # allocate_at conflicts preload
    if tiling['A_overhead_opt_flag'] == 0 and (preload['a_l1'] and al1_pbuffer == 2):
        sch[tensors['a_l1']].preload()
    if preload['c_col'] and l0c_pbuffer == 2:
        sch[tensors['c_col']].preload()


def set_output_mem(fusion_para, c_ddr, sch):
    out_mem = fusion_para.get("output_memory_type")
    if out_mem == "fuse_flag":
        if c_ddr.op.tag == "conv_virtual_res":
            for out_member in c_ddr.op.input_tensors:
                out_member_addr = out_member
                if out_member.dtype in ("float16", "bfloat16"):
                    out_member_addr = out_member.op.input_tensors[0]
                res_addr_type = 0
                if "addr_type" in out_member_addr.op.attrs:
                    res_addr_type = out_member_addr.op.attrs["addr_type"].value
                output_memory_type = [res_addr_type]
                if res_addr_type == 1:
                    sch[out_member].set_scope(tbe_platform_info.scope_cbuf_fusion)
        else:
            if "addr_type" in c_ddr.op.attrs:
                res_addr_type = c_ddr.op.attrs["addr_type"].value
            else:
                res_addr_type = 0
            output_memory_type = [res_addr_type]
            if res_addr_type == 1:
                sch[c_ddr].set_scope(tbe_platform_info.scope_cbuf_fusion)
    else:
        if out_mem == 1:
            sch[c_ddr].set_scope(tbe_platform_info.scope_cbuf_fusion)
        output_memory_type = [out_mem]

    return output_memory_type


def _get_deconv_out(c_ddr, tensor_attr, double_out_tensor):
    if c_ddr.op.tag == "conv_virtual_res":
        if "QUANT_DOUBLE_OUT" in c_ddr.op.attrs:
            tensor_attr["QUANT_DOUBLE_OUT"] = True
        double_out_tensor.append(c_ddr.op.input_tensors[0])
        double_out_tensor.append(c_ddr.op.input_tensors[1])
        deconv_res = c_ddr.op.input_tensors[0]
    else:
        deconv_res = c_ddr

    return deconv_res, tensor_attr


def calc_group(tensor_attr, tensor_map):
    g_extend = tensor_attr.get('group_dict')[cube_util.GroupDictKeys.g_extend]
    ci1g = tensor_attr.get('group_dict')[cube_util.GroupDictKeys.ci1g]
    co1g = tensor_attr.get('group_dict')[cube_util.GroupDictKeys.co1g]
    co1g_factor = 2
    if ('a_col' not in tensor_map or tensor_map["a_col"].dtype != "float32"):
        co1g_factor = 1
    return g_extend, ci1g, co1g, co1g_factor


def checkout_quant_fusion(deconv_res, tensor_attr, tensor_map, all_tensor, sch):
    if deconv_res.op.tag == "quant":
        tensor_attr["q_mode"] = tensor_attr["round_mode"]
    for key, value in all_tensor.items():
        key = clear_suffix(key)
        if key in ("dequant", "dequant1", "fixpipe_reform"):
            tensor_map["c_ub"] = value
            tensor_attr["quant_fuse"] = True
            if len(value.op.input_tensors) > 1:
                tensor_map["deq"] = value.op.input_tensors[1]
                tensor_attr["deq_vector"] = ("vector" in value.op.tag)
        elif key == "dequant_remove_pad" and deconv_res.op.tag != value.op.tag:
            sch[value].compute_inline()
    return tensor_attr.get("quant_fuse", False)


def check_dx_quant_fusion(deconv_res, tensor_attr, tensor_map, all_tensor):
    """
    Pattern:
        C - c_ub - (bias_add_vector) - c_ddr - input_ub - reform_by_vmuls - offset_ub - cast_i8_ub - deconv_res
                            |
                      (tensor_bias)
    """
    if not tensor_attr.get("QUANT_DOUBLE_OUT"):
        return False
    if deconv_res.op.tag == "quant":
        tensor_attr["q_mode"] = tensor_attr["round_mode"]
    for key, value in all_tensor.items():
        if key == "c_ddr":
            if value.op.input_tensors[0].op.name in ("bias_add_vector", "c_ub"):
                tensor_attr["quant_fuse"] = True
                if len(value.op.input_tensors[0].op.input_tensors) > 1:
                    tensor_map["bias_add_vector"] = value.op.input_tensors[0]
                    if tensor_map["bias_add_vector"].op.input_tensors[0].name == "c_ub":
                        tensor_map["c_ub"] = tensor_map["bias_add_vector"].op.input_tensors[0]
                else:
                    tensor_map["c_ub"] = value.op.input_tensors[0]
    return tensor_attr.get("quant_fuse", False)


def _get_fusion_type(a_ddr, deconv_res, dyn_util, tensor_attr):
    fusion_type = 0
    # the single deconv fusion is 1 for fp16, 2 for int8 or int4
    if deconv_res.op.tag == "conv2d_backprop_input":
        if a_ddr.dtype in QUANT_DTYPES:
            fusion_type = 2
        else:
            fusion_type = 1
    # deonv+add+drelu fusion type is 4, deonv+drelu fusion type is 8
    elif (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
            (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
        if "elewise_binary_add" in deconv_res.op.input_tensors[1].op.tag:
            fusion_type = 4
        else:
            fusion_type = 8
    # deconv+requant is 7
    elif deconv_res.op.tag == "requant_remove_pad":
        fusion_type = 7
    # dx + quant
    elif DeconvParam.dx_multioutput_flag and tensor_attr.get("quant_fuse") and \
         (deconv_res.op.tag == "quant"):
        fusion_type = 9
    # deconv+dequant is 5 and quant is 6
    elif tensor_attr.get("quant_fuse"):
        if deconv_res.op.tag == "quant":
            fusion_type = 6
        else:
            fusion_type = 5
    # deconv + relu is 3
    elif "elewise" in deconv_res.op.tag:
        fusion_type = 3
    return fusion_type


def _fetch_quant_info(deconv_res, all_tensor, sch, leaf_tensor, tensor_map):
    ub_list = []
    input_cache_buffer = []
    if deconv_res.op.tag not in ("quant", "dequant_remove_pad"):
        c_ub_res = sch.cache_write(deconv_res, tbe_platform_info.scope_ubuf)
        ub_list.append(c_ub_res)
    for key, value in all_tensor.items():
        if key == "input_ub":
            if value.op.attrs["c_out"].value % 2:
                ub_list.append(value)
            else:
                sch[value].compute_inline()
        elif value.op.input_tensors and clear_suffix(key) not in ("dequant_remove_pad", "dequant1", "c_ddr",
                                                                  "dequant", "fixpipe_reform", "fixpipe"):
            ub_list.append(value)
        elif not value.op.input_tensors and ("dequant" not in leaf_tensor.get(key).op.name and
                                             "fixpipe" not in leaf_tensor.get(key).op.name):
            if leaf_tensor.get(key).op.tag == deconv_res.op.tag:
                input_cache_buffer.append([value, c_ub_res])
            else:
                input_cache_buffer.append([value, leaf_tensor.get(key)])
    tensor_map["input_tensor"] = input_cache_buffer
    tensor_map["ub_list"] = ub_list


def _fetch_dx_quant_info(all_tensor, sch, tensor_map):
    """
    Pattern:
        C - c_ub - (bias_add_vector - c_ddr - input_ub - reform_by_vmuls - offset_ub - cast_i8_ub - deconv_res
                            |
                      (tensor_bias)
    After inline:
                       (tensor_bias)
                            |
        C -> c_ub - (bias_add_vector) - reform_by_vmuls - offset_ub - cast_i8_ub -> deconv_res(res)
                            |
                            ->  c_ddr(res_out_fp16)
    """
    ub_list = []
    ub_list.append(tensor_map["c_ub"])
    for key, value in all_tensor.items():
        if key == "input_ub":
            sch[value].compute_inline()
            sch[value.op.input_tensors[0]].compute_inline()
        elif value.op.input_tensors and key not in ("c_ddr"):
            ub_list.append(value)
    tensor_map["ub_list"] = ub_list
    tensor_map["input_tensor"] = []


def _set_filter_shape(tensor_map, tensor_attr):
    b_l1 = tensor_map['b_l1']
    b_ddr = tensor_map['b_ddr']
    deconv_res = tensor_map['deconv_res']
    g_extend = tensor_attr['group_dict']['g_extend']
    co1g = tensor_attr['group_dict']['dy_c1_extend']
    ci1g = tensor_attr['group_dict']['dx_c1_extend']
    kernel_h = tensor_attr['kernel_h']
    kernel_w = tensor_attr['kernel_w']
    channel_merge_ratio = tensor_attr.get("channel_merge_ratio")

    if tensor_attr.get("WEIGHT_NHWC_TRANS_FZ"):
        weight_fz_tensor = b_l1
    else:
        weight_fz_tensor = b_ddr
    if b_ddr.dtype in QUANT_DTYPES:
        # GCout1HkWk, Cin1, Cin0, Cout0
        _, _, b_ddr_n0, b_ddr_k0 = list(i.value for i in weight_fz_tensor.shape)
    else:
        # GCin1, HkWk, Cout1, Cout0, Cin0 for WEIGHT_NHWC_TRANS_FZ and the dtype of b_ddr is fp32
        # GCin1HkWk, Cout1, Cout0, Cin0 for other cases
        *_, b_ddr_k0, b_ddr_n0 = cube_util.shape_to_list(weight_fz_tensor.shape)
    # G, Cout, Cin1, Hk, Wk, Cin0
    filter_shape_g = [co1g * b_ddr_k0, ci1g, kernel_h, kernel_w, b_ddr_n0]
    if tensor_attr.get("sparse_4to2_flag"):
        filter_shape_g[0] = ceil(co1g, SPARSE_4TO2_K_RATIO) * b_ddr_k0
    l0c_multi_group_flag = False
    if channel_merge_ratio is not None:
        if ci1g % channel_merge_ratio != 0 and g_extend > 1:
            l0c_multi_group_flag = True
        else:
            filter_shape_g[1] = align(filter_shape_g[1], channel_merge_ratio)
    tensor_attr['l0c_multi_group_flag'] = l0c_multi_group_flag
    return filter_shape_g


def construct_ub_list_and_input_tensor_list(og, tensor_map):
    # ub_list: ub tensor after c_ddr
    # input_tensor_list: cache_read tensors
    searched_tensor_dx_gm = False
    for layer in og.iterate_by_layer():
        if searched_tensor_dx_gm:
            break

        for node in layer:
            if node.alias == 'tensor_dx_gm':
                searched_tensor_dx_gm = True
                continue
            if node.scope != tbe_platform_info.scope_ubuf:
                continue

            if node.input_in_same_scope:
                if 'ub_list' in tensor_map:
                    tensor_map['ub_list'].append(node.tensor)
                else:
                    tensor_map['ub_list'] = [node.tensor]
            else:
                if 'input_tensor_list' in tensor_map:
                    tensor_map['input_tensor_list'].append(node.tensor)
                else:
                    tensor_map['input_tensor_list'] = [node.tensor]


def _get_fm_5_hd_shape(tensor_attr, tensor_map):
    if (tensor_attr.get("FM_NHWC_TRANS_5HD") or tensor_attr.get("trans_5HD_fusion")):
        return tensor_map['a_l1'].shape
    return tensor_map['a_ddr'].shape


def remove_name_suffix_num(name):
    """
    remove the suffix num in names
    """
    if not isinstance(name, str):
        err_man.raise_err_specific_input_shape("conv2dTranpose", "{} is not str".format(name))
    name_arr = name.split("_")
    if name_arr[-1].isdigit():
        return "_".join(name_arr[0:-1])
    return "_".join(name_arr)


def is_support_fixpipe():
    return tbe_platform_info.intrinsic_check_support(INTRINSIC_FIXPIPE_UNIT_LIST)


def check_dx_quant_dualout(outs):
    """
    Check whether it is dual output situation
    """
    if not isinstance(outs, list) or len(outs) != 2:
        return False

    out_0, out_1 = outs
    if not isinstance(out_0, tvm.Tensor) or \
        not isinstance(out_1, tvm.Tensor):
        return False

    for tensor in outs:
        if search_op(tensor, "conv2d_backprop_input") is None:
            return False
        else:
            break

    scenes_double_out = \
        [{"out_0": {"tag": "conv2d_backprop_input", "dtype": "float16"},
          "out_1": {"tag": "quant", "dtype": "int8"}},
         {"out_0": {"tag": "conv2d_backprop_input", "dtype": "float16"},
          "out_1": {"tag": "fixpipe_reform", "dtype": "int8"}},
         {"out_0": {"tag": "fixpipe_reform", "dtype": "float16"},
          "out_1": {"tag": "fixpipe_reform", "dtype": "int8"}},
         {"out_0": {"tag": "fixpipe_reform", "dtype": "int8"},
          "out_1": {"tag": "fixpipe_reform", "dtype": "int8"}}]

    for scene in scenes_double_out:
        out_0_info = scene.get("out_0", {})
        out_1_info = scene.get("out_1", {})
        tag_check_flag = out_0.op.tag == out_0_info.get("tag", "") and \
                         out_1.op.tag == out_1_info.get("tag", "")
        dtype_check_flag = out_0.dtype == out_0_info.get("dtype", "") and \
                           out_1.dtype == out_1_info.get("dtype", "")

        if tag_check_flag and dtype_check_flag:
            return True

    return False


def reget_dx_multioutput(outs):
    """
    add a virtual node to connect double outs for tvm coding rule
    """
    if check_dx_quant_dualout(outs):
        out_0, out_1 = outs

        if not is_support_fixpipe():
            out_0 = tvm.compute(out_0.shape, lambda *indice: out_0(*indice),
                                name="res_out_fp16", tag="res_out_fp16")

        c0_dict = {"float16": 16, "int8": 32, "int4": 64}
        out_0_c0 = c0_dict.get(out_0.dtype)
        out_1_c0 = c0_dict.get(out_1.dtype)

        attrs = {"QUANT_DOUBLE_OUT": True}
        shape_dim_3d = 3
        shape_dim_4d = 4
        shape_dim_5d = 5

        if (len(out_0.shape) == shape_dim_4d) and (len(out_1.shape) == shape_dim_3d):
            virtual_res = tvm.compute(out_1.shape,
                                      lambda b, hw_idx, c0_idx:
                                      out_1(b, hw_idx, c0_idx) +
                                      out_0(b, c0_idx // out_0_c0, hw_idx, c0_idx % out_0_c0),
                                      name="conv_virtual_res", tag="conv_virtual_res",
                                      attrs=attrs)
        elif len(out_1.shape) == shape_dim_3d:
            virtual_res = tvm.compute(out_1.shape,
                                      lambda *idx:
                                      out_1(*idx) + out_0(*idx),
                                      name="conv_virtual_res", tag="conv_virtual_res",
                                      attrs=attrs)
        elif len(out_1.shape) == shape_dim_4d:
            virtual_res = tvm.compute(out_1.shape,
                                      lambda i, j, k, l:
                                      out_1(i, j, k, l) +
                                      out_0(i, (j * out_1_c0 + l) // out_0_c0, k,
                                               (j * out_1_c0 + l) % out_0_c0),
                                      name="conv_virtual_res", tag="conv_virtual_res",
                                      attrs=attrs)
        elif len(out_1.shape) == shape_dim_5d:
            virtual_res = tvm.compute(out_1.shape,
                                      lambda i, j, h, w, l:
                                      out_1(i, j, h, w, l) +
                                      out_0(i, (j * out_1_c0 + l) // out_0_c0, h, w,
                                               (j * out_1_c0 + l) % out_0_c0),
                                      name="conv_virtual_res", tag="conv_virtual_res",
                                      attrs=attrs)
        else:
            _raise_dx_general_err("double out" "invalid double out shape")

        outs = [virtual_res, out_0, out_1]
        DeconvParam.dx_multioutput_flag = True
    return outs


def check_dx_quantfuse_doubleout(tensor_list, ori_tensor_list, sch):
    """
    checkout if dx + quant double out or out

    Parameters
    ----------
    tensor_list: the tensor of output

    sch: tvm.schedule, schedule to build or to print lower code

    Returns
    -------
    tensor_list
    """
    if is_support_fixpipe():
        DeconvParam.dx_multioutput_flag = False
        return ori_tensor_list

    if not DeconvParam.dx_multioutput_flag:
        return ori_tensor_list

    tensor_list = tensor_list[:-2]
    tensor_list.append(sch.cce_special['real_out_tensor'][1])
    tensor_list.append(sch.cce_special['real_out_tensor'][2])
    DeconvParam.dx_multioutput_flag = False
    for tensor in tensor_list:
        if "conv_virtual_res" in remove_name_suffix_num(tensor.op.name):
            tensor_list.remove(tensor)
    return tensor_list


def general_schedule(tensor, sch_list, tiling_case=None, var_range=None):
    """
    auto_schedule for cce AI-CORE.
    For now, only one convolution operation is supported.

    Parameters
    ----------
    res : tvm.tensor

    sch_list: use sch_list[0] to return conv schedule

    tiling_case: fix tiling for dynamic shape

    var_range: var_range for dynamic shape

    Returns
    -------
    True for sucess, False for no schedule
    """
    def _fetch_tensor_info(sch, tensor_attr, dyn_util):

        def _fill_tensor_map(c_col, dyn_util, tensor_map, tensor_attr):
            a_col = c_col.op.input_tensors[0]  # im2col_fractal in L0A
            b_col = c_col.op.input_tensors[1]  # weight_transform in L0B
            b_l1 = b_col.op.input_tensors[0]  # weight in ddr
            tensor_attr["no_ub_and_dma_copy_flag"] = a_col.op.tag == "dy_col_no_ub_dma"
            tensor_attr["sparse_4to2_flag"] = (bool)("sparse_4to2_flag" in b_col.op.attrs and
                                                     b_col.op.attrs["sparse_4to2_flag"])
            if tensor_attr.get("sparse_4to2_flag"):
                compress_index = b_col.op.input_tensors[1]
                compress_index_l1 = sch.cache_read(compress_index, tbe_platform_info.scope_cbuf, [b_col])
                tensor_map["compress_index_ddr"] = compress_index
                tensor_map["compress_index_l1"] = compress_index_l1

            if b_l1.op.input_tensors:
                b_ddr = b_l1.op.input_tensors[0]
                sch[b_l1].set_scope(tbe_platform_info.scope_cbuf)
                if "NHWC_trans_FZ" in b_l1.op.tag:
                    tensor_attr["WEIGHT_NHWC_TRANS_FZ"] = True
            else:
                b_ddr = b_l1

            # ub stride expand does not support int4
            tensor_attr["support_stride_expand_in_ub"] = not tensor_attr.get(
                "support_l0c_to_out") and not b_ddr.dtype == "int4"
            if dyn_util.dynamic_mode:
                a_col_before = a_col
                kernel_h = dyn_util.shape_vars.get("kernel_h")
                kernel_w = dyn_util.shape_vars.get("kernel_w")
                if kernel_h is None:
                    kernel_h, kernel_w = (int(a_col.op.attrs["kernel_h"]), int(a_col.op.attrs["kernel_w"]))
                tensor_attr["kernel_h"] = kernel_h
                tensor_attr["kernel_w"] = kernel_w
            elif tensor_attr.get("no_ub_and_dma_copy_flag"):
                # without ub and dma_copy scence, ddr -> L1(fractal) ->L0,
                # only dy_ddr and dy_fractal tensor, l0a_dma_flag and other attr in a_col tensor
                a_col_before = a_col
            else:
                # im2col_row_major in L1
                a_col_before = a_col.op.input_tensors[0]

            padding = tensor_attr["padding"]
            dilations = tensor_attr["dilation"]

            tensor_map["c_col"] = c_col
            tensor_map["a_col"] = a_col
            tensor_map["a_col_before"] = a_col_before
            tensor_map["b_col"] = b_col
            tensor_map["b_ddr"] = b_ddr
            tensor_attr["padding"] = padding
            tensor_attr["dilations"] = dilations

            def _fill_a_tensormap_dynamic():
                a_l1 = a_col_before.op.input_tensors[0]
                sch[a_l1].set_scope(tbe_platform_info.scope_cbuf)
                after_ddr = a_l1
                stride_h, stride_w = tensor_attr['stride_h'], tensor_attr['stride_w']
                if isinstance(stride_h, tvm.Var):
                    tensor_attr["need_expand_stride"] = True
                else:
                    stride_h, stride_w = cube_util.shape_to_list([stride_h, stride_w])
                    tensor_attr["need_expand_stride"] = stride_h > 1 or stride_w > 1
                dyn_util.stride_expand = tensor_attr["need_expand_stride"]

                if tensor_attr.get("need_expand_stride"):
                    stride_scope = tbe_platform_info.scope_ubuf
                    if tensor_attr.get("support_out_to_l1_nd_to_nz"):
                        stride_scope = tbe_platform_info.scope_cbuf
                    dy_vn = a_l1.op.input_tensors[0]
                    a_zero = dy_vn.op.input_tensors[0]
                    a_filling = dy_vn.op.input_tensors[1]
                    sch[a_zero].set_scope(stride_scope)
                    sch[dy_vn].set_scope(stride_scope)
                    sch[a_filling].set_scope(stride_scope)
                    tensor_map["a_filling"] = a_filling
                    tensor_map["dy_vn"] = dy_vn
                    tensor_map["a_zero"] = a_zero
                    after_ddr = a_filling

                tensor_attr['NCHW_TRANS_5HD'] = (after_ddr.op.input_tensors[0].op.tag == "NCHW_trans_5HD")
                tensor_attr['NHWC_to_5HD_fusion'] = (after_ddr.op.input_tensors[0].op.tag == "NHWC_to_5HD_fusion")
                tensor_attr["trans_5HD_fusion"] = (tensor_attr.get("NCHW_TRANS_5HD")
                                                   or tensor_attr.get("NHWC_to_5HD_fusion"))
                tensor_attr["FM_NHWC_TRANS_5HD"] = after_ddr.op.input_tensors[0].op.tag == "NHWC_trans_5HD"
                if after_ddr.op.input_tensors[0].op.tag == "dy_avg":
                    a_avg = after_ddr.op.input_tensors[0]
                    tensor_map["a_avg"] = a_avg
                    sch[a_avg].set_scope(tbe_platform_info.scope_ubuf)
                    a_ddr = a_avg.op.input_tensors[0]
                    a_ub = sch.cache_read(a_ddr, tbe_platform_info.scope_ubuf, [a_avg])
                    tensor_map["a_ub"] = a_ub
                elif tensor_attr.get("trans_5HD_fusion"):
                    a_ub_nc1hwc0 = a_l1.op.input_tensors[0]
                    a_ub_transpose = a_ub_nc1hwc0.op.input_tensors[0]
                    a_ub_reshape = a_ub_transpose.op.input_tensors[0]
                    a_ub_pad_vn = a_ub_reshape.op.input_tensors[0]
                    a_ub_padc = a_ub_pad_vn.op.input_tensors[0]
                    a_ub_zero = a_ub_pad_vn.op.input_tensors[1]
                    a_ddr = a_ub_padc.op.input_tensors[0]
                    sch[a_ub_transpose].set_scope(tbe_platform_info.scope_ubuf)
                    sch[a_ub_padc].set_scope(tbe_platform_info.scope_ubuf)
                    sch[a_ub_zero].set_scope(tbe_platform_info.scope_ubuf)
                    sch[a_ub_pad_vn].set_scope(tbe_platform_info.scope_ubuf)
                    sch[a_ub_reshape].set_scope(tbe_platform_info.scope_ubuf)
                    tensor_map['a_ub_transpose'] = a_ub_transpose
                    tensor_map['a_ub_padc'] = a_ub_padc
                    tensor_map['a_ub_zero'] = a_ub_zero
                    tensor_map['a_ub_pad_vn'] = a_ub_pad_vn
                    tensor_map['a_ub_reshape'] = a_ub_reshape
                    tensor_map['a_ub_nc1hwc0'] = a_ub_nc1hwc0
                    sch[a_ub_nc1hwc0].compute_inline()
                elif tensor_attr.get("FM_NHWC_TRANS_5HD"):
                    a_nc1hwc0_on_the_fly = after_ddr.op.input_tensors[0]
                    sch[a_nc1hwc0_on_the_fly].compute_inline()
                    a_ddr = a_nc1hwc0_on_the_fly.op.input_tensors[0]
                    tensor_map["a_nc1hwc0_on_the_fly"] = a_nc1hwc0_on_the_fly
                else:
                    a_ddr = after_ddr.op.input_tensors[0]

                tensor_map["a_l1"] = a_l1
                tensor_map["a_ddr"] = a_ddr

                if "a_avg" in tensor_map:
                    if a_avg.op.input_tensors[1].op.tag == "mean_matrix_rec":
                        mean_matrix_rec = a_avg.op.input_tensors[1]
                        tensor_map["mean_matrix_rec"] = mean_matrix_rec
                        sch[mean_matrix_rec].set_scope(tbe_platform_info.scope_ubuf)
                        mean_matrix_fp16 = mean_matrix_rec.op.input_tensors[0]
                    else:
                        mean_matrix_fp16 = a_avg.op.input_tensors[1]
                    tensor_map["mean_matrix_fp16"] = mean_matrix_fp16
                    sch[mean_matrix_fp16].set_scope(tbe_platform_info.scope_ubuf)
                    mean_matrix = mean_matrix_fp16.op.input_tensors[0]
                    tensor_map["mean_matrix"] = mean_matrix
                    sch[mean_matrix].set_scope(tbe_platform_info.scope_ubuf)

            def _fill_a_tensormap():
                def _fill_a_dilate(a_col_before):
                    if not tensor_attr.get("l0a_dma_flag"):
                        a_l1 = a_col_before.op.input_tensors[0]
                        sch[a_l1].set_scope(tbe_platform_info.scope_cbuf)
                    else:
                        # replace load3d by dma_copy, padding is in ub or in l1
                        a_l1 = a_col_before
                        if tensor_attr.get("no_ub_and_dma_copy_flag"):
                            # whithout ub and dma_copy scenes
                            sch[a_l1].set_scope(tbe_platform_info.scope_cbuf)
                            tensor_attr["dma_pad"] = list(i.value for i in a_col_before.op.attrs["dma_pad"])
                        else:
                            a_filling = a_col_before.op.input_tensors[0]
                            tensor_attr["dma_pad"] = list(i.value for i in a_filling.op.attrs["dma_pad"])
                            a_col_before = tensor_map["a_col"]
                        a_col = sch.cache_read(a_col_before, tbe_platform_info.scope_ca, [c_col])
                        tensor_map["a_col"] = a_col
                        tensor_map["a_col_before"] = a_col_before

                    if tensor_attr.get("no_ub_and_dma_copy_flag"):
                        a_zero = a_col_before.op.input_tensors[1]
                        tensor_map["a_zero"] = a_zero
                        stride_h, stride_w = tensor_attr['stride_h'], tensor_attr['stride_w']
                        a_ddr = a_col_before.op.input_tensors[0]
                    else:
                        a_filling = a_l1.op.input_tensors[0]
                        stride_h, stride_w = tensor_attr['stride_h'], tensor_attr['stride_w']
                        if stride_h > 1 or stride_w > 1:
                            # in l0a_dma_copy scenes pad!=0 and stride=1, there is no a_zero
                            a_zero = a_filling.op.input_tensors[1]  # dEdY_zero in ub
                            tensor_map["a_zero"] = a_zero
                        tensor_map["a_filling"] = a_filling
                        a_ddr = a_filling.op.input_tensors[0]
                    tensor_map["a_l1"] = a_l1

                    if tensor_attr.get("support_fixpipe") and ("NHWC_trans_5HD" in a_ddr.op.tag):
                        tensor_attr["support_stride_expand_in_ub"] = False
                    if not tensor_attr.get("support_stride_expand_in_ub"):
                        a_ub = None
                        if a_ddr.op.input_tensors:
                            if "NHWC_trans_5HD" in a_ddr.op.tag:
                                tensor_attr["FM_NHWC_TRANS_5HD"] = True
                            sch[a_ddr].compute_inline()
                            a_ddr = a_ddr.op.input_tensors[0]
                    elif input_mem == ScopeDDR.DDR and l1_fusion_type != L1FusionType.L1_BREADTH_FUSION:
                        a_ub = sch.cache_read(
                            a_ddr, tbe_platform_info.scope_ubuf, [a_filling]
                        )
                    elif input_mem == ScopeDDR.DDR:
                        a_ub = sch.cache_read(
                            a_ddr, tbe_platform_info.scope_ubuf, [a_filling]
                        )
                        sch[a_ddr].set_scope(tbe_platform_info.scope_cbuf_fusion)
                    elif l1_fusion_type == L1FusionType.L1_BREADTH_FUSION:
                        a_l1_full = sch.cache_read(
                            a_ddr, tbe_platform_info.scope_cbuf_fusion, [a_filling]
                        )
                        tensor_map["a_l1_full"] = a_l1_full
                        a_ub = sch.cache_read(
                            a_l1_full, tbe_platform_info.scope_ubuf, [a_filling]
                        )

                    a_zero_scope = tbe_platform_info.scope_cbuf
                    if tensor_attr.get("support_stride_expand_in_ub"):
                        a_zero_scope = tbe_platform_info.scope_ubuf
                    if not tensor_attr.get("no_ub_and_dma_copy_flag"):
                        sch[a_filling].set_scope(a_zero_scope)
                    if tensor_map.get('a_zero') is not None:
                        sch[tensor_map['a_zero']].set_scope(a_zero_scope)
                    tensor_map["a_ub"] = a_ub
                    if tensor_map.get("a_l1_full") is not None:
                        a_l1_full = tensor_map.get("a_l1_full")
                        al1_shape = a_l1_full.shape
                        sch[a_l1_full].buffer_align(
                            (1, 1),
                            (1, 1),
                            (al1_shape[2], al1_shape[2]),
                            (al1_shape[3], al1_shape[3]),
                            (1, 1)
                        )
                    tensor_map["a_ddr"] = a_ddr
                    tensor_attr["stride_h"] = stride_h
                    tensor_attr["stride_w"] = stride_w

                def _fill_a(a_col_before):
                    if a_col_before.op.input_tensors[0].op.tag == "dy_l1_modify":
                        a_l1 = a_col_before.op.input_tensors[0]
                        a_ddr = a_l1.op.input_tensors[0]
                        sch[a_l1].set_scope(tbe_platform_info.scope_cbuf)
                    elif tensor_attr.get("l0a_dma_flag"):
                        a_l1 = a_col_before
                        a_ddr = a_l1.op.input_tensors[0]
                        a_col_before = tensor_map["a_col"]
                        a_col = sch.cache_read(a_col_before, tbe_platform_info.scope_ca, [c_col])
                        tensor_map["a_col"] = a_col
                        tensor_map["a_col_before"] = a_col_before
                    else:
                        a_l1 = a_col_before.op.input_tensors[0]
                        if a_l1.op.input_tensors:
                            a_ddr = a_l1.op.input_tensors[0]
                            sch[a_l1].set_scope(tbe_platform_info.scope_cbuf)
                            if "NHWC_trans_5HD" in a_l1.op.tag:
                                tensor_attr["FM_NHWC_TRANS_5HD"] = True
                        else:
                            a_ddr = a_l1
                        if l1_fusion_type != L1FusionType.DISABLE:
                            a_l1 = sch.cache_read(
                                a_ddr, tbe_platform_info.scope_cbuf_fusion, [a_col_before]
                            )
                        else:
                            if not tensor_attr.get("FM_NHWC_TRANS_5HD"):
                                a_l1 = sch.cache_read(
                                    a_ddr, tbe_platform_info.scope_cbuf, [a_col_before]
                                )
                        if input_mem == ScopeDDR.L1:
                            sch[a_ddr].set_scope(tbe_platform_info.scope_cbuf_fusion)
                    tensor_map["a_l1"] = a_l1
                    if input_mem == ScopeDDR.L1 or l1_fusion_type == L1FusionType.L1_BREADTH_FUSION:
                        al1_shape = a_l1.shape
                        sch[a_l1].buffer_align(
                            (1, 1),
                            (1, 1),
                            (al1_shape[2], al1_shape[2]),
                            (al1_shape[3], al1_shape[3]),
                            (1, 1)
                        )
                    tensor_map["a_ddr"] = a_ddr
                    tensor_attr["stride_h"] = 1
                    tensor_attr["stride_w"] = 1

                al1_tag = a_col_before.op.input_tensors[0].op.tag

                # -----------------------------------------------------------------
                #     tag           | stride | pad | l0_dma_flag | tensor
                # -----------------------------------------------------------------
                #   "dy_l1"         |   >1   | >=0 |    false    | a_ddr->a_ub->a_filling->a_l1->load3d
                #   "dy_l1"         |   >1   |  <0 |    false    | a_ddr->a_ub->a_filling->a_filling_cut->load3d
                # "dy_l1_modify"    |   1    |  <0 |    false    | a_ddr->a_l1_cut->l0ad3d
                #     ""            |   1    | >=0 |    false    | a_ddr->(cache_read al1)->load3d
                #     ""            |   1    |  0  |    true     | a_ddr->a_col_before_a_col(dma_copy)
                # "dy_filling_dma"  |   >1   | all |    true     | a_ddr->a_ub->a_filling->a_col_before_a_col(dma_copy)
                #  "dy_pad_dma"     |   1    | !=0 |    true     | a_ddr->a_ub->a_filling->a_col_before_a_col(dma_copy)
                # "dy_col_no_ub_dma"|  >=1   | all |    true     | a_ddr->a_col_before_a_col(dma_copy)
                ub_tensor_tag = ["dy_l1", "ub_filling_dma", "ub_pad_dma"]
                tensor_attr["need_expand_stride"] = al1_tag in ub_tensor_tag

                if tensor_attr["need_expand_stride"] or tensor_attr.get("no_ub_and_dma_copy_flag"):
                    _fill_a_dilate(a_col_before)
                else:
                    _fill_a(a_col_before)

            if dyn_util.dynamic_mode:
                _fill_a_tensormap_dynamic()
            else:
                _fill_a_tensormap()

            if not b_l1.op.input_tensors:
                b_l1 = sch.cache_read(b_ddr, tbe_platform_info.scope_cbuf, [b_col])
            tensor_map["b_l1"] = b_l1
            # dataflow management
            a_col = tensor_map["a_col"]
            a_col_before = tensor_map["a_col_before"]
            sch[b_col].set_scope(tbe_platform_info.scope_cb)
            sch[a_col_before].set_scope(tbe_platform_info.scope_cbuf)
            sch[a_col].set_scope(tbe_platform_info.scope_ca)
            sch[c_col].set_scope(tbe_platform_info.scope_cc)
            if not tensor_attr.get("support_l0c_to_out"):
                sch[tensor_map.get("c_ub")].set_scope(tbe_platform_info.scope_ubuf)

            return tensor_map

        def _bias_tensor_setscope():
            # when add bias in ub
            bias_add_vector = tensor_map.get("bias_add_vector")
            if bias_add_vector is not None:
                sch[bias_add_vector].set_scope(tbe_platform_info.scope_ubuf)
                bias_tensor = bias_add_vector.op.input_tensors[1]
                bias_ub = sch.cache_read(
                    bias_tensor, tbe_platform_info.scope_ubuf, [bias_add_vector]
                )
                tensor_map["bias_ub"] = bias_ub

            # when add bias in l0c
            if tensor_map.get("c_add_bias") is not None:
                c_add_bias = tensor_map.get("c_add_bias")
                bias_l0c = tensor_map.get("bias_l0c")
                bias_ub_brc = tensor_map.get("bias_ub_brc")
                sch[c_add_bias].set_scope(tbe_platform_info.scope_cc)
                sch[bias_l0c].set_scope(tbe_platform_info.scope_cc)
                sch[bias_ub_brc].set_scope(tbe_platform_info.scope_ubuf)
            elif len(c_col.op.input_tensors) > 2 and tensor_attr.get("support_l1_to_bt"):
                bias_bt = c_col.op.input_tensors[2]
                tensor_map["bias_bt"] = bias_bt
                sch[bias_bt].set_scope('local.BT')
                bias_l1 = bias_bt.op.input_tensors[0]
                sch[bias_l1].set_scope(tbe_platform_info.scope_cbuf)
                #  平台要求bias要64B对齐
                if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                    sch[bias_l1].compute_align(bias_l1.op.axis[-1], 16)
                    sch[bias_l1].buffer_align((1, 16))
                    sch[bias_bt].compute_align(bias_bt.op.axis[-1], 16)
                    sch[bias_bt].buffer_align((1, 16))
                tensor_map["bias_l1"] = bias_l1
                tensor_attr["bias_need_align"] = bias_bt.op.tag == "bias_need_align"
                if tensor_attr["bias_need_align"]:
                    if (tbe_platform.platform_info.intrinsic_check_support("Intrinsic_set_l1") and
                        not tbe_platform.platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
                        bias_ub_align = bias_l1.op.input_tensors[0]
                        sch[bias_ub_align].set_scope(tbe_platform_info.scope_ubuf)
                        tensor_map["bias_ub_align"] = bias_ub_align
                    else:
                        bias_zero = bias_l1.op.input_tensors[1]
                        sch[bias_zero].set_scope(tbe_platform_info.scope_cbuf)
                        tensor_map["bias_zero"] = bias_zero

        def _bias_tensor():
            # quant must with bias
            c_add_bias = get_c_add_bias_tensor(tensor_map)
            if c_add_bias is not None:
                bias_l0c = c_add_bias.op.input_tensors[0]
                c_col = c_add_bias.op.input_tensors[1]
                bias_ub_brc = bias_l0c.op.input_tensors[0]
                tensor_bias = bias_ub_brc.op.input_tensors[0]
                bias_ub = sch.cache_read(tensor_bias, tbe_platform_info.scope_ubuf, [bias_ub_brc])
                tensor_map["c_add_bias"] = c_add_bias
                tensor_map["bias_l0c"] = bias_l0c
                tensor_map["bias_ub_brc"] = bias_ub_brc
                tensor_map["tensor_bias"] = tensor_bias
                tensor_map["bias_ub"] = bias_ub
            else:
                if tensor_attr.get("support_l0c_to_out"):
                    c_col = tensor_map.get("tensor_dx_gm").op.input_tensors[0]
                else:
                    c_col = c_ub.op.input_tensors[0]
            return c_col

        def _ubtensor_setscope(ub_tensor_list, input_tensor_list, fusion_num, deq_list):
            for ub_tensor in ub_tensor_list:
                sch[ub_tensor].set_scope(tbe_platform_info.scope_ubuf)
                if "dequant2" in ub_tensor.op.name:
                    deq_list.append(ub_tensor)
                if deconv_res.op.tag != "quant":
                    if "dequant" in ub_tensor.op.name:
                        fusion_num += 1
                    elif len(ub_tensor.op.input_tensors) > 1:
                        fusion_num += 1
                    fusion_num = min(2, fusion_num)

            input_list = []
            for input_tensor_mem in input_tensor_list:
                input_ub = sch.cache_read(input_tensor_mem[0], tbe_platform_info.scope_ubuf, input_tensor_mem[1])
                input_list.append(input_ub)
            tensor_map["input_tensor"] = input_list
            return fusion_num

        def _vadd_drelu_set_scope():
            """
            set scope of (vadd) + drelu fusion
            """
            fusion_param = 1 / 16
            if "elewise_binary_add" in deconv_res.op.input_tensors[1].op.tag:
                vadd_res = tensor_map.get("vadd_res")
                tensor_vadd = tensor_map.get("tensor_vadd")
                tensor_vadd_1 = tensor_map.get("tensor_vadd_1")
                tensor_inter_add_compute = tensor_map.get("tensor_inter_add_compute")
                fusion_param += 1
                sch[vadd_res].set_scope(tbe_platform_info.scope_ubuf)
                tensor_vadd_ub = sch.cache_read(tensor_vadd, tbe_platform_info.scope_ubuf, [vadd_res])
                tensor_map["tensor_vadd_ub"] = tensor_vadd_ub
                if tensor_vadd_1 is not None:
                    sch[tensor_inter_add_compute].set_scope(tbe_platform_info.scope_ubuf)
                    tensor_vadd_1_ub = sch.cache_read(tensor_vadd_1, tbe_platform_info.scope_ubuf,
                                                      [tensor_inter_add_compute])
                    fusion_param += 1
                    tensor_map["tensor_vadd_1_ub"] = tensor_vadd_1_ub
            if not tensor_attr.get("support_l0c_to_out"):
                sch[tensor_map.get("tensor_dx_gm")].compute_inline()
            if tensor_map.get("tensor_broadcast") is not None:
                sch[tensor_map.get("tensor_broadcast")].compute_inline()
                fusion_param += 1
            mask_ub = sch.cache_read(tensor_map.get("mask"), tbe_platform_info.scope_ubuf, [deconv_res])
            c_ub_drelu = sch.cache_write(deconv_res, tbe_platform_info.scope_ubuf)
            tensor_map["mask_ub"] = mask_ub
            tensor_map["c_ub_drelu"] = c_ub_drelu
            return fusion_param

        def _tensor_setscope():
            fusion_param = 0
            if (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                    (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
                fusion_param = _vadd_drelu_set_scope()
            elif "requant_remove_pad" in deconv_res.op.tag:
                deq = tensor_map.get("deq")
                deq_ub = sch.cache_read(deq, tbe_platform_info.scope_ubuf, tensor_map.get("c_ub"))
                tensor_map["deq"] = deq_ub
                sch[tensor_map.get("data_transfer")].set_scope(tbe_platform_info.scope_ubuf)
                sch[tensor_map.get("c_ub")].compute_inline()
                tensor_map["c_ub"] = tensor_map.get("data_transfer")
            elif DeconvParam.dx_multioutput_flag and tensor_attr.get("quant_fuse") and \
                 (deconv_res.op.tag == "quant"):
                ub_tensor_list = tensor_map.get("ub_list")
                for ub_tensor in ub_tensor_list:
                    sch[ub_tensor].set_scope(tbe_platform_info.scope_ubuf)
                    fusion_param += 1
                if "bias_add_vector" in tensor_map:
                    fusion_param += 0.125
            elif tensor_attr.get("quant_fuse"):
                deq_list = [tensor_map.get("c_ub")]
                ub_list = tensor_map.get("ub_list")
                input_tensor = tensor_map.get("input_tensor")
                if deconv_res.op.tag == "quant":
                    fusion_param = 4
                else:
                    fusion_param = 0
                fusion_param = _ubtensor_setscope(ub_list, input_tensor, fusion_param, deq_list)
                deq = tensor_map.get("deq")
                if deq is not None:
                    tensor_map["deq"] = sch.cache_read(deq, tbe_platform_info.scope_ubuf, deq_list)
                    fusion_param += 0.125
            elif "elewise" in deconv_res.op.tag:
                for ub_tensor in tensor_map.get("ub_list"):
                    if len(ub_tensor.op.input_tensors) > 1:
                        fusion_param += 1
                    sch[ub_tensor].set_scope(tbe_platform_info.scope_ubuf)
                fusion_param = min(2, fusion_param)
                input_list = []
                for input_tensor in tensor_map.get("input_tensor_list"):
                    input_ub = sch.cache_read(input_tensor[0], tbe_platform_info.scope_ubuf, input_tensor[1])
                    input_list.append(input_ub)
                tensor_map["input_tensor_list"] = input_list
                if "bias_add_vector" in tensor_map:
                    fusion_param += 0.125
                if not tensor_attr.get("support_l0c_to_out"):
                    sch[tensor_map.get("tensor_dx_gm")].compute_inline()
            if tensor_map.get("c_ub") is not None and tensor_attr.get("support_l0c_to_ub"):
                sch[tensor_map.get("c_ub")].set_scope(tbe_platform_info.scope_ubuf)
            return fusion_param

        def _fetch_ub_info(sch, tensor_map, tensor_attr):
            if deconv_res.op.tag == "requant_remove_pad":
                sch, tensor_map, tensor_attr = fetch_requant_fusion_ub_info(sch, tensor_map, tensor_attr)
            elif "quant" in deconv_res.op.tag or "elewise" in deconv_res.op.tag:
                # dx -> dequant/fixpipe + elewise + quant
                if checkout_quant_fusion(deconv_res, tensor_attr, tensor_map, all_tensor, sch):
                    _fetch_quant_info(deconv_res, all_tensor, sch, leaf_tensor, tensor_map)
                    inline_tensor = tensor_map.get("c_ub").op.input_tensors[0]
                    while inline_tensor.op.name not in ["C", "c_add_bias"]:
                        sch[inline_tensor].compute_inline()
                        inline_tensor = inline_tensor.op.input_tensors[0]

                # dx + quant
                if check_dx_quant_fusion(deconv_res, tensor_attr, tensor_map, all_tensor):
                    _fetch_dx_quant_info(all_tensor, sch, tensor_map)
                    inline_tensor = tensor_map.get("c_ub").op.input_tensors[0]
                    while inline_tensor.op.name not in ["C", "bias_add_vector"]:
                        sch[inline_tensor].compute_inline()
                        inline_tensor = inline_tensor.op.input_tensors[0]

            if tensor_attr.get("quant_fuse"):
                pass
            elif (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                  (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
                # dx -> (add/addn) -> relugrad
                sch, tensor_map = fetch_relugard_fusion_ub_info(sch, tensor_map, tensor_attr)
            elif "elewise" in deconv_res.op.tag:
                # dx -> elewise
                sch, tensor_map, tensor_attr = fetch_elewise_fusion_ub_info(sch, tensor_map, tensor_attr)
            elif tensor_attr.get("5HD_TO_4D_DYN"):
                c_ub_transpose = deconv_res.op.input_tensors[0]
                tensor_dx_gm = c_ub_transpose.op.input_tensors[0]
                c_ub = tensor_dx_gm.op.input_tensors[0]
                sch[c_ub_transpose].set_scope(tbe_platform_info.scope_ubuf)
                sch[tensor_dx_gm].compute_inline()
                tensor_map['c_ub'] = c_ub
                tensor_map['c_ub_transpose'] = c_ub_transpose
            else:
                _raise_dx_general_err(DX_SUPPORT_TAG_LOG_PREFIX + str(FUSION_SUPPORT_LIST))
            return sch, tensor_map, tensor_attr

        tensor_map = {}
        tensor_map["deconv_res"] = deconv_res
        tensor_map["double_out_tensor"] = double_out_tensor
        set_intrinsic_support(tensor_attr)
        collect_attrs_from_compute_graph(deconv_res, tensor_attr)
        all_tensor, leaf_tensor = get_all_tensors(deconv_res)
        if deconv_res.op.tag != "conv2d_backprop_input":
            # op has ub/fixpipe fusion
            sch, tensor_map = fetch_fixpipe_tensor(sch, all_tensor, tensor_map)
            # fixpipe can trans 5HD to N, HW, C whose shape lenth is 3
            tensor_attr["5HD_TO_NHWC_FP"] = tensor_map.get("fixpipe_tensor") is not None and \
                len(cube_util.shape_to_list(deconv_res.shape)) == 3
            # 5HD to 4D in dynamic
            tensor_attr["5HD_TO_NCHW_DYN"] = "5HD_TRANS_NCHW" in deconv_res.op.tag
            tensor_attr["5HD_TO_NHWC_DYN"] = "5HD_to_NHWC_fusion" in deconv_res.op.tag
            tensor_attr["5HD_TO_4D_DYN"] = tensor_attr.get("5HD_TO_NCHW_DYN") or tensor_attr.get("5HD_TO_NHWC_DYN")
            if deconv_res.dtype in QUANT_DTYPES:
                # int int8(int4) requant scenarios, N0 in L0C is 16, N0 in c_ddr is 32(64)
                tensor_attr["channel_merge_ratio"] = tbe_platform.CUBE_MKN.get(deconv_res.dtype)["mac"][1] // 16
            if deconv_res.op.tag == "fixpipe_reform":
                # fusion_type: dx -> fixpipe(ddr)
                c_ub = None
            else:
                # fusion_type: dx -> fixpipe/dequant/quant -> ub
                sch, tensor_map, tensor_attr = _fetch_ub_info(sch, tensor_map, tensor_attr)
                c_ub = tensor_map["c_ub"]
        else:
            # dx singal op
            if not tensor_attr.get("support_l0c_to_out"):
                # l0c -> ub -> ddr
                c_ub = deconv_res.op.input_tensors[0]
                if c_ub.op.name == "bias_add_vector":
                    tensor_map["bias_add_vector"] = c_ub
                    c_ub = c_ub.op.input_tensors[0]
            else:
                # l0c -> ddr
                c_ub = None
            tensor_map["c_ub"] = c_ub
            tensor_map["tensor_dx_gm"] = deconv_res

        c_col = _bias_tensor()
        tensor_map = _fill_tensor_map(c_col, dyn_util, tensor_map, tensor_attr)
        _bias_tensor_setscope()

        tensor_attr["fusion_param"] = _tensor_setscope()
        if not tensor_attr.get("support_l0c_to_ub") and tensor_map.get("c_ub") is not None:
            sch, tensor_map = get_tensor_workspace(sch, tensor_map, tensor_attr)
        return tensor_map, tensor_attr

    # check tiling and set default tiling
    def check_and_set_default_tiling(
        tiling, ab_tensor_type, bias_l1, filter_shape, l0c_multi_group_flag
    ):
        """
        check and set default tiling
        :param tiling:
        :param ab_tensor_type: atype and btype
        :param bias_l1:
        :param filter_shape:
        :return: default tiling
        """
        def _get_factors(val, val_max):
            """
            get the factor of val that smaller than val_max
            """
            factor_max = min(val, val_max)
            factors = []
            for m_fac in range(factor_max, 0, -1):
                if val % m_fac == 0:
                    factors.append(m_fac)
            return factors

        def _get_block_dim(batch, m, n, core_num):
            """
            Generate block dims for default tiling.
            """
            batch_dims = _get_factors(batch, core_num)
            n_dims = _get_factors(n, core_num)
            m_dims = _get_factors(m, core_num)
            block_dims = [batch_dims[0], 1, 1]
            if batch_dims[0] < core_num:
                n_dim = _get_factors(n, core_num // batch_dims[0])[0]
                m_dim = 1
                if n_dim * batch_dims[0] < core_num:
                    m_dim = _get_factors(m, core_num // (batch_dims[0] * n_dim))[0]
                block_dims = [batch_dims[0], n_dim, m_dim]

            if reduce(lambda x, y: x * y, block_dims) < core_num:
                for comb in product(batch_dims, n_dims, m_dims):
                    if reduce(lambda x, y: x * y, comb) == core_num:
                        block_dims = list(comb)
                        break

            return block_dims

        if not is_valid_tiling(tiling):
            tiling = {"default_tiling": True}
            _, cin1, k_h, k_w, _ = filter_shape
            bit_dir = {
                "float32": 8, "int32": 16, "float16": 16,
                "int8": 32, "int4": 64, "bfloat16": 16
            }
            atype, btype = ab_tensor_type
            if atype in bit_dir.keys():
                k_0 = bit_dir.get(atype)
                k_al1 = k_w * k_0
                if split_w:
                    k_al1 = k_h * k_al1
            else:
                # defaut value 32
                k_al1 = 32
                k_0 = 32

            if btype in bit_dir.keys():
                k_bl1 = k_0 * k_w
            else:
                # defaut value 32
                k_bl1 = 32
            if tensor_attr.get("need_expand_stride") and tensor_attr.get("support_stride_expand_in_ub"):
                tiling["AUB_shape"] = [k_w * bit_dir.get(atype), 1, 1, 1]
                tiling["BUB_shape"] = None
            else:
                tiling["AUB_shape"] = None
                tiling["BUB_shape"] = None

            n_l0 = 1
            group_l0 = 1
            # in int4 and int8 requant scenarios nl0 need to be a multiple of channel_merge_ratio
            if channel_merge_ratio is not None:
                n_l0 = channel_merge_ratio
                if l0c_multi_group_flag:
                    n_l0 = cin1
                    group_l0 = channel_merge_ratio

            core_num = tbe_platform_info.get_soc_spec("CORE_NUM")
            batch, _, fmap_h, _, _ = tensor_attr.get("output_shape")
            batch_dim, n_dim, m_dim = _get_block_dim(batch, fmap_h, max(cin1 // n_l0, 1), core_num)
            ka_factor = kb_factor = 1
            if tensor_attr.get("sparse_4to2_flag"):
                ka_factor *= SPARSE_4TO2_K_RATIO # ka shoule be 2 kb in sparse_4to2_scend
            tiling["AL1_shape"] = [k_al1 * ka_factor, 1, 1, 1]
            tiling["BL1_shape"] = [k_bl1 * kb_factor, 1, 1, 1]
            tiling["AL0_matrix"] = [1, ka_factor, 16, k_0, 1, 1]
            tiling["BL0_matrix"] = [kb_factor, n_l0, 16, k_0, 1, 1]
            tiling["CL0_matrix"] = [n_l0, 1, 16, 16, 1, group_l0]
            tiling["CUB_matrix"] = [n_l0, 1, 16, 16, 1, group_l0]
            tiling["block_dim"] = [batch_dim, n_dim, m_dim, 1]
            tiling["n_bef_batch_flag"] = 0
            tiling["n_bef_group_flag"] = 0
            tiling["batch_bef_group_fla"] = 0
            tiling["A_overhead_opt_flag"] = 0
            tiling["B_overhead_opt_flag"] = 0
            tiling["AUB_channel_wise_flag"] = None
            tiling["BUB_channel_wise_flag"] = None
            tiling["CUB_channel_wise_flag"] = None
            tiling["manual_pingpong_buffer"] = {
                "AUB_pbuffer": 1,
                "BUB_pbuffer": 1,
                "AL1_pbuffer": 1,
                "BL1_pbuffer": 1,
                "AL0_pbuffer": 1,
                "BL0_pbuffer": 1,
                "CL0_pbuffer": 1,
                "CUB_pbuffer": 1,
                "UBG_pbuffer": 1
            }

        # Default tiling load full kernel_w, may exceed l1 buffer in dma_copy scenes
        # fix al1_k1 and bl1_k1 by
        if tensor_attr.get("no_ub_and_dma_copy_flag") and tiling.get("default_tiling"):
            tiling_al0_m1, _, tiling_al0_m0, _, _, _ = tiling.get("AL0_matrix")
            _, tiling_bl0_n1, tiling_bl0_n0, _, _, _ = tiling.get("BL0_matrix")
            bias_reserve_byte = 32
            if bias_l1 is not None:
                bias_l1_size = tiling_bl0_n1 * tiling_bl0_n0 * BIT_RATIO_DICT.get(bias_l1.dtype) + bias_reserve_byte
            else:
                bias_l1_size = 0

            factor = k_w
            min_cout =  tbe_platform.CUBE_MKN.get(atype)["mac"][1]
            # get the max and available k_w factor
            while factor > 0:
                if k_w % factor == 0:
                    # filter_fractal and dy_fractal in l1
                    al1_size = factor * min_cout * tiling_al0_m1 * tiling_al0_m0 * BIT_RATIO_DICT.get(atype)
                    bl1_size = factor * min_cout * tiling_bl0_n1 * tiling_bl0_n0 * BIT_RATIO_DICT.get(btype)
                    if btype == "float32":
                        # need load 2*cin0 on L1
                        bl1_size *= 2
                    if al1_size + bl1_size + bias_l1_size <= tbe_platform_info.get_soc_spec("L1_SIZE"):
                        tiling.get("AL1_shape")[0] = factor * min_cout
                        tiling.get("BL1_shape")[0] = factor * min_cout
                        break
                factor -= 1

        return tiling

    def _get_tiling():
        a_ddr = tensor_map['a_ddr']
        b_ddr = tensor_map['b_ddr']
        bias_l1 = tensor_map.get('bias_l1')
        c_add_bias = tensor_map.get("c_add_bias")
        bias_add_vector = tensor_map.get("bias_add_vector")
        bias_bt = tensor_map.get("bias_bt")
        filter_shape_g = _set_filter_shape(tensor_map, tensor_attr)
        fused_double_operand_num = tensor_attr.get("fusion_param")
        _kernel_name = tensor_attr["kernel_name"]

        if not dyn_util.dynamic_mode:
            img_shape = cube_util.shape_to_list(_get_fm_5_hd_shape(tensor_attr, tensor_map))
            img_shape_g = [g_extend, img_shape[0], co1g * co1g_factor] + img_shape[2:]
            output_shape = tensor_attr.get("output_shape")
            output_shape_g = [g_extend, output_shape[0], ci1g] + output_shape[2:]
            bias_flag = int(c_add_bias is not None or bias_add_vector is not None or bias_bt is not None)

            fusion_type = _get_fusion_type(a_ddr, deconv_res, dyn_util, tensor_attr)
            info_dict = {
                "op_type": "conv2d_backprop_input",
                "A_shape": list(img_shape_g[1:]),
                "B_shape": list(filter_shape_g),
                "C_shape": list(output_shape_g[1:]),
                "A_dtype": str(tiling_dtype[0]),
                "B_dtype": str(tiling_dtype[1]),
                "C_dtype": str(tiling_dtype[2]),
                "mad_dtype": str(tiling_dtype[3]),
                "padl": padl,
                "padr": padr,
                "padu": padu,
                "padd": padd,
                "strideH": 1,
                "strideW": 1,
                "strideH_expand": stride_h,
                "strideW_expand": stride_w,
                "dilationH": dilation_h,
                "dilationW": dilation_w,
                "group": g_extend,
                "bias_flag": bias_flag,
                "fused_double_operand_num": fused_double_operand_num,
                "kernel_name": str(_kernel_name),
                "in_fm_memory_type": [input_mem.value],
                "out_fm_memory_type": out_mem,
                "l1_fusion_type": l1_fusion_type.value,
                "fusion_type": fusion_type,
                "general_flag": True,
                "split_axis_mode": tensor_attr.get(SPLIT_AXIS_MODE_STR, 0)
            }
            info_dict_special_params = {
                "sparse_4to2_flag": tensor_attr.get("sparse_4to2_flag")
            }
            update_info_dict(
                tensor_attr.get("support_l0c_to_out"), tensor_map.get("fixpipe_tensor"), bias_l1, info_dict,
                info_dict_special_params)
            tiling = tiling_api.get_tiling(info_dict)
            decode_tiling(tiling)
        else:
            tiling = tiling_case['tiling_strategy']
            decode_tiling(tiling)

        post_process_get_tiling(tiling, tensor_attr, tensor_map, dyn_util.dynamic_mode)

        tiling = check_and_set_default_tiling(tiling, (a_ddr.dtype, b_ddr.dtype), bias_l1,
                                              filter_shape_g, tensor_attr["l0c_multi_group_flag"])
        return tiling

    sch = sch_list[0]
    c_ddr = tensor
    tiling = None
    fusion_para = DeConvPattern.fusion_para_map
    double_out_tensor = []
    l1_fusion_type = L1FusionType(fusion_para.get("l1_fusion_type"))
    input_mem = ScopeDDR(fusion_para.get("input_memory_type"))

    if tiling_case is not None and 'tensor_attr' in tiling_case:
        log.debug("enter tiling from outer")
        tensor_attr = tiling_case['tensor_attr']
        tensor_map = tiling_case['tensor_map']
        og = tiling_case['og']
        tiling = tiling_case['tiling_strategy']
        tiling_data = tiling_case['tiling_data']
        channel_merge_ratio = None

        set_scope(og, sch, tensor_map, tensor_attr)
        construct_ub_list_and_input_tensor_list(og, tensor_map)
        if og.post_process_after_set_scope is not None:
            og.post_process_after_set_scope(og, tensor_map, tensor_attr)

        sub_map = {}
        for key, val in og.tensor_map.items():
            if isinstance(val, Iterable):
                sub_map[key] = [x.tensor for x in val]
            else:
                sub_map[key] = val.tensor
        tensor_map.update(sub_map)
        if 'compute_inline_tensors' in tensor_map:
            for tensor in tensor_map['compute_inline_tensors']:
                sch[tensor].compute_inline()

        dyn_util = DxDynamicUtil(tiling, var_range, tensor_attr)
        dyn_util.set_dynamic_scene()
        dyn_util.get_buffer_status()

        # special tiling flag
        preload_dict = {'c_col': False, 'a_l1': False}
        out_of_order = False

        g_extend, ci1g, co1g, co1g_factor = calc_group(tensor_attr, tensor_map)
        stride_h = tensor_attr["stride_h"]
        stride_w = tensor_attr["stride_w"]
        dilation_h, dilation_w = tensor_attr.get("dilation")
        split_w = tensor_attr["split_w"]
        kernel_h = tensor_attr.get("kernel_h")
        kernel_w = tensor_attr.get("kernel_w")
        padu, padd, padl, padr = calc_padding_for_auto_tiling(tensor_attr)
        tiling_dtype = get_inout_dtype(tensor_map["a_ddr"], tensor_map["b_ddr"], tensor_map["deconv_res"],
                                       "Conv2DBackpropInput")
    else:
        log.debug("enter tiling from inner")
        if tiling_case is not None:
            tiling = tiling_case['tiling_strategy']
        else:
            tiling = None
        out_mem = set_output_mem(fusion_para, c_ddr, sch)
        tensor_attr = {}
        deconv_res, tensor_attr = _get_deconv_out(c_ddr, tensor_attr, double_out_tensor)
        dyn_util = DxDynamicUtil(tiling, var_range)
        dyn_util.set_dynamic_scene()
        dyn_util.get_buffer_status()
        tensor_map, tensor_attr = _fetch_tensor_info(sch, tensor_attr, dyn_util)
        res, fail_key = is_valid_tensor_attr(tensor_attr)
        if not res:
            error_manager_cube.raise_err_message_cube(f"cannot find {fail_key} in tensor_attr")
        res, fail_key = is_valid_tensor_map(tensor_map)
        if not res:
            error_manager_cube.raise_err_message_cube(f"cannot find {fail_key} in tensor_map")

        g_extend, ci1g, co1g, co1g_factor = calc_group(tensor_attr, tensor_map)
        # get a_dtype, b_dtype, c_dtype, mmad_dtype
        tiling_dtype = get_inout_dtype(tensor_map["a_ddr"], tensor_map["b_ddr"], deconv_res, "Conv2DBackpropInput")
        padu, padd, padl, padr = calc_padding_for_auto_tiling(tensor_attr)
        stride_h = tensor_attr["stride_h"]
        stride_w = tensor_attr["stride_w"]
        dilation_h, dilation_w = tensor_attr.get("dilation")
        kernel_h = tensor_attr.get("kernel_h")
        kernel_w = tensor_attr.get("kernel_w")
        # int4 and int8 quantizations are only supported in static scene
        channel_merge_ratio = tensor_attr.get("channel_merge_ratio")
        # additional tensor_attr
        tensor_attr['split_w'] = tensor_attr.get(
            SPLIT_AXIS_MODE_STR, SplitAxisMode.split_hw.value) == SplitAxisMode.split_w.value
        split_w = tensor_attr['split_w']
        tiling = _get_tiling()

        # special tiling flag
        tbe_compile_param = tiling.get("tbe_compile_para")
        # when tbe_compile_param is None, sch.tbe_compile_para is None
        sch.tbe_compile_para, tbe_sch_control_para = parse_tbe_compile_para(tbe_compile_param)
        preload_dict = {'c_col': tbe_sch_control_para.get("preload"), 'a_l1': tbe_sch_control_para.get("preload_l1")}
        out_of_order = False
        if (sch.tbe_compile_para is not None) and (not tensor_attr.get("support_l0c_to_out")):
            out_of_order = sch.tbe_compile_para.get("out_of_order")

    # tensor
    ## single op case: Conv2DBackpropInput
    a_ddr = tensor_map.get("a_ddr")
    c_ub = tensor_map.get("c_ub")
    c_col = tensor_map.get("c_col")
    a_col = tensor_map.get("a_col")
    b_col = tensor_map.get("b_col")
    b_ddr = tensor_map.get("b_ddr")
    deconv_res = tensor_map.get("deconv_res")
    a_col_before = tensor_map.get("a_col_before")
    a_l1 = tensor_map.get("a_l1")
    a_filling = tensor_map.get("a_filling")
    dy_vn = tensor_map.get("dy_vn")
    a_zero = tensor_map.get("a_zero")
    b_l1 = tensor_map.get("b_l1")
    a_ub = tensor_map.get("a_ub")
    a_l1_full = tensor_map.get("a_l1_full")
    ## single op case: Conv2DTranspose
    bias_add_vector = tensor_map.get("bias_add_vector")
    bias_ub = tensor_map.get("bias_ub")
    c_add_bias = tensor_map.get("c_add_bias")
    bias_l0c = tensor_map.get("bias_l0c")
    bias_ub_brc = tensor_map.get("bias_ub_brc")
    bias_bt = tensor_map.get("bias_bt")
    bias_l1 = tensor_map.get("bias_l1")
    bias_ub_align = tensor_map.get("bias_ub_align")
    bias_zero = tensor_map.get("bias_zero")
    ## single_op: dynamic avgpoolgrad
    a_avg = tensor_map.get("a_avg")
    mean_matrix_fp16 = tensor_map.get("mean_matrix_fp16")
    mean_matrix = tensor_map.get("mean_matrix")
    mean_matrix_rec = tensor_map.get("mean_matrix_rec")
    ## ub_fusion
    vadd_res = tensor_map.get("vadd_res")
    ## ub_fusion: add_n(only support num_inputs is 2,3) + relugradv2
    c_ub_drelu = tensor_map.get("c_ub_drelu")
    vadd_res = tensor_map.get("vadd_res")
    tensor_vadd_ub = tensor_map.get("tensor_vadd_ub")
    tensor_vadd_1_ub = tensor_map.get("tensor_vadd_1_ub")
    tensor_inter_add_compute = tensor_map.get("tensor_inter_add_compute")
    mask_ub = tensor_map.get("mask_ub")
    ## ub_fusion: nc1hwc0 tran to nchw
    c_ub_transpose = tensor_map.get("c_ub_transpose")
    ## ub_fusion: nchw trans to nc1hwc0
    a_ub_padc = tensor_map.get("a_ub_padc")
    a_ub_transpose = tensor_map.get("a_ub_transpose")
    a_ub_zero = tensor_map.get("a_ub_zero")
    a_ub_pad_vn = tensor_map.get("a_ub_pad_vn")
    a_ub_reshape = tensor_map.get("a_ub_reshape")
    a_ub_nc1hwc0 = tensor_map.get("a_ub_nc1hwc0")

    # fusion params
    fmap_l1_addr_flag = fusion_para.get("fmap_l1_addr_flag")
    fmap_l1_valid_size = fusion_para.get("fmap_l1_valid_size")

    # attrs
    dyn_util.shape_vars["dy_c1_extend"] = co1g
    ho = tensor_attr["ho"]
    wo = tensor_attr["wo"]
    shape_y_nc1hwc0 = tensor_attr.get("output_shape")
    row_major_shape = tensor_attr.get("row_major_shape")
    _, _, hi, wi, _ = shape_y_nc1hwc0
    kernel_w_dilation = (kernel_w - 1) * dilation_w + 1
    sparse_4to2_flag = tensor_attr.get("sparse_4to2_flag")

    # conv1d situation
    def _check_conv1d_situation():
        if not dyn_util.dynamic_mode:
            return tensor_attr[IS_CONV1D_SITUATION_STR]
        elif dyn_util.dynamic_mode == 'binary':
            return dyn_util.tiling_case.get("conv1d_flag", False)
        return False
    is_conv1d_situation = _check_conv1d_situation()
    dyn_util.set_spec_var_range(sch, var_range, is_conv1d_situation)

    load3d_special_multiply = tensor_attr.get("load3d_special_multiply", 1)
    load3d_special_binary = 1
    if dyn_util.dynamic_mode == "binary":
        load3d_special_binary = get_te_var("load3d_special").get_tvm_var()
        sch.set_var_value(load3d_special_binary, dyn_util.tiling_case.get("load3d_special_flag"))

    # n, howo, c1, k_h, k_w, c0
    if not dyn_util.dynamic_mode:
        if not tensor_attr.get("l0a_dma_flag"):
            if split_w:
                _, _, wo_mad, _, kernel_h, kernel_w, _ = cube_util.shape_to_list(a_col_before.shape)
            else:
                _, howo_mad, _, kernel_h, kernel_w, _ = cube_util.shape_to_list(a_col_before.shape)
        elif tensor_attr.get("no_ub_and_dma_copy_flag"):
            _, howo_mad, _, kernel_h, kernel_w, _ = row_major_shape
        else:
            _, howo_mad, _, kernel_h, kernel_w, _ = cube_util.shape_to_list(a_l1.shape)
    else:
        kernel_h = tensor_attr.get("kernel_h")
        kernel_w = tensor_attr.get("kernel_w")
        kernel_w_dilation = (kernel_w - 1) * dilation_w + 1
    _, _, dx_h, dx_w, _ = tensor_attr['output_shape']
    shape_y_nc1hwc0_g = [g_extend, tensor_attr['output_shape'][0], ci1g] + tensor_attr['output_shape'][2:]

    img_shape = cube_util.shape_to_list(_get_fm_5_hd_shape(tensor_attr, tensor_map))
    dy_h, dy_w = img_shape[2:4]
    img_shape_g = [g_extend, img_shape[0], co1g * co1g_factor] + img_shape[2:]
    filter_shape_g = _set_filter_shape(tensor_map, tensor_attr)

    log.debug(f"general input tiling: {tiling}")

    def _tiling_check_none():
        if (
            (tiling.get("AL1_shape") is None)
            or (tiling.get("BL1_shape") is None)
            or (tiling.get("CUB_matrix") is None)
        ):
            _raise_dx_general_err("AL1_shape/BL1_shape/CUB_matrix " "can't be None.")
        if (
            (tiling.get("AL0_matrix") is None)
            or (tiling.get("BL0_matrix") is None)
            or (tiling.get("CL0_matrix") is None)
        ):
            _raise_dx_general_err("AL0_matrix/BL0_matrix/CL0_matrix " "can't be None.")

    def _tiling_l0_process():
        al0_full = (al0_tiling_ma == a_col_ma and al0_tiling_ka == a_col_ka and a_col_batch == 1 and g_extend == 1)
        if split_w:
            al0_full = (al0_full and a_col_h == 1)
        if al0_full:
            tiling["AL0_matrix"] = []
        bl0_tiling_g = 1
        if tiling.get("BL0_matrix") != []:
            (
                bl0_tiling_kb,
                bl0_tiling_nb,
                bl0_tiling_n0,
                bl0_tiling_k0,
                _,
                bl0_tiling_g
            ) = tiling.get("BL0_matrix")
        else:
            (
                _,
                bl0_tiling_kb,
                bl0_tiling_nb,
                bl0_tiling_n0,
                bl0_tiling_k0
            ) = list(i.value for i in b_col.shape)
        return bl0_tiling_kb, bl0_tiling_nb, bl0_tiling_n0, bl0_tiling_k0, bl0_tiling_g

    def _tiling_l1_process():
        al1_tiling_g = 1
        if tiling.get("AL1_shape") != []:
            al1_tiling_k, al1_tiling_m, _, al1_tiling_g = tiling.get("AL1_shape")
            if split_w:
                m_full_load_value = c_l0c_ho
            else:
                m_full_load_value = ceil(c_l0c_hw, tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][0] * cl0_tiling_mc)
            al1_full_load = (al1_tiling_k == kernel_h * kernel_w * co1g * co1g_factor * al1_co0
                             and al1_tiling_g == 1
                             and al1_tiling_m == m_full_load_value)
            if split_w:
                al1_full_load = al1_full_load and al0_tiling_ma == a_col_ma
            if al1_full_load:
                tiling["AL1_shape"] = []
        else:
            # batch and group is 1, other axes full load
            al1_tiling_k = kernel_h * kernel_w * co1g * co1g_factor * al1_co0
            if split_w:
                al1_tiling_m = 1 if tensor_attr['l0c_multi_group_flag'] else c_l0c_ho
            else:
                al1_tiling_m = 1
                if not tensor_attr['l0c_multi_group_flag']:
                    al1_tiling_m = ceil(c_l0c_hw, tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][0] * cl0_tiling_mc)

        bl1_tiling_g = 1
        if tiling.get("BL1_shape") != []:
            bl1_tiling_k, bl1_tiling_n, _, bl1_tiling_g = tiling.get("BL1_shape")
        else:
            if b_ddr.dtype in QUANT_DTYPES:
                # [G*Cout1*Hk*Wk, cin1, cin0, cout0]: bl1_co1, bl1_k1, _, bl1_co0
                bl1_tiling_k = bl1_co0 * bl1_co1 // g_extend
                bl1_tiling_n = 1 if tensor_attr['l0c_multi_group_flag'] else bl1_k1 // cl0_tiling_nc
            elif b_ddr.dtype == "float32":
                # [G*Cin1, Hk*Wk, cou1, cou0, cin0]: bl1_gcin1, bl1_hw, bl1_co1, bl1_co0, _
                bl1_tiling_k = kernel_h * kernel_w * bl1_co0 * bl1_co1
                bl1_tiling_n = (bl1_gcin1 * bl1_hw) // (kernel_h * kernel_w * cl0_tiling_nc * g_extend)
            else:
                # [G*Cin1*Hk*Wk, cou1, cou0, cin0]: bl1_k1, bl1_co1, bl1_co0, _
                bl1_tiling_k = kernel_h * kernel_w * bl1_co0 * bl1_co1
                bl1_tiling_n = bl1_k1 // (kernel_h * kernel_w * cl0_tiling_nc * g_extend)
        return al1_tiling_k, al1_tiling_m, al1_tiling_g, bl1_tiling_k, bl1_tiling_n, bl1_tiling_g

    # check tiling
    def _tiling_check_equal():
        if tiling.get("BL0_matrix") != [] and bl0_tiling_g == 1:
            if not sparse_4to2_flag and al0_tiling_ka != bl0_tiling_kb:
                _raise_dx_general_err("ka != kb.")
            if bl0_tiling_nb != cl0_tiling_nc:
                _raise_dx_general_err("nb != nc.")

        if al0_tiling_ma != cl0_tiling_mc:
            _raise_dx_general_err("ma != mc.")
        if tensor_attr['l0c_multi_group_flag']:
            quant_fusion_rule = (bl0_tiling_nb == ci1g and cl0_tiling_nc == ci1g and
                                 cub_tiling_nc_factor == ci1g and cl0_tiling_g == 2 and cub_tiling_g == 2)
            if not quant_fusion_rule:
                _raise_dx_general_err("illegal tiling in dequant + quant or requant fusion scene.")

    def _tiling_check_factor():
        if not sparse_4to2_flag:
            if (kernel_w * kernel_h * co1g * co1g_factor) % al0_tiling_ka != 0:
                _raise_dx_general_err("Co1*Hk*Wk % ka != 0")
            if al1_tiling_k % al0_tiling_ka != 0:
                _raise_dx_general_err("k_AL1 % ka != 0.")
            if bl1_tiling_k % bl0_tiling_kb != 0:
                _raise_dx_general_err("k_BL1 % kb != 0.")

        if (cl0_tiling_nc % cub_tiling_nc_factor != 0) and (not tensor_attr.get("support_l0c_to_out")):
            _raise_dx_general_err("nc % nc_factor != 0.")

        tiling_k_g = ((al1_tiling_k // al1_co0, al1_tiling_g), (bl1_tiling_k // al1_co0, bl1_tiling_g),
                      (al0_tiling_ka, al0_tiling_g), (bl0_tiling_kb, bl0_tiling_g))
        illegal_k_g = any([(k_g[0] != (kernel_w * kernel_h * co1g * co1g_factor) and
                          k_g[1] > 1) for k_g in tiling_k_g])
        if illegal_k_g:
            _raise_dx_general_err("Illegal tiling: If split k, factor of g in buffer must be 1")

    def _tiling_check_load():
        if tensor_attr.get("support_stride_expand_in_ub"):
            if not tensor_attr.get("need_expand_stride"):
                if tiling.get("AUB_shape") is not None:
                    _raise_dx_general_err("stride = 1 but AUB_shape is not None.")
            elif tiling.get("AUB_shape") is None:
                _raise_dx_general_err("stride > 1 but AUB_shape is None.")

        if tiling.get("BL0_matrix") == [] and tiling.get("BL1_shape") != [] and not tensor_attr['l0c_multi_group_flag']:
            _raise_dx_general_err("BL0 full load but BL1 not!")

    def _tiling_check_pbuffer():
        if stride_h > 1 or stride_w > 1:
            if aub_pbuffer not in (1, 2):
                _raise_dx_general_err("value of AUB_pbuffer can only be 1 or 2")

        if al1_pbuffer not in (1, 2):
            _raise_dx_general_err("value of AL1_pbuffer can only be 1 or 2")

        if bl1_pbuffer not in (1, 2):
            _raise_dx_general_err("value of BL1_pbuffer can only be 1 or 2")

        if al0_pbuffer not in (1, 2):
            _raise_dx_general_err("value of AL0_pbuffer can only be 1 or 2")

        if bl0_pbuffer not in (1, 2):
            _raise_dx_general_err("value of BL0_pbuffer can only be 1 or 2")

        if l0c_pbuffer not in (1, 2):
            _raise_dx_general_err("value of L0C_pbuffer can only be 1 or 2")

        if cub_pbuffer not in (1, 2):
            _raise_dx_general_err("value of CUB_pbuffer can only be 1 or 2")

    def _tiling_check_sparse_4to2():
        if not sparse_4to2_flag:
            return
        if tiling.get("AL0_matrix") == []:
            return
        if tiling.get("BL1_shape") is None:
            _raise_dx_general_err("BL1 matrix cannot be NOne in sparse 4to2 scene")
            # in quant dtypes, bl1_co1 equal G * Cout1 * Hk * Wk
        kb_full = bl1_co1
        ka = tiling.get("AL0_matrix")[1]
        kb = kb_full if tiling.get("BL0_matrix") == [] else tiling.get("BL0_matrix")[0]
        if ka == kb and kb == kb_full:
            return
        if ka != SPARSE_4TO2_K_RATIO * kb:
            _raise_dx_general_err("L0 ka should be 2kb in sparse 4to2 scene")



    def _full_load_flag():
        """
        check whether al1's m bl1's k and n is fully loaded

        Returns
        -------
        true for full loaded
        """
        # Check whether al1_tiling_m is fully loaded
        if split_w:
            al1_m_full_load = (al0_tiling_ma == a_col_ma and a_col_h == al1_tiling_m)
        else:
            al1_m_full_load = (al1_tiling_m == c_l0c_hw // (
                tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][0] * cl0_tiling_mc))

        # Check whether bl1_tiling_k is fully loaded
        bl1_k_full_load = False
        if b_ddr.dtype in QUANT_DTYPES and (bl1_tiling_k == bl1_co0 * bl1_co1):
            bl1_k_full_load = True
        elif bl1_tiling_k == kernel_h * kernel_w * bl1_co0 * bl1_co1:
            bl1_k_full_load = True

        bl1_n_full_load = False
        if b_ddr.dtype in QUANT_DTYPES and (bl1_tiling_n == bl1_k1 // cl0_tiling_nc):
            bl1_n_full_load = True
        elif b_ddr.dtype == "float32":
            if (bl1_tiling_n == (bl1_gcin1 * bl1_hw) // (kernel_h * kernel_w * cl0_tiling_nc)):
                bl1_n_full_load = True
        elif bl1_tiling_n == bl1_k1 // (kernel_h * kernel_w * cl0_tiling_nc):
            bl1_n_full_load = True

        return al1_m_full_load, bl1_k_full_load, bl1_n_full_load

    def _check_overload_dy():
        """
        check whether dy is overload
        Use the following conditions to judge:
        1. if multi core in n axis, dy will overload
        2. if split al1's and bl1's k, and al1_k < bl1_k
        3. if stride < kernel and spilt al1's m
        4. if spilt n axis
        Returns
        -------
        true for overload, false for not overload
        """

        if dyn_util.dynamic_mode == "binary":
            return dyn_util.tiling_case.get("A_overhead_opt_flag")

        al1_m_full_load, bl1_k_full_load, bl1_n_full_load = _full_load_flag()
        _, block_dim_n, _, _ = tiling.get("block_dim")

        if block_dim_n > 1:
            return True

        if (stride_h < kernel_h or stride_w < kernel_w) and (not al1_m_full_load):
            return True

        if ((not bl1_k_full_load) and (al1_tiling_k < bl1_tiling_k)) or (
            not bl1_n_full_load
        ):
            return True

        return False

    def _set_overload_flag(param, overload_flag, overload_axis):
        """
        set flag on the first axis
        """
        cache_read_mode = 0 if overload_flag else 1
        param.pragma(overload_axis, "json_info_cache_read_mode", cache_read_mode)

    _tiling_check_none()
    (
        cub_tiling_nc_factor,
        cub_tiling_mc_factor,
        cub_tiling_m0,
        cub_tiling_n0,
        _,
        cub_tiling_g
    ) = tiling.get("CUB_matrix")
    cl0_tiling_nc, cl0_tiling_mc, cl0_tiling_m0, cl0_tiling_n0, _, cl0_tiling_g = tiling.get(
        "CL0_matrix"
    )
    al0_tiling_ma, al0_tiling_ka, al0_tiling_m0, al0_tiling_k0, _, al0_tiling_g = tiling.get(
        "AL0_matrix"
    )

    batch_dim, n_dim, m_dim, group_dim = tiling.get("block_dim")
    aub_pbuffer = tiling.get("manual_pingpong_buffer").get("AUB_pbuffer")
    al1_pbuffer = tiling.get("manual_pingpong_buffer").get("AL1_pbuffer")
    bl1_pbuffer = tiling.get("manual_pingpong_buffer").get("BL1_pbuffer")
    al0_pbuffer = tiling.get("manual_pingpong_buffer").get("AL0_pbuffer")
    bl0_pbuffer = tiling.get("manual_pingpong_buffer").get("BL0_pbuffer")
    l0c_pbuffer = tiling.get("manual_pingpong_buffer").get("CL0_pbuffer")
    cub_pbuffer = tiling.get("manual_pingpong_buffer").get("CUB_pbuffer")
    a_col_shape = cube_util.shape_to_list(a_col.shape)
    al1_co0 = cube_util.shape_to_list(a_l1.shape)[-1]
    if split_w:
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            _, a_col_batch, a_col_h, a_col_ka, a_col_ma, _, _ = a_col_shape
        else:
            _, a_col_batch, a_col_h, a_col_ma, a_col_ka, _, _ = a_col_shape
        _, _, c_l0c_ho, _, c_l0c_wo, _ = cube_util.shape_to_list(c_col.shape)
    else:
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            _, a_col_batch, a_col_ka, a_col_ma, _, _ = a_col_shape
        else:
            _, a_col_batch, a_col_ma, a_col_ka, _, _ = a_col_shape
        c_l0c_hw = cube_util.shape_to_list(c_col.shape)[3]
    if b_ddr.dtype in QUANT_DTYPES:
        # G*Cout1*Hk*Wk, Cin1, Cin0, Cout0
        bl1_co1, bl1_k1, _, bl1_co0 = list(i.value for i in b_l1.shape)
    elif b_ddr.dtype == "float32":
        # G*Cin1, Hk*Wk, Cout1, Cout0, Cin0
        bl1_gcin1, bl1_hw, bl1_co1, bl1_co0, _ = cube_util.shape_to_list(b_l1.shape)
    else:
        # G*Cin1*Hk*Wk, Cout1, Cout0, Cin0
        bl1_k1, bl1_co1, bl1_co0, _ = cube_util.shape_to_list(b_l1.shape)
    if dyn_util.dynamic_mode == "binary":
        c_col_k1, c_col_k0 = list(ax.dom.extent for ax in c_col.op.reduce_axis)
    else:
        c_col_k1, c_col_k0 = list(ax.dom.extent.value for ax in c_col.op.reduce_axis)

    bl0_tiling_kb, bl0_tiling_nb, bl0_tiling_n0, bl0_tiling_k0, bl0_tiling_g = _tiling_l0_process()
    al1_tiling_k, al1_tiling_m, al1_tiling_g, bl1_tiling_k, bl1_tiling_n, bl1_tiling_g = _tiling_l1_process()
    if dyn_util.dynamic_mode != "binary":
        _tiling_check_equal()
        _tiling_check_factor()
        _tiling_check_load()
        _tiling_check_pbuffer()
        _tiling_check_sparse_4to2()

    def _fixpipe_op_process(fixpipe_tensor, vector_params, vector_tensors):
        for idx, params_mem in enumerate(vector_params):
            fixpipe_input = vector_tensors[idx]
            fixpipe_input_l1 = sch.cache_read(fixpipe_input, tbe_platform_info.scope_cbuf, [fixpipe_tensor])
            # in fixpipe eltwise scene, size of fixpipe_input_l1 and l0C tensor should be equal, which m is 16 aligned
            if params_mem == "eltwise_src":
                sch[fixpipe_input_l1].storage_align(fixpipe_input_l1.op.axis[-3], 256, 0)
                if fixpipe_input_l1.dtype == "float16" and c_ddr.dtype == "float16":
                    sch[fixpipe_input_l1].storage_align(fixpipe_input_l1.op.axis[-2], 16, 0)
            # fixp read from l1 need to be 128B aligned
            if params_mem == "relu_weight_0":
                sch[fixpipe_input_l1].storage_align(fixpipe_input_l1.op.axis[0], 32, 0)
            fixpipe_scope_name = FIXPIPE_SCOPE_MAP.get(params_mem)
            if fixpipe_scope_name:
                fixpipe_fb_tensor = sch.cache_read(fixpipe_input_l1, fixpipe_scope_name, [fixpipe_tensor])
                sch_agent.same_attach(fixpipe_fb_tensor, c_col)
                sch[fixpipe_fb_tensor].emit_insn(fixpipe_fb_tensor.op.axis[0], "dma_copy")
                # int32 to int8 (or int4), l0c is [1, hw, 16] ddr is [1, hw, 32](or [1, hw, 64])
                # the bound should n1*n0 in fbuf
                sch[fixpipe_fb_tensor].set_buffer_size(tiling.get("CL0_matrix")[0] * tiling.get("CL0_matrix")[3])
            sch_agent.same_attach(fixpipe_input_l1, c_col)
            sch[fixpipe_input_l1].emit_insn(fixpipe_input_l1.op.axis[0], "dma_copy")

    def _fixpipe_process():
        fixpipe_tensor = tensor_map.get("fixpipe_tensor")
        if fixpipe_tensor is None:
            return

        if len(double_out_tensor) == 2:
            for tensor in double_out_tensor:
                sch_agent.same_attach(tensor, c_col)
                if tensor.op.tag != "fixpipe_reform":
                    continue
                fixpipe_tensor = tensor.op.input_tensors[0]
                vector_params = fixpipe_tensor.op.attrs.get("vector_params", [])
                vector_tensors = fixpipe_tensor.op.attrs.get("vector_tensors", [])
                _fixpipe_op_process(fixpipe_tensor, vector_params, vector_tensors)
            return

        vector_params = tensor_attr.get("vector_params", [])
        vector_tensors = tensor_attr.get("vector_tensors", [])
        _fixpipe_op_process(fixpipe_tensor, vector_params, vector_tensors)

    def _cub_process():
        if tensor_map.get("c_ub") is None:
            return None

        def _attach_cub():
            # c_ub will attach on deconv_res in dynamic shape by default
            if not dyn_util.dynamic_mode:
                affine_cub[-2] = min(affine_cub[-2], op_shape[-2])
                status = Compare.compare(affine_cub[1:], op_shape)
            elif dyn_util.dynamic_mode == "binary":
                status = dyn_util.status.get("cub_status")
            else:
                status = Compare.LESS_EQ

            if status == Compare.EQUAL:
                pass
            elif status == Compare.LESS_EQ:
                sch_agent.attach_at(c_ub, c_ddr, affine_shape=affine_cub,
                                    ceil_mode_dict=dyn_util.get_ceil_mode(
                                        affine_cub, dyn_util.g_dim, dyn_util.ddr_gnm_dim))
            else:
                _raise_dx_general_err("c_ub attach error.")
            return status

        def _handle_dx_drelu_fusion_cub_process(c_ub):
            if "elewise_binary_add" in deconv_res.op.input_tensors[1].op.tag:
                sch_agent[c_ub].reused_by(vadd_res, c_ub_drelu)
                sch_agent.same_attach(vadd_res, c_ub)
                sch_agent.same_attach(tensor_vadd_ub, c_ub)
                sch_agent.same_attach(mask_ub, c_ub)
                if tensor_vadd_1_ub is not None:
                    sch_agent.same_attach(tensor_vadd_1_ub, c_ub)
                    sch_agent.same_attach(tensor_inter_add_compute, c_ub)
            else:
                sch_agent[c_ub].reused_by(c_ub_drelu)
                if not out_of_order:
                    sch_agent.same_attach(mask_ub, c_ub)
                # the bit must 128 align, so the m axis need 8 align
                if not tensor_attr.get("support_l0c_to_out") and not out_of_order:
                    n_dim = tiling.get("block_dim")[1]
                    mask_m = mask_ub.shape[-2]
                    mask_n = mask_ub.shape[-3]
                    if mask_m % 8 != 0 and mask_n % n_dim != 0:
                        sch[mask_ub].compute_align(mask_ub.op.axis[-2], 8)
                        sch[mask_ub].storage_align(mask_ub.op.axis[-3], 128, 0)
            sch_agent.same_attach(c_ub_drelu, c_ub)

        def _handle_dx_quant_fusion_cub_process(c_ub):
            if tensor_map.get("deq") is not None:
                sch_agent.same_attach(tensor_map.get("deq"), c_ub)
            for input_tensor_mem in tensor_map.get("input_tensor"):
                sch_agent.same_attach(input_tensor_mem, c_ub)
            for ub_tensor in tensor_map.get("ub_list"):
                if "broadcast" in ub_tensor.op.tag:
                    sch[ub_tensor].compute_inline()
                else:
                    sch_agent.same_attach(ub_tensor, c_ub)
            for double_out_tensor_mem in double_out_tensor:
                sch_agent.same_attach(double_out_tensor_mem, c_ub)

        def _fusion_cub_process():
            if (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                    (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
                _handle_dx_drelu_fusion_cub_process(c_ub)
            elif deconv_res.op.tag == "requant_remove_pad":
                sch_agent.same_attach(tensor_map.get("deq"), c_ub)
            elif tensor_attr.get("quant_fuse"):
                _handle_dx_quant_fusion_cub_process(c_ub)
            elif "elewise" in deconv_res.op.tag:
                for ub_tensor in tensor_map.get("ub_list"):
                    if "broadcast" in ub_tensor.op.tag:
                        sch[ub_tensor].compute_inline()
                    else:
                        sch_agent.same_attach(ub_tensor, c_ub)
                scope, unit = sch_agent[c_ddr].get_active_scope_and_unit()
                ax_m, len_axis = scope[-2], unit[-2]
                len_align = None
                if status != Compare.EQUAL:
                    len_align = tvm.min(len_axis, c_ub.shape[-2] - ax_m * len_axis) * 16
                _process_ub_reuse(tensor_map.get("ub_list"), c_ub, tensor_map.get("tensor_dx_gm"), sch, len_align)
                for input_tensor_mem in tensor_map.get("input_tensor_list"):
                    sch_agent.same_attach(input_tensor_mem, c_ub)
            elif tensor_attr.get("5HD_TO_4D_DYN"):
                sch_agent.same_attach(c_ub_transpose, c_ub)

        def _get_affine_cub_by_tensor_attr():
            if tensor_attr.get("5HD_TO_NCHW_DYN"):
                affine_cub = [
                    1, 1, c_ub_nc_factor * cub_tiling_n0,
                    cub_tiling_mc_factor * cub_tiling_m0 // load3d_special_multiply
                ]
            elif tensor_attr.get("5HD_TO_NHWC_DYN"):
                affine_cub = [
                    1, 1, cub_tiling_mc_factor * cub_tiling_m0 // load3d_special_multiply,
                    c_ub_nc_factor * cub_tiling_n0
                ]
            else:
                affine_cub = [
                    1, 1, c_ub_nc_factor, cub_tiling_mc_factor * cub_tiling_m0 // load3d_special_multiply, cub_tiling_n0
                ]
            return affine_cub

        c_ub_nc_factor = cub_tiling_nc_factor
        if (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
            dx_hw = hi * wi
            tiling_m_axis = cl0_tiling_mc * cl0_tiling_m0
            tail_dx_hw = dx_hw - dx_hw // tiling_m_axis * tiling_m_axis
            need_change_nc_factor = (tensor_attr.get("support_l0c_to_ub") and
                                     tail_dx_hw % 16 != 0) or (not tensor_attr.get("support_l0c_to_ub")
                                                               and align(dx_hw, cl0_tiling_m0) != tiling_m_axis)
            if need_change_nc_factor:
                c_ub_nc_factor = 1
            if split_w and (wi % cl0_tiling_mc != 0):
                c_ub_nc_factor = 1
            tiling.get("CUB_matrix")[0] = c_ub_nc_factor

        # dx_batch, dx_cin1, dx_m, dx_cin0
        op_shape = cube_util.shape_to_list(c_ub.shape)
        if split_w:
            _, _, c_col_h, _, c_col_w, _ = cube_util.shape_to_list(c_col.shape)
            op_shape = op_shape[0:2] + [c_col_h, c_col_w] + [op_shape[-1]]
        if channel_merge_ratio is not None:
            affine_cub = [
                1, 1, c_ub_nc_factor * cub_tiling_g // channel_merge_ratio,
                cub_tiling_mc_factor * cub_tiling_m0 // load3d_special_multiply,
                cub_tiling_n0 * channel_merge_ratio
            ]
            if deconv_res.op.tag == "quant":
                op_shape[1] = ceil(op_shape[1], channel_merge_ratio)
                op_shape[-1] = op_shape[-1] * channel_merge_ratio
        else:
            affine_cub = _get_affine_cub_by_tensor_attr()
        if split_w:
            affine_cub.insert(3, 1)
        status = _attach_cub()

        sch[c_ub].buffer_align(
            *([(1, 1)] * (len(c_ub.shape) - 2)),
            (1, tbe_platform.CUBE_MKN.get(c_ub.dtype)["mac"][0]),
            (1, tbe_platform.CUBE_MKN.get(c_ub.dtype)["mac"][2])
        )
        if bias_add_vector is not None:
            sch_agent[c_ub].reused_by(bias_add_vector)
            sch[bias_add_vector].buffer_align(
                *([(1, 1)] * (len(bias_add_vector.shape) - 2)),
                (1, tbe_platform.CUBE_MKN.get(c_ub.dtype)["mac"][0]),
                (1, tbe_platform.CUBE_MKN.get(c_ub.dtype)["mac"][2])
            )

        _fusion_cub_process()

        return affine_cub

    def _get_affine_l0c():
        if channel_merge_ratio is not None:
            if tensor_attr.get("5HD_TO_NHWC_FP") or tensor_attr.get("5HD_TO_NHWC_DYN"):
                affine_l0c = [1, 1, cl0_tiling_mc * cl0_tiling_m0 // load3d_special_multiply,
                              cl0_tiling_n0 * cl0_tiling_nc * cl0_tiling_g]
            else:
                affine_l0c = [1, 1, cl0_tiling_nc * cl0_tiling_g // channel_merge_ratio,
                              cl0_tiling_mc * cl0_tiling_m0 // load3d_special_multiply,
                              cl0_tiling_n0 * channel_merge_ratio]
        else:
            if tensor_attr.get("5HD_TO_NHWC_FP") or tensor_attr.get("5HD_TO_NHWC_DYN"):
                affine_l0c = [1, 1, cl0_tiling_mc * cl0_tiling_m0 // load3d_special_multiply,
                              cl0_tiling_n0 * cl0_tiling_nc]
            elif tensor_attr.get("5HD_TO_NCHW_DYN"):
                affine_l0c = [1, 1, cl0_tiling_n0 * cl0_tiling_nc,
                              cl0_tiling_mc * cl0_tiling_m0 // load3d_special_multiply]
            else:
                factor = 2 if c_ddr.dtype == "float32" and a_col.dtype == "float32" else 1
                affine_l0c = [1, 1, cl0_tiling_nc * factor, cl0_tiling_mc * cl0_tiling_m0 // load3d_special_multiply,
                              cl0_tiling_n0 // factor]
        if split_w:
            affine_l0c.insert(-2, 1)
        return affine_l0c

    def _cl0_process(affine_cub):
        affine_l0c = _get_affine_l0c()
        if c_ub is not None:
            c_col_shape = cube_util.shape_to_list(c_col.shape)

            # c_col will attach on c_ub or c_ddr in dynamic shape by default
            if dyn_util.dynamic_mode == "binary":
                status_ori = dyn_util.status.get("cl0_status_ori")
                status = dyn_util.status.get("cl0_status")
            else:
                if dyn_util.dynamic_mode:
                    status_ori = Compare.LESS_EQ
                else:
                    status_ori = Compare.compare(affine_l0c, c_col_shape)
                status = Compare.compare(affine_l0c, affine_cub)

            if status_ori == Compare.EQUAL:
                pass
            elif status == Compare.EQUAL:
                sch_agent.same_attach(c_col, c_ub)
            elif status == Compare.LESS_EQ:
                sch_agent.attach_at(c_col, c_ub, affine_shape=affine_l0c)
            elif status == Compare.GREATE_EQ:
                sch_agent.attach_at(c_col, c_ddr, affine_shape=affine_l0c,
                                    ceil_mode_dict=dyn_util.get_ceil_mode(
                                        affine_l0c, dyn_util.g_dim, dyn_util.ddr_gnm_dim))
            else:
                _raise_dx_general_err("c_col attach error.")
            if tensor_map.get("tensor_workspace_list"):
                sch_agent.same_attach(tensor_map.get("tensor_workspace_list")[0], c_col)
        else:
            sch_agent.attach_at(c_col, c_ddr, affine_shape=affine_l0c)
        drelu_mask_align = ((deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                            (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")) and
                            "conv2d_backprop_input" in deconv_res.op.input_tensors[1].op.tag and
                            out_of_order and tensor_attr.get("support_l0c_to_ub"))
        if drelu_mask_align:
            align_buffer = 0
            if (hi * wi) % tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][0] != 0:
                align_buffer = reduce(lambda x, y: x * y, tiling.get("CUB_matrix")[1:4])
                sch[mask_ub].bind_buffer(mask_ub.op.axis[1], align_buffer, 0)
            log.debug(f"mask_ub same_attach c_col, align_buffer:{align_buffer}")
            sch_agent.same_attach(mask_ub, c_col)
        sch[c_col].buffer_align(
            *([(1, 1)] * (len(c_col.shape) - 2)),
            (1, tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][0]),
            (1, tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][2]),
            (1, 1),
            (1, tbe_platform.CUBE_MKN.get(c_col.dtype)["mac"][1])
        )

    def _l0a_process():
        l0a2l0c_affine_shape = [
            1, 1, None, al0_tiling_ma * al0_tiling_m0 * load3d_special_binary,
            cl0_tiling_n0, al0_tiling_ka, al0_tiling_k0
        ]
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            tiling_ori_l0a = [
                1, 1, al0_tiling_ka, al0_tiling_ma, al0_tiling_m0, al0_tiling_k0
            ]
        else:
            tiling_ori_l0a = [
                1, 1, al0_tiling_ma, al0_tiling_ka, al0_tiling_m0, al0_tiling_k0
            ]
        l0a2out_affine_shape = [1, 1, None, al0_tiling_ma * al0_tiling_m0, cl0_tiling_n0]
        if tensor_attr.get("5HD_TO_NHWC_FP"):
            l0a2out_affine_shape = [1, 1, al0_tiling_ma * al0_tiling_m0, cl0_tiling_nc * cl0_tiling_n0]
        if split_w:
            l0a2l0c_affine_shape.insert(2, 1)  # calculate 1 * w
            tiling_ori_l0a.insert(2, 1)
            l0a2out_affine_shape.insert(-2, 1)
        # a_col will attach on c_col, c_ub or c_ddr in dynamic shape
        if dyn_util.dynamic_mode == "binary":
            status_ori = dyn_util.status.get("al0_status_ori")
            status = dyn_util.status.get("al0_status")
        else:
            if dyn_util.dynamic_mode:
                status_ori = Compare.LESS_EQ
            else:
                status_ori = Compare.compare(tiling_ori_l0a, a_col_shape)
            status = Compare.compare(
                [1, 1, al0_tiling_ma, al0_tiling_m0, al0_tiling_ka, al0_tiling_k0],
                [cl0_tiling_g, 1, cl0_tiling_mc, cl0_tiling_m0, c_col_k1, c_col_k0]
            )

        if status_ori == Compare.EQUAL:
            pass
        elif status == Compare.EQUAL:
            sch_agent.same_attach(a_col, c_col)
        elif status == Compare.LESS_EQ:
            sch_agent.attach_at(a_col, c_col, affine_shape=l0a2l0c_affine_shape,
                                ceil_mode_dict=dyn_util.get_ceil_mode(
                                    l0a2l0c_affine_shape, split_ceil_dim=dyn_util.cl0_mn0_dim))
        elif status == Compare.GREATE_EQ:
            sch_agent.attach_at(a_col, c_ddr, affine_shape=l0a2out_affine_shape)
        else:
            _raise_dx_general_err("l0a attach error.")

    def _l0b_process():
        neg_src_stride = True
        if tiling.get("BL0_matrix") != []:
            l0b2l0c_affine_shape = [
                1, None, bl0_tiling_nb, cl0_tiling_mc * cl0_tiling_m0 * load3d_special_binary,
                bl0_tiling_n0, bl0_tiling_kb, bl0_tiling_k0
            ]
            tiling_ori_l0b = [
                1, bl0_tiling_kb, bl0_tiling_nb, bl0_tiling_n0, bl0_tiling_k0
            ]
            l0b2out_affine_shape = [1, 1, bl0_tiling_nb, cl0_tiling_m0, bl0_tiling_n0]
            if tensor_attr.get("5HD_TO_NHWC_FP"):
                l0b2out_affine_shape = [1, 1, cl0_tiling_m0, bl0_tiling_n0 * bl0_tiling_nb]
            if split_w:
                l0b2l0c_affine_shape.insert(2, None)
                l0b2out_affine_shape.insert(-2, None)
            b_col_shape = cube_util.shape_to_list(b_col.shape)

            if dyn_util.dynamic_mode == "binary":
                status_ori = dyn_util.status.get("bl0_status_ori")
                status = dyn_util.status.get("bl0_status")
            else:
                status_ori = Compare.compare(tiling_ori_l0b, b_col_shape)
                status = Compare.compare(
                    [bl0_tiling_g, bl0_tiling_nb, bl0_tiling_n0, bl0_tiling_kb * kb_multi_factor, bl0_tiling_k0],
                    [cl0_tiling_g, cl0_tiling_nc, cl0_tiling_n0, c_col_k1, c_col_k0]
                )
            neg_src_stride = False
            if status_ori == Compare.EQUAL:
                neg_src_stride = True
            elif status == Compare.EQUAL:
                sch_agent.same_attach(b_col, c_col)
            elif status == Compare.LESS_EQ:
                sch_agent.attach_at(b_col, c_col, affine_shape=l0b2l0c_affine_shape,
                                    ceil_mode_dict=dyn_util.get_ceil_mode(l0b2l0c_affine_shape))
            elif status == Compare.GREATE_EQ:
                sch_agent.attach_at(b_col, c_ddr, affine_shape=l0b2out_affine_shape)
            else:
                _raise_dx_general_err("l0b attach error.")
        elif tensor_attr['l0c_multi_group_flag']:
            l0b2l0c_affine_shape = (
                1, None, bl1_tiling_n, cl0_tiling_mc * cl0_tiling_m0 * load3d_special_binary,
                bl0_tiling_n0, al0_tiling_ka, bl0_tiling_k0
            )
            sch_agent.attach_at(b_col, c_col, affine_shape=l0b2l0c_affine_shape)
        return neg_src_stride

    def _al1_process():
        l1_ma = al1_tiling_m * al0_tiling_ma
        l1_ka = al1_tiling_k // al0_tiling_k0
        l1a2l0c_affine_shape = [
            al1_tiling_g, 1, None, l1_ma * al0_tiling_m0 * load3d_special_binary,
            bl0_tiling_n0, l1_ka, al0_tiling_k0
        ]
        if split_w:
            l1a2l0c_affine_shape = [
                al1_tiling_g, 1, 1, None, al0_tiling_ma * al0_tiling_m0,
                bl0_tiling_n0, l1_ka, al0_tiling_k0
            ]

        def _get_attach_status():
            if dyn_util.dynamic_mode == "binary":
                return dyn_util.status.get("al1_status")

            if ("dedy_h" in dyn_util.shape_vars or "dedy_w" in dyn_util.shape_vars) and tiling.get("AL1_shape") == []:
                status = Compare.GREATE_EQ
            else:
                if split_w:
                    status = Compare.compare(
                        [al1_tiling_g, 1, al1_tiling_m, al0_tiling_m0, l1_ka, al0_tiling_k0],
                        [cl0_tiling_g, 1, 1, cl0_tiling_m0, c_col_k1, c_col_k0]
                    )
                else:
                    status = Compare.compare(
                        [al1_tiling_g, 1, l1_ma, al0_tiling_m0, l1_ka, al0_tiling_k0],
                        [cl0_tiling_g, 1, cl0_tiling_mc, cl0_tiling_m0, c_col_k1, c_col_k0]
                    )
            return status

        status = _get_attach_status()

        def _get_l1a2out_affine_shape():
            if tensor_attr.get("5HD_TO_NHWC_FP"):
                if split_w:
                    return [1, 1, al1_tiling_m, al0_tiling_ma * al0_tiling_m0, cl0_tiling_nc * cl0_tiling_n0]
                else:
                    return [1, 1, l1_ma * al0_tiling_m0, cl0_tiling_nc * cl0_tiling_n0]
            elif tensor_attr.get("5HD_TO_NCHW_DYN"):
                return [1, 1, cl0_tiling_nc * cl0_tiling_n0, l1_ma * al0_tiling_m0]
            elif tensor_attr.get("5HD_TO_NHWC_DYN"):
                return [1, 1, l1_ma * al0_tiling_m0, cl0_tiling_nc * cl0_tiling_n0]
            else:
                if split_w:
                    return [1, 1, None, al1_tiling_m, al0_tiling_ma * al0_tiling_m0, cl0_tiling_n0]
                else:
                    return [1, 1, None, l1_ma * al0_tiling_m0, cl0_tiling_n0]
        l1a2out_affine_shape = _get_l1a2out_affine_shape()

        attach_tensor = a_l1 if not tensor_attr.get("l0a_dma_flag") else a_col_before

        def _al1_attach_process():
            if ((tiling.get("AL1_shape") == [] and tiling.get("AL0_matrix") == []) or
                    (dyn_util.dynamic_mode == "binary" and dyn_util.attach_flag.get("al1_attach_flag") == 0)):
                pass
            elif status == Compare.EQUAL:
                sch_agent.same_attach(attach_tensor, c_col)
            elif status == Compare.LESS_EQ:
                sch_agent.attach_at(attach_tensor, c_col, affine_shape=l1a2l0c_affine_shape,
                                    ceil_mode_dict=dyn_util.get_ceil_mode(
                                        l1a2l0c_affine_shape, split_ceil_dim=dyn_util.cl0_mn0_dim))
            elif status == Compare.GREATE_EQ:
                sch_agent.attach_at(attach_tensor, c_ddr, affine_shape=l1a2out_affine_shape)
            else:
                _raise_dx_general_err("A_L1 atach error.")
        _al1_attach_process()

        def _a_col_before_attach_process():
            if (is_conv1d_situation and (not dyn_util.dynamic_mode)) or tensor_attr.get("l0a_dma_flag"):
                w_align = 1
            else:
                w_align = wi

            if not dyn_util.dynamic_mode:
                sch_agent.pre_apply()
                attach_dict = sch_agent.get_attach_dict()
                if not tensor_attr.get("l0a_dma_flag"):
                    if split_w:
                        if attach_dict.get(sch[a_l1]) == sch[c_ddr]:
                            sch_agent.same_attach(a_col_before, c_col)
                        else:
                            sch_agent.same_attach(a_col_before, a_l1)
                    else:
                        sch_agent.same_attach(a_col_before, a_l1)
                if split_w:
                    sch[a_col_before].buffer_align(
                        (1, 1),
                        (1, 1),
                        (1, 1),
                        (1, 1),
                        (1, kernel_h),  # align for PASS to infer load3d params
                        (1, kernel_w),
                        (1, tbe_platform.CUBE_MKN.get(a_col_before.dtype)["mac"][1])
                    )
                else:
                    sch[a_col_before].buffer_align(
                        (1, 1),
                        (w_align, w_align),
                        (1, 1),
                        (1, 1),
                        (1, 1),
                        (1, tbe_platform.CUBE_MKN.get(a_col_before.dtype)["mac"][1])
                    )
        _a_col_before_attach_process()
        if tensor_attr.get("no_ub_and_dma_copy_flag"):
            sch_agent.same_attach(a_zero, a_l1)
        elif tensor_attr.get("need_expand_stride"):
            if dyn_util.dynamic_mode != "binary" and tensor_attr.get("support_stride_expand_in_ub"):
                # in static scene, if expand in UB, there is no need for attach a_filling
                return
            if dyn_util.dynamic_mode == "binary" and not tensor_attr.get("support_out_to_l1_nd_to_nz"):
                # in dynamic scene, if out_to_l1_nd_to_nz is not supported, there is no need for attach a_filling
                return
            sch_agent.same_attach(a_filling, a_l1)
            sch_agent.same_attach(a_zero, a_l1)
            if dyn_util.dynamic_mode:
                sch_agent.same_attach(dy_vn, a_l1)

    def _get_bl1_status():
        l1_nb = bl1_tiling_n * bl0_tiling_nb
        _, _k0, _n0 = tbe_platform.CUBE_MKN.get(b_l1.dtype)["mac"]
        bl1_shape = cube_util.shape_to_list(b_l1.shape)
        bl0_tiling_n0_temp = bl0_tiling_n0
        cl0_tiling_nc_temp = cl0_tiling_nc
        cl0_tiling_n0_temp = cl0_tiling_n0
        if channel_merge_ratio is not None and tensor_attr['l0c_multi_group_flag']:
            l1_nb //= channel_merge_ratio
            _n0 *= channel_merge_ratio
            bl1_shape[1] = ceil(bl1_shape[1], channel_merge_ratio)
            bl1_shape[2] *= channel_merge_ratio
            cl0_tiling_n0_temp *= channel_merge_ratio
            bl0_tiling_n0_temp *= channel_merge_ratio
            cl0_tiling_nc_temp //= channel_merge_ratio
        l1_kb = bl1_tiling_k // _k0
        l1b2l0c_affine_shape = [
            bl1_tiling_g, None, l1_nb, cl0_tiling_m0,
            bl0_tiling_n0_temp, l1_kb, bl0_tiling_k0
        ]
        if split_w:
            l1b2l0c_affine_shape.insert(2, None)
        if b_ddr.dtype in QUANT_DTYPES:
            tiling_ori_bl1 = bl1_tiling_g * bl1_tiling_k // _k0, l1_nb, _n0, _k0
        else:
            _, k0, n0 = tbe_platform.CUBE_MKN.get(b_l1.dtype)["mac"]
            tiling_ori_bl1_k = bl1_tiling_k // (kernel_h * kernel_w * n0)
            tiling_ori_bl1_n = l1_nb * kernel_h * kernel_w
            tiling_ori_bl1_n = tiling_ori_bl1_n * 2 if b_ddr.dtype == "float32" else tiling_ori_bl1_n
            if tiling_ori_bl1_k == 0:
                tiling_ori_bl1_k = 1
                tiling_ori_bl1_n = l1_nb * kernel_w * (bl1_tiling_k // kernel_w // k0)
            if b_ddr.dtype == "float32":
                tiling_ori_bl1_n1 = 2 * l1_nb
                tiling_ori_bl1_n2 = bl1_tiling_k // (n0 * tiling_ori_bl1_k)
                tiling_ori_bl1 = (tiling_ori_bl1_n1, tiling_ori_bl1_n2, tiling_ori_bl1_k, n0, k0)
            else:
                tiling_ori_bl1 = (tiling_ori_bl1_n, tiling_ori_bl1_k, n0, k0)

        if dyn_util.dynamic_mode == "binary":
            status_ori = dyn_util.status.get("bl1_status_ori")
            status = dyn_util.status.get("bl1_status")
        else:
            status_ori = Compare.compare(tiling_ori_bl1, bl1_shape)
            status = Compare.compare(
                [bl1_tiling_g, 1, l1_nb, bl0_tiling_n0_temp, l1_kb * kb_multi_factor, bl0_tiling_k0],
                [cl0_tiling_g, 1, cl0_tiling_nc_temp, cl0_tiling_n0_temp, c_col_k1, c_col_k0]
            )
        return status_ori, status, l1b2l0c_affine_shape

    def _bl1_process():
        if tiling.get('BL1_shape') == [] and group_dim > 1 and n_dim > 1 and b_l1.dtype in ("float16", "bfloat16"):
            sch[b_l1].buffer_tile((None, ceil_div(bl1_k1, g_extend * n_dim)), (None, None), (None, None), (None, None))
        if tiling.get("BL1_shape") != [] or (
                dyn_util.dynamic_mode == "binary" and dyn_util.attach_flag.get("bl1_attach_flag") != 0):
            status_ori, status, l1b2l0c_affine_shape = _get_bl1_status()

            def _bl1_attach():
                if status_ori == Compare.EQUAL or status_ori == Compare.GREATE_EQ:
                    # bl1 full load but tiling.get("BL1_shape") is not []
                    pass
                elif status == Compare.EQUAL:
                    sch_agent.same_attach(b_l1, c_col)
                elif status == Compare.LESS_EQ:
                    sch_agent.attach_at(b_l1, c_col, affine_shape=l1b2l0c_affine_shape,
                                        ceil_mode_dict=dyn_util.get_ceil_mode(
                                            l1b2l0c_affine_shape, dyn_util.cl0_n1m_dim, dyn_util.cl0_n1m_dim))
                elif status == Compare.GREATE_EQ:
                    l1_nb = bl1_tiling_n * bl0_tiling_nb
                    _, _, _n0 = tbe_platform.CUBE_MKN.get(b_l1.dtype)["mac"]
                    if channel_merge_ratio is not None:
                        l1_nb = l1_nb * bl1_tiling_g // channel_merge_ratio
                        _n0 *= channel_merge_ratio
                    l1b2out_affine_shape = [1, None, l1_nb, cl0_tiling_m0, _n0]
                    if tensor_attr.get("5HD_TO_NHWC_FP"):
                        l1b2out_affine_shape = [1, 1, cl0_tiling_m0, l1_nb * _n0]
                    if split_w:
                        l1b2out_affine_shape.insert(-2, 1)
                    sch_agent.attach_at(b_l1, c_ddr, affine_shape=l1b2out_affine_shape)
                else:
                    _raise_dx_general_err("b_l1 attach error.")
                if tensor_map.get("compress_index_l1") is not None:
                    sch_agent.same_attach(tensor_map.get("compress_index_l1"), b_l1)
            _bl1_attach()
        elif tensor_attr['l0c_multi_group_flag']:
            l1b2l0c_affine_shape = [
                1, None, bl1_tiling_n, cl0_tiling_m0,
                bl0_tiling_n0, c_col_k1, bl0_tiling_k0
            ]
            if split_w:
                l1b2l0c_affine_shape.insert(2, None)
            sch_agent.attach_at(b_l1, c_col, affine_shape=l1b2l0c_affine_shape)
        if b_l1.dtype == "float32":
            # If bl1_tiling_k < Hk * Wk * Co0,  Co1 * Co0 will be inferred as 1 * 8 on BL1, which can not
            # be identified as transposed scene to do load3d. Buffer align the last two dims to (16, 8) is
            # necessary
            _, cube_k0, cube_n0 = tbe_platform.CUBE_MKN.get(b_l1.dtype).get("mac")
            sch[b_l1].buffer_align(
                *([(1, 1)] * (len(b_l1.shape) - 2)),
                (cube_n0, cube_n0),
                (cube_k0, cube_k0)
            )

    def _get_ub_shape(ub_shape_k, aub_h, aub_w, filling_w):
        """
        In l0a_dma_flag scenes, attach_tensor's shape is same as al0;
        if fmap_w % 16 is not 0, aub will load at least 2*w if mdim loads 16,
        it may greater than ub_size, then mdim loads 1.
        """
        m_0, _, n_0 = tbe_platform.CUBE_MKN.get(a_filling.dtype)["mac"][0:3]
        aub_tiling_k, aub_tiling_m, _, _ = tiling.get("AUB_shape")
        if ub_shape_k == 0:
            ub_shape_k = 1
        if dyn_util.dynamic_mode:
            ub_shape = [
                al1_tiling_g,
                1,
                ub_shape_k,
                aub_h,
                aub_w + kernel_w_dilation - 1,
                al1_co0
            ]
        elif tensor_attr.get("l0a_dma_flag"):
            aub_m_dim = aub_tiling_m * filling_w // 16 if not is_conv1d_situation else aub_tiling_m
            aub_m_dim = 1 if aub_m_dim == 0 else aub_m_dim
            shape_k = aub_tiling_k // al1_co0
            if shape_k == 0:
                shape_k = 1
            ub_shape = [
                1,
                1,
                aub_m_dim,
                shape_k,
                m_0,
                al1_co0
            ]
            if shape_y_nc1hwc0_g[4] % m_0 != 0:
                min_aub = (filling_w + img_shape_g[4]) * 2 * al1_co0 * BIT_RATIO_DICT.get(a_filling.dtype)
                min_cub = m_0 * n_0 * BIT_RATIO_DICT.get(c_ub.dtype)
                if min_aub + min_cub > tbe_platform_info.get_soc_spec("UB_SIZE"):
                    # mdim loads 1
                    ub_shape = [1, 1, 1, shape_k, 1, al1_co0]
        else:
            ub_shape = [
                1,
                ub_shape_k,
                aub_h,
                aub_w + kernel_w - 1,
                al1_co0
            ]
        return ub_shape

    def _aub_process_dynamic():
        if dyn_util.dynamic_mode == "binary":
            sch_agent.same_attach(a_filling, dy_vn)
        else:
            sch_agent.same_attach(dy_vn, a_filling)
        if "a_avg" in tensor_map:
            sch_agent.same_attach(a_avg, a_filling)
            sch_agent.same_attach(mean_matrix, a_filling)
            sch_agent.same_attach(mean_matrix_fp16, a_filling)
            sch_agent.same_attach(a_ub, a_filling)
            if "mean_matrix_rec" in tensor_map:
                sch_agent.same_attach(mean_matrix_rec, a_filling)

    def _aub_process_stride_expand():
        aub_tiling_k, aub_tiling_m, aub_tiling_wo, _ = tiling.get("AUB_shape")
        filling_w = cube_util.shape_to_list(a_filling.shape)[3]
        if aub_tiling_m == 0:
            sch_agent.same_attach(a_filling, a_l1)
        else:
            aub_h = aub_tiling_m
            aub_w = filling_w
            if is_conv1d_situation and (not dyn_util.dynamic_mode):
                aub_h = 1
                aub_w = aub_tiling_m

            if dyn_util.dynamic_mode == "binary":
                if is_conv1d_situation or split_w:
                    aub_w = aub_tiling_wo
                ub_shape = _get_ub_shape(aub_tiling_k, aub_h, aub_w, filling_w)
                sch_agent.attach_at(dy_vn, a_l1, ub_shape, ceil_mode_dict=dyn_util.get_ceil_mode(
                    ub_shape, split_ceil_dim=dyn_util.al1_khw_dim))
            else:
                ub_shape_k = aub_tiling_k // (kernel_h * kernel_w * 16)
                ub_shape = _get_ub_shape(ub_shape_k, aub_h, aub_w, filling_w)
                if not tensor_attr.get("l0a_dma_flag"):
                    sch_agent.attach_at(a_filling, a_l1, ub_shape)
                else:
                    sch_agent.attach_at(a_filling, a_col_before, ub_shape)

        if dyn_util.dynamic_mode:
            _aub_process_dynamic()
        else:
            sch_agent.same_attach(a_ub, a_filling)
        if "a_zero" in tensor_map:
            # l0a_dma_scenes if pad!=0 and stride is 1, there is no a_zero
            sch_agent.same_attach(a_zero, a_filling)

    def _aub_process():
        if tensor_attr.get("need_expand_stride"):
            _aub_process_stride_expand()
        elif tensor_attr.get("trans_5HD_fusion"):
            aub_tiling_k, aub_tiling_m, aub_tiling_wo, _ = tiling.get("AUB_shape")
            filling_w = cube_util.shape_to_list(a_ub_nc1hwc0.shape)[3]
            ub_shape = (
                al1_tiling_g,
                1,
                aub_tiling_k,
                aub_tiling_m,
                aub_tiling_wo + kernel_w - 1,
                al1_co0
            )
            sch_agent.attach_at(a_ub_padc, a_l1, ub_shape)
            sch_agent.same_attach(a_ub_transpose, a_ub_padc)
            sch_agent.same_attach(a_ub_zero, a_ub_padc)
            sch_agent.same_attach(a_ub_pad_vn, a_ub_padc)
            sch_agent.same_attach(a_ub_reshape, a_ub_padc)
            sch[a_ub_padc].reused_by(a_ub_zero, a_ub_pad_vn, a_ub_reshape)
        else:
            filling_w = cube_util.shape_to_list(a_avg.shape)[3]
            ub_shape = [
                al1_tiling_g,
                1,
                1,
                1,
                filling_w + kernel_w - 1,
                al1_co0
            ]
            sch_agent.attach_at(a_avg, a_l1, ub_shape)
            sch_agent.same_attach(mean_matrix, a_avg)
            sch_agent.same_attach(mean_matrix_fp16, a_avg)
            sch_agent.same_attach(a_ub, a_avg)
            if "mean_matrix_rec" in tensor_map:
                sch_agent.same_attach(mean_matrix_rec, a_avg)

    def _attach_bias():
        split_bias_flag = tiling.get("CUB_channel_wise_flag") or dyn_util.dynamic_mode == "binary"
        if c_add_bias is not None:
            sch_agent.same_attach(c_add_bias, c_col)
            sch_agent.same_attach(bias_l0c, c_col)
            sch_agent.same_attach(bias_ub_brc, c_col)
            if bias_ub is not None and split_bias_flag:
                sch_agent.same_attach(bias_ub, c_col)

        if bias_add_vector is not None:
            sch_agent.same_attach(bias_add_vector, c_ub)
            if bias_ub is not None and split_bias_flag:
                sch_agent.same_attach(bias_ub, c_ub)
        if bias_bt is not None:
            sch_agent.same_attach(bias_bt, c_col)
            sch_agent.same_attach(bias_l1, c_col)
            if bias_zero is not None:
                sch_agent.same_attach(bias_zero, c_col)
                sch[bias_l1].reused_by(bias_zero)
            if bias_ub_align is not None:
                sch_agent.same_attach(bias_ub_align, c_col)

    def _do_l1andub_process():
        if dyn_util.dynamic_mode == "binary":
            if dyn_util.attach_flag.get("abkl1_attach_flag") == 2:
                _al1_process()
                _bl1_process()
            else:
                _bl1_process()
                _al1_process()
        else:

            if al1_tiling_k < bl1_tiling_k:
                _al1_process()
                _bl1_process()
            else:
                if al1_tiling_g < bl1_tiling_g:
                    _al1_process()
                    _bl1_process()
                else:
                    _bl1_process()
                    _al1_process()

        if (tensor_attr.get("need_expand_stride") or "a_avg" in tensor_map or tensor_attr.get("trans_5HD_fusion")):
            if dyn_util.dynamic_mode != "binary" and not tensor_attr.get("support_stride_expand_in_ub"):
                # in static scene, if not expand in UB, there is no need for process ub
                return
            if dyn_util.dynamic_mode == "binary" and tensor_attr.get("support_out_to_l1_nd_to_nz"):
                # in dynamic scene, if out_to_l1_nd_to_nz is supported, there is no need for process ub
                return
            _aub_process()

    def _do_nbuffer_split():
        """
        calculate the nbuffer size to ensure that AL1 compute size is enough
        """
        attach_dict = sch_agent.get_attach_dict()
        if (tiling.get('A_overhead_opt_flag')
            and attach_dict.get(sch[a_l1]) == sch[c_col]
                and attach_dict.get(sch[a_col]) == sch[c_col]):
            # calculate nbuffer size
            if (kernel_h * kernel_w) % al0_tiling_ka == 0:
                nbuffer_size = (kernel_h * kernel_w) // al0_tiling_ka
            else:
                nbuffer_size = _lcm(kernel_h * kernel_w, al0_tiling_ka) // al0_tiling_ka
            # get the k1 dim
            end_scope = sch_agent.apply_var(sch[a_col])
            k1_axis_list = sch_agent[c_col].get_relate_scope(
                c_col.op.reduce_axis[0], end_scope
            )
            k1_axis = k1_axis_list[-1]
            k1_axis_length = sch_agent[c_col]._axis_unit.get(k1_axis)[1]
            # split k1 dim
            if k1_axis_length % nbuffer_size == 0 and nbuffer_size != 1:
                outter, inner = sch_agent[c_col].split(k1_axis, nbuffer_size)
                sch_agent.update_attach_scope(k1_axis, outter)
                return [outter, inner]
        return

    def _double_buffer():
        def _fusion_double_buffer():
            if (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                    (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
                sch[c_ub_drelu].double_buffer()
                sch[mask_ub].double_buffer()
                if "elewise_binary_add" in deconv_res.op.input_tensors[1].op.tag:
                    sch[vadd_res].double_buffer()
                    sch[tensor_vadd_ub].double_buffer()
            elif "elewise" in deconv_res.op.tag and not tensor_attr.get("quant_fuse"):
                for ub_tensor in tensor_map.get("ub_list"):
                    sch[ub_tensor].double_buffer()
                for input_tensor_mem in tensor_map.get("input_tensor_list"):
                    sch[input_tensor_mem].double_buffer()
                    sch[input_tensor_mem].preload()
            elif tensor_attr.get("5HD_TO_4D_DYN"):
                sch[c_ub_transpose].double_buffer()

        def _aub_double_buffer():
            if tensor_attr.get("need_expand_stride") and \
                tiling.get("manual_pingpong_buffer").get("AUB_pbuffer") == 2 and \
                    "a_avg" not in tensor_map:
                if dyn_util.dynamic_mode != "binary" and not tensor_attr.get("support_stride_expand_in_ub"):
                    # in static scene, if not expand in UB, there is no need for attach process ub's db
                    return
                if dyn_util.dynamic_mode == "binary" and tensor_attr.get("support_out_to_l1_nd_to_nz"):
                    # in dynamic scene, if out_to_l1_nd_to_nz is supported, there is no need for process ub's db
                    return

                sch[a_filling].double_buffer()
                if "a_zero" in tensor_map:
                    sch[a_zero].double_buffer()
                if dyn_util.dynamic_mode:
                    sch[dy_vn].double_buffer()
                else:
                    sch[a_ub].double_buffer()

        def _l1_double_buffer():
            if tiling.get("manual_pingpong_buffer").get("BL1_pbuffer") == 2:
                sch[b_l1].double_buffer()

            if tiling.get("manual_pingpong_buffer").get('AL1_pbuffer') == 2:
                sch[a_l1].double_buffer()
                if tensor_attr.get("no_ub_and_dma_copy_flag"):
                    sch[a_zero].double_buffer()
                elif tensor_attr.get("need_expand_stride"):
                    if dyn_util.dynamic_mode != "binary" and tensor_attr.get("support_stride_expand_in_ub"):
                        # in static scene, if expand in UB, there is no need for process l1's db
                        return
                    if dyn_util.dynamic_mode == "binary" and not tensor_attr.get("support_out_to_l1_nd_to_nz"):
                        # in dynamic scene, if out_to_l1_nd_to_nz is not supported, there is no need for process l1's db
                        return
                    sch[a_filling].double_buffer()
                    sch[a_zero].double_buffer()
                    if dyn_util.dynamic_mode:
                        sch[dy_vn].double_buffer()


        _aub_double_buffer()
        if dyn_util.dynamic_mode and not tiling.get("AL1_shape"):
            tiling["manual_pingpong_buffer"].update(AL1_pbuffer=1)

        _l1_double_buffer()

        if tiling.get("manual_pingpong_buffer").get("AL0_pbuffer") == 2:
            sch[a_col].double_buffer()

        if tiling.get("manual_pingpong_buffer").get("BL0_pbuffer") == 2:
            sch[b_col].double_buffer()

        if tiling.get("manual_pingpong_buffer").get("CL0_pbuffer") == 2:
            sch[c_col].double_buffer()
            if c_add_bias is not None:
                sch[c_add_bias].double_buffer()
                sch[bias_l0c].double_buffer()
                sch[bias_l0c].preload()
                sch[bias_ub_brc].double_buffer()
                sch[bias_ub_brc].preload()

        if tiling.get("manual_pingpong_buffer").get("CUB_pbuffer") == 2 and "a_avg" not in tensor_map:
            if tensor_map.get("c_ub") is not None:
                sch[c_ub].double_buffer()
            if bias_add_vector is not None:
                sch[bias_add_vector].double_buffer()
            _fusion_double_buffer()

    def _emit_fusion_insn():
        vector_emit_index = 3 if split_w else 2  # m axis index, means w axis if split_w else hw axis
        deq_axis_mode = {False: (0, "scalar"), True: (vector_emit_index, "vector")}

        def _emit_requant_fusion_insn():
            sch_agent[tensor_map.get("deq")].emit_insn(
                tensor_map.get("deq").op.axis[0], "dma_copy"
            )
            c_ub_reform = tensor_map.get("c_ub")
            reform_outer, reform_inner = sch[c_ub_reform].split(
                c_ub_reform.op.axis[-1], nparts=2
            )
            if split_w:
                sch[c_ub_reform].reorder(
                    reform_outer,
                    c_ub_reform.op.axis[0],
                    c_ub_reform.op.axis[1],
                    c_ub_reform.op.axis[2],
                    c_ub_reform.op.axis[3],
                    reform_inner
                )
            else:
                sch[c_ub_reform].reorder(
                    reform_outer,
                    c_ub_reform.op.axis[0],
                    c_ub_reform.op.axis[1],
                    c_ub_reform.op.axis[2],
                    reform_inner
                )
            sch[c_ub_reform].emit_insn(c_ub_reform.op.axis[-2], "dma_copy")

        def _emit_quant_fusion_insn():
            if tensor_map.get("deq") is not None:
                sch_agent[tensor_map.get("deq")].emit_insn(tensor_map.get("deq").op.axis[0], "dma_copy")
                deq_axis = deq_axis_mode.get(tensor_attr.get("deq_vector"))[0]
                if cube_util.is_v200_version_new():
                    sch[tensor_map.get("c_ub")].emit_insn(tensor_map.get("c_ub").op.axis[deq_axis], "dma_copy")
                else:
                    deq_mode = deq_axis_mode.get(tensor_attr.get("deq_vector"))[1]
                    sch_agent[tensor_map.get("c_ub")].pragma(
                        sch_agent[tensor_map.get("c_ub")].op.axis[deq_axis], "deq_scale", deq_mode
                    )
            else:
                _emit_l0c_to_out_insn(sch, tensor_map.get("c_ub"))
            for ub_tensor in tensor_map.get("ub_list"):
                if ub_tensor.op.name == "input_ub":
                    sch_agent[ub_tensor].emit_insn(
                        sch_agent[ub_tensor].op.axis[0], "dma_padding"
                    )
                elif "reform" in ub_tensor.op.name:
                    ndim = len(sch[ub_tensor].op.axis)
                    coo, _ = sch[ub_tensor].split(
                        sch[ub_tensor].op.axis[ndim - 1],
                        tbe_platform.CUBE_MKN.get("float16")["mac"][1]
                    )
                    axis_list = sch[ub_tensor].op.axis[0:ndim - 1]
                    sch[ub_tensor].reorder(coo, *axis_list)
                    sch[ub_tensor].emit_insn(sch[ub_tensor].op.axis[2], "vector_auto")
                elif ub_tensor.op.name == "cast_i8_ub":
                    round_mode = tensor_attr.get("q_mode").lower()
                    if round_mode != 'round':
                        error_manager_cube.raise_err_message_cube(
                            f'Round mode should be Round only, {round_mode} is not supported')
                    conv_mode = "vector_conv"
                    sch_agent[ub_tensor].emit_insn(
                        sch_agent[ub_tensor].op.axis[0], conv_mode
                    )
                else:
                    sch_agent[ub_tensor].emit_insn(
                        sch_agent[ub_tensor].op.axis[0], "vector_auto"
                    )
            for input_tensor_mem in tensor_map.get("input_tensor", []):
                sch_agent[input_tensor_mem].emit_insn(
                    sch_agent[input_tensor_mem].op.axis[0], "dma_copy"
                )

        def _emit_elewise_multiple_sel_insn():
            sch[mask_ub].emit_insn(mask_ub.op.axis[0], "dma_copy")

            if "elewise_binary_add" in deconv_res.op.input_tensors[1].op.tag:
                sch[tensor_vadd_ub].emit_insn(tensor_vadd_ub.op.axis[0], "dma_copy")
                if tensor_vadd_1_ub is not None:
                    sch[tensor_vadd_1_ub].emit_insn(tensor_vadd_1_ub.op.axis[0], "dma_copy")
                    sch[tensor_inter_add_compute].emit_insn(tensor_inter_add_compute.op.axis[0], "vector_add")
                sch[vadd_res].emit_insn(vadd_res.op.axis[0], "vector_add")
            sch[c_ub_drelu].emit_insn(c_ub_drelu.op.axis[0], "vector_selects_bool")

        if deconv_res.op.tag == "requant_remove_pad":
            _emit_requant_fusion_insn()
        elif tensor_attr.get("quant_fuse"):
            _emit_quant_fusion_insn()
        elif tensor_map.get("c_ub") is not None and tensor_map.get("tensor_workspace_ub") is None:
            if tensor_attr.get("support_fixpipe"):
                _emit_l0c_to_out_insn(sch, tensor_map.get("c_ub"))
            else:
                insn_dict = {}
                if dyn_util.dynamic_mode == "binary":
                    insn_dict = {"map_policy": "2d"}
                    id_val = tvm.call_extern('int32', 'axis_group', 0, 'overwrite')
                    for i in [-1, -2, -3]:
                        sch_agent[c_ub].pragma(sch_agent[c_ub].op.axis[i], 'axis_group', id_val)
                sch_agent[c_ub].emit_insn(sch_agent[c_ub].op.axis[0], "dma_copy", insn_dict)

        if (deconv_res.op.tag == "emit_insn_elewise_multiple_sel|bool" or
                (dyn_util.dynamic_mode and deconv_res.op.tag == "elewise_multiple_sel")):
            _emit_elewise_multiple_sel_insn()
        elif "elewise" in deconv_res.op.tag and not tensor_attr.get("quant_fuse"):
            for ub_tensor in tensor_map.get("ub_list"):
                sch_agent[ub_tensor].emit_insn(
                    sch_agent[ub_tensor].op.axis[0], "vector_auto"
                )
            for input_tensor_mem in tensor_map.get("input_tensor_list"):
                sch_agent[input_tensor_mem].emit_insn(
                    sch_agent[input_tensor_mem].op.axis[0], "dma_copy"
                )
        elif tensor_attr.get("5HD_TO_NCHW_DYN"):
            sch[c_ub_transpose].storage_align(c_ub_transpose.op.axis[-2], 16, 0)
            sch[c_ub_transpose].buffer_align([1, 1], [1, 1], [1, 16], [1, 16])
            src_in_dst_order = tvm.call_intrin("handle", 'tir.tvm_tuple', 1, 0)
            sch[c_ub_transpose].emit_insn(c_ub_transpose.op.axis[2], 'vector_transpose',
                                          attrs={'src_in_dst_order': src_in_dst_order})
        elif tensor_attr.get("5HD_TO_NHWC_DYN"):
            sch[c_ub_transpose].emit_insn(c_ub_transpose.op.axis[0], 'vector_or')

        if tensor_map.get("tensor_workspace_ub") is not None:
            tensor_workspace_ub = tensor_map.get("tensor_workspace_ub")
            _emit_l0c_to_out_insn(sch, tensor_map.get("tensor_dx_gm"))
            sch[tensor_workspace_ub].emit_insn(tensor_workspace_ub.op.axis[0], "dma_copy")

    def _emit_l1fusion_insn(setfmatrix_dict):
        if a_l1_full is not None:
            sch[a_l1_full].emit_insn(sch[a_l1_full].op.axis[0], "dma_copy")
        if tensor_attr.get("no_ub_and_dma_copy_flag"):
            sch_agent[a_l1].reused_by(a_zero)
            sch_agent[a_zero].emit_insn(sch_agent[a_zero].op.axis[0], "set_2d")
        elif not tensor_attr.get("need_expand_stride"):
            if l1_fusion_type != L1FusionType.DISABLE and input_mem[0] == ScopeDDR.L1:
                sch_agent[a_l1].emit_insn(sch_agent[a_l1].op.axis[0], "phony_insn")
            else:
                if tensor_attr.get("FM_NHWC_TRANS_5HD"):
                    # nd2nz emit insn should be under the batch axis
                    sch_agent[a_l1].emit_insn(sch_agent[a_l1].op.axis[1], "dma_copy", {"layout_transform": "nd2nz"})
                else:
                    sch_agent[a_l1].emit_insn(sch_agent[a_l1].op.axis[0], "dma_copy")
                if l1_fusion_type != L1FusionType.DISABLE:
                    sch_agent[a_l1].pragma(a_l1.op.axis[0], "jump_data", 1)
        else:
            afill_n, afill_c, afill_h, afill_w, _ = sch_agent[a_filling].get_active_scopes()
            afill_w_out, afill_w_inner = sch_agent[a_filling].split(afill_w, factor=stride_w)
            afill_h_out, afill_h_inner = sch_agent[a_filling].split(afill_h, factor=stride_h)
            sch_agent[a_filling].reorder(afill_h_inner, afill_w_inner, afill_n, afill_c, afill_h_out, afill_w_out)
            sch_agent[a_filling].unroll(afill_h_inner)
            sch_agent[a_filling].unroll(afill_w_inner)

            al1_insn, _, _ = sch_agent[a_l1].nlast_scopes(3)
            if not tensor_attr.get("support_stride_expand_in_ub") and (stride_h > 1 or stride_w > 1):
                sch_agent[a_filling].reused_by(a_zero)
                sch_agent[a_zero].emit_insn(sch_agent[a_zero].op.axis[0], "set_2d")
                if tensor_attr.get("FM_NHWC_TRANS_5HD"):
                    sch_agent[a_filling].emit_insn(afill_c, "dma_copy", {"layout_transform": "nd2nz"})
                else:
                    sch_agent[a_filling].emit_insn(afill_n, "dma_copy")
                sch_agent[a_l1].emit_insn(al1_insn, "phony_insn")
                sch_agent[a_l1].reused_by(a_filling)
            else:
                sch_agent[a_ub].emit_insn(sch_agent[a_ub].op.axis[0], "dma_copy")
                if "a_zero" in tensor_map:
                    sch_agent[a_filling].reused_by(a_zero)
                    sch_agent[a_zero].emit_insn(sch_agent[a_zero].op.axis[0], "vector_dup")
                a_filling_emit_insn = "vector_muls"
                if (b_ddr.dtype in QUANT_DTYPES) or "a_zero" not in tensor_map:
                    a_filling_emit_insn = "dma_copy"
                sch_agent[a_filling].emit_insn(afill_n, a_filling_emit_insn)
                sch_agent[a_l1].emit_insn(al1_insn, "dma_copy")

        def _emit_insn_l0a():
            setfmatrix_dict["conv_fm_c"] = a_l1.shape[1] * a_l1.shape[4]
            setfmatrix_dict["conv_fm_h"] = a_l1.shape[2]
            setfmatrix_dict["conv_fm_w"] = a_l1.shape[3]
            if not tensor_attr.get("l0a_dma_flag"):
                if split_w:
                    sch_agent[a_col_before].fuse(a_col_before.op.axis[1], a_col_before.op.axis[2])  # fuse h, w
                    setfmatrix_dict["conv_w_split"] = True
                sch_agent[a_col_before].emit_insn(a_col_before.op.axis[0], 'set_fmatrix', setfmatrix_dict)
                # load3d not effect with full pads, init to zero of l0a
                if split_w:
                    sch_agent[a_col].emit_insn(a_col.op.axis[2], 'im2col',
                                               {'l0a_enable_zero': tiling.get("default_tiling", False)})
                else:
                    sch_agent[a_col].emit_insn(a_col.op.axis[1], 'im2col',
                                               {'l0a_enable_zero': tiling.get("default_tiling", False)})
            else:
                sch[a_l1].compute_inline()
                sch_agent[a_col].emit_insn(a_col.op.axis[1], 'dma_copy')
                # emit insn must in the ma axis
                # the axis is m1, m0, k1, k0, and k1 will be splited to c1_factor, hw
                sch_agent[a_col_before].split(sch[a_col_before].leaf_iter_vars[-3], kernel_h*kernel_w)
                sch[a_col_before].reorder(
                    sch[a_col_before].leaf_iter_vars[-2],  # m0_dim
                    sch[a_col_before].leaf_iter_vars[-4],  # c1_factor
                    sch[a_col_before].leaf_iter_vars[-3],  # k1_dim/c1_factor
                    sch[a_col_before].leaf_iter_vars[-1],  # k0_dim
                )
                sch_agent[a_col_before].emit_insn(sch[a_col_before].leaf_iter_vars[-3], 'dma_copy')
        _emit_insn_l0a()

    def _dynamic_l1_emit_insn(setfmatrix_dict):
        avg_not_expand_stride = "a_avg" in tensor_map and not tensor_attr.get("need_expand_stride")
        if avg_not_expand_stride or tensor_attr.get("trans_5HD_fusion"):
            c1_inner, _, _, _ = sch_agent[a_l1].nlast_scopes(4)
            sch_agent[a_l1].emit_insn(c1_inner, "dma_copy", setfmatrix_dict)
        elif not tensor_attr.get("need_expand_stride"):
            id_val = tvm.call_extern('int32', 'axis_group', 0, 'overwrite')
            if not split_w and tvm.all(dyn_util.shape_vars.get("shape_right_modify") == 0):
                # w is not consecutive when split_w
                # hw is not consecutive when shape_righg_modify < 0
                if dyn_util.attach_flag.get("al1_attach_flag") == 0:
                    for i in [-2, -3, -4, -5]:
                        sch_agent[a_l1].pragma(sch_agent[a_l1].op.axis[i], 'axis_group', id_val)
                else:
                    for i in [-2, -3]:
                        sch_agent[a_l1].pragma(sch_agent[a_l1].op.axis[i], 'axis_group', id_val)
            emit_axis = a_l1.op.axis[0]
            if tensor_attr.get("FM_NHWC_TRANS_5HD"):
                setfmatrix_dict["layout_transform"] = "nd2nz"
                emit_axis = a_l1.op.axis[1]
            if dyn_util.dynamic_mode == 'binary':
                sch[a_l1].pragma(emit_axis, "loop_with_no_overlap_tensor")
            sch[a_l1].emit_insn(emit_axis, "dma_copy", setfmatrix_dict)
        else:
            afill_n, afill_c, afill_h, afill_w, afill_c0 = sch_agent[a_filling].get_active_scopes()
            afill_emit_axis = afill_n
            vec_dup_attr = {}
            if dyn_util.dynamic_mode == 'binary':
                if is_conv1d_situation:
                    afill_emit_axis = afill_c0  # Very poor performance
                else:
                    afill_w_outer, afill_w_inner = sch_agent[a_filling].split(afill_w, factor=stride_w)
                    afill_h_outer, afill_h_inner = sch_agent[a_filling].split(afill_h, factor=stride_h)
                    sch_agent[a_filling].reorder(afill_h_inner, afill_w_inner, afill_n, afill_c, afill_h_outer,
                                                 afill_w_outer)
                id_val = tvm.call_extern('int32', 'axis_group', 0, 'overwrite')
                sch_agent[a_zero].pragma(sch_agent[a_zero].op.axis[2], 'axis_group', id_val)
                if not tensor_attr.get("support_out_to_l1_nd_to_nz"):
                    # To magnify in ub using vec_dup and reduce insns, need to axis_group (m_aub, w).
                    sch_agent[a_zero].pragma(sch_agent[a_zero].op.axis[3], 'axis_group', id_val)
                else:
                    # Optimize set_2d insn
                    # To magnify in ub using set_2d, insn can already be optimized (m_l1, w).
                    # So axis_group (k_l1, m_l1) to further reduce the number of insns.
                    sch_agent[a_zero].pragma(sch_agent[a_zero].op.axis[1], 'axis_group', id_val)
                # remove invalid pipev
                sch_agent[a_zero].pragma(sch_agent[a_zero].op.axis[0], "loop_with_no_overlap_tensor")
                vec_dup_attr = {'extent_overflow_flag': False, 'stride_overflow_flag': False}
                setfmatrix_dict['map_policy'] = '2d'
            else:
                afill_w_outer, afill_w_inner = sch_agent[a_filling].split(afill_w, factor=stride_w)
                afill_h_outer, afill_h_inner = sch_agent[a_filling].split(afill_h, factor=stride_h)
                sch_agent[a_filling].reorder(afill_h_inner, afill_w_inner, afill_n, afill_c, afill_h_outer,
                                             afill_w_outer)
                sch_agent[a_filling].unroll(afill_h_inner)
                sch_agent[a_filling].unroll(afill_w_inner)
            if split_w:
                afill_emit_axis = afill_w_outer
            sch_agent[a_zero].reused_by(a_filling)
            sch_agent[a_zero].reused_by(dy_vn)
            a_zero_emit_insn = "vector_dup"
            a_l1_emit_insn = "dma_copy"
            if tensor_attr.get("support_out_to_l1_nd_to_nz"):
                sch_agent[a_zero].reused_by(a_l1)
                a_zero_emit_insn = "set_2d"
                a_l1_emit_insn = "phony_insn"
            sch_agent[a_zero].emit_insn(sch_agent[a_zero].op.axis[0], a_zero_emit_insn, vec_dup_attr)
            sch[dy_vn].emit_insn(dy_vn.op.axis[0], "phony_insn")
            if tensor_attr.get("FM_NHWC_TRANS_5HD"):
                sch_agent[a_filling].emit_insn(afill_emit_axis, "dma_copy", {"layout_transform": "nd2nz"})
            else:
                sch_agent[a_filling].emit_insn(afill_emit_axis, "dma_copy")
            c1_inner, _, _, _ = sch_agent[a_l1].nlast_scopes(4)
            sch_agent[a_l1].emit_insn(c1_inner, a_l1_emit_insn, setfmatrix_dict)

    def _dynamic_emit_insn():
        # In the load3dv2 scene, conv_fm_c shoudle be the actual channel on L1.
        co_l1_a = al1_tiling_k // kernel_h // kernel_w

        setfmatrix_dict = {
            "set_fmatrix": 1,
            "conv_kernel_h": kernel_h,
            "conv_kernel_w": kernel_w,
            "conv_padding_top": padu,
            "conv_padding_bottom": padd,
            "conv_padding_left": padl,
            "conv_padding_right": padr,
            "conv_stride_h": 1,
            "conv_stride_w": 1,
            "conv_fm_c": co_l1_a,
            "conv_fm_c1": co_l1_a // a_l1.shape[5],
            "conv_fm_h": a_l1.shape[3],
            "conv_fm_w": a_l1.shape[4],
            "conv_fm_c0": a_l1.shape[5],
            "group_flag": 1,
            "l1_group_flag": 1,
            "conv_dilation_h": dilation_h,
            "conv_dilation_w": dilation_w
        }
        setfmatrix_dict_0 = {
            "set_fmatrix": 0,
            "conv_kernel_h": kernel_h,
            "conv_kernel_w": kernel_w,
            "conv_padding_top": padu,
            "conv_padding_bottom": padd,
            "conv_padding_left": padl,
            "conv_padding_right": padr,
            "conv_stride_h": 1,
            "conv_stride_w": 1,
            "conv_fm_c": a_l1.shape[2] * a_l1.shape[5],
            "conv_fm_c1": a_l1.shape[2],
            "conv_fm_h": a_l1.shape[3],
            "conv_fm_w": a_l1.shape[4],
            "conv_fm_c0": a_l1.shape[5],
            "group_flag": 1,
            "l1_group_flag": 1,
            "conv_dilation_h": dilation_h,
            "conv_dilation_w": dilation_w
        }

        _dynamic_l1_emit_insn(setfmatrix_dict)
        a_col_setfmatrix = setfmatrix_dict_0
        if tensor_attr.get("support_out_to_l1_nd_to_nz"):
            a_col_setfmatrix = setfmatrix_dict
        if split_w:
            a_col_setfmatrix["conv_w_split"] = 1
            sch[a_col].emit_insn(a_col.op.axis[2], 'im2col_v2', a_col_setfmatrix)
        else:
            sch[a_col].emit_insn(a_col.op.axis[1], 'im2col_v2', a_col_setfmatrix)
        if dyn_util.dynamic_mode == "binary":
            sch_agent[a_col].pragma(sch_agent[a_col].op.axis[0], "loop_with_no_overlap_tensor")
        if "a_avg" in tensor_map:
            sch_agent[a_ub].emit_insn(sch_agent[a_ub].op.axis[0], "dma_copy")
            sch_agent[a_avg].emit_insn(a_avg.op.axis[0], "vector_auto")
            sch_agent[mean_matrix_fp16].emit_insn(mean_matrix_fp16.op.axis[0], "vector_conv")
            sch_agent[mean_matrix].emit_insn(mean_matrix.op.axis[-1], "vector_dup")
            sch_agent[mean_matrix].reused_by(a_avg)
            if "mean_matrix_rec" in tensor_map:
                sch_agent[mean_matrix_rec].emit_insn(mean_matrix_rec.op.axis[0], "vector_rec")
                sch_agent[mean_matrix].reused_by(mean_matrix_rec)
        if tensor_attr.get("NCHW_TRANS_5HD"):
            sch[a_ub_zero].storage_align(a_ub_zero.op.axis[-2], 16, 0)
            sch[a_ub_padc].storage_align(a_ub_padc.op.axis[-2], 16, 0)
            sch[a_ub_pad_vn].storage_align(a_ub_pad_vn.op.axis[-2], 16, 0)
            sch[a_ub_reshape].storage_align(a_ub_reshape.op.axis[-2], 16, 0)
            sch[a_ub_transpose].storage_align(a_ub_transpose.op.axis[1], 256, 0)
            sch[a_ub_padc].emit_insn(sch_agent[a_ub_padc].op.axis[0], 'dma_copy')
            src_in_dst_order = tvm.call_intrin('handle', 'tir.tvm_tuple', 1, 0)
            sch[a_ub_pad_vn].emit_insn(a_ub_pad_vn.op.axis[0], 'phony_insn')
            sch[a_ub_reshape].emit_insn(a_ub_reshape.op.axis[0], 'phony_insn')
            sch[a_ub_zero].emit_insn(sch_agent[a_ub_zero].op.axis[0], 'vector_dup')
            sch[a_ub_transpose].emit_insn(a_ub_transpose.op.axis[2], 'vector_transpose',
                                          attrs={'src_in_dst_order': src_in_dst_order})
        elif tensor_attr.get("NHWC_to_5HD_fusion"):
            sch[a_ub_padc].emit_insn(sch_agent[a_ub_padc].op.axis[0], 'dma_copy')
            sch[a_ub_pad_vn].emit_insn(a_ub_pad_vn.op.axis[0], 'phony_insn')
            sch[a_ub_reshape].emit_insn(a_ub_reshape.op.axis[0], 'phony_insn')
            sch[a_ub_zero].emit_insn(sch_agent[a_ub_zero].op.axis[0], 'vector_dup')
            sch[a_ub_transpose].emit_insn(a_ub_transpose.op.axis[0], 'vector_or')

    def _b_tensor_emit_insn():
        if tensor_attr.get("WEIGHT_NHWC_TRANS_FZ"):
            if b_ddr.dtype == "float32":
                sch_agent[b_l1].emit_insn(sch_agent[b_l1].op.axis[0], "dma_copy", {"layout_transform": "nd2nz"})
            else:
                nd_factor = kernel_h * kernel_w
                bl1_outer, _ = sch_agent[b_l1].split(sch_agent[b_l1].op.axis[0], nd_factor)
                sch_agent[b_l1].emit_insn(bl1_outer, "dma_copy", {"layout_transform": "nd2nz"})
        else:
            if dyn_util.attach_flag.get("bl1_attach_flag") == 0:
                id_val = tvm.call_extern("int32", "axis_group", 0, "overwrite")
                sch_agent[b_l1].pragma(sch_agent[b_l1].op.axis[-1], "axis_group", id_val)
                sch_agent[b_l1].pragma(sch_agent[b_l1].op.axis[-2], "axis_group", id_val)
                sch_agent[b_l1].pragma(sch_agent[b_l1].op.axis[-3], "axis_group", id_val)
            if dyn_util.dynamic_mode == "binary":
                sch_agent[b_l1].pragma(sch_agent[b_l1].op.axis[0], "loop_with_no_overlap_tensor")
            sch_agent[b_l1].emit_insn(sch_agent[b_l1].op.axis[0], "dma_copy")

        compress_index_l1 = tensor_map.get("compress_index_l1")
        if compress_index_l1 is not None:
            sch_agent[compress_index_l1].emit_insn(sch_agent[compress_index_l1].op.axis[0], "dma_copy")

        if bias_add_vector is not None:
            sch[bias_ub].emit_insn(sch[bias_ub].op.axis[0], "dma_copy")
            sch[bias_add_vector].emit_insn(sch[bias_add_vector].op.axis[0], "vector_auto")
            if dyn_util.dynamic_mode == "binary":
                bias_condition = tvm.all(dyn_util.shape_vars.get("bias_flag") == 1)
                sch[bias_ub].set_store_predicate(bias_condition)
                sch[bias_add_vector].set_store_predicate(bias_condition)

        b_col_emit_dict = {}
        if sparse_4to2_flag:
            b_col_emit_dict["enable_sparse"] = 1
        if dyn_util.dynamic_mode == "binary":
            sch_agent[b_col].pragma(sch_agent[b_col].op.axis[0], "loop_with_no_overlap_tensor")
        if b_col.dtype == "float32":
            _, cube_k0, cube_n0 = tbe_platform.CUBE_MKN.get(b_col.dtype).get("mac")
            sch_agent[b_col].split(sch_agent[b_col].op.axis[-2], factor=8)
            sch_agent[b_col].emit_insn(sch_agent[b_col].op.axis[-3], "dma_copy", {'img2col': 1})
            sch[b_col].buffer_align(
                *([(1, 1)] * (len(b_col.shape) - 2)),
                (cube_n0, cube_n0),
                (cube_k0, cube_k0))
        elif neg_src_stride and (cube_util.is_cloud_version() or cube_util.is_v200_version_new()
                                 or cube_util.is_lhisi_version()):
            _, b_col_inner = sch_agent[b_col].split(sch_agent[b_col].op.axis[1], factor=kernel_h * kernel_w)
            sch_agent[b_col].emit_insn(b_col_inner, "dma_copy", b_col_emit_dict)
        else:
            sch_agent[b_col].emit_insn(sch_agent[b_col].op.axis[2], "dma_copy", b_col_emit_dict)

    def _emit_ub_to_output_insn(sch, output_tensor):
        """
        emit insn for ub to out
        """
        if tensor_attr.get("5HD_TO_4D_DYN"):
            sch[output_tensor].emit_insn(sch_agent[output_tensor].nlast_scopes(2)[0], 'dma_copy',
                                         {'no_overlap': "default"})
        else:
            if dyn_util.dynamic_mode == "binary":
                sch[output_tensor].pragma(sch_agent[output_tensor].nlast_scopes(3)[0], 'loop_with_no_overlap_tensor')
                sch[output_tensor].emit_insn(sch_agent[output_tensor].nlast_scopes(3)[0], 'dma_copy')
            else:
                sch[output_tensor].emit_insn(sch_agent[output_tensor].nlast_scopes(2)[0], 'dma_copy')

    def _pragma_fixpipe_continuous(tensor, pragma_axis):
        """pragma fixpipe_continuous to help pass recognizing continue axis when split_w
        """
        if tensor_attr.get("support_l0c_to_out") and dyn_util.dynamic_mode == "binary" and split_w:
            sch[tensor].pragma(pragma_axis, 'fixpipe_c1_continuous')

    def _emit_l0c_to_out_insn(sch, output_tensor):
        """
        emit insn for fixpipe to out
        """
        emit_str = get_fixpipe_emit_str()
        if tensor_attr.get("5HD_TO_NHWC_FP"):
            hw_dim, c_dim = sch_agent[output_tensor].nlast_scopes(2)
            sch[output_tensor].emit_insn(hw_dim, emit_str, {"layout_transform": "nz2nd"})
        elif output_tensor.dtype == "float32" and b_col.dtype == "float32":
            channel_axis = sch_agent[output_tensor].nlast_scopes(3)[0]
            _, channel_axis_inner = sch_agent[output_tensor].split(channel_axis, factor=2)
            sch[output_tensor].emit_insn(channel_axis_inner, emit_str, {"layout_transform": "channel_split"})
            sch_agent.update_attach_scope(channel_axis, channel_axis_inner)
            _pragma_fixpipe_continuous(output_tensor, channel_axis_inner)
        elif output_tensor.dtype in QUANT_DTYPES:
            hw_dim, c_dim = sch_agent[output_tensor].nlast_scopes(2)
            if not dyn_util.dynamic_mode:
                sch_agent[output_tensor].split(c_dim, 16)
            sch[output_tensor].emit_insn(hw_dim, emit_str)
        elif double_out_tensor:
            sch[output_tensor].emit_insn(sch_agent[output_tensor].op.axis[0], emit_str)
        else:
            sch[output_tensor].emit_insn(sch_agent[output_tensor].nlast_scopes(3)[0], emit_str)
            _pragma_fixpipe_continuous(output_tensor, sch_agent[output_tensor].nlast_scopes(3)[0])

    def _emit_insn():
        _b_tensor_emit_insn()

        setfmatrix_dict = {
            "conv_kernel_h": kernel_h,
            "conv_kernel_w": kernel_w,
            "conv_padding_top": padu,
            "conv_padding_bottom": padd,
            "conv_padding_left": padl,
            "conv_padding_right": padr,
            "conv_stride_h": 1,
            "conv_stride_w": 1,
            "conv_dilation_h": dilation_h,
            "conv_dilation_w": dilation_w
        }

        if dyn_util.dynamic_mode:
            _dynamic_emit_insn()
        else:
            _emit_l1fusion_insn(setfmatrix_dict)

        scopes_intrins = sch_agent[c_col].intrin_scopes(7)
        scope_insn = scopes_intrins[1]
        inner_k_axis = sch_agent[c_col].get_relate_scope(
            c_col.op.reduce_axis[0], scope_insn
        )
        if inner_k_axis:
            mad_dict = {
                "mad_pattern": 2,
                "k_outer": sch_agent[c_col].get_relate_scope(
                    c_col.op.reduce_axis[0], scope_insn
                )
            }
        else:
            inner_g, inner_n, inner_co, inner_m, inner_co0, inner_k1, inner_k0 = scopes_intrins
            inner_ko, inner_ki = sch_agent[c_col].split(inner_k1, nparts=1)
            sch_agent[c_col].reorder(
                inner_ko, inner_g, inner_n, inner_co, inner_m, inner_co0, inner_ki, inner_k0)
            mad_dict = {"mad_pattern": 2, "k_outer": [inner_ko]}

        # NOTE When gg * ci1g > ci1, the number of fractals processed by mad is greater than that of fixpipe.
        # This causes mad to no longer be able to write data to l0c.
        is_enable_unit_flag = dyn_util.dynamic_mode == "binary" and not tiling.get("groups_gt_1") and tensor_attr.get(
            "support_l0c_to_out") and tiling.get("manual_pingpong_buffer").get("CL0_pbuffer") == 1
        if is_enable_unit_flag:
            k_div_max_kl1 = dyn_util.tiling_vars.get("k_div_max_kl1")
            max_kl1_div_min_kl1 = dyn_util.tiling_vars.get("max_kl1_div_min_kl1")
            min_kl1_div_kl0 = dyn_util.tiling_vars.get("min_kl1_div_kl0")
            last_kl0_idx = k_div_max_kl1 * max_kl1_div_min_kl1 * min_kl1_div_kl0 - 1

            cur_kl0_idx = 0
            extend_list = [1, min_kl1_div_kl0, min_kl1_div_kl0 * max_kl1_div_min_kl1]
            for i in range(len(mad_dict["k_outer"]) - 1):
                cur_kl0_idx += mad_dict["k_outer"][len(mad_dict["k_outer"]) - 2 - i] * extend_list[i]
            sch_agent[c_col].pragma(mad_dict["k_outer"][-1], "unit_flag_condition", cur_kl0_idx >= last_kl0_idx)

        if bias_ub_brc is not None:
            sch[bias_l0c].reused_by(c_add_bias, c_col)
            sch[c_add_bias].emit_insn(c_add_bias.op.axis[0], 'phony_insn')
            # broadcast must align to 16
            sch[bias_l0c].split(bias_l0c.op.axis[3], 16)
            sch[bias_l0c].emit_insn(bias_l0c.op.axis[2], 'dma_copy')
            sch[bias_ub].emit_insn(bias_ub.op.axis[0], 'dma_copy')
            sch[bias_ub_brc].emit_insn(bias_ub_brc.op.axis[0], 'vector_auto')
            mad_dict["init_bias"] = 1

        if dyn_util.dynamic_mode == "binary":
            mad_dict['spec_offset'] = tvm.call_intrin('handle', 'tir.tvm_tuple', 0, 0, 0)
            if a_ddr.dtype == "float32":
                mad_dict['hf32'] = get_te_var("hf32_flag").get_tvm_var()
        elif tiling_dtype[3] == "hfloat32":
            mad_dict["hf32"] = 1
        if sparse_4to2_flag:
            mad_dict["mad_type"] = 1

        sch_agent[c_col].emit_insn(scope_insn, "mad", mad_dict)

        if tensor_map.get("c_ub") is not None:
            if not double_out_tensor:
                _emit_ub_to_output_insn(sch, c_ddr)
            else:
                sch[c_ddr].emit_insn(sch_agent[c_ddr].nlast_scopes(2)[0], "phony_insn")
                sch[double_out_tensor[0]].emit_insn(sch_agent[double_out_tensor[0]].nlast_scopes(2)[0], "dma_copy")
                sch[double_out_tensor[1]].emit_insn(sch_agent[double_out_tensor[1]].nlast_scopes(2)[0], "dma_copy")
        elif double_out_tensor:
            sch[c_ddr].emit_insn(sch_agent[c_ddr].nlast_scopes(2)[0], "phony_insn")
            for output_tensor in double_out_tensor:
                _emit_l0c_to_out_insn(sch, output_tensor)
        else:
            # fixpipe to out
            _emit_l0c_to_out_insn(sch, c_ddr)
        if tensor_map.get("bias_bt") is not None:
            sch[bias_bt].emit_insn(bias_bt.op.axis[0], "dma_copy", {'mem_align': 1})
            if bias_zero is not None:
                sch[bias_zero].emit_insn(bias_zero.op.axis[0], "set_2d")
            if dyn_util.dynamic_mode == "binary":
                sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy", {"layout_transform": "nd2nz"})
            elif tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy")
            elif tensor_attr.get("bias_need_align"):
                # static and need bias align
                if (tbe_platform.platform_info.intrinsic_check_support("Intrinsic_set_l1") and
                    not tbe_platform.platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
                    sch[bias_ub_align].emit_insn(bias_ub_align.op.axis[0], "dma_padding")
                    sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy")
                else:
                    sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy",
                                           {"layout_transform": "nd2nz", "split_select": 1})
            else:
                sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy", {'mem_align': 1})

        overload_flag = _check_overload_dy()
        _set_overload_flag(sch[c_ddr], overload_flag, sch_agent[c_ddr].nlast_scopes(3)[0])
        _emit_fusion_insn()

    def _handle_dynamic_workspace(stride_w):
        def _get_al1_m_extent(al1_m):
            al1_h_small = tvm.select(
                (tvm.floormod(shape_y_nc1hwc0[3], al1_m) == 0),
                kernel_h,
                kernel_h + 1)
            al1_h_large = tvm.select(
                (tvm.floormod(al1_m, shape_y_nc1hwc0[3]) == 0),
                kernel_h + (al1_m // shape_y_nc1hwc0[3]) - 1,
                kernel_h + (al1_m // shape_y_nc1hwc0[3]) + 1)
            al1_h = tvm.select(
                al1_m < shape_y_nc1hwc0[3],
                al1_h_small,
                al1_h_large)
            al1_h = tvm.select(
                al1_h < ho * stride_h,
                al1_h,
                ho * stride_h)
            al1_w = a_ddr.shape[3] * stride_w
            return al1_h, al1_w

        def _get_al1_bound():
            al1_bound = dyn_util.tiling_vars.get("al1_bound")
            if al1_bound is not None:
                return al1_bound
            if len(tiling.get("AL1_shape")) != 0:
                k_al1, multi_m_al1 = tiling.get("AL1_shape")[:2]
                al1_m = multi_m_al1 * cl0_tiling_mc * cl0_tiling_m0
                al1_c = k_al1 // kernel_h // kernel_w
                al1_c = max(al1_c, 16)
                al1_h, al1_w = _get_al1_m_extent(al1_m)
                al1_bound = al1_c * al1_h * al1_w
            else:
                al1_h, al1_w = a_l1.shape[3], a_l1.shape[4]
                al1_m = ceil(al1_h * al1_w, cl0_tiling_m0) * cl0_tiling_m0
                al1_c = co1g * al1_co0 * co1g_factor
                al1_bound = al1_c * al1_m

            return al1_bound

        def _set_aub_bound():
            aub_bound = dyn_util.tiling_vars.get("aub_bound")
            if tensor_attr.get("need_expand_stride"):
                if aub_bound is None:
                    aub_co0 = tbe_platform.CUBE_MKN.get(a_filling.dtype)["mac"][1]
                    aub_tiling_k, aub_tiling_m, _, _ = tiling.get("AUB_shape")
                    aub_co1 = aub_tiling_k // (kernel_h * kernel_w * aub_co0)
                    if tiling.get("AL1_shape"):
                        al1_co1 = tiling.get("AL1_shape")[0] // (kernel_h * kernel_w * aub_co0)
                        aub_co1 = min(aub_co1, al1_co1)
                    aub_co1 = max(aub_co1, 1)
                    aub_filling_w = wo * stride_w
                    aub_h = aub_tiling_m // stride_h + 1
                    aub_bound = aub_co1 * aub_tiling_m * aub_filling_w * aub_co0
                filling_zero_bound = aub_bound
                if tensor_attr.get("support_out_to_l1_nd_to_nz"):
                    filling_zero_bound = _get_al1_bound()
                sch[a_filling].set_buffer_size(filling_zero_bound)
                sch[dy_vn].set_buffer_size(filling_zero_bound)
                sch[a_zero].set_buffer_size(filling_zero_bound)
                if "a_avg" in tensor_map:
                    aub_bound = aub_co1 * aub_h * wo * aub_co0
                    sch[a_ub].set_buffer_size(aub_bound)
                    sch[a_avg].set_buffer_size(aub_bound)
                    sch[mean_matrix].set_buffer_size(aub_bound)
                    sch[mean_matrix_fp16].set_buffer_size(aub_bound)
            elif tensor_attr.get("trans_5HD_fusion"):
                sch[a_ub_transpose].set_buffer_size(aub_bound)
                sch[a_ub_padc].set_buffer_size(aub_bound)
                sch[a_ub_zero].set_buffer_size(aub_bound)
                sch[a_ub_reshape].set_buffer_size(aub_bound)
                sch[a_ub_pad_vn].set_buffer_size(aub_bound)

        def _set_cub_bound():
            # nc1hwc0 trans to nchw
            if affine_cub is None:
                return
            cub_bound = reduce(lambda x, y: x * y, affine_cub)
            if c_ub is not None:
                sch[c_ub].set_buffer_size(cub_bound * load3d_special_binary)
            if tensor_attr.get("5HD_TO_4D_DYN"):
                sch[c_ub_transpose].set_buffer_size(cub_bound)
            if bias_add_vector is not None:
                sch[bias_add_vector].set_buffer_size(cub_bound)

        sch[a_l1].set_buffer_size(_get_al1_bound())
        if dyn_util.dynamic_mode == "binary":
            sch[b_l1].set_buffer_size(dyn_util.tiling_vars.get("bl1_bound"))
            if tensor_map.get("bias_bt") is not None:
                sch[bias_bt].set_buffer_size(dyn_util.tiling_vars.get("bias_table_bound"))
        _set_aub_bound()
        _set_cub_bound()
        if dyn_util.dynamic_mode == "binary" and tensor_attr.get("trans_5HD_fusion"):
            sch[b_col].skip_bound_check()
        dyn_util.set_spec_var_value(sch, b_col)

        sch.sequential_malloc(tbe_platform_info.scope_cbuf)
        sch.sequential_malloc(tbe_platform_info.scope_ca)
        sch.sequential_malloc(tbe_platform_info.scope_cb)
        sch.sequential_malloc(tbe_platform_info.scope_cc)
        sch.sequential_malloc(tbe_platform_info.scope_ubuf)

        sch[a_l1].mem_unique()
        sch[a_col].mem_unique()
        sch[b_l1].mem_unique()
        sch[b_col].mem_unique()
        sch[c_col].mem_unique()
        if deconv_res.op.tag != "elewise_multiple_sel" and bias_add_vector is None and c_ub is not None:
            sch[c_ub].mem_unique()

    def _set_bl0_bound():
        # buffer_size calculated by load3d_to_l0b is smaller than the actual used, reverse some space
        # to avoid exceeding buffer
        if b_col.dtype == "float32":
            sch[b_col].set_buffer_size((tiling.get("BL0_matrix")[0] + 1) * tiling.get("BL0_matrix")[1] *
                                       tiling.get("BL0_matrix")[2] * tiling.get("BL0_matrix")[3])

    def _handle_workspace():
        if dyn_util.dynamic_mode:
            _handle_dynamic_workspace(stride_w)
            return

        l1_tensor_map = {}
        if not fmap_l1_addr_flag:
            l1_tensor_map = None
        else:
            fmap = DeConvPattern.dedy
            if a_l1_full is not None:
                sch[a_l1_full].set_buffer_size(fmap_l1_valid_size)
                l1_tensor_map[fmap] = a_l1_full
            elif (
                l1_fusion_type != L1FusionType.DISABLE
                and input_mem == ScopeDDR.DDR
                and stride_w == 1
                and stride_h == 1
            ):
                sch[a_l1].set_buffer_size(fmap_l1_valid_size)
                l1_tensor_map[fmap] = a_l1
            else:
                l1_tensor_map = None
        L1CommonParam.l1_fusion_tensors_map = l1_tensor_map

    def _allocate_apply():
        """
        process allocate_at
        """
        sch_agent.pre_apply()
        attach_dict = sch_agent.get_attach_dict()
        compute_path = sch_agent.get_compute_path()
        attach_dict.update(attach_dict_b)
        compute_path.update(compute_path_b)
        # l0c axes: g, batch, co1, m, co0
        _, _, l0c_n, l0c_m, _ = sch[c_col].op.axis
        if tensor_attr.get("5HD_TO_NHWC_FP"):
            # ddr axes: n, hw, c
            _, ddr_m, ddr_n = sch[c_ddr].op.axis
        else:
            # ddr axes: n, c1, hw, c0
            _, ddr_n, ddr_m, _ = sch[c_ddr].op.axis

        # in this situation the data will be intermittent so cannot do allocate
        if al1_tiling_m > 1:
            tiling['A_overhead_opt_flag'] = 0

        # the head in dma_flag is poor performance, which is not suitalbe in overhead_opt
        if tensor_attr.get("l0a_dma_flag"):
            tiling['A_overhead_opt_flag'] = 0

        if tiling.get('A_overhead_opt_flag') and attach_dict.get(sch[a_col]):
            # process AL1 full load
            if not attach_dict.get(sch[a_l1]):
                attach_dict[sch[a_l1]] = sch[c_ddr]
                compute_path[sch[a_l1]] = ax_core
            # process CL0 full load
            if not attach_dict.get(sch[c_col]):
                attach_dict[sch[c_col]] = sch[c_ddr]
                compute_path[sch[c_col]] = ax_core
            # get run_once axes
            run_once_list = []
            if attach_dict.get(sch[a_col]) == attach_dict.get(sch[a_l1]):
                # get related tensor and axis
                if nbuffer_split_list:
                    al1_compute_at_axis = nbuffer_split_list[0]
                elif kernel_h * kernel_w == al0_tiling_ka:
                    al1_compute_at_axis = compute_path.get(sch[a_col])
                else:
                    al1_compute_at_axis = compute_path.get(sch[a_l1])
                al0_compute_at_tensor = attach_dict.get(sch[a_col])
                # AL1 and AL0 compute at same tensor
                n_axis = ddr_n if al0_compute_at_tensor == sch[c_ddr] else l0c_n
                tmp_tensor = c_ddr if al0_compute_at_tensor == sch[c_ddr] else c_col
                l1_set = set(sch_agent[tmp_tensor].get_relate_scope(n_axis, compute_path.get(sch[a_l1]))).union(
                         set([compute_path.get(sch[a_l1])]))
                allocate_set = set(sch_agent[tmp_tensor].get_relate_scope(n_axis, al1_compute_at_axis))
                run_once_list = list(allocate_set.difference(l1_set))
                # process allocate
                sch[a_l1].allocate_at(attach_dict.get(sch[a_l1]),
                                      compute_path.get(sch[a_l1]),
                                      run_once_axes=run_once_list)
                sch[a_l1].compute_at(al0_compute_at_tensor, al1_compute_at_axis)
                sch[a_col_before].compute_at(al0_compute_at_tensor, al1_compute_at_axis)
            else:
                # AL1 and AL0 compute at different tensor
                l1_set = set(sch_agent[c_ddr].get_relate_scope(ddr_n, compute_path.get(sch[a_l1]))).union(
                         set([compute_path.get(sch[a_l1])]))
                l0c_set = set(sch_agent[c_ddr].get_relate_scope(ddr_n, compute_path.get(sch[c_col])))
                run_once_list = list(l0c_set.difference(l1_set))
                # process allocate
                sch[a_l1].allocate_at(attach_dict.get(sch[a_l1]),
                                      compute_path.get(sch[a_l1]),
                                      run_once_axes=run_once_list)
                sch[a_l1].compute_at(sch[c_ddr], compute_path.get(sch[c_col]))
                sch[a_col_before].compute_at(sch[c_ddr], compute_path.get(sch[c_col]))
            del attach_dict[sch[a_l1]]
            if attach_dict.get(sch[a_col_before]):
                del attach_dict[sch[a_col_before]]

        if tiling.get('B_overhead_opt_flag') and attach_dict.get(sch[b_col]):
            # process BL1 full load
            if not attach_dict.get(sch[b_l1]):
                attach_dict[sch[b_l1]] = sch[c_ddr]
                compute_path[sch[b_l1]] = ax_core
            # process CL0 full load
            if not attach_dict.get(sch[c_col]):
                attach_dict[sch[c_col]] = sch[c_ddr]
                compute_path[sch[c_col]] = ax_core
            # get run_once axes
            run_once_list = []
            bl0_compute_at_tensor = attach_dict.get(sch[b_col])
            bl0_compute_at_axis = compute_path.get(sch[b_col])
            if bl0_compute_at_tensor == attach_dict.get(sch[b_l1]):
                # BL1 and BL0 compute at same tensor
                m_axis = ddr_m if bl0_compute_at_tensor == sch[c_ddr] else l0c_m
                tmp_tensor = c_ddr if bl0_compute_at_tensor == sch[c_ddr] else c_col
                l1_set = set(sch_agent[tmp_tensor].get_relate_scope(m_axis, compute_path.get(sch[b_l1]))).union(
                         set([compute_path.get(sch[b_l1])]))
                allocate_set = set(sch_agent[tmp_tensor].get_relate_scope(m_axis, bl0_compute_at_axis))
                run_once_list = list(allocate_set.difference(l1_set))
            else:
                # BL1 and BL0 compute at differenet tensor
                l1_set = set(sch_agent[c_ddr].get_relate_scope(ddr_m, compute_path.get(sch[b_l1]))).union(
                         set([compute_path.get(sch[b_l1])]))
                l0c_set = set(sch_agent[c_ddr].get_relate_scope(ddr_m, compute_path.get(sch[c_col])))
                allocate_set = set(sch_agent[c_col].get_relate_scope(l0c_m, bl0_compute_at_axis))
                run_once_list = list(l0c_set.difference(l1_set) | allocate_set)
            # process allocate
            sch[b_l1].allocate_at(attach_dict.get(sch[b_l1]),
                                  compute_path.get(sch[b_l1]),
                                  run_once_axes=run_once_list)
            sch[b_l1].compute_at(bl0_compute_at_tensor, bl0_compute_at_axis)
            del attach_dict[sch[b_l1]]

        sch_agent.apply_compute(attach_dict, compute_path)

    def _split_group():
        # split g_dim for ddr; outer is g_axis, inner is c1
        # c_index, 1: NC1HWC0/NCHW, 2: N H*W C
        c_index = 2 if tensor_attr.get("5HD_TO_NHWC_FP") or tensor_attr.get("5HD_TO_NHWC_DYN") else 1
        n_0 = tbe_platform.CUBE_MKN.get(b_l1.dtype).get("mac")[1] if b_l1.dtype == "float32" else tbe_platform.C0_SIZE
        if dyn_util.dynamic_mode == "binary":
            # c_ddr[c_index] means dx_c1
            c_factor_split_group = ci1g
            if any(x in tensor_attr and tensor_attr.get(x) for x in OUTPUT_WITH_4D_SHAPE):
                # c_ddr[c_index] means dx_c
                c_factor_split_group = ci1g * n_0
            sch_agent[c_ddr].split_group(
                c_ddr.op.axis[c_index], c_factor_split_group, c_index=c_index)
            return
        c_0 = c_ddr.shape[-1].value if not tensor_attr.get("5HD_TO_NHWC_FP") else 1
        # dx_c1_extend is c / n0. for c_gm, c1 is c / c0.
        c_factor_split_group = ceil(ci1g * n_0, c_0)
        # ci1g % 2 is not 0, l0c need to calculate cl0_tiling_g * ci1g at c1 dim
        if tensor_attr['l0c_multi_group_flag']:
            c_factor_split_group = ceil(cl0_tiling_g * ci1g * n_0, c_0)
        sch_agent[c_ddr].split_group(c_ddr.op.axis[c_index], c_factor_split_group)

    sch_agent = ScheduleAgent(sch)

    _split_group()

    if split_w:
        sch_agent[c_ddr].split_hw(c_ddr.op.axis[-2], nparts=hi)
    affine_cub = _cub_process()
    _cl0_process(affine_cub)
    _attach_bias()
    _fixpipe_process()
    _l0a_process()
    kb_multi_factor = SPARSE_4TO2_K_RATIO if sparse_4to2_flag else 1
    neg_src_stride = _l0b_process()
    _do_l1andub_process()
    nbuffer_split_list = _do_nbuffer_split()

    def _bind_core():
        axs = sch_agent[c_ddr].get_active_scopes()
        if tensor_attr.get("5HD_TO_NHWC_FP"):
            if split_w:
                ax_g, ax_ni, ax_mo, _, ax_ci = axs
            else:
                ax_g, ax_ni, ax_mo, ax_ci = axs
        elif tensor_attr.get("5HD_TO_NHWC_DYN"):
            ax_g, ax_ni, ax_mo, ax_ci = axs
        else:
            ax_g, ax_ni, ax_ci, ax_mo, *_ = axs

        if dyn_util.dynamic_mode == "binary":
            split_params = SplitParam(split_ceil_mode=False)
            ax_batch_outer, ax_batch_inner = sch_agent[c_ddr].split(ax_ni, nparts=batch_dim)
            ax_m_outer, ax_m_inner = sch_agent[c_ddr].split(ax_mo, nparts=m_dim)
            ax_n_outer, ax_n_inner = sch_agent[c_ddr].split(ax_ci, nparts=n_dim)
            ax_g_outer, ax_g_inner = sch_agent[c_ddr].split(ax_g, nparts=group_dim, split_params=split_params)
            sch[c_ddr].reorder(ax_g_outer, ax_batch_outer, ax_n_outer, ax_m_outer,
                               ax_g_inner, ax_batch_inner, ax_n_inner, ax_m_inner)
            sch.bind_axes([ax_g_outer, ax_batch_outer, ax_n_outer, ax_m_outer], tvm.thread_axis('blockIdx.x'))
            return ax_m_outer
        else:
            # g, c both relate to BL1[0], must in the inner of fuse_bind_axis
            ax_core = sch_agent[c_ddr].bind_core(
                    [ax_ni, ax_mo, ax_g, ax_ci], [batch_dim, m_dim, group_dim, n_dim])
            ax_core_in = sch_agent[c_ddr].get_superkernel_axis_pragma()
            sch_agent.root_stage_at(c_ddr, ax_core)
            blocks = batch_dim * group_dim * n_dim * m_dim
            if blocks == batch_dim:
                sch[c_ddr].pragma(ax_core_in, "json_info_batchBindOnly")
            return ax_core

    def _cal_binary_conv1d_w_offset_extent():
        howo_mad_align = align(hi * wi, al0_tiling_m0)
        if dyn_util.attach_flag.get('al1_attach_flag') == 0:
            m_axis_length_l1 = align(howo_mad_align, al0_tiling_ma * al0_tiling_m0)
            w_offset = -padl
        else:
            m_axis_length_l1 = al1_tiling_m * al0_tiling_ma * al0_tiling_m0
            # compute the axis's offset between cores
            howo_offset_on_block = align(ceil(howo_mad_align, m_dim), m_axis_length_l1)
            block_offset = ax_core * howo_offset_on_block
            # compute the axis's part offset within the core
            hw_index = 1 if tensor_attr.get("5HD_TO_NHWC_DYN") else 2
            hw_axis_list = sch_agent[c_ddr].get_relate_scope(sch[c_ddr].op.axis[hw_index])
            m_axis_origin = hw_axis_list[1]
            m_parts_offset = m_axis_origin * m_axis_length_l1
            # compute the final offset
            w_offset = block_offset + m_parts_offset - padl

        load3d_stride = 1
        w_extend = (m_axis_length_l1 - 1) * load3d_stride + kernel_w_dilation
        return w_offset, w_extend

    def _conv1d_split_tile():
        """
        conv1d situation
        use buffer tile to split width
        """
        if dyn_util.dynamic_mode == 'binary' and is_conv1d_situation:
            w_offset, w_extend = _cal_binary_conv1d_w_offset_extent()
            sch[a_l1].buffer_tile((None, None), (None, None), (None, None), (None, None), (w_offset, w_extend),
                                  (None, None))
            return
        if not is_conv1d_situation or dyn_util.dynamic_mode or tensor_attr.get("5HD_TO_NHWC_FP", False):
            return
        kernel_w_dilation = (kernel_w - 1) * dilation_w + 1
        if split_w:
            m_axis_length_l1 = al0_tiling_ma * al0_tiling_m0
            howo_mad_align = align(wo_mad, al0_tiling_m0)
            m_full_load = (c_l0c_wo == m_axis_length_l1)
        else:
            m_axis_length_l1 = al1_tiling_m * al0_tiling_ma * al0_tiling_m0
            howo_mad_align = align(howo_mad, al0_tiling_m0)
            m_full_load = (c_l0c_hw == m_axis_length_l1)
        m_axis_origin = sch_agent[c_ddr].get_bindcore_m_axis(split_w)
        # compute the axis's block offset and part offset after split
        howo_offset_on_block = align(ceil(howo_mad_align, m_dim), m_axis_length_l1)
        # compute the real n dim when tiling_n_dim is not a factor of cin1_g
        cin1_g = tensor_attr.get("group_dict")["dx_c1_extend"]
        real_n_dim = ceil(cin1_g, ceil(cin1_g, n_dim))
        block_offset = ax_core // real_n_dim // group_dim % m_dim * howo_offset_on_block
        # M axis full load in Al1 or Not
        if m_full_load:
            m_parts_offset = 0
        else:
            m_parts_offset = m_axis_origin * m_axis_length_l1
        # compute the final offset
        w_offset = block_offset + m_parts_offset - padl

        # compute the m_length of A tensor used in Al1 by the Al0 length
        # using the convolution rule
        # expand in UB , so load3d stride is 1
        load3d_stride = 1
        w_extend = (m_axis_length_l1 - 1) * load3d_stride + kernel_w_dilation
        if not tensor_attr.get("l0a_dma_flag"):
            sch[a_l1].buffer_tile((None, None), (None, None), (None, None), (w_offset, w_extend), (None, None))

    def _cal_buffer_tile_offset_extent(tensor, parent, axis_index, extent, dtype_sensitive=False):
        """
        calculate buffer tile offset and extent according to parent tensor and axis index
        """
        if tensor is None:
            return None, None
        axis_split_list, axis_unit, axis_offset = sch_agent[parent].get_axis_split_list_and_extend(axis_index)
        attach_var = sch_agent.apply_var(sch[tensor])
        if attach_var is None:
            return None, None
        attached_idx = list(sch[parent].leaf_iter_vars).index(attach_var)
        var_list = sch[parent].leaf_iter_vars[0:attached_idx]
        for var in var_list[::-1]:
            if var not in axis_split_list:
                continue
            var_idx = axis_split_list.index(var)
            axis = axis_split_list[0:var_idx + 1]
            unit = axis_unit[0:var_idx + 1]
            offset = axis_offset[0:var_idx + 1]
            offset_res = 0
            for idx in range(var_idx + 1):
                offset_idx = offset[idx]
                if dtype_sensitive and channel_merge_ratio is not None:
                    offset_idx = offset[idx] * channel_merge_ratio
                if dtype_sensitive and b_l1.dtype == "float32":
                    offset_idx = offset[idx] // 2
                factor_len = (0 if unit[idx] == 1 else offset_idx)
                offset_res = offset_res + axis[idx] * factor_len
            out_trans_fusion = (tensor_attr.get("5HD_TO_4D_DYN") or
                                (tensor_attr.get("5HD_TO_NHWC_FP") and dyn_util.dynamic_mode))
            if out_trans_fusion:
                offset_res = tvm.div(offset_res, 16)
            return offset_res, extent
        return None, None

    def _c_col_buffer_tile():
        """
        do l0c buffer tile
        buffer tile c1 axis if not split_w scene else w and h axis
        """
        if tensor_attr['l0c_multi_group_flag']:
            return
        cl0_matrix_n = tiling.get("CL0_matrix")[0]
        ddr_c1_index = 3 if (tensor_attr.get("5HD_TO_NHWC_DYN") or tensor_attr.get("5HD_TO_NHWC_FP")) else 2
        c_offset, c_extent = _cal_buffer_tile_offset_extent(c_col, c_ddr, ddr_c1_index, cl0_matrix_n, True)
        if split_w:
            cl0_matrix_m = tiling.get("CL0_matrix")[1] * tiling.get("CL0_matrix")[2]
            ddr_w_index = -2
            w_offset, w_extent = _cal_buffer_tile_offset_extent(c_col, c_ddr, ddr_w_index, cl0_matrix_m, False)
            if not dyn_util.dynamic_mode:
                c_offset, c_extent = None, None
            sch[c_col].buffer_tile(
                (None, 1), (None, None), (None, 1), (c_offset, c_extent),
                (w_offset, w_extent), (None, None), (None, None), (None, None)
            )
            if c_add_bias is not None:
                sch[c_add_bias].buffer_tile(
                    (None, None), (None, None), (None, 1),
                    (None, None), (None, None), (None, None)
                )
        else:
            sch[c_col].buffer_tile(
                (None, 1), (None, None), (c_offset, c_extent), (None, None),
                (None, None), (None, None), (None, None)
            )
            if c_add_bias is not None:
                sch[c_add_bias].buffer_tile(
                    (None, 1), (None, None), (c_offset, c_extent),
                    (None, None), (None, None)
                )

        quant_output = False
        for out_tensor in double_out_tensor:
            if out_tensor.dtype in QUANT_DTYPES:
                quant_output = True
                break
        quant_output = c_ddr.dtype in QUANT_DTYPES or quant_output

        # int32 to int8 (or int4), l0c is [1, hw, 16] ddr is [1, hw, 32] (or [1, hw, 64])
        # the bound should 1*hw*16 in l0c
        if (tensor_attr.get("support_l0c_to_out") and quant_output and c_col.shape[-3] & 1):
            sch[c_col].set_buffer_size(reduce(lambda x, y: x * y, tiling.get("CL0_matrix")[0:4]))

    def _c_ub_buffer_tile():
        if not split_w or tensor_attr.get("support_l0c_to_out"):
            return
        c_ub_matrix_m = tiling.get("CUB_matrix")[1] * tiling.get("CUB_matrix")[2]
        cub_w_index = -2
        w_offset, w_extent = _cal_buffer_tile_offset_extent(c_ub, c_ddr, cub_w_index, c_ub_matrix_m, False)
        ub_list = tensor_map.get("ub_list")
        buffer_tile_tensor_list = []
        if ub_list is not None:
            # len(x.shape) == 5 means tensors calculated during l0c to ub
            buffer_tile_tensor_list = list(filter(lambda x: len(x.shape) == 5, ub_list))
        if bias_add_vector is not None:
            buffer_tile_tensor_list.append(bias_add_vector)
        buffer_tile_tensor_list.append(c_ub)
        for tensor in buffer_tile_tensor_list:
            sch[tensor].buffer_tile(
                (None, None), (None, None), (None, 1),
                (w_offset, w_extent), (None, None)
            )

    ax_core = _bind_core()

    _conv1d_split_tile()
    _double_buffer()
    _emit_insn()
    dyn_util.l1_full_load(c_ddr, sch, tensor_map, tensor_attr)

    def _full_load_bl1_bl0():
        """
        g dimension only loads 1 each time
        """
        attach_dict_b = {}
        compute_path_b = {}
        attach_to_group_axis_in_static = not tiling.get(
            "BL1_shape") and g_extend != 1 and not tensor_attr['l0c_multi_group_flag']
        # NOTE not support l0c_multi_group_flag yet
        attach_to_group_axis_in_dynamic = dyn_util.attach_flag.get("bl1_attach_flag") == 0  # 0 means full_load
        if attach_to_group_axis_in_static or attach_to_group_axis_in_dynamic:
            axs = sch_agent[c_ddr].get_active_scopes()
            ax_g = axs[0]
            _, bl1_at_inner = sch_agent[c_ddr].split(ax_g, factor=1)
            sch[b_l1].compute_at(sch[c_ddr], bl1_at_inner)
            attach_dict_b[sch[b_l1]] = sch[c_ddr]
            compute_path_b[sch[b_l1]] = bl1_at_inner
            if not tiling.get("BL0_matrix"):
                sch[b_col].compute_at(sch[c_ddr], bl1_at_inner)
                attach_dict_b[sch[b_col]] = sch[c_ddr]
                compute_path_b[sch[b_col]] = bl1_at_inner
        return attach_dict_b, compute_path_b

    attach_dict_b, compute_path_b = _full_load_bl1_bl0()

    def _bind_subblock():
        """
        only in mix_l2 scene and tiling_cub is smaller than tiling_cl0.

        bind subblock cann't be enabled when n_l0c is odd, which will exceed L1 buffer.
        """
        # use 2 to check n_l0c is not odd.
        if c_ub is not None and not tensor_attr.get("support_l0c_to_ub") and tiling.get("CUB_matrix")[0] < tiling.get(
                "CL0_matrix")[0] and (tiling.get("CL0_matrix")[0] % 2 == 0):
            mix_axis = list(sch[c_ddr].leaf_iter_vars)[-4]
            mix_outer, _ = sch_agent[c_ddr].split(
                mix_axis, nparts=get_mix_ratio())
            sch[c_ddr].bind(mix_outer, tvm.thread_axis("subBlockIdx.x"))
            sch_agent.update_attach_scope(mix_axis, mix_outer)

    _bind_subblock()

    if tiling.get('A_overhead_opt_flag') or tiling.get('B_overhead_opt_flag'):
        _allocate_apply()
    else:
        sch_agent.apply()

    _handle_workspace()
    _set_bl0_bound()
    _c_col_buffer_tile()
    _c_ub_buffer_tile()

    tensors = {}
    tensors['a_l1'] = a_l1
    tensors['c_col'] = c_col
    _do_preload(sch, tensors, tiling, preload_dict)
    print_ir_conv("final IR", sch, True)
    tensor_workspace_list = tensor_map.get("tensor_workspace_list", [])
    if dyn_util.dynamic_mode == 'binary':
        get_context().get_current_compute().get_current_schedule().add(
            "_build_config", {"enable_branch_eliminator_else_case": False})

    context = op_context.get_context()
    if context.get_addition("support_binary_constant") and context.get_addition("enable_binary_constant"):
        for var_name, var_value in tiling_data.items():
            sch.set_var_range(get_te_var(var_name).get_tvm_var(), var_value, var_value)

    double_out_tensor.clear()
    tiling.clear()
    return tensor_workspace_list
