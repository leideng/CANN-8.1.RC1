#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Tool functions of conv2d schedule.
"""
from collections import deque
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe import tvm

WINOGRAD_POST_TAG = "cub_wino_post"
WINOGRAD_RES_TAG = "conv_wino_res"


def get_dsl_insn(tensor):
    """
    When do UB Fusion, remove the suffix after "|"
    druing instruction mapping for tensor of the eltwise type
    Example:
    param tensor: elewise_single_cast|not_auto_cast
    return: elewise_single_cast
    """
    tag = tensor.op.tag
    if tensor.op.tag.find("|") != -1:
        insn = tag.split("|")[0]
    else:
        insn = tag
    return insn


def ceil_div(num_a, num_b):
    """
    Do upper division.
    """
    if num_b == 0:
        err_man.raise_err_specific("conv2d", "division by zero")
    return (num_a + num_b - 1) // num_b


def ceil(num_a, num_b):
    """
    Do upper align.
    """
    if num_b == 0:
        err_man.raise_err_specific("conv2d", "division by zero")
    return (num_a + num_b - 1) // num_b*num_b


def get_src_tensor(tensor):
    """
    Get the source tensor of input tensor.
    """
    src_tensor = tensor.op.input_tensors[0]
    return src_tensor


def is_placeholder(tensor):
    """
    Check whether the input tensor is a placeholder.
    """
    if tensor.op.input_tensors:
        return False
    return True


def is_elewise(tensor):
    """
    Check whether the input tensor is a eltwise op.
    """
    if tensor.op.tag.startswith("elewise_") or tensor.op.tag == "unknown_broadcast" \
        or tensor.op.tag.startswith("emit_insn_elewise_"):
        return True
    return False


def is_cub_wino_post_tensor(tensor):
    """
    Check whether tensor is cub winopost tensor.
    """
    return clear_suffix(tensor.op.tag) == WINOGRAD_POST_TAG


def is_wino_res_tensor(tensor):
    """
    Check whether tensor is wino res tensor.
    """
    return clear_suffix(tensor.op.tag) == WINOGRAD_RES_TAG


def is_quantconv2d_eltwise_fusion_tensor(tensor):
    """ Check whether tensor is quant_conv ub fusion tensor.
    """
    return tensor.op.tag in ["one_shape_broadcast", "dequant_remove_pad"]
           

def is_shape_equal(tensor_a, tensor_b):
    """
    Compare the shape of two input tensors.
    """
    if len(tensor_a.shape) != len(tensor_b.shape):
        return False

    for i, item_a in enumerate(tensor_a.shape):
        item_b = tensor_b.shape[i]
        if not tvm.tvm.tir.analysis.expr_deep_equal(item_a, item_b):
            return False

    return True


def delete_op(del_op, body_ops, sch, dtype=None):
    """
    delete op from body_ops and inline the dst_buffer tensor.

    Parameters
    ----------
    del_op: str
        Op name.
    body_ops: dict
        A dict that contains all ops in the dataflow.
    sch:
        Schedule.
    dtype: str
        Optional dtype constraint.

    Returns
    -------
    None
    """
    for i, j in enumerate(body_ops):
        if dtype:
            if j["op"] == del_op and j["dst_buffer"].dtype == dtype:
                sch[j["dst_buffer"]].compute_inline()
                del body_ops[i]
                break
        else:
            if j["op"] == del_op:
                sch[j["dst_buffer"]].compute_inline()
                del body_ops[i]
                break


def search_op(res, op_tag):
    """
    Search certain op according to op_tag.
    """
    tensor_queue = deque()
    tensor_queue.append(res)
    while tensor_queue:
        src_tensor = tensor_queue.popleft()
        tag = src_tensor.op.tag

        if tag in ("convolution_c_col", "convolution_c_col_bias"):
            break

        if op_tag == tag:
            return src_tensor

        if src_tensor.op.input_tensors:
            append_list = list(i for i in src_tensor.op.input_tensors)
            append_list.reverse()
            tensor_queue.extend(append_list)
    return None


def clear_suffix(name):
    """
    Clear the number suffix of op name or tag.
    """
    if not isinstance(name, str):
        err_man.raise_err_specific("conv2d", f"clear_suffix input is {type(name)}, which should be str type")

    name_str_list = name.split("_")

    if not name_str_list:
        return name
    if name_str_list and not name_str_list[-1].isdigit():
        return name
    return "_".join(name_str_list[:-1])


def get_fixpipe_tag_list():
    """
    Config the op tag for fixpipe res tensors.
    In fixpipe verison, fixpipe_tag_list refers to the tag of tensors that mark "fixpipe_op" pragma for emit insn,
    which is also conv_res in single_op, fixpipe res in fixpipe fusion and cub in ub fusion.
    In non-fixpipe version, fixpipe_tag_list refers to the tag of last tensor in conv2d_compute.
    """

    if is_support_fixpipe():
        return ["convolution_res_fp32_conv2d",
                "convolution_C",
                "fixpipe_reform",
                "dequant_vector"]
    # add "convolution_cub" for NCHW transdata post fusion.
    return ["convolution_C", "convolution_cub", "dequant_vector"]


class ConvIRSimplification:
    """
    Class for IR simplification
    """
    def __init__(self, schedule, tensor_map, open_al0_bound_check=False):
        """
        init function for ir simplification.

        Parameters:
        schedule: tvm schedule, schedule of conv2d ops.
        tensor_map: dict, tvm tensor map dict.
        """
        self.schedule = schedule
        self.tensor_map = tensor_map
        self.open_al0_bound_check = open_al0_bound_check

    def ir_simplify(self):
        "ir simplify api function."
        self._bound_check_simplify()

    def _bound_check_simplify(self):
        """
        skip unnessary bound check for all the aixes of the given tensor.
        Note that: the axies of the given tensors must be split without tail blocks:
        for exampe:
        To skip bound check of al0, all of m_dim->ml1->mAL0 aixes should be factor of its outer aix,
        or be insured by padding extra data using shift_inward / round_up.
        """
        skip_bound_tensor_list = ("al0", "bl0",)
        if self.open_al0_bound_check:
            skip_bound_tensor_list = ("bl0",)
        for tensor_name in skip_bound_tensor_list:
            var = self.tensor_map.get(tensor_name, None)
            if var is not None:
                self.schedule[var].skip_bound_check()