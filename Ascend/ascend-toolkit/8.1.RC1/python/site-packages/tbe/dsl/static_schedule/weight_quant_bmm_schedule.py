#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
weight_quant_batchmatmul schedule
"""
from functools import reduce
from tbe import tvm
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.buildcfg import build_config
from tbe.common.context import op_context
from tbe.dsl.compute.util import int_ceil_div
from tbe.dsl.static_schedule.util import get_all_tensor
from tbe.dsl.compute.cube_util import shape_to_list
from tbe.dsl.unify_schedule.gemm_tiling_util import transfer_tiling
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.static_schedule.conv_util import BinaryUtil
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import VarManage

_MAD_INT32_K_THRESHOLD = 4


def weight_quant_bmm_schedule(res, sch_list, dynamic_para=None):
    sch_obj = WQBmmSchedule(res, sch_list, dynamic_para)
    return sch_obj.weight_quant_bmm_schedule()


class WQBmmSchedule(object):

    def __init__(self, res, sch_list, dynamic_para=None):
        self.sch = sch_list[0]
        self.res = res
        self.all_tensor = get_all_tensor(res)
        self.compute_tensors, self.placeholder_tensors, self.output_tensors = self.all_tensor
        self.l1_tensors_list = []
        self.l0a_tensors_list = []
        self.l0b_tensors_list = []
        self.l0c_tensors_list = []
        self.bt_tensors_list = []
        self.fb_tensors_list = []
        self.compute_inline_tensors = []
        self.tiling = {}
        self.k_outer = None
        self.n_emit = None
        self.input_dict = {}
        self.dynamic_para = dynamic_para
        # k_l0_bound decide the mad size of int32mad,chose 4 by experience
        self.k_l0_bound = 4 if not dynamic_para else get_te_var("k_l0").get_tvm_var()
        self.binary_schedule = WeightQuantBmmBinaryDynamic(self.sch)
        self.attach_flag = {}
        self.double_buffer_flag = {}

    def weight_quant_bmm_schedule(self):
        self._get_tensors_list()
        self._get_input_info()
        self._set_scope()
        self._get_tiling()
        self._set_buffer_size()
        self._do_compute_at()
        self._do_buffer_align()
        self._do_emit_insn()
        self._double_buffer()
        self._do_buffer_tile()
        self._do_compute_inline()
        if self.dynamic_para:
            self.binary_schedule.set_tiling_var_range(self.sch)
        return self.sch

    def _do_buffer_tile(self):
        block_reduce = 16
        k_bl0 = self.tiling.get('BL0_matrix')[0] * block_reduce
        g_extent = int_ceil_div(k_bl0, self.k_l0_bound * block_reduce)
        self.sch[self.compute_tensors.get("tensor_weight_fp16")].buffer_tile(
            (None, None), (None, g_extent), (None, None), (None, None), (None, None), (None, None))

    def _set_buffer_size(self):
        if self.dynamic_para:
            self.sch.sequential_malloc(tbe_platform_info.scope_cbuf)
            self.sch.sequential_malloc(tbe_platform_info.scope_ca)
            self.sch.sequential_malloc(tbe_platform_info.scope_cb)
            self.sch.sequential_malloc(tbe_platform_info.scope_cc)
            self.sch.sequential_malloc(tbe_platform_info.scope_ubuf)
            self.sch.sequential_malloc(tbe_platform_info.scope_fb)
            bound_l1a = self.tiling.get("AL1_shape")[0] * self.tiling.get("AL1_shape")[1] * self.tiling.get(
                "AL0_matrix")[0] * self.tiling.get("AL0_matrix")[2]
            self.sch[self.compute_tensors.get("tensor_a_nz_fp16")].set_buffer_size(bound_l1a)
            bound_l1b = self.tiling.get("BL1_shape")[0] * self.tiling.get("BL1_shape")[1] * self.tiling.get(
                "BL0_matrix")[1] * self.tiling.get("BL0_matrix")[2]
            self.sch[self.compute_tensors.get("tensor_b_nz_int8")].set_buffer_size(bound_l1b)
            bound_l0c = reduce(lambda x, y: x * y, self.tiling.get("CL0_matrix")) * self.tiling.get("BL1_shape")[1]
            self.sch[self.compute_tensors.get("tensor_mad_fp32")].set_buffer_size(bound_l0c)
            bound_l0b = reduce(lambda x, y: x * y, self.tiling.get("BL0_matrix"))
            self.sch[self.compute_tensors.get("tensor_weight_fp16")].set_buffer_size(bound_l0b)
            self.sch[self.compute_tensors.get("tensor_mad_int32")].set_buffer_size(bound_l0b)
            self.sch[self.compute_tensors.get("tensor_b_zn_fp16")].set_buffer_size(bound_l0b)
            bound_l0a_int8 = reduce(lambda x, y: x * y,
                                    self.tiling.get("AL0_matrix")[1:]) * self.tiling.get("AL0_matrix")[1]
            bound_l0a_fp16 = reduce(lambda x, y: x * y, self.tiling.get("AL0_matrix"))
            self.sch[self.compute_tensors.get("tensor_a_zz_fp16")].set_buffer_size(bound_l0a_fp16)
            self.sch[self.compute_tensors.get("tensor_a_zz_int8")].set_buffer_size(bound_l0a_int8)
            # diag matrix shape is 32 * 32
            bound_diag = 32 * 32
            self.sch[self.compute_tensors.get("tensor_a_nz_int8")].set_buffer_size(bound_diag)
            bound_bias = self.tiling.get("BL1_shape")[1] * self.tiling.get("BL0_matrix")[1] * self.tiling.get(
                "BL0_matrix")[2]
            if self.compute_tensors.get("tensor_bias_int32") is not None:
                self.sch[self.compute_tensors.get("tensor_bias_int32")].set_buffer_size(bound_bias)
            if self.compute_tensors.get("tensor_bias_fp32") is not None:
                self.sch[self.compute_tensors.get("tensor_bias_fp32")].set_buffer_size(bound_bias)
            if self.compute_tensors.get("tensor_deq_scale_l1") is not None:
                self.sch[self.compute_tensors.get("tensor_deq_scale_l1")].set_buffer_size(bound_bias)
            self.sch[self.compute_tensors.get("tensor_b_zn_fp16")].reused_by(
                self.compute_tensors.get("tensor_b_zn_int8"))
            bound_bt = self.tiling.get("BL0_matrix")[1] * self.tiling.get("BL0_matrix")[2]
            if self.compute_tensors.get("tensor_bias_in32_bt") is not None:
                self.sch[self.compute_tensors.get("tensor_bias_int32_bt")].set_buffer_size(bound_bt)

    def _get_input_info(self):
        tensor_mad_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        a_ori_shape = shape_to_list(tensor_mad_fp32.op.attrs["a_ori_shape"])
        b_ori_shape = shape_to_list(tensor_mad_fp32.op.attrs["b_ori_shape"])
        format_a = tensor_mad_fp32.op.attrs["format_a"]
        format_b = tensor_mad_fp32.op.attrs["format_b"]
        trans_a = bool(tensor_mad_fp32.op.attrs["trans_a"].value)
        trans_b = bool(tensor_mad_fp32.op.attrs["trans_b"].value)
        fixpipe_op_dict = tensor_mad_fp32.op.attrs["fixpipe_op_dict"]
        ori_m, ori_k = a_ori_shape[-2], a_ori_shape[-1]
        if trans_a:
            ori_m, ori_k = ori_k, ori_m
        ori_n = b_ori_shape[-2] if trans_b else b_ori_shape[-1]
        self.input_dict = {
            "a_ori_shape": a_ori_shape,
            "b_ori_shape": b_ori_shape,
            "format_a": format_a,
            "format_b": format_b,
            "trans_a": trans_a,
            "trans_b": trans_b,
            "ori_m": ori_m,
            "ori_k": ori_k,
            "ori_n": ori_n,
            "fixpipe_op_dict": {
                "quant_scale_0": fixpipe_op_dict.get("quant_scale_0").value,
                "relu_weight_0": fixpipe_op_dict.get("relu_weight_0").value,
                "relu_weight_1": fixpipe_op_dict.get("relu_weight_1").value,
                "quant_scale_1": fixpipe_op_dict.get("quant_scale_1").value,
                "eltwise_src": fixpipe_op_dict.get("eltwise_src").value
            }
        }

    def _print_ir(self, process, sch):
        '''
        print ir for input sch
        :param process: tag
        :param sch: schedule
        :return: IR process
        '''
        with build_config():
            start = process + " IR start"
            end = process + " IR end\n"
            sch = sch.normalize()
            print(start)
            bounds = tvm.schedule.InferBound(sch)
            stmt = tvm.ScheduleOps(sch, bounds, True)
            print(stmt)
            print(end)

    def _do_buffer_align(self):
        tensor_l0c_int32 = self.compute_tensors.get("tensor_mad_int32")
        # l0c int32 shape (shape_batch, shape_kg, shape_n1, shape_m1, shape_m0, shape_n0)
        align_args = [
            (1, 1),
            (1, 1),
            (1, 1),
            (1, 1),
            (1, 16),
            (1, 16),
            (1, 1),
            (1, 32),
        ]
        if not self.input_dict.get("trans_b"):
            align_args[2] = (1, 2)
        self.sch[tensor_l0c_int32].buffer_align(*align_args)

        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        # l0c fp32 shape (shape_batch_a, shape_n1, shape_m1, shape_m0, shape_n0)
        align_args = [(1, 1), (1, 1), (1, 1), (1, 16), (1, 16), (1, 1), (1, 16)]
        self.sch[tensor_l0c_fp32].buffer_align(*align_args)
        tensor_b_nz_int8 = self.compute_tensors.get('tensor_b_nz_int8')
        tensor_b_zn_int8 = self.compute_tensors.get('tensor_b_zn_int8')
        if not self.input_dict.get('trans_b'):
            self.sch[tensor_b_zn_int8].buffer_align((1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 32))
            self.sch[tensor_b_nz_int8].buffer_align((1, 1), (1, 1), (1, 2), (1, 1), (1, 32))

    def _do_compute_inline(self):
        for tensor in self.compute_inline_tensors:
            if tensor is not None:
                self.sch[tensor].compute_inline(instant=True)

    def _get_fb_tensors(self):
        tensor_weight_fp16 = self.compute_tensors.get("tensor_weight_fp16")
        if tensor_weight_fp16.op.attrs.get("pre_conv") == "VS322F16":
            l1_deq_scale = self.sch.cache_read(tensor_weight_fp16.op.input_tensors[1], tbe_platform_info.scope_cbuf,
                                               [tensor_weight_fp16])
            fb_deq_scale = self.sch.cache_read(l1_deq_scale, tbe_platform_info.scope_fb0, [tensor_weight_fp16])
            self.l1_tensors_list.append(l1_deq_scale)
            self.fb_tensors_list.append(fb_deq_scale)
            self.compute_tensors["tensor_deq_scale_l1"] = l1_deq_scale

    def _get_bt_tensors(self):
        bt_bias_int32 = self.sch.cache_read(self.compute_tensors.get("tensor_bias_int32"), tbe_platform_info.scope_bt,
                                            [self.compute_tensors.get("tensor_mad_int32")])
        self.l1_tensors_list.append(self.compute_tensors.get("tensor_bias_int32"))
        self.compute_tensors["tensor_bias_int32_bt"] = bt_bias_int32
        self.bt_tensors_list.append(bt_bias_int32)
        if self.compute_tensors.get("tensor_bias_fp32") is not None:
            self.l1_tensors_list.append(self.compute_tensors.get("tensor_bias_fp32"))
            self.compute_tensors["tensor_bias_fp32_l1"] = self.compute_tensors.get("tensor_bias_fp32")
            if self.compute_tensors.get("tensor_bias_fp32_cast", None) is None:
                bt_bias_fp32 = self.sch.cache_read(self.compute_tensors.get("tensor_bias_fp32"),
                                                   tbe_platform_info.scope_bt,
                                                   [self.compute_tensors.get("tensor_mad_fp32")])
                self.bt_tensors_list.append(bt_bias_fp32)
                self.compute_tensors["tensor_bias_fp32_bt"] = bt_bias_fp32
            else:
                self.bt_tensors_list.append(self.compute_tensors.get("tensor_bias_fp32_cast"))
                self.compute_tensors["tensor_bias_fp32_bt"] = self.compute_tensors.get("tensor_bias_fp32_cast")

    def _get_tensors_list(self):
        self.l1_tensors_list = [
            self.compute_tensors.get("tensor_a_nz_int8"),
            self.compute_tensors.get("tensor_b_nz_int8"),
            self.compute_tensors.get("tensor_a_nz_fp16"),
            self.compute_tensors.get("tensor_weight_fp16"),
        ]
        self._get_fb_tensors()
        self._get_bt_tensors()
        self.l0a_tensors_list = [
            self.compute_tensors.get("tensor_a_zz_int8"),
            self.compute_tensors.get("tensor_a_zz_fp16")
        ]
        self.l0b_tensors_list = [
            self.compute_tensors.get("tensor_b_zn_int8"),
            self.compute_tensors.get("tensor_b_zn_fp16")
        ]
        self.l0c_tensors_list = [
            self.compute_tensors.get("tensor_mad_int32"),
            self.compute_tensors.get("tensor_mad_fp32")
        ]
        self.compute_inline_tensors.append(self.compute_tensors.get("tensor_fixpipe_fp16"))

    def _set_tensor_scope(self, tensors, scope):
        for tensor in tensors:
            if tensor is not None:
                self.sch[tensor].set_scope(scope)

    def _set_scope(self):
        self._set_tensor_scope(self.l1_tensors_list, tbe_platform_info.scope_cbuf)
        self._set_tensor_scope(self.l0a_tensors_list, tbe_platform_info.scope_ca)
        self._set_tensor_scope(self.l0b_tensors_list, tbe_platform_info.scope_cb)
        self._set_tensor_scope(self.l0c_tensors_list, tbe_platform_info.scope_cc)
        self._set_tensor_scope(self.bt_tensors_list, tbe_platform_info.scope_bt)

    def _set_context(self, context):
        """
        get cache_tiling result with static shape input
        """
        tensor_mad_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        format_out = tensor_mad_fp32.op.attrs["format_out"]
        bias_ori_shape = tensor_mad_fp32.op.attrs["q_bias_ori_shape"]
        shape_out = tensor_mad_fp32.op.attrs["shape_out"]

        context.add_addition("static_cache_tiling", 1)
        context.add_addition("input_a_shape", self.input_dict.get("a_ori_shape"))
        context.add_addition("input_b_shape", self.input_dict.get("b_ori_shape"))
        context.add_addition("output_y_shape", shape_to_list(shape_out))
        context.add_addition("input_a_ori_shape", self.input_dict.get("a_ori_shape"))
        context.add_addition("input_b_ori_shape", self.input_dict.get("b_ori_shape"))
        context.add_addition("output_y_ori_shape", [])
        context.add_addition("input_a_dtype", "float16")
        context.add_addition("input_b_dtype", "int8")
        context.add_addition("output_y_dtype", "float16")
        context.add_addition("input_a_format", self.input_dict.get("format_a"))
        context.add_addition("input_b_format", self.input_dict.get("format_b"))
        context.add_addition("output_y_format", format_out)
        context.add_addition("trans_a", self.input_dict.get("trans_a"))
        context.add_addition("trans_b", self.input_dict.get("trans_b"))
        context.add_addition("bias_shape", shape_to_list(bias_ori_shape))
        context.add_addition("bias_dtype", "float32")
        context.add_addition("bias_format", "ND")
        context.add_addition("bias_ori_shape", shape_to_list(bias_ori_shape))
        context.add_addition("binary_constant_type", "weight_quant_batchmatmul")

        # add_compile_info
        add_compile_info("fixpipe_op_dict", self.input_dict.get("fixpipe_op_dict"))
        add_compile_info("binary_mode_flag", True)
        add_compile_info("binary_constant_flag", True)
        tensor_weight_fp16 = self.compute_tensors.get("tensor_weight_fp16")
        if tensor_weight_fp16.op.attrs.get("pre_conv") == "VS322F16":
            add_compile_info("vector_pre_conv_mode", True)
        else:
            add_compile_info("vector_pre_conv_mode", False)
        add_compile_info("binary_attrs", {
            "bias_flag": True,
            "nd_flag": True,
            "split_k_flag": False,
            "zero_flag": False,
            "weight_nz": False
        })
        add_compile_info("dynamic_mode", "weight_quant_bmm")
        add_compile_info("fused_double_operand_num", 0)

    def _get_tiling(self):
        # get tiling from tilingcase
        if self.dynamic_para:
            tiling = self.dynamic_para.get('tiling_strategy')
            # updata tiling in binary dynamic scene
            self.tiling = self.binary_schedule.config_cache_tiling(tiling)
        else:
            # calculate the info dict of matmul
            context = op_context.get_context()
            self._set_context(context)
            self.tiling = transfer_tiling(context, op_type="weight_quant_batchmatmul")

        block = 16
        ori_m = self.input_dict.get("ori_m")
        ori_k = self.input_dict.get("ori_k")
        ori_n = self.input_dict.get("ori_n")
        m_l0 = self.tiling.get('AL0_matrix')[0]
        n_l0 = self.tiling.get('BL0_matrix')[1]
        k_l0 = self.tiling.get('AL0_matrix')[1]
        n_dim = self.tiling.get('block_dim')[1]
        m_dim = self.tiling.get('block_dim')[2]
        m1 = int_ceil_div(int_ceil_div(ori_m, block * m_l0), m_dim)
        n1 = int_ceil_div(int_ceil_div(ori_n, block * n_l0), n_dim)
        k1 = int_ceil_div(ori_k, block * k_l0) * block * k_l0
        if self.tiling.get('AL1_shape') == []:
            self.tiling['AL1_shape'] = [k1, m1, 1, 1]
        if self.tiling.get('BL1_shape') == []:
            self.tiling['BL1_shape'] = [k1, n1, 1, 1]

        if self.dynamic_para :
            self.attach_flag["al1_attach_flag"] = self.tiling.get("attach_at_flag").get("al1_attach_flag")
            self.attach_flag["bl1_attach_flag"] = self.tiling.get("attach_at_flag").get("bl1_attach_flag")
            self.attach_flag["abkl1_attach_flag"] = self.tiling.get("attach_at_flag").get("abkl1_attach_flag")
        else:
            self.attach_flag["al1_attach_flag"] = \
                1 if self.tiling.get('AL1_shape')[0] >= self.input_dict.get("ori_k") else 0
            self.attach_flag["bl1_attach_flag"] = \
                1 if self.tiling.get('BL1_shape')[0] >= self.input_dict.get("ori_k") else 0
            self.attach_flag["abkl1_attach_flag"] = \
                1 if self.tiling.get('AL1_shape')[0] <= self.tiling.get('BL1_shape')[0] else 0

    def _do_compute_at_l0_fp16(self, l0a_attach_axis, l0b_attach_axis):
        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        # fp16 l0a l0b
        self.sch[self.compute_tensors.get("tensor_a_zz_fp16")].compute_at(self.sch[tensor_l0c_fp32], l0a_attach_axis)
        self.sch[self.compute_tensors.get("tensor_b_zn_fp16")].compute_at(self.sch[tensor_l0c_fp32], l0b_attach_axis)

    def _do_compute_at_al1(self, al1_attach_axis, single_core_axis):
        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        # int8 al1 full load
        # fp16 al1
        if not self.attach_flag["al1_attach_flag"]:
            self.sch[self.compute_tensors.get("tensor_a_nz_fp16")].compute_at(self.sch[tensor_l0c_fp32],
                                                                              al1_attach_axis)
        else:
            self.sch[self.compute_tensors.get("tensor_a_nz_fp16")].compute_at(self.sch[self.res], single_core_axis[1])

    def _do_compute_at_bl1(self, bl1_attach_axis, single_core_axis, l0b_attach_axis):
        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        self.sch[self.compute_tensors.get("tensor_b_nz_int8")].compute_at(self.sch[tensor_l0c_fp32],
                                                                            bl1_attach_axis)
        self.sch[self.compute_tensors.get("tensor_weight_fp16")].compute_at(self.sch[tensor_l0c_fp32],
                                                                            l0b_attach_axis)

        if self.compute_tensors.get("tensor_deq_scale_l1") is not None:
            self.sch[self.compute_tensors.get("tensor_deq_scale_l1")].compute_at(self.sch[self.res],
                                                                                 single_core_axis[2])
        self.sch[self.compute_tensors.get("tensor_bias_int32")].compute_at(self.sch[self.res], single_core_axis[2])

    def _check_l0a_buffer_overflow(self):
        fractal_size = 512
        # fp16计算时L0的大小
        m_l0, k_l0 = self.tiling.get('AL0_matrix')[:2]
        pingpong_buffer = self.tiling.get('manual_pingpong_buffer')
        al0_pb = pingpong_buffer.get('AL0_pbuffer')
        tensor_fp16_use = m_l0 * k_l0 * fractal_size * al0_pb
        tensor_int8_use = k_l0 * k_l0 // 2 * fractal_size
        return (tensor_fp16_use +
                tensor_int8_use) >= tbe_platform_info.get_soc_spec("L0A_SIZE")

    def _do_compute_at_bt_fb(self, n_outer, single_core_axis):
        tensor_weight_fp16 = self.compute_tensors.get("tensor_weight_fp16")
        tensor_mad_int32 = self.compute_tensors.get("tensor_mad_int32")
        tensor_bias_fp32 = self.compute_tensors.get("tensor_bias_fp32", None)
        if self.dynamic_para is None and self.tiling.get('BL1_shape')[1] == 1:
            if self.fb_tensors_list:
                self.sch[self.fb_tensors_list[0]].compute_at(self.sch[self.res], single_core_axis[2])
            # int32 bias
            if tensor_bias_fp32 is None:
                self.sch[self.compute_tensors.get("tensor_bias_int32_bt")].compute_at(
                    self.sch[self.res], single_core_axis[2])
            else:
                self.sch[self.compute_tensors.get("tensor_bias_int32_bt")].compute_at(
                    self.sch[tensor_mad_int32], tensor_mad_int32.op.axis[1])
        else:
            if self.fb_tensors_list:
                self.sch[self.fb_tensors_list[0]].compute_at(self.sch[tensor_weight_fp16], n_outer)
            # int32 bias
            self.sch[self.compute_tensors.get("tensor_bias_int32_bt")].compute_at(self.sch[tensor_mad_int32],
                                                                                  tensor_mad_int32.op.axis[1])

    def _do_compute_at_l0_int8(self, single_core_axis):
        tensor_weight_fp16 = self.compute_tensors.get("tensor_weight_fp16")
        tensor_mad_int32 = self.compute_tensors.get("tensor_mad_int32")
        n_outer, _ = self.sch[tensor_weight_fp16].split(tensor_weight_fp16.op.axis[2], self.tiling.get('BL0_matrix')[1])
        # l0c int32
        self.sch[self.compute_tensors.get("tensor_mad_int32")].compute_at(self.sch[tensor_weight_fp16], n_outer)
        self._do_compute_at_bt_fb(n_outer, single_core_axis)
        # int8 l0a l0b
        # l0a空间足够时，可以将该tensor缓存在L0A中
        if self.dynamic_para is None and self._check_l0a_buffer_overflow():
            self.sch[self.compute_tensors.get("tensor_a_zz_int8")].compute_at(
                self.sch[tensor_mad_int32], tensor_mad_int32.op.axis[1])
        self.sch[self.compute_tensors.get("tensor_b_zn_int8")].compute_at(self.sch[tensor_mad_int32],
                                                                          tensor_mad_int32.op.axis[1])

    def _do_compute_at(self):
        dim_axis, single_core_axis, inner_axis = self._out_nd_tensor_split()
        self._bind_core(dim_axis)
        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        self.sch[tensor_l0c_fp32].compute_at(self.sch[self.res], inner_axis[1])
        self.k_outer = self._l0c_tensor_split()
        bl1_attach_axis, al1_attach_axis, l0b_attach_axis, l0a_attach_axis = self.k_outer
        self._do_compute_at_l0_fp16(l0a_attach_axis, l0b_attach_axis)
        self._do_compute_at_al1(al1_attach_axis, single_core_axis)
        self._do_compute_at_bl1(bl1_attach_axis, single_core_axis, l0b_attach_axis)
        self._do_compute_at_l0_int8(single_core_axis)

        if self.compute_tensors.get("tensor_bias_fp32") is not None:
            self.sch[self.compute_tensors.get("tensor_bias_fp32_bt")].compute_at(self.sch[tensor_l0c_fp32],
                                                                                 l0b_attach_axis)
            self.sch[self.compute_tensors.get("tensor_bias_fp32_l1")].compute_at(self.sch[self.res],
                                                                                 single_core_axis[-1])

    def _bind_core(self, dim_axis):
        block = tvm.thread_axis("blockIdx.x")
        self.sch.bind_axes(dim_axis, block)

    def _l0c_tensor_split(self):
        '''
        shape=(b, n1, m1, m0, n0)
        '''
        block_reduce = 16
        l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        b, n, m, m0, n0 = self.sch[l0c_fp32].op.axis
        n_bl1 = self.tiling.get('BL1_shape')[1]
        n_bl0 = self.tiling.get("BL0_matrix")[1]
        if self.dynamic_para:
            n_outer, n = self.sch[l0c_fp32].split(n, n_bl0 * block_reduce)
        else:
            n_outer, n = self.sch[l0c_fp32].split(n, nparts=n_bl1)
        self.n_emit = n
        k_outer, k_inner = self.sch[l0c_fp32].op.reduce_axis
        k_l0 = self.tiling.get("AL0_matrix")[1]
        k_al1 = self.tiling.get("AL1_shape")[0]
        k_bl1 = self.tiling.get('BL1_shape')[0]
        k_outer, k_l0_inner = self.sch[l0c_fp32].split(k_outer, k_l0)
        k_al1 = k_al1 // (k_l0 * block_reduce)
        k_bl1 = k_bl1 // (k_l0 * block_reduce)
        l0_attach_axis = None
        al1_attach_axis = None
        bl1_attach_axis = None
        if not self.attach_flag.get("abkl1_attach_flag"):
            if self.dynamic_para:
                k_outer, k_al1_inner = self.sch[l0c_fp32].split(k_outer, k_al1)
                k_al1_inner, k_bl1_inner = self.sch[l0c_fp32].split(k_al1_inner, k_bl1)
            else:
                k_outer, k_bl1_inner = self.sch[l0c_fp32].split(k_outer, k_bl1)
                k_outer, k_al1_inner = self.sch[l0c_fp32].split(k_outer, int_ceil_div(k_al1, k_bl1))
            l0_attach_axis = k_bl1_inner
            al1_attach_axis = k_outer
            bl1_attach_axis = k_al1_inner
        else:
            if self.dynamic_para:
                k_outer, k_bl1_inner = self.sch[l0c_fp32].split(k_outer, k_bl1)
                k_bl1_inner, k_al1_inner = self.sch[l0c_fp32].split(k_bl1_inner, k_al1)
            else:
                k_outer, k_al1_inner = self.sch[l0c_fp32].split(k_outer, k_al1)
                k_outer, k_bl1_inner = self.sch[l0c_fp32].split(k_outer, int_ceil_div(k_bl1, k_al1))
            l0_attach_axis = k_al1_inner
            al1_attach_axis = k_bl1_inner
            bl1_attach_axis = k_outer
        l0b_attach_axis, l0a_attach_axis = self.sch[l0c_fp32].split(l0_attach_axis, 1)
        self.sch[l0c_fp32].reorder(bl1_attach_axis, al1_attach_axis, n_outer, l0b_attach_axis,
                                   l0a_attach_axis, b, n, m, m0, n0, k_l0_inner, k_inner)
        return [bl1_attach_axis, al1_attach_axis, l0b_attach_axis, l0a_attach_axis]

    def _out_nd_tensor_split(self):
        block_out = 16
        axis_b, axis_m, axis_n = self.sch[self.res].op.axis
        batch_dim, n_dim, m_dim, _ = self.tiling.get("block_dim")
        n_l0, m_l0 = self.tiling.get("CL0_matrix")[:2]
        m_al1 = self.tiling.get("AL1_shape")[1]
        n_bl1 = self.tiling.get("BL1_shape")[1]
        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        shape_n1, shape_m1 = shape_to_list(tensor_l0c_fp32.shape)[-4:-2]
        batch_outer, batch_single_core = self.sch[self.res].split(axis_b, nparts=batch_dim)
        if self.dynamic_para:
            m_single_core_factor = get_te_var("m_single_core").get_tvm_var() * m_al1 * m_l0 * block_out
            n_single_core_factor = get_te_var("n_single_core").get_tvm_var() * n_bl1 * n_l0 * block_out
            m_outer, m_single_core = self.sch[self.res].split(axis_m, m_single_core_factor)
            m_single_core, m_l1_inner = self.sch[self.res].split(m_single_core, m_al1 * m_l0 * block_out)
            m_l1_inner, m_l0_inner = self.sch[self.res].split(m_l1_inner, m_l0 * block_out)
            n_outer, n_single_core = self.sch[self.res].split(axis_n, n_single_core_factor)
            n_single_core, n_l1_inner = self.sch[self.res].split(n_single_core, n_bl1 * n_l0 * block_out)
            n_l1_inner, n_l0_inner = self.sch[self.res].split(n_l1_inner, n_bl1 * n_l0 * block_out)
        else:
            m_outer, m_l0_inner = self.sch[self.res].split(axis_m, m_l0 * block_out)
            m_outer, m_l1_inner = self.sch[self.res].split(m_outer, m_al1)
            m_single_core_factor = int_ceil_div(int_ceil_div(shape_m1, m_dim), m_al1 * m_l0)
            m_outer, m_single_core = self.sch[self.res].split(m_outer, m_single_core_factor)
            n_outer, n_l0_inner = self.sch[self.res].split(axis_n, n_bl1 * n_l0 * block_out)
            n_outer, n_l1_inner = self.sch[self.res].split(n_outer, 1)
            n_single_core_factor = int_ceil_div(int_ceil_div(shape_n1, n_dim), n_bl1 * n_l0)
            n_outer, n_single_core = self.sch[self.res].split(n_outer, n_single_core_factor)

        self.sch[self.res].reorder(batch_outer, m_outer, n_outer, batch_single_core, m_single_core, n_single_core,
                                   m_l1_inner, n_l1_inner, m_l0_inner, n_l0_inner)

        dim_axis = (batch_outer, m_outer, n_outer)
        single_core_axis = (batch_single_core, m_single_core, n_single_core)
        inner_axis = (m_l1_inner, n_l1_inner, m_l0_inner, n_l0_inner)
        return dim_axis, single_core_axis, inner_axis

    def _do_emit_insn_l0ab(self):
        for tensor in self.l0a_tensors_list:
            self.sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")
        self.sch[self.compute_tensors.get('tensor_a_zz_int8')].set_value(
            tvm.call_intrin("int8", "tir.likely",
                            self.compute_tensors.get('tensor_a_zz_int8').op.axis[0] >= 0), tvm.const(0, dtype="int8"),
            True)

        tensor_b_zn_int8 = self.compute_tensors.get('tensor_b_zn_int8')
        # 需要做int8的分型转置，n1轴需要切个2出来
        self.sch[tensor_b_zn_int8].pragma(tensor_b_zn_int8.op.axis[0], "loop_with_no_overlap_tensor")
        if not self.input_dict.get('trans_b'):
            _, b_l0b_inner = self.sch[tensor_b_zn_int8].split(tensor_b_zn_int8.op.axis[-3], 2)
            self.sch[tensor_b_zn_int8].emit_insn(b_l0b_inner, "dma_copy", {"is_2dtranspose": 1})
        else:
            self.sch[tensor_b_zn_int8].emit_insn(tensor_b_zn_int8.op.axis[0], "dma_copy")
        tensor_b_zn_fp16 = self.compute_tensors.get('tensor_b_zn_fp16')
        # int8时k_l0的大小与fp16时的k_l0大小可能不一致，需要切分出来，避免精度问题, in8时kl0为4
        _, b_l0b_inner = self.sch[tensor_b_zn_fp16].split(tensor_b_zn_fp16.op.axis[-4], self.k_l0_bound)
        self.sch[tensor_b_zn_fp16].pragma(tensor_b_zn_fp16.op.axis[0], "loop_with_no_overlap_tensor")
        self.sch[tensor_b_zn_fp16].emit_insn(b_l0b_inner, "dma_copy")

    def _do_emit_insn_l1(self):
        nd2nz_dict = {"layout_transform": "nd2nz", "split_select": 1}
        if self.dynamic_para:
            nd2nz_dict = {"layout_transform": "nd2nz", "no_select2if": 1}
        for tensor in self.l1_tensors_list:
            if tensor.op.name == "tensor_weight_fp16":
                leaf_axis = self.sch[tensor].leaf_iter_vars
                self.sch[tensor].pragma(tensor.op.axis[0], "loop_with_no_overlap_tensor")
                self.sch[tensor].emit_insn(leaf_axis[3], "fixpipe_op")
            else:
                self.sch[tensor].pragma(tensor.op.axis[0], "loop_with_no_overlap_tensor")
                self.sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy", nd2nz_dict)

    def _do_emit_insn_l0c(self):
        mad_dict_fp32 = {"mad_pattern": 0, "k_outer": self.k_outer}
        tensor_l0c_fp32 = self.compute_tensors.get("tensor_mad_fp32")
        self.sch[tensor_l0c_fp32].emit_insn(self.n_emit, "mad", mad_dict_fp32)

        tensor_l0c_int32 = self.compute_tensors.get("tensor_mad_int32")
        k_outer = tensor_l0c_int32.op.reduce_axis[0]
        k_outer, _ = self.sch[tensor_l0c_int32].split(k_outer, nparts=1)
        self.sch[tensor_l0c_int32].reorder(k_outer, *tensor_l0c_int32.op.axis[2:])
        mad_dict_int32 = {"mad_pattern": 0, "k_outer": k_outer}
        self.sch[tensor_l0c_int32].emit_insn(tensor_l0c_int32.op.axis[2], "mad", mad_dict_int32)

    def _do_emit_insn_fb_bt(self):
        tensor_bias_fp32 = None
        for tensor in self.bt_tensors_list + self.fb_tensors_list:
            if "fp32" in tensor.op.name:
                tensor_bias_fp32 = tensor
                self.sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy", {"mem_align": 1})
            else:
                self.sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")
        if self.dynamic_para and tensor_bias_fp32 is not None and self.compute_tensors.get(
                "tensor_bias_int32_bt") is not None:
            self.sch[self.compute_tensors.get("tensor_bias_int32_bt")].reused_by(tensor_bias_fp32)

    def _do_emit_insn(self):
        self._do_emit_insn_l0ab()
        self._do_emit_insn_l1()
        self._do_emit_insn_fb_bt()
        self._do_emit_insn_l0c()
        leaf_axis = self.sch[self.res].leaf_iter_vars
        self.sch[self.res].emit_insn(leaf_axis[-2], "fixpipe_op")

    def _is_l0c_overflow(self):
        fractal_size = 16 * 16 * 4
        n_l0, m_l0 = self.tiling.get('CL0_matrix')[:2]
        l0c_db = self.tiling.get('manual_pingpong_buffer').get('CL0_pbuffer')
        total_use = self.k_l0_bound * n_l0 * fractal_size * 2 + l0c_db * n_l0 * m_l0 * fractal_size

        return total_use > tbe_platform_info.get_soc_spec('L0C_SIZE')

    def _double_buffer(self):
        pingpong_buffer = self.tiling.get('manual_pingpong_buffer')
        for tensor in (self.l0a_tensors_list + self.l0b_tensors_list):
            self.sch[tensor].double_buffer()
        if pingpong_buffer.get('AL1_pbuffer') == 2:
            self.sch[self.compute_tensors.get("tensor_a_nz_fp16")].double_buffer()
            self.sch[self.compute_tensors.get("tensor_a_nz_fp16")].preload()
        if pingpong_buffer.get('BL1_pbuffer') == 2:
            self.sch[self.compute_tensors.get("tensor_b_nz_int8")].double_buffer()
            self.sch[self.compute_tensors.get("tensor_b_nz_int8")].preload()
            self.sch[self.compute_tensors.get("tensor_bias_int32")].double_buffer()
            self.sch[self.compute_tensors.get("tensor_bias_int32")].preload()
            self.sch[self.compute_tensors.get("tensor_weight_fp16")].double_buffer()
            if not self.dynamic_para:
                self.sch[self.compute_tensors.get("tensor_weight_fp16")].preload()
            if self.compute_tensors.get("tensor_deq_scale_l1") is not None:
                self.sch[self.compute_tensors.get("tensor_deq_scale_l1")].double_buffer()
                self.sch[self.compute_tensors.get("tensor_deq_scale_l1")].preload()
        # int32 mad
        if pingpong_buffer.get('CL0_pbuffer') == 2:
            self.sch[self.compute_tensors.get("tensor_mad_int32")].double_buffer()
            self.sch[self.compute_tensors.get("tensor_mad_fp32")].double_buffer()


class WeightQuantBmmBinaryDynamic:
    """
    special for dynamic binary
    """

    def __init__(self, sch):
        self.tiling_utils = BinaryUtil.TILING_UTILS
        self._tiling_range = BinaryUtil.TILING_RANGE
        self._sch = sch
        self._binary_tiling_data = None
        self.cache_tiling = {}
        self.binary_nparts_facotr = {}
        self.common_var_range = {
            "range_block_dim": (1, 32),
            "range_64": (1, 64),
            "range_128": (1, 128),
            "range_1024": (1, 1024),
            "range_2": (1, 2),
            "range_1": (0, 1),
            "range_none": (1, None),
            "range_48": (1, 48),
            "range_16_65280": (16, 65280),
            "range_65280": (1, 65280),
        }

    @staticmethod
    def _get_optional_te_var(var_name):
        """get optional te var"""
        return None if not get_te_var(var_name) else get_te_var(var_name).get_tvm_var()

    def set_tiling_var_range(self, sch):
        range_1024 = self.common_var_range.get("range_1024")
        range_64 = self.common_var_range.get("range_64")
        range_128 = self.common_var_range.get("range_128")
        range_block_dim = self.common_var_range.get("range_block_dim")
        range_2 = self.common_var_range.get("range_2")
        range_1 = self.common_var_range.get("range_1")
        range_none = self.common_var_range.get("range_none")
        range_48 = self.common_var_range.get("range_48")
        range_65280 = self.common_var_range.get("range_65280")
        range_16_65280 = self.common_var_range.get("range_16_65280")
        var_range_dict = {
            "batch_dim": range_block_dim,
            "n_dim": range_block_dim,
            "m_dim": range_block_dim,
            "m_single_core": range_1024,
            "n_single_core": range_1024,
            "m_al1": range_1024,
            "n_bl1": (1, 2),
            "cub_n1": (1, 1),
            "n_ub_l0_time": (1, 8),
            "m_l0": range_64,
            "k_l0": range_64,
            "kal0_factor": range_64,
            "kbl0_factor": range_64,
            "kal1_factor": range_64,
            "kbl1_factor": range_64,
            "kl1_times": range_64
        }
        for var, var_range in var_range_dict.items():
            sch.set_var_range(self.cache_tiling.get(var), *var_range)

    def config_cache_tiling(self, tiling):
        """
        config base tiling information for cache tiling
        """

        self._binary_tiling_data = tiling.get("binary_tiling_data")
        self._get_cache_tiling()
        tiling = self._tiling_infer_norange(tiling)
        return tiling

    def _tiling_infer_norange(self, tiling):
        """
        config tiling variable for cache tiling
        """
        tiling['block_dim'] = [
            self.cache_tiling.get('batch_dim'),
            self.cache_tiling.get("n_dim"),
            self.cache_tiling.get("m_dim"), 1
        ]
        tiling.get('AL1_shape')[0] = self.cache_tiling.get("kal1_16") * tiling.get('AL0_matrix')[3]
        tiling.get('AL1_shape')[1] = self.cache_tiling.get("m_al1")
        tiling.get('BL1_shape')[0] = self.cache_tiling.get("kbl1_16") * tiling.get('BL0_matrix')[3]
        tiling.get('BL1_shape')[1] = self.cache_tiling.get("n_bl1")
        tiling.get('AL0_matrix')[0] = self.cache_tiling.get("m_l0")
        tiling.get('CL0_matrix')[1] = self.cache_tiling.get("m_l0")
        tiling.get('AL0_matrix')[1] = self.cache_tiling.get("k_al0")
        tiling.get('BL0_matrix')[0] = self.cache_tiling.get("k_bl0")
        tiling.get('BL0_matrix')[1] = self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1")
        tiling.get('CL0_matrix')[0] = tiling.get('BL0_matrix')[1]
        return tiling

    def _get_cache_tiling(self):
        """
        get cache_tiling
        """
        self.cache_tiling = {
            "batch_dim": get_te_var("batch_dim").get_tvm_var(),
            "batch_single_core": get_te_var("batch_single_core").get_tvm_var(),
            "n_single_core": get_te_var("n_single_core").get_tvm_var(),
            "n_dim": get_te_var("n_dim").get_tvm_var(),
            "n_bl1": get_te_var("n_bl1").get_tvm_var(),
            "n_ub_l0_time": get_te_var("n_ub_l0_time").get_tvm_var(),
            "cub_n1": get_te_var("cub_n1").get_tvm_var(),
            "m_dim": get_te_var("m_dim").get_tvm_var(),
            "m_single_core": get_te_var("m_single_core").get_tvm_var(),
            "m_al1": get_te_var("m_al1").get_tvm_var(),
            "m_l0": get_te_var("m_l0").get_tvm_var(),
            "k_l0": get_te_var("k_l0").get_tvm_var(),
            "k_al0": get_te_var("k_l0").get_tvm_var(),
            "k_bl0": get_te_var("k_l0").get_tvm_var(),
            "kal1_factor": get_te_var("kal1_factor").get_tvm_var(),
            "kbl1_factor": get_te_var("kbl1_factor").get_tvm_var(),
            "kal0_factor": get_te_var("kal0_factor").get_tvm_var(),
            "kbl0_factor": get_te_var("kbl0_factor").get_tvm_var(),
            "kl1_times": get_te_var("kl1_times").get_tvm_var(),
        }
        self.cache_tiling["kal1_16"] = self.cache_tiling.get("kal0_factor") * self.cache_tiling.get("k_l0")
        self.cache_tiling["kbl1_16"] = self.cache_tiling.get("kbl0_factor") * self.cache_tiling.get("k_l0")
