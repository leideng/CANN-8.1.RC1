#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Schedule of conv3d.
"""
from functools import reduce

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils import log
from tbe.common.utils.errormgr import error_manager_util
from tbe.common.utils.errormgr import error_manager_cube as cube_err
from tbe.common.utils.op_util.op_util_cube import decode_tiling_v1_to_v2
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import get_op_context
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.compute import conv3d_compute
from tbe.dsl.compute import util as compute_util
from tbe.dsl.static_schedule.conv_util import BinaryUtil
from tbe.dsl.static_schedule.conv_util import fetch_fixpipe_tensor
from tbe.dsl.static_schedule.conv_util import FIXPIPE_SCOPE_MAP
from tbe.dsl.static_schedule.conv_util import get_inout_dtype

# tiling check
_TILING_FLOAT16_MKN = 16
_VALID_TILING_NUM = 32

MAD_DTYPE_INDEX = 3

class CceConv3dOp:
    """
    cce index op

    Parameters
    ----------
    scope : cal buffer like local.UB

    need_tensorize : if need to doing tensorize when using calculate

    need_pragma : if need to doing paagma when using calculate

    Returns
    -------
    cceop_instance : instance of cceop

    """

    def __init__(self, scope, need_tensorize=True, need_pragma=True):
        self._need_tensorize = need_tensorize
        self._need_pragma = need_pragma
        self._scope = scope
        self._schedule = None
        self._tensor_map = conv3d_compute.Conv3DParam.tensor_map
        self._dim_map = conv3d_compute.Conv3DParam.dim_map
        self._tiling = conv3d_compute.Conv3DParam.tiling
        self._attrs_dict = conv3d_compute.Conv3DParam.attrs_dict
        self._support_l0c_to_out_flag = conv3d_compute.Conv3DParam.support_l0c_to_out_flag
        self.binary_mode = conv3d_compute.Conv3DParam.binary_mode
        self.binary_schedule = None
        self.weight_gm2bl1_flag = 1
        self.fmap_trans_flag = False
        self._res_tensor = None
        self.body_ops = []
        self.input_ops = []
        self.output_ops = []
        self.in_dtype = conv3d_compute.Conv3DParam.tiling_info_dict['a_dtype'].lower()
        self.ub_fusion_flag = self._tensor_map["dsl_flag"]
        self.dsl_flag = self._tensor_map["dsl_flag"]
        self.var_map = conv3d_compute.Conv3DParam.var_map
        self.tiling_case = {}
        self.var_range = {}
        self.tiling_para = {}
        self.axis_dict = {}
        self.buffer_dict = {}
        self.load3d_special_multiply = self._tensor_map.get("load3d_special_multiply")
        self.is_v200_version = self._tensor_map.get("is_v200_version")
        self.support_l1_to_bt = self._tensor_map.get("support_l1_to_bt")
        self.bias_table_flag = self._tensor_map.get("bias_table_flag")
        self.quant_fused_flag = self._tensor_map.get("quant_fused_flag")
        self.quant_bias_flag = self._tensor_map.get("quant_bias_flag")
        self.noquant_bias_flag = self._tensor_map.get("noquant_bias_flag")
        self.requant_multi_group_flag = False
        self.ub_start_tensor = conv3d_compute.Conv3DParam.ub_start_tensor
        self.inline_tensors = conv3d_compute.Conv3DParam.inline_tensors

    @staticmethod
    def _get_value(ele):
        res_ele = [ele.value if isinstance(ele, tvm.tir.IntImm) else ele][0]
        return res_ele

    @staticmethod
    def _int_ceil_div_tvm(num_a, num_b):
        """
        tvm.floordiv result
        """
        return tvm.floordiv((num_a + num_b - 1), num_b)

    @staticmethod
    def _get_cycle_buffer_flag(cycle_buffer_flag_param_dict):
        """
        calculate whether to do cyclebuffer

        Parameters
        ----------
        cycle_buffer_flag_param_dict:

            tiling : tiling_new

            shape_w : filter shape

            shape_fmap : fmap shape

            stride_d : d channel stride

            pad_d : pad of d direction

            l0a_load2d_flag : whether fmap to load2d

            dilation_h : dilation on h direction

            dilation_w : dilation on w direction

        return
        ---------
        cycle_buffer_flag
        """
        tiling = cycle_buffer_flag_param_dict.get('tiling_new')
        if tiling.get("default_tiling"):
            return False
        shape_w = cycle_buffer_flag_param_dict.get('shape_w_ndc1hwc0')
        w_dtype = cycle_buffer_flag_param_dict.get('w_dtype')
        shape_fmap = cycle_buffer_flag_param_dict.get('fmap_shape_ndc1hwc0')
        stride_d = cycle_buffer_flag_param_dict.get('strided')
        pad_d = cycle_buffer_flag_param_dict.get('padd')
        l0a_load2d_flag = cycle_buffer_flag_param_dict.get('l0a_load2d_flag')
        dilation_d = cycle_buffer_flag_param_dict.get('dilationd')
        dilation_h = cycle_buffer_flag_param_dict.get('dilationh')
        dilation_w = cycle_buffer_flag_param_dict.get('dilationw')
        cycle_buffer_flag = False
        filter_d = shape_w[1]
        filter_h = shape_w[3]
        filter_w = shape_w[4]
        fmap_d = shape_fmap[1]
        channel_c1 = shape_fmap[2]
        # -2 is index of d dim tiling param
        d_dim = tiling["block_dim"][-2]
        matrix_ka = tiling["AL0_matrix"][1] * tiling["AL0_matrix"][-2]
        d_out = (fmap_d + pad_d[0] + pad_d[1] - filter_d) // stride_d + 1
        cyc_size = 0
        dilated_k_w = (filter_w - 1) * dilation_w + 1
        dilated_k_h = (filter_h - 1) * dilation_h + 1
        if tiling["AL1_shape"]:
            cyc_size = int(tiling["AL1_shape"][0] * tiling["AL1_shape"][-2] // \
                           (dilated_k_w * dilated_k_h * tbe_platform.CUBE_MKN[w_dtype]['mac'][1]))

        if cyc_size == filter_d * channel_c1:
            cycle_buffer_flag = True

        if l0a_load2d_flag or filter_d <= stride_d or d_out == d_dim:
            cycle_buffer_flag = False

        if matrix_ka != 0 and channel_c1 * filter_h * filter_w % matrix_ka != 0:
            cycle_buffer_flag = False

        if dilation_d != 1:
            cycle_buffer_flag = False

        return cycle_buffer_flag

    @staticmethod
    def _get_elmwise_instr(elm_instr):
        """
        Get the instr for element-wise ops.
        """
        ele_map = {"elewise_single_relu": "vector_relu",
                   "elewise_single_round_d": "vector_conv_round",
                   "elewise_single_VS_max": "vector_maxs",
                   "elewise_single_VS_min": "vector_mins",
                   "elewise_binary_div": "vector_div",
                   "elewise_binary_vcmpv_gt": "vector_gt",
                   "elewise_binary_vcmpv_ge": "vector_ge",
                   "elewise_binary_vcmpv_lt": "vector_lt",
                   "elewise_binary_vcmpv_le": "vector_le",
                   "elewise_binary_vcmpv_eq": "vector_eq",
                   "elewise_binary_vcmpv_ne": "vector_ne",
                   "elewise_binary_cmpsel": "vector_cmpsel",
                   "elewise_binary_add": "vector_add",
                   "elewise_binary_sub": "vector_sub",
                   "elewise_binary_mul": "vector_mul",
                   "elewise_binary_min": "vector_min",
                   "elewise_binary_max": "vector_max",
                   "elewise_binary_or": "vector_or",
                   "elewise_binary_and": "vector_and",
                   "elewise_single_lrelu": "vector_lrelu",
                   "elewise_binary_addrelu": "vector_addrelu",
                   "elewise_binary_subrelu": "vector_subrelu"}
        emit_insn_pragma = ele_map.get(elm_instr)
        if emit_insn_pragma:
            out_instr = emit_insn_pragma
        else:
            out_instr = elm_instr

        return out_instr

    @staticmethod
    def _is_full_load(l1_shape):
        """
        check if full load
        """
        return l1_shape == []

    @staticmethod
    def _is_k_full_load(l1_shape_k, shape_k):
        """
        check if full load
        """
        return l1_shape_k == shape_k

    @staticmethod
    def _is_weight_not_via_l1(l1_shape):
        """
        check if full load
        """
        return l1_shape is None

    def schedule(self, res, spec_node_list, sch_list, dynamic_para=None):
        """
        auto_schedule for cce AI-CORE.
        For now, only one convolution operation is supported.

        Parameters
        ----------
        res : tvm.tensor

        spec_node_list : same as other template in cce_schedule

        sch_list: use sch_list[0] to return conv schedule

        Returns
        -------
        True for sucess, False for no schedule
        """
        # BF16/FP32 only supports dynamic constantization and binary, static compilation fails
        if (self.in_dtype in ("float32", "bfloat16") and not
            (get_op_context().get_addition("is_dynamic_constantization") or self.binary_mode)):
            return True

        tensor_map = self._tensor_map
        dim_map = self._dim_map
        c_ub = tensor_map["c_ub"]

        sch = sch_list[0]
        self._schedule = sch

        color_op = AutoScheduleOp(res)
        self.body_ops = color_op.body_ops
        self.input_ops = color_op.input_ops
        self.output_ops = color_op.output_ops

        self.binary_schedule = BinaryDynamic(sch, self.binary_mode)

        self.ub_fusion_flag = self.ub_fusion_flag and c_ub is not None
        if "fixpipe" in self.output_ops[0].get("op"):
            sch, tensor_map = fetch_fixpipe_tensor(sch, self.body_ops, tensor_map)
            self._tensor_map = tensor_map

        self._res_tensor = res
        res_c = self._res_tensor
        self._cachebuffer(spec_node_list)
        l0a_load2d_flag = tensor_map["l0a_load2d_flag"]
        self._get_requant_multi_group()
        self.tiling_case = dynamic_para.get("tiling")
        self.var_range = dynamic_para.get("var_range")

        fmap = tensor_map["fmap"]
        self.fmap_trans_flag = True if fmap.op.tag == "NDHWC_trans_6HD" else False
        weight = tensor_map["filter"]
        c_col = tensor_map["c_col"]
        self.buffer_dict = {"res_c": res_c, "c_col": c_col}

        config = tbe_platform.CUBE_MKN[weight.dtype]

        pad_right = self._attrs_dict['padding'][2]
        pad_left = self._attrs_dict['padding'][3]
        kernel_w = self._attrs_dict['kernel_w']
        kernel_d = self._attrs_dict['kernel_d']

        fmap_w = fmap.shape[-2] if fmap.op.input_tensors else fmap.op.shape[-2]
        stride_w = self._attrs_dict['stride'][1]
        dilation_d = self._attrs_dict['dilation'][0]
        dilation_w = self._attrs_dict['dilation'][2]
        self._set_var_range()
        self._show_flags()

        def _get_w_out():
            if "fmap_w" in self.var_map:
                return self.var_map.get("w_out")
            else:
                return (fmap_w + pad_left + pad_right - ((kernel_w - 1) * dilation_w + 1)) // stride_w + 1
        w_out = _get_w_out()

        def _load2d_process():
            if l0a_load2d_flag:
                _al1 = tensor_map["al1_load2d"]
                al0 = tensor_map["al0_load2d"]
                # dtype float16, 16*16 aligned; dtype int8, 16*32 aligned.
                m0, k0, _ = tbe_platform.CUBE_MKN.get(fmap.dtype).get("mac")
                sch[_al1].storage_align(sch[_al1].op.axis[1], m0 * k0, 0)
                _fmap_col = al0
                sch[_al1].set_scope(tbe_platform_info.scope_cbuf)
                _fuse_fmap_tensor = 0
                _fmap_col_before = 0
            else:
                _fuse_fmap_tensor = tensor_map["fmap_do_tensor"]
                _fmap_col_before = 0
                if not self.var_map:
                    _fmap_col_before = tensor_map["fmap_im2col_row_major_res"]
                    sch[_fmap_col_before].buffer_align(
                        (1, 1), (w_out, w_out), (1, 1), (1, 1), (1, 1),
                        (1, tbe_platform.CUBE_MKN[_fmap_col_before.dtype]["mac"][1]))
                    sch[_fmap_col_before].set_scope(tbe_platform_info.scope_cbuf)
                _fmap_col = tensor_map["fmap_im2col_fractal_res"]
                sch[_fuse_fmap_tensor].set_scope(tbe_platform_info.scope_cbuf)
                _al1 = _fuse_fmap_tensor
            return _fmap_col_before, _fmap_col, _al1

        fmap_col_before, fmap_col, al1 = _load2d_process()

        # for fusion vector
        if self.ub_fusion_flag:
            res_ub = sch.cache_write(res, tbe_platform_info.scope_ubuf)
            self.output_ops[0]["tensorize_axis"] = \
                self._schedule[res_ub].op.axis[0]
            self.output_ops[0]["dst_buffer"] = res_ub

        tiling, cycle_buffer_flag = self._tiling_fetch()

        self._tensor_map["cycle_buffer_flag"] = cycle_buffer_flag

        filter_matrix = list(dim_map["filter_matrix_dim"])
        filter_matrix[1] = compute_util.int_ceil_div(filter_matrix[1], tiling["block_dim"][1])

        bl1 = self._weight_to_bl1(tiling, filter_matrix, weight, c_col)
        bl0 = sch.cache_read(bl1, tbe_platform_info.scope_cb, [c_col])

        sch[c_col].set_scope(tbe_platform_info.scope_cc)
        if not self.quant_fused_flag and c_ub is not None:
            sch[c_ub].buffer_align((1, 1), (1, 1),
                                   (1, tbe_platform.CUBE_MKN[c_ub.dtype]["mac"][0]),
                                   (1, tbe_platform.CUBE_MKN[c_ub.dtype]["mac"][2]))
            sch[c_ub].set_scope(tbe_platform_info.scope_ubuf)
        elif c_ub is None:
            sch[c_col].buffer_align((1, 1), (1, 1), (1, 1),
                                    (1, tbe_platform.CUBE_MKN[c_col.dtype]["mac"][0]),
                                    (1, config.get('mac')[2]), (1, 1), (1, 1))

        compute_at_buffer = []
        compute_at_axis = []

        sch[fmap_col].set_scope(tbe_platform_info.scope_ca)

        factor_m = tiling["AL0_matrix"][0]
        factor_k = tiling["AL0_matrix"][1]

        # split N begin

        a1_axis, a3_axis = sch[fmap_col].split(sch[fmap_col].op.axis[2],
                                               factor_m)
        a2_axis, a4_axis = sch[fmap_col].split(sch[fmap_col].op.axis[3],
                                               factor_k)

        fmap_col_no, fmap_col_ni = sch[fmap_col].split(
            sch[fmap_col].op.axis[1], 1)
        sch[fmap_col].reorder(fmap_col_no, a1_axis, a2_axis, fmap_col_ni,
                              a3_axis, a4_axis, sch[fmap_col].op.axis[4],
                              sch[fmap_col].op.axis[5])
        new_fmap_col_axis = [
            fmap_col_no, a1_axis, a2_axis, fmap_col_ni, a3_axis, a4_axis,
            sch[fmap_col].op.axis[4], sch[fmap_col].op.axis[5]
        ]

        new_c_col_axis = [
            sch[c_col].op.axis[1], sch[c_col].op.axis[2],
            sch[c_col].op.axis[3], sch[c_col].op.axis[4]
        ]
        _, _, _, nn_axis = new_c_col_axis
        n_0 = config["mac"][2]
        self._calc_tiling_para(tiling, n_0, res_c.shape[0])

        if res_c.op.name == "fixpipe_channel_merge":
        # eliminate non-linearity caused by channel merge
            sch.set_constraint(((res_c.op.axis[1].var*2) + tvm.floordiv(res_c.op.axis[-1].var, 16)) \
                               < CceConv3dOp._int_ceil_div_tvm(\
                conv3d_compute.Conv3DParam.tiling_info_dict["b_shape"][0], 16))

        # --------------------------double buffer------------------------
        double_buffer_flag = {
            'AL1_pbuffer': False,
            'BL1_pbuffer': False,
            'AL0_pbuffer': False,
            'BL0_pbuffer': False,
            'CL0_pbuffer': False,
            'L0C_OUTPUT_pbuffer': False,
            'UBG_pbuffer': False,
        }

        double_buffer_flag = tiling["manual_pingpong_buffer"]

        m_outer_inner, c_outer_inner = self._res_ddr_split(tiling)
        block = self._bind_core(tiling)

        noi_tree = self.axis_dict.get("cycbuf_axis")
        noo, noi = self._schedule[res_c].split(noi_tree, factor=1)

        reorder_flag = self._reorder_l1_mn_axis(noi, tiling)
        m_outer_inner_outer, m_outer_inner_inner = self._schedule[res_c].split(
            m_outer_inner, nparts=1)

        # ============ tile CUB ========================
        if c_ub is not None:
            c_outer_inner_outer, c_outer_inner_inner = self._schedule[res_c].split(
                c_outer_inner, nparts=self.tiling_para.get("c_ub_factor")[0])
            self._schedule[res_c].reorder(
                c_outer_inner_outer, m_outer_inner_outer,
                c_outer_inner_inner, m_outer_inner_inner)
            if not self.quant_fused_flag:
                self._schedule[c_ub].compute_at(self._schedule[res_c], m_outer_inner_outer)
            c_pragma_axis = c_outer_inner_inner

        # ============ tile c_col =======================
        compute_at_buffer.append(res_c)
        compute_at_axis.append(self.axis_dict.get("c_slice_axis"))
        compute_at_buffer.append(res_c)
        compute_at_axis.append(m_outer_inner_outer)

        sch[c_col].compute_at(sch[res_c], self.axis_dict.get("c_slice_axis"))

        _, reduce_kk = sch[c_col].op.reduce_axis

        axis_factor = list(self.tiling_para.get("al0_axis_factor").get("axis_factor").items())
        # for now
        boo, boi = sch[c_col].split(new_c_col_axis[axis_factor[0][0]],
                                    axis_factor[0][1] * config["mac"][0])

        axis_factor = list(self.tiling_para.get("bl0_axis_factor").get("axis_factor").items())
        coo, coi = sch[c_col].split(new_c_col_axis[axis_factor[0][0]],
                                    axis_factor[0][1])

        # for reduce axis, al0 and bl0 should be the same
        reduce_axis_factor = list(self.tiling_para.get("al0_axis_factor").get("reduce_factor").items())

        # k_outer_outer should be no less than kd
        k_outer_outer, k_outer_inner = sch[c_col].split(
            c_col.op.reduce_axis[reduce_axis_factor[0][0]],
            reduce_axis_factor[0][1])
        k_outer_outer_size = c_col.op.reduce_axis[
                                 reduce_axis_factor[0][0]].dom.extent // \
                             reduce_axis_factor[0][1]

        # split N begin
        _, cn_axis = sch[c_col].split(c_col.op.axis[1], 1)

        sch[c_col].reorder(k_outer_outer, coo, boo, cn_axis, coi, boi, nn_axis,
                           k_outer_inner, reduce_kk)
        sch[fmap_col].compute_at(sch[c_col], boo)
        attach_bl0_param_dict = {'tiling': tiling,
                                 'bl0': bl0, 'coo': coo, 'noo': noo}
        self._attach_bl0(attach_bl0_param_dict)

        if self.bias_table_flag:
            attach_bt_param_dict = {'tiling': tiling, 'bt': self._tensor_map.get("tensor_bt"), 'coo': coo, 'noo': noo}
            self._attach_bt(attach_bt_param_dict)

        #  ============ al1 and bl1 slice can be different with CUB & CL0 =====
        al1_at_ccol_axis, bl1_at_ccol_axis, k_axis_dict = self._split_al1_bl1_k_axis(k_outer_outer, tiling)

        k_outer_outer_outer_outer = k_axis_dict["k_outer_outer_outer_outer"]
        k_outer_outer_outer_inner = k_axis_dict["k_outer_outer_outer_inner"]
        k_outer_outer_inner = k_axis_dict["k_outer_outer_inner"]
        tmp_buffer_dict = {
            "al1": al1,
            "bl1": bl1,
            "fmap_col": fmap_col,
            "bl0": bl0,
            "c_ub": c_ub,
        }
        self.buffer_dict.update(tmp_buffer_dict)

        def _update_load2d_buffer_dict():
            if not l0a_load2d_flag:
                self.buffer_dict["fmap_col_before"] = fmap_col_before

        _update_load2d_buffer_dict()

        # al1 compute_at
        compute_al1_axis = {
            "al1_at_ccol_axis": al1_at_ccol_axis,
            "al1_at_c_axis": self.axis_dict.get("al1_at_c_axis"),
            "noo": noo
        }
        shape_w = conv3d_compute.Conv3DParam.tiling_info_dict["b_shape"]
        k_outer_outer_inner_size = k_outer_outer_size // \
                                   tvm.max(self.tiling_para.get("al1_factor")[0], self.tiling_para.get("bl1_factor")[0])

        nbuffer_al1_param_dict = {'tiling': tiling, 'compute_al1_axis': compute_al1_axis,
                                  'k_outer_outer_inner': k_outer_outer_inner,
                                  'k_outer_outer_inner_size': k_outer_outer_inner_size,
                                  'shape_w': shape_w}
        nbuffer_al1_flag_dict = self._get_nbuffer_al1_flag(nbuffer_al1_param_dict)
        nbuffer_flag_al1 = nbuffer_al1_flag_dict.get("nbuffer_flag_al1")
        compute_al1_axis = nbuffer_al1_flag_dict.get("compute_al1_axis")
        allocate_al1_axis = {
            "al1_at_ccol_axis": al1_at_ccol_axis,
            "al1_at_c_axis": self.axis_dict.get("al1_at_c_axis"),
            "noo": noo
        }
        index_al1_dict = {0: "al1_at_ccol_axis", 1: "al1_at_c_axis", 2: "noo"}
        stage = {0: c_col, 1: res_c, 2: res_c}
        set_al1_at_axis_param_dict = {'l0a_load2d_flag': l0a_load2d_flag, 'nbuffer_flag_al1': nbuffer_flag_al1,
                                      'reorder_flag': reorder_flag, 'tiling': tiling,
                                      'compute_al1_axis': compute_al1_axis, 'allocate_al1_axis': allocate_al1_axis,
                                      'index_al1_dict': index_al1_dict, 'stage': stage}
        self._set_al1_at_axis(set_al1_at_axis_param_dict)

        # bl1 compute_at
        compute_bl1_axis = {
            "coo": coo,
            "bl1_at_ccol_axis": bl1_at_ccol_axis,
            "bl1_at_c_axis": self.axis_dict.get("bl1_at_c_axis"),
            "c_outer_g_inner": self.axis_dict.get("c_outer_g_inner")
        }
        allocate_bl1_axis = {
            "bl1_at_ccol_axis": bl1_at_ccol_axis,
            "bl1_at_c_axis": self.axis_dict.get("bl1_at_c_axis"),
            "c_outer_g_inner": self.axis_dict.get("c_outer_g_inner")
        }
        bl1_index_dict = {0: "bl1_at_ccol_axis", 1: "bl1_at_c_axis", 2: "c_outer_g_inner"}
        bl1_at_axis_param_dict = {'reorder_flag': reorder_flag, 'tiling': tiling,
                                  'compute_bl1_axis': compute_bl1_axis, 'allocate_bl1_axis': allocate_bl1_axis,
                                  'bl1_index_dict': bl1_index_dict, 'stage': stage}
        self._set_bl1_at_axis(bl1_at_axis_param_dict)

        ############################ double buffer ###########################
        self._double_buffer(double_buffer_flag)
        ############################ intrin mapping ###########################
        stride_d = c_col.op.attrs['stride_d']
        pad_head = c_col.op.attrs['pad_head']
        fmap_d = c_col.op.attrs['fmap_d']
        d_out = c_col.op.attrs['d_out']
        w_h = shape_w[-3]
        w_w = shape_w[-2]

        def _get_batch_axis():
            batch_single_core = self.tiling_para.get("batch_do_single_core")
            block_dim_m_n_g = self.tiling_para.get("block_dim_n") * \
                self.tiling_para.get("block_dim_m") * tiling.get("g_dim")
            batch_axis = tvm.floordiv(block, block_dim_m_n_g) * batch_single_core + noo
            if self._tensor_map["cycle_buffer_flag"]:
                batch_axis = batch_axis + self.axis_dict.get("batch_inner_inner")
            return batch_axis

        batch_axis = _get_batch_axis()

        if self.binary_mode:
            kl1_times = self.binary_schedule.cache_tiling.get("kl1_times")
        else:
            outer_factor = max(self.tiling_para.get("al1_factor")[0], self.tiling_para.get("bl1_factor")[0])
            inner_factor = min(self.tiling_para.get("al1_factor")[0], self.tiling_para.get("bl1_factor")[0])
            kl1_times = outer_factor // inner_factor

        x_factor = self._tensor_map.get("group_dict").get("cin1_g")

        k_outer_extend = (k_outer_outer_outer_outer * kl1_times + k_outer_outer_outer_inner) * \
            k_outer_outer_inner_size + k_outer_outer_inner

        fmap_d_index = batch_axis % d_out * stride_d + \
            (k_outer_extend * reduce_axis_factor[0][1] // (w_h * w_w)) // x_factor * dilation_d

        fmap_d_lower_index = batch_axis % d_out * stride_d + \
            (k_outer_extend * reduce_axis_factor[0][1] // (w_h * w_w) // x_factor - 1) * dilation_d

        k_coeff_cond = tvm.all((fmap_d_index >= pad_head), (fmap_d_index < fmap_d + pad_head))
        kernel_d_dilation = (kernel_d - 1) * dilation_d + 1
        if not self.var_map or tiling.get("pad_greater_than_filter"):
            k_coeff_cond = tvm.any((batch_axis % d_out * stride_d <= pad_head - kernel_d_dilation),
                                   (batch_axis % d_out * stride_d >= fmap_d + pad_head), k_coeff_cond)
        mad_dict = {
            "mad_pattern":
                2,
            "k_outer": [
                k_outer_outer_outer_outer, k_outer_outer_outer_inner,
                k_outer_outer_inner
            ],
            "k_coeff": k_coeff_cond,
            "k_cond":
                tvm.any(
                    tvm.all(
                        (fmap_d_index >= pad_head),
                        (fmap_d_lower_index < pad_head),
                        (k_outer_extend * reduce_axis_factor[0][1] %
                         (w_h * w_w * x_factor) <= 0)),
                    k_outer_extend == 0),
        }
        if self.quant_bias_flag:
            mad_dict['k_cond'] = False
        tiling_dtype = get_inout_dtype(fmap, weight, c_col, "Conv3D")
        if self.binary_mode and self.in_dtype == "float32":
                mad_dict["hf32"] = get_te_var("hf32_flag").get_tvm_var()
        elif tiling_dtype[MAD_DTYPE_INDEX] == "hfloat32":
            mad_dict["hf32"] = 1
        intrin_mapping_param_dict = {'fmap': fmap, 'mad_dict': mad_dict,
                                     'new_fmap_col_axis': new_fmap_col_axis, 'tiling': tiling,
                                     'cn_axis': cn_axis, 'l0a_load2d_flag': l0a_load2d_flag}
        self._intrin_mapping(intrin_mapping_param_dict)
        if not self.var_map or tiling.get("pad_greater_than_filter"):
            cond = tvm.any(batch_axis % d_out * stride_d <= pad_head - kernel_d_dilation, batch_axis % d_out * stride_d
                           >= fmap_d + pad_head)
            in_dtype = conv3d_compute.Conv3DParam.tiling_info_dict["a_dtype"]
            sch[fmap_col].set_value(cond, tvm.const(0, dtype=in_dtype))

        if self.ub_fusion_flag:
            sch[res_c].emit_insn(c_pragma_axis, 'dma_copy')

        ########################### cube schedule end #########################
        self._attach_at(compute_at_buffer, compute_at_axis, tiling)
        self._fixpipe_process(double_buffer_flag)
        if c_ub is None:
            if res_c.op.attrs.get("6HD_TRANS_NDHWC") or res_c.op.name == "fixpipe_channel_merge" \
                or res_c.op.name == "fixpipe_channel_split":
                res_axis = m_outer_inner_inner
            else:
                res_axis = c_outer_inner
        else:
            res_axis = c_outer_inner_inner
        self._to_pragma(self.body_ops, self.input_ops, res_axis)
        self.binary_schedule.binary_simplify(tensor_map, self.axis_dict, tiling)
        self._c_col_buffer_tile(tiling)

        def _get_al1_bound():
            cin1_g = self._tensor_map.get("group_dict").get("cin1_g")
            _, _, _, fmap_hi, fmap_wi, fmap_c0 = fmap.shape
            stride_h = conv3d_compute.Conv3DParam.tiling_info_dict["stride"][1]
            stride_update = 1 if self._tensor_map["opti_h_flag"] else stride_h
            extend_h_calculate_factor = 2
            if tiling["AL1_shape"]:
                al1_m_tiling = tiling["AL1_shape"][1] * self.tiling_para.get("c_tiling_factor")[1]
                if l0a_load2d_flag:
                    al1_m = al1_m_tiling
                elif "fmap_w" in self.var_map:
                    # dynamic_hw choose the value of additional_rows according to w_out
                    additional_rows = tvm.select(
                        tvm.floormod(al1_m_tiling, w_out) == 0,
                        0,
                        tvm.select(tvm.floormod(al1_m_tiling * extend_h_calculate_factor, w_out) == 0,
                            1, 2))
                    ho_len = tvm.floordiv(al1_m_tiling, self.var_map['w_out']) + additional_rows
                    hi_max = self._attrs_dict['kernel_h'] + (ho_len - 1)*stride_update
                    al1_m = hi_max * self.var_map['fmap_w']
                else:
                    if al1_m_tiling % int(w_out) == 0:
                        additional_rows = 0
                    elif al1_m_tiling * extend_h_calculate_factor % int(w_out) == 0:
                        additional_rows = 1
                    else:
                        additional_rows = 2
                    ho_len = tvm.floordiv(al1_m_tiling, w_out) + additional_rows
                    hi_max = self._attrs_dict['kernel_h'] + (ho_len - 1)*stride_update
                    al1_m = hi_max * fmap_wi
                return al1_m * tiling["AL1_shape"][0] * fmap_c0
            else:
                if self._tensor_map["opti_h_flag"]:
                    fmap_hi = (fmap_hi - 1) // stride_h + 1
                al1_m = fmap_hi * fmap_wi
                if l0a_load2d_flag:
                    align_util = 16
                    al1_m = compute_util.align(al1_m, align_util)
                return al1_m * cin1_g * fmap_c0 * kernel_d

        # Reused UB memory
        if self.noquant_bias_flag:
            bias_add_tensor = tensor_map['bias_add_tensor']
            sch[c_ub].reused_by(bias_add_tensor)
            sch[bias_add_tensor].buffer_align((1, 1), (1, 1),
                               (1, tbe_platform.CUBE_MKN[bias_add_tensor.dtype]["mac"][0]),
                               (1, tbe_platform.CUBE_MKN[bias_add_tensor.dtype]["mac"][2]))

        al1_bound = _get_al1_bound()
        self._set_buffer_size(tiling, al1_bound)
        if self.var_map:
            return True

        tensor_map.clear()
        dim_map.clear()
        tiling.clear()
        return True

    def _get_bl1_bound(self, tiling):
        # bl1 set storage bound
        bl1 = self.buffer_dict.get("bl1")
        kernel_h, kernel_w = get_te_var("kernel_h").get_tvm_var(), get_te_var("kernel_w").get_tvm_var()
        if tiling.get("BL1_shape"):
            bl1_n = tiling.get("BL1_shape")[1] * tiling.get("BL0_matrix")[1] * \
                tbe_platform.CUBE_MKN[bl1.dtype]["mac"][0]
            bl1_k = tiling.get("BL1_shape")[0] * kernel_h * kernel_w * tbe_platform.CUBE_MKN[bl1.dtype]["mac"][1]
            bl1_bound = bl1_k * bl1_n
        else:
            bl1_bound = reduce(lambda x, y: x * y, self._tensor_map("filter"))
        return bl1_bound

    def _set_buffer_size(self, tiling, al1_bound):
        """
        set buffer size
        """
        cl0_bound = reduce(lambda x, y: x * y, tiling.get("CL0_matrix"))
        self._schedule[self.buffer_dict.get("c_col")].set_buffer_size(cl0_bound)
        if self.var_map:
            al1_bound = al1_bound if not self.binary_mode else self.binary_schedule.cache_tiling.get("al1_bound")
            self._schedule[self.buffer_dict.get("al1")].set_buffer_size(al1_bound)
            self._schedule.sequential_malloc(tbe_platform_info.scope_cbuf)
            self._schedule.sequential_malloc(tbe_platform_info.scope_ca)
            self._schedule.sequential_malloc(tbe_platform_info.scope_cb)
            self._schedule.sequential_malloc(tbe_platform_info.scope_cc)
            self._schedule.sequential_malloc(tbe_platform_info.scope_ubuf)

            # mem_unique
            self._schedule[self.buffer_dict.get("al1")].mem_unique()
            self._schedule[self.buffer_dict.get("fmap_col")].mem_unique()
            if self.weight_gm2bl1_flag:
                self._schedule[self.buffer_dict.get("bl1")].mem_unique()
            self._schedule[self.buffer_dict.get("bl0")].mem_unique()
            self._schedule[self.buffer_dict.get("c_col")].mem_unique()

            if self.binary_mode:
                bt_bound = self.binary_schedule.cache_tiling.get("bt_bound")
                bias_l1_bound = bt_bound * tiling.get("BL1_shape")[1]
                if self.weight_gm2bl1_flag:
                    bl1_bound = self._get_bl1_bound(tiling)
                    self._schedule[self.buffer_dict.get("bl1")].set_buffer_size(bl1_bound)
                else:
                    # bias full load into L1 when weight load into L0B directly
                    n_axis_extend = CceConv3dOp._int_ceil_div_tvm(self.tiling_para.get("cout1_g"),
                                                                    self.tiling_para.get("block_dim_n"))
                    bias_l1_bound = bt_bound * tvm.floordiv(n_axis_extend, tiling.get("BL0_matrix")[1])

                al0_bound = reduce(lambda x, y: x * y, tiling.get("AL0_matrix"))
                bl0_bound = reduce(lambda x, y: x * y, tiling.get("BL0_matrix"))
                self._schedule[self.buffer_dict.get("fmap_col")].set_buffer_size(al0_bound)
                self._schedule[self.buffer_dict.get("bl0")].set_buffer_size(bl0_bound)
                if self._tensor_map.get("tensor_bt") is not None:
                    self._schedule[self._tensor_map.get("bias_l1")].set_buffer_size(bias_l1_bound)
                    if self._tensor_map.get("bias_zero") is not None:
                        self._schedule[self._tensor_map.get("bias_zero")].set_buffer_size(bias_l1_bound)
                    self._schedule[self._tensor_map.get("tensor_bt")].set_buffer_size(bt_bound)
                    self._schedule.sequential_malloc(tbe_platform_info.scope_bt)
                    self._schedule[self._tensor_map.get("tensor_bt")].mem_unique()

    def _calc_factor_l0(self, tiling, n_0):
        """
        calculate tiling param for l0.

        Parameters
        ----------
        tiling : case tiling

        n_0 : size of n0

        Returns
        -------
        None
        """
        c_tiling_factor = [
            tiling["CL0_matrix"][0],
            tiling["CL0_matrix"][1] * tiling["CL0_matrix"][2] // self.load3d_special_multiply
        ]

        c_factor = [
            compute_util.int_ceil_div(self._tensor_map["group_dict"]["cout_g"] // n_0,
                                      c_tiling_factor[0]),
            compute_util.int_ceil_div(self._dim_map["out_img_shape"][-2],
                                      c_tiling_factor[1])
        ]
        self.tiling_para["c_tiling_factor"] = c_tiling_factor
        self.tiling_para["c_factor"] = c_factor

    def _calc_factor_ub(self, tiling):
        """
        calculate tiling param for ub.

        Parameters
        ----------
        tiling : case tiling

        Returns
        -------
        None
        """
        c_tiling_factor = self.tiling_para.get("c_tiling_factor")
        c_ub_tiling_factor = tiling["L0C_OUTPUT_matrix"]
        c_ub_factor = [
            compute_util.int_ceil_div(
                c_tiling_factor[0], c_ub_tiling_factor[0]),
            compute_util.int_ceil_div(
                c_tiling_factor[1], c_ub_tiling_factor[1] * c_ub_tiling_factor[2] // self.load3d_special_multiply)
        ]
        self.tiling_para["c_ub_tiling_factor"] = c_ub_tiling_factor
        self.tiling_para["c_ub_factor"] = c_ub_factor

    def _calc_factor_block(self, tiling, n_0, batch_dim):
        """
        calculate tiling param for block.

        Parameters
        ----------
        tiling : case tiling

        n_0 : size of n0

        batch_dim : batch dim of fmap

        Returns
        -------
        None
        """
        self.tiling_para["batch_do_single_core"] = compute_util.int_ceil_div(
            batch_dim, tiling["block_dim"][0])
        if not self.var_map and self._tensor_map.get("cycle_buffer_flag"):
            # In the static cyclebuffer scenario, need to ensure that the do_single_core is dout // d_dim
            do_single_core = self._tensor_map.get("d_out") // tiling["block_dim"][-2]
            batch_single_core = compute_util.int_ceil_div(
                batch_dim // self._tensor_map.get("d_out"), self.tiling_para.get("block_dim_batch"))
            self.tiling_para["batch_do_single_core"] = batch_single_core * do_single_core
        self.tiling_para["block_dim_batch_do"] = compute_util.int_ceil_div(
            batch_dim, self.tiling_para.get("batch_do_single_core"))
        self.tiling_para["cout1_g"] = self._tensor_map["group_dict"]["cout_g"] // n_0
        m_outer_outer_outer_size = tvm.max(1, self.tiling_para.get("al1_factor")[1] // self.load3d_special_multiply)
        self.tiling_para["block_dim_m"] = tvm.min(tiling["block_dim"][2], m_outer_outer_outer_size)
        self.tiling_para["block_dim_n"] = tvm.min(tiling["block_dim"][1], self.tiling_para.get("bl1_factor")[1])

    def _calc_tiling_para(self, tiling, n_0, batch_dim):
        """
        calculate tiling param.

        Parameters
        ----------
        tiling : case tiling

        n_0 : size of n0

        batch_dim : batch dim of fmap

        Returns
        -------
        None
        """
        if self.binary_mode:
            self.tiling_para = self.binary_schedule.update_tiling_nparts_factor()
            return
        self._calc_factor_l0(tiling, n_0)
        if self._tensor_map.get("c_ub") is not None:
            self._calc_factor_ub(tiling)
        self._calc_factor_al1_bl1(tiling)
        self.tiling_para["al0_axis_factor"] = self._tiling_l0a_l0b(
            tiling["AL0_matrix"], tiling["CL0_matrix"], 'A')
        self.tiling_para["bl0_axis_factor"] = self._tiling_l0a_l0b(
            tiling["BL0_matrix"], tiling["CL0_matrix"], 'B')
        self._calc_factor_block(tiling, n_0, batch_dim)

    def _reorder_res_axis_for_cyclebuffer(self, tiling):
        """
        reorder the batch, m, and n of res for cyclebuffer condition.

        Parameters
        ----------
        tiling : case tiling

        Returns
        -------
        None
        """
        res_c = self.buffer_dict.get("res_c")
        split_condition = self.var_map and \
            tiling["manual_pingpong_buffer"]['AL1_pbuffer'] == 2 and \
            tiling["manual_pingpong_buffer"]['AL0_pbuffer'] == 2
        if split_condition:
            # split + reorder to adjust ping pong data division,
            # instead of cycle double buffer
            batch_inner_outer, batch_inner_inner = self._schedule[res_c].split(
                self.axis_dict.get("batch_inner"), nparts=1)
            batch_inner_inner_outer, batch_inner_inner_inner = self._schedule[res_c].split(
                batch_inner_inner, nparts=2)
            self.axis_dict["al1_at_c_axis"] = batch_inner_inner_outer
            self._schedule[res_c].reorder(
                self.axis_dict.get("batch_outer"),
                self.axis_dict.get("c_outer_g_outer"),
                self.axis_dict.get("c_outer_outer_outer_outer"),
                self.axis_dict.get("m_outer_outer_outer_outer"),
                self.axis_dict.get("c_outer_g_inner"),
                batch_inner_outer,
                self.axis_dict.get("c_outer_outer_outer_inner"),
                self.axis_dict.get("m_outer_outer_outer_inner"),
                batch_inner_inner_inner,
                batch_inner_inner_outer)
            self.axis_dict["cycbuf_axis"] = batch_inner_outer
            self.axis_dict["batch_inner_inner"] = batch_inner_inner
        else:
            batch_inner_outer, batch_inner_inner = self._schedule[res_c].split(
                self.axis_dict.get("batch_inner"), nparts=1)
            self.axis_dict["al1_at_c_axis"] = batch_inner_inner
            self._schedule[res_c].reorder(
                self.axis_dict.get("batch_outer"),
                self.axis_dict.get("c_outer_g_outer"),
                self.axis_dict.get("c_outer_outer_outer_outer"),
                self.axis_dict.get("m_outer_outer_outer_outer"),
                self.axis_dict.get("c_outer_g_inner"),
                batch_inner_outer,
                self.axis_dict.get("c_outer_outer_outer_inner"),
                self.axis_dict.get("m_outer_outer_outer_inner"), batch_inner_inner)
            self.axis_dict["cycbuf_axis"] = batch_inner_outer
            self.axis_dict["batch_inner_inner"] = batch_inner_inner

    def _reorder_res_axis(self, tiling):
        """
        reorder the batch, m, and n of res.

        Parameters
        ----------
        tiling : case tiling

        Returns
        -------
        None
        """
        res_c = self.buffer_dict.get("res_c")
        if self._tensor_map["cycle_buffer_flag"]:
            self._reorder_res_axis_for_cyclebuffer(tiling)
        else:
            self._schedule[res_c].reorder(
                self.axis_dict.get("batch_outer"),
                self.axis_dict.get("c_outer_g_outer"),
                self.axis_dict.get("c_outer_outer_outer_outer"),
                self.axis_dict.get("m_outer_outer_outer_outer"),
                self.axis_dict.get("c_outer_g_inner"),
                self.axis_dict.get("batch_inner"),
                self.axis_dict.get("c_outer_outer_outer_inner"),
                self.axis_dict.get("m_outer_outer_outer_inner"))
            self.axis_dict["cycbuf_axis"] = self.axis_dict.get("batch_inner")

    def _get_res_c_axis_index(self,):
        """
        get res_c axis index

        Parameters
        ----------
        None

        Returns
        -------
        c_index, hw_index
        """
        # c_index, 1:(ND)C1(HW)C0, 2:(ND)(HW)C, # hw_index, 2:(ND)C1(HW)C0, 1:(ND)(HW)C
        c_index, hw_index = 1, 2
        if self.buffer_dict.get("res_c").op.attrs.get("6HD_TRANS_NDHWC"):
            c_index, hw_index = 2, 1
        return c_index, hw_index

    def _cal_factor_n_dim(self,):
        """
        calc factor for cout1 and nl0

        Parameters
        ----------
        None

        Returns
        -------
        factor_cout, factor_n_l0
        """
        factor_cout = self.tiling_para.get("cout1_g")
        factor_n_l0 = self.tiling_para.get("c_tiling_factor")[0]
        if self.buffer_dict.get("res_c").dtype == 'int8':
            # n0 is 32 when conv3d_requant, axis n1 need split 2
            factor_cout = compute_util.int_ceil_div(factor_cout, 2)
            factor_n_l0 = compute_util.int_ceil_div(factor_n_l0, 2)
        elif self.buffer_dict.get("res_c").op.attrs.get("6HD_TRANS_NDHWC"):
            factor_cout = self._tensor_map['group_dict']['cout_g']
            factor_n_l0 = self.tiling_para.get("c_tiling_factor")[0] * \
                tbe_platform.CUBE_MKN[self.buffer_dict.get("c_col").dtype]["mac"][2]
        if self.in_dtype == "float32":
            factor_cout *= 2
            factor_n_l0 *= 2
        return factor_cout, factor_n_l0

    def _res_ddr_split(self, tiling):
        """
        split res axis, for L0/L1/BLOCK.

        Parameters
        ----------
        tiling : case tiling

        Returns
        -------
        m_outer_inner, c_outer_inner
        """
        res_c = self.buffer_dict.get("res_c")
        factor_cout, factor_n_l0 = self._cal_factor_n_dim()
        c_index, hw_index = self._get_res_c_axis_index()
        # for group split
        c_outer_g, c_outer = self._schedule[res_c].split(res_c.op.axis[c_index], factor_cout)
        c_outer_g_outer, c_outer_g_inner = self._schedule[res_c].split(c_outer_g, nparts=tiling["g_dim"])
        # for l0 split
        c_outer_outer, c_outer_inner = self._schedule[res_c].split(c_outer, factor_n_l0)
        m_outer_outer, m_outer_inner = self._schedule[res_c].split(
            res_c.op.axis[hw_index], self.tiling_para.get("c_tiling_factor")[1])
        if self.buffer_dict.get("res_c").op.attrs.get("6HD_TRANS_NDHWC"):
            self._schedule[res_c].reorder(c_outer_g_outer, c_outer_g_inner, c_outer_outer, m_outer_outer,
                                          m_outer_inner, c_outer_inner)
            c_outer_inner_outer, c_outer_inner_inner = self._schedule[res_c].split(c_outer_inner, 16)
        else:
            self._schedule[res_c].reorder(c_outer_outer, m_outer_outer, c_outer_inner, m_outer_inner)
        # for l1 split
        m_outer_outer_outer, m_outer_outer_inner = self._schedule[res_c].split(
            m_outer_outer, nparts=self.tiling_para.get("al1_factor")[1])
        c_outer_outer_outer, c_outer_outer_inner = self._schedule[res_c].split(
            c_outer_outer, nparts=self.tiling_para.get("bl1_factor")[1])
        # fp32: n channel split and n0 should be 8,  2: c0(16) -> n0(8)
        if self.in_dtype == "float32":
            c_outer_inner, c_outer_inner_inner = self._schedule[res_c].split(c_outer_inner, 2)
        # for muti-core split
        batch_outer, batch_inner = self._schedule[res_c].split(
            res_c.op.axis[0], self.tiling_para.get("batch_do_single_core"))
        c_outer_outer_outer_outer, c_outer_outer_outer_inner = self._schedule[
            res_c].split(c_outer_outer_outer, nparts=self.tiling_para.get("block_dim_n"))
        m_outer_outer_outer_outer, m_outer_outer_outer_inner = self._schedule[
            res_c].split(m_outer_outer_outer, nparts=self.tiling_para.get("block_dim_m"))

        self.axis_dict["bl1_at_c_axis"] = c_outer_outer_outer_inner
        self.axis_dict["al1_at_c_axis"] = m_outer_outer_outer_inner
        self.axis_dict["c_slice_axis"] = m_outer_outer_inner
        self.axis_dict["batch_inner"] = batch_inner
        self.axis_dict["batch_outer"] = batch_outer
        self.axis_dict["c_outer_g_outer"] = c_outer_g_outer
        self.axis_dict["c_outer_g_inner"] = c_outer_g_inner
        self.axis_dict["c_outer_outer_outer_outer"] = c_outer_outer_outer_outer
        self.axis_dict["m_outer_outer_outer_outer"] = m_outer_outer_outer_outer
        self.axis_dict["c_outer_outer_outer_inner"] = c_outer_outer_outer_inner
        self.axis_dict["m_outer_outer_outer_inner"] = m_outer_outer_outer_inner
        self.axis_dict["c_outer_outer_inner"] = c_outer_outer_inner
        self.axis_dict["m_outer_outer_inner"] = m_outer_outer_inner
        self.axis_dict["c_outer_inner"] = c_outer_inner
        self._reorder_res_axis(tiling)
        return m_outer_inner, c_outer_inner

    def _bind_core(self, tiling):
        """
        bind core for batch, n, m, group

        Parameters
        ----------
        tiling : case tiling

        Returns
        -------
        block
        """
        res_c = self.buffer_dict.get("res_c")
        blocks = self.tiling_para.get("block_dim_batch_do") * self.tiling_para.get("block_dim_n") * \
            self.tiling_para.get("block_dim_m") * tiling["g_dim"]
        noo_true = self.axis_dict.get("cycbuf_axis")
        block = 1
        if self.binary_mode:
            axis_list = [self.axis_dict.get("batch_outer"), self.axis_dict.get("c_outer_g_outer"),
                         self.axis_dict.get("c_outer_outer_outer_outer"),
                         self.axis_dict.get("m_outer_outer_outer_outer")]
            block = tvm.thread_axis("blockIdx.x")
            self._schedule.bind_axes(axis_list, block)
        elif not isinstance(blocks, tvm.Var) and blocks != 1:
            batch_cout_fused = self._schedule[res_c].fuse(
                self.axis_dict.get("batch_outer"),
                self.axis_dict.get("c_outer_g_outer"),
                self.axis_dict.get("c_outer_outer_outer_outer"),
                self.axis_dict.get("m_outer_outer_outer_outer"))
            noo_true, _ = self._schedule[res_c].split(batch_cout_fused, nparts=blocks)
            bido, _ = self._schedule[res_c].split(noo_true, 1)
            block = tvm.thread_axis("blockIdx.x")
            self._schedule[res_c].bind(bido, block)

        return block

    def _weight_to_bl1(self, tiling, filter_matrix, weight, c_col):
        """
        weight to bl1.

        Parameters
        ----------
        tiling : case tiling

        filter_matrix : full load

        weight: input weight tensor

        c_col: stage loc

        Returns
        -------
        bl1
        """
        sch = self._schedule
        if not self.binary_mode:
            if tiling["BL0_matrix"] == filter_matrix:
                tiling["BL0_matrix"] = []
                attach_flag = tiling.get("attach_at_flag")
                attach_flag["bl0_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_full_load")

            if tiling["BL0_matrix"] == []:
                tiling["BL1_shape"] = None
                attach_flag = tiling.get("attach_at_flag")
                attach_flag["bl1_attach_flag"] = self.binary_schedule.tiling_utils.get("no_attach")

            if tiling["BL1_shape"] is None:
                self.weight_gm2bl1_flag = 0

        if self.weight_gm2bl1_flag:
            bl1 = sch.cache_read(weight, tbe_platform_info.scope_cbuf, [c_col])
        else:
            # tiling["BL1_shape"] is not None ---> weight from OUT To l0b directly
            bl1 = weight
        return bl1

    def _fixpipe_process(self, double_buffer_flag):
        fixpipe_tensor = self._tensor_map.get("fixpipe_tensor", None)
        if fixpipe_tensor is None:
            return
        c_col = self._tensor_map.get("c_col")
        vector_params = fixpipe_tensor.op.attrs["vector_params"]
        vector_tensors = fixpipe_tensor.op.attrs["vector_tensors"]
        for idx, params_mem in enumerate(vector_params):
            fixpipe_input = vector_tensors[idx]
            fixpipe_input_l1 = self._schedule.cache_read(fixpipe_input, tbe_platform_info.scope_cbuf, [fixpipe_tensor])
            fixpipe_scope_name = FIXPIPE_SCOPE_MAP.get(params_mem)
            if fixpipe_scope_name:
                fixpipe_fb_tensor = self._schedule.cache_read(fixpipe_input_l1, fixpipe_scope_name, [fixpipe_tensor])
                self._schedule[fixpipe_fb_tensor].compute_at(self._schedule[self.buffer_dict.get("res_c")],
                                                             self.axis_dict.get("c_slice_axis"))
                self._schedule[fixpipe_fb_tensor].mem_unique()
                self._schedule[fixpipe_fb_tensor].emit_insn(fixpipe_fb_tensor.op.axis[0], "dma_copy")
            if params_mem == "eltwise_src":
                weight = self._tensor_map.get("filter")
                config = tbe_platform.CUBE_MKN[weight.dtype]
                self._schedule[fixpipe_input_l1].buffer_align((1, 1), (1, 1), (1, 1), (1, config.get('mac')[2]))
            self._schedule[fixpipe_input_l1].compute_at(self._schedule[self.buffer_dict.get("res_c")],
                                                        self.axis_dict.get("c_slice_axis"))
            self._schedule[fixpipe_input_l1].emit_insn(fixpipe_input_l1.op.axis[0], "dma_copy")
            if double_buffer_flag["CL0_pbuffer"] == 2:
                self._schedule[fixpipe_input_l1].double_buffer()

    def _calc_factor_al1_bl1(self, tiling):
        """
        get al1_factor and bl1_factor.

        Parameters
        ----------
        tiling : case tiling

        Returns
        -------
        None

        """
        c_factor = self.tiling_para.get("c_factor")
        if len(tiling["AL1_shape"]) == 1:
            tiling["AL1_shape"] = tiling["AL1_shape"] + [1]
        equivalent_k = self._tensor_map["group_dict"]["cin1_g"] * self._tensor_map["filter_d"]
        if tiling["AL1_shape"]:
            al1_factor = [
                equivalent_k // tiling["AL1_shape"][0],
                compute_util.int_ceil_div(c_factor[1], tiling["AL1_shape"][1])
            ]
        else:
            al1_factor = [1, 1]

        if tiling["BL1_shape"]:
            if len(tiling["BL1_shape"]) > 1:
                if c_factor[0] % tiling["BL1_shape"][1] != 0:
                    cube_err.raise_err_one_para('E62301', 'conv3d', str(tiling["BL1_shape"][1]))

            if len(tiling["BL1_shape"]) == 1:
                tiling["BL1_shape"] = tiling["BL1_shape"] + [1]
            bl1_factor = [
                compute_util.int_ceil_div(equivalent_k, tiling["BL1_shape"][0]),
                compute_util.int_ceil_div(c_factor[0], tiling["BL1_shape"][1])
            ]
        else:
            bl1_factor = [1, tvm.min(tiling["block_dim"][1], c_factor[0])]

        outer_factor = max(al1_factor[0], bl1_factor[0])
        inner_factor = min(al1_factor[0], bl1_factor[0])
        if outer_factor % inner_factor != 0:
            cube_err.raise_err_two_paras('E62303', 'conv3d', str(al1_factor[0]), str(bl1_factor[0]))
        self.tiling_para["al1_factor"] = al1_factor
        self.tiling_para["bl1_factor"] = bl1_factor

    def _get_reorder_flag_normal(self, tiling):
        """
        reorder axis.

        Returns
        -------
        reorder flag
        """
        al1_factor = self.tiling_para.get("al1_factor")
        bl1_factor = self.tiling_para.get('bl1_factor')
        reorder_flag = False
        if not tiling["BL1_shape"]:
            reorder_flag = True
        elif tiling["manual_pingpong_buffer"]["AL1_pbuffer"] == tiling["manual_pingpong_buffer"]["BL1_pbuffer"] and \
                isinstance(al1_factor[1], int):
            if bl1_factor[1] >= al1_factor[1]:
                reorder_flag = True
        elif tiling["manual_pingpong_buffer"]["BL1_pbuffer"] == 2:
            reorder_flag = True
        return reorder_flag

    def _reorder_l1_mn_axis(self, noi, tiling):
        """
        reorder axis.

        Parameters
        ----------
        reorder_axis_param_dict:

            tiling : case tiling

            al1_factor : al1 split factor [c1//kAl1, howo//mc//m0//m_Al1]

            bl1_factor : bl1 split factor [c1//kBl1, howo//nc//n0//n_Bl1]

            double_buffer_flag : flag of double buffer

            reorder_axis_dict : axis to reorder

        Returns
        -------
        reorder flag

        """
        sch = self._schedule

        m_outer_outer_outer_inner = self.axis_dict.get("m_outer_outer_outer_inner")
        c_outer_outer_outer_inner = self.axis_dict.get("c_outer_outer_outer_inner")

        if self.binary_mode:
            reorder_flag = False
        else:
            reorder_flag = self._get_reorder_flag_normal(tiling)

        if reorder_flag:
            sch[self.buffer_dict.get("res_c")].reorder(
                m_outer_outer_outer_inner, noi, c_outer_outer_outer_inner)
        else:
            sch[self.buffer_dict.get("res_c")].reorder(
                c_outer_outer_outer_inner, m_outer_outer_outer_inner, noi)
        return reorder_flag

    def _attach_bl0(self, attach_bl0_param_dict):
        """
        bl0 compute at.

        Parameters
        ----------
        attach_bl0_param_dict:

            tiling : case tiling

            buffer_dict : c_col res_c

            bl0 : loc axis

            coo : loc axis

            noo : res axis

        Returns
        -------

        """
        tiling = attach_bl0_param_dict.get('tiling')
        bl0 = attach_bl0_param_dict.get('bl0')
        coo = attach_bl0_param_dict.get('coo')
        noo = attach_bl0_param_dict.get('noo')
        sch = self._schedule
        res_c = self.buffer_dict.get("res_c")
        c_col = self.buffer_dict.get("c_col")
        dynamic_bl0_attach = tiling.get("attach_at_flag").get("bl0_attach_flag") != \
            self.binary_schedule.tiling_utils.get("attach_full_load")
        dynamic_bl0_attach = "c_col" if dynamic_bl0_attach else "res_c"

        if dynamic_bl0_attach == "c_col":
            sch[bl0].compute_at(sch[c_col], coo)
        else:
            sch[bl0].compute_at(sch[res_c], noo)

        return True

    def _attach_bt(self, attach_bt_param_dict):
        """
        bl0 compute at.

        Parameters
        ----------
        attach_bl0_param_dict:

            tiling : case tiling

            buffer_dict : c_col res_c

            bt : tensor_bt axis

            coo : loc axis

            noo : res axis

        Returns
        -------

        """
        tiling = attach_bt_param_dict.get('tiling')
        bt = attach_bt_param_dict.get('bt')
        coo = attach_bt_param_dict.get('coo')
        noo = attach_bt_param_dict.get('noo')
        sch = self._schedule
        res_c = self.buffer_dict.get("res_c")
        c_col = self.buffer_dict.get("c_col")
        if tiling["BL0_matrix"]:
            sch[bt].compute_at(sch[c_col], coo)
        else:
            sch[bt].compute_at(sch[res_c], noo)

        return True

    def _split_al1_bl1_k_axis(self, k_outer_outer, tiling):
        """
        splite al1 and bl1 k_axis.

        Parameters
        ----------
        buffer_dict : c_col res_c

        k_outer_outer : loc axis

        Returns
        -------
        al1_at_ccol_axis, bl1_at_ccol_axis, k_axis_dict

        """
        c_col = self.buffer_dict.get("c_col")
        sch = self._schedule
        al1_factor = self.tiling_para.get("al1_factor")
        bl1_factor = self.tiling_para.get("bl1_factor")
        kbl1_large_kal1 = tiling.get("attach_at_flag").get("abkl1_attach_flag") == \
            self.binary_schedule.tiling_utils.get("attach_less") or self.weight_gm2bl1_flag == 0
        if kbl1_large_kal1:
            k_outer_outer_outer, k_outer_outer_inner = sch[c_col].split(
                k_outer_outer, nparts=al1_factor[0])
            k_outer_outer_outer_outer, k_outer_outer_outer_inner = sch[
                c_col].split(k_outer_outer_outer, nparts=(bl1_factor[0]))
            al1_at_ccol_axis = k_outer_outer_outer_inner
            bl1_at_ccol_axis = k_outer_outer_outer_outer
        else:
            k_outer_outer_outer, k_outer_outer_inner = sch[c_col].split(
                k_outer_outer, nparts=bl1_factor[0])
            k_outer_outer_outer_outer, k_outer_outer_outer_inner = sch[
                c_col].split(k_outer_outer_outer, nparts=(al1_factor[0]))
            al1_at_ccol_axis = k_outer_outer_outer_outer
            bl1_at_ccol_axis = k_outer_outer_outer_inner

        k_axis_dict = {
            "k_outer_outer_outer_outer": k_outer_outer_outer_outer,
            "k_outer_outer_outer_inner": k_outer_outer_outer_inner,
            "k_outer_outer_inner": k_outer_outer_inner
        }

        return al1_at_ccol_axis, bl1_at_ccol_axis, k_axis_dict

    def _get_nbuffer_al1_flag(self, nbuffer_al1_param_dict):
        """
        get al1 nbuffer flag.

        Parameters
        ----------
        nbuffer_al1_param_dict:

            tiling : case tiling

            compute_al1_axis : al1 compute at axis

            buffer_dict : al1/bl1 al0/bl0 c_col c_ub

            k_outer_outer_inner : loc axis

            k_outer_outer_inner_size : k_outer_outer_inner size

            shape_w : weight shape

        Returns
        -------
        nbuffer_flag_al1, compute_al1_axis, nbuffer_axis

        """
        tiling = nbuffer_al1_param_dict.get('tiling')
        compute_al1_axis = nbuffer_al1_param_dict.get('compute_al1_axis')
        k_outer_outer_inner = nbuffer_al1_param_dict.get('k_outer_outer_inner')
        k_outer_outer_inner_size = nbuffer_al1_param_dict.get('k_outer_outer_inner_size')
        shape_w = nbuffer_al1_param_dict.get('shape_w')
        sch = self._schedule
        c_col = self.buffer_dict["c_col"]
        nbuffer_flag_al1 = False
        nbuffer_axis = {}
        nbuffer_size = 1
        if tiling["A_overhead_opt_flag"]:
            if (shape_w[-3] * shape_w[-2]) % tiling["AL0_matrix"][1] == 0:
                nbuffer_size = shape_w[-3] * shape_w[-2] // tiling["AL0_matrix"][1]
            else:
                nbuffer_size = shape_w[-3] * shape_w[-2]
            if int(k_outer_outer_inner_size % nbuffer_size
                   ) == 0 and k_outer_outer_inner_size > nbuffer_size:
                k_outer_outer_inner_outer, k_outer_outer_inner_inner = sch[
                    c_col].split(k_outer_outer_inner, nbuffer_size)
                nbuffer_flag_al1 = True
                compute_al1_axis[
                    "k_outer_outer_inner_outer"] = k_outer_outer_inner_outer
                nbuffer_axis = {
                    "k_outer_outer_inner_outer": k_outer_outer_inner_outer,
                    "k_outer_outer_inner_inner": k_outer_outer_inner_inner
                }
        res = {"nbuffer_flag_al1":nbuffer_flag_al1,
                "compute_al1_axis":compute_al1_axis,
                "nbuffer_axis":nbuffer_axis,
                "k_outer_outer_inner":k_outer_outer_inner_size // nbuffer_size
        }
        return res

    def _get_fused_tensor(self, lop):
        if lop['op'] in self.inline_tensors or lop['op'] in self.ub_start_tensor:
            self._tensor_map[lop['op']] = lop['dst_buffer']
        elif lop['op'] == 'conv_vector_remove_pad':
            self._tensor_map[lop['op']] = lop['dst_buffer']

    def _bias_set_scope(self, lop):
        if lop['op'] == 'conv3d_bias_zn' or lop['op'] == 'conv3d_c_col_bias':
            self._schedule[lop["dst_buffer"]].set_scope(tbe_platform_info.scope_cc)
        elif lop['op'] == 'conv3d_bias_brdcst' or lop['op'] == 'conv3d_bias_align':
            self._schedule[lop["dst_buffer"]].set_scope(tbe_platform_info.scope_ubuf)
        elif lop['op'] == 'conv3d_bias_l1':
            self._schedule[lop["dst_buffer"]].set_scope(tbe_platform_info.scope_cbuf)
        elif lop['op'] == 'conv3d_bias_bt_conv3d_Input':
            self._schedule[lop["dst_buffer"]].set_scope(tbe_platform_info.scope_bt)

    def _cachebuffer(self, spec_node_list):
        """
        tensor not for conv set scope.

        Parameters
        ----------
        bodyops : body dict

        inputops : input dict

        spec_node_list : res tensor

        Returns
        -------

        """
        for lop in self.body_ops:
            if lop['op'] == 'fixpipe':
                continue
            self._get_fused_tensor(lop)
            if (("conv3d" not in lop["op"] or
                 (self.ub_fusion_flag and (lop["op"] == "conv3d_C"))) and
                lop['dst_buffer'] not in spec_node_list):
                self._schedule[lop["dst_buffer"]].set_scope(tbe_platform_info.scope_ubuf)
            else:
                self._bias_set_scope(lop)

        for lop in self.input_ops:  # not for A, B, DeqScale, ReqScale,
            if 'conv3d_bias_zero' in lop['op']:
                self._schedule[lop["dst_buffer"]].reused_by(self._tensor_map.get("bias_l1"))
                self._schedule[lop["dst_buffer"]].set_scope(tbe_platform_info.scope_cbuf)
                continue
            if "conv3d" in lop["op"]:
                continue
            if lop["next_op"][0]['op'] == 'fixpipe':
                continue
            if lop["next_op"][0]["dst_buffer"].op.tag == "NDHWC_trans_6HD":
                continue
            is_read_bias = ((self.ub_fusion_flag and lop['next_op'][0]['op'] == 'conv3d_bias_align')
                            or "bias_tensor" in lop["dst_buffer"].name) and ("bias_align" in self._tensor_map)
            is_bt_bias = lop['next_op'][0]['op'] == 'conv3d_bias_l1'
            if is_read_bias or is_bt_bias:
                continue
            tmp_read_map = []
            for nop in lop["next_op"]:
                tmp_read_map.append(nop["dst_buffer"])
            tmp_cache_buffer = self._schedule.cache_read(
                lop["dst_buffer"], tbe_platform_info.scope_ubuf,
                list(set(tmp_read_map)))
            lop["cache_buffer"] = tmp_cache_buffer

        return True

    def _tiling_l0a_l0b(self, partial_ab, full_c, instr):
        """
        reduce factor.

        Parameters
        ----------
        partial_ab : tiling["AL0_matrix"] or tiling["BL0_matrix"]

        full_c : tiling["CL0_matrix"]

        instr: "A" or "B"

        Returns
        -------
        axis_factor, reduce factor
        """
        reduce_dim = [
            self._dim_map["fmap_matrix_dim"][-3],
            self._dim_map["fmap_matrix_dim"][-1]
        ]

        if instr == 'A':
            full_ab = [full_c[-3], reduce_dim[-2], full_c[-2], reduce_dim[-1]]
        elif instr == 'B':
            full_ab = [reduce_dim[-2], full_c[-4], full_c[-1], reduce_dim[-1]]

        partial_ab = list(partial_ab) if partial_ab else full_ab
        i_axis = 0
        for i_axis in range(len(partial_ab))[::-1]:
            if partial_ab[i_axis] != full_ab[i_axis]:
                break

        axis_factor = {}
        reduce_factor = {}
        red_axis = 0

        if instr == 'A':
            axis_map_a2c = {0: 1, 2: 2}
            axis_factor = {axis_map_a2c[0]: full_ab[0]}
            reduce_factor[0] = full_ab[1]
            for i in range(i_axis + 1):
                if i in [0, 2]:
                    axis_factor[axis_map_a2c[i]] = partial_ab[i]
                else:
                    reduce_factor[red_axis] = partial_ab[i]
                    red_axis += 1
        elif instr == 'B':
            axis_map_b2c = {1: 0, 2: 3}
            axis_factor = {axis_map_b2c[1]: full_ab[1]}
            reduce_factor[0] = full_ab[0]
            for i in range(i_axis + 1):
                reduce_factor[red_axis] = partial_ab[i]
                red_axis += 1
        axis_factor_for_batch = {}
        for i in axis_factor:
            axis_factor_for_batch[i + 1] = axis_factor[i]

        return {
            "axis_factor": axis_factor_for_batch,
            "reduce_factor": reduce_factor
        }

    def _get_tiling_attach_flag(self, tiling):
        """
        get tiling attach flag
        """
        tiling["attach_at_flag"] = {}
        fmap_shape_ndc1hwc0 = conv3d_compute.Conv3DParam.tiling_info_dict["a_shape"]
        shape_w_ndc1hwc0 = conv3d_compute.Conv3DParam.tiling_info_dict["b_shape"]

        tiling["attach_at_flag"]["bl0_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_full_load")
        if tiling["BL0_matrix"]:
            tiling["attach_at_flag"]["bl0_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_equal")

        tiling["attach_at_flag"]["al1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_full_load")
        k_al1 = shape_w_ndc1hwc0[1] * fmap_shape_ndc1hwc0[2]
        if tiling["AL1_shape"]:
            k_al1 = tiling.get("AL1_shape")[0]
            tiling["attach_at_flag"]["al1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_less")
            if self._is_k_full_load(tiling.get("AL1_shape")[0], shape_w_ndc1hwc0[1] * fmap_shape_ndc1hwc0[2]):
                tiling["attach_at_flag"]["al1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_equal")

        if compute_util.get_or_res(self._is_full_load(tiling["BL1_shape"]),
                                   self._is_weight_not_via_l1(tiling["BL1_shape"])):
            k_bl1 = shape_w_ndc1hwc0[1] * fmap_shape_ndc1hwc0[2]
            tiling["attach_at_flag"]["bl1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_full_load")
            if self._is_weight_not_via_l1(tiling.get("BL1_shape")):
                tiling["attach_at_flag"]["bl1_attach_flag"] = self.binary_schedule.tiling_utils.get("no_attach")
        else:
            k_bl1 = tiling.get("BL1_shape")[0]
            tiling["attach_at_flag"]["bl1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_less")
            if self._is_k_full_load(tiling.get("BL1_shape")[0], shape_w_ndc1hwc0[1] * fmap_shape_ndc1hwc0[2]):
                tiling["attach_at_flag"]["bl1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_equal")

        tiling["attach_at_flag"]["abkl1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_equal")
        if k_bl1 < k_al1:
            tiling["attach_at_flag"]["abkl1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_full_load")
        elif k_bl1 > k_al1:
            tiling["attach_at_flag"]["abkl1_attach_flag"] = self.binary_schedule.tiling_utils.get("attach_less")
        return tiling

    def _tiling_fetch(self):
        """
        get tiling.

        Returns
        -------
        tiling
        """
        fmap_shape_ndc1hwc0 = conv3d_compute.Conv3DParam.tiling_info_dict["a_shape"]
        shape_w_ndc1hwc0 = conv3d_compute.Conv3DParam.tiling_info_dict["b_shape"]
        w_dtype = conv3d_compute.Conv3DParam.tiling_info_dict["b_dtype"]
        padd = conv3d_compute.Conv3DParam.tiling_info_dict["pad"][0:2]
        strided = conv3d_compute.Conv3DParam.tiling_info_dict["stride"][0]
        dilationd = conv3d_compute.Conv3DParam.tiling_info_dict["dilation"][0]
        dilationh = conv3d_compute.Conv3DParam.tiling_info_dict["dilation"][1]
        dilationw = conv3d_compute.Conv3DParam.tiling_info_dict["dilation"][2]
        cycle_buffer_flag = conv3d_compute.Conv3DParam.cycle_buffer_flag
        kernel_h = shape_w_ndc1hwc0[-3]
        kernel_w = shape_w_ndc1hwc0[-2]
        tiling_new = self.tiling_case
        if self.var_map and not tiling_new.get("default_tiling"):
            decode_tiling_v1_to_v2(tiling_new)
            if self.binary_mode:
                bl1_attach_flag = tiling_new.get("attach_at_flag")["bl1_attach_flag"]
                self.weight_gm2bl1_flag = bl1_attach_flag != self.binary_schedule.tiling_utils.get("no_attach")
                tiling = self.binary_schedule.config_cache_tiling(tiling_new)
                return tiling, cycle_buffer_flag

        l0a_load2d_flag = self._tensor_map["l0a_load2d_flag"]
        self._schedule.set_var_range(self._tensor_map["d_dim"],
                                     max(int(tiling_new.get("block_dim")[-2]), 1),
                                     max(int(tiling_new.get("block_dim")[-2]), 1))
        cycle_buffer_flag_param_dict = {'tiling_new': tiling_new, 'shape_w_ndc1hwc0': shape_w_ndc1hwc0,
                                    'w_dtype': w_dtype, 'fmap_shape_ndc1hwc0': fmap_shape_ndc1hwc0,
                                    'strided': strided, 'padd': padd, 'l0a_load2d_flag': l0a_load2d_flag,
                                    'dilationd': dilationd, 'dilationh': dilationh, 'dilationw': dilationw}
        cycle_buffer_flag = CceConv3dOp._get_cycle_buffer_flag(cycle_buffer_flag_param_dict)
        cyclebuffer = self._tensor_map["cycle_buffer_ori_flag"]
        specified_bound = int(cycle_buffer_flag)
        self._schedule.set_var_range(cyclebuffer, specified_bound, specified_bound)

        tiling = {}
        if tiling_new.get("default_tiling"):
            tiling = tiling_new
        else:
            tiling["AL0_matrix"] = tiling_new.get("AL0_matrix")[0:4]
            tiling["AL0_matrix"][
                1] = tiling.get("AL0_matrix")[1] * tiling_new.get("AL0_matrix")[-2]
            tiling["CL0_matrix"] = tiling_new["CL0_matrix"][0:4]
            tiling["L0C_OUTPUT_matrix"] = tiling_new.get("L0C_OUTPUT_matrix")[0:4]
            exponent_base = 2
            tiling["A_overhead_opt_flag"] = tiling_new.get("special_optimize_flag") % exponent_base
            tiling["B_overhead_opt_flag"] = tiling_new.get("special_optimize_flag") // exponent_base % exponent_base

            tiling["BL0_matrix"] = []
            if tiling_new["BL0_matrix"]:
                tiling["BL0_matrix"] = tiling_new["BL0_matrix"][0:4]
                tiling["BL0_matrix"][1] = tiling.get("BL0_matrix")[1] * tiling_new.get("BL0_matrix")[-2]

            tiling["manual_pingpong_buffer"] = tiling_new["manual_pingpong_buffer"]
            tiling["n_bef_batch_flag"] = 1 if tiling_new.get("control_reorder_flag") == 1 else 0

            tiling["AL1_shape"] = []
            self.tiling_para["al1_shape_c1"] = self._attrs_dict.get("cin_ori")
            if tiling_new["AL1_shape"]:
                tiling["AL1_shape"] = tiling_new["AL1_shape"][0:2]
                tiling["AL1_shape"][0] = tvm.floordiv(tiling.get("AL1_shape")[0] * tiling_new.get("AL1_shape")[-2],
                    (((kernel_h - 1) * dilationh + 1) * ((kernel_w - 1) * dilationw + 1) *
                    tbe_platform.CUBE_MKN[w_dtype]['mac'][1]))
                self.tiling_para["al1_shape_c1"] = tiling["AL1_shape"][0] // tiling_new.get("AL1_shape")[-2]

            if compute_util.get_or_res(self._is_full_load(tiling_new["BL1_shape"]),
                                    self._is_weight_not_via_l1(tiling_new["BL1_shape"])):
                tiling["BL1_shape"] = tiling_new["BL1_shape"]
            else:
                tiling["BL1_shape"] = tiling_new["BL1_shape"][0:2]
                bl1_shape = tiling["BL1_shape"]
                bl1_shape[0] = tvm.floordiv(tiling.get("BL1_shape")[0] * tiling_new.get("BL1_shape")[-2],
                    (kernel_h * kernel_w * tbe_platform.CUBE_MKN[w_dtype]['mac'][1]))

            tiling["block_dim"] = tiling_new["block_dim"]
            self.tiling_para["block_dim_batch"] = tiling.get("block_dim")[0]
            tiling["block_dim"] = [tiling.get("block_dim")[0] *
                                   tiling.get("block_dim")[-2]] + tiling.get("block_dim")[1:]
            tiling["scale_drq_split_flag"] = False
            tiling["bias_split_flag"] = False

        tiling = self._get_tiling_attach_flag(tiling)

        def _g_dim_tiling():
            # g_dim
            if (tiling_new.get("BUB_shape") is None
                    or tiling_new["BUB_shape"][0] is None
                    or tiling_new["BUB_shape"][0] == 0):
                tiling["g_dim"] = 1
            else:
                tiling["g_dim"] = tiling_new["BUB_shape"][0]

            # n_cub must be even, so real_g may only bind half cores.
            bind_half_gdim_flag = self.requant_multi_group_flag and \
                tiling["g_dim"] == self._tensor_map["group_dict"]["real_g"]
            if bind_half_gdim_flag:
                tiling["g_dim"] = compute_util.int_ceil_div(tiling["g_dim"], 2)

        _g_dim_tiling()

        return tiling, cycle_buffer_flag

    def _double_buffer(self, double_buffer_flag):
        """
        double buffer.

        Parameters
        ----------
        buffer_dict : al1/bl1 al0/bl0 c_col c_ub

        double_buffer_flag : flag for double buffer

        Returns
        -------

        """
        sch = self._schedule
        cycle_buffer_flag = self._tensor_map["cycle_buffer_flag"]
        # al1
        if double_buffer_flag["AL1_pbuffer"] == 2:
            if cycle_buffer_flag and not self.var_map:
                # cycle_double_buffer is not used in the dynamic shape
                sch[self.buffer_dict["al1"]].cycle_double_buffer()
            else:
                sch[self.buffer_dict["al1"]].double_buffer()
        # bl1
        if double_buffer_flag["BL1_pbuffer"] == 2:
            sch[self.buffer_dict["bl1"]].double_buffer()
        # l0a
        if double_buffer_flag["AL0_pbuffer"] == 2:
            sch[self.buffer_dict["fmap_col"]].double_buffer()
        # l0b
        if double_buffer_flag["BL0_pbuffer"] == 2:
            sch[self.buffer_dict["bl0"]].double_buffer()
        # L0C
        if double_buffer_flag["CL0_pbuffer"] == 2:
            sch[self.buffer_dict["c_col"]].double_buffer()
            if self.quant_bias_flag:
                sch[self._tensor_map["c_col_bias"]].double_buffer()
                sch[self._tensor_map["bias_zn"]].double_buffer()
                if not self.is_v200_version:
                    sch[self._tensor_map["bias_brdcst"]].double_buffer()
            if self.bias_table_flag:
                sch[self._tensor_map["bias_l1"]].double_buffer()
                bias_zero = self._tensor_map.get("bias_zero")
                if bias_zero is not None:
                    sch[bias_zero].double_buffer()
                sch[self._tensor_map["tensor_bt"]].double_buffer()

        # CUB
        if double_buffer_flag["L0C_OUTPUT_pbuffer"] == 2 and not self.quant_fused_flag and self._tensor_map.get(
                "c_ub") is not None:
            sch[self.buffer_dict["c_ub"]].double_buffer()
            if self.noquant_bias_flag:
                sch[self._tensor_map['bias_add_tensor']].double_buffer()

    def _condition_cycle_buffer_dynamic(self, cycle_buffer_dynamic_param_dict):
        cycle_buffer_flag = cycle_buffer_dynamic_param_dict.get('cycle_buffer_flag')
        al1 = cycle_buffer_dynamic_param_dict.get('al1')
        c_col = cycle_buffer_dynamic_param_dict.get('c_col')
        c_ub = cycle_buffer_dynamic_param_dict.get('c_ub')
        tiling = cycle_buffer_dynamic_param_dict.get('tiling')
        cin1_g = cycle_buffer_dynamic_param_dict.get('cin1_g')
        sch = self._schedule
        d_out = self._tensor_map["d_out"]
        pad_head = c_col.op.attrs['pad_head']
        stride_d = c_col.op.attrs['stride_d']
        kernel_d = self._attrs_dict['kernel_d']

        if cycle_buffer_flag:
            # set_store_predicate for cycle buffer
            _, dc_index = sch[al1].split(al1.op.axis[2], nparts=1)
            _, n_index = sch[al1].split(al1.op.axis[1], nparts=1)
            d_index = n_index % d_out * stride_d + (dc_index // cin1_g +
                        n_index % d_out * (kernel_d - stride_d)) % kernel_d - pad_head
            # condition_update is a refers to the conditions that
            # need to be updated during this load and the last load
            condition_update = d_index + pad_head > (n_index % d_out - 1) * \
                                stride_d + kernel_d - 1
            cyclebuffer_factor = CceConv3dOp._int_ceil_div_tvm(self._tensor_map.get("d_out"),
                                                               self._tensor_map.get("d_dim"))
            db_expr = tvm.select(tvm.convert(cyclebuffer_factor) == 1,
                                    0,
                                    tvm.floormod(n_index,
                                                 cyclebuffer_factor))
            # 1: Full load is required for the first load in a single core
            # 2: In the case of db, both ping and pong must be full loaded for the first time
            if tiling["manual_pingpong_buffer"]['AL1_pbuffer'] == 1:
                condition_db = (db_expr == 0).asobject()
            else:
                condition_db = tvm.any((db_expr == 0).asobject(),
                                        (db_expr == (cyclebuffer_factor + 1) // 2).asobject())
            condition_cycle = tvm.any(condition_update, condition_db)
            sch[al1].set_store_predicate(condition_cycle)

    def _intrin_mapping(self, intrin_mapping_param_dict):
        """
        intrin_mapping.

        Parameters
        ----------
        intrin_mapping_param_dict:

            famp : input tensor

            mad_dict : for mad pragma

            buffer_dict : al1/bl1 al0/bl0 c_col c_ub

            new_fmap_col_axis : fmap_col axis

            tiling : case tiling

            cn_axis : loc axis

            l0a_load2d_flag : true or false

        Returns
        -------

        """
        fmap = intrin_mapping_param_dict.get('fmap')
        mad_dict = intrin_mapping_param_dict.get('mad_dict')
        new_fmap_col_axis = intrin_mapping_param_dict.get('new_fmap_col_axis')
        tiling = intrin_mapping_param_dict.get('tiling')
        cn_axis = intrin_mapping_param_dict.get('cn_axis')
        l0a_load2d_flag = intrin_mapping_param_dict.get('l0a_load2d_flag')
        sch = self._schedule
        al1 = self.buffer_dict["al1"]
        bl1 = self.buffer_dict["bl1"]
        fmap_col = self.buffer_dict["fmap_col"]
        bl0 = self.buffer_dict["bl0"]
        c_col = self.buffer_dict["c_col"]
        c_ub = self.buffer_dict["c_ub"]

        cin1_g = self._tensor_map["group_dict"]["cin1_g"]
        setfmatrix_dict = {
            "conv_kernel_h": self._attrs_dict['kernel_h'],
            "conv_kernel_w": self._attrs_dict['kernel_w'],
            "conv_padding_top": self._attrs_dict['padding'][0],
            "conv_padding_bottom": self._attrs_dict['padding'][1],
            "conv_padding_left": self._attrs_dict['padding'][2],
            "conv_padding_right": self._attrs_dict['padding'][3],
            "conv_stride_h": self._attrs_dict['stride'][0],
            "conv_stride_w": self._attrs_dict['stride'][1],
            "conv_dilation_h": self._attrs_dict['dilation'][1],
            "conv_dilation_w":  self._attrs_dict['dilation'][2],
            "conv_fm_c": cin1_g * fmap.shape[5],
            "conv_fm_h": fmap.shape[3],
            "conv_fm_w": fmap.shape[4]
        }

        stride_d = c_col.op.attrs['stride_d']
        cycle_buffer_flag = self._tensor_map["cycle_buffer_flag"]
        kernel_d = self._attrs_dict['kernel_d']
        dilation_dhw = self._attrs_dict['dilation']

        def _al1_intrin_mapping():
            if cycle_buffer_flag and not self.var_map:
                # Specifies AL1 memory
                sch[al1].mem_unique()
                if self.fmap_trans_flag:
                    al1_shape_c1 = self.tiling_para.get("al1_shape_c1")
                    dc_outer, dc_inner = sch[al1].split(al1.op.axis[1], al1_shape_c1)
                    sch[al1].emit_insn(dc_inner, 'dma_copy', {"layout_transform": "nd2nz"})
                else:
                    sch[al1].emit_insn(al1.op.axis[1], 'dma_copy')
            elif self.var_map and not l0a_load2d_flag:
                self._condition_cycle_buffer_dynamic({'cycle_buffer_flag': cycle_buffer_flag,
                                                      'al1': al1,
                                                      'c_col': c_col,
                                                      'c_ub': c_ub,
                                                      'tiling': tiling,
                                                      'cin1_g': cin1_g
                                                     })
            elif self.var_map and l0a_load2d_flag:
                sch[al1].emit_insn(al1.op.axis[1], 'dma_copy')
            else:
                if self.fmap_trans_flag:
                    al1_shape_c1 = self.tiling_para.get("al1_shape_c1")
                    dc_outer, dc_inner = sch[al1].split(al1.op.axis[1], al1_shape_c1)
                    sch[al1].emit_insn(dc_inner, 'dma_copy', {"layout_transform": "nd2nz"})
                elif dilation_dhw[0] > 1:
                    sch[al1].emit_insn(al1.op.axis[2], 'dma_copy')
                else:
                    sch[al1].emit_insn(al1.op.axis[0], 'dma_copy')
        _al1_intrin_mapping()

        if l0a_load2d_flag:
            sch[fmap_col].emit_insn(new_fmap_col_axis[3], 'dma_copy', {'mem_align': 1})
        elif self.var_map:
            # The split c may have accuracy issues, so in not support l0c to out case, use complete c
            conv_fm_c = cin1_g * fmap.shape[5]
            conv_fm_c1 = cin1_g
            if self._support_l0c_to_out_flag:
                conv_fm_c = tiling.get("AL1_shape")[0] * fmap.shape[5]
                conv_fm_c1 = tiling.get("AL1_shape")[0]

            stride_update = 1 if self._tensor_map["opti_h_flag"] else self._attrs_dict['stride'][0]
            im2col_attr = {
                'set_fmatrix': 1,
                'conv_kernel_d': kernel_d,
                'conv_kernel_h': self._attrs_dict['kernel_h'],
                'conv_kernel_w': self._attrs_dict['kernel_w'],
                'conv_padding_top': self._attrs_dict['padding'][0],
                'conv_padding_bottom': self._attrs_dict['padding'][1],
                'conv_padding_left': self._attrs_dict['padding'][2],
                'conv_padding_right': self._attrs_dict['padding'][3],
                'conv_stride_d': stride_d,
                'conv_stride_h': stride_update,
                'conv_stride_w': self._attrs_dict['stride'][1],
                "conv_dilation_h": self._attrs_dict['dilation'][1],
                "conv_dilation_w":  self._attrs_dict['dilation'][2],
                'conv_fm_c': conv_fm_c,
                'conv_fm_c1': conv_fm_c1,
                'conv_fm_h': fmap.shape[3],
                'conv_fm_w': fmap.shape[4],
                'conv_fm_c0': fmap.shape[5],
                'group_flag': 1,
                'l1_group_flag': 1,
                'circular_buf': cycle_buffer_flag,
                'conv_batch': fmap_col.op.axis[1],
                'conv_intrin_batch': new_fmap_col_axis[-5],
                'conv_d_out': self._tensor_map.get("d_out"),
            }
            sch[fmap_col].emit_insn(new_fmap_col_axis[-5], 'im2col_v2', im2col_attr)
            sch[al1].emit_insn(al1.op.axis[3], 'dma_copy')
        else:
            if self._tensor_map["opti_h_flag"]:
                setfmatrix_dict["conv_stride_h"] = 1
            fmap_col_before = self.buffer_dict["fmap_col_before"]
            sch[fmap_col_before].emit_insn(fmap_col_before.op.axis[0],
                                           'set_fmatrix', setfmatrix_dict)
            sch[fmap_col].emit_insn(new_fmap_col_axis[-5], 'im2col')

        emit_insn_bl1 = self.weight_gm2bl1_flag
        if emit_insn_bl1:
            sch[bl1].emit_insn(sch[bl1].op.axis[0], 'dma_copy')
        sch[bl0].emit_insn(bl0.op.axis[0], 'dma_copy')
        if self.binary_mode:
            if emit_insn_bl1:
                sch[bl1].pragma(sch[bl1].op.axis[0], 'loop_with_no_overlap_tensor')
            sch[fmap_col].pragma(new_fmap_col_axis[-5], 'loop_with_no_overlap_tensor')
            sch[al1].pragma(al1.op.axis[3], 'loop_with_no_overlap_tensor')
            sch[bl0].pragma(bl0.op.axis[0], 'loop_with_no_overlap_tensor')

        if not self.quant_fused_flag and c_ub is not None:
            sch[c_ub].emit_insn(c_ub.op.axis[0], 'dma_copy')
        sch[c_col].emit_insn(cn_axis, 'mad', mad_dict)

    def _handle_requant(self, data_transfer, compute_at_buffer, compute_at_axis):
        reform_outer, reform_inner = self._schedule[data_transfer].split(data_transfer.op.axis[-1], nparts=2)
        self._schedule[data_transfer].compute_at(self._schedule[compute_at_buffer[1]], compute_at_axis[1])
        if 'requant_vector' in self._tensor_map:
            requant_tensor = self._tensor_map.get('requant_vector')
        else:
            requant_tensor = self._tensor_map.get('requant_scale')
        config = tbe_platform.CUBE_MKN[requant_tensor.dtype]
        if len(data_transfer.op.axis) == 5:
            self._schedule[data_transfer].reorder(data_transfer.op.axis[0], data_transfer.op.axis[1],
                                                  data_transfer.op.axis[2], reform_outer, data_transfer.op.axis[3],
                                                  reform_inner)
            self._schedule[data_transfer].buffer_align((1, 1), (1, 1), (1, 1), (1, config["mac"][0]),
                                                       (1, config["mac"][2]))
        else:
            self._schedule[data_transfer].reorder(data_transfer.op.axis[0], data_transfer.op.axis[1], reform_outer,
                                                  data_transfer.op.axis[2], reform_inner)
            self._schedule[data_transfer].buffer_align((1, 1), (1, 1), (1, config["mac"][0]), (1, config["mac"][2]))

    def _attach_at(self, compute_at_buffer, compute_at_axis, tiling):
        """
        tensor not for conv compute at.

        Parameters
        ----------

        compute_at_buffer : col res_c

        compute_at_axis : axis for compute at

        tiling : case tiling

        Returns
        -------

        """
        tiling_mc = tiling["CL0_matrix"][1] * tiling["CL0_matrix"][2] // self.load3d_special_multiply

        for lop in self.body_ops:
            if "conv3d" not in lop["op"] or "convolution_A" in lop["op"] or \
                (self.ub_fusion_flag and (lop["op"] == "conv3d_C")):
                if lop["op"] == "conv_vector_remove_pad":
                    continue
                if lop['op'] == 'data_transfer':
                    self._handle_requant(lop['dst_buffer'], compute_at_buffer, compute_at_axis)
                    continue
                if lop["op"] == "fixpipe_reform" or lop["op"] == "fixpipe":
                    continue
                self._schedule[lop["dst_buffer"]].compute_at(
                    self._schedule[compute_at_buffer[1]],
                    compute_at_axis[1])
                self._schedule[lop["dst_buffer"]].buffer_align(
                    (1, 1), (1, 1),
                    (1, tiling_mc),
                    (1, 1))
            elif lop['op'] in ('conv3d_bias_zn', 'conv3d_c_col_bias', 'conv3d_bias_brdcst', 'conv3d_bias_align'):
                self._schedule[lop['dst_buffer']].compute_at(self._schedule[compute_at_buffer[1]], compute_at_axis[0])

        for lop in self.input_ops:
            if "conv3d" in lop["op"]:
                continue
            if lop["next_op"][0]['op'] == 'fixpipe':
                continue
            if lop["next_op"][0]["dst_buffer"].op.tag == "NDHWC_trans_6HD":
                continue
            is_read_bias = ((self.ub_fusion_flag and lop['next_op'][0]['op'] == 'conv3d_bias_align')
                            or "bias_tensor" in lop["dst_buffer"].name) and ("bias_align" in self._tensor_map)
            is_bt_bias = lop['next_op'][0]['op'] == 'conv3d_bias_l1'
            if is_read_bias or is_bt_bias:
                continue
            self._schedule[lop["cache_buffer"]].compute_at(self._schedule[compute_at_buffer[0]], compute_at_axis[1])

    def _c_col_buffer_tile(self, tiling):
        """
        do l0c n axis buffer tile
        """
        if self.binary_mode:
            cout1_g = get_te_var("cout1_g").get_tvm_var()
            nl0 = tiling.get("CL0_matrix")[0]
            c_outer_outer_outer_inner_offset = self.axis_dict.get("c_outer_outer_outer_inner") * \
                tvm.floordiv(tvm.floordiv(cout1_g, nl0), self.tiling_para.get("bl1_factor")[1]) * nl0
            c_outer_outer_inner_offset = self.axis_dict.get("c_outer_outer_inner") * nl0
            c_outer_outer_outer_outer_offset = self.axis_dict.get("c_outer_outer_outer_outer") * \
                tvm.floordiv(tvm.floordiv(cout1_g, nl0), self.tiling_para.get("bl1_factor")[1]) * \
                tvm.floordiv(self.tiling_para.get("bl1_factor")[1], self.tiling_para.get("block_dim_n")) * nl0
            c_offset = c_outer_outer_outer_inner_offset + c_outer_outer_inner_offset + c_outer_outer_outer_outer_offset
            c_extnd = nl0
            c_col = self.buffer_dict.get("c_col")
            self._schedule[c_col].buffer_tile((None, None), (None, None), (c_offset, c_extnd),
                                              (None, None), (None, None), (None, None), (None, None))
        group_dict = self._tensor_map.get("group_dict")
        real_g = group_dict.get("real_g")
        c_outer_g_inner_extend = real_g // tiling.get('g_dim')
        if self.buffer_dict.get("res_c").op.name == "fixpipe_channel_merge" and c_outer_g_inner_extend > 1:
            c_col = self.buffer_dict.get("c_col")
            g_offset = self.axis_dict.get('c_outer_g_outer') * c_outer_g_inner_extend + \
                self.axis_dict.get('c_outer_g_inner') * 1
            g_extend = 1
            self._schedule[c_col].buffer_tile((g_offset, g_extend), (None, None), (None, None),
                                              (None, None), (None, None), (None, None), (None, None))


    def _bias_intrin_mapping(self, lop):
        if lop['op'] == 'conv3d_bias_zn':
            self._schedule[lop['dst_buffer']].reused_by(self._tensor_map['c_col_bias'], self._tensor_map['c_col'])
            if 'bias_brdcst' in self._tensor_map:
                self._schedule[lop['dst_buffer']].split(lop['dst_buffer'].op.axis[3], 16)
                self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[2], 'dma_copy')
            else:
                self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[1], 'dma_copy')
        elif lop['op'] == 'conv3d_c_col_bias':
            self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[0], 'phony_insn')
        elif lop['op'] == 'conv3d_bias_brdcst':
            self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[1], 'vector_auto')
        elif lop['op'] == 'conv3d_bias_align':
            self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[0], 'dma_copy')
        elif lop['op'] == 'conv3d_bias_l1':
            self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[0], 'dma_copy',
                                                            {"layout_transform": "nd2nz"})
        elif lop['op'] == 'conv3d_bias_bt_conv3d_Input':
            self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[0], 'dma_copy', {'mem_align': 1})

    def _to_pragma(self, bodyops, inputops, res_axis):
        """
        tensor not for conv to pragma.

        Parameters
        ----------
        bodyops : body dict

        inputops : input dict

        fmap : input tensor

        c_ub : conv res in ub

        res_axis : res axis

        Returns
        -------

        """
        for lop in bodyops:
            if "conv3d" not in lop["op"] or "conv3d_A" in lop["op"] or \
                (self.dsl_flag and self._tensor_map.get("fixpipe_tensor") is None and lop["op"] == "conv3d_C"):
                lop["tensorize_axis"] = self._schedule[
                    lop["dst_buffer"]].op.axis[0]
                if "Before" in lop["op"]:
                    lop["op"] = lop["op"].replace("_Before", "")
                if "_conv3d_A" in lop["op"]:
                    lop["op"] = lop["op"].replace("_conv3d_A", "")
                self.__pragma_for_op(lop, res_axis)
            else:
                self._bias_intrin_mapping(lop)

        for lop in inputops:
            if 'conv3d_bias_zero' in lop['op']:
                self._schedule[lop['dst_buffer']].emit_insn(lop['dst_buffer'].op.axis[0], 'set_2d')
                continue
            if "conv3d" in lop["op"]:
                continue
            if lop["next_op"][0]['op'] == 'fixpipe':
                continue
            if lop["next_op"][0]["dst_buffer"].op.tag == "NDHWC_trans_6HD":
                continue
            is_read_bias = ((self.ub_fusion_flag and lop['next_op'][0]['op'] == 'conv3d_bias_align')
                            or "bias_tensor" in lop["op"]) and ("bias_align" in self._tensor_map)
            is_bt_bias = lop['next_op'][0]['op'] == 'conv3d_bias_l1'
            if is_read_bias or is_bt_bias:
                continue
            self._schedule[lop["cache_buffer"]].emit_insn(lop["cache_buffer"].op.axis[0], 'dma_copy')

    def _set_al1_at_axis(self, set_al1_at_axis_param_dict):
        """
        al1 compute_at.

        Parameters
        ----------
        set_al1_at_axis_param_dict:

            l0a_load2d_flag : true or false

            nbuffer_flag_al1 : true or false

            reorder_flag : true or false

            tiling : case tiling

            compute_axis : al1 axis to compute at

            allocate_axis : al1 axis to allocate at

            index_axis : al1 index to stage

            buffer_dict : al1/bl1 al0/bl0 c_col c_ub

            stage : c_col res_c

        Returns
        -------

        """
        l0a_load2d_flag = set_al1_at_axis_param_dict.get('l0a_load2d_flag')
        nbuffer_flag_al1 = set_al1_at_axis_param_dict.get('nbuffer_flag_al1')
        reorder_flag = set_al1_at_axis_param_dict.get('reorder_flag')
        tiling = set_al1_at_axis_param_dict.get('tiling')
        al1_factor = self.tiling_para.get("al1_factor")
        compute_axis = set_al1_at_axis_param_dict.get('compute_al1_axis')
        allocate_axis = set_al1_at_axis_param_dict.get('allocate_al1_axis')
        index_axis = set_al1_at_axis_param_dict.get('index_al1_dict')
        stage = set_al1_at_axis_param_dict.get('stage')
        cycle_buffer_flag = self._tensor_map["cycle_buffer_flag"]
        sch = self._schedule
        al1_allocate_axis = None
        al1_run_once_axis = []
        compute_stage = None
        allocate_stage = None
        al1 = self.buffer_dict["al1"]
        index_map = {self.binary_schedule.tiling_utils.get("attach_full_load"): 2,
                        self.binary_schedule.tiling_utils.get("attach_equal"): 1,
                        self.binary_schedule.tiling_utils.get("attach_less"): 0}
        index_map_key = set_al1_at_axis_param_dict.get('tiling').get("attach_at_flag").get("al1_attach_flag")
        index = index_map.get(index_map_key)
        if not l0a_load2d_flag and not self.var_map:
            fmap_col_before = self.buffer_dict["fmap_col_before"]
        if compute_util.get_and_res(l0a_load2d_flag, nbuffer_flag_al1):
            al1_compute_axis = compute_axis["k_outer_outer_inner_outer"]
            compute_stage = stage[0]
            al1_allocate_axis = allocate_axis[index_axis[index]]
            allocate_stage = stage[index]
            run_flag = compute_util.get_and_res(index == 1, reorder_flag)
            if compute_util.get_or_res(run_flag, index == 2):
                al1_run_once_axis = [
                    self.axis_dict.get("c_outer_outer_inner"),
                    self.axis_dict.get("c_outer_outer_outer_inner")
                ]
        elif nbuffer_flag_al1:
            if index == 0:
                al1_compute_axis = compute_axis["k_outer_outer_inner_outer"]
                compute_stage = stage[0]
                al1_allocate_axis = allocate_axis[index_axis[0]]
                allocate_stage = stage[0]
            else:
                al1_compute_axis = compute_axis[index_axis[index]]
                compute_stage = stage[index]
        else:
            al1_compute_axis = compute_axis[index_axis[index]]
            compute_stage = stage[index]

        if l0a_load2d_flag:
            sch[al1].compute_at(sch[compute_stage], al1_compute_axis)
        else:
            sch[al1].compute_at(sch[compute_stage], al1_compute_axis)
            if not self.var_map:
                sch[fmap_col_before].compute_at(sch[compute_stage],
                                                al1_compute_axis)

        if not self.var_map:
            do_num = al1.op.axis[0].dom.extent.value
            if compute_util.get_and_res(cycle_buffer_flag, do_num != 1):
                cyclebuffer_factor = self._tensor_map["d_out"] // self._tensor_map["d_dim"]
                expr = tvm.select(tvm.convert(cyclebuffer_factor) == 1,
                                  al1.op.axis[0].var,
                                  tvm.floormod(al1.op.axis[0].var,
                                               cyclebuffer_factor))
                sch[al1].pragma(al1.op.axis[0],
                                "cyclebuffer",
                                (expr == 0).asobject())

            if al1_run_once_axis:
                sch[al1].allocate_at(sch[allocate_stage],
                                     al1_allocate_axis,
                                     run_once_axes=al1_run_once_axis)
            elif al1_allocate_axis is not None:
                sch[al1].allocate_at(sch[allocate_stage], al1_allocate_axis)

        return True

    def _set_bl1_at_axis(self, bl1_at_axis_param_dict):
        """
        bl1 compute_at.

        Parameters
        ----------
        bl1_at_axis_param_dict:

            reorder_flag : true or false

            tiling : case tiling

            compute_bl1_axis : bl1 axis to compute at

            allocate_bl1_axis : bl1 axis to allocate at

            bl1_index_axis : bl1 index to stage

            stage : c_col res_c

        Returns
        -------

        """
        reorder_flag = bl1_at_axis_param_dict.get('reorder_flag')
        tiling = bl1_at_axis_param_dict.get('tiling')
        bl1_factor = self.tiling_para.get("bl1_factor")
        compute_bl1_axis = bl1_at_axis_param_dict.get('compute_bl1_axis')
        allocate_bl1_axis = bl1_at_axis_param_dict.get('allocate_bl1_axis')
        bl1_index_dict = bl1_at_axis_param_dict.get('bl1_index_dict')
        stage = bl1_at_axis_param_dict.get('stage')

        index_map = {self.binary_schedule.tiling_utils.get("attach_full_load"): 2,
                        self.binary_schedule.tiling_utils.get("no_attach"): 2,
                        self.binary_schedule.tiling_utils.get("attach_equal"): 1,
                        self.binary_schedule.tiling_utils.get("attach_less"): 0}
        index_map_key = tiling.get("attach_at_flag").get("bl1_attach_flag")
        index = index_map.get(index_map_key)

        sch = self._schedule
        bl1_compute_axis = None
        bl1_allocate_axis = None
        bl1_run_once_axis = []
        bl1_compute_stage = None
        bl1_allocate_stage = None
        bl1 = self.buffer_dict["bl1"]
        if tiling["B_overhead_opt_flag"]:
            if index == 0 or (index == 1 and reorder_flag):
                bl1_compute_axis = compute_bl1_axis["coo"]
                bl1_compute_stage = stage[0]
                bl1_allocate_axis = allocate_bl1_axis[bl1_index_dict[index]]
                bl1_allocate_stage = stage[index]
                if index == 1 and reorder_flag:
                    bl1_run_once_axis = [
                        self.axis_dict.get("m_outer_outer_inner")
                    ]
            else:
                bl1_compute_axis = compute_bl1_axis[bl1_index_dict[index]]
                bl1_compute_stage = stage[index]
        else:
            bl1_compute_axis = compute_bl1_axis[bl1_index_dict[index]]
            bl1_compute_stage = stage[index]

        sch[bl1].compute_at(sch[bl1_compute_stage], bl1_compute_axis)
        if bl1_run_once_axis:
            sch[bl1].allocate_at(sch[bl1_allocate_stage],
                                 bl1_allocate_axis,
                                 run_once_axes=bl1_run_once_axis)
        elif bl1_allocate_axis is not None:
            sch[bl1].allocate_at(sch[bl1_allocate_stage], bl1_allocate_axis)

        # bias l1/bias zero same attach with bl1
        if isinstance(self._tensor_map.get("bias_l1"), tvm.Tensor):
            bias_l1 = self._tensor_map.get("bias_l1")
            sch[bias_l1].compute_at(sch[bl1_compute_stage], bl1_compute_axis)
        if isinstance(self._tensor_map.get("bias_zero"), tvm.Tensor):
            bias_zero = self._tensor_map.get("bias_zero")
            sch[bias_zero].compute_at(sch[bl1_compute_stage], bl1_compute_axis)

        return True

    def _set_var_range(self):
        if self.binary_mode:
            self.binary_schedule.set_tiling_var_range()
            self.binary_schedule.set_shape_var_range()
            return
        # dynamic shape mode
        if "fmap_d" in self.var_map:
            self._schedule.set_var_range(self.var_map.get("fmap_d"), *self.var_range.get('fmap_d'))
            self._schedule.set_var_range(self.var_map.get("d_out"), *self.var_range.get('d_out'))
        if "fmap_h" in self.var_map:
            self._schedule.set_var_range(self.var_map.get("fmap_h"), *self.var_range.get('fmap_h'))
            self._schedule.set_var_range(self.var_map.get("h_out"), *self.var_range.get('h_out'))
        if "fmap_w" in self.var_map:
            self._schedule.set_var_range(self.var_map.get("fmap_w"), *self.var_range.get('fmap_w'))
            self._schedule.set_var_range(self.var_map.get("w_out"), *self.var_range.get('w_out'))
        if "batch_n" in self.var_map:
            self._schedule.set_var_range(self.var_map.get("batch_n"), *self.var_range.get('batch_n'))

    def _get_requant_multi_group(self):
        cout1_g = self._tensor_map['group_dict']['cout_g'] // _TILING_FLOAT16_MKN
        real_g = self._tensor_map['group_dict']['real_g']
        self.requant_multi_group_flag = (cout1_g % 2 == 1 and real_g > 1 and self._res_tensor.dtype == 'int8')

    def _quant_intrin_mapping(self, lop):
        cache_buffer = lop["dst_buffer"]
        if lop['op'] in ('dequant_remove_pad', 'requant_remove_pad'):
            self._schedule[cache_buffer].compute_inline()
        elif lop['op'] in ('requant_vector', 'requant_scale'):
            self._schedule[cache_buffer].compute_inline()
        elif lop['op'] == 'data_transfer':
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[2], 'dma_copy')
        elif lop['op'] in ('dequant_vector', 'dequant_scale'):
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[2], 'dma_copy')
        elif lop['op'] == 'dequant1_vector':
            self._schedule[cache_buffer].pragma(self._schedule[cache_buffer].op.axis[2], "deq_scale", 'vector')
        elif lop['op'] == 'dequant1_scale':
            self._schedule[cache_buffer].pragma(self._schedule[cache_buffer].op.axis[0], "deq_scale", 'scalar')
        elif lop["op"] in ('dequant2_vector', 'dequant2_scale'):
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[0], "vector_auto")

    def __pragma_for_op(self, lop, res_axis=None):
        # for not in conv op pragma
        op_cmd = lop["op"].split("_")
        cache_buffer = lop["dst_buffer"]
        tensorize_axis = lop["tensorize_axis"]

        if op_cmd[0].lower() == "elewise":
            ele_instr = CceConv3dOp._get_elmwise_instr(lop["op"])
            self._schedule[cache_buffer].emit_insn(tensorize_axis, ele_instr)
        elif self.ub_fusion_flag and lop["op"] == 'conv3d_C':
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[0], 'dma_copy')
        elif not self.ub_fusion_flag and lop["op"] == 'conv3d_C':
            self._schedule[cache_buffer].emit_insn(res_axis, 'dma_copy')
        elif lop["op"] == "NDHWC_trans_6HD":
            self._schedule[cache_buffer].compute_inline()
        elif lop["op"] == "fixpipe_reform":
            if lop["dst_buffer"].op.attrs.get("6HD_TRANS_NDHWC"):
                self._schedule[cache_buffer].emit_insn(res_axis, 'fixpipe_op', {"layout_transform" : "nz2nd"})
            elif lop["dst_buffer"].op.name == "fixpipe_channel_merge":
                _, c0_inner = self._schedule[cache_buffer].split(self._schedule[cache_buffer].op.axis[-1], 16)
                # eliminate non-linearity caused by channel merge
                self._schedule[cache_buffer].pragma(c0_inner, "constraint",
                (self._schedule[cache_buffer].op.axis[1].var*2) \
                    + tvm.floordiv(self._schedule[cache_buffer].op.axis[-1].var, 16) \
                < CceConv3dOp._int_ceil_div_tvm(conv3d_compute.Conv3DParam.tiling_info_dict["b_shape"][0], 16))
                self._schedule[cache_buffer].emit_insn(res_axis, 'fixpipe_op')
            else:
                self._schedule[cache_buffer].emit_insn(res_axis, 'fixpipe_op')
        elif lop["op"] == 'conv_vector_remove_pad':
            self._schedule[cache_buffer].emit_insn(res_axis, 'dma_copy')
            if self.binary_mode:
                self._schedule[cache_buffer].pragma(res_axis, 'loop_with_no_overlap_tensor')
        elif lop["op"] == 'conv_vector_bias_add':
            self._schedule[cache_buffer].emit_insn(tensorize_axis,
                                                   "vector_add")
        elif lop["op"] == 'broadcast_for_tensor':
            self._schedule[cache_buffer].emit_insn(tensorize_axis,
                                                   "vector_auto")
        elif lop["op"] == 'mean_matrix_init':
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[-1],
                                                   "vector_dup")
        elif lop["op"] == 'mean_matrix_fp16':
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[0],
                                                   "vector_auto")
        elif lop["op"] == 'mean_matrix_mul':
            self._schedule[cache_buffer].emit_insn(self._schedule[cache_buffer].op.axis[-1],
                                                   "vector_auto")
        else:
            self._quant_intrin_mapping(lop)

    def _show_flags(self):
        """
        show all flags
        """
        log.info("[{}] ub_fusion_flag {} dsl_flag = {} bias_table_flag = {} ".format(
            self._tensor_map["kernel_name"], self.ub_fusion_flag, self.dsl_flag, self.bias_table_flag))
        log.info("[{}] quant_bias_flag = {} quant_fused_flag = {} noquant_bias_flag = {} ".format(
            self._tensor_map["kernel_name"], self.quant_bias_flag, self.quant_fused_flag, self.noquant_bias_flag))


class AutoScheduleDict(dict):
    """
    AutoScheduleDict
    """

    def __init__(self, **kwargs):
        super(AutoScheduleDict, self).__init__(**kwargs)
        self.read_only = False


class AutoScheduleOp:
    """
    AutoScheduleOp
    """

    def __init__(self, *init_args):
        if len(init_args) == 1 and isinstance(init_args[0], tvm.Tensor):
            res_tensor = init_args[0]
            self._color_count = 0
            self._op = []
            self.body_ops = []
            self.input_ops = []
            self.output_ops = []
            self._res_tensor = res_tensor
            self._before_conv_flag = False
            self.__scrapy_tensor_graph(self._res_tensor)
            self.__connect_op()
            self._end_op = self._get_op_by_tensor(self._res_tensor)
            self._end_op["color"] = self._color_count
            self.__init_color(self._end_op)
            self.__analyse_input_output()

    def __split_tensor(self, tensor):
        tmp_op = AutoScheduleDict()
        operator = tensor.op
        if hasattr(operator, "tag"):
            if operator.tag == "":
                tmp_op["op"] = operator.name
            else:
                tmp_op["op"] = operator.tag
        if tmp_op["op"].find("|") != -1:
            str_list = operator.tag.split("|")
            tmp_op["op"] = str_list[0]
        if hasattr(tensor, "tag"):
            tmp_op["op"] = tmp_op["op"] + "_" + tensor.tag
        tmp_op["dst_buffer"] = tensor
        tmp_op["src_buffer"] = list(operator.input_tensors)

        if "conv3d_A" in tmp_op["op"]:
            self._before_conv_flag = True
        if self._before_conv_flag:
            tmp_op["op"] = tmp_op["op"] + "_Before"

        return tmp_op

    def __scrapy_tensor_graph(self, res_tensor):
        operation_list = [res_tensor]
        while operation_list:
            tmp_operation_list = []
            for operation in operation_list:
                tmp_op = self.__split_tensor(operation)
                self._op.append(tmp_op)
                for i in tmp_op["src_buffer"]:
                    i.next = operation
                    operation.prev = i
                    if i not in tmp_operation_list:
                        tmp_operation_list.append(i)
                    if tmp_op["op"] == "conv3d_c_col":
                        i.tag = "conv3d_Input"
                    if tmp_op["op"] == "conv3d_fuse_fmap_tensor":
                        i.tag = "conv3d_A"
                    if tmp_op["op"] == "conv3d_al1_load2d":
                        i.tag = "conv3d_A"
            operation_list = tmp_operation_list

    def __connect_op(self):
        for lop in self._op:
            lop["prev_op"] = []
            lop["next_op"] = []

        for lop in self._op:
            prev_op = lop.get("prev_op")
            for src_tensor in lop.get("src_buffer"):
                tmp_op = self._get_op_by_tensor(src_tensor)
                tmp_op_next_op = tmp_op.get("next_op")
                prev_op.append(tmp_op)
                tmp_op_next_op.append(lop)

    def __init_color(self, start_op):
        for p_op in start_op.get("prev_op"):
            p_op["color"] = start_op.get("color")
            self.__init_color(p_op)

    def _get_op_by_tensor(self, src_tensor):
        """
        get op by source tensor

        Parameters
        ----------
        src_tensor: the source tensor

        Returns
        -------
        tensor : op
        """
        for i in self._op:
            if i["dst_buffer"].same_as(src_tensor):
                return i
        return {}

    def __analyse_input_output(self):
        spec_body_ops = {"mean_matrix_init"}
        input_ops = []
        output_ops = []
        body_ops = []
        input_tensor_name = []
        body_tensor_name = []
        for lop in self._op:
            if not lop["prev_op"] and lop["op"] not in spec_body_ops:
                lop["color"] = -1
                if lop["dst_buffer"].name not in input_tensor_name:
                    input_ops.append(lop)
                    input_tensor_name.append(lop["dst_buffer"].name)
                else:
                    continue
            else:
                if lop["dst_buffer"].name not in body_tensor_name:
                    body_ops.append(lop)
                    body_tensor_name.append(lop["dst_buffer"].name)
                else:
                    continue
                if not lop["next_op"]:
                    output_ops.append(lop)

        for i in input_ops:
            i["color"] = -1
        self.input_ops = input_ops
        self.output_ops = output_ops
        self.body_ops = body_ops


class BinaryDynamic:
    """
    special for dynamic binary
    """
    def __init__(self, sch, binary_mode):
        self.tiling_utils = BinaryUtil.TILING_UTILS
        self._tiling_range = BinaryUtil.TILING_RANGE
        self._sch = sch
        self.cache_tiling = {}
        self._binary_mode = binary_mode
        self.k_full_load_list = (self.tiling_utils.get("attach_full_load"), self.tiling_utils.get("attach_equal"))
        self.binary_nparts_facotr = {}
        self._binary_tiling_data = None

    @staticmethod
    def _get_optional_te_var(var_name):
        """get optional te var"""
        return None if not get_te_var(var_name) else get_te_var(var_name).get_tvm_var()

    def set_shape_var_range(self):
        """
        set var range for cache tiling vars and shape vars
        """
        range_shape = self._tiling_range.get("range_shape")
        range_pad = self._tiling_range.get("range_pad")
        range_stride = self._tiling_range.get("range_stride")
        range_kernel = self._tiling_range.get("range_kernel")
        range_dilation = self._tiling_range.get("range_dilation")
        range_c = self._tiling_range.get("range_c")
        range_c1 = self._tiling_range.get("range_c1")
        range_max = self._tiling_range.get("range_max")
        range_hf32 = self._tiling_range.get("range_bool")
        shape_var_range = {
            "batch_n": range_shape,
            "fmap_c": range_c,
            "fmap_d": range_max,
            "fmap_h": range_shape,
            "fmap_w": range_shape,
            "c_out": range_c,
            "d_out": range_max,
            "h_out": range_shape,
            "w_out": range_shape,
            "kernel_d": range_kernel,
            "kernel_h": range_kernel,
            "kernel_w": range_kernel,
            "fmap_c1": range_c1,
            "c1_out": range_c1,
            "stride_d": range_stride,
            "stride_h": range_stride,
            "stride_w": range_stride,
            "padf": range_pad,
            "padb": range_pad,
            "padu": range_pad,
            "padd": range_pad,
            "padl": range_pad,
            "padr": range_pad,
            "dilation_d": range_dilation,
            "dilation_h": range_dilation,
            "dilation_w": range_dilation,
            "cout1_g": range_c1,
            "cin1_g": range_c1,
            "real_g": range_shape,
            "mag_factor": range_shape,
            "hf32_flag": range_hf32,
        }
        for var, var_range in shape_var_range.items():
            self._sch.set_var_range(get_te_var(var).get_tvm_var(), *var_range)

    def set_tiling_var_range(self):
        """set tiling var range"""
        range_shape = self._tiling_range.get("range_shape")
        range_outer_axis = self._tiling_range.get("range_outer_axis")
        range_l0_ub_param = self._tiling_range.get("range_l0_ub_param")
        range_l1 = self._tiling_range.get("range_l1")
        range_bt = self._tiling_range.get("range_bt")
        range_block_dim = self._tiling_range.get("range_block_dim")
        tiling_var_range = {
            "group_dim": range_block_dim,
            "batch_dim": range_block_dim,
            "d_dim": range_block_dim,
            "n_dim": range_block_dim,
            "m_dim": range_block_dim,
            "batch_dout_single_core": range_shape,
            "m_single_core": range_outer_axis,
            "n_single_core": range_outer_axis,
            "m_al1": range_outer_axis,
            "n_bl1": range_outer_axis,
            "cub_n1": range_l0_ub_param,
            "m_l0": range_l0_ub_param,
            "k_l0": range_l0_ub_param,
            "n_ub_l0_time": range_l0_ub_param,
            "kal1_factor": range_outer_axis,
            "kbl1_factor": range_outer_axis,
            "kal0_factor": range_outer_axis,
            "kbl0_factor": range_outer_axis,
            "al1_bound": range_l1,
            "bt_bound": range_bt,
            "kl1_times": self._tiling_range.get("range_kl1_times"),
            "load3d_special": self._tiling_range.get("range_load3d_special"),
            }

        for var, var_range in tiling_var_range.items():
            self._sch.set_var_range(get_te_var(var).get_tvm_var(), *var_range)

    def update_tiling_nparts_factor(self):
        """
        get tiling_nparts in binary dynamic scene
        """
        return self.binary_nparts_facotr

    def config_cache_tiling(self, tiling):
        """
        config base tiling information for cache tiling
        """
        self._binary_tiling_data = tiling.get("binary_tiling_data")
        load3d_special = get_te_var("load3d_special").get_tvm_var()
        self._get_cache_tiling()
        tiling = self._tiling_infer_norange(tiling)
        self.binary_nparts_facotr["c_tiling_factor"] = [tiling.get('CL0_matrix')[0],
                                                        tvm.floordiv(tiling.get('CL0_matrix')[1] *
                                                                     tiling.get('CL0_matrix')[2], load3d_special)]
        self.binary_nparts_facotr["c_factor"] = [
            self.cache_tiling.get("n_single_core") * self.cache_tiling.get("n_bl1") * self.cache_tiling.get("n_dim"),
            self.cache_tiling.get("m_single_core") * self.cache_tiling.get("m_al1") * self.cache_tiling.get("m_dim")]
        self.binary_nparts_facotr["c_ub_factor"] = [self.cache_tiling.get("n_ub_l0_time"), 1]
        self.binary_nparts_facotr["al1_factor"] = [self.cache_tiling["kal1_factor"],
                                                   self.cache_tiling["m_single_core"] * self.cache_tiling["m_dim"]]
        self.binary_nparts_facotr["bl1_factor"] = [self.cache_tiling["kbl1_factor"],
                                                   self.cache_tiling["n_single_core"] * self.cache_tiling["n_dim"]]
        self.binary_nparts_facotr["al0_axis_factor"] = {"axis_factor": {2: self.cache_tiling.get("m_l0")},
                                                        "reduce_factor": {0: self.cache_tiling.get("k_l0")}}
        self.binary_nparts_facotr["bl0_axis_factor"] = {"axis_factor": {1: tiling.get('BL0_matrix')[1]},
                                                        "reduce_factor": {0: self.cache_tiling.get("k_bl0")}}
        self.binary_nparts_facotr["cout1_g"] = get_te_var("cout1_g").get_tvm_var()
        self.binary_nparts_facotr["batch_do_single_core"] = self.cache_tiling["batch_dout_single_core"]
        self.binary_nparts_facotr["block_dim_batch_do"] = self.cache_tiling["batch_dim"] * self.cache_tiling["d_dim"]
        self.binary_nparts_facotr["block_dim_batch"] = self.cache_tiling["batch_dim"]

        kernel_d, kernel_h, kernel_w = get_te_var("kernel_d").get_tvm_var(), \
                                       get_te_var("kernel_h").get_tvm_var(), get_te_var("kernel_w").get_tvm_var()
        cin1_g = get_te_var("cin1_g").get_tvm_var()
        # bl0 full load
        if tiling.get("attach_at_flag").get("bl0_attach_flag") == self.tiling_utils.get("attach_full_load") and \
            tiling.get("attach_at_flag").get("bl1_attach_flag") == self.tiling_utils.get("no_attach"):
            self._sch.set_var_value(self.cache_tiling.get("n_single_core"), 1)
            bl0_reduce_factor = self.binary_nparts_facotr.get("bl0_axis_factor")
            bl0_reduce_factor["reduce_factor"] = kernel_d * kernel_h * kernel_w * cin1_g
        abkl1_attach_flag = tiling.get("attach_at_flag").get("abkl1_attach_flag")
        if abkl1_attach_flag == 0:
            self._sch.set_var_value(self.cache_tiling.get("kl1_times"), 1)
            self._norange_kal1_kbl1_equal(tiling)
        elif abkl1_attach_flag == 1:
            self._norange_kal1(tiling)
        else:
            if tiling.get("attach_at_flag").get("bl1_attach_flag") != self.tiling_utils.get("no_attach"):
                self._norange_kbl1(tiling)
            else:
                self._norange_no_bl1()

        m_outer_outer_outer_size = tvm.max(1, self.binary_nparts_facotr.get("al1_factor")[1])
        self.binary_nparts_facotr["block_dim_m"] = tvm.min(self.cache_tiling["m_dim"], m_outer_outer_outer_size)
        self.binary_nparts_facotr["block_dim_n"] = tvm.min(self.cache_tiling["n_dim"],
                                                           self.binary_nparts_facotr.get("bl1_factor")[1])
        return tiling

    def binary_simplify(self, tensor_map, axis_dict, tiling):
        if not self._binary_mode:
            return
        self._sch.set_constraint(
            get_te_var("padl").get_tvm_var() < get_te_var("kernel_w").get_tvm_var())
        load3d_special = get_te_var("load3d_special").get_tvm_var()
        kernel_d, kernel_h, kernel_w = get_te_var("kernel_d").get_tvm_var(), \
                                       get_te_var("kernel_h").get_tvm_var(), get_te_var("kernel_w").get_tvm_var()
        cin1_g = get_te_var("cin1_g").get_tvm_var()
        # load3d_special simplify
        self._sch.set_var_value(load3d_special, tiling.get("load3d_special_flag") + 1)
        # al1 and bl1 full load
        if tiling.get("attach_at_flag").get("al1_attach_flag") == self.tiling_utils.get("attach_full_load"):
            self._sch.set_var_value(self.cache_tiling.get("m_single_core"), 1)
        if tiling.get("attach_at_flag").get("bl1_attach_flag") == self.tiling_utils.get("attach_full_load"):
            self._sch.set_var_value(self.cache_tiling.get("n_single_core"), 1)
        # bl1 is none
        if tiling.get("attach_at_flag").get("bl1_attach_flag") == self.tiling_utils.get("no_attach"):
            self._sch.set_var_value(self.cache_tiling.get("n_bl1"), 1)
        # al1 compute at l0c
        if tiling.get("attach_at_flag").get("al1_attach_flag") == self.tiling_utils.get("attach_less"):
            self._sch.set_var_value(self.cache_tiling.get("m_al1"), 1)
        # bl1 compute at l0c
        if tiling.get("attach_at_flag").get("bl1_attach_flag") == self.tiling_utils.get("attach_less"):
            self._sch.set_var_value(self.cache_tiling.get("n_bl1"), 1)
        self._binary_constant()
        get_context().get_current_compute().get_current_schedule().add("_build_config",
                                                                      {"enable_branch_eliminator_else_case": False})

    def _binary_constant(self):
        if not self._binary_tiling_data:
            return
        for var_name, var_value in self._binary_tiling_data.items():
            self._sch.set_var_range(get_te_var(var_name).get_tvm_var(), var_value, var_value)

    def _get_cache_tiling(self):
        """
        get cache_tiling
        """
        self.cache_tiling = {
            "group_dim": get_te_var("group_dim").get_tvm_var(),
            "batch_dim": get_te_var("batch_dim").get_tvm_var(),
            "d_dim": get_te_var("d_dim").get_tvm_var(),
            "batch_dout_single_core": get_te_var("batch_dout_single_core").get_tvm_var(),
            "n_single_core": get_te_var("n_single_core").get_tvm_var(),
            "n_dim": get_te_var("n_dim").get_tvm_var(),
            "n_bl1": get_te_var("n_bl1").get_tvm_var(),
            "n_ub_l0_time": get_te_var("n_ub_l0_time").get_tvm_var(),
            "cub_n1": get_te_var("cub_n1").get_tvm_var(),
            "m_dim": get_te_var("m_dim").get_tvm_var(),
            "m_single_core": get_te_var("m_single_core").get_tvm_var(),
            "m_al1": get_te_var("m_al1").get_tvm_var(),
            "m_l0": get_te_var("m_l0").get_tvm_var(),
            "k_l0": get_te_var("k_l0").get_tvm_var(),
            "k_al0": get_te_var("k_l0").get_tvm_var(),
            "k_bl0": get_te_var("k_l0").get_tvm_var(),
            "kal1_factor": get_te_var("kal1_factor").get_tvm_var(),
            "kbl1_factor": get_te_var("kbl1_factor").get_tvm_var(),
            "kal0_factor": get_te_var("kal0_factor").get_tvm_var(),
            "kbl0_factor": get_te_var("kbl0_factor").get_tvm_var(),
            "kl1_times": get_te_var("kl1_times").get_tvm_var(),
            "al1_bound": get_te_var("al1_bound").get_tvm_var(),
            "bt_bound": get_te_var("bt_bound").get_tvm_var(),
        }
        self.cache_tiling["kal1_16"] = self.cache_tiling.get("kal0_factor") * self.cache_tiling.get("k_l0")
        self.cache_tiling["kbl1_16"] = self.cache_tiling.get("kbl0_factor") * self.cache_tiling.get("k_l0")

    def _norange_kal1_kbl1_equal(self, tiling):
        """
        config k related tiling variable when kal1 equals kbl1
        """
        kal1_factor = self.cache_tiling.get("kal1_factor")
        self.binary_nparts_facotr.get("al1_factor")[0] = kal1_factor
        self.binary_nparts_facotr.get("bl1_factor")[0] = kal1_factor
        if tiling.get("attach_at_flag").get("al1_attach_flag") in self.k_full_load_list:
            self._sch.set_var_value(kal1_factor, 1)
        if tiling.get("attach_at_flag").get("bl1_attach_flag") == self.tiling_utils.get("attach_full_load"):
            self.binary_nparts_facotr.get("bl1_factor")[1] = tvm.min(self.binary_nparts_facotr.get("c_factor")[0],
                                                                     self.cache_tiling.get("n_dim"))

    def _norange_kal1(self, tiling):
        """
        config k related tiling variable when kal1 large than kbl1
        """
        kl1_times = self.cache_tiling.get("kl1_times")
        kal1_factor = self.cache_tiling.get("kal1_factor")
        self.binary_nparts_facotr.get("al1_factor")[0] = kal1_factor
        self.binary_nparts_facotr.get("bl1_factor")[0] = kal1_factor * kl1_times
        if tiling.get("attach_at_flag").get("al1_attach_flag") in self.k_full_load_list:
            self._sch.set_var_value(kal1_factor, 1)
            if tiling.get("attach_at_flag").get("al1_attach_flag") == self.tiling_utils.get("attach_full_load"):
                self.binary_nparts_facotr.get("al1_factor")[0] = 1

    def _norange_kbl1(self, tiling):
        """
        config k related tiling variable when kal1 smaller than kbl1
        """
        kl1_times = self.cache_tiling.get("kl1_times")
        kbl1_factor = self.cache_tiling.get("kbl1_factor")
        self.binary_nparts_facotr.get("al1_factor")[0] = kbl1_factor * kl1_times
        self.binary_nparts_facotr.get("bl1_factor")[0] = kbl1_factor
        if tiling.get("attach_at_flag").get("bl1_attach_flag") in self.k_full_load_list:
            self._sch.set_var_value(kbl1_factor, 1)
            if tiling.get("attach_at_flag").get("bl1_attach_flag") == self.tiling_utils.get("attach_full_load"):
                self.binary_nparts_facotr.get("bl1_factor")[1] = tvm.min(self.binary_nparts_facotr.get("c_factor")[0],
                                                                         self.cache_tiling.get("n_dim"))

    def _norange_no_bl1(self):
        """
        no access bl1
        """
        self.binary_nparts_facotr.get("bl1_factor")[0] = 1
        self.binary_nparts_facotr.get("bl1_factor")[1] = tvm.min(self.binary_nparts_facotr.get("c_factor")[0],
                                                                 self.cache_tiling.get("n_dim"))

    def _tiling_infer_norange(self, tiling):
        """
        config tiling variable for cache tiling
        """
        kernel_h, kernel_w = get_te_var("kernel_h").get_tvm_var(), get_te_var("kernel_w").get_tvm_var()
        tiling['block_dim'] = [self.cache_tiling.get('batch_dim') * self.cache_tiling.get("d_dim"),
                               self.cache_tiling.get("n_dim"),
                               self.cache_tiling.get("m_dim"),
                               self.cache_tiling.get("d_dim"), 0]
        tiling['g_dim'] = self.cache_tiling.get('group_dim')
        tiling.get('AL1_shape')[0] = tvm.floordiv(self.cache_tiling.get("kal1_16"), (kernel_h * kernel_w))
        tiling.get('AL1_shape')[1] = self.cache_tiling.get("m_al1")
        tiling.get('BL1_shape')[0] = tvm.floordiv(self.cache_tiling.get("kbl1_16"), (kernel_h * kernel_w))
        tiling.get('BL1_shape')[1] = self.cache_tiling.get("n_bl1")
        tiling.get('AL0_matrix')[0] = self.cache_tiling.get("m_l0")
        tiling.get('CL0_matrix')[1] = self.cache_tiling.get("m_l0")
        tiling.get('L0C_OUTPUT_matrix')[1] = self.cache_tiling.get("m_l0")
        tiling.get('L0C_OUTPUT_matrix')[0] = self.cache_tiling.get("cub_n1")
        tiling.get('AL0_matrix')[1] = self.cache_tiling.get("k_al0")
        tiling.get('BL0_matrix')[0] = self.cache_tiling.get("k_bl0")
        tiling.get('BL0_matrix')[1] = self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1")
        tiling.get('CL0_matrix')[0] = tiling.get('BL0_matrix')[1]
        exponent_base = 2
        tiling["A_overhead_opt_flag"] = tiling.get("special_optimize_flag") % exponent_base
        tiling["B_overhead_opt_flag"] = tiling.get("special_optimize_flag") // exponent_base % exponent_base
        tiling["n_bef_batch_flag"] = 1 if tiling.get("control_reorder_flag") == 1 else 0
        tiling["scale_drq_split_flag"] = False
        tiling["bias_split_flag"] = False
        return tiling