#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Schedule of conv2d fixpipefusion in v220/v300.
"""
from collections import deque
import tvm
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe.common.utils.op_util.op_util_conv2d import is_support_v220
from tbe.common.utils.op_util.op_util_conv2d import BIT_RATIO_MAP
from tbe.common.utils.op_util.op_util_conv2d import WINO_OUT_TILE_HW
from tbe.dsl.static_schedule.conv_schedule_util import clear_suffix
from tbe.dsl.static_schedule.conv_schedule_util import get_fixpipe_tag_list
from tbe.common import platform as tbe_platform

POST_5HD_DIM = 4 # 5hd dim after conv2d.
CHANNEL_SPLIT_FACTOR = 2 # C08 -> C16
FIXPIPE_REFORM_TAG = "fixpipe_reform"
FIXPIPE_OP_TAG = "fixpipe"
QUANT_SCALE_0_STR = "quant_scale_0"
QUANT_SCALE_1_STR = "quant_scale_1"
RELU_WEIGHT_0_STR = "relu_weight_0"
RELU_WEIGHT_1_STR = "relu_weight_1"
ELTWISE_SRC_STR = "eltwise_src"
UINT64_DTYPE_STR = "uint64"
CUBE_N_SIZE_STR = "cube_n_size"

INTRINSIC_FIXPIPE_UNIT_LIST = "Intrinsic_fix_pipe_unit_list"
UNIT_POST_ELTWISE = "post_eltwise"
FIXPIPE_SCOPE_MAP = {
    QUANT_SCALE_0_STR: tbe_platform.scope_fb0,
    QUANT_SCALE_1_STR: tbe_platform.scope_fb3,
    RELU_WEIGHT_0_STR: tbe_platform.scope_fb1,
    RELU_WEIGHT_1_STR: tbe_platform.scope_fb2,
    ELTWISE_SRC_STR: tbe_platform.scope_cbuf
}
FIXPIPE_V350_SCOPE_MAP = {
    QUANT_SCALE_0_STR: tbe_platform.scope_fb
}


class FixpipeFusion:
    """
    Class of fixpipe op fusion.
    """
    def __init__(self, sch, res, op_graph, conv_param):
        """
        Class FixpipeFusion init func.
        """
        self.fixpipe_res_tag_list = get_fixpipe_tag_list()
        self.cl0_tag_list = ["convolution_c_col",
                             "convolution_c_col_bias"]
        self.fixpipe_res_list = []
        self.fixpipe_res_gm2ub_list = [] # fixpipe_res_gm2ub is the gm->ub tensor in v220 mix_L2 fusion.
        self.fixpipe_res_clone_tensors = []
        self.fixpipe_fused_type = []
        self.fixpipe_channel_coeff = 0
        self.fixpipe_eltwise_coeff = 0
        self.fixpipe_info_dict = {}
        self.fixpipe_splitw_tile_block_info_dict = {}
        self.multiout_flag = res.op.name == "conv_virtual_res"
        self.mixl2_flag = False
        self.fixpipe_nz2nd_flag = False # whether res tensor is processed as nd format.
        self.fixpipe_antiquant_flag = False # whether res is antiquant fixpipe_res.
        self.fbparams_fullload_flag = False # whether fixpipe params in FB buffer full load.
        self.winograd_conv_flag = False
        self.binary_static_flag = conv_param.binary_static_flag
        self.pre_op_list = []
        self.next_op_list = []

        self.parse_next_op(op_graph)
        self.config_fixpipe_res_tag_list(conv_param)
        self.parse_fixpipe_res_list(res)

        # self.fixpipe_fusion_only_flag means only fixpipe fusion,
        # res needs to do axis split process for fixpipe_op emit_insn.
        self.fixpipe_fusion_only_flag = len(self.fixpipe_res_list) == 1 and self.fixpipe_res_list[0] == res
        self.parse_mixl2_tensor(sch, res, conv_param)
        self.config_fixpipe_info_dict()
        self.config_flags(res)
        self.config_fixpipe_dataflow(sch, res)
        self.cache_clone_fixpipe_res(sch, res)

        # max fixpipe buffer size
        self.max_fixpipe_c1 = tbe_platform.get_soc_spec(tbe_platform.FB0_SIZE) // \
                              BIT_RATIO_MAP.get(UINT64_DTYPE_STR) // int(tbe_platform.get_soc_spec(CUBE_N_SIZE_STR))

    def get_fixpipe_res_list(self):
        """
        Get fixpipe_res_list and fixpipe_res_gm2ub_list.
        """
        return self.fixpipe_res_list, self.fixpipe_res_gm2ub_list

    def get_nz2nd_flag(self):
        """
        Get fixpipe_nz2nd_flag.
        """
        return self.fixpipe_nz2nd_flag

    def get_antiquant_flag(self):
        """
        fp16 and not nd2nz and anti_quant, split 2 for pass claculate c0 stride.
        """
        return self.fixpipe_antiquant_flag

    def get_mix_l2_flag(self):
        """
        Get mix_l2 flag.
        """
        return self.mixl2_flag

    def get_pre_next_op_list(self):
        """
        Get pre_op_list and next_op_list.
        """
        return self.pre_op_list, self.next_op_list

    def config_fixpipe_res_tag_list(self, conv_param):
        """
        Config fixpipe_res tag list.
        """
        if conv_param.convbn1_flag:
            self.fixpipe_res_tag_list = ["convolution_c_ub"]

    def is_fixpipe_res(self, tensor):
        """
        Check if tensor is fixpipe_res.
        """
        return clear_suffix(tensor.op.tag) in self.fixpipe_res_tag_list

    def parse_next_op(self, op_graph):
        """
        Parse the dependencies between nearby tensors.
        The elements at the same index in pre_op_list and post_op_list are in pair.
        """
        for ops in op_graph.input_ops + op_graph.body_ops:
            pre_op = ops["dst_buffer"]
            for next_ops in ops["next_op"]:
                if pre_op not in self.pre_op_list:
                    self.pre_op_list.append(pre_op)
                    self.next_op_list.append([next_ops["dst_buffer"]])
                else:
                    self.next_op_list[self.pre_op_list.index(pre_op)].append(next_ops["dst_buffer"])

    def parse_fixpipe_res_list(self, res):
        """
        Get all fixpipe_res tensors.
        """
        tensor_queue = deque([res])

        while tensor_queue:
            src_tensor = tensor_queue.popleft()
            if src_tensor in self.fixpipe_res_list:
                pass
            elif self.is_fixpipe_res(src_tensor):
                self.fixpipe_res_list.append(src_tensor)
            else:
                append_list = list(i for i in src_tensor.op.input_tensors)
                tensor_queue.extend(append_list)

    def parse_mixl2_tensor(self, sch, res, conv_param):
        """
        Parse fixpipe_res and fixpipe_res_gm2ub in mix_L2 situation.
        """
        def parse_mixl2_flag():
            if not is_support_v220() or conv_param.convbn1_flag:
                return False
            if self.fixpipe_fusion_only_flag:
                return False
            if res.op.name == "conv_virtual_res":
                for tensor in res.op.input_tensors:
                    if not self.is_fixpipe_res(tensor):
                        return True
                return False
            return True

        self.mixl2_flag = parse_mixl2_flag()

        if self.mixl2_flag:
            for fixpipe_res_gm_tensor in self.fixpipe_res_list:
                cub_next = self.next_op_list[self.pre_op_list.index(fixpipe_res_gm_tensor)]
                cub_tensor = sch.cache_read(fixpipe_res_gm_tensor, tbe_platform.scope_ubuf, cub_next)
                self.fixpipe_res_gm2ub_list.append(cub_tensor)

            # add workspace_tensor.
            sch.cce_special = {}
            sch.cce_special["tensor_list"] = self.fixpipe_res_list

    def config_fixpipe_info_dict(self):
        """
        Config fixpipe_info_dict to get tiling.
        """
        # The memory occupation coefficient of fixpipe params in L1 is converted by fp16.
        fp16_convert_dict = {
            "int4": 0.25,
            "int8": 0.5,
            "float16": 1,
            "float32": 2,
            "uint64": 4,
            "int64": 4
        }

        for tensor in self.fixpipe_res_list:
            if clear_suffix(tensor.op.tag) == FIXPIPE_REFORM_TAG:
                init_info = {
                    "cache_read_tensors_fb": [],
                    "cache_read_tensors_l1": [],
                    "cache_read_tensors_elewise": [],
                    "fixpipe_param_names": tensor.op.attrs["vector_params"],
                    "fixpipe_tensors": tensor.op.attrs["vector_tensors"],
                    "nz2nd_flag": bool(tensor.op.attrs["nz2nd_flag"].value),
                    "anti_quant_flag": bool(tensor.op.attrs["anti_quant_flag"].value),
                    "winograd_conv_flag": bool(tensor.op.attrs.get("winograd_conv_flag", False))
                }
                if init_info["winograd_conv_flag"]:
                    init_info["fixpipe_fb_tensors"] = tensor.op.attrs["vector_fb_tensors"]

                self.fixpipe_info_dict[tensor] = init_info

                #====================parse fixpipe tiling info==========================
                for idx, fixpipe_param_name in enumerate(init_info.get("fixpipe_param_names")):
                    if fixpipe_param_name not in FIXPIPE_SCOPE_MAP:
                        err_man.raise_err_specific(
                            "conv2d",
                            f"tensor {fixpipe_param_name} cannot set scope to fb"
                            )
                    fixpipe_tensor = init_info.get("fixpipe_tensors")[idx]
                    if fixpipe_param_name == ELTWISE_SRC_STR:
                        self.fixpipe_eltwise_coeff += fp16_convert_dict.get(fixpipe_tensor.dtype)
                    else:
                        self.fixpipe_channel_coeff += fp16_convert_dict.get(fixpipe_tensor.dtype)

                fixpipe_type_dict = {}
                for unit, state in tensor.op.attrs["op_dict"].items():
                    if state != "":
                        fixpipe_type_dict[unit] = state.value
                self.fixpipe_fused_type.append(fixpipe_type_dict)
            else:
                self.fixpipe_fused_type.append({})
                init_info = {
                    "cache_read_tensors_fb": [],
                    "cache_read_tensors_l1": [],
                    "cache_read_tensors_elewise": [],
                    "fixpipe_param_names": [],
                    "fixpipe_tensors": [],
                    "nz2nd_flag": False,
                    "anti_quant_flag": False,
                    "winograd_conv_flag": False,
                }
                self.fixpipe_info_dict[tensor] = init_info

    def config_flags(self, res):
        """
        Whether res tensor needs to process axis split to avoid nonlinear ir in nz2nd or anti-quant cases.
        """
        for fixpipe_res in self.fixpipe_res_list:
            if clear_suffix(fixpipe_res.op.name) == "fixpipe_nz2nd":
                self.fixpipe_nz2nd_flag = True
                break
        
        self.winograd_conv_flag = self.fixpipe_info_dict.get(self.fixpipe_res_list[0]).get("winograd_conv_flag")

        if self.fixpipe_fusion_only_flag:
            fixpipe_info = self.fixpipe_info_dict.get(self.fixpipe_res_list[0])
            if fixpipe_info.get("anti_quant_flag") and \
                res.dtype == "float16" and \
                not fixpipe_info.get("nz2nd_flag"):
                self.fixpipe_antiquant_flag = True

    def config_fixpipe_dataflow(self, sch, res):
        """
        Get fixpipe inline tensors and cache read for fixpipe vector input tensors.
        """
        if is_support_fixpipe():
            disable_inline_tensors = [res]
            if self.multiout_flag:
                disable_inline_tensors.extend(list(res.op.input_tensors))

            for fixpipe_res in self.fixpipe_res_list:
                self.inline_fixpipe_tensors(sch, fixpipe_res, disable_inline_tensors)
                self.inputs_cache_read(sch, fixpipe_res)

    def inline_fixpipe_tensors(self, sch, fixpipe_res, disable_inline_tensors):
        """
        Inline tensors for fixpipe op emit insn.
        """
        def inline_pre_tensors(sch, tensor):
            while tensor.op.tag not in self.cl0_tag_list:
                if tensor not in disable_inline_tensors:
                    sch[tensor].compute_inline()
                tensor = tensor.op.input_tensors[0]

        inline_pre_tensors(sch, fixpipe_res.op.input_tensors[0])

    def inputs_cache_read(self, sch, fixpipe_res):
        """
        Cache read for fixpipe vector input tensors.
        """
        cache_read_tensors_fb = []
        cache_read_tensors_l1 = []
        cache_read_tensors_elewise = []
        fixpipe_param_names = self.fixpipe_info_dict.get(fixpipe_res).get("fixpipe_param_names")
        fixpipe_tensors = self.fixpipe_info_dict.get(fixpipe_res).get("fixpipe_tensors")

        for idx, fixpipe_param_name in enumerate(fixpipe_param_names):
            fixpipe_tensor = fixpipe_tensors[idx]
            scope = FIXPIPE_SCOPE_MAP.get(fixpipe_param_name)
            if self.winograd_conv_flag:
                fixpipe_fb_tensors = self.fixpipe_info_dict.get(fixpipe_res).get("fixpipe_fb_tensors")
                fixpipe_fb_tensor = fixpipe_fb_tensors.get(fixpipe_param_name)
                if fixpipe_param_name == ELTWISE_SRC_STR:
                    sch[fixpipe_tensor].set_scope(scope)
                    cache_read_tensors_elewise.append(fixpipe_tensor)
                else:
                    sch[fixpipe_fb_tensor].set_scope(scope)
                    input_l1 = sch.cache_read(fixpipe_tensor, tbe_platform.scope_cbuf, fixpipe_fb_tensor)
                    cache_read_tensors_fb.append(fixpipe_fb_tensor)
                    cache_read_tensors_l1.append(input_l1)
            else:
                if self.binary_static_flag:
                    sch[fixpipe_tensor].set_scope(tbe_platform.scope_cbuf)
                    input_fb = sch.cache_read(fixpipe_tensor, scope, self.get_next_op(fixpipe_tensor))
                    cache_read_tensors_fb.append(input_fb)
                elif fixpipe_param_name == ELTWISE_SRC_STR:
                    input_l1 = sch.cache_read(fixpipe_tensor, scope, self.get_next_op(fixpipe_tensor))
                    cache_read_tensors_elewise.append(input_l1)
                else:
                    input_fb = sch.cache_read(fixpipe_tensor, scope, self.get_next_op(fixpipe_tensor))
                    input_l1 = sch.cache_read(fixpipe_tensor, tbe_platform.scope_cbuf, input_fb)
                    cache_read_tensors_fb.append(input_fb)
                    cache_read_tensors_l1.append(input_l1)

        self.fixpipe_info_dict.get(fixpipe_res).update(
            {"cache_read_tensors_fb": cache_read_tensors_fb,
             "cache_read_tensors_l1": cache_read_tensors_l1,
             "cache_read_tensors_elewise": cache_read_tensors_elewise
             })

    def get_next_op(self, tensor):
        """
        Get the next op list of tensor.
        """
        for index, pre_op in enumerate(self.pre_op_list):
            if pre_op == tensor:
                return self.next_op_list[index]
        err_man.raise_err_specific(
            "conv2d",
            f" cannot get tensor {tensor} in pre_op_list."
            )
        return None

    def cache_clone_fixpipe_res(self, sch, res):
        """
        Cache clone for fixpipe_res when fixpipe multioutput.
        """
        def clone_fixpipe_res(res, fixpipe_res, fixpipe_res_next):
            """
            Cache clone for fixpipe_res.
            """
            def update_fixpipe_info(fixpipe_res, clone_info):
                """
                Update next_op_list, fixpipe_res_list and fixpipe_info_dict after cache_clone.
                """
                reader, fixpipe_res_clone, append_cub_flag, fixpipe_res_next_op_list = clone_info

                for tensors in self.next_op_list:
                    if fixpipe_res in tensors and fixpipe_res_clone not in tensors:
                        tensors.append(fixpipe_res_clone)

                self.next_op_list[self.pre_op_list.index(fixpipe_res)] = fixpipe_res_next_op_list
                self.pre_op_list.append(fixpipe_res_clone)
                self.next_op_list.append(reader)

                if append_cub_flag:
                    self.fixpipe_res_list.append(fixpipe_res_clone)
                    self.fixpipe_info_dict[fixpipe_res_clone] = self.fixpipe_info_dict.get(fixpipe_res)

            def parse_next_tensor(res, fixpipe_res_next):
                """
                Parse the pattern of the next tensor, which can be a conv_virtual_res, fixpipe_res or ub tensor.
                """
                next_tensor_dict = {}
                next_ub_tensor_list = []
                """
                special ub case(such as mish): single ub op has multi usage of pre node,
                cache_clone readers should be multi ub tensors.
                mish structure be like:
                xx -- softplus -- tanh -- mul --
                 |                         |
                 ---------------------------
                """
                for tensor in fixpipe_res_next:
                    if tensor == res:
                        next_tensor_dict["virtual_res_next"] = tensor
                    elif clear_suffix(tensor.op.tag) == FIXPIPE_OP_TAG:
                        next_tensor_dict["fixpipe_next"] = tensor
                    else:
                        next_ub_tensor_list.append(tensor)
                next_tensor_dict["ub_next"] = next_ub_tensor_list
                return next_tensor_dict

            next_tensor_dict = parse_next_tensor(res, fixpipe_res_next)
            """
            cloned fixpipe tensor only have ub_next (multi), no need to clone and update fixpipe info.
            if cloned, will create an infinite loop because cloned tensor will inherit all ub_next,
            still have more than one next_op, lead to another clone process.
            """
            if "fixpipe_next" not in next_tensor_dict and "virtual_res_next" not in next_tensor_dict:
                return
            """
            The reader chosen for cache_clone refers to the following principles:
            1. The clone tensor should never become actual output tensor,
                otherwise tensor_list needs to be updated.
            2. Try to inline the clone tensor,
                such as clone res_conv2d to the succeeding fixpipe node.
            """

            if "fixpipe_next" in next_tensor_dict:
                reader = [next_tensor_dict.get("fixpipe_next")]
                fixpipe_res_clone = sch.cache_clone(fixpipe_res, tbe_platform.scope_gm, reader)
                sch[fixpipe_res_clone].compute_inline()
                append_cub_flag = False
            else:
                reader = next_tensor_dict.get("ub_next")
                fixpipe_res_clone = sch.cache_clone(fixpipe_res, tbe_platform.scope_gm, reader)
                append_cub_flag = True

            if "virtual_res_next" in next_tensor_dict:
                fixpipe_res_next_op_list = [next_tensor_dict.get("virtual_res_next")]
            else:
                fixpipe_res_next_op_list = next_tensor_dict.get("ub_next")

            clone_info = (reader, fixpipe_res_clone, append_cub_flag, fixpipe_res_next_op_list)
            update_fixpipe_info(fixpipe_res, clone_info)

        """
        The following processing of fixpipe_res clone contains:
        1. In multi output fusion case, fixpipe node or conv2d node is referenced by 2 nodes,
            use cache_clone to fix the missing compute ir for emit insn.
        2. In single output fusion case, conv2d node is referenced by the succeeding fixpipe node and ub node,
            such as:
            ori ir:
            res_conv2d —> fixpipe_res —> Add/Mul —> ...
                        |                     |
                        ———————————>
            final ir:
            [res_conv2d_clone] —> fixpipe_res —> Add/Mul —> ...
                                                      |
            res_conv2d ———————————————>
        """
        if self.multiout_flag or len(self.fixpipe_res_list) > 1:
            for fixpipe_res in self.fixpipe_res_list:
                fixpipe_res_next = self.next_op_list[self.pre_op_list.index(fixpipe_res)]

                if len(fixpipe_res_next) >= 2:
                    clone_fixpipe_res(res, fixpipe_res, fixpipe_res_next)

    def special_process_pre(self, sch, res):
        """
        Special process before get tiling for fixpipe fusion.
        """
        self.fixpipe_inputs_buffer_align(sch)
        self.fixpipe_inputs_compute_align(sch)
        self.multioutput_buffer_align(sch, res)

    def fixpipe_inputs_buffer_align(self, sch):
        """
        Buffer align for fixpipe input tensors.
        """
        # fixpipe + nz2nd fusion, cout can be 1, dma should be 32B aligned
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_l1 = fixpipe_info.get("cache_read_tensors_l1")
            cache_read_tensors_elewise = fixpipe_info.get("cache_read_tensors_elewise")

            for tensor in cache_read_tensors_elewise:
                buffer_align_list = []
                # buffer_align c0
                for i in range(len(tensor.shape) - 1):
                    buffer_align_list.append((1, 1))
                buffer_align_list.append((tensor.shape[-1], tensor.shape[-1]))
                sch[tensor].buffer_align(*buffer_align_list)

            for tensor in cache_read_tensors_l1:
                # channelwise input shape [N, C1, H, W, C0]
                sch[tensor].buffer_align((1, 1),
                                         (1, 1),
                                         (1, 1),
                                         (1, 1),
                                         (tensor.shape[-1], tensor.shape[-1]))

    def fixpipe_inputs_compute_align(self, sch):
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_elewise = fixpipe_info.get("cache_read_tensors_elewise")

            for tensor in cache_read_tensors_elewise:
                # elewise inpute shape [N, C1, HOWO, C0]
                # when channelmerge case: cout is non-divisor by res_c0, eltwise_src should align to res_c0
                # anti-quant case and nz2nd(nhwc format and quant as output node) case not need to align
                needs_align_flag = fixpipe_res.dtype in ("int4", "int8") and tensor.dtype == "float16" \
                    and "fixpipe_nz2nd" not in fixpipe_res.name
                factor_c0 = fixpipe_res.shape[-1] // tensor.shape[-1]
                if needs_align_flag and tensor.shape[1] % factor_c0 != 0:
                    sch[tensor].compute_align(tensor.op.axis[1], factor_c0)

    def multioutput_buffer_align(self, sch, res):
        """
        Buffer align for 5hd output tensor when conv_virtual_res is ND format.
        """
        if res.op.tag == "nd_5hd_add_compute":
            for index, next_op in enumerate(self.next_op_list):
                if res in next_op:
                    pre_op = self.pre_op_list[index]
                    if len(pre_op.shape) == POST_5HD_DIM:
                        # 5hd output tensor shape [N, C1, HW, C0]
                        sch[pre_op].buffer_align((1, 1),
                                                 (1, 1),
                                                 (1, 1),
                                                 (pre_op.shape[-1], pre_op.shape[-1]))

    def tile_tensor_fixpipe_res(self, sch, block_n0, nc_factor_cub, fm_dtype):
        """
        Tile tensor fixpipe_res for fixpipe_op emit insn.
        """

        def check_channel_split_scenerio(cub, fixpipe_info_dict, fm_dtype):
            """
            check if channel_split
            """
            # channel-split: fp32-in and fp32-out
            return fm_dtype == "float32" and cub.dtype == "float32" \
                and not fixpipe_info_dict.get(cub).get("nz2nd_flag")

        cub_pragma_axis_dict = {}

        if is_support_fixpipe() and not self.fixpipe_fusion_only_flag:
            for cub in self.fixpipe_res_list:
                if self.winograd_conv_flag:
                    _, cub_n1_axis, p1_axis, cub_m_axis, p2_axis, cub_n0_axis = cub.op.axis
                elif self.fixpipe_info_dict.get(cub).get("nz2nd_flag"):
                    cub_n1_axis, cub_n0_axis = sch[cub].split(cub.op.axis[2], block_n0)
                    cub_m_axis = cub.op.axis[1]
                else:
                    _, cub_n1_axis, cub_m_axis, cub_n0_axis = cub.op.axis

                channel_split_flag = check_channel_split_scenerio(cub, self.fixpipe_info_dict, fm_dtype)
                if not channel_split_flag:
                    # not c016 -> c08 channel-split scenerio, n1_axis can be splitted by nc_factor directly
                    cub_out2ub_loopn_axis, cub_nc_factor_axis = sch[cub].split(cub_n1_axis, nc_factor_cub)
                cub_out2ub_loopm_axis, cub_m_factor_axis = sch[cub].split(cub_m_axis, nparts=1)

                if self.winograd_conv_flag:
                    sch[cub].reorder(
                        cub_out2ub_loopn_axis,
                        cub_out2ub_loopm_axis,
                        cub_nc_factor_axis,
                        p1_axis,
                        cub_m_factor_axis
                        )
                elif self.fixpipe_info_dict.get(cub).get("nz2nd_flag"):
                    # fp32 conv + fixpipe multi-output
                    sch[cub].reorder(
                        cub_out2ub_loopn_axis,
                        cub_out2ub_loopm_axis,
                        cub_m_factor_axis,
                        cub_nc_factor_axis,
                        cub_n0_axis
                        )
                elif cub.dtype in ("int4", "int8"):
                    # split c0=16 in channel merging to avoid nonlinear ir
                    _, _ = sch[cub].split(cub_n0_axis, factor=16)
                    sch[cub].reorder(
                        cub_out2ub_loopn_axis,
                        cub_out2ub_loopm_axis,
                        cub_nc_factor_axis,
                        cub_m_factor_axis
                        )
                elif channel_split_flag:
                    # fp32 conv + fixpipe multi-output
                    # channel-split scenerio, n1_axis needs to be splitted by facotr of 2.
                    cub_out2ub_loopn_axis, channelsplit_c0_npart_axis = sch[cub].split(
                        cub_n1_axis, CHANNEL_SPLIT_FACTOR)
                    cub_out2ub_loopn_axis, cub_nc_factor_axis = sch[cub].split(cub_out2ub_loopn_axis, nc_factor_cub)
                    sch[cub].reorder(
                        cub_out2ub_loopn_axis,
                        cub_out2ub_loopm_axis,
                        cub_nc_factor_axis,
                        channelsplit_c0_npart_axis,
                        cub_m_factor_axis
                        )
                elif cub.dtype == "float16" and self.fixpipe_info_dict.get(cub).get("anti_quant_flag"):
                    # antiquant fixpipe res.
                    cub_nc_factor_axis, antiquant_c0_npart_axis = sch[cub].split(cub_nc_factor_axis, 2)
                    sch[cub].reorder(
                        cub_out2ub_loopn_axis,
                        cub_out2ub_loopm_axis,
                        cub_nc_factor_axis,
                        antiquant_c0_npart_axis,
                        cub_m_factor_axis
                        )
                else:
                    sch[cub].reorder(
                        cub_out2ub_loopn_axis,
                        cub_out2ub_loopm_axis,
                        cub_nc_factor_axis,
                        cub_m_factor_axis
                        )

                self.fixpipe_splitw_tile_block_info_dict.update(
                    {cub: {"cub_out2ub_loopm_axis": cub_out2ub_loopm_axis, "cub_m_factor_axis": cub_m_factor_axis}})

                cub_pragma_axis = cub_m_factor_axis if self.fixpipe_info_dict.get(cub).get("nz2nd_flag") \
                    else cub_nc_factor_axis
                cub_pragma_axis_dict[cub] = cub_pragma_axis
            return cub_pragma_axis_dict

        cub = self.fixpipe_res_list[0]
        cub_pragma_axis_dict[cub] = cub.op.axis[0]

        return cub_pragma_axis_dict

    def get_fixpipe_fullload_flag(self, co1):
        """
        Check if fixpipe params can full load into fixpipe buffer.
        """
        if self.winograd_conv_flag:
            # fb layout is P1 P2 C1 C0, if fullload, every time reads P1 P2 C1' C0
            # which is not continuous fb memory
            # SPR.FPC only support one Quant_PRE_ADDR
            return False

        if co1 > self.max_fixpipe_c1:
            return False

        scope_dict = {
            "FB0": set(),
            "FB1": set(),
            "FB2": set(),
            "FB3": set()
        }

        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_fb = fixpipe_info.get("cache_read_tensors_fb")
            for tensor in cache_read_tensors_fb:
                tensor_scope = tensor.op.name.split(".")[-1]
                scope_dict.get(tensor_scope).add(tensor)

        for _, reader_set in scope_dict.items():
            if len(reader_set) * co1 > self.max_fixpipe_c1:
                return False

        return True

    def fixpipe_inputs_compute_at(self, sch, res, fixpipe_fb_attach_axis, fixpipe_l1_attach_axis, eltwise_attach_axis):
        """
        Attach the inputs of fixpipe fusion ops to res tensor.
        fixpipe_fb_attach_axis: attach axis in res for fixpipe params in fixpipe buffer.
        fixpipe_l1_attach_axis: attach axis in res for fixpipe params in L1.
        eltwise_attach_axis: attach axis in res for fixpipe eltwise inputs in L1.
        """
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_fb = fixpipe_info.get("cache_read_tensors_fb")
            cache_read_tensors_l1 = fixpipe_info.get("cache_read_tensors_l1")
            cache_read_tensors_elewise = fixpipe_info.get("cache_read_tensors_elewise")

            for tensor in cache_read_tensors_fb:
                sch[tensor].compute_at(sch[res], fixpipe_fb_attach_axis)

            for tensor in cache_read_tensors_l1:
                sch[tensor].compute_at(sch[res], fixpipe_l1_attach_axis)

            for tensor in cache_read_tensors_elewise:
                sch[tensor].compute_at(sch[res], eltwise_attach_axis)

    def fixpipe_res_store_predict(self, sch, attach_axis_dict, out_width, cl0_m_bound):
        """
        splitw case, pass can not deal with fixpipe_res tensor Tail Block Calculation in wout direction:
        need conv op to set_store_predict of wout direction to make sure correctness of Tail Block Calculation
        """
        if not self.fixpipe_fusion_only_flag:
            for fixpipe_res in self.fixpipe_res_list:
                split_w_out2cl0_loopm_axis = attach_axis_dict.get("split_w_out2cl0_loopm_axis")
                cub_m_factor_axis = self.fixpipe_splitw_tile_block_info_dict.get(fixpipe_res).get("cub_m_factor_axis")
                cub_out2ub_loopm_axis = \
                    self.fixpipe_splitw_tile_block_info_dict.get(fixpipe_res).get("cub_out2ub_loopm_axis")
                condition = tvm.any(
                    tvm.all(
                        split_w_out2cl0_loopm_axis * cl0_m_bound + cub_out2ub_loopm_axis * cl0_m_bound
                        + cub_m_factor_axis < out_width
                    )
                )
                sch[fixpipe_res].set_store_predicate(condition, partition=True)

    def set_fixpipe_buffer_size(self, sch, buffer_size):
        """
        Set buffer size for fixpipe params when channel_merge.
        buffer_size is nc_factor * n0.
        """
        if not self.fbparams_fullload_flag:
            for fixpipe_res in self.fixpipe_res_list:
                if "fixpipe_channel_merge" in fixpipe_res.op.name:
                    fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
                    fb_tensors = fixpipe_info.get("cache_read_tensors_fb")
                    for tensor in fb_tensors:
                        sch[tensor].set_buffer_size(buffer_size)

    def double_buffer(self, sch, pingpong_buffer):
        """
        Enable double buffer for fixpipe input tensors.
        """
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_l1 = fixpipe_info.get("cache_read_tensors_l1")
            cache_read_tensors_elewise = fixpipe_info.get("cache_read_tensors_elewise")

            if pingpong_buffer.get("INPUT_L1_FB_pbuffer") == 2:
                for tensor in cache_read_tensors_l1:
                    sch[tensor].double_buffer()

            if pingpong_buffer.get("INPUT_L1_eltwise_pbuffer") == 2:
                for tensor in cache_read_tensors_elewise:
                    sch[tensor].double_buffer()

    def fixpipe_inputs_emit_insn(self, sch):
        """
        Dma for the inputs of fixpipe fusion ops.
        """
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors = fixpipe_info.get("cache_read_tensors_fb") +\
                fixpipe_info.get("cache_read_tensors_l1") +\
                fixpipe_info.get("cache_read_tensors_elewise")
            for tensor in cache_read_tensors:
                sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")

    def get_fixpipe_cache_read_elewise(self):
        cache_read_elewise_list = []
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            tensors_elewise = fixpipe_info.get("cache_read_tensors_elewise")
            if tensors_elewise:
                cache_read_elewise_list.append(tensors_elewise)
            else:
                cache_read_elewise_list.append(None)
        return cache_read_elewise_list


class FixpipeFusionV350:
    """
    Class of fixpipe op fusion for v350.
    """
    def __init__(self, sch, res, conv_param, op_graph):
        """
        Class FixpipeFusionV350 init func.
        """
        self.fixpipe_res_tag_list = get_fixpipe_tag_list()
        self.cub_tag_list = ["convolution_conv_res"]
        self.fixpipe_res_list = []
        self.fixpipe_fused_type = []
        self.fixpipe_channel_coeff = 0
        self.fixpipe_eltwise_coeff = 0
        self.fixpipe_info_dict = {}
        self.fixpipe_splitw_tile_block_info_dict = {}
        self.pre_op_list = []
        self.next_op_list = []

        self.parse_next_op(op_graph)
        self.parse_fixpipe_res_list(res)
        self.config_fixpipe_info_dict()

        self.flag = len(self.fixpipe_res_list) == 1

        # max fixpipe buffer size
        self.max_fixpipe_c1 = tbe_platform.get_soc_spec(tbe_platform.FB_SIZE) // \
                              BIT_RATIO_MAP.get(UINT64_DTYPE_STR) // int(tbe_platform.get_soc_spec(CUBE_N_SIZE_STR))

    def parse_next_op(self, op_graph):
        """
        Parse the dependencies between nearby tensors.
        The elements at the same index in pre_op_list and post_op_list are in pair.
        """
        for ops in op_graph.input_ops + op_graph.body_ops:
            pre_op = ops["dst_buffer"]
            for next_ops in ops["next_op"]:
                if pre_op not in self.pre_op_list:
                    self.pre_op_list.append(pre_op)
                    self.next_op_list.append([next_ops["dst_buffer"]])
                else:
                    self.next_op_list[self.pre_op_list.index(pre_op)].append(next_ops["dst_buffer"])

    def is_fixpipe_res(self, tensor):
        """
        Check if tensor is fixpipe_res.
        """
        return clear_suffix(tensor.op.tag) in self.fixpipe_res_tag_list

    def parse_fixpipe_res_list(self, res):
        """
        Get all fixpipe_res tensors.
        """
        tensor_queue = deque([res])

        while tensor_queue:
            src_tensor = tensor_queue.popleft()
            if src_tensor in self.fixpipe_res_list:
                pass
            elif self.is_fixpipe_res(src_tensor):
                self.fixpipe_res_list.append(src_tensor)
            else:
                append_list = list(i for i in src_tensor.op.input_tensors)
                tensor_queue.extend(append_list)

    def config_fixpipe_info_dict(self):
        """
        Config fixpipe_info_dict to get tiling.
        """
        # The memory occupation coefficient of fixpipe params in UB is converted by fp16.
        fp16_convert_dict = {
            "int4": 0.25,
            "int8": 0.5,
            "float16": 1,
            "float32": 2,
            "uint64": 4,
            "int32": 2,
            "int16": 1,
            "uint32": 2
        }

        for tensor in self.fixpipe_res_list:
            if clear_suffix(tensor.op.tag) == FIXPIPE_REFORM_TAG:
                init_info = {
                    "cache_read_tensors_fb": [],
                    "cache_read_tensors_ub": [],
                    "fixpipe_param_names": tensor.op.attrs["vector_params"],
                    "fixpipe_tensors": tensor.op.attrs["vector_tensors"]
                    }

                self.fixpipe_info_dict[tensor] = init_info

                #====================parse fixpipe tiling info==========================
                for idx, fixpipe_param_name in enumerate(init_info.get("fixpipe_param_names")):
                    if fixpipe_param_name not in FIXPIPE_V350_SCOPE_MAP:
                        err_man.raise_err_specific(
                            "conv2d",
                            f"tensor {fixpipe_param_name} cannot set scope to fb"
                            )
                    fixpipe_tensor = init_info.get("fixpipe_tensors")[idx]
                    # v350 has no eltwise fixpipe
                    self.fixpipe_channel_coeff += fp16_convert_dict.get(fixpipe_tensor.dtype)

                fixpipe_type_dict = {}
                for unit, state in tensor.op.attrs["op_dict"].items():
                    if state != "":
                        fixpipe_type_dict[unit] = state.value
                self.fixpipe_fused_type.append(fixpipe_type_dict)
            else:
                self.fixpipe_fused_type.append({})
                init_info = {
                    "cache_read_tensors_fb": [],
                    "cache_read_tensors_ub": [],
                    "fixpipe_param_names": [],
                    "fixpipe_tensors": []
                }
                self.fixpipe_info_dict[tensor] = init_info

    def config_fixpipe_dataflow(self, sch, res):
        """
        Get fixpipe inline tensors and cache read for fixpipe vector input tensors.
        """
        disable_inline_tensors = [res]
        for fixpipe_res in self.fixpipe_res_list:
            self.inputs_cache_read(sch, fixpipe_res)
            self.inline_fixpipe_tensors(sch, fixpipe_res, disable_inline_tensors)

    def inline_fixpipe_tensors(self, sch, fixpipe_res, disable_inline_tensors):
        """
        Inline tensors for fixpipe op emit insn.
        """
        def inline_pre_tensors(sch, tensor):
            while tensor.op.tag not in self.cub_tag_list:
                if tensor not in disable_inline_tensors:
                    sch[tensor].compute_inline(instant=True)
                tensor = tensor.op.input_tensors[0]

        inline_pre_tensors(sch, fixpipe_res.op.input_tensors[0])

    def inputs_cache_read(self, sch, fixpipe_res):
        """
        Cache read for fixpipe vector input tensors.
        """
        cache_read_tensors_fb = []
        cache_read_tensors_ub = []
        fixpipe_param_names = self.fixpipe_info_dict.get(fixpipe_res).get("fixpipe_param_names")
        fixpipe_tensors = self.fixpipe_info_dict.get(fixpipe_res).get("fixpipe_tensors")

        for idx, fixpipe_param_name in enumerate(fixpipe_param_names):
            fixpipe_tensor = fixpipe_tensors[idx]
            scope = FIXPIPE_V350_SCOPE_MAP.get(fixpipe_param_name)
            input_fb = sch.cache_read(fixpipe_tensor, scope, self.get_next_op(fixpipe_tensor))
            input_ub = sch.cache_read(fixpipe_tensor, tbe_platform.scope_ubuf, input_fb)
            cache_read_tensors_fb.append(input_fb)
            cache_read_tensors_ub.append(input_ub)

        self.fixpipe_info_dict.get(fixpipe_res).update(
            {"cache_read_tensors_fb": cache_read_tensors_fb,
             "cache_read_tensors_ub": cache_read_tensors_ub
             })

    def special_process_pre(self, sch, res, tensor_param):
        """
        Special process before get tiling for fixpipe fusion.
        """
        # compute inline for conv_op in fixpipe fusion
        conv_res = tensor_param["conv_res"]
        sch[conv_res].compute_inline(instant=True)
        # inline fixpipe_op and config scop
        self.config_fixpipe_dataflow(sch, res)

    def get_fixpipe_fullload_flag(self, co1):
        """
        Check if fixpipe params can full load into fixpipe buffer.
        """
        if co1 > self.max_fixpipe_c1:
            return False

        scope_dict = {
            "FB": set()
        }

        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_fb = fixpipe_info.get("cache_read_tensors_fb")
            for tensor in cache_read_tensors_fb:
                tensor_scope = tensor.op.name.split(".")[-1]
                scope_dict.get(tensor_scope).add(tensor)

        for _, reader_set in scope_dict.items():
            if len(reader_set) * co1 > self.max_fixpipe_c1:
                return False

        return True

    def get_next_op(self, tensor):
        """
        Get the next op list of tensor.
        """
        for index, pre_op in enumerate(self.pre_op_list):
            if pre_op == tensor:
                return self.next_op_list[index]
        err_man.raise_err_specific(
            "conv2d",
            f" cannot get tensor {tensor} in pre_op_list."
            )
        return None

    def fixpipe_inputs_compute_at(self, sch, res, fixpipe_fb_attach_axis, fixpipe_ub_attach_axis):
        """
        Attach the inputs of fixpipe fusion ops to res tensor.
        fixpipe_fb_attach_axis: attach axis in res for fixpipe params in fixpipe buffer.
        fixpipe_ub_attach_axis: attach axis in res for fixpipe params in ub.
        eltwise_attach_axis: attach axis in res for fixpipe eltwise inputs in ub.
        """
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_fb = fixpipe_info.get("cache_read_tensors_fb")
            cache_read_tensors_ub = fixpipe_info.get("cache_read_tensors_ub")

            for tensor in cache_read_tensors_ub:
                sch[tensor].compute_at(sch[res], fixpipe_ub_attach_axis)

            for tensor in cache_read_tensors_fb:
                sch[tensor].compute_at(sch[res], fixpipe_fb_attach_axis)

    def set_tensor_bound(self, sch, fixpipe_ub_bound, fixpipe_fb_bound):
        for fixpipe_res in self.fixpipe_res_list:
            fb_tensor_list = self.fixpipe_info_dict.get(fixpipe_res).get("cache_read_tensors_fb")
            ub_tensor_list = self.fixpipe_info_dict.get(fixpipe_res).get("cache_read_tensors_ub")
            for tensor in fb_tensor_list:
                sch[tensor].set_buffer_size(fixpipe_fb_bound)
            for tensor in ub_tensor_list:
                sch[tensor].set_buffer_size(fixpipe_ub_bound)

    def double_buffer(self, sch, pingpong_buffer):
        """
        Enable double buffer for fixpipe input tensors.
        """
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors_ub = fixpipe_info.get("cache_read_tensors_ub")

            if pingpong_buffer.get("INPUT_L1_FB_pbuffer") == 2:
                for tensor in cache_read_tensors_ub:
                    sch[tensor].double_buffer()

    def fixpipe_inputs_emit_insn(self, sch):
        """
        Dma for the inputs of fixpipe fusion ops.
        """
        for fixpipe_res in self.fixpipe_res_list:
            fixpipe_info = self.fixpipe_info_dict.get(fixpipe_res)
            cache_read_tensors = fixpipe_info.get("cache_read_tensors_fb") +\
                fixpipe_info.get("cache_read_tensors_ub")
            for tensor in cache_read_tensors:
                sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")
