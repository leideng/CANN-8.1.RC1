#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gemm schedule
"""

import functools
import copy
from collections.abc import Iterable

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.common.context import op_context
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.register import get_fusion_buildcfg
from tbe.common.register import set_fusion_buildcfg
from tbe.common.utils.const import ComputeFlow
from tbe.common.utils.errormgr import error_manager_cube
from tbe.common.utils.errormgr import error_manager_util
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.boost_schedule_kit import Compare
from tbe.dsl.boost_schedule_kit import ScheduleAgent
from tbe.dsl.boost_schedule_kit import SplitParam
from tbe.dsl.compute import cube_util
from tbe.dsl.compute.util import get_value
from tbe.dsl.compute.util import int_ceil_div
from tbe.dsl.compute.util import align
from tbe.dsl.instrinsic import cce_emitinsn_params
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import copy_attrs
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import debug
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import get_al1_m_fix_value
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import print_ir_matmul
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import GemmScheduleContainer
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import GemmScheduleStatusController
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import BufferChecker
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import UbBufferReuser
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import CceSimplification
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import get_optional_te_var
from tbe.dsl.static_schedule.gemm_integrated_layout import GemmDataLayoutAMatrix
from tbe.dsl.static_schedule.gemm_integrated_layout import GemmDataLayoutBMatrix
from tbe.dsl.static_schedule.util import DTYPE_BYTE_MAPPING
from tbe.dsl.static_schedule.util import get_all_tensor
from tbe.dsl.static_schedule.util import get_precision_mode
from tbe.dsl.compute.gemm_compute_util import GEMMComputeParam
from tbe.dsl.static_schedule.util import L1CommonParam
from tbe.dsl.static_schedule.util import parse_tbe_compile_para
from tbe.dsl.static_schedule.util import shape_to_list
from tbe.dsl.static_schedule.util import FIXPIPE_SCOPE_MAP


# the scope map
SCOPE_MAP = {
    "UB": tbe_platform_info.scope_ubuf,
    "L1": tbe_platform_info.scope_cbuf,
    "L0C": tbe_platform_info.scope_cc,
    "L0A": tbe_platform_info.scope_ca,
    "L0B": tbe_platform_info.scope_cb,
    "BT": tbe_platform_info.scope_bt
}


def gemm_schedule(res, sch_list, dynamic_para=None):
    """
    schedule enter
    param:
    res: tensor
    sch_list: list of schedule
    """
    gemm_sch = GemmSchedule(res, sch_list, dynamic_para)

    return gemm_sch.gemm_schedule()


def _check_double_out(outs):
    """ check double out condition for matmul ub fusion
    """
    if not isinstance(outs, list):
        return False
    # 2 means two output tensors
    if len(outs) != 2:
        return False
    out1, out2 = outs
    if not isinstance(out1, tvm.Tensor) or not isinstance(out2, tvm.Tensor):
        return False
    # not support matmul multi-output
    if out1.op.name == "tensor_c_gm" and out1.op.tag != "gemm":
        return False
    all_tensor, _, _  = get_all_tensor(out2)
    if all_tensor.get("tensor_c_gm") is None or \
        all_tensor.get("tensor_c_gm").op.tag not in ("gemm", "matmul_gemv", "matmul_gevm", "matmul"):
        return False

    shape0 = [x.value for x in outs[0].shape]
    shape1 = [x.value for x in outs[1].shape]
    # only support elementwise
    return shape0 == shape1


def reget_matmul_multioutput(outs):
    """ add a virtual node to connect double outs for tvm coding rule
    """
    if _check_double_out(outs):

        out1, out2 = outs
        op_tag = "gemm" if out1.op.tag == "gemm" else "matmul_elewise"
        input_tensor = {"gemm": out1.op.input_tensors[0], "matmul_elewise": out1}
        out1_copy = tvm.compute(
            out1.shape,
            lambda *indices: input_tensor.get(op_tag)(*indices).astype(out1.dtype),
            name=op_tag + "out1",
            tag=op_tag
        )
        # 4 means that virtual node only support four dims for now
        if (len(out1.shape) == 4 and len(out2.shape) == 4) or op_tag == "gemm":
            virtual_res = tvm.compute(
                out1.shape,
                lambda *indices: out1_copy(*indices) + out2(*indices),
                name=op_tag + "_virtual_res",
                tag=op_tag + "_virtual_res"
            )
            outs = [virtual_res, out1_copy, out2]
    return outs


class GemmSchedule:
    """
    schedule enter
    param:
    res: tensor
    sch_list: list of schedule
    dynamic_para: dynamic para from gemm_tilingcase
    """
    PER_TENSOR_MODE = ["F322F16", "F322B8", "F322S4", "F322BF16", "S322F16", "S322B8", "S322S4", "S322S16"]
    DEBUG_PARAM = False
    DEBUG_IR = False
    DYN_ALIGNED_MODE = "Aligned_mode"
    GENERAL_MODE = "General_mode"
    DTYPE_WIDTH_MAP = {"uint64": 4, "float16": 1, "float32": 2, "int32": 2,
                       "int16": 1, "uint16": 1, "int8": 0.5, "uint8": 0.5,
                       "int4": 0.25, "bool": 0.5}
    INPUT_SIZE = {"fp162fp16": 2, "fp162fp32": 2, "int82int32": 1, "int82fp32": 1, "int42int32": 0.5}
    L1_L0_SIZE = {"fp162fp16": 2, "fp162fp32": 2, "int82int32": 1, "int82fp32": 2, "int42int32": 0.5}
    OUTPUT_SIZE = {"fp162fp16": 2, "fp162fp32": 4, "int82int32": 4, "int82fp32": 4, "int42int32": 4}
    emit_fusion_insn_map = {
        "dequant_NZ": "phony_insn",
        "cast_f16_ub": "vector_conv",
        "input_ub": "phony_insn",
        "reform_by_vmuls": "vector_muls",
        "scale_sqrt_ub": "vector_muls",
        "offset_ub": "vector_adds",
        "cast_i8_ub": "vector_conv",
        "reform_by_vadds": "vector_adds"
    }

    reform_tensor_tag_list = ("reform_by_vadds",
                              "reform_by_vmuls",
                              "data_transfer")

    emit_insn_map = {"elewise_single_cast": "vector_conv",
                     "elewise_single_round_d": "vector_conv_round",
                     "elewise_single_VS_max": "vector_maxs",
                     "elewise_single_VS_min": "vector_mins",
                     "elewise_single_ceil": "elewise_single_ceil",
                     "elewise_single_log": "vector_ln",
                     "elewise_single_exp": "vector_exp",
                     "elewise_single_relu": "vector_relu",
                     "elewise_single_abs": "vector_abs",
                     "elewise_single_not": "vector_not",
                     "elewise_single_sqrt": "vector_sqrt",
                     "elewise_single_rsqrt": "vector_rsqrt",
                     "elewise_binary_mul": "vector_mul",
                     "elewise_single_rec": "vector_rec",
                     "elewise_single_VS_mul": "vector_muls",
                     "elewise_binary_div": "vector_div",
                     "elewise_binary_sub": "vector_sub",
                     "elewise_binary_add": "vector_add",
                     "elewise_single_VS_add": "vector_adds",
                     "elewise_binary_min": "vector_min",
                     "elewise_binary_max": "vector_max",
                     "elewise_binary_vcmpv_gt": "vector_gt",
                     "elewise_binary_vcmpv_ge": "vector_ge",
                     "elewise_binary_vcmpv_lt": "vector_lt",
                     "elewise_binary_vcmpv_le": "vector_le",
                     "elewise_binary_vcmpv_eq": "vector_eq",
                     "elewise_binary_vcmpv_ne": "vector_ne",
                     "elewise_binary_cmpsel_gt": "vector_select_gt",
                     "elewise_binary_cmpsel_ge": "vector_select_ge",
                     "elewise_binary_cmpsel_lt": "vector_select_lt",
                     "elewise_binary_cmpsel_le": "vector_select_le",
                     "elewise_binary_cmpsel_eq": "vector_select_eq",
                     "elewise_binary_cmpsel_ne": "vector_select_ne",
                     "elewise_binary_or": "vector_or",
                     "elewise_binary_and": "vector_and",
                     "elewise_multiple_mla": "vector_multiple",
                     "elewise_multiple_madd": "vector_multiple",
                     "elewise_multiple_maddrelu": "vector_multiple",
                     "elewise_multiple_sel": "vector_select_bool",
                     "elewise_binary_scalar_axpy": "vector_axpy",
                     "elewise_binary_cmpsel": "vector_cmpsel",
                     "broadcast": "vector_dup",
                     "emit_insn_elewise_multiple_sel": "elewise_multiple_sel",
                     "emit_insn_elewise_binary_cmp": "elewise_binary_cmp"
                     }

    requant_fusion_insn_map = {"tensor_c_gm": "phony_insn",
                               "tensor_c_ub": "phony_insn",
                               "s32_to_s8": "dma_copy",
                               "data_transfer": "dma_copy"}

    DEQ_SCALE_CHILD_LIST = [
        "dequant",
        "dequant_scale",
        "dequant_sqrt",
    ]
    M1_VAR_NAME = "m"
    N1_VAR_NAME = "n"
    K1_VAR_NAME = "k"
    THRESHOLD_DATA_NUM = 64
    ND_M_INDEX = -2
    ND_N_INDEX = -1
    FRACTAL_NZ_M_INDEX = -3
    FRACTAL_NZ_N_INDEX = -4
    FRACTAL_NZ_M0_INDEX = -2
    FRACTAL_NZ_N0_INDEX = -1
    FRACTAL_Z_M_INDEX = -4
    FRACTAL_Z_N_INDEX = -3
    FRACTAL_Z_KA_INDEX = -3
    FRACTAL_Z_LEN = 4
    BLOCK_BATCH_DIM_INDEX = 0
    BLOCK_N_DIM_INDEX = 1
    BLOCK_M_DIM_INDEX = 2
    ALLOCATE_OFF = 0
    ALLOCATE_HALF = 1
    ALLOCATE_FULL = 2
    KBL1_LARGER_FLAG = 2
    BLOCKS_PER_REPEAT = 8

    # index of tiling
    IDX_BATCH_DIM = 0
    IDX_K_DIM = 3

    IDX_MULTI_M1 = 1

    UINT16_MAX = 65535

    # for fp32 and int8 input, some factors should be fixed from default value 16
    MULTI_FACTOR = 2
    AXIS_ALIGN_FACTOR = 2

    BATCH_MATMUL_LEN_ND = 3
    MATMUL_LEN_ND = 2

    MAX_SPLIT_K_TIMES = 3
    is_dynamic = False
    # pad-fusion
    PAD_B = 1
    PAD_A = 2
    PAD_AB = 3
    # transdata-fusion
    NZ_VEC_B = 1
    NZ_VEC_A = 2
    NZ_VEC_AB = 3
    # transdata_fusion_mode
    NZ_SERIAL = 1
    NZ_PIPELINE_ATTACH = 2
    NZ_PIPELINE_NOT_ATTACH = 3

    def __init__(self, res, sch_list, dynamic_para):
        self.tensor_list = dynamic_para["tensor_list"]
        self.tensor_map, self.para_map = self.tensor_list[3:]
        self.res_ori = res
        self.res = res[-1] if isinstance(res, list) else res
        self.root_tensor = res[-1] if isinstance(res, list) else res
        self.sch = sch_list[0]
        self.sch_list = sch_list
        self.sch_agent = None
        self.dynamic_para = dynamic_para
        # used to control aligned/general mode in dynamic shape
        self.schedule_mode = self.GENERAL_MODE
        self.input_l1_size = 0
        self.in_addr_type, self.out_addr_type = 0, 0
        self.tensor_a_l1_workspace = 0
        self.dynamic_m, self.dynamic_k, self.dynamic_n, self.dynamic_batch = dynamic_para["dynamic_seed_shape"]
        debug(self.DEBUG_PARAM, dynamic_para["dynamic_seed_shape"], "dynamic_seed_shape:")
        self.dtype_info = {"a": self.para_map.get("a_dtype"), "b": self.para_map.get("b_dtype"),
                           "out": self.res.dtype}
        self.format_info = {"a": self.para_map.get("format_a"), "b": self.para_map.get("format_b"),
                            "out": self.para_map.get("format_out")}
        self.get_a_matrix_mode = self.para_map.get("a_matrix_mode", "none")
        self.get_b_matrix_mode = self.para_map.get("b_matrix_mode", "none")
        self.l1_fusion_type = 0
        self.compute_param = dynamic_para.get("compute_param")
        self.status_ori_dict = {
            0: Compare.EQUAL,
            1: Compare.LESS_EQ,
            2: Compare.LESS_EQ,
            3: Compare.LESS_EQ
        }
        self.status_dict = {
            0: Compare.EQUAL,
            1: Compare.EQUAL,
            2: Compare.LESS_EQ,
            3: Compare.GREATE_EQ
        }
        self.status_abub_dict = {
            4: Compare.LESS_EQ,
            5: Compare.EQUAL
        }
        # Call initialization function
        self.container = GemmScheduleContainer(self.para_map)
        self.status_controller = GemmScheduleStatusController(self.para_map)
        self.buffer_checker = BufferChecker()
        self.cce_simplification_obj = CceSimplification(self.sch, dynamic_para)
        self.tensor_b_reshape = 0
        # tiling initialization
        self.cache_tiling_mgr = dynamic_para["cache_tiling_mgr"]
        self.cache_tiling_mgr.sch = self.sch
        self.cache_tiling = self.cache_tiling_mgr.cache_tiling
        if self.cache_tiling:
            self.res.op.attrs["cache_tiling"] = 1
        self.tiling_work = dynamic_para["tiling_work"]
        debug(self.DEBUG_PARAM, self.tiling_work.tiling, "auto tiling result")
        self.status_controller.attach_at_flag = self.tiling_work.tiling.get("attach_at_flag")
        self.block_in = self.tiling_work.block_in
        self.block_out = self.tiling_work.block_out
        self.block_reduce = self.tiling_work.block_reduce
        self.bind_core_when_full_load_bl1 = self.tiling_work.bind_core_when_full_load_bl1
        self.mn_axis = [1, 1]
        self.binary_constant = op_context.get_context().get_addition("is_binary_constant")
        self.al1_db_flag = op_context.get_context().get_addition("al1_db")
        self.bl1_db_flag = op_context.get_context().get_addition("bl1_db")
        self.sparse_4to2_flag = bool(self.para_map.get("alg", None) == "weight_sparse_4_2")
        self.bias_broadcast_to_l0c = False
        self.support_shift_block = False
        self.private_attach_dict = {}

    @staticmethod
    def _get_addr_type(tensor):
        addr_type = 0
        if "addr_type" in tensor.op.attrs and tensor.op.attrs["addr_type"].value == 1:
            addr_type = 1
        return addr_type

    @staticmethod
    def _add_tensor_to_list(tensor, tensors_list_list):
        if tensor is not None:
            for tensors_list in tensors_list_list:
                if tensor not in tensors_list:
                    tensors_list.append(tensor)

    @staticmethod
    def _check_reused_none(value):
        if isinstance(value, list):
            return all(item is not None for item in value)
        else:
            return value is not None

    @staticmethod
    def _match_and_get_tensor(tensors, tensor_name):
        """
        match and get tensor
        """
        for i in tensors:
            if tensor_name == i.op.name:
                return i
        return None

    @staticmethod
    def _round_emit_insn(round_mode):
        """
        Obtains the conv instruction by the round mode attr

        Parameters
        ----------
        round_mode: the attr of round mode

        Returns
        -------
        instruction
        """
        if cube_util.is_mini_version():
            emit_insn_str = "vector_conv"
        else:
            if round_mode == "Round":
                emit_insn_str = "vector_conv"
            else:
                raise RuntimeError("Round mode should be Round only, %s is not supported" % round_mode)
        return emit_insn_str

    @staticmethod
    def _get_input_l1_paras(tensor):
        input_l1_flag = -1
        input_l1_size = -1
        if 'L1_addr_flag' in tensor.op.attrs:
            input_l1_flag = tensor.op.attrs['L1_addr_flag'].value

        if input_l1_flag == 1:
            if 'L1_valid_size' in tensor.op.attrs:
                input_l1_size = tensor.op.attrs['L1_valid_size'].value
            else:
                input_l1_flag = -1

        return input_l1_flag, input_l1_size

    @staticmethod
    def _get_l1_fusion_type(tensor):
        l1_fusion_type = -1
        if "L1_fusion_type" in tensor.op.attrs:
            l1_fusion_type = tensor.op.attrs["L1_fusion_type"].value
        return l1_fusion_type

    @staticmethod
    def _get_compress_block_info(tile_k, tile_n):
        """
        get weigths compress info, like, block size, index size
        """
        block_size_max = 32 * 1024
        block_unit = 512
        data_size = tile_k * tile_n * block_unit
        if isinstance(data_size, int) and data_size > block_size_max:
            error_manager_cube.raise_err_message_cube("block_size cannot be greater than block_size_max")
        return int(data_size)

    @staticmethod
    def _get_compute_tensor(tensor):
        """
        scan all the transient tensor during calculation
        tensor: target tensor which needs to find placeholder tensor
        """
        compute_tensors_local = []

        def enter(tensor):
            """
            get compute tensors by search
            """
            if tensor not in compute_tensors_local:
                compute_tensors_local.append(tensor)
            tensor_list = tensor.op.input_tensors
            for one_tensor in tensor_list:
                # check which tensor has not been checked
                if not isinstance(one_tensor.op, tvm.PlaceholderOp):
                    if one_tensor not in compute_tensors_local:
                        compute_tensors_local.append(one_tensor)
                        enter(one_tensor)
        enter(tensor)
        return compute_tensors_local

    @staticmethod
    def _get_compress_index_size(mode, k_block_num, n_block_num):
        index_size = k_block_num * n_block_num

        tight_len = 2
        if mode == 1:
            tight_len = 8
        index_size = index_size * tight_len
        return index_size

    def gemm_schedule(self):
        """
        the main func of gemm_schedule
        """
        print_ir_matmul(self.DEBUG_IR, "original ir", self.sch)
        self._set_para_for_genenal()
        if in_dynamic():
            self._set_para_for_dynamic()
        self._set_data_layout()
        self._set_disable_l2_cache()
        self._set_buffer_reuse_dict()
        self._update_flag_after_tiling()
        self._set_nd_out_compute_inline()
        self._atomic_add_k_axis()
        self._do_ub_buffer_reuse()
        self._compute_align_for_int8()
        self.sch_agent = ScheduleAgent(self.sch)
        self.cache_tiling_mgr.sch_agent = self.sch_agent
        self.tiling_work.set_factor_shape(self.cache_tiling_mgr, self.format_info,
                                          self.status_controller, self.para_map)
        self.cache_tiling_mgr.simplify_cache_tiling(self.cce_simplification_obj, self.compute_param,
                                                    self.container)
        # multi-core is binded before split in non-factor binary scene
        self._bind_multi_core(False)
        self._cub_process()
        self._do_l0_process()
        self._do_l1_ub_process()
        self._do_bias_table_process()
        self._do_mix_l2_process()
        self._do_fixpipe_process()
        self._bind_multi_core(True)
        if self.cache_tiling_mgr.reorder_flag and self.cache_tiling_mgr.mata_fuzzy_scenario == False:
            offset = 0 if self.format_info.get('out') == "ND" else 4
            root_axis = self.sch[self.root_tensor].leaf_iter_vars
            self.sch[self.root_tensor].reorder(root_axis[-3 - offset], root_axis[-4 - offset])
        self.cache_tiling_mgr.multi_batch_process(self.container, self.status_controller, self.sch_agent)
        self._do_emit_insn()
        self._do_buffer_reuse()
        self._do_buffer_align()
        self._set_store_predicate()
        self._solve_bank_conflict()
        self._solve_split_k_dirty_data()
        a_run_once, b_run_once = self._allocate_axis(self.status_controller.over_head_flag)
        self.cache_tiling_mgr.cache_tiling_full_load(self.container, self.status_controller, self.sch_agent,
                                                     self.root_tensor)
        self._double_buffer(a_run_once, b_run_once)
        self._reorder_axis(self.status_controller.over_head_flag, a_run_once, b_run_once)
        self._do_compute_inline()
        self._mem_process()
        self._set_continuous_axis()
        if self.is_dynamic:
            self.cce_simplification_obj.cce_simplify(self.compute_param, self.cache_tiling_mgr,
                                                     self.container, self.para_map)
        print_ir_matmul(self.DEBUG_IR, "final ir", self.sch)
        self._update_fusion_buildcfg()
        special_tensor_list = self._collect_all_spec_mid_tensor()
        self.tiling_work.tiling.clear()
        self.container.tensor_map.clear()
        return special_tensor_list

    def _compute_align_for_int8(self):
        need_compute_align_shape = (self.status_controller.support_fix_pipe_l0c2out and in_dynamic() and
                                    self.para_map.get("a_dtype") == "int8" and
                                    self.para_map.get("format_out") == "FRACTAL_NZ")
        if need_compute_align_shape:
            if self.status_controller.transpose_a:
                a_l0a = self.container.tensor_map.get("a_l0a")
                self.sch[a_l0a].compute_align(a_l0a.op.axis[self.FRACTAL_Z_M_INDEX], self.AXIS_ALIGN_FACTOR)
            if not self.status_controller.transpose_b:
                b_l0b = self.container.tensor_map.get("b_l0b")
                self.sch[b_l0b].compute_align(b_l0b.op.axis[self.FRACTAL_Z_N_INDEX], self.AXIS_ALIGN_FACTOR)

        need_compute_align_l1_shape = (self.status_controller.support_fix_pipe_l0c2out and (not in_dynamic()) and
                                       self.para_map.get("a_dtype") == "int8" and
                                       self.para_map.get("format_out") == "FRACTAL_NZ")
        if need_compute_align_l1_shape:
            if self.status_controller.transpose_a:
                a_l1 = self.container.tensor_map.get("a_l1")
                self.sch[a_l1].compute_align(a_l1.op.axis[-3], self.AXIS_ALIGN_FACTOR)
            if not self.status_controller.transpose_b:
                b_l1 = self.container.tensor_map.get("b_l1")
                self.sch[b_l1].compute_align(b_l1.op.axis[-3], self.AXIS_ALIGN_FACTOR)

    def _get_overlap_cond_list(self):
        # Using condition to check whether last 2 blockdim has overlap data
        # The condition consists of a non-factorial block dim cut and shift-inward enabled
        m = self.cache_tiling.get("m")
        n = self.cache_tiling.get("n")
        close_k_shift_flag = self.cache_tiling.get("close_k_shift")
        m_split = (self.cache_tiling.get("m_single_core") * self.cache_tiling.get("m_al1") *
                   self.cache_tiling.get("m_l0"))
        n_split = (self.cache_tiling.get("n_single_core") * self.cache_tiling.get("n_bl1") *
                   self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1"))
        m_overlap_cond = tvm.tir.any(tvm.tir.all(m % m_split != 0, (self.mn_axis[0] + 2) * m_split > m),
                                     close_k_shift_flag >= 1)
        n_overlap_cond = tvm.tir.any(tvm.tir.all(n % n_split != 0, (self.mn_axis[1] + 2) * n_split > n),
                                     close_k_shift_flag >= 1)
        return [m_overlap_cond, n_overlap_cond]

    def _support_unit_flag(self):
        unsupport_unit_flag = self.status_controller.nz_fusion_mode == self.NZ_PIPELINE_NOT_ATTACH
        unsupport_unit_flag = unsupport_unit_flag or self.status_controller.unaligned_flag
        l0c_pb = self.cce_simplification_obj.tiling.get("manual_pingpong_buffer").get("CL0_pbuffer")
        format_a = self.para_map.get("format_a")
        format_b = self.para_map.get("format_b")
        format_out = self.para_map.get("format_out")
        nz_flag = (format_a == "FRACTAL_NZ" and format_b == "FRACTAL_NZ" and format_out == "FRACTAL_NZ")
        nd_flag_a = (format_a == "ND") or (self.status_controller.nz_fusion_flag & self.NZ_VEC_A != 0)
        nd_flag_b = (format_b == "ND") or (self.status_controller.nz_fusion_flag & self.NZ_VEC_B != 0)
        nd_flag = (nd_flag_a and nd_flag_b and format_out == "ND")
        usable_format = nz_flag or nd_flag
        split_k_flag = self.status_controller.split_k or self.status_controller.split_k_axis_by_tiling
        if l0c_pb != 1 or split_k_flag or not usable_format or unsupport_unit_flag:
            return False
        return True

    def _get_unit_flag_type(self):
        # if the move-out size does not equal to mmad size, pragma needs to be "non-align-condition"
        pad_m_inner_axis = (
            self.status_controller.pad_flag in [self.PAD_A, self.PAD_AB] and self.status_controller.transpose_a)
        pad_n_inner_axis = (
            self.status_controller.pad_flag in [self.PAD_B, self.PAD_AB] and not self.status_controller.transpose_b)
        unit_flag_type = (
            "unit_flag_non_align_condtion" if (pad_m_inner_axis or pad_n_inner_axis) else "unit_flag_condition")
        return unit_flag_type

    def _do_unit_flag(self, mad_dict, shift_inwards_flag):
        if not self._support_unit_flag():
            return
        c_l0c = self.container.tensor_map.get("c_l0c")
        abkl1_attach_flag = self.cache_tiling_mgr.attach_at_flag.get("abkl1_attach_flag")
        min_kl1_cmp_kl0 = self.cache_tiling_mgr.attach_at_flag.get("min_kl1_cmp_kl0")
        k_outer = self.sch[c_l0c].leaf_iter_vars[-1]
        # Int8 input return k_var which is calculated using k0 = 16.
        k_var_name = "k_ori" if not self.status_controller.pad_flag else "k_pad"
        k_ori_1 = int_ceil_div(self.cache_tiling.get(k_var_name), self.block_reduce)
        k1 = self.cache_tiling.get("k") if (self.para_map.get("a_dtype") != "int8"
                                            and not self.status_controller.pad_flag) else k_ori_1
        k_l0 = self.cache_tiling.get("k_l0")
        kal0_factor = self.cache_tiling.get("kal0_factor")
        kbl0_factor = self.cache_tiling.get("kbl0_factor")
        kl1_times = self.cache_tiling.get("kl1_times")
        k_outer_list = (self.MAX_SPLIT_K_TIMES - len(mad_dict["k_outer"])) * [0] + mad_dict["k_outer"]
        kl0_factor = kal0_factor if abkl1_attach_flag == 2 else kbl0_factor
        # When min_kl1_cmp_kl0 is Zero. The minimum length of K in L1 is the same as the length of K in L0
        k_outer_second_times = kl0_factor if min_kl1_cmp_kl0 == 0 else kl1_times
        k_outer_third_times = kl0_factor * kl1_times
        last_block_index = int_ceil_div((k1 - k_l0), k_l0)
        k_outer_times = (
                    k_outer_list[0] * k_outer_third_times + k_outer_list[1] * k_outer_second_times + k_outer_list[2])
        if min_kl1_cmp_kl0 == 0:
            # L1 = L0的分支，有三根轴
            if shift_inwards_flag:
                last_block_index = ((int_ceil_div(k1, kl1_times * kl0_factor * k_l0) - 1) * (kl1_times * kl0_factor) +
                                    (kl1_times - 1) * kl0_factor + kl0_factor - 1)
                k_outer_times = (
                    k_outer_list[-3] * kl1_times + k_outer_list[-2] + k_outer_list[-1])
            else:
                # 有两根轴
                last_block_index = int_ceil_div((k1 - k_l0), k_l0)
                k_outer_times = (k_outer_list[1] * kl1_times + k_outer_list[2])
        else:
            # L1 > L0的分支，此时outer轴有 2 根
            if len(mad_dict["k_outer"]) == 2:
                k_outer_times = k_outer_list[-2] * kl0_factor + k_outer_list[-1]
                last_block_index = ((int_ceil_div(k1, kl1_times * kl0_factor * k_l0) - 1) * (kl1_times * kl0_factor) +
                                    (kl1_times - 1) * kl0_factor + kl0_factor - 1)
            # 此时outer轴有 3 根
            elif len(mad_dict["k_outer"]) == 3:
                k_outer_third_times = kl0_factor * kl1_times
                k_outer_second_times = kl0_factor
                k_outer_times = (
                    k_outer_list[0] * k_outer_third_times + k_outer_list[1] * k_outer_second_times + k_outer_list[2])
                last_block_index = ((int_ceil_div(k1, kl1_times * kl0_factor * k_l0) - 1) * (kl1_times * kl0_factor) +
                                    (kl1_times - 1) * kl0_factor + kl0_factor - 1)
        nz_flag = (self.para_map.get("format_a") == "FRACTAL_NZ" and
                   self.para_map.get("format_b") == "FRACTAL_NZ" and
                   self.para_map.get("format_out") == "FRACTAL_NZ")
        last_block_index = int_ceil_div((k1 - k_l0), k_l0) if (nz_flag and not shift_inwards_flag) else last_block_index
        cond_last_block = tvm.expr.EQ(k_outer_times, last_block_index)
        unit_flag_type = self._get_unit_flag_type()
        self.sch[c_l0c].pragma(k_outer, unit_flag_type, cond_last_block)


    def _do_l0_process(self):
        al1_attach_flag = self.status_controller.attach_at_flag.get("al1_attach_flag")
        bl1_attach_flag = self.status_controller.attach_at_flag.get("bl1_attach_flag")
        abkl1_attach_flag = self.status_controller.attach_at_flag.get("abkl1_attach_flag")
        self._cl0_process()
        non_factor_invalid_flag = not self.status_controller.split_k_axis_by_tiling
        # 2 means no full load
        no_full_load_flag = al1_attach_flag == 2 and bl1_attach_flag == 2
        if self.cache_tiling and no_full_load_flag and non_factor_invalid_flag and \
            not self.status_controller.unaligned_flag:
            al1 = self.container.tensor_map.get("a_l1")
            bl1 = self.container.tensor_map.get("b_l1")
            al0 = self.container.tensor_map.get("a_l0a")
            bl0 = self.container.tensor_map.get("b_l0b")
            cl0 = self.container.tensor_map.get("c_l0c")
            k_l0 = self.cache_tiling.get("k_l0")
            m_l0 = self.cache_tiling.get("m_l0")
            cub_n1 = self.cache_tiling.get("cub_n1")
            n_ub_l0_time = self.cache_tiling.get("n_ub_l0_time")
            n_l0 = n_ub_l0_time * cub_n1
            block_dims = self.tiling_work.tiling.get("block_dim")
            m_dim = block_dims[2]
            n_dim = block_dims[1]
            batch_offset = 1 if self.status_controller.have_batch else 0
            m0 = self.sch[cl0].leaf_iter_vars[2 + batch_offset]
            tail_strategy = "shift_inwards" if not self.para_map.get("compress_flag", False) else "guard_with_if"
            n1_outer, n1_inner = self.sch[cl0].split(
                self.sch[cl0].leaf_iter_vars[batch_offset], n_l0, tail_strategy=tail_strategy)
            m1_outer, m1_inner = self.sch[cl0].split(
                self.sch[cl0].leaf_iter_vars[2 + batch_offset], m_l0, tail_strategy=tail_strategy)
            n0_outer, n0_inner = self.sch[cl0].split(
                self.sch[cl0].leaf_iter_vars[5 + batch_offset], 16, tail_strategy=tail_strategy)
            inner_extents = [self.cache_tiling.get("kal1_16"), self.cache_tiling.get("kbl1_16")]
            inner_tensors = [al1, bl1]
            b_greater_than_a = abkl1_attach_flag == 2
            # split
            k_outer, k_inner = self.sch[cl0].split(
                self.sch[cl0].leaf_iter_vars[-2], inner_extents[b_greater_than_a], tail_strategy=tail_strategy)
            k_inner_outer, k_inner_inner = self.sch[cl0].split(
                k_inner, inner_extents[1 - b_greater_than_a], tail_strategy=tail_strategy)
            k_inner_inner_outer, _ = self.sch[cl0].split(k_inner_inner, k_l0, tail_strategy=tail_strategy)
            split_k_flag = self.status_controller.split_k or self.status_controller.split_k_axis_by_tiling
            self.support_shift_block = self.is_dynamic and self.status_controller.support_fix_pipe_l0c2out and \
                self.mn_axis != [1, 1] and not split_k_flag
            if self.support_shift_block and self.status_controller.nz_fusion_mode != self.NZ_PIPELINE_NOT_ATTACH:
                overlap_cond_list = self._get_overlap_cond_list()
                self.sch.shift_block_access(self.mn_axis, [m_dim, n_dim], k_outer, overlap_cond_list)
            # reorder
            self.sch[cl0].reorder(n1_outer, m1_outer, n0_outer, k_outer,
                                  k_inner_outer, k_inner_inner_outer, n1_inner, m1_inner, m0, n0_inner)
            # compute at
            self.sch[al0].compute_at(self.sch[cl0], k_inner_inner_outer)
            self.sch[bl0].compute_at(self.sch[cl0], k_inner_inner_outer)
            self.sch[inner_tensors[b_greater_than_a]].compute_at(self.sch[cl0], k_outer)
            self.sch[inner_tensors[1 - b_greater_than_a]].compute_at(self.sch[cl0], k_inner_outer)
            self.private_attach_dict.update({
                self.sch[inner_tensors[b_greater_than_a]]: self.sch[cl0],
                self.sch[inner_tensors[1 - b_greater_than_a]]: self.sch[cl0]
            })
            mad_dict = {
                "mad_pattern": self.para_map.get("mad_pattern"),
                "k_outer": [k_outer, k_inner_outer, k_inner_inner_outer],
                "enable_k_alignment": self.status_controller.enable_k_alignment
            }
            mad_dict["hf32"] = (
                get_optional_te_var("hf32_flag") if self.status_controller.ops_data_flow_mode == "fp322fp32" else 0)
            if self.container.tensor_map.get("c_add_bias") is not None and self.para_map.get("compress_flag", False) \
                and not self.sparse_4to2_flag:
                self.sch[cl0].pragma(n1_inner, "replace_output", 0)
                mad_dict["init_bias"] = 1
            if self.status_controller.support_bf16 and al0.dtype == "float16":
                mad_dict["datatype_bf16"] = self.cache_tiling.get("datatype_bf16")
            self.sch[cl0].emit_insn(n1_inner, "mad", mad_dict)
            self.status_controller.al1_attach_status = "c_l0c"
            self.status_controller.bl1_attach_status = "c_l0c"
            # unit flag only l0c not db is usalbe
            self._do_unit_flag(mad_dict, tail_strategy == "shift_inwards")
        else:
            self._al0_process()
            self._bl0_process()

    def _do_ub_buffer_reuse(self):
        """
        binary ub buffer reuse
        """
        if not self.cache_tiling or self.status_controller.support_fix_pipe_l0c2out:
            return
        ub_reuse_obj = UbBufferReuser(self.tiling_work.tiling, self.container.tensor_map,
                                      self.container.buffer_reuse_dict)
        ub_reuse_obj.set_ub_reuse_process(self.cache_tiling, self.status_controller, self.format_info)

    def _set_store_predicate(self):
        if not self.cache_tiling:
            return

        condition_fp16 = (self.cache_tiling_mgr.performance_flag == 1
                          or tvm.all(self.cache_tiling.get("out_branch_flag") <= 1))
        condition_bias_fp16 = tvm.all(self.cache_tiling.get("out_branch_flag") <= 1,
                                      self.cache_tiling.get("bias_flag") == 1)
        cast_to_fp16 = self.container.tensor_map.get("cast_to_fp16")
        c_add_bias_ub_fp16 = self.container.tensor_map.get("c_add_bias_ub_fp16")
        bias_ub_drnn_cast_fp16 = self.container.tensor_map.get("bias_ub_drnn_cast_fp16")
        bias_ub_fp16 = self.container.tensor_map.get("bias_ub_fp16")
        nz_to_nd = self.container.tensor_map.get("nz_to_nd")
        tensor_out_fp16 = self.container.tensor_map.get("tensor_out_fp16")
        for tensor in [cast_to_fp16, nz_to_nd, tensor_out_fp16]:
            if tensor is not None:
                self.sch[tensor].set_store_predicate(condition_fp16)
        for tensor in [c_add_bias_ub_fp16, bias_ub_fp16, bias_ub_drnn_cast_fp16]:
            if tensor is not None:
                self.sch[tensor].set_store_predicate(condition_bias_fp16)

        condition_fp32 = tvm.all(self.cache_tiling.get("out_branch_flag") > 1)
        condition_bias_fp32 = tvm.all(condition_fp32, self.cache_tiling.get("bias_flag") == 1)
        cast_to_fp32 = self.container.tensor_map.get("cast_to_fp32")
        c_add_bias_ub_fp32 = self.container.tensor_map.get("c_add_bias_ub_fp32")
        bias_ub_drnn_cast_fp32 = self.container.tensor_map.get("bias_ub_drnn_cast_fp32")
        bias_ub_fp32 = self.container.tensor_map.get("bias_ub_fp32")
        nz_to_nd_fp32 = self.container.tensor_map.get("nz_to_nd_fp32")
        tensor_out_fp32 = self.container.tensor_map.get("tensor_out_fp32")
        for tensor in [cast_to_fp32, nz_to_nd_fp32, tensor_out_fp32]:
            if tensor is not None:
                self.sch[tensor].set_store_predicate(condition_fp32)
        for tensor in [c_add_bias_ub_fp32, bias_ub_fp32, bias_ub_drnn_cast_fp32]:
            if tensor is not None:
                self.sch[tensor].set_store_predicate(condition_bias_fp32)

    def _set_and_add_tensor_to_list(self, tensor_map, scope_name, name_list, tensors_list=None):
        """
        set scope for tensor and save to tensors_list
        """
        if len(name_list) == 1:
            base_name = save_name = name_list[0]
        else:
            base_name, save_name = name_list
        tensor = tensor_map.get(base_name)
        if save_name is None:
            save_name = base_name
        if tensor is not None:
            self.sch[tensor].set_scope(SCOPE_MAP.get(scope_name))
            self.container.tensor_map[save_name] = tensor
            if tensors_list is not None and tensor not in tensors_list:
                tensors_list.append(tensor)

    def _set_para_for_genenal(self):
        # set status_controller para
        self.status_controller.need_init_bias = self.tensor_map.get("init_value_of_bias_ub") is not None
        self.tensor_b_reshape = 1 if self.tensor_map.get("tensor_b_reshape") is not None else 0
        if self.para_map.get("fusion_multi_output_flag", False):
            self.container.double_out_tensor = self.tensor_map.get("multi_output_list")
            self.root_tensor = self.res_ori[0]
        self._fusion_para()

    def _set_para_for_dynamic(self):
        self.is_dynamic = True
        if self.dynamic_para.get("tiling_strategy").get("schedule_pattern") == "Aligned":
            self.schedule_mode = self.DYN_ALIGNED_MODE
            self.status_controller.a_use_aligned_pattern = True
            self.status_controller.b_use_aligned_pattern = True
        if self.tensor_map.get("a_ub") is not None:
            self.status_controller.a_use_aligned_pattern = \
                self.tensor_map["a_ub"].op.attrs["use_aligned_pattern"].value
        if self.tensor_map.get("b_ub") is not None:
            self.status_controller.a_use_aligned_pattern = \
                self.tensor_map["b_ub"].op.attrs["use_aligned_pattern"].value
        self.cce_simplification_obj.status_controller = self.status_controller

    def _get_real_k_multi_core_axis(self):
        """
        The multi-core loop range must be correctly identified during the operation of removing dirty data when multiple
        cores are bound to the k axis.
        When the bound multi-core is a non-factor, the cyclic range of the inner axis is upward aligned. If the length
        of the X axis is A and N cores are bound, the (N-1)th core has loaded the length A, and if the X axis has no
        segmentation other than the multi-core, In this case, the actual multi-core cycle range is N-1.
        """
        block_dims = self.tiling_work.tiling.get("block_dim")
        m_dim = block_dims[2]
        n_dim = block_dims[1]
        real_multi_core_axis = self.container.axis_core
        if self.is_dynamic:
            block_dims_without_k_dim = n_dim * m_dim
        else:
            tensor_a_l0a = self.container.tensor_map.get("a_l0a")
            m_shape = get_value(tensor_a_l0a.shape[self.FRACTAL_Z_M_INDEX])
            core_align_m = int_ceil_div(m_shape, m_dim)
            if self.tiling_work.al1_tiling_m == core_align_m:
                m_dim = int_ceil_div(m_shape, core_align_m)
            tensor_b_l0b = self.container.tensor_map.get("b_l0b")
            n_shape = get_value(tensor_b_l0b.shape[self.FRACTAL_Z_N_INDEX])
            core_align_n = int_ceil_div(n_shape, n_dim)
            if self.tiling_work.bl1_tiling_n == core_align_n:
                n_dim = int_ceil_div(n_shape, core_align_n)
            block_dims_without_k_dim = n_dim * m_dim
        if block_dims_without_k_dim != 1:
            real_multi_core_axis //= block_dims_without_k_dim
        return real_multi_core_axis

    def _solve_split_k_dirty_data(self):
        # cache_tiling does not have not factor split, do not need this function now
        if not self.status_controller.split_k_axis_by_tiling or self.cache_tiling:
            return

        block_dims = self.tiling_work.tiling.get("block_dim")
        real_multi_core_axis = self._get_real_k_multi_core_axis()
        a_l0 = self.container.tensor_map.get("a_l0a")
        a_l0_shape_k = get_value(a_l0.shape[-3])
        temp_block_k_dim = block_dims[3] - 1
        no_tail_core_k_len = (a_l0_shape_k + temp_block_k_dim) // block_dims[3]
        tail_core_k_len = a_l0_shape_k % no_tail_core_k_len
        lastest_core = a_l0_shape_k // no_tail_core_k_len
        lastest_core_limit = lastest_core
        c_gm = self.container.tensor_map.get("c_gm")
        if self.is_dynamic:
            lastest_core_limit = tvm.select(no_tail_core_k_len * lastest_core == a_l0_shape_k, lastest_core_limit - 1,
                                            lastest_core_limit)
        else:
            if no_tail_core_k_len * lastest_core == a_l0_shape_k:
                lastest_core_limit -= 1
        self.sch[c_gm].set_store_predicate(real_multi_core_axis <= lastest_core_limit, partition=True)
        no_tail_core_condition = self._get_condition_of_multi_k_axis(real_multi_core_axis, lastest_core,
                                                                     no_tail_core_k_len)
        tail_core_condition = self._get_condition_of_multi_k_axis(real_multi_core_axis,
                                                                  lastest_core,
                                                                  tail_core_k_len,
                                                                  is_tail_core=True)
        condition = tvm.any(no_tail_core_condition, tail_core_condition)

        l0_ka = self.tiling_work.al0_tiling_ka * self.block_reduce
        block_size = self.block_in * self.block_reduce
        k_tail_block_stride = (no_tail_core_k_len - no_tail_core_k_len // self.tiling_work.al0_tiling_ka *
                               self.tiling_work.al0_tiling_ka) * block_size
        k_tail_block_stride_tail_core = (tail_core_k_len - tail_core_k_len // self.tiling_work.al0_tiling_ka *
                                         self.tiling_work.al0_tiling_ka) * block_size

        al0_k_dim_full_load = self.status_controller.al0_attach_status != "c_l0c"
        if (not self.is_dynamic and no_tail_core_k_len % self.tiling_work.al0_tiling_ka == 0) or al0_k_dim_full_load:
            return

        self.sch[a_l0].set_store_predicate(
            a_l0.op.axis[-3].var - real_multi_core_axis * no_tail_core_k_len < no_tail_core_k_len, partition=True)

        # the stride of m axis
        ori_stride = l0_ka * self.block_in
        k_tail_block_stride_final = tvm.select(real_multi_core_axis < lastest_core, k_tail_block_stride,
                                               k_tail_block_stride_tail_core)

        stride = tvm.select(condition, k_tail_block_stride_final, ori_stride)
        self.sch[a_l0].bind_buffer(a_l0.op.axis[-4], stride, 0)
        if self.is_dynamic:
            self.sch[a_l0].set_buffer_size(l0_ka * self.tiling_work.al0_tiling_ma * self.block_in)

    def _get_condition_of_multi_k_axis(self, multi_core_axis, block_dim_k, k_len, is_tail_core=False):
        c_l0c = self.container.tensor_map.get("c_l0c")
        # the axis number of l0c is 6
        total_axis_l0c = 6
        scopes_intrins = self.sch_agent[c_l0c].intrin_scopes(total_axis_l0c)
        scope_insn = scopes_intrins[0]
        inner_k_axis = self.sch_agent[c_l0c].get_relate_scope(c_l0c.op.reduce_axis[1], scope_insn)
        inner_k_axis = inner_k_axis[::-1]
        k_axis_values = self._get_k_outer_values(k_len)
        k_axis_values = k_axis_values[::-1]

        for index, axis in enumerate(inner_k_axis):
            if index == 0:
                current_param = self.tiling_work.al0_tiling_ka
                base_condition = axis * current_param
            else:
                base_condition += axis * current_param
            current_param *= k_axis_values[index]
        condition = base_condition >= (k_len // self.tiling_work.al0_tiling_ka * self.tiling_work.al0_tiling_ka)
        if is_tail_core:
            condition = tvm.all(condition, multi_core_axis >= block_dim_k)
        else:
            condition = tvm.all(condition, multi_core_axis < block_dim_k)
        return condition

    def _get_k_outer_values(self, k_len):
        k_axis_values = []
        k_split_info = [
            self.tiling_work.al0_tiling_ka, self.tiling_work.al1_tiling_k // self.block_reduce,
            self.tiling_work.bl1_tiling_k // self.block_reduce
        ]
        if self.status_controller.aub_attach_status == "c_l0c":
            k_split_info.append(self.tiling_work.aub_tiling_k // self.block_reduce)
        if self.status_controller.bub_attach_status == "c_l0c":
            k_split_info.append(self.tiling_work.bub_tiling_k // self.block_reduce)
        k_split_info = list(set(k_split_info))
        k_split_info = sorted(k_split_info)
        k_split_info_new = k_split_info + [k_len]
        before_value = k_split_info[0]
        for i in k_split_info_new[1:]:
            k_axis_values.insert(0, (i + before_value - 1) // before_value)
            before_value = (i + before_value - 1) // before_value * before_value
        return k_axis_values

    def _fusion_para(self):
        res = self.res
        self.out_addr_type = self._get_addr_type(res)

    def _set_data_layout(self):
        self.container.compute_inline_list = []
        self.container.tensor_map = {}
        # for compute at and db
        self._set_data_layout_base_tensor()
        self._set_data_layout_after_mmad()
        self._set_data_layout_a_matrix()
        self._set_data_layout_b_matrix()
        self._set_data_layout_fusion()
        self._set_data_layout_multi_output()
        self._set_data_layout_bias_table()
        self._set_data_layout_fixpipe()
        self._set_data_layout_mix()
        print_ir_matmul(self.DEBUG_IR, "ir after set data layout", self.sch)

    def _set_static_disable_l2_cache(self):
        if not self.tiling_work.tiling:
            return

        tensor_a = self.tensor_map.get("a_placehold")
        if tensor_a.dtype != "float16":
            return
        tensor_b = self.tensor_map.get("b_placehold")
        tensor_c = self.tensor_map.get("tensor_c_gm")
        tensor_bias = self.tensor_map.get("bias")

        a_l2_enable = False
        b_l2_enable = False
        c_l2_enable = False
        bias_l2_enable = False

        m_dim = self.tiling_work.tiling.get("block_dim")[self.BLOCK_M_DIM_INDEX]
        n_dim = self.tiling_work.tiling.get("block_dim")[self.BLOCK_N_DIM_INDEX]
        k_dim = self.tiling_work.tiling.get("block_dim")[self.IDX_K_DIM]
        al1_attach_flag = self.tiling_work.tiling.get("attach_at_flag").get("al1_attach_flag")
        bl1_attach_flag = self.tiling_work.tiling.get("attach_at_flag").get("bl1_attach_flag")
        a_single_core_load_repeatly = (al1_attach_flag == 2 and bl1_attach_flag != 0)
        b_single_core_load_repeatly = ((bl1_attach_flag == 2 and al1_attach_flag != 0) or
            (bl1_attach_flag == 1 and al1_attach_flag == 1))
        a_core_load_repeatly = (m_dim > 1 and n_dim > 1)
        b_core_load_repeatly = (m_dim > 1 and n_dim > 1)

        if k_dim > 1:
            bias_l2_enable = True
            c_l2_enable = True
        else:
            c_shape = functools.reduce(lambda x, y: x * y, tensor_c.shape)
            dtype_map = {"float16": 2, "float32": 4}
            if tensor_c.dtype in dtype_map:
                c_shape *= dtype_map[tensor_c.dtype]
                if c_shape <= tbe_platform_info.get_soc_spec("L2_SIZE"):
                    c_l2_enable = True
        if a_single_core_load_repeatly or a_core_load_repeatly:
            a_l2_enable = True
        if b_single_core_load_repeatly or b_core_load_repeatly:
            b_l2_enable = True

        '''
        if not a_l2_enable:
            self.sch.bind_args(tensor_a, True, tvm.BindArgPurpose.DisableL2Cache)
        if not b_l2_enable:
            self.sch.bind_args(tensor_b, True, tvm.BindArgPurpose.DisableL2Cache)
        if tensor_bias is not None and not bias_l2_enable:
            self.sch.bind_args(tensor_bias, True, tvm.BindArgPurpose.DisableL2Cache)
        if tensor_c == self.res and tensor_c is not None and not c_l2_enable:
            self.sch.bind_args(tensor_c, True, tvm.BindArgPurpose.DisableL2Cache)
        '''

    def _set_disable_l2_cache(self):
        soc_version = tbe_platform_info.get_soc_spec("SHORT_SOC_VERSION")
        if soc_version not in (tbe_platform_info.ASCEND_310P,
                               tbe_platform_info.ASCEND_910B, tbe_platform_info.ASCEND_910_93):
            return

        if not self.is_dynamic:
            self._set_static_disable_l2_cache()
            return

        if not self.cache_tiling:
            return

        tensor_a = self.tensor_map.get("a_placehold")
        if tensor_a.dtype != "float16":
            return
        tensor_b = self.tensor_map.get("b_placehold")
        tensor_c = self.tensor_map.get("tensor_c_gm")
        tensor_bias = self.tensor_map.get("bias")
        l2_cache_flag = self.cache_tiling.get("l2_cache_flag")

        '''
        l2_cache_flag bit说明
        l2_cache_flag:0 默认读写所有数据,都要经过L2cache
        l2_cache_flag:1 读取A矩阵数据,不需要经过L2cache
        l2_cache_flag:2 读取B矩阵数据,不需要经过L2cache
        l2_cache_flag:3 读取bias矩阵数据,不需要经过L2cache
        l2_cache_flag:4 读取C矩阵数据,不需要经过L2cache
        self.sch.bind_args(tensor_a, tvm.all((l2_cache_flag & 1) == 0,
            (l2_cache_flag & 2) != 0), tvm.BindArgPurpose.DisableL2Cache)
        self.sch.bind_args(tensor_b, tvm.all((l2_cache_flag & 1) == 0,
            (l2_cache_flag & 4) != 0), tvm.BindArgPurpose.DisableL2Cache)
        if tensor_bias is not None:
            self.sch.bind_args(tensor_bias, tvm.all((l2_cache_flag & 1) == 0,
                (l2_cache_flag & 8) != 0), tvm.BindArgPurpose.DisableL2Cache)
        if tensor_c == self.res and tensor_c is not None:
            self.sch.bind_args(tensor_c, tvm.all((l2_cache_flag & 1) == 0,
                (l2_cache_flag & 16) != 0), tvm.BindArgPurpose.DisableL2Cache)
        '''

    def _set_nd_out_compute_inline(self):
        """
        in cache_tiling: cl0->cast_to_fp16->before_cgm(compute_inline)->nz_to_nd->res
        """
        if self.status_controller.support_fix_pipe_l0c2out:
            return
        if (self.format_info.get("out") == "ND" and self.cache_tiling
            and self.container.tensor_map.get("before_c_gm") is not None):
            self._add_tensor_to_list(
                self.container.tensor_map.get("before_c_gm"), [self.container.compute_inline_list])
            self.container.tensors_in_cub.remove(self.container.tensor_map.get("before_c_gm"))

    def _set_data_layout_base_tensor(self):
        self.container.tensor_map["c_gm"] = self.tensor_map.get("tensor_c_gm")
        self.container.tensor_map["a_placehold"] = self.tensor_map.get("a_placehold")
        self.container.tensor_map["b_placehold"] = self.tensor_map.get("b_placehold")
        self._set_and_add_tensor_to_list(self.tensor_map, "L0C", ["c_l0c"], self.container.tensors_in_l0c)
        l0a_scope, l0b_scope = tbe_platform_info.scope_ca, tbe_platform_info.scope_cb
        if self.para_map.get("mmad_mode") == "gemv":
            l0a_scope, l0b_scope = tbe_platform_info.scope_cb, tbe_platform_info.scope_ca
        if self.para_map.get("a_l0a_cache_read", False):
            self.container.tensor_map["a_l0a"] = self.sch.cache_read(
                self.container.tensor_map.get("a_placehold"),
                l0a_scope, [self.container.tensor_map.get("c_l0c")])
        else:
            self.container.tensor_map["a_l0a"] = self.tensor_map["a_l0a"]
            self.sch[self.container.tensor_map.get("a_l0a")].set_scope(l0a_scope)

        if self.para_map.get("b_l0b_cache_read", False):
            self.container.tensor_map["b_l0b"] = self.sch.cache_read(
                self.container.tensor_map.get("b_placehold"),
                l0b_scope, [self.container.tensor_map.get("c_l0c")])
        else:
            self.container.tensor_map["b_l0b"] = self.tensor_map["b_l0b"]
            self.sch[self.container.tensor_map.get("b_l0b")].set_scope(l0b_scope)
        if self.sparse_4to2_flag:
            self.container.tensor_map["compress_index"] = self.container.tensor_map.get("b_l0b").op.input_tensors[1]
            self.container.tensor_map["compress_index_l1"] = self.sch.cache_read(
                self.container.tensor_map.get("compress_index"),
                tbe_platform_info.scope_cbuf, [self.container.tensor_map.get("b_l0b")])


    def _set_tensor_scope(self, tensor, buffer_local):
        if tensor is not None:
            self.sch[tensor].set_scope(buffer_local)

    def _set_data_layout_a_matrix(self):
        data_layout_a_matrix = GemmDataLayoutAMatrix(self.sch, self.container,
                                                     self.status_controller, self.dynamic_para)
        data_layout_a_matrix.set_data_layout()

    def _set_data_layout_b_matrix(self):
        data_layout_b_matrix = GemmDataLayoutBMatrix(self.sch, self.container,
                                                     self.status_controller, self.dynamic_para)
        data_layout_b_matrix.set_data_layout()

    def _set_data_layout_bias_table(self):
        """
        set bias table scope if support
        """
        tensor_bias = self.tensor_map.get("bias")
        self.container.tensor_map["bias"] = tensor_bias
        if not self.status_controller.support_bias_table:
            return
        if tensor_bias is None:
            return
        if tensor_bias.dtype == "float16":
            # bias cast fp16 to fp32 vias l1->bias_table
            bias_bt = self.container.tensor_map.get("c_l0c").op.input_tensors[2]
            self.sch[bias_bt].set_scope(tbe_platform_info.scope_bt)
            self.container.tensor_map["bias_bt"] = bias_bt
            self.container.tensor_map["bias_l1"] = bias_bt.op.input_tensors[0]
            self.sch[self.container.tensor_map.get("bias_l1")].set_scope(tbe_platform_info.scope_cbuf)
        else:
            # input dtype of tensor_bias is the same as c_l0c
            self.container.tensor_map["bias_l1"] = self.container.tensor_map.get("c_l0c").op.input_tensors[2]
            self.sch[self.container.tensor_map.get("bias_l1")].set_scope(tbe_platform_info.scope_cbuf)
            self.container.tensor_map["bias_bt"] = self.sch.cache_read(
                self.container.tensor_map.get("bias_l1"),
                tbe_platform_info.scope_bt,
                [self.container.tensor_map.get("c_l0c")]
            )
            for tensor_bias in self.container.tensor_map.get("bias_l1").op.input_tensors:
                if "bias_zero" in tensor_bias.name:
                    self.container.tensor_map["bias_zero"] = tensor_bias
                    self.sch[self.container.tensor_map.get("bias_zero")].set_scope(tbe_platform_info.scope_cbuf)
                    break

    def _set_data_layout_fixpipe(self):
        """
        set fixpipe tensors scope
        """
        if not self.status_controller.support_fix_pipe:
            return
        if self.tensor_map.get("fixpipe_matmul") is not None:
            self.container.compute_inline_list.append(self.tensor_map.get("fixpipe_matmul"))
        fixpipe_tensor = self.tensor_map.get("fixpipe")
        if fixpipe_tensor is None:
            return

        fixpipe_fb_list = []
        fixpile_l1_list = []
        for idx, params_mem in enumerate(self.tensor_map.get("fixpipe_input_name")):
            fixpipe_input = self.tensor_map.get("fixpipe_input_tensor")[idx]
            fixpipe_scope_name = FIXPIPE_SCOPE_MAP.get(get_value(params_mem))
            if fixpipe_scope_name:
                if self.para_map.get("pre_conv_mode") in self.PER_TENSOR_MODE:
                    pass
                else:
                    fixpipe_input_l1 = self.sch.cache_read(fixpipe_input, tbe_platform_info.scope_cbuf,
                                                           [fixpipe_tensor])
                    fixpipe_fb_list.append(self.sch.cache_read(fixpipe_input_l1, fixpipe_scope_name, [fixpipe_tensor]))
                    fixpile_l1_list.append(fixpipe_input_l1)
            else:
                # if elewise input is 5HD, trans to Nz on L1, else cache_read directly
                if self.tensor_map.get("fixpipe_trans_eltwise") is not None:
                    fixpipe_input_l1 = self.tensor_map.get("fixpipe_trans_eltwise")
                    self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["fixpipe_trans_eltwise"])
                else:
                    fixpipe_input_l1 = self.sch.cache_read(
                        fixpipe_input, tbe_platform_info.scope_cbuf, [fixpipe_tensor]
                        )
                self.container.tensor_map["fixpipe_l1_elewise"] = fixpipe_input_l1
        self.container.tensor_map["fixpipe_fb"] = fixpipe_fb_list
        self.container.tensor_map["fixpipe_l1"] = fixpile_l1_list
        self.container.compute_inline_list.append(fixpipe_tensor)

    def _set_data_layout_mix(self):
        """
        set tensor scope for mix-L2 scene
        """
        if not self.para_map.get("has_ub_fusion") or not self.status_controller.support_fix_pipe_l0c2out:
            return

        tensor_c_gm = self.tensor_map.get("tensor_c_gm")
        if self.status_controller.support_fix_pipe_l0c2ub:
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["tensor_c_gm", "c_ub_fract"],
                                             self.container.tensors_in_cub)
        else:
            self.container.tensor_map["workspace_to_ub"] = self.sch.cache_read(
                tensor_c_gm,
                tbe_platform_info.scope_ubuf,
                self.container.mix_cache_read_list
            )
            c_ub_fract = self.container.tensor_map.get("workspace_to_ub")
            self.container.tensor_map["c_ub_fract"] = c_ub_fract
            align_factor = tbe_platform.CUBE_MKN.get(c_ub_fract.dtype).get("mac")[1]
            self.sch[c_ub_fract].storage_align(c_ub_fract.op.axis[-2], align_factor, 0)
            self.container.mix_workspace_tensor["tensor_c_gm_workspace"] = tensor_c_gm

    def _set_data_layout_after_mmad(self):
        tensors_in_cub = self.container.tensors_in_cub
        tensors_in_l0c = self.container.tensors_in_l0c
        for ub_tensor_name in ("beta_bias", "alpha_c", "c_ub_fract", "cast_to_fp16", "c_add_bias_ub",
                               "bias_cast_to_fp32", "nz_to_nd", "before_c_gm", "cast_to_fp32", "nz_to_nd_fp32",
                               "c_add_bias_ub_fp16", "c_add_bias_ub_fp32", "bias_ub_fp16", "bias_ub_fp32",
                               "bias_ub_drnn_cast_fp16", "bias_ub_drnn_cast_fp32", "fixpipe_matmul_ub"):
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", [ub_tensor_name], tensors_in_cub)
        # bias add in l0c
        if self.tensor_map.get("bias") is not None and not self.status_controller.support_fix_pipe_l0c2out:
            for l0c_tensor_name in ("c_add_bias", "bias_l0c"):
                self._set_and_add_tensor_to_list(self.tensor_map, "L0C", [l0c_tensor_name], tensors_in_l0c)
            for ub_tensor_name in ("bias_ub", "init_value_of_bias_ub", "virtual_add_bias"):
                self._set_and_add_tensor_to_list(self.tensor_map, "UB", [ub_tensor_name], tensors_in_l0c)
            tensor_bias_fp16 = self.container.tensor_map.get("bias_ub_drnn_cast_fp16")
            tensor_bias_fp32 = self.container.tensor_map.get("bias_ub_drnn_cast_fp32")
            if self.tensor_map.get("bias").dtype == "float16":
                self._add_tensor_to_list(tensor_bias_fp16, [self.container.compute_inline_list])
            if self.tensor_map.get("bias").dtype == "float32":
                self._add_tensor_to_list(tensor_bias_fp32, [self.container.compute_inline_list])
        # c add in ub
        if self.tensor_map.get("tensor_c") is not None:
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["bias_ub"], tensors_in_cub)
            if self.tensor_map.get("beta_fp162fp32") is not None:
                self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["beta_fp162fp32"])
                beta_next_tensor = self.container.tensor_map.get("beta_fp162fp32")
            else:
                beta_next_tensor = self.container.tensor_map.get("beta_bias")
            if self.tensor_map.get("alpha_fp162fp32") is not None:
                self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["alpha_fp162fp32"])
                alpha_next_tensor = self.container.tensor_map.get("alpha_fp162fp32")
            else:
                alpha_next_tensor = self.container.tensor_map.get("alpha_c")
            self.container.tensor_map["beta_ub"] = self.sch.cache_read(self.tensor_map.get("beta"),
                                                                       tbe_platform_info.scope_ubuf,
                                                                       [beta_next_tensor])
            self.container.tensor_map["alpha_ub"] = self.sch.cache_read(self.tensor_map.get("alpha"),
                                                                        tbe_platform_info.scope_ubuf,
                                                                        [alpha_next_tensor])

        self.container.tensor_map["tensor_out_fp16"] = self.tensor_map["tensor_out_fp16"]
        self.container.tensor_map["tensor_out_fp32"] = self.tensor_map["tensor_out_fp32"]

        c_ub_fract = self.container.tensor_map.get("c_ub_fract")
        if self.para_map.get("c_ub_fract_inline", False):
            self._add_tensor_to_list(c_ub_fract, [self.container.compute_inline_list])

    def _set_data_layout_multi_output(self):
        """ set tensor_c_gm to ubuf scope and not do compute_inline
        """
        if self.para_map.get("fusion_multi_output_flag", False) and not self.container.mix_cache_read_list:
            tensor_c_gm = self.container.tensor_map.get("c_gm")
            self._add_tensor_to_list(tensor_c_gm, [self.container.tensors_in_cub])
            self.sch[tensor_c_gm].set_scope(tbe_platform_info.scope_ubuf)

    def _set_and_add_tensor(self, tensor, tensors_lists, buffer_type):
        if tensor is not None:
            self.sch[tensor].set_scope(buffer_type)
            for a_list in tensors_lists:
                a_list.append(tensor)

    def _get_quant_fusion_tensor(self):
        matmul_dequant_tensor = self.container.matmul_dequant_tensor
        tensor_fusion_list = self.container.tensor_fusion_list
        if not self.para_map.get("quant_fusion", False):
            return
        for ten_in in self.tensor_list[0].values():
            if ten_in == self.res:
                continue
            if ten_in not in matmul_dequant_tensor and ten_in.op.name in self.emit_fusion_insn_map:
                tensor_fusion_list.append(ten_in)

    def _set_mix_cache_read_list(self, elewise_tensor, elewise_ub):
        if (self.status_controller.support_mix_l2_fusion and
            self.tensor_map.get("tensor_c_gm") in elewise_tensor.op.input_tensors
            and elewise_ub not in self.container.mix_cache_read_list):
            self.container.mix_cache_read_list.append(elewise_ub)

    def _get_elewise_ub_tensors(self, tensor_ele_ub):
        """
        get axpy_ub to axpy_parents[1]_ub dict, in order to set reused_by.
        """
        axpy_and_parent = {}
        for ten_i in self.container.elemwise_tensors:
            if "broadcast" in ten_i.op.tag and (not ten_i.op.attrs or 'broadcast_flag' not in ten_i.op.attrs):
                self.container.compute_inline_list.append(ten_i)
                self._set_mix_cache_read_list(ten_i, ten_i)
            else:
                if ten_i in self.container.elewise_compute_inline_list:
                    continue
                ele_ub = self.sch.cache_write(ten_i, tbe_platform_info.scope_ubuf)
                tensor_ele_ub.append(ele_ub)
                self._set_mix_cache_read_list(ten_i, ele_ub)
                self.container.elewise_compute_inline_list.append(ten_i)
                if "elewise_binary_scalar_axpy" in ten_i.op.tag:
                    ele_ub_input = self.sch.cache_write(ten_i.op.input_tensors[1], tbe_platform_info.scope_ubuf)
                    self.container.elewise_compute_inline_list.append(ten_i.op.input_tensors[1])
                    axpy_and_parent[ele_ub] = ele_ub_input
                    self.container.buffer_reuse_dict[ele_ub_input] = ele_ub

    def _emit_requant_fusion_insn(self):
        tensor_reform = self.container.tensor_map.get("tensor_reform")
        if tensor_reform is None:
            return
        insn = self.requant_fusion_insn_map.get(tensor_reform.op.name)
        # axiss are batch, n1, m1, n0.outer(2), m0, n0.inner(16), the axis n0.outer should be out of emit_insn axis
        self.sch[tensor_reform].emit_insn(tensor_reform.op.axis[-3], insn)
        return

    def _dequant_fusion_proc(self):
        dequant_tensor = self.container.tensor_map.get("dequant_tensor")
        tensor_sqrt = self.container.tensor_map.get("tensor_sqrt")
        self._emit_insn_func(self.container.tensor_map.get("tensor_deq_ub"), 0, "dma_copy")
        self._dequant_activation_emit_insn_simple()
        dequant_emit_axis, deq_scale_mode = (1, "vector") \
            if "vector" in dequant_tensor.op.tag else (0, "scalar")

        if cube_util.is_ng1_version() or cube_util.is_lhisi_cs_version():
            self.sch[dequant_tensor].emit_insn(
                dequant_tensor.op.axis[dequant_emit_axis], "dma_copy")
        else:
            self.sch[dequant_tensor].pragma(
                dequant_tensor.op.axis[dequant_emit_axis], "deq_scale", deq_scale_mode)
            self._emit_insn_func(tensor_sqrt, 0, "vector_auto")

        self._compute_inline_dequant_output()

    def _compute_inline_dequant_output(self):
        """
        compute inline dequant output tensor when dequant is not the last op.
        """
        compute_inline_list = self.container.compute_inline_list
        dequant_nz = self.container.tensor_map.get("dequant_nz")
        dequant_nd = self.container.tensor_map.get("dequant_nd")
        if dequant_nz is not None and self.res != dequant_nz:
            self._add_tensor_to_list(dequant_nz, [compute_inline_list])
        if dequant_nd is not None and self.res != dequant_nd:
            self._add_tensor_to_list(dequant_nd, [compute_inline_list])

    def _quant_fusion_proc(self):
        input_ub = self.container.tensor_map.get("input_ub")
        self._add_tensor_to_list(input_ub, [self.container.compute_inline_list])
        for ten_in in self.container.tensor_fusion_list:
            if ten_in.op.name == "cast_i8_ub":
                round_mode = self.container.tensor_map.get("quant_tensor").op.attrs["round_mode"]
                insn = self._round_emit_insn(round_mode)
            else:
                insn = self.emit_fusion_insn_map.get(ten_in.op.name)
            if ten_in.op.name in self.reform_tensor_tag_list:
                self.sch[ten_in].emit_insn(ten_in.op.axis[2], insn)
            else:
                self.sch[ten_in].emit_insn(ten_in.op.axis[0], insn)

    def _dequant_activation_emit_insn_simple(self):
        if self.para_map.get("dequant_fusion", False):
            for ten_in in self.container.dequant_activation_tensor:
                if ten_in.op.tag.find("|") != -1:
                    str_list = ten_in.op.tag.split("|")
                    insn = self.emit_insn_map.get(str_list[0])
                else:
                    insn = self.emit_insn_map.get(ten_in.op.tag)
                if ten_in in self.container.header_ub_tensors:
                    insn = "dma_copy"
                if insn is None:
                    insn = "vector_auto"
                if "elewise_binary_scalar_axpy" in ten_in.op.tag:
                    self.sch[ten_in].reused_by(ten_in.op.input_tensors[1])
                self.sch[ten_in].emit_insn(ten_in.op.axis[0], insn)

    def _requant_fusion_proc(self):
        requant_tensor = self.container.tensor_map.get("requant_tensor")
        tensor_drq = requant_tensor.op.input_tensors[1]
        tensor_drq_ub = self.sch.cache_read(tensor_drq, tbe_platform_info.scope_ubuf, [requant_tensor])
        self.sch[tensor_drq_ub].emit_insn(tensor_drq_ub.op.axis[0], "dma_copy")

        self._add_tensor_to_list(requant_tensor, [self.container.compute_inline_list])
        self._emit_requant_fusion_insn()

    def _quantify_fusion_entry(self):
        if not self.para_map.get("quantify_fusion", False):
            return
        if self.para_map.get("requant_fusion", False):
            self._requant_fusion_proc()
        if self.para_map.get("dequant_fusion", False):
            self._dequant_fusion_proc()
        if self.para_map.get("quant_fusion", False):
            self._quant_fusion_proc()

        reform_fusion = self.para_map.get("quant_fusion", False) or self.para_map.get("requant_fusion", False)
        if reform_fusion:
            tensor_len_c = len(self.container.tensor_map.get("c_l0c").shape)
            tensor_reform = self.container.tensor_map.get("tensor_reform")
            reform_c_outer, reform_c_inner = self.sch[tensor_reform].split(
                tensor_reform.op.axis[tensor_len_c - 1], factor=16)
            self.sch[tensor_reform].reorder(
                tensor_reform.op.axis[tensor_len_c - 4],
                reform_c_outer, # emit_insn axis
                tensor_reform.op.axis[tensor_len_c - 3],
                tensor_reform.op.axis[tensor_len_c - 2],
                reform_c_inner)
        return

    def _set_scope_fusion(self):
        dequant_tensor_list = []

        if self.out_addr_type == 1:
            self._set_tensor_scope(self.res, tbe_platform_info.scope_cbuf_fusion)
        if self.in_addr_type == 1:
            tensor_a = self.container.tensor_map.get("a_placehold")
            self._set_tensor_scope(tensor_a, tbe_platform_info.scope_cbuf_fusion)

        if self.para_map.get("dequant_fusion", False):
            dequant_tensor_list.append(self.container.tensor_map["dequant_tensor"])
            if self.container.tensor_map.get("tensor_sqrt") is not None:
                tensor_sqrt = self.container.tensor_map.get("tensor_sqrt")
                self.sch[tensor_sqrt].set_scope(tbe_platform_info.scope_ubuf)
                dequant_tensor_list.append(tensor_sqrt)
            for tensor in self.container.dequant_activation_tensor:
                self.sch[tensor].set_scope(tbe_platform_info.scope_ubuf)
            for tensor in self.container.tensor_fusion_list:
                self.sch[tensor].set_scope(tbe_platform_info.scope_ubuf)

        for tensor in self.container.fusion_list:
            if tensor in self.container.double_out_tensor:
                continue
            self.sch[tensor].set_scope(tbe_platform_info.scope_ubuf)

        self._get_tensor_deq(dequant_tensor_list)

    def _get_tensor_deq(self, dequant_tensor_list):
        tensor_deq = self.tensor_list[1].get("tensor_deq")
        if tensor_deq is None:
            tensor_dequant = self.container.tensor_map.get("dequant_tensor")
            if tensor_dequant is not None:
                tensor_deq = tensor_dequant.op.input_tensors[1]
        if tensor_deq is not None:
            self.container.tensor_map["tensor_deq"] = tensor_deq
            self.container.tensor_map["tensor_deq_ub"] = self.sch.cache_read(
                self.container.tensor_map.get("tensor_deq"), tbe_platform_info.scope_ubuf, dequant_tensor_list)

    def _get_quant_fusion_tensor_and_flag(self):
        fusion_tensor_cub = self.container.fusion_tensor_cub
        for tensor_name in ("dequant_tensor", "quant_tensor", "requant_tensor", "dequant_sqrt",
                            "dequant_nz", "dequant_nd", "input_ub", "tensor_reform"):
            if self.tensor_map.get(tensor_name) is not None:
                self.container.tensor_map[tensor_name] = self.tensor_map[tensor_name]
                if tensor_name in ("dequant_tensor", "dequant_nz", "dequant_nd", "dequant_sqrt", "tensor_reform"):
                    fusion_tensor_cub.append(self.tensor_map[tensor_name])

    def _atomic_add_batch(self, res):
        """
        atomic add according to refactor res
        """
        if "reduce_sum" in res.op.tag:
            self.status_controller.reduce_fusion = True
            res = self.root_tensor
            # set all batch to ddr add
            block_dim_batch = get_value(self.container.tensor_map.get("c_l0c").shape)[0]
            batch_outer, _ = self.sch[res].split(res.op.reduce_axis[0], nparts=block_dim_batch)
            res_after = res
            res_ub = self.sch.rfactor(res, batch_outer)
            self.sch[res_ub].set_scope(tbe_platform_info.scope_ubuf)
            # put reduce axis first
            self.sch[res_after].reorder(self.sch[res_after].op.reduce_axis[0], *self.sch[res_after].op.axis)
            self.sch[res_ub].reorder(self.sch[res_ub].op.reduce_axis[0], *self.sch[res_ub].op.axis[1:])
            self.container.tensor_map["res_atomic_add_ub"] = res_ub
            self.container.tensors_in_cub.append(res_ub)
            print_ir_matmul(self.DEBUG_IR, "after atomic_add", self.sch)

    def _atomic_add_ub_process(self, atomic_add_l0c, atomic_add_ddr):
        """
        atomic_add reset ub_tensor
        """
        if not self.status_controller.support_fix_pipe_l0c2out:
            if self.cache_tiling and not self.status_controller.unaligned_flag:
                self.container.tensors_in_cub.remove(self.container.tensor_map.get("cast_to_fp16"))
                self.sch[self.container.tensor_map.get("cast_to_fp16")].set_scope(tbe_platform_info.scope_gm)
                self.container.tensors_in_cub.remove(self.container.tensor_map.get("cast_to_fp32"))
                self.sch[self.container.tensor_map.get("cast_to_fp32")].set_scope(tbe_platform_info.scope_gm)
                self.sch[self.container.tensor_map.get("tensor_out_fp16")].set_scope(tbe_platform_info.scope_gm)
                self.sch[self.container.tensor_map.get("tensor_out_fp32")].set_scope(tbe_platform_info.scope_gm)

            atomic_add_ub = self.sch.cache_read(atomic_add_l0c, tbe_platform_info.scope_ubuf, [atomic_add_ddr])
            self._add_tensor_to_list(atomic_add_ub, [self.container.tensors_in_cub])
            self.container.tensor_map["ori_c_ub_fract"] = self.container.tensor_map.get("c_ub_fract")
            self.container.tensor_map["c_ub_fract"] = atomic_add_ub
            copy_attrs(self.container.tensor_map.get("ori_c_ub_fract"), atomic_add_ub)
            self.container.tensors_in_cub.remove(self.container.tensor_map.get("ori_c_ub_fract"))
            self.sch[self.container.tensor_map.get("ori_c_ub_fract")].set_scope(tbe_platform_info.scope_gm)

    def _atomic_add_k_axis(self):
        if not self.status_controller.split_k_axis_by_tiling:
            return
        tensor_l0c = self.container.tensor_map.get("c_l0c")
        real_k = self.sch[tensor_l0c].op.reduce_axis[0]

        block_dim_k = self.tiling_work.tiling.get("block_dim")[3]
        if self.cache_tiling:
            factor_k = self.cache_tiling.get("kal1_factor") * self.cache_tiling.get("kal1_16")
            tail_strategy_tag = "guard_with_if" if self.status_controller.unaligned_flag else "shift_inwards"
            k_multicore, real_k = self.sch[tensor_l0c].split(real_k, factor=factor_k, tail_strategy=tail_strategy_tag)
        else:
            k_multicore, real_k = self.sch[tensor_l0c].split(real_k, nparts=block_dim_k)
        atomic_add_l0c = self.sch.rfactor(tensor_l0c, k_multicore)
        atomic_add_ddr = tensor_l0c
        self.sch[atomic_add_ddr].reorder(self.sch[atomic_add_ddr].op.reduce_axis[0], *self.sch[atomic_add_ddr].op.axis)
        self.sch[atomic_add_l0c].reorder(atomic_add_l0c.op.reduce_axis[1], atomic_add_l0c.op.reduce_axis[0])
        self.sch[atomic_add_l0c].set_scope(tbe_platform_info.scope_cc)
        self._add_tensor_to_list(atomic_add_l0c, [self.container.tensors_in_l0c])
        self.sch[atomic_add_ddr].set_scope(tbe_platform_info.scope_gm)
        # remove initialize reduce axis with zero
        self.sch[atomic_add_ddr].remove_init()
        self._atomic_add_ub_process(atomic_add_l0c, atomic_add_ddr)
        if self.status_controller.split_k_with_nd_out:
            # atomic_add_l0c need compute_inline before fixpipe_matmul
            self.container.compute_inline_list.insert(0, atomic_add_ddr)
            self._do_compute_inline()
            copy_attrs(atomic_add_ddr, self.container.tensor_map.get("c_gm"))
            self.sch[self.root_tensor].reorder(self.sch[self.root_tensor].op.reduce_axis[0],
                                               *(self.sch[self.root_tensor].op.axis))
        else:
            self.container.tensor_map["ori_c_l0c"] = self.container.tensor_map.get("c_l0c")
            self.container.tensors_in_l0c.remove(atomic_add_ddr)
            self.container.tensor_map["ori_c_gm"] = self.container.tensor_map.get("c_gm")
            self.container.tensor_map["c_gm"] = atomic_add_ddr
            self.root_tensor = atomic_add_ddr
            copy_attrs(self.container.tensor_map.get("ori_c_gm"), atomic_add_ddr)
            self.sch[self.container.tensor_map.get("ori_c_gm")].set_scope(tbe_platform_info.scope_gm)
            self.sch_list.append(atomic_add_ddr)
        self.container.tensor_map["c_l0c"] = atomic_add_l0c
        print_ir_matmul(self.DEBUG_IR, "ir after atomic_add_k_axis", self.sch)

    def _emit_insn_after_split_k(self):
        if self.status_controller.split_k_axis_by_tiling:
            ori_c_gm = self.container.tensor_map.get("ori_c_gm")
            ori_c_ub_fract = self.container.tensor_map.get("ori_c_ub_fract")
            if ori_c_gm is not None:
                self.sch[ori_c_gm].emit_insn(self.sch[ori_c_gm].op.axis[0], "phony_insn")
            if ori_c_ub_fract is not None:
                self.sch[ori_c_ub_fract].emit_insn(self.sch[ori_c_ub_fract].op.axis[0], "phony_insn")
        return True

    def _set_data_layout_quant_fusion(self, fusion_tensor_cub):
        self._get_quant_fusion_tensor_and_flag()
        self.container.matmul_dequant_tensor = self._get_matmul_dequant_tensor()
        debug(self.DEBUG_PARAM, self.container.matmul_dequant_tensor, "matmul_dequant_tensor")

        self._get_quant_fusion_tensor()
        if self.container.tensor_fusion_list is not None:
            fusion_tensor_cub += self.container.tensor_fusion_list
        debug(self.DEBUG_PARAM, self.container.tensor_fusion_list, "tensor_fusion_list")

        self._get_matmul_dequant_activation_tensor()
        self._add_res_ub(self.container.dequant_activation_tensor)
        fusion_tensor_cub += self.container.dequant_activation_tensor
        debug(self.DEBUG_PARAM, self.container.dequant_activation_tensor, "dequant_activation_tensor")

        self.container.header_ub_tensors = self._get_header_tensor_in_dequant_ew_fusion()
        fusion_tensor_cub += self.container.header_ub_tensors
        debug(self.DEBUG_PARAM, self.container.header_ub_tensors, "header_ub_tensors")

    def _set_data_layout_l1_fusion(self):
        tensor_a = self.container.tensor_map.get("a_placehold")
        tensor_b = self.container.tensor_map.get("b_placehold")
        is_fractal_a = len(tensor_a.shape) in (4, 5)
        is_fractal_b = len(tensor_b.shape) in (4, 5)
        self.in_addr_type = self._get_addr_type(tensor_a)
        self.l1_fusion_type = self._get_l1_fusion_type(tensor_a)
        self.status_controller.input_l1_flag, self.input_l1_size = self._get_input_l1_paras(tensor_a)
        self._check_placeholders_shared(tensor_a, tensor_b)

        l1_fusion_and_l1_size_0 = self._get_l1_fusion_and_l1_size_0_flag(self.l1_fusion_type)
        self.status_controller.l1_fusion_and_l1_size_0 = l1_fusion_and_l1_size_0
        tensor_a_l1_workspace = self._get_tensor_a_l1_workspace(l1_fusion_and_l1_size_0)
        self._set_l1_fusion_workspace_tensor(tensor_a, tensor_a_l1_workspace)
        self._set_l1_fusion_workspace_size(tensor_a_l1_workspace)
        self.tensor_a_l1_workspace = tensor_a_l1_workspace

        if self.para_map.get("mmad_mode") == "gemv":
            tensor_a_l1 = self.container.tensor_map.get("b_l1")
            tensor_b_l1 = self.container.tensor_map.get("a_l1")
        else:
            tensor_a_l1 = self.container.tensor_map.get("a_l1")
            tensor_b_l1 = self.container.tensor_map.get("b_l1")

        self._fc_tensor_a_l1_inline(tensor_a_l1, is_fractal_a, l1_fusion_and_l1_size_0)
        self.status_controller.b_l1_inline_flag = self._fc_tensor_b_l1_inline(
            tensor_b_l1, is_fractal_b, l1_fusion_and_l1_size_0)

    def _set_data_layout_fusion(self):
        fusion_tensor_cub = self.container.fusion_tensor_cub
        self._atomic_add_batch(self.res)

        matmul_end_tensor = self.container.tensor_map.get("c_gm")
        self.container.matmul_tensors = self._get_compute_tensor(matmul_end_tensor)
        debug(self.DEBUG_PARAM, self.container.matmul_tensors, "matmul_tensors")

        self._set_data_layout_quant_fusion(fusion_tensor_cub)

        self.container.fusion_ele = self._get_elewise_fusion_tensor()
        debug(self.DEBUG_PARAM, self.container.fusion_ele, "fusion_ele")
        (self.status_controller.gm_ub, self.container.ele_header_ub_tensors) = \
            self._set_scope_buffer_type()
        debug(self.DEBUG_PARAM, self.container.elemwise_tensors, "elemwise_tensors")

        self._get_fusion_tensor()
        fusion_tensor_cub += self.container.fusion_list
        debug(self.DEBUG_PARAM, self.container.fusion_list, "fusion_list")

        self._set_data_layout_l1_fusion()
        self._set_scope_fusion()

        if self.container.fusion_ele:
            res_ub = self.sch.cache_write(self.res, tbe_platform_info.scope_ubuf)
            self.container.elemwise_tensors.append(res_ub)
            self._set_mix_cache_read_list(self.res, res_ub)

        if self.res != self.container.tensor_map.get("c_gm") and not self.para_map.get("multi_out", False):
            if not self.status_controller.support_fix_pipe_l0c2out:
                self.container.compute_inline_list.append(self.container.tensor_map.get("c_gm"))
        # if in quant mode, the tensor of matmul l0c_to_ub tensor need inline
        if self.para_map.get("quantify_fusion", False):
            self.container.compute_inline_list.append(self.container.tensor_map.get("c_ub_fract"))

        if self.tensor_b_reshape:
            self.container.compute_inline_list.append(self.container.tensor_map.get("b_placehold"))
        fusion_tensor_cub += self.container.elemwise_tensors

    def _tensor_a_l1_workspace_emit(self):
        if self.status_controller.input_l1_flag == 1:
            self._emit_insn_func(self.tensor_a_l1_workspace, 0, "dma_copy")

    def _fc_tensor_a_l1_inline(self, tensor_a_l1, is_fractal_a, l1_fusion_and_l1_size_0):
        if (((self.in_addr_type == 1 or self.status_controller.input_l1_flag == 1) and is_fractal_a)
            or l1_fusion_and_l1_size_0):
            self._add_tensor_to_list(tensor_a_l1, [self.container.compute_inline_list])

    def _fc_tensor_b_l1_inline(self, tensor_b_l1, is_fractal_b, l1_fusion_and_l1_size_0):
        inline_flag = False
        if l1_fusion_and_l1_size_0 and is_fractal_b:
            self._add_tensor_to_list(tensor_b_l1, [self.container.compute_inline_list])
            inline_flag = True
        return inline_flag

    def _get_batch_factors(self, tensor_a_l0a, tensor_b_l0b):
        if self.status_controller.have_batch:
            if self.is_dynamic:
                batch = self.dynamic_batch
            else:
                batch = get_value(tensor_a_l0a.shape[0])
                if self.para_map.get("mmad_mode") == "gemv":
                    batch = get_value(tensor_b_l0b.shape[0])
        else:
            batch = 0
        return batch

    def _set_l1_fusion_workspace_tensor(self, tensor_a, tensor_a_l1_workspace):
        if self.status_controller.input_l1_flag == 0:
            L1CommonParam.l1_fusion_tensors_map = {}
            L1CommonParam.l1_fusion_tensors_map[tensor_a] = tvm.var("dummy")
        elif self.status_controller.input_l1_flag == 1:
            L1CommonParam.l1_fusion_tensors_map = {}
            L1CommonParam.l1_fusion_tensors_map[tensor_a] = tensor_a_l1_workspace

    def _set_l1_fusion_workspace_size(self, tensor_a_l1_workspace):
        if self.status_controller.input_l1_flag == 1 and self.input_l1_size > 0:
            self.sch[tensor_a_l1_workspace].set_buffer_size(self.input_l1_size)

    def _get_tensor_a_l1_workspace(self, l1_fusion_and_l1_size_0):
        tensor_a_l1_workspace = None
        tensor_a_ub = self.container.tensor_map.get("a_ub")
        tensor_a_l1 = self.container.tensor_map.get("a_l1")
        tensor_a_l0a = self.container.tensor_map.get("a_l0a")
        tensor_a = self.container.tensor_map.get("a_placehold")
        if self.status_controller.input_l1_flag == 1:
            if tensor_a_ub is not None:
                tensor_a_l1_workspace = self.sch.cache_read(tensor_a, tbe_platform_info.scope_cbuf_fusion, tensor_a_ub)
            elif tensor_a_l1 is not None and not l1_fusion_and_l1_size_0:
                tensor_a_l1_workspace = self.sch.cache_read(tensor_a, tbe_platform_info.scope_cbuf_fusion, tensor_a_l1)
            elif tensor_a_l0a is not None and l1_fusion_and_l1_size_0:
                tensor_a_l1_workspace = self.sch.cache_read(tensor_a, tbe_platform_info.scope_cbuf_fusion, tensor_a_l1)
        return tensor_a_l1_workspace

    def _get_l1_fusion_and_l1_size_0_flag(self, l1_fusion_type):
        trans_b = self.status_controller.transpose_b
        is_l1fusion = l1_fusion_type in (0, 1)
        size = tbe_platform_info.get_soc_spec("L1_SIZE")
        if size == 0 and is_l1fusion:
            if trans_b:
                raise RuntimeError(
                    "If the size of L1 is zero, trans_b is not unexpected.")
            return True
        return False

    def _check_placeholders_shared(self, tensor_a, tensor_b):
        """check placeholders shared"""
        matmul_tensors = self.container.matmul_tensors
        if not self.container.fusion_ele:
            return True
        if tensor_a in self.tensor_list[2]:
            for ten_i in self.tensor_list[2].get(tensor_a):
                if ten_i not in matmul_tensors:
                    raise RuntimeError("matmul placeholders can't be shared "
                                    "with elementwise op")
        if tensor_b in self.tensor_list[2]:
            for ten_i in self.tensor_list[2].get(tensor_b):
                if ten_i not in matmul_tensors:
                    raise RuntimeError("matmul placeholders can't be shared "
                                    "with elementwise op")
        return True

    def _get_fusion_tensor(self):
        matmul_tensors = self.container.matmul_tensors
        fusion_list = self.container.fusion_list
        tensor_c_gm = self.container.tensor_map.get("c_gm")
        if tensor_c_gm != self.res and tensor_c_gm is not None:
            for ten_in in self.tensor_list[0].values():
                if ten_in == self.res and "virtual_res" not in self.root_tensor.op.tag:
                    continue
                if ten_in not in matmul_tensors:
                    fusion_list.append(ten_in)

    def _get_header_tensor_in_dequant_ew_fusion(self):
        """
        add header_ub tensor to dequant_activation_tensor.
        """
        dequant_activation_tensor = self.container.dequant_activation_tensor
        header_ub_tensors = []
        comm_2_elwt = {}
        for ten_i in dequant_activation_tensor:
            common_tensors = set(self.tensor_list[1].values()) & set(ten_i.op.input_tensors)
            for common_tensor in common_tensors:
                if common_tensor in comm_2_elwt:
                    comm_2_elwt[common_tensor].append(ten_i)
                else:
                    comm_2_elwt[common_tensor] = [ten_i]
        for common_tensor, ten_in_list in comm_2_elwt.items():
            common_tensor_ub = self.sch.cache_read(
                common_tensor, tbe_platform_info.scope_ubuf, ten_in_list)
            header_ub_tensors.append(common_tensor_ub)
        dequant_activation_tensor += header_ub_tensors
        return header_ub_tensors

    def _add_res_ub(self, dequant_activation_tensor):
        """
        add res_ub tensor to dequant_activation_tensor.
        """
        for tensor in dequant_activation_tensor:
            if tensor == self.res:
                res_ub = self.sch.cache_write(self.res, tbe_platform_info.scope_ubuf)
                dequant_activation_tensor.remove(tensor)
                dequant_activation_tensor.append(res_ub)

    def _get_matmul_dequant_activation_tensor(self):
        dequant_activation_tensor = self.container.dequant_activation_tensor
        tensor_fusion_list = self.container.tensor_fusion_list
        if not self.para_map.get("dequant_fusion", False):
            return
        dequant_nz = self.container.tensor_map.get("dequant_nz")
        dequant_nd = self.container.tensor_map.get("dequant_nd")
        dequant_tensor = self.container.tensor_map.get("dequant_tensor")
        tensor_sqrt = self.container.tensor_map.get("tensor_sqrt")
        quant_tensor = self.container.tensor_map.get("quant_tensor")
        tensor_front_dequant = self._get_compute_tensor(dequant_tensor)
        for ten_in in self.tensor_list[0].values():
            if ten_in not in tensor_front_dequant:
                dequant_activation_tensor.append(ten_in)
        if tensor_sqrt is not None:
            dequant_activation_tensor.remove(tensor_sqrt)
        for ten_quant in tensor_fusion_list:
            if ten_quant in dequant_activation_tensor:
                dequant_activation_tensor.remove(ten_quant)
        dequant_type_tensor = dequant_nz if dequant_nz is not None else dequant_nd
        if dequant_type_tensor in dequant_activation_tensor:
            dequant_activation_tensor.remove(dequant_type_tensor)
        if self.res in dequant_activation_tensor and quant_tensor is not None:
            dequant_activation_tensor.remove(self.res)

    def _get_elewise_fusion_tensor(self):
        elemwise_tensors = self.container.elemwise_tensors
        if self.para_map.get("quantify_fusion", False):
            return False
        if self.status_controller.reduce_fusion:
            real_res = self.res.op.input_tensors[0]
        else:
            real_res = self.res
        tensor_c_gm = self.container.tensor_map.get("c_gm")
        if tensor_c_gm != real_res and tensor_c_gm is not None:
            for ten_in in self.tensor_list[0].values():
                if ten_in in (self.res, real_res):
                    continue
                if ten_in not in self.container.matmul_tensors and ten_in not in elemwise_tensors:
                    elemwise_tensors.append(ten_in)
            return True
        return False

    def _get_matmul_dequant_tensor(self):
        if self.container.tensor_map.get("quant_tensor") is not None and self.container.tensor_map.get(
                "dequant_nz") is not None:
            compute_tensors = self._get_compute_tensor(self.container.tensor_map.get("dequant_nz"))
            return compute_tensors
        return []

    def _update_flag_after_tiling(self):
        """
        Update variables that depend on tiling.
        over_head_flag: try use nbuffer. Depend on A_overhead_opt_flag and split_k.
        split_k_axis_by_tiling: Enable the bind multi core in k axis. Depend on block_dim_k.
        """
        # split k not support nbuffer now
        self.status_controller.over_head_flag = bool(
            self.tiling_work.tiling.get("A_overhead_opt_flag")) and (not self.status_controller.split_k)
        if self.status_controller.ops_data_flow_mode == "fp322fp32" and self.cache_tiling:
            self.status_controller.split_k_axis_by_tiling = self.cache_tiling_mgr.performance_flag
            self.compute_param.split_k_flag = self.cache_tiling_mgr.performance_flag
        else:
            self.status_controller.split_k_axis_by_tiling = (
                self.tiling_work.tiling.get("block_dim")[self.IDX_K_DIM] != 1)
        self.status_controller.split_k_with_nd_out = (self.status_controller.split_k_axis_by_tiling and
                                                      self.status_controller.support_fix_pipe_l0c2out and
                                                      self.format_info.get("out") == "ND")
        self.status_controller.enable_ub_resuse = (self.cache_tiling and self.format_info.get("a") == "ND")

    def _is_int82fp32_nd(self):
        is_int82fp32_nd = ((self.format_info.get("a") == "ND")
                           and (self.format_info.get("b") == "ND")
                           and (self.status_controller.ops_data_flow_mode == "int82fp32"))
        return is_int82fp32_nd

    def _get_tiling_param(self):
        l0a_shape = [get_value(i) for i in self.container.tensor_map.get("a_l0a").shape]
        l0b_shape = [get_value(i) for i in self.container.tensor_map.get("b_l0b").shape]
        debug(self.DEBUG_PARAM, l0a_shape, "l0a_shape")
        debug(self.DEBUG_PARAM, l0b_shape, "l0b_shape")
        if self.para_map.get("mmad_mode") == "gemv":
            l0a_shape, l0b_shape = l0b_shape, l0a_shape
        a_shape = [
            1,
            l0a_shape[-3],
            l0a_shape[-4],
            self.block_in,
            self.block_reduce
        ]
        if not self.is_dynamic:
            # becasuse A_shape dimension 2 only 16 bits
            while a_shape[2] >= 65536:
                a_shape[2] //= 2
        b_shape = [
            l0b_shape[-4] * self.block_reduce,
            l0b_shape[-3],
            1,
            1,
            self.block_out
        ]

        if len(l0a_shape) == 5:
            a_shape[0] = l0a_shape[0]
        if self.status_controller.ops_data_flow_mode == "int82fp32":
            a_shape[1] = a_shape[1] // 2
            a_shape[1] = a_shape[1] if a_shape[1] != 0 else 1
            a_shape[4] *= 2

        return a_shape, b_shape

    def _cub_process(self):
        if self.container.tensor_map.get("c_ub_fract") is None:
            return
        debug(self.DEBUG_PARAM, "-------debug info in cub_process-------")
        cub_tiling = self.tiling_work.tiling.get("CUB_matrix")
        cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0, cub_tiling_batch, _ = cub_tiling
        if self.res.dtype == "int8":
            cub_tiling_nc_factor = int_ceil_div(cub_tiling_nc_factor, 2)
        if self.format_info.get("out") == "ND":
            affine_cub = [cub_tiling_mc_factor * cub_tiling_m0, cub_tiling_nc_factor * cub_tiling_n0]
        else:
            affine_cub = [cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0]
        if self.status_controller.have_batch:
            affine_cub.insert(0, cub_tiling_batch)
        self._fix_affine_params_for_atomic_k(affine_cub)
        c_ub_fract = self.container.tensor_map.get("c_ub_fract")
        if self.status_controller.attach_at_flag:
            cub_attach_flag = self.status_controller.attach_at_flag.get("cub_attach_flag")
            status = self.status_dict.get(cub_attach_flag)
        debug(self.DEBUG_PARAM, [affine_cub, self.root_tensor.shape], "affine_cub with root_tensor's shape")
        self._do_attach_cub(status, c_ub_fract, affine_cub)
        debug(self.DEBUG_PARAM, "-------debug info in cub_process end-------")

    def _do_attach_cub(self, status, c_ub_fract, affine_cub):
        if self.container.fusion_tensor_cub:
            self.container.tensors_in_cub += self.container.fusion_tensor_cub

        if self.is_dynamic and not self.cache_tiling:
            status = Compare.LESS_EQ

        if status == Compare.EQUAL:
            pass
        elif status == Compare.LESS_EQ:
            self._do_attach_cub_less_eq(c_ub_fract, affine_cub)
        else:
            args_dict = {
                "errCode": "E60114",
                "reason": "c_ub attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )

    def _do_attach_cub_less_eq(self, c_ub_fract, affine_cub):
        """ sub func of _do_attach_cub
        """
        if self.para_map.get("quant_fusion", False) or self.para_map.get("requant_fusion", False):
            affine_cub[-1] *= 2
            debug(self.DEBUG_PARAM, affine_cub, "affine_cub in quant_fusion and requant_fusion")
        self.sch_agent.attach_at(c_ub_fract, self.root_tensor, affine_shape=affine_cub,
                                 factor_shape=self.tiling_work.factor_shape.get("cub"),
                                 ceil_mode_dict=self.tiling_work.get_split_param(self.cache_tiling_mgr))
        self.status_controller.c_ub_attach_status = "c_gm"

        same_attach_cub = self.container.tensors_in_cub
        for tensor in same_attach_cub:
            if tensor in (c_ub_fract, self.root_tensor):
                continue
            self.sch_agent.same_attach(tensor, c_ub_fract)
        if "tensor_virtual_res" in self.root_tensor.name:
            res_fp16, res_fp32 = self.root_tensor.op.input_tensors
            self.sch_agent.same_attach(res_fp16, c_ub_fract)
            self.sch_agent.same_attach(res_fp32, c_ub_fract)
            self.sch[res_fp16].allocate_root()
            self.sch[res_fp32].allocate_root()
        if self.container.double_out_tensor and not self.status_controller.support_mix_l2_fusion:
            self.sch_agent.same_attach(self.container.double_out_tensor[0], c_ub_fract)

    def _get_affine_l0c(self):
        """
        calculate affine l0c shape
        """
        debug(self.DEBUG_PARAM, "-------debug info in _get_affine_l0c-------")
        cl0_tiling_nc, cl0_tiling_mc = self.tiling_work.cl0_tiling_nc, self.tiling_work.cl0_tiling_mc
        cl0_tiling_m0, cl0_tiling_n0 = self.tiling_work.cl0_tiling_m0, self.tiling_work.cl0_tiling_n0
        if self.format_info.get("out") == "ND":
            affine_l0c = [cl0_tiling_mc * cl0_tiling_m0, cl0_tiling_nc * cl0_tiling_n0]
        else:
            affine_l0c = [cl0_tiling_nc, cl0_tiling_mc, cl0_tiling_m0, cl0_tiling_n0]
        if self.status_controller.have_batch:
            affine_l0c.insert(0, self.tiling_work.cl0_tiling_batch)
        self._fix_affine_params_for_atomic_k(affine_l0c)
        if (self.para_map.get("quant_fusion", False) or
            self.para_map.get("requant_fusion", False) or
            self.para_map.get("fixpipe_quant", False)) and self.format_info.get("out") != "ND":
            affine_l0c[-1] *= self.MULTI_FACTOR
            affine_l0c[-4] //= self.MULTI_FACTOR
            affine_l0c[-4] = max(affine_l0c[-4], 1)
        if self.status_controller.ops_data_flow_mode == "fp322fp32" and self.format_info.get("out") != "ND":
            affine_l0c[-1] //= self.MULTI_FACTOR
            affine_l0c[-4] *= self.MULTI_FACTOR

        return affine_l0c

    def _cl0_process(self):
        debug(self.DEBUG_PARAM, "-------debug info in cl0_process-------")
        affine_l0c = self._get_affine_l0c()
        c_l0c = self.container.tensor_map.get("c_l0c")
        if self.container.tensor_map.get("c_ub_fract") is None:
            self.status_controller.c_l0c_attach_status = "c_gm"
            self.sch_agent.update_ignore_init(self.status_controller.split_k_with_nd_out)
            if self.status_controller.support_fix_pipe_l0c2out:
                self.sch_agent.attach_at(c_l0c, self.root_tensor, affine_shape=affine_l0c,
                                         ceil_mode_dict=self.tiling_work.get_split_param(self.cache_tiling_mgr,
                                                                                         tail_strategy="shift_inwards"))
            else:
                self.sch_agent.attach_at(c_l0c, self.root_tensor, affine_shape=affine_l0c)
            self.sch_agent.update_ignore_init(False)
            return
        if self.status_controller.attach_at_flag:
            cl0_attach_flag = self.status_controller.attach_at_flag.get("cl0_attach_flag")
            status_ori = self.status_ori_dict.get(cl0_attach_flag)
            status = self.status_dict.get(cl0_attach_flag)
        debug(self.DEBUG_PARAM, [affine_l0c, self.root_tensor.shape], "affine_l0c with root_tensor.shape")
        self._do_attach_cl0(status_ori, status, c_l0c, affine_l0c)
        debug(self.DEBUG_PARAM, "-------debug info in cl0_process end -------")

    def _do_attach_cl0(self, status_ori, status, c_l0c, affine_l0c):
        self.status_controller.c_l0c_attach_status = "full_load"

        if self.is_dynamic and not self.cache_tiling:
            status_ori = Compare.LESS_EQ

        if status_ori == Compare.MISC:
            args_dict = {
                "errCode": "E60114",
                "reason": "cl0 attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )
        elif status_ori == Compare.EQUAL:
            pass
        elif status == Compare.EQUAL:
            self.status_controller.c_l0c_attach_status = self.status_controller.c_ub_attach_status
            c_ub_fract = self.container.tensor_map.get("c_ub_fract")
            self.sch_agent.same_attach(c_l0c, c_ub_fract)
        elif status == Compare.GREATE_EQ:
            self.sch_agent.attach_at(c_l0c, self.root_tensor, affine_shape=affine_l0c,
                                     factor_shape=self.tiling_work.factor_shape.get("cl0"),
                                     ceil_mode_dict=self.tiling_work.get_split_param(self.cache_tiling_mgr))
            self.status_controller.c_l0c_attach_status = "c_gm"
        else:
            args_dict = {
                "errCode": "E60114",
                "reason": "tensor_c_l0c attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )

        for tensor in self.container.tensors_in_l0c:
            if tensor == c_l0c:
                continue
            self.sch_agent.same_attach(tensor, c_l0c)

    def _get_affine_m_full_load_dynamic(self):
        # now only support fractal
        tiling_m, _, _, _ = self.container.tensor_map.get("a_l0a").shape
        tiling_m = int_ceil_div(tiling_m, self.tiling_work.tiling.get("block_dim")[2])
        return tiling_m

    def _get_affine_n_full_load_dynamic(self):
        # now only support fractal
        _, tiling_n, _, _ = self.container.tensor_map.get("b_l0b").shape
        tiling_n = int_ceil_div(tiling_n, self.tiling_work.tiling.get("block_dim")[1])
        return tiling_n

    def _fix_affine_params_for_atomic_k(self, affine_param):
        if self.status_controller.split_k_axis_by_tiling:
            affine_param.insert(0, 1)

    def _get_affine_k_full_load_dynamic(self):
        _, tiling_k, _, _ = self.container.tensor_map.get("a_l0a").shape
        return tiling_k

    def _al0_process(self):
        debug(self.DEBUG_PARAM, "-------debug info in al0_process-------")
        if (self.tiling_work.tiling.get("AL0_matrix") == [] and
            not self.is_dynamic and self.status_controller.have_batch_a):
            return
        l0a2l0c_affine_shape = [
            None, self.tiling_work.al0_tiling_ma,
            None, self.tiling_work.cl0_tiling_n0,
            self.tiling_work.al0_tiling_ka,
            self.tiling_work.al0_tiling_k0
        ]
        a_l0a = self.container.tensor_map.get("a_l0a")
        if self.para_map.get("mmad_mode") == "gemv":
            a_l0a = self.container.tensor_map.get("b_l0b")
        else:
            a_l0a = self.container.tensor_map.get("a_l0a")
        if self.format_info.get("out") == "ND":
            l0a2out_affine_shape = [self.tiling_work.al0_tiling_ma * self.tiling_work.al0_tiling_m0, None]
        else:
            l0a2out_affine_shape = [None, self.tiling_work.al0_tiling_ma, self.tiling_work.al0_tiling_m0, None]
        if self.status_controller.have_batch_a:
            l0a2l0c_affine_shape.insert(0, self.tiling_work.al0_tiling_batch)
            l0a2out_affine_shape.insert(0, self.tiling_work.al0_tiling_batch)
        elif self.status_controller.have_batch:
            l0a2l0c_affine_shape.insert(0, None)
            l0a2out_affine_shape.insert(0, None)
        self._fix_affine_params_for_atomic_k(l0a2out_affine_shape)
        self._fix_affine_params_for_atomic_k(l0a2l0c_affine_shape)
        if self.status_controller.attach_at_flag:
            l0a_attach_flag = self.status_controller.attach_at_flag.get("al0_attach_flag")
            status_ori = self.status_ori_dict.get(l0a_attach_flag)
            status = self.status_dict.get(l0a_attach_flag)
        all_status = (status_ori, status)
        self._do_attach_l0a(all_status, a_l0a, l0a2l0c_affine_shape, l0a2out_affine_shape)
        debug(self.DEBUG_PARAM, "-------debug info in al0_process end-------")

    def _do_attach_l0a(self, all_status, a_l0a, l0a2l0c_affine_shape, l0a2out_affine_shape):
        status_ori, status = all_status
        if self.is_dynamic and not self.cache_tiling:
            status_ori = Compare.LESS_EQ
            status = Compare.LESS_EQ

        if status_ori == Compare.MISC:
            args_dict = {
                "errCode": "E60114",
                "reason": "a_l0a attach error.",
                "value": "compare status = {}".format(status_ori)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )
        elif status_ori == Compare.EQUAL:
            pass
        elif status == Compare.EQUAL:
            self.sch_agent.same_attach(a_l0a, self.container.tensor_map.get("c_l0c"))
            self.status_controller.al0_attach_status = self.status_controller.c_l0c_attach_status
        elif status == Compare.LESS_EQ:
            self.sch_agent.attach_at(
                a_l0a, self.container.tensor_map.get("c_l0c"), affine_shape=l0a2l0c_affine_shape,
                factor_shape=self.tiling_work.factor_shape.get("al0"),
                ceil_mode_dict=self.tiling_work.get_split_param(
                    self.cache_tiling_mgr, tail_strategy=self.tiling_work.sc_non_factor_tail_strategy)
            )
            self.status_controller.al0_attach_status = "c_l0c"
        elif status == Compare.GREATE_EQ:
            self.sch_agent.attach_at(a_l0a, self.root_tensor, affine_shape=l0a2out_affine_shape,
                                     ceil_mode_dict=self.tiling_work.get_split_param(self.cache_tiling_mgr))
            self.status_controller.al0_attach_status = "c_gm"
        else:
            args_dict = {
                "errCode": "E60114",
                "reason": "l0a attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )

    def _bl0_process(self):
        debug(self.DEBUG_PARAM, "-------debug info in bl0_process-------")
        l0b2l0c_affine_shape = [
            self.tiling_work.bl0_tiling_nb, None, None, self.tiling_work.bl0_tiling_n0,
            self.tiling_work.bl0_tiling_kb, self.tiling_work.bl0_tiling_k0
        ]
        if self.format_info.get("out") == "ND":
            l0b2out_affine_shape = [None, self.tiling_work.bl0_tiling_nb * self.tiling_work.bl0_tiling_n0]
        else:
            l0b2out_affine_shape = [self.tiling_work.bl0_tiling_nb, None, None, self.tiling_work.bl0_tiling_n0]
        if self.para_map.get("mmad_mode") == "gemv":
            b_l0b = self.container.tensor_map.get("a_l0a")
        else:
            b_l0b = self.container.tensor_map.get("b_l0b")
        if self.status_controller.have_batch_b:
            l0b2l0c_affine_shape.insert(0, self.tiling_work.bl0_tiling_batch)
            l0b2out_affine_shape.insert(0, self.tiling_work.bl0_tiling_batch)
        elif self.status_controller.have_batch:
            l0b2l0c_affine_shape.insert(0, None)
            l0b2out_affine_shape.insert(0, None)
        self._fix_affine_params_for_atomic_k(l0b2out_affine_shape)
        self._fix_affine_params_for_atomic_k(l0b2l0c_affine_shape)
        if self.status_controller.attach_at_flag:
            l0b_attach_flag = self.status_controller.attach_at_flag.get("bl0_attach_flag")
            status_ori = self.status_ori_dict.get(l0b_attach_flag)
            status = self.status_dict.get(l0b_attach_flag)
        self._do_attach_bl0([status_ori, status], b_l0b, [l0b2l0c_affine_shape, l0b2out_affine_shape])
        debug(self.DEBUG_PARAM, "-------debug info in bl0_process end-------")

    def _do_attach_bl0(self, affine_status, b_l0b, affine_shapes):
        status_ori, status = affine_status
        l0b2l0c_affine_shape, l0b2out_affine_shape = affine_shapes
        if self.is_dynamic and not self.cache_tiling:
            status_ori = Compare.LESS_EQ
            status = Compare.LESS_EQ

        if status_ori == Compare.MISC:
            args_dict = {
                "errCode": "E60114",
                "reason": "b_l0b attach error.",
                "value": "compare status = {}".format(status_ori)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )
        if status_ori == Compare.EQUAL and (not self.bind_core_when_full_load_bl1):
            pass
        elif status == Compare.EQUAL:
            self.sch_agent.same_attach(b_l0b, self.container.tensor_map.get("c_l0c"))
        elif status == Compare.LESS_EQ:
            self.sch_agent.attach_at(b_l0b, self.container.tensor_map.get("c_l0c"), affine_shape=l0b2l0c_affine_shape,
                                     factor_shape=self.tiling_work.factor_shape.get("bl0"),
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr,
                                        tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
        elif status == Compare.GREATE_EQ:
            l0b2out_affine_shape = self._fix_affine_out_int8(b_l0b.dtype, l0b2out_affine_shape)
            self.sch_agent.attach_at(b_l0b, self.root_tensor, affine_shape=l0b2out_affine_shape,
                                     ceil_mode_dict=self.tiling_work.get_split_param(self.cache_tiling_mgr))
        else:
            args_dict = {
                "errCode": "E60114",
                "reason": "l0b attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )

    def _al1_process(self):
        al1_m_fix_value = get_al1_m_fix_value(
            get_value(self.container.tensor_map.get("a_l0a").shape[self.FRACTAL_Z_M_INDEX]))
        self.status_controller.al1_attach_status = "full_load"
        not_need_process = (self.tiling_work.tiling.get("AL1_shape") in (None, []) and (not self.is_dynamic)
                            and (al1_m_fix_value == 1))
        not_need_process = not_need_process and (not self.status_controller.have_batch_a)
        if not_need_process:
            return
        debug(self.DEBUG_PARAM, "-------debug info in al1_process-------")
        al1_tiling_m, al1_tiling_k = self.tiling_work.al1_tiling_m, self.tiling_work.al1_tiling_k
        al0_tiling_m0, al0_tiling_k0 = self.tiling_work.al0_tiling_m0, self.tiling_work.al0_tiling_k0
        cl0_tiling_n0 = self.tiling_work.cl0_tiling_n0
        l1_ma = al1_tiling_m
        l1_ka = al1_tiling_k // al0_tiling_k0 if self.cache_tiling else int_ceil_div(al1_tiling_k, al0_tiling_k0)
        a_l1 = self.container.tensor_map.get("a_l1")
        if self.para_map.get("mmad_mode") == "gemv":
            a_l1 = self.container.tensor_map.get("b_l1")
        l1a2l0c_affine_shape = [
            None,
            l1_ma,
            None,
            cl0_tiling_n0,
            l1_ka,
            al0_tiling_k0
        ]
        # add bl1_tiling_n in order to out n_inner axis down
        l1a2out_affine_shape = [self.tiling_work.bl1_tiling_n, l1_ma, al0_tiling_m0, None]
        if self.format_info.get("out") == "ND":
            al1_n_shape = None
            if self.cache_tiling:
                al1_n_shape = self.cache_tiling.get('n_single_core') * self.tiling_work.bl1_tiling_n * cl0_tiling_n0
            l1a2out_affine_shape = [l1_ma * al0_tiling_m0, al1_n_shape]
        if self.status_controller.have_batch_a:
            l1a2l0c_affine_shape.insert(0, self.tiling_work.al1_tiling_batch)
            l1a2out_affine_shape.insert(0, self.tiling_work.al1_tiling_batch)
        elif self.status_controller.have_batch:
            l1a2l0c_affine_shape.insert(0, None)
            l1a2out_affine_shape.insert(0, None)
        self._fix_affine_params_for_atomic_k(l1a2l0c_affine_shape)
        self._fix_affine_params_for_atomic_k(l1a2out_affine_shape)
        if self.status_controller.attach_at_flag:
            al1_attach_flag = self.status_controller.attach_at_flag.get("al1_attach_flag")
            status_ori = self.status_ori_dict.get(al1_attach_flag)
            status = self.status_dict.get(al1_attach_flag)
        self._do_attach_al1(status_ori, status, a_l1, l1a2l0c_affine_shape, l1a2out_affine_shape)
        debug(self.DEBUG_PARAM, "-------debug info in al1_process end-------")

    def _do_attach_al1(self, status_ori, status, a_l1, l1a2l0c_affine_shape, l1a2out_affine_shape):
        if self.is_dynamic and not self.cache_tiling and not self.tiling_work.tiling.get("attach_same_to_static"):
            status_ori = Compare.LESS_EQ
            status = Compare.LESS_EQ

        if status_ori == Compare.MISC:
            args_dict = {
                "errCode": "E60114",
                "reason": "a_l1 attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )
        elif status_ori == Compare.EQUAL:
            pass
        elif status == Compare.EQUAL:
            self.status_controller.al1_attach_status = self.status_controller.c_l0c_attach_status
            # reduce duplicate loading, probably not only in cachetiling scene
            if self.cache_tiling and self.format_info.get("a") == "ND":
                self.sch_agent.attach_at(a_l1, self.root_tensor, affine_shape=l1a2out_affine_shape,
                                         factor_shape=self.tiling_work.factor_shape.get("al12ddr"),
                                         ceil_mode_dict=self.tiling_work.get_split_param(
                                            self.cache_tiling_mgr,
                                            tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
            else:
                self.sch_agent.same_attach(a_l1, self.container.tensor_map.get("c_l0c"))
        elif status == Compare.LESS_EQ:
            self.status_controller.al1_attach_status = "c_l0c"
            self.sch_agent.attach_at(a_l1, self.container.tensor_map.get("c_l0c"), affine_shape=l1a2l0c_affine_shape,
                                     factor_shape=self.tiling_work.factor_shape.get("al12cl0"),
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr,
                                        tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
        elif status == Compare.GREATE_EQ:
            self.status_controller.al1_attach_status = "c_gm"
            l1a2out_affine_shape = self._fix_affine_out_int8(a_l1.dtype, l1a2out_affine_shape)
            self.sch_agent.attach_at(a_l1, self.root_tensor, affine_shape=l1a2out_affine_shape,
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr,
                                        tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
        else:
            args_dict = {
                "errCode": "E60114",
                "reason": "a_l1 attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )

    def _bl1_process(self):
        self.status_controller.bl1_attach_status = "full_load"
        not_need_bl1_process = self.tiling_work.tiling.get("BL1_shape") in (None, []) and (not self.is_dynamic)
        not_need_bl1_process = not_need_bl1_process and (
            not self.status_controller.have_batch_b) and (not self.bind_core_when_full_load_bl1)
        if not_need_bl1_process:
            return
        debug(self.DEBUG_PARAM, "-------debug info in bl1_process-------")
        bl1_tiling_n, bl1_tiling_k = self.tiling_work.bl1_tiling_n, self.tiling_work.bl1_tiling_k
        bl0_tiling_n0, bl0_tiling_k0 = self.tiling_work.bl0_tiling_n0, self.tiling_work.bl0_tiling_k0
        l1_nb = bl1_tiling_n
        l1_kb = bl1_tiling_k // bl0_tiling_k0 if self.cache_tiling else int_ceil_div(bl1_tiling_k, bl0_tiling_k0)
        l1b2l0c_affine_shape = [
            l1_nb, None, None, bl0_tiling_n0, l1_kb, bl0_tiling_k0
        ]
        l1b2out_affine_shape = self._get_l1b2out_affine_shape()
        b_l1 = self.container.tensor_map.get("b_l1")
        if self.para_map.get("mmad_mode") == "gemv":
            b_l1 = self.container.tensor_map.get("a_l1")
        if self.status_controller.have_batch_b:
            l1b2l0c_affine_shape.insert(0, self.tiling_work.bl1_tiling_batch)
            l1b2out_affine_shape.insert(0, self.tiling_work.bl1_tiling_batch)
        elif self.status_controller.have_batch:
            l1b2l0c_affine_shape.insert(0, None)
            l1b2out_affine_shape.insert(0, None)
        self._fix_affine_params_for_atomic_k(l1b2l0c_affine_shape)
        self._fix_affine_params_for_atomic_k(l1b2out_affine_shape)
        if self.status_controller.attach_at_flag:
            bl1_attach_flag = self.status_controller.attach_at_flag.get("bl1_attach_flag")
            status_ori = self.status_ori_dict.get(bl1_attach_flag)
            status = self.status_dict.get(bl1_attach_flag)
        self._do_attach_bl1(status_ori, status, b_l1, l1b2l0c_affine_shape, l1b2out_affine_shape)
        if self.container.tensor_map.get("compress_index_l1") is not None:
            self.sch_agent.same_attach(self.container.tensor_map.get("compress_index_l1"), b_l1)
        debug(self.DEBUG_PARAM, "-------debug info in bl1_process end-------")

    def _get_l1b2out_affine_shape(self):
        """
        get l1b2out_affine_shape
        """
        bl1_m_shape = None
        l1b2out_affine_shape = [bl1_m_shape, self.tiling_work.bl1_tiling_n * self.tiling_work.bl0_tiling_n0]
        if self.format_info.get("out") != "ND":
            l1b2out_affine_shape = [self.tiling_work.bl1_tiling_n, None, None, self.tiling_work.bl0_tiling_n0]
            template_bl1_full_load = (
                self.cache_tiling and self.status_controller.attach_at_flag.get("bl1_attach_flag") == 0)
            if template_bl1_full_load:
                l1b2out_affine_shape[1] = self.tiling_work.al1_tiling_m
        return l1b2out_affine_shape

    def _do_attach_bl1(self, status_ori, status, b_l1, l1b2l0c_affine_shape, l1b2out_affine_shape):
        if self.is_dynamic and not self.cache_tiling and not self.tiling_work.tiling.get("attach_same_to_static"):
            status_ori = Compare.LESS_EQ
            status = Compare.LESS_EQ

        if status_ori == Compare.MISC:
            args_dict = {
                "errCode": "E60114",
                "reason": "b_l1 attach error.",
                "value": "compare status = {}".format(status_ori)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )
        elif status_ori == Compare.EQUAL:
            pass
        elif status == Compare.EQUAL:
            self.status_controller.bl1_attach_status = self.status_controller.c_l0c_attach_status
            if self.cache_tiling:
                self.sch_agent.attach_at(b_l1, self.root_tensor, affine_shape=l1b2out_affine_shape,
                                         factor_shape=self.tiling_work.factor_shape.get("bl12ddr"),
                                         ceil_mode_dict=self.tiling_work.get_split_param(
                                            self.cache_tiling_mgr,
                                            tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
            else:
                self.status_controller.bl1_attach_status = self.status_controller.c_l0c_attach_status
                self.sch_agent.same_attach(b_l1, self.container.tensor_map.get("c_l0c"))
        elif status == Compare.LESS_EQ:
            self.status_controller.bl1_attach_status = "c_l0c"
            self.sch_agent.attach_at(b_l1, self.container.tensor_map.get("c_l0c"), affine_shape=l1b2l0c_affine_shape,
                                     factor_shape = self.tiling_work.factor_shape.get("bl12cl0"),
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr,
                                        tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
        elif status == Compare.GREATE_EQ:
            self.status_controller.bl1_attach_status = "c_gm"
            l1b2out_affine_shape = self._fix_affine_out_int8(b_l1.dtype, l1b2out_affine_shape)
            self.sch_agent.attach_at(b_l1, self.root_tensor, affine_shape=l1b2out_affine_shape,
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr,
                                        tail_strategy=self.tiling_work.sc_non_factor_tail_strategy))
        else:
            args_dict = {
                "errCode": "E60114",
                "reason": "b_l1 attach error.",
                "value": "compare status = {}".format(status)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )

    def _fix_affine_out_int8(self, tensor_dtype, affine_shape):
        """
        if input and output both int8, tiling_n only half
        """
        if self.res.dtype == "int8" and tensor_dtype in ("int8", "uint8"):
            n_dim_index = -1 if self.format_info.get("out") == "ND" else -4
            n_factor = 0 if not affine_shape[n_dim_index] else affine_shape[n_dim_index] // 2
            affine_shape[n_dim_index] = 1 if n_factor == 0 else n_factor
        return affine_shape

    def _aub_process(self):
        debug(self.DEBUG_PARAM, "-------debug info in aub_process-------")
        a_ub = self.container.tensor_map.get("a_ub")
        if a_ub in (None, []):
            return
        transpose_a = self.status_controller.transpose_a
        aub_tiling_k, aub_tiling_m = self.tiling_work.aub_tiling_k, self.tiling_work.aub_tiling_m
        aub_tiling_k0 = self.block_reduce
        aub_tiling_m0 = 1 if self.para_map.get("mmad_mode") == "gevm" else self.block_in
        cl0_tiling_n0 = self.tiling_work.cl0_tiling_n0
        ub_ka = (aub_tiling_k + aub_tiling_k0 - 1) // aub_tiling_k0
        aub_l1_affine_shape = [aub_tiling_m, ub_ka, aub_tiling_m0, aub_tiling_k0]
        unaligned_flag = self.status_controller.unaligned_flag
        # in nd2Zz_vnchwconv scene L1 tensor is 3d
        if self.get_a_matrix_mode == "nd2Zz_vnchwconv":
            if transpose_a:
                aub_l1_affine_shape = [ub_ka, aub_tiling_m, aub_tiling_m0, aub_tiling_k0]
                if not unaligned_flag:
                    aub_l1_affine_shape = [ub_ka, aub_tiling_m * aub_tiling_m0, aub_tiling_k0]
            else:
                aub_l1_affine_shape = [aub_tiling_m, ub_ka, aub_tiling_k0, aub_tiling_m0]
                if not unaligned_flag:
                    aub_l1_affine_shape = [aub_tiling_m, ub_ka * aub_tiling_k0, aub_tiling_m0]
        aub_out_affine_shape = [None, aub_tiling_m, aub_tiling_m0, None]
        if self.format_info.get("out") == "ND":
            aub_out_affine_shape = [aub_tiling_m * aub_tiling_m0, None]
        aub_l0c_affine_shape = [
            None,
            aub_tiling_m,
            None,
            cl0_tiling_n0,
            (aub_tiling_k + aub_tiling_k0 - 1) // aub_tiling_k0,
            aub_tiling_k0
        ]
        if self.status_controller.have_batch_a:
            aub_l1_affine_shape.insert(0, self.tiling_work.aub_tiling_batch)
            aub_out_affine_shape.insert(0, self.tiling_work.aub_tiling_batch)
            aub_l0c_affine_shape.insert(0, self.tiling_work.aub_tiling_batch)
        elif self.status_controller.have_batch:
            aub_out_affine_shape.insert(0, None)
            aub_l0c_affine_shape.insert(0, None)
        self._fix_affine_params_for_atomic_k(aub_l0c_affine_shape)
        self._fix_affine_params_for_atomic_k(aub_out_affine_shape)
        if self.status_controller.attach_at_flag:
            aub_attach_flag = self.status_controller.attach_at_flag.get("aub_attach_flag")
            if aub_attach_flag not in self.status_abub_dict:
                status_ori = self.status_ori_dict.get(aub_attach_flag)
                status_l1 = self.status_dict.get(aub_attach_flag)
                status_l0c = Compare.GREATE_EQ
            else:
                status_ori = Compare.LESS_EQ
                status_l1 = Compare.GREATE_EQ
                status_l0c = self.status_abub_dict.get(aub_attach_flag)
        self._do_attach_aub(status_ori, status_l1, status_l0c, a_ub, aub_l1_affine_shape,
                            aub_l0c_affine_shape, aub_out_affine_shape)
        debug(self.DEBUG_PARAM, "-------debug info in aub_process end-------")

    def _do_attach_aub(self, status_ori, status_l1, status_l0c, a_ub, aub_l1_affine_shape,
                       aub_l0c_affine_shape, aub_out_affine_shape):
        if self.is_dynamic and status_ori == Compare.EQUAL:
            status_ori = Compare.LESS_EQ
            self.status_controller.aub_attach_status = "c_gm"
        if status_ori == Compare.EQUAL:
            pass
        elif status_l1 == Compare.EQUAL:
            self.status_controller.aub_attach_status = self.status_controller.al1_attach_status
            self.sch_agent.same_attach(a_ub, self.container.tensor_map.get("a_l1"))
        elif status_l1 == Compare.LESS_EQ:
            self.status_controller.aub_attach_status = "a_l1"
            self.sch_agent.attach_at(a_ub, self.container.tensor_map.get("a_l1"), aub_l1_affine_shape,
                                     factor_shape=self.tiling_work.factor_shape.get("aub"),
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr, tail_strategy="shift_inwards"))
        else:
            if status_l0c == Compare.EQUAL:
                self.status_controller.aub_attach_status = "c_gm"
                self.sch_agent.same_attach(a_ub, self.container.tensor_map.get("c_l0c"))
            elif status_l0c == Compare.LESS_EQ:
                self.status_controller.aub_attach_status = "c_l0c"
                self.sch_agent.attach_at(a_ub, self.container.tensor_map.get("c_l0c"),
                                         affine_shape=aub_l0c_affine_shape,
                                         ceil_mode_dict=self.tiling_work.get_split_param(
                                            self.cache_tiling_mgr, tail_strategy="shift_inwards"))
            else:
                self.status_controller.aub_attach_status = "c_gm"
                self.sch_agent.attach_at(a_ub, self.root_tensor, affine_shape=aub_out_affine_shape,
                                         ceil_mode_dict=self.tiling_work.get_split_param(
                                            self.cache_tiling_mgr, tail_strategy="shift_inwards"))
        same_attach_tensors = self.container.tensors_in_aub
        for tensor in same_attach_tensors:
            if tensor == a_ub:
                continue
            self.sch_agent.same_attach(tensor, a_ub)

    def _bub_process(self):
        b_ub = self.container.tensor_map.get("b_ub")
        if b_ub in (None, []):
            return
        debug(self.DEBUG_PARAM, "-------debug info in bub_process-------")
        bub_tiling_k0, bub_tiling_n0 = self.block_reduce, self.block_out
        ub_kb = (self.tiling_work.bub_tiling_k + bub_tiling_k0 - 1) // bub_tiling_k0
        bub_l1_affine_shape = [ub_kb, self.tiling_work.bub_tiling_n, bub_tiling_n0, bub_tiling_k0]
        unaligned_flag = self.status_controller.unaligned_flag
        if self.get_b_matrix_mode == "nd2Zn_vnchwconv":
            if self.status_controller.transpose_b:
                bub_l1_affine_shape = [self.tiling_work.bub_tiling_n, ub_kb, bub_tiling_k0, bub_tiling_n0]
                if not unaligned_flag:
                    bub_l1_affine_shape = [self.tiling_work.bub_tiling_n, self.tiling_work.bub_tiling_k, bub_tiling_n0]
            else:
                bub_l1_affine_shape = [ub_kb, self.tiling_work.bub_tiling_n, bub_tiling_n0, bub_tiling_k0]
                if not unaligned_flag:
                    bub_l1_affine_shape = [ub_kb, self.tiling_work.bub_tiling_n * bub_tiling_n0, bub_tiling_k0]
        elif self.get_b_matrix_mode == "Nz2Zn":
            bub_l1_affine_shape = [self.tiling_work.bub_tiling_n, ub_kb, bub_tiling_k0, bub_tiling_n0]
        if self.format_info.get("out") == "ND":
            bub_out_affine_shape = [None, self.tiling_work.bub_tiling_n * bub_tiling_n0]
        else:
            bub_out_affine_shape = [self.tiling_work.bub_tiling_n, None, None, bub_tiling_n0]
        bub_l0c_affine_shape = [
            self.tiling_work.bub_tiling_n,
            None,
            None,
            self.tiling_work.bl0_tiling_n0,
            (self.tiling_work.bub_tiling_k + bub_tiling_k0 - 1) // bub_tiling_k0,
            bub_tiling_k0
        ]
        if self.status_controller.have_batch_b:
            bub_l1_affine_shape.insert(0, self.tiling_work.bub_tiling_batch)
            bub_out_affine_shape.insert(0, self.tiling_work.bub_tiling_batch)
            bub_l0c_affine_shape.insert(0, self.tiling_work.bub_tiling_batch)
        elif self.status_controller.have_batch:
            bub_out_affine_shape.insert(0, None)
            bub_l0c_affine_shape.insert(0, None)
        self._fix_affine_params_for_atomic_k(bub_l0c_affine_shape)
        self._fix_affine_params_for_atomic_k(bub_out_affine_shape)
        if self.status_controller.attach_at_flag:
            bub_attach_flag = self.status_controller.attach_at_flag.get("bub_attach_flag")
            if bub_attach_flag not in self.status_abub_dict:
                status_ori = self.status_ori_dict.get(bub_attach_flag)
                status_l1 = self.status_dict.get(bub_attach_flag)
                status_l0c = Compare.GREATE_EQ
            else:
                status_ori = Compare.LESS_EQ
                status_l1 = Compare.GREATE_EQ
                status_l0c = self.status_abub_dict.get(bub_attach_flag)
        self._do_attach_bub(status_ori, status_l1, status_l0c, b_ub, bub_l1_affine_shape,
                            bub_l0c_affine_shape, bub_out_affine_shape)
        debug(self.DEBUG_PARAM, "-------debug info in bub_process end-------")

    def _do_attach_bub(self, status_ori, status_l1, status_l0c, b_ub, bub_l1_affine_shape,
                       bub_l0c_affine_shape, bub_out_affine_shape):
        if self.is_dynamic and status_ori == Compare.EQUAL:
            self.status_controller.bub_attach_status = "c_gm"
            status_ori = Compare.LESS_EQ
        if status_ori == Compare.MISC:
            args_dict = {
                "errCode": "E60114",
                "reason": "bub attach error.",
                "value": "compare status = {}".format(status_ori)
            }
            raise RuntimeError(
                args_dict, error_manager_util.get_error_message(args_dict)
            )
        elif status_ori == Compare.EQUAL:
            pass
        elif status_l1 == Compare.EQUAL:
            self.status_controller.bub_attach_status = self.status_controller.bl1_attach_status
            self.sch_agent.same_attach(b_ub, self.container.tensor_map.get("b_l1"))
        elif status_l1 == Compare.LESS_EQ:
            self.status_controller.bub_attach_status = "b_l1"
            self.sch_agent.attach_at(b_ub, self.container.tensor_map.get("b_l1"), bub_l1_affine_shape,
                                     factor_shape=self.tiling_work.factor_shape.get("bub"),
                                     ceil_mode_dict=self.tiling_work.get_split_param(
                                        self.cache_tiling_mgr, tail_strategy="shift_inwards"))
        else:
            if status_l0c == Compare.EQUAL:
                self.status_controller.bub_attach_status = "c_gm"
                self.sch_agent.same_attach(b_ub, self.container.tensor_map.get("c_l0c"))
            elif status_l0c == Compare.LESS_EQ:
                self.status_controller.bub_attach_status = "c_l0c"
                self.sch_agent.attach_at(b_ub, self.container.tensor_map.get("c_l0c"),
                                         affine_shape=bub_l0c_affine_shape,
                                         ceil_mode_dict=self.tiling_work.get_split_param(
                                            self.cache_tiling_mgr, tail_strategy="shift_inwards"))
            else:
                self.status_controller.bub_attach_status = "c_gm"
                bub_out_affine_shape = self._fix_affine_out_int8(b_ub.dtype, bub_out_affine_shape)
                self.sch_agent.attach_at(b_ub, self.root_tensor, affine_shape=bub_out_affine_shape,
                                         ceil_mode_dict=self.tiling_work.get_split_param(
                                            self.cache_tiling_mgr, tail_strategy="shift_inwards"))
        for tensor in self.container.tensors_in_bub:
            if tensor == b_ub:
                continue
            self.sch_agent.same_attach(tensor, b_ub)

    def _get_cache_tiling_axis_order(self):
        """
        get axis_order in cache_tiling scene
        """
        abkl1_attach_flag = self.status_controller.attach_at_flag.get("abkl1_attach_flag")
        if abkl1_attach_flag == self.KBL1_LARGER_FLAG:
            axis_order = ["bl1", "al1"]
            axis_ub_order = ["bub", "aub"]
        else:
            axis_order = ["al1", "bl1"]
            axis_ub_order = ["aub", "bub"]
        if self.format_info.get("a") == "ND":
            axis_order = axis_order + axis_ub_order
        return axis_order

    def _do_l1_ub_process(self):
        """
        do l1 ub process by correct order
        """
        if self.cache_tiling and self.status_controller.attach_at_flag:
            axis_order = self._get_cache_tiling_axis_order()
        else:
            # get order
            order_dict = {
                "aub": [self.tiling_work.aub_tiling_k // self.block_reduce, self.tiling_work.aub_tiling_batch],
                "bub": [self.tiling_work.bub_tiling_k // self.block_reduce, self.tiling_work.bub_tiling_batch],
                "al1": [
                    self.tiling_work.al1_tiling_k // int(self.tiling_work.al0_tiling_k0),
                    self.tiling_work.al1_tiling_batch
                ],
                "bl1": [
                    self.tiling_work.bl1_tiling_k // int(self.tiling_work.bl0_tiling_k0),
                    self.tiling_work.bl1_tiling_batch
                ],
            }
            tmp_order = sorted(order_dict.items(), key=lambda d: [d[1][0], d[1][1]], reverse=True)
            axis_order = [i[0] for i in tmp_order]

            def _adjust_order(axis_order, ub_tag, l1_tag):
                if (axis_order.index(ub_tag) > axis_order.index(l1_tag) and
                    order_dict.get(ub_tag) == order_dict.get(l1_tag)):
                    index_ub = axis_order.index(ub_tag)
                    index_l1 = axis_order.index(l1_tag)
                    axis_order[index_ub] = l1_tag
                    axis_order[index_l1] = ub_tag

            _adjust_order(axis_order, "aub", "al1")
            _adjust_order(axis_order, "bub", "bl1")

        al1_attach_flag = self.status_controller.attach_at_flag.get("al1_attach_flag")
        bl1_attach_flag = self.status_controller.attach_at_flag.get("bl1_attach_flag")
        non_factor_invalid_flag = not self.status_controller.split_k_axis_by_tiling
        # 2 means no full load
        no_full_load_flag = al1_attach_flag == 2 and bl1_attach_flag == 2
        if self.cache_tiling and no_full_load_flag and non_factor_invalid_flag and \
            not self.status_controller.unaligned_flag:
            axis_order = axis_order[2:]
        for tag in axis_order[::-1]:
            if tag == "bl1":
                self._bl1_process()
            elif tag == "al1":
                self._al1_process()
            elif tag == "bub":
                self._bub_process()
            else:
                self._aub_process()

    def _do_bias_table_process(self):
        """
        bias_l1 split same as n_bl1
        bias_bt split same as n_l0c
        """
        if self.container.tensor_map.get("bias_l1") is None:
            return
        b_l1 = self.container.tensor_map.get("b_l1")
        bias_l1 = self.container.tensor_map.get("bias_l1")
        cl0_c = self.container.tensor_map.get("c_l0c")
        bl1_attach_flag = self.status_controller.attach_at_flag.get("bl1_attach_flag")
        self.sch_agent.pre_apply()
        attach_dict = self.sch_agent.get_attach_dict()
        if attach_dict.get(self.sch[b_l1]) == self.sch[self.root_tensor]:
            self.sch_agent.same_attach(bias_l1, b_l1)
        elif attach_dict.get(self.sch[b_l1]) == self.sch[cl0_c] or bl1_attach_flag != 0:
            self.sch_agent.same_attach(bias_l1, cl0_c)
        if self.container.tensor_map.get("bias_zero") is not None:
            self.sch_agent.same_attach(self.container.tensor_map.get("bias_zero"), bias_l1)
        self.sch_agent.same_attach(self.container.tensor_map.get("bias_bt"), self.container.tensor_map.get("c_l0c"))

    def _do_mix_pad_attach(self, tensor_workspace, tensor_ub, tiling, ub_dim, tensor_name=None):
        if tensor_workspace is None:
            return None

        tiling_k_ub, tiling_mn_ub = tiling[:2]
        if ((self.status_controller.transpose_a and self.status_controller.pad_flag in [self.PAD_A, self.PAD_AB]
             and tensor_name == "a")
                or (not self.status_controller.transpose_b
                    and self.status_controller.pad_flag in [self.PAD_B, self.PAD_AB] and tensor_name == "b")):
            tiling_k_ub, tiling_mn_ub = tiling_mn_ub, tiling_k_ub
        k_outer, k_inner = self.sch[tensor_workspace].split(tensor_workspace.op.axis[-1], tiling_k_ub)
        mn_outer, mn_inner = self.sch[tensor_workspace].split(tensor_workspace.op.axis[-2], tiling_mn_ub)
        self.sch[tensor_workspace].reorder(mn_outer, k_outer, mn_inner, k_inner)
        dim_outer, single_core = self.sch[tensor_workspace].split(mn_outer, nparts=ub_dim)
        self.sch[tensor_workspace].bind(dim_outer, tvm.thread_axis("blockIdx.x"))
        self.sch[tensor_ub].compute_at(self.sch[tensor_workspace], k_outer)
        sub_core, single_core = self.sch[tensor_workspace].split(single_core, nparts=2)
        single_core_outer, _ = self.sch[tensor_workspace].split(single_core, nparts=1)
        self.sch[tensor_workspace].bind(sub_core, tvm.thread_axis("subBlockIdx.x"))
        self.sch[tensor_ub].compute_at(self.sch[tensor_workspace], k_outer)
        return single_core_outer

    def _do_mix_pad_process(self):
        tensor_a_workspace = self.container.mix_workspace_tensor.get("tensor_a_workspace")
        tensor_a_ub = self.container.tensor_map.get("a_ub_pad")
        tensor_b_workspace = self.container.mix_workspace_tensor.get("tensor_b_workspace")
        tensor_b_ub = self.container.tensor_map.get("b_ub_pad")
        aub_dim, bub_dim = self.tiling_work.tiling.get("mix_ub_dim")

        single_core_a = self._do_mix_pad_attach(tensor_a_workspace, tensor_a_ub,
            self.tiling_work.tiling.get("AUB_shape"), aub_dim, tensor_name="a")
        single_core_b = self._do_mix_pad_attach(tensor_b_workspace, tensor_b_ub,
            self.tiling_work.tiling.get("BUB_shape"), bub_dim, tensor_name="b")
        if tensor_b_workspace is not None:
            self.sch[tensor_b_workspace].pragma(single_core_b, "multicore_sync_core_type", "vector_core")
            self.sch[tensor_b_workspace].wait_block_sync(single_core_b, 0, bottom=True)
            self.sch[tensor_b_workspace].set_block_sync(single_core_b, 0, bottom=True)
        if tensor_a_workspace is not None:
            self.sch[tensor_a_workspace].pragma(single_core_a, "multicore_sync_core_type", "vector_core")
            self.sch[tensor_a_workspace].wait_block_sync(single_core_a, 0, bottom=True)
            self.sch[tensor_a_workspace].set_block_sync(single_core_a, 0, bottom=True)

    def _do_mix_transdata_not_attach(self, tensors, tiling, tensor_name):
        tensor_workspace, tensor_ub_nz, tensor_ub_nd = tensors
        if tensor_workspace is None:
            return None
        k_axis_idx = -4
        mn_axis_idx = -3
        if ((self.status_controller.transpose_a
             and self.status_controller.nz_fusion_flag in [self.NZ_VEC_A, self.NZ_VEC_AB] and tensor_name == "a")
                or (not self.status_controller.transpose_b
                    and self.status_controller.nz_fusion_flag in [self.NZ_VEC_B, self.NZ_VEC_AB]
                    and tensor_name == "b")):
            k_axis_idx, mn_axis_idx = mn_axis_idx, k_axis_idx

        tiling_k_ub, tiling_mn_ub, tiling_k_dim, tiling_mn_dim, db_flag = tiling
        k_outer, k_inner = self.sch[tensor_workspace].split(tensor_workspace.op.axis[k_axis_idx], tiling_k_ub)
        mn_outer, mn_inner = self.sch[tensor_workspace].split(tensor_workspace.op.axis[mn_axis_idx], tiling_mn_ub)
        self.sch[tensor_workspace].reorder(mn_outer, k_outer, mn_inner, k_inner)

        kal1_16 = self.cache_tiling.get("kal1_16")
        kbl1_16 = self.cache_tiling.get("kbl1_16")
        abkl1_attach_flag = self.cache_tiling_mgr.attach_at_flag.get("abkl1_attach_flag")
        max_kl1 = kbl1_16 if abkl1_attach_flag == 2 else kal1_16
        factor = max_kl1 * db_flag // tiling_k_ub
        k_single_core_outer, k_single_core_inner = self.sch[tensor_workspace].split(k_outer, factor=factor)
        k_dim_outer, k_single_core_outer = self.sch[tensor_workspace].split(k_single_core_outer, nparts=tiling_k_dim)
        mn_dim_outer, mn_single_core = self.sch[tensor_workspace].split(mn_outer, nparts=tiling_mn_dim)
        self.sch[tensor_workspace].reorder(mn_dim_outer, k_dim_outer, mn_single_core, k_single_core_outer,
                                           k_single_core_inner)

        block_fused = self.sch[tensor_workspace].fuse(mn_dim_outer, k_dim_outer)
        self.sch[tensor_workspace].bind(block_fused, tvm.thread_axis("blockIdx.x"))
        sub_core, _ = self.sch[tensor_workspace].split(k_single_core_outer, nparts=2)
        self.sch[tensor_workspace].bind(sub_core, tvm.thread_axis("subBlockIdx.x"))
        self.sch[tensor_ub_nz].compute_at(self.sch[tensor_workspace], k_single_core_inner)
        self.sch[tensor_ub_nd].compute_at(self.sch[tensor_workspace], k_single_core_inner)
        return k_single_core_inner

    def _do_mix_transdata_attach(self, tensors, tiling, tensor_name):
        tensor_workspace, tensor_ub_nz, tensor_ub_nd = tensors
        if tensor_workspace is None:
            return None

        tiling_k_ub, tiling_mn_ub, tiling_k_dim, tiling_mn_dim = tiling
        if ((self.status_controller.transpose_a
             and self.status_controller.nz_fusion_flag in [self.NZ_VEC_A, self.NZ_VEC_AB] and tensor_name == "a")
                or (not self.status_controller.transpose_b
                    and self.status_controller.nz_fusion_flag in [self.NZ_VEC_B, self.NZ_VEC_AB]
                    and tensor_name == "b")):
            tiling_k_ub, tiling_mn_ub = tiling_mn_ub, tiling_k_ub
            tiling_k_dim, tiling_mn_dim = tiling_mn_dim, tiling_k_dim
        k_outer, k_inner = self.sch[tensor_workspace].split(tensor_workspace.op.axis[-4], tiling_k_ub)
        mn_outer, mn_inner = self.sch[tensor_workspace].split(tensor_workspace.op.axis[-3], tiling_mn_ub)
        self.sch[tensor_workspace].reorder(k_outer, mn_outer, k_inner, mn_inner)
        k_dim_outer, k_single_core = self.sch[tensor_workspace].split(k_outer, nparts=tiling_k_dim)
        mn_dim_outer, mn_single_core = self.sch[tensor_workspace].split(mn_outer, nparts=tiling_mn_dim)
        self.sch[tensor_workspace].reorder(k_dim_outer, mn_dim_outer, mn_single_core, k_single_core)
        block_fused = self.sch[tensor_workspace].fuse(k_dim_outer, mn_dim_outer)
        self.sch[tensor_workspace].bind(block_fused, tvm.thread_axis("blockIdx.x"))
        self.sch[tensor_ub_nz].compute_at(self.sch[tensor_workspace], k_single_core)
        self.sch[tensor_ub_nd].compute_at(self.sch[tensor_workspace], k_single_core)
        sub_core, mn_single_core = self.sch[tensor_workspace].split(mn_single_core, nparts=2)
        mn_single_core_outer, _ = self.sch[tensor_workspace].split(mn_single_core, nparts=1)
        self.sch[tensor_workspace].bind(sub_core, tvm.thread_axis("subBlockIdx.x"))
        self.sch[tensor_ub_nz].compute_at(self.sch[tensor_workspace], k_single_core)
        self.sch[tensor_ub_nd].compute_at(self.sch[tensor_workspace], k_single_core)
        return mn_single_core_outer

    def _set_cube_vector_sync(self, tensor_workspace, single_core, db_l1, k_ub_dim):
        if self.status_controller.nz_fusion_mode == self.NZ_PIPELINE_NOT_ATTACH:
            c_l0c = self.container.tensor_map.get("c_l0c")
            k1_outer, k1_inner = self.sch[c_l0c].split(self.sch[c_l0c].leaf_iter_vars[3], factor=db_l1)
            # 2 means sub block in vector core
            k1_outer_nparts = k_ub_dim * 2
            k1_outer_outer, k1_outer_inner = self.sch[c_l0c].split(k1_outer, nparts=k1_outer_nparts)
            self.sch[c_l0c].reorder(k1_outer_inner, k1_outer_outer)

            if self.support_shift_block:
                block_dims = self.tiling_work.tiling.get("block_dim")
                m_dim = block_dims[2]
                n_dim = block_dims[1]
                overlap_cond_list = self._get_overlap_cond_list()
                self.sch.shift_block_access(self.mn_axis, [m_dim, n_dim], k1_outer_outer, overlap_cond_list)

            self.sch[c_l0c].pragma(k1_outer_outer, "multicore_sync_core_type", "cube_core")
            self.sch[c_l0c].wait_intra_group_sync(k1_outer_outer, 0, bottom=False)
            self.sch[tensor_workspace].pragma(single_core, "multicore_sync_core_type", "vector_core")
            self.sch[tensor_workspace].set_intra_group_sync(single_core, 0, bottom=True)

        self.sch[tensor_workspace].pragma(single_core, "multicore_sync_core_type", "vector_core")
        self.sch[tensor_workspace].wait_block_sync(single_core, 0, bottom=True)
        self.sch[tensor_workspace].set_block_sync(single_core, 0, bottom=True)

    def _do_mix_transdata_process(self):
        tensor_a_workspace = self.container.mix_workspace_tensor.get("tensor_a_workspace")
        tensor_a_ub_nz = self.container.tensor_map.get("a_ub_nz")
        tensor_a_ub_nd = self.container.tensor_map.get("a_ub_nd")
        tensor_b_workspace = self.container.mix_workspace_tensor.get("tensor_b_workspace")
        tensor_b_ub_nz = self.container.tensor_map.get("b_ub_nz")
        tensor_b_ub_nd = self.container.tensor_map.get("b_ub_nd")
        m_aub_dim, n_bub_dim, k_aub_dim, k_bub_dim = self.tiling_work.tiling.get("mix_ub_dim")
        k1_aub, m1_aub = self.tiling_work.tiling.get("AUB_shape")[:2]
        k1_bub, n1_bub = self.tiling_work.tiling.get("BUB_shape")[:2]
        db_al1 = self.tiling_work.tiling.get("manual_pingpong_buffer").get("AL1_pbuffer")
        db_bl1 = self.tiling_work.tiling.get("manual_pingpong_buffer").get("BL1_pbuffer")

        if self.status_controller.nz_fusion_mode == self.NZ_PIPELINE_NOT_ATTACH:
            single_core_a = self._do_mix_transdata_not_attach([tensor_a_workspace, tensor_a_ub_nz, tensor_a_ub_nd],
                                                              [k1_aub, m1_aub, k_aub_dim, m_aub_dim, db_al1], "a")
            single_core_b = self._do_mix_transdata_not_attach([tensor_b_workspace, tensor_b_ub_nz, tensor_b_ub_nd],
                                                              [k1_bub, n1_bub, k_bub_dim, n_bub_dim, db_bl1], "b")
        else:
            single_core_a = self._do_mix_transdata_attach([tensor_a_workspace, tensor_a_ub_nz, tensor_a_ub_nd],
                                                          [k1_aub, m1_aub, k_aub_dim, m_aub_dim], "a")
            single_core_b = self._do_mix_transdata_attach([tensor_b_workspace, tensor_b_ub_nz, tensor_b_ub_nd],
                                                          [k1_bub, n1_bub, k_bub_dim, n_bub_dim], "b")

        if tensor_b_workspace is not None:
            self._set_cube_vector_sync(tensor_b_workspace, single_core_b, db_bl1, k_bub_dim)
        if tensor_a_workspace is not None:
            self._set_cube_vector_sync(tensor_a_workspace, single_core_a, db_al1, k_aub_dim)

    def _do_mix_l2_process(self):
        if self.status_controller.nz_fusion_flag:
            self._do_mix_transdata_process()
            return
        if self.status_controller.pad_flag:
            self._do_mix_pad_process()
            return
        c_l0c = self.container.tensor_map.get("c_l0c")
        tensor_c_gm_workspace = self.container.mix_workspace_tensor.get("tensor_c_gm_workspace")
        if tensor_c_gm_workspace is not None:
            self.sch_agent.same_attach(tensor_c_gm_workspace, c_l0c)
        tensor_a_workspace = self.container.mix_workspace_tensor.get("tensor_a_workspace")
        if tensor_a_workspace is not None:
            self.sch_agent.same_attach(tensor_a_workspace, self.container.tensor_map.get("a_ub_fract"))
        tensor_b_workspace = self.container.mix_workspace_tensor.get("tensor_b_workspace")
        if tensor_b_workspace is not None:
            self.sch_agent.same_attach(tensor_b_workspace, self.container.tensor_map.get("b_ub_fract"))

        if self.container.double_out_tensor and self.status_controller.support_mix_l2_fusion:
            double_out_gm_tensor = self.container.double_out_tensor[0]
            self.sch_agent.same_attach(double_out_gm_tensor, c_l0c)

    def _collect_all_spec_mid_tensor(self):
        return list(self.container.mix_workspace_tensor.values())

    def _do_fixpipe_process(self):
        b_l1 = self.container.tensor_map.get("b_l1")
        self.sch_agent.pre_apply()
        attach_dict = self.sch_agent.get_attach_dict()
        attach_dict.update(self.private_attach_dict)

        if self.status_controller.support_fix_pipe_l0c2ub and self.para_map.get("has_ub_fusion"):
            same_attach_tensor = self.container.tensor_map.get("c_ub_fract")
        else:
            same_attach_tensor = self.container.tensor_map.get("c_l0c")

        for fixpipe_l1_mem in self.container.tensor_map.get("fixpipe_l1", []):
            if self.para_map.get("pre_conv_mode") not in ("VF322B8",) and attach_dict.get(self.sch[b_l1]) is None:
                self.sch_agent.same_attach(fixpipe_l1_mem, b_l1)
            else:
                self.sch_agent.same_attach(fixpipe_l1_mem, same_attach_tensor)
        if self.container.tensor_map.get("fixpipe_l1_elewise") is not None:
            self.sch_agent.same_attach(
                self.container.tensor_map.get("fixpipe_l1_elewise"),
                same_attach_tensor
            )
        for fixpipe_fb_mem in self.container.tensor_map.get("fixpipe_fb", []):
            self.sch_agent.same_attach(fixpipe_fb_mem, same_attach_tensor)

    def _bind_multi_core(self, splited_flag):
        """
        split multi-core and bind axes
        multi-core is binded before split in non-factor binary scene, and binded after splited in other scenes
        """
        if self.cache_tiling:
            ax_result = self.cache_tiling_mgr.bind_multi_core_cache_tiling(
                self.root_tensor, self.status_controller, self.dtype_info, self.format_info, splited_flag)
            self.container.axis_core, k_axis_core, ax_m_out, ax_n_out = ax_result
            self.mn_axis = [ax_m_out, ax_n_out]
            if not splited_flag:
                self.container.k_axis_core = k_axis_core
            return
        if not splited_flag:
            return
        axis_mn = self.sch_agent[self.root_tensor].get_active_scopes()
        ax_batch = 1
        offset_value = 1 if self.status_controller.split_k_axis_by_tiling else 0
        if (not self.status_controller.have_batch) and (not self.status_controller.reduce_fusion):
            upper_range = 2 + offset_value
            ax_m, ax_n = axis_mn[offset_value:upper_range]
        else:
            upper_range = 3 + offset_value
            ax_batch, ax_m, ax_n = axis_mn[offset_value:upper_range]
        if self.format_info.get("out") != "ND":
            ax_m, ax_n = ax_n, ax_m

        batch_dim, n_dim, m_dim, reduce_dim = self.tiling_work.tiling.get("block_dim")
        if self.status_controller.reduce_fusion:
            batch_dim = get_value(self.container.tensor_map.get("c_l0c").shape)[0]
        axis_list = [ax_m, ax_n]
        axis_dim = [m_dim, n_dim]
        if self.status_controller.have_batch or self.status_controller.reduce_fusion:
            axis_list.insert(0, ax_batch)
            axis_dim.insert(0, batch_dim)
        if self.status_controller.split_k_axis_by_tiling:
            axis_list.insert(0, axis_mn[0])
            axis_dim.insert(0, reduce_dim)

        self.container.axis_core = self.sch_agent[self.root_tensor].bind_core(axis_list, axis_dim)
        # bind subblock
        if self.container.tensor_map.get("workspace_to_ub") is not None:
            c_l0c = self.container.tensor_map.get("c_l0c")
            c_ub_fract = self.container.tensor_map.get("c_ub_fract")
            self.sch_agent.pre_apply()
            compute_path = self.sch_agent.get_compute_path()
            axis_c_l0c = compute_path.get(self.sch[c_l0c])
            axis_c_ub = compute_path.get(self.sch[c_ub_fract])
            if axis_c_l0c != axis_c_ub:
                # calculate ratio
                aic_core_num = int(tbe_platform_info.get_soc_spec("CUBE_CORE_CNT"))
                aiv_core_num = int(tbe_platform_info.get_soc_spec("VECTOR_CORE_CNT"))
                ratio = aiv_core_num // aic_core_num
                # bind subblock
                subblock_outer, _ = self.sch_agent[self.root_tensor].split(axis_c_ub, nparts=ratio)
                self.sch[self.root_tensor].bind(subblock_outer, tvm.thread_axis("subBlockIdx.x"))
                self.sch_agent.update_attach_scope(axis_c_ub, subblock_outer)

    def _buffer_align_func(self, tensor, have_batch, *align_args):

        if tensor is not None:
            if have_batch:
                self.sch[tensor].buffer_align((1, 1), *align_args)
            else:
                self.sch[tensor].buffer_align(*align_args)

    def _do_buffer_align(self):
        have_batch_b = self.status_controller.have_batch_b
        have_batch_a = self.status_controller.have_batch_a
        have_batch = self.status_controller.have_batch
        self._do_buffer_align_l0c()
        self._set_requant_transfer_buffer_align()
        is_int82fp32_nd = self._is_int82fp32_nd()
        if self.status_controller.ops_data_flow_mode == "fp322fp32":
            self._buffer_align_func(
                self.container.tensor_map.get("a_l1"), have_batch_a, (1, 1), (1, 1), (1, 16), (1, 8))
            self._buffer_align_func(
                self.container.tensor_map.get("b_l1"), have_batch_b, (1, 1), (1, 1), (1, 16), (1, 8))

        self._buffer_align_func(self.container.tensor_map.get("b_transpose"), have_batch_b, (1, 32), (1, 32))
        self._buffer_align_func(self.container.tensor_map.get("a_transpose"), have_batch_a, (1, 32), (1, 32))
        if is_int82fp32_nd:
            self._buffer_align_func(self.container.tensor_map.get("b_ub"), have_batch_b, (1, 32), (1, 32))
            self._buffer_align_func(self.container.tensor_map.get("a_ub"), have_batch_a, (1, 32), (1, 32))

        if self.format_info.get("out") == "ND":
            self._buffer_align_func(self.container.tensor_map.get("c_add_bias_ub"), have_batch, (1, 16), (1, 16))
            self._buffer_align_func(self.container.tensor_map.get("beta_bias"), have_batch, (1, 16), (1, 16))

        cast_to_fp16 = self.container.tensor_map.get("cast_to_fp16")
        if cast_to_fp16 is not None:
            if len(cast_to_fp16.shape) in (2, 3):
                self._buffer_align_func(self.container.tensor_map.get("cast_to_fp16"), have_batch, (1, 16), (1, 16))
            else:
                self._buffer_align_func(self.container.tensor_map.get("cast_to_fp16"),
                    have_batch, (1, 1), (1, 1), (1, 16), (1, 16))
        c_ub_fract = self.container.tensor_map.get("c_ub_fract")
        if self.status_controller.split_k_axis_by_tiling:
            self._buffer_align_func(c_ub_fract, have_batch, (1, 1), (1, 1), (1, 1),
                                    (1, 16), (1, 16))
        else:
            if c_ub_fract is not None and len(c_ub_fract.shape) > self.BATCH_MATMUL_LEN_ND:
                self._buffer_align_func(self.container.tensor_map.get("c_ub_fract"), have_batch,
                                        (1, 1), (1, 1), (1, 16), (1, 16))
        self._buffer_align_func(
            self.container.tensor_map.get("bias_l0c"), have_batch, (1, 1), (1, 1), (1, 16), (1, 16))
        self._buffer_align_func(
            self.container.tensor_map.get("c_add_bias"), have_batch, (1, 1), (1, 1), (1, 16), (1, 16))
        self._buffer_align_func(self.container.tensor_map.get("c_add_bias_ub_fp16"),
                    have_batch, (1, 1), (1, 1), (1, 16), (1, 16))
        self._buffer_align_func(self.container.tensor_map.get("c_add_bias_ub_fp32"),
                    have_batch, (1, 1), (1, 1), (1, 16), (1, 16))

        if self.para_map.get("mmad_mode") == "gevm":
            self._buffer_align_func(
                self.container.tensor_map.get("a_l1"), have_batch_a, (1, 1), (1, 1), (1, 16), (1, 16))
        self._do_buffer_align_fixpipe_l1()
        self._do_buffer_align_bias_bt()
        self._do_buffer_align_for_l0b()

    def _do_buffer_align_for_l0b(self):
        if (in_dynamic() or (not self.status_controller.support_fix_pipe_l0c2out) or
            (self.para_map.get("a_dtype") != "int8")):
            return

        if not self.status_controller.transpose_b:
            have_batch_b = self.status_controller.have_batch_b
            self._buffer_align_func(self.container.tensor_map.get("b_l0b"),
                                    have_batch_b, (1, 1), (2, 2), (1, 16), (1, 32))

    def _has_bias_broadcast_to_l0c(self):
        for tensor in self.container.tensors_in_l0c:
            if "bias" in tensor.op.name:
                return True
        return False

    def _check_tensor_in_l0c_preload(self):
        """
        batch_matmul + bias + gelu may use pipev2 in 1951, open bias preload.
        if compile_para has preload, do bias preload when bias broadcast to l0c else do l0c preload
        """
        tiling = self.tiling_work.tiling
        is_gelu_fusion = "gelu" in str(self.para_map.get("kernel_name"))
        pipev2_constraints = tbe_platform_info.get_soc_spec("pipev2_constraints") == "1"
        preload_flag = False
        tbe_compile_para = self.tiling_work.tiling.get("tbe_compile_para")
        if tbe_compile_para:
            _, tbe_sch_control_para = parse_tbe_compile_para(tbe_compile_para)
            preload_flag = tbe_sch_control_para.get("preload")
        bias_preload = (pipev2_constraints and is_gelu_fusion and \
               (tiling.get("manual_pingpong_buffer").get("CL0_pbuffer") == 2 and \
                tiling.get("manual_pingpong_buffer").get("CUB_pbuffer") == 1)) or preload_flag
        l0c_preload = self.cache_tiling_mgr.flag_l0c_preload or (not self._has_bias_broadcast_to_l0c() and preload_flag)
        return l0c_preload, bias_preload

    def _double_buffer_b_l1(self, bl1_db):
        double_buffer_list = [self.container.tensor_map.get("b_l1")]
        # space of fixpipe tensor on L1 and bias on L1 are cacluated by N sise,
        # should do double buffer with b_l1
        double_buffer_list += self.container.tensor_map.get("fixpipe_l1", [])
        bias_l1 = self.container.tensor_map.get("bias_l1")
        bias_zero = self.container.tensor_map.get("bias_zero")
        if bias_l1 is not None:
            double_buffer_list.append(bias_l1)
        if bias_zero is not None:
            double_buffer_list.append(bias_zero)

        if self.container.tensor_map.get("fixpipe_l1_elewise") is not None:
            double_buffer_list.append(self.container.tensor_map.get("fixpipe_l1_elewise"))

        for double_buffer_mem in double_buffer_list:
            if isinstance(bl1_db, tvm.Var):
                self.sch[double_buffer_mem].double_buffer(bl1_db)
            else:
                self.sch[double_buffer_mem].double_buffer()

    def _double_buffer_same_with_l0c(self):
        if self.tiling_work.tiling.get("manual_pingpong_buffer").get("CL0_pbuffer") != 2:
            return
        double_buffer_list = (self.container.tensor_map.get("fixpipe_fb", [])).copy()

        bias_bt = self.container.tensor_map.get("bias_bt")
        if bias_bt is not None:
            if self._check_double_buffer_size_bias_table(bias_bt.dtype):
                double_buffer_list.append(bias_bt)

        for double_buffer_mem in double_buffer_list:
            self.sch[double_buffer_mem].double_buffer()

    def _check_double_buffer_size_bias_table(self, bias_bt_dtype):
        if self.cache_tiling:
            return True
        tiling_cl0_n = self.tiling_work.tiling.get("CL0_matrix")[0]
        block_out = tbe_platform_info.CUBE_MKN.get(bias_bt_dtype).get("mac")[2]
        bias_table_size = tbe_platform_info.get_soc_spec(tbe_platform_info.BT_SIZE)
        # the number 2 means open double buffer
        if tiling_cl0_n * block_out * DTYPE_BYTE_MAPPING.get(bias_bt_dtype) * 2 <= bias_table_size:
            return True
        return False


    def _double_buffer_aub_bub(self):
        tiling = self.tiling_work.tiling
        params = {"container": self.container, "status_controller": self.status_controller,
                  "cache_tiling": self.cache_tiling}
        # disable aub_bub_preload when l0c_preload enabled for accuracy error
        if tiling.get("manual_pingpong_buffer").get("AUB_pbuffer") == 2:
            aub_preload_condition = (self.container.tensor_map.get("a_ub") is not None and
                (not self.is_dynamic or (self.cache_tiling and self.cache_tiling_mgr.flag_aub_preload)) and
                self.buffer_checker.check_aub_preload(tiling, params))
            if aub_preload_condition:
                self.sch[self.container.tensor_map.get("a_ub")].preload()
                if (self.container.tensor_map.get("a_ub_aligned") is not None and
                    self.container.tensor_map.get("a_ub_general") is not None):
                    self.sch[self.container.tensor_map.get("a_ub_aligned")].preload()
                    self.sch[self.container.tensor_map.get("a_ub_general")].preload()
            for tensor in self.container.tensors_in_aub:
                self.sch[tensor].double_buffer()
        if tiling.get("manual_pingpong_buffer").get("BUB_pbuffer") == 2:
            bub_preload_condition = (self.container.tensor_map.get("b_ub") is not None and
                (not self.is_dynamic or (self.cache_tiling and self.cache_tiling_mgr.flag_bub_preload)) and
                self.buffer_checker.check_bub_preload(tiling, params))
            if bub_preload_condition:
                self.sch[self.container.tensor_map.get("b_ub")].preload()
                if (self.container.tensor_map.get("b_ub_aligned") is not None and
                    self.container.tensor_map.get("b_ub_general") is not None):
                    self.sch[self.container.tensor_map.get("b_ub_aligned")].preload()
                    self.sch[self.container.tensor_map.get("b_ub_general")].preload()
            for tensor in self.container.tensors_in_bub:
                self.sch[tensor].double_buffer()

    def _double_buffer_cub(self):
        tiling = self.tiling_work.tiling
        params = {"container": self.container, "status_controller": self.status_controller,
                  "cache_tiling": self.cache_tiling}
        if tiling.get("manual_pingpong_buffer").get("CUB_pbuffer") == 2:
            bias_ub = self.container.tensor_map.get("bias_ub")
            if bias_ub is not None:
                if not self.is_dynamic and self.buffer_checker.check_bias_preload(tiling, params):
                    self.sch[bias_ub].preload()
                self.sch[bias_ub].double_buffer()

                if self.status_controller.need_init_bias:
                    if not self.is_dynamic and self.buffer_checker.check_bias_preload(tiling, params):
                        self.sch[self.container.tensor_map.get('init_value_of_bias_ub')].preload()
                        self.sch[self.container.tensor_map.get('virtual_add_bias')].preload()
                    self.sch[self.container.tensor_map.get('init_value_of_bias_ub')].double_buffer()
                    self.sch[self.container.tensor_map.get('virtual_add_bias')].double_buffer()
            for tensor in self.container.tensors_in_cub:
                if tensor in (self.res, self.container.tensor_map.get("c_gm")):
                    continue
                self.sch[tensor].double_buffer()
            if self.para_map.get("fusion_multi_output_flag", False):
                if self.status_controller.support_mix_l2_fusion:
                    self.sch[self.container.tensor_map.get("workspace_to_ub")].double_buffer()
                else:
                    self.sch[self.container.tensor_map.get("c_gm")].double_buffer()

    def _double_buffer(self, a_run_once, b_run_once):
        tiling = self.tiling_work.tiling
        al1_db = tiling.get("manual_pingpong_buffer").get("AL1_pbuffer")
        if isinstance(al1_db, tvm.Var):
            self.sch[self.container.tensor_map.get("a_l1")].double_buffer(al1_db)
        elif (al1_db == 2
            and (a_run_once == self.ALLOCATE_OFF)):
            self.sch[self.container.tensor_map.get("a_l1")].double_buffer()
        bl1_db = tiling.get("manual_pingpong_buffer").get("BL1_pbuffer")
        if ((isinstance(bl1_db, tvm.Var) or (bl1_db == 2))
            and (b_run_once == self.ALLOCATE_OFF)):
            self._double_buffer_b_l1(bl1_db)
        if tiling.get("manual_pingpong_buffer").get("AL0_pbuffer") == 2:
            self.sch[self.container.tensor_map.get("a_l0a")].double_buffer()
        if tiling.get("manual_pingpong_buffer").get("BL0_pbuffer") == 2:
            self.sch[self.container.tensor_map.get("b_l0b")].double_buffer()
        if tiling.get("manual_pingpong_buffer").get("CL0_pbuffer") == 2:
            l0c_preload, bias_preload = self._check_tensor_in_l0c_preload()
            for tensor in self.container.tensors_in_l0c:
                self.sch[tensor].double_buffer()
                # cache_tiling enable config: enable_db_fold
                if not (self.cache_tiling and self.para_map.get("compress_flag", False)) and \
                    (bias_preload and "bias" in tensor.op.name) or (l0c_preload and not self.cache_tiling):
                    self.sch[tensor].preload()
        self._double_buffer_aub_bub()
        self._double_buffer_cub()
        self._double_buffer_same_with_l0c()

    def _emit_insn_func(self, insn_tensor, insn_axis_num, insn_tag, insn_dict=None, mode=0, offset=0):
        normal_mode = 0
        if insn_tensor is not None:
            tensor_len = len(insn_tensor.shape) + offset
            if mode == normal_mode:
                scope_insn = self.sch_agent[insn_tensor].op.axis[insn_axis_num]
            else:
                scopes_intrins = self.sch_agent[insn_tensor].intrin_scopes(tensor_len)
                scope_insn = scopes_intrins[insn_axis_num]

            if insn_dict is None:
                self.sch_agent[insn_tensor].emit_insn(scope_insn, insn_tag)
            else:
                self.sch_agent[insn_tensor].emit_insn(scope_insn, insn_tag, insn_dict)

    def _pragma_func(self, insn_tensor, insn_axis_num, insn_tag):
        if insn_tensor is not None:
            self.sch_agent[insn_tensor].pragma(insn_tensor.op.axis[insn_axis_num], insn_tag, insn_axis_num)


    def _do_emit_insn_l0c(self):
        # emit insn for l0c
        al1_attach_flag = self.status_controller.attach_at_flag.get("al1_attach_flag")
        bl1_attach_flag = self.status_controller.attach_at_flag.get("bl1_attach_flag")
        non_factor_invalid_flag = not self.status_controller.split_k_axis_by_tiling
        # 2 means no full load
        no_full_load_flag = al1_attach_flag == 2 and bl1_attach_flag == 2
        # other tensors on l0c
        # only in |matmul|ND Nz|all data type|
        if not self.cache_tiling or self.para_map.get("compress_flag", False):
            self._emit_insn_func(self.container.tensor_map.get("bias_l0c"), 0, "dma_copy")
            self._emit_insn_func(self.container.tensor_map.get("c_add_bias"), 0, "phony_insn")
            self._pragma_func(self.container.tensor_map.get("bias_l0c"), 0, "reuse_output")
            self._pragma_func(self.container.tensor_map.get("c_add_bias"), 0, "replace_output")
        if self.cache_tiling and no_full_load_flag and non_factor_invalid_flag and \
            not self.status_controller.unaligned_flag:
            return
        c_l0c = self.container.tensor_map.get("c_l0c")
        scopes_intrins = self.sch_agent[c_l0c].intrin_scopes(6)
        scope_insn = scopes_intrins[0]
        if self.status_controller.split_k_axis_by_tiling:
            reduce_axis_index = 1
        else:
            reduce_axis_index = 0
        inner_k_axis = self.sch_agent[c_l0c].get_relate_scope(c_l0c.op.reduce_axis[reduce_axis_index], scope_insn)
        if inner_k_axis:
            mad_dict = {
                "mad_pattern": self.para_map.get("mad_pattern"),
                "k_outer": self.sch_agent[c_l0c].get_relate_scope(c_l0c.op.reduce_axis[reduce_axis_index], scope_insn)
            }
        else:
            (inner_nb, inner_mb, inner_mp, inner_np, inner_kb, inner_kp) = scopes_intrins
            inner_ko, inner_ki = self.sch_agent[c_l0c].split(inner_kb, nparts=1)
            self.sch_agent[c_l0c].reorder(
                inner_ko, inner_nb, inner_mb, inner_mp, inner_np, inner_ki, inner_kp
            )
            mad_dict = {"mad_pattern": self.para_map.get("mad_pattern"), "k_outer": [inner_ko]}

        if self.container.tensor_map.get("c_add_bias") is not None:
            self.sch_agent[c_l0c].pragma(scope_insn, "replace_output", 0)
            mad_dict["init_bias"] = 1

        if self.status_controller.ops_data_flow_mode == "fp322fp32":
            if get_precision_mode("MatMul") in ["default", "high_performance", "enable_hi_float_32_execution"]:
                mad_dict["hf32"] = 1
            if self.cache_tiling:
                mad_dict["hf32"] = get_optional_te_var("hf32_flag")
        block_dims = self.tiling_work.tiling.get("block_dim")
        m_dim = block_dims[2]
        n_dim = block_dims[1]
        split_k_flag = self.status_controller.split_k or self.status_controller.split_k_axis_by_tiling
        self.support_shift_block = self.is_dynamic and self.status_controller.support_fix_pipe_l0c2out and \
            self.mn_axis != [1, 1] and not split_k_flag
        if self.support_shift_block:
            shift_k_axis = mad_dict["k_outer"][0]
            if self.status_controller.nz_fusion_mode == self.NZ_PIPELINE_NOT_ATTACH:
                shift_k_axis = mad_dict["k_outer"][1]
            overlap_cond_list = self._get_overlap_cond_list()
            self.sch.shift_block_access(self.mn_axis, [m_dim, n_dim], shift_k_axis, overlap_cond_list)
        if self.sparse_4to2_flag:
            mad_dict["mad_type"] = 1
        mad_dict["enable_k_alignment"] = self.status_controller.enable_k_alignment
        if self.is_dynamic and self.status_controller.support_fix_pipe_l0c2out:
            # unit flag only l0c not db is usalbe
            self._do_unit_flag(mad_dict, False)
            if self.status_controller.support_bf16 and self.container.tensor_map.get("a_l0a").dtype == "float16":
                mad_dict["datatype_bf16"] = self.cache_tiling.get("datatype_bf16")
        self.sch_agent[c_l0c].emit_insn(scope_insn, "mad", mad_dict)

    def _do_emit_insn_bias_table(self):
        bias_l1 = self.container.tensor_map.get("bias_l1")
        bias_zero = self.container.tensor_map.get("bias_zero")
        if bias_l1 is None:
            return
        bias_shape_align = False
        if not in_dynamic():
            bias_shape = shape_to_list(bias_l1.op.input_tensors[0].shape)
            bias_shape_align = bias_shape[-1] % tbe_platform.CUBE_MKN.get(bias_l1.dtype).get("mac")[1] == 0
        bias_use_dma = bias_shape_align or not self.status_controller.support_out2l1_nd2nz
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            padding_value = tvm.const(0, self.tensor_map.get("bias").dtype)
            dma_dict = {"pad": 1, "pad_value": padding_value}
            self.sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy", dma_dict)
        else:
            if bias_use_dma:
                self.sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy")
            else:
                # use nd2nz to padding 0 for bias.
                nd2nz_dict = self.container.nd2nz_dict.copy()
                nd2nz_dict["force_nd2nz"] = 1
                self.sch[bias_l1].emit_insn(bias_l1.op.axis[0], "dma_copy", nd2nz_dict)
        if bias_zero is not None:
            self.sch[bias_zero].emit_insn(bias_zero.op.axis[0], "set_2d")

        bias_bt = self.container.tensor_map.get("bias_bt")
        if "tensor_bias_f162f32" == bias_bt.op.name:
            self._emit_insn_func(bias_bt, 0, "dma_copy", insn_dict={"mem_align": 1})
        else:
            self._emit_insn_func(bias_bt, 0, "dma_copy")

    def _do_emit_insn_fixpipe_from_l0c(self, fixpipe_tensor, need_align):
        out_emit_insn_dict = {}
        tensor_l0c = self.container.tensor_map.get("c_l0c")
        if self.cache_tiling and self.status_controller.support_bf16 and fixpipe_tensor.dtype == "float16" and \
            tensor_l0c.dtype != "int32":
            out_emit_insn_dict["datatype_bf16"] = self.cache_tiling.get("datatype_bf16")
        emit_axises = self.sch_agent[fixpipe_tensor].nlast_scopes(len(fixpipe_tensor.shape))
        if len(fixpipe_tensor.shape) in (self.BATCH_MATMUL_LEN_ND, self.MATMUL_LEN_ND):
            if need_align:
                align_factor = tbe_platform.CUBE_MKN.get(fixpipe_tensor.dtype).get("mac")[1]
                self.sch[fixpipe_tensor].compute_align(fixpipe_tensor.op.axis[-1], align_factor)
                self.sch[fixpipe_tensor].storage_align(fixpipe_tensor.op.axis[-2], align_factor, 0)
            out_emit_insn_dict["layout_transform"] = "nz2nd"
            self.sch_agent[fixpipe_tensor].emit_insn(emit_axises[-2], "fixpipe_op", out_emit_insn_dict)
        else:
            if fixpipe_tensor.dtype == "int8":
                self.sch_agent[fixpipe_tensor].split(emit_axises[-1], tbe_platform_info.BLOCK_OUT)
            if self.status_controller.ops_data_flow_mode == "fp322fp32":
                _, channel_split_in = self.sch[fixpipe_tensor].split(emit_axises[-4], factor=2) # n axis
                out_emit_insn_dict["layout_transform"] = "channel_split"
                self.sch[fixpipe_tensor].emit_insn(channel_split_in, "fixpipe_op", out_emit_insn_dict)
            else:
                self.sch[fixpipe_tensor].emit_insn(emit_axises[-4], "fixpipe_op", out_emit_insn_dict)

    def _do_emit_insn_output(self):
        if self.status_controller.support_fix_pipe_l0c2out:
            tensor_c_gm = self.container.tensor_map.get("c_gm")
            if self.para_map.get("has_ub_fusion"):
                self._do_emit_insn_fixpipe_from_l0c(tensor_c_gm, True)
                if self.container.double_out_tensor:
                    for tensor in self.container.double_out_tensor:
                        emit_str = "fixpipe_op" if tensor.op.tag == "gemm" else "dma_copy"
                        self.sch[tensor].emit_insn(tensor.op.axis[0], emit_str)
                    self._emit_insn_func(self.root_tensor, 0, "phony_insn", mode=1)
                else:
                    self._emit_insn_func(self.root_tensor, 0, "dma_copy", mode=1)
            else:
                if self.status_controller.support_l0c2out_nz2nd or self.format_info.get("out") == "FRACTAL_NZ":
                    self._do_emit_insn_fixpipe_from_l0c(self.root_tensor, False)
                else:
                    self._do_emit_insn_fixpipe_from_l0c(self.tensor_map.get("c_ub_fract"), False)
                    self._emit_insn_func(self.container.tensor_map.get("c_gm"), 0, "dma_copy", mode=1)
        else:
            self._do_emit_insn_multi_output()

    def _do_emit_insn_fixpipe(self):
        for fixpipe_l1_mem in self.container.tensor_map.get("fixpipe_l1", []):
            self._emit_insn_func(fixpipe_l1_mem, 0, "dma_copy")
        if self.container.tensor_map.get("fixpipe_l1_elewise") is not None:
            fixpipe_l1_elewise = self.container.tensor_map.get("fixpipe_l1_elewise")
            self._emit_insn_func(fixpipe_l1_elewise, 0, "dma_copy")
        for fixpipe_fb_mem in self.container.tensor_map.get("fixpipe_fb", []):
            self._emit_insn_func(fixpipe_fb_mem, 0, "dma_copy")

    def _set_for_zero_tensor(self, zero_flag):
        al1 = self.container.tensor_map.get("a_l1")
        bl1 = self.container.tensor_map.get("b_l1")
        tensor_list = [al1, bl1]
        if not self.status_controller.support_fix_pipe_l0c2out:
            if self.para_map.get("need_aub"):
                a_ub = self.container.tensor_map.get("a_ub")
                a_ub_aligned = self.container.tensor_map.get("a_ub_aligned")
                a_ub_general = self.container.tensor_map.get("a_ub_general")
                a_ub_list = [a_ub, a_ub_aligned, a_ub_general]
                tensor_list.extend(a_ub_list)
            if self.para_map.get("need_bub"):
                b_ub = self.container.tensor_map.get("b_ub")
                b_ub_aligned = self.container.tensor_map.get("b_ub_aligned")
                b_ub_general = self.container.tensor_map.get("b_ub_general")
                b_ub_list = [b_ub, b_ub_aligned, b_ub_general]
                tensor_list.extend(b_ub_list)
        for tensor in tensor_list:
            if tensor is not None:
                self.sch[tensor].set_store_predicate(zero_flag == 0)

    def _do_emit_insn_l0a(self):
        a_l0a = self.container.tensor_map.get("a_l0a")
        if self.is_dynamic:
            zero_flag = self.cache_tiling.get("zero_flag")
            # in dynamic mode, set l0a zero method is different from static mode because of pass path
            self.sch[a_l0a].set_value(
                lambda *i: self.cache_tiling.get("zero_flag") == 1, tvm.const(0, dtype=a_l0a.dtype))
        else:
            zero_flag = self.tensor_map.get("tensor_c_gm").op.attrs.get("zero_flag", False)
            self.sch[a_l0a].set_value(tvm.all(zero_flag == 1), tvm.const(0, dtype=a_l0a.dtype))
        self._set_for_zero_tensor(zero_flag)
        if self.status_controller.support_fix_pipe_l0c2out:
            if a_l0a.dtype == "int8" and self.status_controller.transpose_a:
                _, a_l0a_inner = self.sch_agent[a_l0a].split(a_l0a.op.axis[-4], 2) # split m1 axis
                k_axis_index = -3
                self.sch_agent[a_l0a].reorder(a_l0a.op.axis[k_axis_index], a_l0a_inner)
                self.sch_agent[a_l0a].emit_insn(a_l0a_inner, "dma_copy", {"is_2dtranspose": 1})
            elif self.status_controller.ops_data_flow_mode == "fp322fp32" and self.status_controller.transpose_a:
                self.sch_agent[a_l0a].split(a_l0a.op.axis[-2], factor=8)
                emit_axis = 0
                if self.status_controller.attach_at_flag.get("l0c_multi_batch"):
                    emit_axis = 1
                    self.sch[a_l0a].compute_align(a_l0a.op.axis[1 + emit_axis], 2)
                self.sch_agent[a_l0a].emit_insn(a_l0a.op.axis[emit_axis], "dma_copy", {"img2col": 1})
            else:
                # enable to select less loops for load2d to improve the insn's perf
                attrs = {"load2d_for_loop_enhance": 1}
                self._emit_insn_func(a_l0a, 0, "dma_copy", insn_dict=attrs)
        else:
            if self.status_controller.unaligned_flag:
                attrs = {"axis_dynamic_shift": len(a_l0a.shape)}
                self._emit_insn_func(a_l0a, 0, "dma_copy", insn_dict=attrs, mode=1)
            else:
                self._emit_insn_func(a_l0a, 0, "dma_copy", mode=1)

    def _do_emit_insn_l0b(self):
        b_l0b = self.container.tensor_map.get("b_l0b")
        l0b_emit_dict = {}
        if self.sparse_4to2_flag:
            l0b_emit_dict["enable_sparse"] = 1
        if not self.is_dynamic:
            zero_flag = self.tensor_map.get("tensor_c_gm").op.attrs.get("zero_flag", False)
            self.sch[b_l0b].set_value(tvm.all(zero_flag == 1), tvm.const(0, dtype=b_l0b.dtype))
        if self.status_controller.support_fix_pipe_l0c2out:
            if b_l0b.dtype == "int8" and not self.status_controller.transpose_b:
                _, b_l0b_inner = self.sch_agent[b_l0b].split(b_l0b.op.axis[-3], 2) # split n1 axis
                l0b_emit_dict["is_2dtranspose"] = 1
                self.sch[b_l0b].pragma(b_l0b_inner, "loop_with_no_overlap_tensor")
                self.sch_agent[b_l0b].emit_insn(b_l0b_inner, "dma_copy", l0b_emit_dict)
                self._set_load_2d_emit_insn(-5)
            elif self.status_controller.ops_data_flow_mode == "fp322fp32" and not self.status_controller.transpose_b:
                self.sch_agent[b_l0b].split(self.sch_agent[b_l0b].op.axis[-2], 8)
                emit_axis = 1 if self.status_controller.attach_at_flag.get("l0c_multi_batch") else 0
                self.sch_agent[b_l0b].emit_insn(self.sch_agent[b_l0b].op.axis[emit_axis], "dma_copy", {"img2col": 1})
                # if dtype is float32 the index of b_l0k is -5, else -4
                self._set_load_2d_emit_insn(-5)
            else:
                # enable to select less loops for load2d to improve the insn's perf
                attrs = {"load2d_for_loop_enhance": 1}
                self._emit_insn_func(b_l0b, 0, "dma_copy", insn_dict=attrs)
                self._set_load_2d_emit_insn()
        else:
            if self.para_map.get("compress_flag", False) and not self.sparse_4to2_flag:
                self._compress_matmul_l0b_process(b_l0b)
            else:
                self._emit_insn_func(b_l0b, 0, "dma_copy", mode=1)
                self._set_load_2d_emit_insn()

    def _compress_matmul_l0b_process(self, b_l0b):
        compress_index = self.container.tensor_map.get("compress_index")
        if not self.status_controller.b_l1_inline_flag:
            self._emit_insn_func(b_l0b, 0, "dma_copy", mode=1)
        else:
            host_axis = self._get_index_at_axis()
            if not self.is_dynamic:
                k_l0_tile = self.tiling_work.bl0_tiling_kb
                n_l0_tile = self.tiling_work.bl0_tiling_nb
            else:
                k_l0_tile = self.cache_tiling.get("k_l0")
                n_l0_tile = self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1")
            self._set_compress_info(b_l0b, compress_index, k_l0_tile, n_l0_tile, host_axis)

    def _get_not_attach_k_axis(self):
        kal1_factor = self.cache_tiling.get("kal1_factor")
        kbl1_factor = self.cache_tiling.get("kbl1_factor")
        abkl1_attach_flag = self.cache_tiling_mgr.attach_at_flag.get("abkl1_attach_flag")
        min_kl1_factor = kbl1_factor if abkl1_attach_flag == 2 else kal1_factor

        c_l0c = self.container.tensor_map.get("c_l0c")
        k1_outer_outer = self.sch[c_l0c].leaf_iter_vars[4]
        k1_outer_inner = self.sch[c_l0c].leaf_iter_vars[3]
        k1_inner = self.sch[c_l0c].leaf_iter_vars[5]
        _, _, k_aub_dim, k_bub_dim = self.tiling_work.tiling.get("mix_ub_dim")
        db_l1, k_ub_dim = self.tiling_work.tiling.get("manual_pingpong_buffer").get("AL1_pbuffer"), k_aub_dim
        if self.status_controller.nz_fusion_flag == self.NZ_VEC_B:
            db_l1, k_ub_dim = self.tiling_work.tiling.get("manual_pingpong_buffer").get("BL1_pbuffer"), k_bub_dim
        return k1_outer_outer * int_ceil_div(min_kl1_factor, k_ub_dim * 2) + k1_outer_inner * db_l1 + k1_inner

    def _set_load_2d_emit_insn(self, b_l0k_index=-4):
        if not self.cache_tiling:
            return
        b_l0b = self.container.tensor_map.get("b_l0b")
        l0b_axis = self.sch[b_l0b].leaf_iter_vars # L0B 从不切分，所以L0B的Leaf_iter_vars就是全部的K
        # Get Tiling Params in cache Tiling
        al1_attach_flag = self.cache_tiling_mgr.attach_at_flag.get("al1_attach_flag")
        bl1_attach_flag = self.cache_tiling_mgr.attach_at_flag.get("bl1_attach_flag")
        abkl1_attach_flag = self.cache_tiling_mgr.attach_at_flag.get("abkl1_attach_flag")
        all_kl1_full_load = (al1_attach_flag in (0, 1) and bl1_attach_flag in (0, 1))
        kal1_16 = self.cache_tiling.get("kal1_16")
        kbl1_16 = self.cache_tiling.get("kbl1_16")
        if self.cache_tiling.get("zero_flag"):
            self.sch[b_l0b].set_value(
                lambda *i: self.cache_tiling.get("zero_flag") == 1, tvm.const(0, dtype=b_l0b.dtype))
            return
        if self.status_controller.split_k_axis_by_tiling and not self.status_controller.unaligned_flag:
            k1_extent = get_optional_te_var(self.compute_param.k_var_name)
            k_single_core = self.cache_tiling.get("kal1_factor") * kal1_16
            cond1 = tvm.min(
                self.container.k_axis_core * k_single_core, k1_extent - k_single_core) == k1_extent - k_single_core
            cond2 = l0b_axis[b_l0k_index] < self.container.k_axis_core * k_single_core
            cond3 = k1_extent % k_single_core != 0
            self.sch[b_l0b].set_value(tvm.all(cond1, cond2, cond3), tvm.const(0, dtype=b_l0b.dtype))
        else:
            # non-factor k in a single core
            tensor_l0c = self.container.tensor_map.get("c_l0c")
            k_single_core = get_optional_te_var(self.compute_param.k_var_name)
            if b_l0b.dtype == "int8":
                # var k is k_ori/16 while tiling_kl0 is kl0/32
                k_single_core = int_ceil_div(k_single_core, 2)
            l0c_axis_list = self.sch[tensor_l0c].leaf_iter_vars
            k1_outer_idx = 4 if self.status_controller.have_batch else 3
            k1_outer = l0c_axis_list[k1_outer_idx]
            kl0_extent = self.cache_tiling.get("k_bl0")
            if self.status_controller.nz_fusion_mode == self.NZ_PIPELINE_NOT_ATTACH:
                k1_outer = self._get_not_attach_k_axis()
            if all_kl1_full_load:
                inner_extent = kl0_extent
            elif (self.status_controller.al1_attach_status == "c_l0c" and
                  self.status_controller.bl1_attach_status == "c_l0c"):
                # All k no full load
                # abkl1_attach_flag == 2 means kal1 smaller than kbl1, in this Case bl1 is closer to k1_outer axis.
                inner_extent = kbl1_16 if abkl1_attach_flag == 2 else kal1_16
            else:
                # Partially full load(AL1 full load but Bl1 not full load or BL1 full load but Al1 not full load)
                # In this Case, the smaller L1 tensor is attach under L0c k1_outer.
                inner_extent = kal1_16 if abkl1_attach_flag == 2 else kbl1_16
            # inner extent is L1 if l1 is larger than l0 and one of l1 tensor is attach under l0c
            cond1 = tvm.min(k1_outer * inner_extent, k_single_core - inner_extent) == k_single_core - inner_extent
            cond2 = l0b_axis[b_l0k_index] < k1_outer * inner_extent
            cond3 = k_single_core % inner_extent != 0
            self.sch[b_l0b].set_value(tvm.all(cond1, cond2, cond3), tvm.const(0, dtype=b_l0b.dtype))

    def _do_emit_insn_bl1(self):
        if self.status_controller.b_l1_inline_flag:
            return
        b_l1 = self.container.tensor_map.get("b_l1")
        compress_index_l1 = self.container.tensor_map.get("compress_index_l1")
        if compress_index_l1 is not None:
            self.sch[compress_index_l1].emit_insn(compress_index_l1.op.axis[0], "dma_copy")
        if self.status_controller.support_out2l1_nd2nz:
            if self._check_nd2nz_tag(b_l1, "b"):
                self.sch[b_l1].pragma(b_l1.op.axis[0], "loop_with_no_overlap_tensor")
                self.sch[b_l1].emit_insn(b_l1.op.axis[0], "dma_copy", self.container.nd2nz_dict)
                # Mad cann't use origin shape in quant scene, so need to clean l1 in quant scene
                # Need to clean l1 when output format is not ND
                if self.status_controller.transpose_b and (self.format_info.get("out") != "ND"):
                    if self.is_dynamic:
                        n_ori_shape = get_optional_te_var("n_ori")
                    else:
                        n_ori_shape = shape_to_list(b_l1.op.input_tensors[0].shape)[-2]
                    bl1_axis = self.sch_agent[b_l1].get_active_scopes()
                    factor = bl1_axis[-2].dom.extent
                    self.sch[b_l1].set_value(tvm.call_intrin(b_l1.dtype, "tir.likely",
                                             bl1_axis[-3] * factor + bl1_axis[-2] >= n_ori_shape),
                                             tvm.const(0, dtype=b_l1.dtype))
            else:
                if self.para_map.get("nd2nz_type", 0) == ComputeFlow.mix_l2.value + 1:
                    self.sch[b_l1].pragma(b_l1.op.axis[0], "loop_with_no_overlap_tensor")
                self._emit_insn_func(b_l1, 0, "dma_copy", mode=1)
        else:
            b_l0b = self.container.tensor_map.get("b_l0b")
            if self.para_map.get("compress_flag", False) and not self.sparse_4to2_flag:
                self._compress_matmul_bl1_process(b_l0b, b_l1)
            else:
                self._emit_insn_func(b_l1, 0, "dma_copy", mode=1)

    def _compress_matmul_bl1_process(self, b_l0b, b_l1):
        host_axis = self._get_index_at_axis()
        compress_index = self.container.tensor_map.get("compress_index")
        b_l1.op.attrs["tile_L1_k"] = b_l0b.op.attrs["tile_L1_k"]
        b_l1.op.attrs["tile_L1_n"] = b_l0b.op.attrs["tile_L1_n"]
        # k_l1_tile n_l1_tile host_axis
        if not self.is_dynamic:
            k_l1_tile = self.tiling_work.bl1_tiling_k // self.block_reduce
            n_l1_tile = self.tiling_work.bl1_tiling_n
        else:
            k_l1_tile = self.cache_tiling.get("kbl1_16")
            n_l1_tile = self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1")
        self._set_compress_info(b_l1, compress_index, k_l1_tile, n_l1_tile, host_axis)

    def _check_nd2nz_tag(self, tensor_l1, tensor_name):
        """
        nd2nz only support src_d <= 65535
        :param tensor_l1: nd2nz tensor on l1
        :return: bool, True while src_d > 65535 else False
        """
        if self.cache_tiling:
            if self.format_info.get(tensor_name) != "ND" or self.para_map["nd2nz_type"] == ComputeFlow.mix_l2.value:
                return False
            return True

        if tensor_l1.op.tag not in ["5HD_trans_FZ", "ND_trans_NZ"]:
            return False

        return True

    def _do_emit_insn_al1(self):
        a_l1 = self.container.tensor_map.get("a_l1")
        if self.status_controller.support_out2l1_nd2nz and self._check_nd2nz_tag(a_l1, "a"):
            if a_l1.op.tag == "5HD_trans_FZ":
                tensor_5hd = a_l1.op.input_tensors[0]
                m_ori_shape, _, h_in, w_in, _ = shape_to_list(tensor_5hd.shape)
                #c1hw should be split as c1 and hw when trans nhwc to fractal_z
                chw_out, _ = self.sch_agent[a_l1].split(a_l1.op.axis[0], factor=h_in * w_in)
                emit_insn_axis = chw_out
                if not self.cache_tiling:
                    l1_ka = int_ceil_div(self.tiling_work.al1_tiling_k, self.tiling_work.al0_tiling_k0)
                    if l1_ka % (h_in * w_in) != 0 and (h_in * w_in) % l1_ka != 0:
                        emit_insn_axis = a_l1.op.axis[1]
                self.sch[a_l1].emit_insn(emit_insn_axis, "dma_copy", self.container.nd2nz_dict)
            else:
                self.sch[a_l1].pragma(a_l1.op.axis[0], "loop_with_no_overlap_tensor")
                self.sch[a_l1].emit_insn(a_l1.op.axis[0], "dma_copy", self.container.nd2nz_dict)
                if self.is_dynamic:
                    m_ori_shape = get_optional_te_var("m_ori")
                else:
                    m_ori_shape = shape_to_list(a_l1.op.input_tensors[0].shape)[-2]
            unaligned_flag = self.status_controller.unaligned_flag
            if not self.is_dynamic:
                unaligned_flag = m_ori_shape % self.block_in != 0
            if not self.status_controller.transpose_a and unaligned_flag:
                # Mad cann't use origin shape in quant scene, so need to clean l1 in quant scene
                # Need to clean l1 when output format is not ND
                if self.format_info.get("out") != "ND":
                    al1_axis = self.sch_agent[a_l1].get_active_scopes()
                    factor = al1_axis[-2].dom.extent
                    self.sch[a_l1].set_value(tvm.call_intrin(a_l1.dtype, "tir.likely",
                                             al1_axis[-3] * factor + al1_axis[-2] >= m_ori_shape),
                                             tvm.const(0, dtype=a_l1.dtype))
                else:
                    # Need to clean l1 for gemv
                    self.sch[a_l1].set_value(tvm.call_intrin(a_l1.dtype, "tir.likely", m_ori_shape == 1),
                                             tvm.const(0, dtype=a_l1.dtype), pre_assign=True)
        else:
            self._emit_insn_func(a_l1, 0, "dma_copy", mode=1)

    def _do_emit_insn_cub(self):
        # only in |gemm|ND Nz| all data type|
        self._emit_insn_func(self.container.tensor_map.get("alpha_ub"), 0, "dma_copy")
        self._emit_insn_func(self.container.tensor_map.get("beta_ub"), 0, "dma_copy")
        self._emit_insn_func(self.container.tensor_map.get("alpha_c"), 0, "vector_muls", mode=1)
        self._emit_insn_func(self.container.tensor_map.get("beta_bias"), 0, "vector_muls")
        self._emit_insn_func(self.container.tensor_map.get("bias_ub"), 0, "dma_copy")
        self._emit_insn_func(self.container.tensor_map.get("bias_ub_fp16"), 0, "dma_copy")
        self._emit_insn_func(self.container.tensor_map.get("bias_ub_fp32"), 0, "dma_copy")
        self._emit_insn_func(self.container.tensor_map.get("bias_ub_drnn_cast_fp16"), 0, "vector_conv")
        self._emit_insn_func(self.container.tensor_map.get("bias_ub_drnn_cast_fp32"), 0, "vector_conv")
        self._emit_insn_func(self.container.tensor_map.get("c_add_bias_ub_fp16"), 0, "vector_add")
        self._emit_insn_func(self.container.tensor_map.get("c_add_bias_ub_fp32"), 0, "vector_add")
        # only in matmul ND out solve nonline problem
        self._emit_insn_func(self.container.tensor_map.get("before_c_gm"), 0, "vector_muls")

        if self.status_controller.need_init_bias:
            self._emit_insn_func(self.container.tensor_map.get("init_value_of_bias_ub"), 0, "dma_copy")
            self._emit_insn_func(self.container.tensor_map.get("virtual_add_bias"), 0, "phony_insn")

        #only in |gemm|ND Nz|fp162fp16|
        self._emit_insn_func(self.container.tensor_map.get("alpha_fp162fp32"), 0, "vector_conv")
        self._emit_insn_func(self.container.tensor_map.get("beta_fp162fp32"), 0, "vector_conv")
        self._emit_insn_func(self.container.tensor_map.get("bias_cast_to_fp32"), 0, "vector_conv", mode=1)

        cast_to_fp16_cmd = "dma_copy" if self.para_map.get("c_ub_fract_inline", False) else "vector_conv"
        # only in |gemm matmul|ND Nz|to fp16|
        if not self.status_controller.split_k_axis_by_tiling:
            self._emit_insn_func(self.container.tensor_map.get("cast_to_fp16"), 0, cast_to_fp16_cmd, mode=1)
            self._emit_insn_func(self.container.tensor_map.get("cast_to_fp32"), 0, cast_to_fp16_cmd, mode=1)
        else:
            self._emit_insn_func(self.container.tensor_map.get("cast_to_fp16"), 0, "phony_insn", mode=1)
            self._emit_insn_func(self.container.tensor_map.get("cast_to_fp32"), 0, "phony_insn", mode=1)

    def _do_emit_insn_l1(self):
        self._do_emit_insn_al1()
        self._do_emit_insn_bl1()

    def _do_emit_insn_ub(self):
        self._do_emit_insn_aub()
        self._do_emit_insn_bub()
        self._do_emit_insn_cub()

    def _do_emit_insn_mix(self):
        """
        do emit for mix-L2 tensor
        """
        tensor_a_workspace = self.container.mix_workspace_tensor.get("tensor_a_workspace")
        tensor_b_workspace = self.container.mix_workspace_tensor.get("tensor_b_workspace")
        if self.status_controller.nz_fusion_flag:
            if tensor_a_workspace is not None:
                self.sch[tensor_a_workspace].emit_insn(self.sch[tensor_a_workspace].leaf_iter_vars[-4], "dma_copy")
            if tensor_b_workspace is not None:
                self.sch[tensor_b_workspace].emit_insn(self.sch[tensor_b_workspace].leaf_iter_vars[-4], "dma_copy")
        elif self.status_controller.pad_flag:
            if tensor_a_workspace is not None:
                self.sch[tensor_a_workspace].emit_insn(self.sch[tensor_a_workspace].leaf_iter_vars[-2], "dma_copy")
            if tensor_b_workspace is not None:
                self.sch[tensor_b_workspace].emit_insn(self.sch[tensor_b_workspace].leaf_iter_vars[-2], "dma_copy")
            al1_attach_flag = self.status_controller.attach_at_flag.get("al1_attach_flag")
            bl1_attach_flag = self.status_controller.attach_at_flag.get("bl1_attach_flag")
            # clean the whole L1 space
            if al1_attach_flag == 0:
                al1 = self.container.tensor_map.get('a_l1')
                axis = self.sch[al1].leaf_iter_vars
                self.sch.memset(axis[0], 0, tbe_platform_info.scope_cbuf)
            elif bl1_attach_flag == 0:
                bl1 = self.container.tensor_map.get('b_l1')
                axis = self.sch[bl1].leaf_iter_vars
                self.sch.memset(axis[0], 0, tbe_platform_info.scope_cbuf)
            else:
                tensor_c_gm = self.container.tensor_map.get('c_gm')
                axis = self.sch[tensor_c_gm].leaf_iter_vars
                self.sch.memset(axis[1], 0, tbe_platform_info.scope_cbuf)
        else:
            self._emit_insn_func(tensor_a_workspace, 0, "dma_copy", mode=0)
            self._emit_insn_func(tensor_b_workspace, 0, "dma_copy", mode=0)

    def _do_emit_insn(self):
        self._do_emit_insn_ub()
        self._emit_insn_nz_to_nd()
        self._do_emit_insn_l1()
        self._do_emit_insn_l0a()
        self._do_emit_insn_l0b()

        # fusion
        c_ub_fract = self.container.tensor_map.get("c_ub_fract")
        if c_ub_fract is not None and not self.status_controller.support_fix_pipe_l0c2ub:
            self._emit_insn_func(self.container.tensor_map.get("c_ub_fract"), 0, "dma_copy", mode=1)
        if not self.status_controller.support_fix_pipe_l0c2out:
            self._choose_dma_copy_for_res()
        self._do_emit_insn_l0c()
        self.sch_agent.apply()
        # fusion
        self._quantify_fusion_entry()
        self._tensor_a_l1_workspace_emit()
        self._emit_insn_elemwise_tensor()
        self._do_emit_insn_output()

        if self.status_controller.reduce_fusion:
            self._emit_insn_func(self.container.tensor_map.get("res_atomic_add_ub"), 0, "dma_copy", mode=1)

        self._emit_insn_after_split_k()
        self._do_emit_insn_bias_table()
        self._do_emit_insn_fixpipe()
        self._do_emit_insn_mix()

    def _do_emit_insn_multi_output(self):
        if self.status_controller.gm_ub is not None:
            if not self.para_map.get("matmul_multi_output_flag", False):
                self._emit_insn_func(self.container.tensor_map.get("c_gm"), 0, "dma_copy", mode=1)
            else:
                self._emit_insn_for_multi_output()
            self._emit_insn_func(self.status_controller.gm_ub, 0, "phony_insn", mode=1)
        elif self.container.double_out_tensor:
            self._emit_insn_for_multi_output()
            self._emit_insn_func(self.container.double_out_tensor[0], 0, "dma_copy", mode=1)
            self._emit_insn_func(self.root_tensor, 0, "phony_insn", mode=1)

    def _emit_insn_for_multi_output(self):
        if len(self.container.tensor_map.get("c_gm").shape) in (4, 5):
            gm_n_outer, gm_n_inner = self.sch_agent[self.container.tensor_map.get("c_gm")].split(
                self.container.tensor_map.get("c_gm").op.axis[-4], nparts=1)
            gm_m_outer, gm_m_inner = self.sch_agent[self.container.tensor_map.get("c_gm")].split(
                self.container.tensor_map.get("c_gm").op.axis[-3], nparts=1)
            self.sch_agent[self.container.tensor_map.get("c_gm")].reorder(
                gm_n_outer, gm_m_outer, gm_n_inner, gm_m_inner)
            if self.para_map.get("fusion_multi_output_flag", False):
                self.sch_agent[self.container.tensor_map.get("c_gm")].emit_insn(gm_n_inner, "phony_insn")
            else:
                self.sch_agent[self.container.tensor_map.get("c_gm")].emit_insn(gm_n_inner, "dma_copy")
        else:
            gm_n_outer, gm_n_inner = self.sch_agent[self.container.tensor_map.get("c_gm")].split(
                self.container.tensor_map.get("c_gm").op.axis[-1], nparts=1)
            gm_m_outer, gm_m_inner = self.sch_agent[self.container.tensor_map.get("c_gm")].split(
                self.container.tensor_map.get("c_gm").op.axis[-2], nparts=1)
            self.sch_agent[self.container.tensor_map.get("c_gm")].reorder(
                gm_m_outer, gm_n_outer, gm_m_inner, gm_n_inner)
            self.sch_agent[self.container.tensor_map.get("c_gm")].emit_insn(gm_m_inner, "dma_copy")

    def _do_emit_insn_for_tensor_aub(self):
        if self.is_dynamic:
            # only in |batch_matmul/ matmul|nd|
            if self.status_controller.a_use_aligned_pattern and not self.status_controller.unaligned_flag:
                align_insn_dict = {"map_policy": "2d"}
                self._emit_insn_func(self.container.tensor_map.get("a_ub_aligned"), 0, "dma_copy",
                                     insn_dict=align_insn_dict, mode=1)
                self._emit_insn_func(self.container.tensor_map.get("a_ub_general"), 0, "phony_insn", mode=1)
            else:
                insn_dict = {"gm_to_ub_gap_opt": 1}
                self._emit_insn_func(self.container.tensor_map.get("a_ub_aligned"), 0, "phony_insn", mode=1)
                self._emit_insn_func(self.container.tensor_map.get("a_ub_general"), 0, "dma_copy", insn_dict=insn_dict,
                                     mode=1)
            self._emit_insn_func(self.container.tensor_map.get("a_ub"), 0, "phony_insn", mode=1)
            if self.status_controller.nz_fusion_flag in [self.NZ_VEC_A, self.NZ_VEC_AB]:
                tensor_a_ub_nz = self.container.tensor_map["a_ub_nz"]
                tensor_a_ub_nd = self.container.tensor_map["a_ub_nd"]
                self._emit_insn_func(tensor_a_ub_nz, 0, "vector_auto", mode=1)
                dma_dict = {"pad_value": tvm.const(0, dtype=tensor_a_ub_nd.dtype)}
                self._emit_insn_func(tensor_a_ub_nd, 0, "dma_copy", mode=1, insn_dict=dma_dict)
            elif self.status_controller.pad_flag in [self.PAD_A, self.PAD_AB]:
                a_ub_pad = self.container.tensor_map.get("a_ub_pad")
                dma_dict = {"pad_value": tvm.const(0, dtype=a_ub_pad.dtype)}
                self._emit_insn_func(a_ub_pad, 0, "dma_copy", mode=1, insn_dict=dma_dict)
                condition_pad = tvm.call_intrin("bool", "tir.likely", self.cache_tiling.get("k_ori") > 0)
                self.sch[a_ub_pad].set_value(condition_pad, tvm.const(0, dtype=a_ub_pad.dtype), pre_assign=True)
        else:
            # only in |gemm matmul|nd|all| or |matmul|nz|int82fp32| etc
            self._emit_insn_func(self.container.tensor_map.get("a_ub"), 0, "dma_copy", mode=1)
            self._emit_insn_func(self.container.tensor_map.get("a_ub_virtual_align"), 0, "phony_insn", mode=1)

        # only in gemm int82fp16
        self._emit_insn_func(self.container.tensor_map.get("a_int82fp16"), 0, "vector_conv", mode=1)

    def _do_compute_align(self, tensor, align_factor=None):
        if not self.status_controller.support_fix_pipe_l0c2out or tensor is None or len(tensor.op.axis) < 2:
            return
        if align_factor is None or not isinstance(align_factor, int):
            align_factor = tbe_platform.CUBE_MKN.get(tensor.dtype).get("mac")[1]
        if not self.status_controller.support_fix_pipe_l0c2ub:
            self.sch[tensor].compute_align(tensor.op.axis[-1], align_factor)
        self.sch[tensor].storage_align(tensor.op.axis[-2], align_factor, 0)

    def _emit_insn_elemwise_tensor(self):
        for ten_in in self.container.elemwise_tensors:
            if ten_in.op.tag.find("|") != -1:
                str_list = ten_in.op.tag.split("|")
                insn = self.emit_insn_map.get(str_list[0])
            else:
                insn = self.emit_insn_map.get(ten_in.op.tag)
            if ten_in in self.container.ele_header_ub_tensors:
                insn = "dma_copy"
            if insn is None:
                insn = "vector_auto"
            self._do_compute_align(ten_in)
            self._emit_insn_func(ten_in, 0, insn)

    def _do_emit_insn_ub_transpose(self, tensor):
        if tensor in (None, []):
            return
        tensor_len = len(tensor.shape)
        axit_list = [self.sch[tensor].leaf_iter_vars[-1], self.sch[tensor].leaf_iter_vars[-2]]
        self._combine_cce_pragma(tensor, axit_list)
        if tensor_len > 4:
            axit_list_1 = [self.sch[tensor].leaf_iter_vars[i] for i in range(tensor_len - 3)]
            self._combine_cce_pragma(tensor, axit_list_1)
        perm = [i for i in range(tensor_len - 3)]
        perm.extend([tensor_len - 2, tensor_len - 1, tensor_len - 3])
        attr = tvm.call_intrin("handle", "tir.tvm_tuple", *perm)
        attrs = {"src_in_dst_order": attr, "is_trans_align": 1}
        self._emit_insn_func(tensor, 0, "vector_transpose", insn_dict=attrs)

    def _do_emit_insn_aub(self):
        offset_a = 1 if self.status_controller.have_batch_a else 0
        self._do_emit_insn_for_tensor_aub()

        a_cast_and_reshape = (
            (self.status_controller.ops_data_flow_mode == "int82fp32") and (self.format_info.get("a") == "FRACTAL_NZ"))
        a_only_reshape = (
            (self.para_map.get("mmad_mode") in ("gevm", "gemv")) or (self.get_a_matrix_mode == "nd2Zz_int8"))
        a_only_reshape = a_only_reshape or (self.get_a_matrix_mode == "nd2Zz" and self.is_dynamic)
        a_ub_fract = self.container.tensor_map.get("a_ub_fract")
        if a_cast_and_reshape:
            # only in |gemm matmul|nz|int82fp32|
            if self.status_controller.have_batch_a:
                _, a_ub_scope_outer, a_ub_scope_inner, _, _ = self.sch_agent[a_ub_fract].get_active_scopes()
            else:
                a_ub_scope_outer, a_ub_scope_inner, _, _ = self.sch_agent[a_ub_fract].get_active_scopes()
            self.sch_agent[a_ub_fract].split(a_ub_scope_inner, 2)
            self.sch_agent[a_ub_fract].emit_insn(a_ub_scope_outer, "vector_auto")
        elif a_only_reshape:
            # The pass side causes the vector instruction to have performance regression in some scenarios,
            # so this restriction is added
            reshape_cmd = "vector_muls" if ((self.format_info.get("a") == "ND" and self.format_info.get("b") == "ND")
                                            or self.binary_constant or self.is_dynamic) else "dma_copy"
            self._emit_insn_func(a_ub_fract, 0, reshape_cmd, insn_dict=self.container.vector_muls_attr)
        else:
            # only in |gemm matmul|ND|fp162fp16 fp162fp32 int82fp32|
            if self.status_controller.unaligned_flag:
                self._do_emit_insn_ub_transpose(a_ub_fract)
            else:
                self._emit_insn_func(a_ub_fract, 1 + offset_a, "vnchwconv", mode=1)

        # only in |gemm|ND|int82int32|
        a_transpose = self.container.tensor_map.get("a_transpose")
        if a_transpose is not None:
            m_outer, m_inner = self.sch_agent[a_transpose].split(a_transpose.op.axis[1 + offset_a], factor=32)
            self.sch_agent[a_transpose].reorder(m_outer, a_transpose.op.axis[offset_a], m_inner)
            self.sch_agent[a_transpose].emit_insn(self.sch_agent[a_transpose].op.axis[offset_a], "vnchwconv")

    def _do_emit_insn_for_tensor_bub(self):
        if self.is_dynamic:
            # only in |batch_matmul/matmul|nd|
            if self.status_controller.b_use_aligned_pattern and not self.status_controller.unaligned_flag:
                align_insn_dict = {"map_policy": "2d"}
                self._emit_insn_func(self.container.tensor_map.get("b_ub_aligned"), 0, "dma_copy",
                                     insn_dict=align_insn_dict, mode=1)
                self._emit_insn_func(self.container.tensor_map.get("b_ub_general"), 0, "phony_insn", mode=1)
            else:
                insn_dict = {"gm_to_ub_gap_opt": 1}
                self._emit_insn_func(self.container.tensor_map.get("b_ub_aligned"), 0, "phony_insn", mode=1)
                self._emit_insn_func(self.container.tensor_map.get("b_ub_general"), 0, "dma_copy", insn_dict=insn_dict,
                                     mode=1)
            self._emit_insn_func(self.container.tensor_map.get("b_ub"), 0, "phony_insn", mode=1)
            if self.status_controller.nz_fusion_flag in [self.NZ_VEC_B, self.NZ_VEC_AB]:
                tensor_b_ub_nz = self.container.tensor_map["b_ub_nz"]
                tensor_b_ub_nd = self.container.tensor_map["b_ub_nd"]
                self._emit_insn_func(tensor_b_ub_nz, 0, "vector_auto", mode=1)
                dma_dict = {"pad_value": tvm.const(0, dtype=tensor_b_ub_nd.dtype)}
                self._emit_insn_func(tensor_b_ub_nd, 0, "dma_copy", mode=1, insn_dict=dma_dict)
            elif self.status_controller.pad_flag in [self.PAD_B, self.PAD_AB]:
                b_ub_pad = self.container.tensor_map.get("b_ub_pad")
                dma_dict = {"pad_value": tvm.const(0, dtype=b_ub_pad.dtype)}
                self._emit_insn_func(b_ub_pad, 0, "dma_copy", mode=1, insn_dict=dma_dict)
                condition_pad = tvm.call_intrin("bool", "tir.likely", self.cache_tiling.get("k_ori") > 0)
                self.sch[b_ub_pad].set_value(condition_pad, tvm.const(0, dtype=b_ub_pad.dtype), pre_assign=True)
        else:
            # only in |gemm matmul|nd|all| or |matmul|nz|int82fp32| etc
            self._emit_insn_func(self.container.tensor_map.get("b_ub"), 0, "dma_copy", mode=1)

        # only in gemm int82fp16
        self._emit_insn_func(self.container.tensor_map.get("b_int82fp16"), 0, "vector_conv", mode=1)

    def _do_emit_insn_bub(self):
        offset_b = 1 if self.status_controller.have_batch_b else 0
        self._do_emit_insn_for_tensor_bub()

        b_cast_and_reshape = (
            (self.status_controller.ops_data_flow_mode == "int82fp32") and (self.format_info.get("b") == "FRACTAL_Z"))
        b_only_reshape = (self.para_map.get("mmad_mode") == "gemv") or (self.get_b_matrix_mode == "nd2Zn_int8")
        b_only_reshape = b_only_reshape or (self.get_b_matrix_mode == "nd2Zn" and self.is_dynamic)
        b_ub_fract = self.container.tensor_map.get("b_ub_fract")
        if b_cast_and_reshape:
            # only in |gemm matmul|nz|int82fp32|
            if self.status_controller.have_batch_b:
                _, b_ub_scope_outer, _, _, _ = self.sch_agent[b_ub_fract].get_active_scopes()
            else:
                b_ub_scope_outer, _, _, _ = self.sch_agent[b_ub_fract].get_active_scopes()
            b_ub_outer_outer, _ = self.sch_agent[b_ub_fract].split(b_ub_scope_outer, 2)
            self.sch_agent[b_ub_fract].emit_insn(b_ub_outer_outer, "vector_auto")
        elif b_only_reshape:
            # The pass side causes the vector instruction to have performance regression in some scenarios,
            # so this restriction is added
            reshape_cmd = "vector_muls" if ((self.format_info.get("a") == "ND" and self.format_info.get("b") == "ND")
                                            or self.binary_constant) else "dma_copy"
            self._emit_insn_func(b_ub_fract, 0, reshape_cmd, insn_dict=self.container.vector_muls_attr)
        else:
            # only in |gemm matmul|ND|fp162fp16 fp162fp32 int82fp32|
            if self.status_controller.unaligned_flag:
                self._do_emit_insn_ub_transpose(b_ub_fract)
            else:
                self._emit_insn_func(b_ub_fract, 1 + offset_b, "vnchwconv", mode=1)

        # only in |gemm|ND|int82int32|
        b_transpose = self.container.tensor_map.get("b_transpose")
        if b_transpose is not None:
            k_outer, k_inner = self.sch_agent[b_transpose].split(b_transpose.op.axis[1 + offset_b], factor=32)
            self.sch_agent[b_transpose].reorder(k_outer, b_transpose.op.axis[offset_b], k_inner)
            self.sch_agent[b_transpose].emit_insn(self.sch_agent[b_transpose].op.axis[offset_b], "vnchwconv")

    def _emit_insn_nz_to_nd(self):
        c_add_bias_ub = self.container.tensor_map.get("c_add_bias_ub")
        nz_to_nd = self.container.tensor_map.get("nz_to_nd")
        nz_to_nd_fp32 = self.container.tensor_map.get("nz_to_nd_fp32")
        fract_add_nd_to_nd = (self.format_info.get("out") == "ND") and (c_add_bias_ub is not None)

        if fract_add_nd_to_nd:
            self._cut_axis_for_nz_to_nd(c_add_bias_ub, "vector_add")
        elif c_add_bias_ub is not None:
            # only in |gemm|Nz|all data type|
            self._emit_insn_func(c_add_bias_ub, 0, "vector_add", mode=1)

        if nz_to_nd is not None:
            self._cut_axis_for_nz_to_nd(nz_to_nd, "vector_muls", attrs=self.container.vector_muls_attr)
        if nz_to_nd_fp32 is not None:
            self._cut_axis_for_nz_to_nd(nz_to_nd_fp32, "vector_muls", attrs=self.container.vector_muls_attr)

    def _cut_axis_for_nz_to_nd(self, ori_tensor, emit_insn_cmd, attrs=None):
        if attrs is None:
            attrs = {}
        # only in |gemm|ND|all data type|
        if self.status_controller.have_batch:
            scope_batch, scope_outer, scope_inner = self.sch_agent[ori_tensor].get_active_scopes()
        else:
            scope_outer, scope_inner = self.sch_agent[ori_tensor].get_active_scopes()
        split_params = SplitParam(
            self.tiling_work.get_split_param(self.cache_tiling_mgr),
            "guard_with_if", "outer")
        outer_outer, outer_inner = self.sch_agent[ori_tensor].split(scope_outer, self.block_in,
                                                                    split_params=split_params)
        inner_outer, inner_inner = self.sch_agent[ori_tensor].split(scope_inner, self.block_out,
                                                                    split_params=split_params)
        self.sch_agent[ori_tensor].reorder(outer_outer, inner_outer, outer_inner, inner_inner)
        if self.cache_tiling:
            m_inner_outer, _ = self.sch_agent[ori_tensor].split(outer_inner, self.BLOCKS_PER_REPEAT,
                                                                split_params=split_params)
            self.sch_agent[ori_tensor].reorder(m_inner_outer, outer_outer, inner_outer)
            self.sch_agent[ori_tensor].emit_insn(outer_outer, emit_insn_cmd, attrs=attrs)
        elif self.status_controller.have_batch:
            self.sch_agent[ori_tensor].emit_insn(scope_batch, emit_insn_cmd, attrs=attrs)
        else:
            self.sch_agent[ori_tensor].emit_insn(outer_inner, emit_insn_cmd, attrs=attrs)

    def _add_key_value(self, key, value):
        buffer_reuse_dict = self.container.buffer_reuse_dict
        if (key is not None) and self._check_reused_none(value):
            buffer_reuse_dict[key] = value

    def _set_buffer_reuse_dict(self):
        self._add_key_value(self.container.tensor_map.get("c_ub_fract"), self.container.tensor_map.get("alpha_c"))
        if self.format_info.get("out") == "FRACTAL_NZ":
            self._add_key_value(
                self.container.tensor_map.get("c_add_bias_ub"), self.container.tensor_map.get("alpha_c"))
        else:
            self._add_key_value(
                self.container.tensor_map.get("c_add_bias_ub"), self.container.tensor_map.get("beta_bias"))
        if self.status_controller.pad_flag == self.PAD_AB:
            self._add_key_value(self.container.tensor_map.get("a_ub_pad"), self.container.tensor_map.get("b_ub_pad"))
        if self.status_controller.nz_fusion_flag == self.NZ_VEC_AB:
            self._add_key_value(self.container.tensor_map.get("a_ub_nd"), self.container.tensor_map.get("b_ub_nd"))
            self._add_key_value(self.container.tensor_map.get("a_ub_nz"), self.container.tensor_map.get("b_ub_nz"))
        if self.cache_tiling:
            self._add_key_value(
                self.container.tensor_map.get("cast_to_fp16"), self.container.tensor_map.get("cast_to_fp32"))
            self._add_key_value(
                self.container.tensor_map.get("c_add_bias_ub_fp16"), self.container.tensor_map.get("cast_to_fp16"))
            self._add_key_value(
                self.container.tensor_map.get("c_add_bias_ub_fp32"), self.container.tensor_map.get("cast_to_fp32"))
            tensor_bias_cast_fp16 = self.container.tensor_map.get("bias_ub_drnn_cast_fp16")
            tensor_bias_cast_fp32 = self.container.tensor_map.get("bias_ub_drnn_cast_fp32")
            bias_tensor_list = [self.container.tensor_map.get("bias_ub_fp16"), ]
            if tensor_bias_cast_fp16 not in self.container.compute_inline_list:
                bias_tensor_list.append(tensor_bias_cast_fp16)
            if tensor_bias_cast_fp32 not in self.container.compute_inline_list:
                bias_tensor_list.append(tensor_bias_cast_fp32)
            self._add_key_value(self.container.tensor_map.get("bias_ub_fp32"), bias_tensor_list)
            self._add_key_value(
                self.container.tensor_map.get("nz_to_nd"), self.container.tensor_map.get("nz_to_nd_fp32"))
            self._add_key_value(self.root_tensor,
                [self.container.tensor_map.get("tensor_out_fp16"), self.container.tensor_map.get("tensor_out_fp32")])

        if self.status_controller.ops_data_flow_mode == "fp162fp16":
            self._add_key_value(
                self.container.tensor_map.get("beta_fp162fp32"), self.container.tensor_map.get("beta_ub"))
            self._add_key_value(
                self.container.tensor_map.get("alpha_fp162fp32"), self.container.tensor_map.get("alpha_ub"))
            self._add_key_value(
                self.container.tensor_map.get("beta_bias"), self.container.tensor_map.get("bias_cast_to_fp32"))
        else:
            self._add_key_value(self.container.tensor_map.get("beta_bias"), self.container.tensor_map.get("bias_ub"))

        if self.status_controller.need_init_bias:
            self._add_key_value(self.container.tensor_map.get("virtual_add_bias"),
                [self.container.tensor_map.get("bias_ub"), self.container.tensor_map.get("init_value_of_bias_ub")])

        if self.para_map.get("fusion_multi_output_flag", False):
            self._add_key_value(self.container.tensor_map.get("cast_to_fp16"), self.container.tensor_map.get("c_gm"))
        # Enable aub/bub to select schedule pattern
        a_ub = self.container.tensor_map.get("a_ub")
        b_ub = self.container.tensor_map.get("b_ub")
        if a_ub is not None and self.is_dynamic:
            self._add_key_value(
                self.container.tensor_map.get("a_ub_aligned"), [a_ub, self.container.tensor_map.get("a_ub_general")])
        if b_ub is not None and self.is_dynamic:
            self._add_key_value(
                self.container.tensor_map.get("b_ub_aligned"), [b_ub, self.container.tensor_map.get("b_ub_general")])

        self._add_key_value(
            self.container.tensor_map.get("bias_zero"), self.container.tensor_map.get("bias_l1"))

    def _do_buffer_reuse(self):
        for bereused_tensor, tensor in self.container.buffer_reuse_dict.items():
            if (bereused_tensor is not None) and (tensor is not None):
                if isinstance(tensor, Iterable):
                    self.sch[bereused_tensor].reused_by(*tensor)
                else:
                    self.sch[bereused_tensor].reused_by(tensor)

    def _init_run_once_flag(self):
        a_run_once, b_run_once = self.ALLOCATE_OFF, self.ALLOCATE_OFF

        dtype_byte = self.DTYPE_WIDTH_MAP.get(self.container.tensor_map.get("a_l1").dtype) * 2
        size = tbe_platform_info.get_soc_spec("L1_SIZE") // dtype_byte // 2
        block_dim = self.tiling_work.tiling.get("block_dim")

        core_inner_m = int_ceil_div(
            self.container.tensor_map.get("a_l0a").shape[self.FRACTAL_Z_M_INDEX].value * self.block_in,
            block_dim[self.BLOCK_M_DIM_INDEX])
        core_inner_n = int_ceil_div(
            self.container.tensor_map.get("b_l0b").shape[self.FRACTAL_Z_N_INDEX].value * self.block_out,
            block_dim[self.BLOCK_N_DIM_INDEX])
        k_shape = self.container.tensor_map.get("a_l0a").shape[self.FRACTAL_Z_KA_INDEX].value * self.block_reduce

        m_l1_shape = self.tiling_work.al1_tiling_m * self.block_in
        n_l1_shape = self.tiling_work.bl1_tiling_n * self.block_out
        m_max_num = int_ceil_div(core_inner_m, m_l1_shape) * m_l1_shape
        n_max_num = int_ceil_div(core_inner_n, n_l1_shape) * n_l1_shape

        tensor_a_num = m_max_num * k_shape
        tensor_b_num = n_max_num * k_shape
        if m_max_num * k_shape <= size:
            a_run_once = self.ALLOCATE_FULL
        elif m_l1_shape * k_shape <= size:
            a_run_once = self.ALLOCATE_HALF

        if n_max_num * k_shape <= size:
            b_run_once = self.ALLOCATE_FULL
        elif n_l1_shape * k_shape <= size:
            b_run_once = self.ALLOCATE_HALF

        if a_run_once == self.ALLOCATE_HALF and b_run_once == self.ALLOCATE_HALF:
            aprts_a = core_inner_m // m_l1_shape
            aprts_b = core_inner_n // n_l1_shape
            if tensor_a_num * aprts_b < tensor_b_num * aprts_a:
                a_run_once = self.ALLOCATE_OFF
            else:
                b_run_once = self.ALLOCATE_OFF

        batch = 0
        if self.status_controller.have_batch:
            batch = self.container.tensor_map.get("c_l0c").shape[0].value
        batch_double = False
        if batch > 1:
            if tensor_a_num <= size and tensor_b_num <= size:
                batch_double = True

        double_once = self.ALLOCATE_OFF
        if core_inner_m != m_l1_shape and core_inner_n != n_l1_shape:
            double_once = self.ALLOCATE_HALF

        return a_run_once, b_run_once, batch_double, double_once

    def _allocate_axis(self, enable_nbuffer):
        if not enable_nbuffer or self.is_dynamic:
            return self.ALLOCATE_OFF, self.ALLOCATE_OFF
        a_run_once, b_run_once, batch_double, double_once = self._init_run_once_flag()
        axis_outer = self.sch_agent[self.root_tensor].get_active_scopes()
        if self.format_info.get("out") == "FRACTAL_NZ":
            m_outer = axis_outer[self.FRACTAL_NZ_M_INDEX]
            n_outer = axis_outer[self.FRACTAL_NZ_N_INDEX]
        else:
            m_outer = axis_outer[self.ND_M_INDEX]
            n_outer = axis_outer[self.ND_N_INDEX]
        m_outer, n_outer = (n_outer, m_outer) if self.para_map.get("mmad_mode") == "gemv" else (m_outer, n_outer)
        out_axis = [m_outer, n_outer]
        if (self.status_controller.al1_attach_status == "full_load"
            or (self.status_controller.al1_attach_status == "c_l0c"
            and self.status_controller.c_l0c_attach_status == "full_load")):
            a_run_once = self.ALLOCATE_OFF
        if (self.status_controller.bl1_attach_status == "full_load"
            or (self.status_controller.bl1_attach_status == "c_l0c"
                and self.status_controller.c_l0c_attach_status == "full_load")):
            b_run_once = self.ALLOCATE_OFF
        if batch_double:
            if double_once != self.ALLOCATE_OFF:
                a_run_once, b_run_once = self._do_allocate_axis(a_run_once, b_run_once, out_axis)
        else:
            a_run_once, b_run_once = self._do_allocate_axis(a_run_once, b_run_once, out_axis)

        return a_run_once, b_run_once

    def _do_allocate_axis(self, a_run_once, b_run_once, out_axis):
        m_outer, n_outer = out_axis
        al1_ddr_to_l1_flag = (self.in_addr_type == 0 and (not self.status_controller.l1_fusion_and_l1_size_0)
                              and self.status_controller.input_l1_flag != 1)
        if a_run_once != self.ALLOCATE_OFF and al1_ddr_to_l1_flag:
            tensor_a_l1 = self.container.tensor_map.get("a_l1")
            self.sch[tensor_a_l1].allocate_at(self.sch[self.root_tensor], n_outer, run_once_axes=[n_outer])
            self.sch[tensor_a_l1].mem_unique()
        else:
            a_run_once = self.ALLOCATE_OFF
        # unzip donot support allocate_at
        if (b_run_once != self.ALLOCATE_OFF and (not self.status_controller.l1_fusion_and_l1_size_0)
            and (not self.para_map.get("compress_flag", False))):
            tensor_b_l1 = self.container.tensor_map.get("b_l1")
            self.sch[tensor_b_l1].allocate_at(self.sch[self.root_tensor], m_outer, run_once_axes=[m_outer])
            self.sch[tensor_b_l1].mem_unique()
        else:
            b_run_once = self.ALLOCATE_OFF
        return a_run_once, b_run_once

    def _reorder_axis(self, enable_nbuffer, tensor_a_reuse_local, tensor_b_reuse_local):
        not_need_nbuffer = (tensor_a_reuse_local == self.ALLOCATE_OFF) and (tensor_b_reuse_local == self.ALLOCATE_OFF)
        if not enable_nbuffer or not_need_nbuffer:
            return
        axis_outer = self.sch_agent[self.root_tensor].get_active_scopes()
        if self.format_info.get("out") == "FRACTAL_NZ":
            m_outer = axis_outer[self.FRACTAL_NZ_M_INDEX]
            n_outer = axis_outer[self.FRACTAL_NZ_N_INDEX]
            l1_reuse_axis_outter = n_outer
            l1_reuse_axis_inner = m_outer
        else:
            m_outer = axis_outer[self.ND_M_INDEX]
            n_outer = axis_outer[self.ND_N_INDEX]
            l1_reuse_axis_outter = m_outer
            l1_reuse_axis_inner = n_outer
        if tensor_a_reuse_local == self.ALLOCATE_HALF and tensor_b_reuse_local != self.ALLOCATE_HALF:
            l1_reuse_axis_outter = m_outer
            l1_reuse_axis_inner = n_outer
        elif tensor_a_reuse_local != self.ALLOCATE_HALF and tensor_b_reuse_local == self.ALLOCATE_HALF:
            l1_reuse_axis_outter = n_outer
            l1_reuse_axis_inner = m_outer
        elif tensor_a_reuse_local == self.ALLOCATE_HALF and tensor_b_reuse_local == self.ALLOCATE_HALF:
            l1_reuse_axis_outter = m_outer
            l1_reuse_axis_inner = n_outer

        self._do_reorder_axis(l1_reuse_axis_outter, l1_reuse_axis_inner)

    def _do_reorder_axis(self, outer_axis, inner_axis):
        axis_outer = self.sch_agent[self.root_tensor].get_active_scopes()
        reorder_list = [outer_axis, inner_axis]
        if self.format_info.get("out") == "FRACTAL_NZ":
            reorder_list.append(axis_outer[self.FRACTAL_NZ_M0_INDEX])
            reorder_list.append(axis_outer[self.FRACTAL_NZ_N0_INDEX])
        if self.status_controller.have_batch:
            reorder_list.insert(0, axis_outer[0])
        self.sch[self.root_tensor].reorder(*reorder_list)

        if self.format_info.get("out") == "ND":
            tensor_at_res_stage = []
            attach_dict = self.sch_agent.get_attach_dict()
            for i, j in attach_dict.items():
                if j.op.name == self.root_tensor.op.name:
                    tensor_at_res_stage.append(i)
            tensor_at_res = []
            for tensor_stage in tensor_at_res_stage:
                for tensor in self.container.tensor_map.values():
                    if tensor is None:
                        continue
                    if tensor_stage.op.name == tensor.op.name:
                        tensor_at_res.append(tensor)
                        break
            for i in tensor_at_res:
                self.sch[i].compute_at(self.sch[self.res], inner_axis)

    def _do_aub_storage_align(self):
        """
        solve the bank conflict of aub
        """
        # the data gap in ub
        gap_value = self.block_reduce
        tiling = self.tiling_work.tiling
        # solve bank conflict in aub/bub
        aub_k, aub_m, _, _ = tiling.get("AUB_shape")
        aub_m *= self.block_in
        # the data stride in ub
        a_align_value = (aub_m + gap_value) if self.status_controller.transpose_a else (aub_k + gap_value)
        if self.status_controller.transpose_a:
            self.status_controller.storage_m_bound_change = True
        else:
            self.status_controller.storage_ka_bound_change = True
        a_int82fp16 = self.container.tensor_map.get("a_int82fp16")
        a_normalize_ub = self.container.tensor_map.get("a_ub")
        # when the Inner axis is K and attach to C_gm, k_aligned value
        # may be larger than the tiling value.
        if self.status_controller.aub_attach_status == "c_gm" and not self.status_controller.transpose_a:
            self.status_controller.cgm_ka_storage_change = True
            max_k_bound = self.tiling_work.get_a_max_k_bound(self.container.tensor_map.get("a_l0a").shape)
            a_align_value = tvm.select(max_k_bound % self.THRESHOLD_DATA_NUM == 0,
                                       max_k_bound + gap_value, 1)
        if self.cache_tiling:
            a_align_value = self.cache_tiling.get("a_align_value")

        if a_int82fp16 is not None:
            self.sch[a_int82fp16].storage_align(a_int82fp16.op.axis[-2], a_align_value, 0)
        elif (a_normalize_ub.dtype == "float16") or (self.container.tensor_map.get("a_transpose") is not None):
            self.sch[a_normalize_ub].storage_align(a_normalize_ub.op.axis[-2], a_align_value, 0)
            if self.is_dynamic:
                a_ub_aligned = self.container.tensor_map.get("a_ub_aligned")
                a_ub_general = self.container.tensor_map.get("a_ub_general")
                self.sch[a_ub_aligned].storage_align(a_ub_aligned.op.axis[-2], a_align_value, 0)
                self.sch[a_ub_general].storage_align(a_ub_general.op.axis[-2], a_align_value, 0)

    def _do_bub_storage_align(self):
        """
        solve bub bank conflict by storage align
        """
        # the data gap in ub
        gap_value = self.block_reduce
        tiling = self.tiling_work.tiling
        # solve bank conflict in aub/bub
        bub_k, bub_n, _, _ = tiling.get("BUB_shape")
        bub_n *= self.block_out
        # the data stride in ub
        b_align_value = (bub_k + gap_value) if self.status_controller.transpose_b else (bub_n + gap_value)
        if self.status_controller.transpose_b:
            self.status_controller.storage_kb_bound_change = True
        else:
            self.status_controller.storage_n_bound_change = True
        b_int82fp16 = self.container.tensor_map.get("b_int82fp16")
        b_normalize_ub = self.container.tensor_map.get("b_ub")
        # when the Inner axis is K and attach to C_gm, k_aligned value may be larger than the tiling value.
        if self.status_controller.bub_attach_status == "c_gm" and self.status_controller.transpose_b:
            max_k_bound = self.tiling_work.get_b_max_k_bound(self.container.tensor_map.get("b_l0b"),
                                                             self.is_dynamic, self.dynamic_k)
            b_align_value = tvm.select(max_k_bound % self.THRESHOLD_DATA_NUM == 0, max_k_bound + gap_value, 1)
        if self.cache_tiling:
            b_align_value = self.cache_tiling.get("b_align_value")
        b_transpose = self.container.tensor_map.get("b_transpose")
        if b_int82fp16 is not None:
            self.sch[b_int82fp16].storage_align(b_int82fp16.op.axis[-2], b_align_value, 0)
        elif (b_normalize_ub.dtype == "float16") or (b_transpose is not None):
            self.sch[b_normalize_ub].storage_align(b_normalize_ub.op.axis[-2], b_align_value, 0)
            if self.is_dynamic:
                b_ub_aligned = self.container.tensor_map.get("b_ub_aligned")
                b_ub_general = self.container.tensor_map.get("b_ub_general")
                self.sch[b_ub_aligned].storage_align(b_ub_aligned.op.axis[-2], b_align_value, 0)
                self.sch[b_ub_general].storage_align(b_ub_general.op.axis[-2], b_align_value, 0)

    def _solve_bank_conflict(self):
        """
        solve bank conflict by storage_align
        if aub_k or bub_n bigger than THRESHOLD_DATA_NUM,
        use storage_align to solve bank conflict of aub/bub

        c_ub always conflict, must be use storage_align
        Input: None
        ---------------------------------
        Return: None
        """
        tiling = self.tiling_work.tiling
        params = {
            "container": self.container,
            "status_controller": self.status_controller,
            "format_out": self.format_info.get("out"),
            "format_a": self.format_info.get("a"),
            "format_b": self.format_info.get("b"),
            "cache_tiling": self.cache_tiling
        }
        a_ub_storage_align, b_ub_storage_align, c_ub_storage_align = self.buffer_checker.check_exceed_ub(tiling, params)
        # three point not consider now:
        # 1. Although tiling does not lead to bank conflict, may lead to bank conflict after align in dynamic
        # 2. same to 1.,tiling lead to bank conflict, may not lead to bank conflict after align in dynamic
        # 3. When the tensor is on the c_gm, bank conflict is not enabled
        a_ub_storage_align, b_ub_storage_align = self._disable_solve_bank_conflict_in_dynamic(
            a_ub_storage_align, b_ub_storage_align)
        if a_ub_storage_align or (self.cache_tiling and self.para_map.get("need_aub")):
            self._do_aub_storage_align()
        if b_ub_storage_align or (self.cache_tiling and self.para_map.get("need_bub")):
            self._do_bub_storage_align()
        # solve bank conflict in cub
        self._solve_bank_conflict_cub(c_ub_storage_align)

    def _solve_bank_conflict_cub(self, c_ub_storage_align):
        nz_to_nd = self.container.tensor_map.get("nz_to_nd")
        if (self.cache_tiling and self.format_info.get("out") == "ND" and nz_to_nd is not None and \
                not self.status_controller.unaligned_flag):
            # solve bank conflict
            aixs_idx = 1 if self.status_controller.have_batch else 0

            cub_n1 = self.cache_tiling.get("cub_n1")
            axis_stride = tvm.select(
                self.cache_tiling.get("flag_cub_solving_bank_conflict") == 1, (cub_n1 + 1) * self.block_in,
                cub_n1 * self.block_in)
            self.sch[nz_to_nd].bind_buffer(nz_to_nd.op.axis[aixs_idx], axis_stride, offset=0)
        if c_ub_storage_align:
            before_c_gm = self.container.tensor_map.get("before_c_gm")
            cast_to_fp16 = self.container.tensor_map.get("cast_to_fp16")
            if (before_c_gm is not None) and (cast_to_fp16 is not None):
                c_gap_value = self.block_out * self.block_in * self.tiling_work.tiling.get(
                    "CUB_matrix")[1] + self.block_out
                self.sch[cast_to_fp16].storage_align(cast_to_fp16.op.axis[-4], c_gap_value, 0)
                if self.is_dynamic:
                    cur_bound_bound = c_gap_value * self.tiling_work.tiling.get("CUB_matrix")[0]
                    self.sch[cast_to_fp16].set_buffer_size(cur_bound_bound)
            else:
                c_gap_value = (self.block_out + 1) * self.block_in
                c_ub_fract = self.container.tensor_map.get("c_ub_fract")
                alpha_c_ub = self.container.tensor_map.get("alpha_c")
                if c_ub_fract not in self.container.compute_inline_list:
                    self.sch[c_ub_fract].storage_align(c_ub_fract.op.axis[-3], c_gap_value, 0)
                    if alpha_c_ub is not None:
                        self.sch[alpha_c_ub].storage_align(alpha_c_ub.op.axis[-3], c_gap_value, 0)

    def _disable_solve_bank_conflict_in_dynamic(self, a_ub_storage_align, b_ub_storage_align):
        if (self.status_controller.aub_attach_status == "c_gm"
            and not self.status_controller.transpose_a and self.is_dynamic):
            a_ub_storage_align = False
        if (self.status_controller.bub_attach_status == "c_gm"
            and self.status_controller.transpose_b and self.is_dynamic):
            b_ub_storage_align = False

        return a_ub_storage_align, b_ub_storage_align

    def _renew_block_dim(self):
        """
        if tail data small then 16(output=fp16) or 32(output=int32)
        close multi core
        """
        c_gm = self.container.tensor_map.get("c_gm")
        if self.status_controller.ops_data_flow_mode == "int82int32":
            multi_core_min_slice = 32
        else:
            multi_core_min_slice = 16

        if (c_gm.shape[1].value * self.OUTPUT_SIZE.get(
            self.status_controller.ops_data_flow_mode) < multi_core_min_slice):
            self.tiling_work.tiling["block_dim"] = [1, 1, 1, 1]

    def _do_compute_inline(self):
        self.container.elewise_compute_inline_list += self.container.compute_inline_list
        for tensor in self.container.elewise_compute_inline_list:
            if tensor in self.container.double_out_tensor:
                continue
            self.sch[tensor].compute_inline(instant=True)

    def _set_requant_transfer_buffer_align(self):
        if not self.para_map.get("requant_fusion", False):
            return
        requant_data_transfer = self.container.tensor_map.get("tensor_reform")
        unchanged = 1
        if self.status_controller.have_batch:
            self.sch[requant_data_transfer].buffer_align((unchanged, unchanged),
                                                        (unchanged, unchanged),
                                                        (unchanged, unchanged),
                                                        (unchanged, 16),
                                                        (unchanged, 16))
        else:
            self.sch[requant_data_transfer].buffer_align((unchanged, unchanged),
                                                        (unchanged, unchanged),
                                                        (unchanged, 16),
                                                        (unchanged, 16))
        return

    def _do_buffer_align_l0c(self):
        c_l0c = self.container.tensor_map.get("c_l0c")
        block_reduce = self.block_reduce
        block_out = self.block_out
        unchanged = 1
        if self.para_map.get("mmad_mode") in ("gevm", "gemv"):
            align_args = [
                (unchanged, unchanged),
                (unchanged, unchanged),
                (unchanged, block_out),
                (unchanged, block_out),
                (unchanged, unchanged),
                (unchanged, block_reduce)
            ]
        else:
            align_args = [
                (unchanged, unchanged),
                (unchanged, unchanged),
                (unchanged, tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][0]),
                (unchanged, tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][2]),
                (unchanged, unchanged),
                (unchanged, tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][1])
            ]
        if self.status_controller.support_fix_pipe_l0c2out:
            a_l0a = self.container.tensor_map.get("a_l0a")
            b_l0b = self.container.tensor_map.get("b_l0b")
            factor_dtype_dict = {"int8":2, "int4":4}
            if self.status_controller.transpose_a and a_l0a.dtype in ("int8", "int4"):
                align_args[1] = (unchanged, factor_dtype_dict.get(a_l0a.dtype))
            if not self.status_controller.transpose_b and b_l0b.dtype in ("int8", "int4"):
                align_args[0] = (unchanged, factor_dtype_dict.get(b_l0b.dtype))
        if self.status_controller.have_batch:
            align_args.insert(0, (unchanged, unchanged))
        if self.status_controller.split_k_axis_by_tiling:
            align_args.insert(0, (unchanged, unchanged))
            align_args[-1], align_args[-2] = align_args[-2], align_args[-1]
        self.sch[c_l0c].buffer_align(*align_args)

    def _do_buffer_align_bias_bt(self):
        bias_bt = self.container.tensor_map.get("bias_bt", None)
        if bias_bt is None:
            return
        c_l0c = self.container.tensor_map.get("c_l0c")
        if not self.status_controller.transpose_b and c_l0c.dtype == "int32":
            shape_len = len(bias_bt.shape)
            # if trans_b is false n_cl0 for mmad must be even, but write to bt may is odd(tail_block),
            # This may cause the amount of read data(even) to be inconsistent with the amount of data written(odd)
            # align n1 of bias_bt to even to avoid read data large than buffer_size
            align_args = [*((1, 1), ) * (shape_len - 2), (1, 2), (1, 1)]
            self.sch[bias_bt].buffer_align(*align_args)
            if not in_dynamic():
                tiling_cl0_n = self.tiling_work.tiling.get("CL0_matrix")[0]
                block_out = tbe_platform_info.CUBE_MKN.get(bias_bt.dtype).get("mac")[2]
                n_bound = tiling_cl0_n * block_out
                self.sch[bias_bt].set_buffer_size(n_bound)

    def _do_buffer_align_fixpipe_l1(self):
        fixpipe_l1_elewise = self.container.tensor_map.get("fixpipe_l1_elewise")
        if fixpipe_l1_elewise is None:
            return
        c_l0c = self.container.tensor_map.get("c_l0c")
        align_args = [
            (1, 1),
            (1, 1),
            (1, tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][0]),
            (1, tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][2])
        ]
        if self.status_controller.have_batch:
            align_args.insert(0, (1, 1))
        self.sch[fixpipe_l1_elewise].buffer_align(*align_args)

    def _set_compress_info(self, compress_tensor, compress_index, tile_k, tile_n, out_axis):
        """
        set weigths compress info
        """
        if out_axis is None:
            raise RuntimeError("compress index axis is None, it's error.")
        engine, ratios, channel, mode = tbe_platform_info.get_soc_spec("UNZIP")
        frac_size = 512

        index_shape = compress_index.shape

        tile_k_value = compress_tensor.op.attrs["tile_L1_k"]
        tile_n_value = compress_tensor.op.attrs["tile_L1_n"]
        block_dim_n = compress_tensor.op.attrs["block_dim_n"]
        batch_dim, n_dim, m_dim, reduce_dim = self.tiling_work.tiling.get("block_dim")

        if not self.is_dynamic:
            dim_k = compress_tensor.shape[0].value
            dim_n = compress_tensor.shape[1].value
            block_size = self._get_compress_block_info(tile_k, tile_n)
            k_block_num = (dim_k + tile_k - 1) // tile_k
            n_block_num = (dim_n + tile_n - 1) // tile_n
            self.sch.set_var_range(tile_k_value, tile_k, tile_k)
            self.sch.set_var_range(tile_n_value, tile_n, tile_n)
            self.sch.set_var_range(block_dim_n, n_dim, n_dim)
            index_size = self._get_compress_index_size(mode, k_block_num, n_block_num)
        else:
            block_unit = 512
            dim_k = compress_tensor.shape[0]
            dim_n = compress_tensor.shape[1]
            block_size = tile_k * tile_n * block_unit
            k_block_num = tvm.floordiv((dim_k + tile_k - 1), tile_k)
            n_block_num = tvm.floordiv((dim_n + tile_n - 1), tile_n)
            index_size = self._get_compress_index_size(mode, k_block_num, n_block_num)

        index_size = k_block_num * n_block_num

        tight_len = 2
        if mode == 1:
            tight_len = 8
        index_size = index_size * tight_len

        conflict = tvm.call_intrin("int32", "tir.tvm_tuple",
                                   block_size, index_size, mode, engine,
                                   channel, ratios, tile_k, tile_n, n_dim)
        self.sch[compress_tensor].pragma(compress_tensor.op.axis[0],
                                    "json_info_compress_parameters", conflict)
        tensor_len = len(compress_tensor.shape)
        # transform data to continue by block_size
        al1_multi_m = 1
        if self.tiling_work.tiling.get("AL1_shape"):
            al1_multi_m = self.tiling_work.tiling.get("AL1_shape")[self.IDX_MULTI_M1]
        if not self.is_dynamic:
            self.sch.set_var_range(index_shape[0], int(index_size), int(index_size))
            unzip_params = {"compress_mode": mode, "block_size": block_size, "hoist_axis": out_axis}
            if batch_dim * n_dim * m_dim * reduce_dim > 1 or (isinstance(al1_multi_m, int) and al1_multi_m > 1):
                # unzip don't support set hoist_axis when multi core or al1_multi_m > 1
                unzip_params = {"compress_mode": mode, "block_size": block_size}
            self.sch[compress_tensor].emit_insn(compress_tensor.op.axis[tensor_len - self.FRACTAL_Z_LEN],
                                                "unzip", unzip_params)
        else:
            tile_k_value = self.tiling_work.tiling.get("BL0_matrix")[0]
            tile_n_value = self.tiling_work.tiling.get("BL0_matrix")[1]
            axis = self.sch_agent[compress_tensor].get_active_scopes()
            k_outer, k_inner = self.sch[compress_tensor].split(axis[-4], tile_k_value)
            n_outer, n_inner = self.sch[compress_tensor].split(axis[-3], tile_n_value)
            self.sch[compress_tensor].reorder(k_outer, n_outer, n_inner, k_inner)
            tensor_gemm_fp16 = self.tensor_map.get("cast_to_fp16")
            tensor_c_gm = self.tensor_map.get("tensor_c_gm")
            tensor_mmad_with_scale = self.tensor_map.get("c_ub_fract")
            self.container.compute_inline_list.append(tensor_gemm_fp16)
            self.container.compute_inline_list.append(tensor_c_gm)
            self.container.compute_inline_list.append(tensor_mmad_with_scale)
            self.sch[compress_tensor].emit_insn(n_inner, "unzip", {"compress_mode": mode, "block_size": 32 * 1024})

    def _get_index_at_axis(self):
        axis = self.sch_agent[self.root_tensor].get_active_scopes()
        axis_n = axis[0]
        axis_m = axis[1]

        if self.format_info.get("out") == "ND":
            axis_n, axis_m = axis_m, axis_n

        block_dim_m = self.tiling_work.tiling.get("block_dim")[2]
        if not self.is_dynamic:
            m_shape = self.container.tensor_map.get("a_l0a").shape[-4].value
            m_factor = (m_shape + block_dim_m - 1) // block_dim_m
            index_at_axis = axis_m if self.tiling_work.al1_tiling_m * 2 < m_factor else -1
            if index_at_axis != -1 and self.status_controller.bl1_attach_status == 'full_load':
                index_at_axis = self.sch_agent[self.container.tensor_map.get('b_l1')].get_active_scopes()[0]
        else:
            m_shape = self.container.tensor_map.get("a_l0a").shape[-4]
            m_factor = tvm.floordiv((m_shape + block_dim_m - 1), block_dim_m)
            index_at_axis = axis_m
            if self.status_controller.bl1_attach_status == "full_load":
                index_at_axis = self.sch_agent[self.container.tensor_map.get("b_l1")].get_active_scopes()[0]
        return index_at_axis

    def _choose_dma_copy_for_res(self):
        """choose dma copy pattern"""
        # with_transpose is the flag to use emit_insn dma_copy_matmul_transpose
        # this flag set from confusion_transpose_d
        real_res = self.res
        if self.status_controller.split_k_axis_by_tiling:
            real_res = self.container.tensor_map.get("c_gm")
        with_transpose = hasattr(real_res, "matmul_with_transpose")
        emit_insn_cmd = "dma_copy"
        if with_transpose:
            # get matrix axis shapes
            tensor_a_l0a = self.container.tensor_map.get("a_l0a")
            tensor_b_l0b = self.container.tensor_map.get("b_l0b")
            m_shape = self.dynamic_m * self.block_in
            n_shape = self.dynamic_n * self.block_out
            if not self.is_dynamic:
                m_shape = get_value(tensor_a_l0a.shape[-4]) * self.block_in
                n_shape = get_value(tensor_b_l0b.shape[-3]) * self.block_out
            cce_emitinsn_params.cceEmitParamsIns.insert_param("matmul_m", m_shape)
            cce_emitinsn_params.cceEmitParamsIns.insert_param("matmul_n", n_shape)
            block_dim = self.tiling_work.tiling.get("block_dim")
            batch = self._get_batch_factors(tensor_a_l0a, tensor_b_l0b)
            n_factors, m_factors = block_dim[1], block_dim[2]
            m_l0_shape = self.tiling_work.tiling.get("AL0_matrix")[0] * self.block_in
            n_l0_shape = self.tiling_work.tiling.get("BL0_matrix")[1] * self.block_out

            cce_emitinsn_params.cceEmitParamsIns.insert_param("batch", batch)
            cce_emitinsn_params.cceEmitParamsIns.insert_param("matmul_m_blk", m_factors)
            cce_emitinsn_params.cceEmitParamsIns.insert_param("matmul_n_blk", n_factors)
            cce_emitinsn_params.cceEmitParamsIns.insert_param("matmul_m_split", m_l0_shape)
            cce_emitinsn_params.cceEmitParamsIns.insert_param("matmul_n_split", n_l0_shape)
            emit_insn_cmd = "dma_copy_matmul_transpose"

        out_insn_dict = {}
        if self.schedule_mode == self.GENERAL_MODE and not self.para_map.get("align"):
            out_insn_dict["no_overlap"] = 1
        elif self.status_controller.unaligned_flag:
            out_insn_dict["no_overlap"] = 2
        elif self.schedule_mode == self.DYN_ALIGNED_MODE:
            out_insn_dict["no_overlap"] = 0
            out_insn_dict["gm_no_sync"] = 1
            out_insn_dict["map_policy"] = "2d"
        offset_res = 1 if self.status_controller.split_k_axis_by_tiling else 0
        if self.cache_tiling:
            tensor_out_cmd = "phony_insn" if self.status_controller.split_k_axis_by_tiling else "dma_copy"
            self._emit_insn_func(self.container.tensor_map.get("tensor_out_fp16"), 0,
                                 tensor_out_cmd, insn_dict=out_insn_dict, mode=1)
            self._emit_insn_func(self.container.tensor_map.get("tensor_out_fp32"), 0,
                                 tensor_out_cmd, insn_dict=out_insn_dict, mode=1)

            emit_insn_cmd = "phony_insn" if "tensor_virtual_res" in real_res.op.name else emit_insn_cmd
            res_axis = self.cache_tiling_mgr.get_res_tensor_emit_axis(self.format_info, self.status_controller)
            self.sch_agent[real_res].emit_insn(self.sch[real_res].leaf_iter_vars[res_axis], emit_insn_cmd,
                                               out_insn_dict)
        else:
            self._emit_insn_func(real_res, 0, emit_insn_cmd, insn_dict=out_insn_dict, mode=1, offset=offset_res)

        # temp
        overload_flag = True
        self._set_overload_flag(overload_flag, real_res, offset=offset_res)

    def _combine_cce_pragma(self, tensor, axis_list):
        """
        add pragma for emit_insn, in cachetiling scene, pass check whether can Combine multiple cce
        """
        id_val = tvm.call_extern("int32", "axis_group", 0, "append")
        for axis in axis_list:
            self.sch[tensor].pragma(axis, "axis_group", id_val)

    def _set_scope_buffer_type(self):
        gm_ub = None
        ele_header_ub_tensors = []
        if not self.container.fusion_ele:
            return gm_ub, ele_header_ub_tensors
        tensor_c_gm = self.container.tensor_map.get("c_gm")
        # multi output fusion with elementwise
        if (self.para_map.get("matmul_multi_output_flag", False) and tensor_c_gm in self.res_ori and
            not self.status_controller.support_mix_l2_fusion):
            gm_ub = self.sch.cache_read(tensor_c_gm, tbe_platform_info.scope_ubuf,
                                        self.tensor_list[2].get(tensor_c_gm))
            self.container.fusion_tensor_cub.append(gm_ub)
            self.container.fusion_tensor_cub.append(tensor_c_gm)
            self.sch[tensor_c_gm.op.input_tensors[0]].reused_by(gm_ub)

        tensor_ele_ub = []
        for ten_i in self.tensor_list[1].values():
            if self.tensor_list[2].get(ten_i)[0] not in self.container.matmul_tensors:
                ele_ub = self.sch.cache_read(ten_i, tbe_platform_info.scope_ubuf,
                                             self.tensor_list[2].get(ten_i))
                tensor_ele_ub.append(ele_ub)
                ele_header_ub_tensors.append(ele_ub)

        self._get_elewise_ub_tensors(tensor_ele_ub)
        self.container.elemwise_tensors = tensor_ele_ub
        return gm_ub, ele_header_ub_tensors

    def _set_overload_flag(self, overload_flag, flag_on_tensor, offset=0):
        """
        set overload flag
        """
        current_op = self.sch[flag_on_tensor]
        pragma_axis = self.sch_agent[flag_on_tensor].nlast_scopes(offset + 2)[0]
        if current_op is not None and pragma_axis is not None:
            if overload_flag:
                current_op.pragma(pragma_axis, "json_info_cache_read_mode", 0)
            else:
                current_op.pragma(pragma_axis, "json_info_cache_read_mode", 1)

    def _get_l0_bound(self):
        ka_bound = self.cache_tiling.get("k_l0") * self.block_reduce
        kb_bound = ka_bound
        n_l0 = self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1")
        batch_l0_extent = self.cache_tiling.get("batch_cub") * self.cache_tiling.get("batch_ub_l0_time")
        multi_batch = batch_l0_extent if self.status_controller.have_batch else 1
        a_l0a_bound = self.cache_tiling.get("m_l0") * self.block_in * ka_bound * multi_batch
        b_l0b_bound = n_l0 * self.block_in * kb_bound * multi_batch
        return a_l0a_bound, b_l0b_bound

    def _mem_process(self):
        if self.status_controller.ops_data_flow_mode == "fp322fp32":
            if self.is_dynamic:
                ka_bound = align(self.cache_tiling.get("k_l0"), 2) * self.block_reduce
                kb_bound = ka_bound
                n_l0 = self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1")
                batch_l0_extent = self.cache_tiling.get("batch_cub") * self.cache_tiling.get("batch_ub_l0_time")
                multi_batch = batch_l0_extent if self.status_controller.have_batch else 1
                a_l0a_bound = self.cache_tiling.get("m_l0") * self.block_in * ka_bound * multi_batch
                b_l0b_bound = n_l0 * self.block_in * kb_bound * multi_batch
            else:
                ka_bound = align(self.tiling_work.al0_tiling_ka, 2) * self.block_reduce
                kb_bound = align(self.tiling_work.bl0_tiling_kb, 2) * self.block_reduce
                a_l0a_bound = self.tiling_work.al0_tiling_ma * self.block_in * ka_bound
                b_l0b_bound = self.tiling_work.bl0_tiling_nb * self.block_in * kb_bound
            # in fp322fp32 mode, k-alignment will make both l0a and l0b's size larger than the original
            if self.status_controller.transpose_a:
                self.sch[self.container.tensor_map.get("a_l0a")].set_buffer_size(a_l0a_bound)
            if not self.status_controller.transpose_b or self.status_controller.transpose_a:
                self.sch[self.container.tensor_map.get("b_l0b")].set_buffer_size(b_l0b_bound)
        elif (self.status_controller.ops_data_flow_mode == "int82int32" and self.is_dynamic and
              self.status_controller.support_fix_pipe_l0c2out):
            a_l0a_bound, b_l0b_bound = self._get_l0_bound()
            self.sch[self.container.tensor_map.get("a_l0a")].set_buffer_size(a_l0a_bound)
            self.sch[self.container.tensor_map.get("b_l0b")].set_buffer_size(b_l0b_bound)
        if self.is_dynamic:
            self.sch.sequential_malloc(tbe_platform_info.scope_cbuf)
            self.sch.sequential_malloc(tbe_platform_info.scope_ca)
            self.sch.sequential_malloc(tbe_platform_info.scope_cb)
            self.sch.sequential_malloc(tbe_platform_info.scope_cc)
            self.sch.sequential_malloc(tbe_platform_info.scope_ubuf)
            self.sch.sequential_malloc(tbe_platform_info.scope_bt)
            self.sch.sequential_malloc(tbe_platform_info.scope_fb0)
            # get l1
            if self.para_map.get("need_aub"):
                aub_storage_bound, aub_fract_storage_bound = self._get_aub_bound()
                # a_ub is normalized so the storage used for (M,K) is the same as (M1, K1 , M0, K0)
                self.sch[self.container.tensor_map.get("a_ub")].set_buffer_size(aub_storage_bound)
                self.sch[self.container.tensor_map.get("a_ub_aligned")].set_buffer_size(aub_storage_bound)
                self.sch[self.container.tensor_map.get("a_ub_general")].set_buffer_size(aub_storage_bound)
                self.sch[self.container.tensor_map.get("a_ub_fract")].set_buffer_size(aub_fract_storage_bound)
            if self.para_map.get("need_bub"):
                bub_storage_bound, bub_fract_storage_bound = self._get_bub_bound()
                self.sch[self.container.tensor_map.get("b_ub")].set_buffer_size(bub_storage_bound)
                self.sch[self.container.tensor_map.get("b_ub_aligned")].set_buffer_size(bub_storage_bound)
                self.sch[self.container.tensor_map.get("b_ub_general")].set_buffer_size(bub_storage_bound)
                self.sch[self.container.tensor_map.get("b_ub_fract")].set_buffer_size(bub_fract_storage_bound)
            self.sch[self.container.tensor_map.get("a_l1")].set_buffer_size(self._get_al1_bound())
            self.sch[self.container.tensor_map.get("b_l1")].set_buffer_size(self._get_bl1_bound())
            self.cache_tiling_mgr.set_l0c_cub_buffer_size(self.container, self.format_info, self.tiling_work)

            # mem_unique
            self.sch[self.container.tensor_map.get("a_l1")].mem_unique()
            self.sch[self.container.tensor_map.get("b_l1")].mem_unique()
            self.sch[self.container.tensor_map.get("a_l0a")].mem_unique()
            self.sch[self.container.tensor_map.get("b_l0b")].mem_unique()
            # tensor is used_by can't mem_unique; tensor is not reused_by must be unique
            c_ub = self.res.op.input_tensors[0]
            if not self.format_info.get("out") == "ND":
                self.sch[c_ub].mem_unique()
            bias_ub = self.container.tensor_map.get("bias_ub")
            if bias_ub is not None and not self.status_controller.need_init_bias:
                self.sch[bias_ub].mem_unique()
            else:
                self.sch[self.container.tensor_map.get("c_l0c")].mem_unique()
            tensor_attach_with_l0c = []
            tensor_attach_with_l1 = []
            bias_bt = self.container.tensor_map.get("bias_bt")
            if bias_bt is not None:
                bias_zero = self.container.tensor_map.get("bias_zero")
                if bias_zero is not None:
                    tensor_attach_with_l0c.append(bias_zero)
                tensor_attach_with_l0c.append(bias_bt)
                tensor_attach_with_l1.append(self.container.tensor_map.get("bias_l1"))
            fixpipe_fb = self.container.tensor_map.get("fixpipe_fb", [])
            if fixpipe_fb != []:
                tensor_attach_with_l0c += fixpipe_fb
                tensor_attach_with_l1 += self.container.tensor_map.get("fixpipe_l1", [])
            tiling_cl0_n = self.tiling_work.tiling.get("CL0_matrix")[0]
            block_out = tbe_platform_info.CUBE_MKN.get(self.container.tensor_map.get("c_l0c").dtype).get("mac")[2]
            for tensor in tensor_attach_with_l0c:
                #bt must be align to even number
                n_bound = align(tiling_cl0_n, 2) * block_out
                self.sch[tensor].set_buffer_size(n_bound)
                if tensor in fixpipe_fb:
                    self.sch[tensor].mem_unique()
            for tensor in tensor_attach_with_l1:
                n_bound_bias_l1 = tiling_cl0_n * self.tiling_work.tiling.get("BL1_shape")[1] * block_out
                self.sch[tensor].set_buffer_size(n_bound_bias_l1)
                self.sch[tensor].mem_unique()

    def _get_max_m_bound(self):
        """
        This function is used to get the maximum m bound, which will be used in the
        following calculation to set storage bound.
        """
        a_matrix_dim = [get_value(i) for i in self.container.tensor_map.get("a_l0a").shape]
        if self.cache_tiling:
            m_bound = (self.cache_tiling.get("m_single_core") * self.cache_tiling.get("m_al1") *
                       self.cache_tiling.get("m_l0") * self.block_in)
        else:
            if self.tiling_work.tiling.get("block_dim")[2] == 1:
                m_bound = a_matrix_dim[-4] * self.block_in
            else:
                m_parts = int_ceil_div(a_matrix_dim[-4], self.tiling_work.tiling.get("CL0_matrix")[1])
                m_factors = int_ceil_div(m_parts, self.tiling_work.tiling.get("block_dim")[2])
                m_bound = m_factors * self.tiling_work.tiling.get("CL0_matrix")[1] * self.block_in
        return m_bound

    def _get_max_n_bound(self):
        """
        This function is used to get the maximum n bound, which will be used in the
        following calculation to set storage bound.
        """
        b_matrix_dim = [get_value(i) for i in self.container.tensor_map.get("b_l0b").shape]
        if self.cache_tiling:
            n_bound = (self.cache_tiling.get("n_single_core") * self.cache_tiling.get("n_bl1") *
                       self.cache_tiling.get("n_ub_l0_time") * self.cache_tiling.get("cub_n1") * self.block_out)
        else:
            if self.tiling_work.tiling.get("block_dim")[1] == 1:
                n_bound = b_matrix_dim[-3] * self.block_out
            else:
                n_parts = int_ceil_div(b_matrix_dim[-3], self.tiling_work.tiling.get("CL0_matrix")[0])
                n_factors = int_ceil_div(n_parts, self.tiling_work.tiling.get("block_dim")[1])
                n_bound = n_factors * self.tiling_work.tiling.get("CL0_matrix")[0] * self.block_out
        return n_bound

    def _get_aub_bound(self):
        gap_value = self.block_reduce
        m_bound = self.tiling_work.aub_tiling_m * self.block_in
        m_bound = (m_bound + gap_value) if self.status_controller.storage_m_bound_change else m_bound
        if self.status_controller.aub_attach_status == "c_gm":
            max_k_bound = self.tiling_work.get_a_max_k_bound(self.container.tensor_map.get("a_l0a").shape)
            k_bound_not_align = max_k_bound
            # If having bank conflict
            k_bound = tvm.select(
                max_k_bound % self.THRESHOLD_DATA_NUM == 0,
                max_k_bound + gap_value, max_k_bound) if self.status_controller.cgm_ka_storage_change else max_k_bound
        else:
            k_bound = self.tiling_work.aub_tiling_k
            k_bound_not_align = self.tiling_work.aub_tiling_k
            k_bound = (k_bound + gap_value) if self.status_controller.storage_ka_bound_change else k_bound
        aub_bound = m_bound * k_bound
        # aub_fract needn't solve bank conflict
        aub_fract_bound = self.tiling_work.aub_tiling_m * self.block_in * k_bound_not_align
        if self.cache_tiling:
            aub_bound = self.cache_tiling.get("aub_align_bound")
            aub_fract_bound = self.cache_tiling.get("k_aub") * self.cache_tiling.get("m_aub") * \
                self.block_in * self.block_reduce * self.tiling_work.aub_tiling_batch
        return aub_bound, aub_fract_bound

    def _get_bub_bound(self):
        gap_value = self.block_reduce
        n_bound = self.tiling_work.bub_tiling_n * self.block_out
        n_bound = (n_bound + gap_value) if self.status_controller.storage_n_bound_change else n_bound
        if self.status_controller.bub_attach_status == "c_gm":
            max_k_bound = self.tiling_work.get_b_max_k_bound(self.container.tensor_map.get("b_l0b"),
                                                             self.is_dynamic, self.dynamic_k)
            k_bound_not_align = max_k_bound
            # If having bank conflict
            k_bound = tvm.select(
                max_k_bound % self.THRESHOLD_DATA_NUM == 0,
                max_k_bound + gap_value, max_k_bound) if self.status_controller.cgm_kb_storage_change else max_k_bound
        else:
            k_bound = self.tiling_work.bub_tiling_k
            k_bound_not_align = self.tiling_work.bub_tiling_k
            k_bound = (k_bound + gap_value) if self.status_controller.storage_kb_bound_change else k_bound
        bub_bound = n_bound * k_bound
        # aub_fract needn't solve bank conflict
        bub_fract_bound = self.tiling_work.bub_tiling_n * self.block_out * k_bound_not_align
        if self.cache_tiling:
            bub_bound = self.cache_tiling.get("bub_align_bound")
            bub_fract_bound = self.cache_tiling.get("k_bub") * self.cache_tiling.get("n_bub") * \
                self.block_out * self.block_reduce * self.tiling_work.bub_tiling_batch
        return bub_bound, bub_fract_bound

    def _get_al1_bound(self):
        k_bound = self.tiling_work.tiling.get("AL1_shape")[0]
        if self.tiling_work.tiling.get(
                "AL1_shape") and self.status_controller.al1_attach_status != "full_load":
            m_bound = self.tiling_work.tiling.get("AL1_shape")[1] * self.tiling_work.tiling.get(
                "CL0_matrix")[1] * self.block_in
        else:
            m_bound = self._get_max_m_bound()
        # FP32 need to do K alignment
        if self.status_controller.enable_k_alignment:
            k_ori = self.cache_tiling.get("k_ori")
            kal1 = self.cache_tiling.get("kal1_16") * self.block_reduce
            kal1_align = align(kal1, 2 * self.block_reduce)
            k_bound = kal1_align
            # unaligned scene use guard_with_if do not need check start_point
            # in shift_inward scene, offset_address of last k must be 16_align.
            if not self.status_controller.unaligned_flag:
                k_bound = tvm.select(tvm.any(tvm.max(k_ori - kal1, 0) % (2 * self.block_reduce) == 0,
                                     int_ceil_div(k_ori, self.block_reduce) % 2 == 0),
                                     kal1_align, align(kal1_align + self.block_reduce, 2 * self.block_reduce))
        al1_bound = m_bound * k_bound * self.tiling_work.al1_tiling_batch
        return al1_bound

    def _get_bl1_bound(self):
        k_bound = self.tiling_work.tiling.get("BL1_shape")[0]
        if self.tiling_work.tiling.get(
                "BL1_shape") and self.status_controller.bl1_attach_status != "full_load":
            n_bound = self.tiling_work.tiling.get("BL1_shape")[1] * self.tiling_work.tiling.get(
                "CL0_matrix")[0] * self.block_out
        else:
            n_bound = self._get_max_n_bound()
        # FP32 need to do K alignment
        need_to_align_k = (
            self.status_controller.ops_data_flow_mode == "fp322fp32" and not self.status_controller.transpose_b)
        if self.status_controller.enable_k_alignment or need_to_align_k:
            k_var_name = "k_ori" if not self.status_controller.pad_flag else "k_pad"
            k_ori = self.cache_tiling.get(k_var_name)
            kbl1 = self.cache_tiling.get("kbl1_16") * self.block_reduce
            kbl1_align = align(kbl1, 2 * self.block_reduce)
            k_bound = kbl1_align
            # unaligned scene use guard_with_if do not need check start_point
            # in shift_inward scene, offset_address of last k must be 16_align.
            if not self.status_controller.unaligned_flag:
                k_bound = tvm.select(tvm.any(tvm.max(k_ori - kbl1, 0) % (2 * self.block_reduce) == 0,
                                     int_ceil_div(k_ori, self.block_reduce) % 2 == 0),
                                     kbl1_align, align(kbl1_align + self.block_reduce, 2 * self.block_reduce))
        bl1_bound = n_bound * k_bound * self.tiling_work.bl1_tiling_batch # 这个参数就是Batch_l0,L1和L0一样大的Batch量
        return bl1_bound

    def _set_continuous_axis_for_nd_out(self, tensor):
        axis_list = [self.sch[tensor].leaf_iter_vars[-2], self.sch[tensor].leaf_iter_vars[-1]]
        if self.cache_tiling and self.status_controller.have_batch:
            axis_list.append(self.sch[tensor].leaf_iter_vars[-3])
        if not self.status_controller.unaligned_flag:
            self._combine_cce_pragma(tensor, axis_list)

    def _set_continuous_axis(self):
        """ add pragma for pass to check whether or not the address is continus and combine cce orders
        """
        if self.cache_tiling and self.format_info.get("a") != "FRACTAL_NZ":
            a_l0a = self.container.tensor_map.get("a_l0a")
            axis_list = [self.sch[a_l0a].leaf_iter_vars[-3], self.sch[a_l0a].leaf_iter_vars[-4]]
            if self.status_controller.have_batch_a and not self.status_controller.support_fix_pipe_l0c2ub:
                axis_list.append(self.sch[a_l0a].leaf_iter_vars[-5])
            self._combine_cce_pragma(a_l0a, axis_list)
        if self.cache_tiling:
            is_b_matrix_load_3d = ((self.status_controller.ops_data_flow_mode == "fp322fp32")
                                   and (not self.status_controller.transpose_b))
            index_offset = -1 if is_b_matrix_load_3d else 0
            b_l0b = self.container.tensor_map.get("b_l0b")
            axis_list = [self.sch[b_l0b].leaf_iter_vars[-3 + index_offset],
                         self.sch[b_l0b].leaf_iter_vars[-4 + index_offset]]
            if self.status_controller.have_batch_b and not self.status_controller.support_fix_pipe_l0c2ub:
                axis_list.append(self.sch[b_l0b].leaf_iter_vars[-5 + index_offset])
            self._combine_cce_pragma(b_l0b, axis_list)
        if (self.format_info.get("out") == "ND"
            and self.is_dynamic
            and not self.status_controller.support_fix_pipe_l0c2out):
            real_res = self.res
            if self.status_controller.split_k_axis_by_tiling:
                real_res = self.container.tensor_map.get("c_gm")
            self._set_continuous_axis_for_nd_out(real_res)

            if self.cache_tiling:
                self._set_continuous_axis_for_nd_out(self.container.tensor_map.get("tensor_out_fp16"))
                self._set_continuous_axis_for_nd_out(self.container.tensor_map.get("tensor_out_fp32"))

    def _update_fusion_buildcfg(self):
        """ handle special build config requirements for fusion
        """
        c_l0c = self.container.tensor_map.get("c_l0c")
        c_gm = self.container.tensor_map.get("c_gm")
        fc_flag = False
        dtype_out = ""
        format_out = ""
        if c_l0c is not None and "fc_flag" in c_l0c.op.attrs:
            fc_flag = c_l0c.op.attrs["fc_flag"]
        if c_gm is not None and "format" in c_gm.op.attrs:
            dtype_out = c_gm.dtype
            format_out = c_gm.op.attrs["format"]
        is_fc_dequant_quant_fusion = (fc_flag and self.status_controller.ops_data_flow_mode == "int82int32"
                                      and dtype_out == "int8" and format_out == "ND")
        if self.status_controller.support_fix_pipe_l0c2out and is_fc_dequant_quant_fusion:
            build_config = get_fusion_buildcfg().get("gemm_op", {})
            build_config.update({"unit_flag_mode": "close"})
            set_fusion_buildcfg("gemm_op", build_config)
