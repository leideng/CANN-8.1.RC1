#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Schedule of conv2d in v350.
"""
import os
import inspect
from tbe import tvm
from te.platform import cce_params
from tbe.common.utils import log
from tbe.common.tiling import tiling_api
from tbe.common.register import set_fusion_buildcfg
from tbe.common.platform import get_soc_spec
from tbe.common.platform import get_cube_mkn
from tbe.common.tiling.tiling_helper import TILING_INSTANCE
from tbe.common.utils.op_util.op_util_conv2d import ceil_div
from tbe.common.utils.op_util.op_util_conv2d import BIT_RATIO_MAP
from tbe.dsl.compute.conv_compute_v350 import ConvParam
from tbe.dsl.static_schedule.conv_fixpipefusion_schedule import FixpipeFusionV350

M_INDEX = 0
C_INDEX = 1
N_INDEX = 2
FWC_STR = "fwc"
VNCHW_ALIGN = 16
NC1HWC0_C0 = -1
NC1HWC0_HW = -2
NC1HWC0_C1 = -3
UB_DEEP_FUSION_ADDR_TYPE = 3


def show_ir(sch):
    file_path = os.path.dirname(os.path.realpath(__file__))
    line_no = inspect.currentframe().f_back.f_lineno
    funcname = inspect.currentframe().f_back.f_code.co_name
    co_filename = inspect.currentframe().f_back.f_code.co_filename
    filename = os.path.relpath(co_filename, file_path)
    log_str = '[%s:%d][%s] ' % (filename, line_no, funcname)
    sch1 = sch.normalize()
    bounds = tvm.schedule.InferBound(sch1)
    stmt = tvm.ScheduleOps(sch1, bounds, True)
    print("[show_ir]" + log_str)
    print(stmt)


class Conv2dScheduleV350:
    """
    Class of Conv2d Schedule for v350.
    """

    def __init__(self, res, spec_node_list, conv_param, sch_list, op_graph):
        self._sch = sch_list[0]
        self._res = res
        self._conv_param = conv_param
        self._tensor_map = self._conv_param.tensor_map
        self._para_dict = self._conv_param.para_dict
        self._op_graph = op_graph

        # dtype
        self._fmap_dtype = self._tensor_map.get("fmap").dtype
        self._weight_dtype = self._tensor_map.get("weight").dtype
        self._res_dtype = self._conv_param.res_dtype
        self._mad_dtype = self._conv_param.mad_dtype

        # conv params
        self._cin1_data = self._para_dict["cin1_data"]
        self._cin1_weight = self._para_dict["cin1_weight"]
        self._cout1 = self._para_dict["cout1"]
        self._batch = self._conv_param.batch
        self._h_in = self._conv_param.h_in
        self._w_in = self._conv_param.w_in
        self._weight_h = self._conv_param.weight_h
        self._weight_w = self._conv_param.weight_w
        self._pad_top = self._conv_param.pad_top
        self._pad_bottom = self._conv_param.pad_bottom
        self._pad_left = self._conv_param.pad_left
        self._pad_right = self._conv_param.pad_right
        self._stride_h = self._conv_param.stride_h
        self._stride_w = self._conv_param.stride_w
        self._dilate_h = self._conv_param.dilate_h
        self._dilate_w = self._conv_param.dilate_w
        self._group = self._conv_param.group
        self._bias_tensor = self._para_dict["bias_tensor"]
        self._hk_dilation = (self._weight_h - 1) * self._dilate_h + 1
        self._wk_dilation = (self._weight_w - 1) * self._dilate_w + 1
        self._group_opt = self._conv_param.group_opt

        # flag
        self._bias_flag = self._bias_tensor is not None
        self._split_k_flag = False
        self._enable_depthwise_flag = self._group == self._conv_param.cin_ori and \
                                      self._group == self._conv_param.cout_ori
        self._normal_group_conv_flag = self._group > 1 and not self._enable_depthwise_flag
        self._ub_deep_fusion_flag = self.get_ub_deep_fusion_flag()

        # tiling params
        self._tiling = {}

        # fusion class define
        self._fixpipe_fusion = FixpipeFusionV350(self._sch, self._res, self._conv_param, self._op_graph)

        # fillwindowcache + conv2d pre_fusion_flag
        self._fwc_pre_fusion_flag = FWC_STR in self._tensor_map.get("fmap").op.tag
        self._not_align_flag = self._tensor_map.get("fmap").op.attrs.get("not_align_flag")

    def get_ub_deep_fusion_flag(self):
        """
        Get the ub deep fusion flag of op.
        """
        fmap_addr_val = self._tensor_map.get("fmap").op.attrs.get("addr_type")
        res_addr_val = self._res.op.attrs.get("addr_type")
        fmap_addr_type = int(fmap_addr_val) if fmap_addr_val else 0
        res_addr_type = int(res_addr_val) if res_addr_val else 0
        return fmap_addr_type == UB_DEEP_FUSION_ADDR_TYPE or res_addr_type == UB_DEEP_FUSION_ADDR_TYPE

    def fetch_info_dict(self):
        """
        Fetch the info_dict to get tiling.
        """

        def get_fixpipe_info():
            """
            get fixpipe_type_list and fixpipe_fused_coeff to get tiling.
            """
            fixpipe_type_list = self._fixpipe_fusion.fixpipe_fused_type
            fixpipe_fused_coeff = [self._fixpipe_fusion.fixpipe_channel_coeff,
                                   self._fixpipe_fusion.fixpipe_eltwise_coeff]

            return fixpipe_type_list, fixpipe_fused_coeff

        fixpipe_type_list, fixpipe_fused_coeff = get_fixpipe_info()

        fmap_shape_nc1hwc0 = self._conv_param.dim_map.get("fmap_tiling_shape")
        shape_w_nc1hwc0 = self._conv_param.dim_map.get("weight_tiling_shape")
        result_shape_nc1hwc0 = self._conv_param.dim_map.get("output_tiling_shape")

        # create info_dict for tiling
        info_dict = {
            "op_type": 'conv2d',
            "fm_shape": fmap_shape_nc1hwc0,
            "cin_ori": self._conv_param.cin_ori,
            "filter_shape": shape_w_nc1hwc0,
            "result_shape": result_shape_nc1hwc0,
            "output_num": 1,
            "fm_dtype": self._fmap_dtype,
            "filter_dtype": self._weight_dtype,
            "result_dtype": self._res_dtype,
            "mad_dtype": self._mad_dtype,
            "pad": [self._pad_top, self._pad_bottom,
                    self._pad_left, self._pad_right],
            "stride": [self._stride_h, self._stride_w],
            "dilation": [self._dilate_h, self._dilate_w],
            "group": self._group,
            "bias_flag": self._bias_flag,
            "bias_dtype": "int32",
            "in_fm_memory_type": [0],
            "out_fm_memory_type": [0],
            "out_c_memory_type": [],
            "l1_fusion_type": 0,
            "fm_l1_valid_size": -1,
            "fusion_type": "0",
            "kernel_name": self._conv_param.kernel_name,
            "fixpipe_fused_type": fixpipe_type_list,
            "fixpipe_fused_coefficient": fixpipe_fused_coeff,
            "fused_coefficient": [0, 0, 0],
            "fused_channel_wise": [0, 0, 0],
            "pooling_shape": [0, 0],
            "pooling_stride": [0, 0],
            "special_mode": {
                "ub_deep_fusion_flag": self._ub_deep_fusion_flag,
            },
            "enable_depthwise_flag": self._enable_depthwise_flag,
            "fwc_pre_fusion_flag": self._fwc_pre_fusion_flag
        }

        return info_dict

    def fetch_tiling(self, info_dict):
        """
        Fetch tiling info.
        """

        def get_default_tiling():
            """
            Set default tiling when not hit repo.
            """
            batch_ub = 1
            default_cin = 16  # default_cin = 16
            ci0_aub = get_cube_mkn(self._fmap_dtype)[C_INDEX]
            ci1_aub = default_cin // ci0_aub
            ci0_bub = get_cube_mkn(self._weight_dtype)[C_INDEX]
            ci1_bub = default_cin // ci0_bub
            default_cout = 16 # default_cout = 16
            co0_cub = self._res.shape[-1]
            co1_cub = default_cout // co0_cub  # default_cout1 = 16 / co0
            co1_bub = co1_cub
            co0_bub = co0_cub
            h_cub = 4  # default_hout = 16
            w_cub = 4  # default_wout = 16

            # fillwindowcache+conv cin h w need full_load
            if self._fwc_pre_fusion_flag:
                ci1_aub = self._conv_param.cin1_data
                ci1_bub = ceil_div(self._conv_param.cin_ori, ci0_bub)
                h_cub = self._conv_param.h_out
                w_cub = self._conv_param.w_out

            default_tiling = {
                # default AUB tiling [1(batch), cin1, cin0(cin1*cin0=16)]
                "AUB_shape": [batch_ub, ci1_aub, ci0_aub],
                # default BUB tiling [full_load_k1, full_load_k0, 1(cout1), cout0]
                "BUB_shape": [ci1_bub, ci0_bub, co1_bub, co0_bub],
                # default CUB tiling [1(batch), 1(cout1), cout0, 4(h), 4(w)]
                "CUB_shape": [batch_ub, co1_cub, co0_cub, h_cub, w_cub],
                # mn reorder
                "control_reorder_flag": 0,
                # BT, FB, eltwise param input ub slice or full_load
                "INPUT_L1_BT_param": "slice", "INPUT_L1_FB_param": "slice",
                "INPUT_L1_eltwise_param": "slice",
                # manual_pingpong_buffer
                "manual_pingpong_buffer": {"AUB_pbuffer": 1, "BUB_pbuffer": 1, "UB2OUT_pbuffer": 1,
                                           "INPUT_L1_FB_pbuffer": 1, "INPUT_L1_eltwise_pbuffer": 1}
            }

            return default_tiling

        def get_tiling(info_dict):
            """
            Get tiling in v350 situation.
            """
            tiling = tiling_api.get_tiling(info_dict)
            if tiling.get("AL0_matrix") is not None:
                log.warn("can not find tiling in repo, default tiling will be used")
                tiling = get_default_tiling()
            return tiling

        # v350 only support repo tiling
        TILING_INSTANCE.set_tiling_type("repository_tiling")
        self._tiling = get_tiling(info_dict)

        return

    def config_scope(self):
        """
        Config tensor scope.

        Returns
        -------
        tensor_param: dict
            Tensors those set scope.
        """

        def config_aub():
            """
            Config aub scope.
            """
            fmap = tensor_map.get("fmap")
            aub = sch.cache_read(fmap, cce_params.scope_ubuf, [conv_res])
            return aub

        def config_bub():
            """
            Config bub scope.
            """
            weight = tensor_map.get("weight")
            bub = sch.cache_read(weight, cce_params.scope_ubuf, [conv_res])
            return bub

        def config_bias():
            """
            Config bias scope.
            """
            if self._bias_flag:
                bias = tensor_map.get("bias")
                bias_ub = sch.cache_read(bias, cce_params.scope_ubuf, [conv_res])
                return bias_ub
            return None

        def config_conv_res():
            """
            Config conv_op res scope.
            """
            conv_res = tensor_map.get("conv_res")
            sch[conv_res].set_scope(cce_params.scope_ubuf)
            return conv_res

        def config_res():
            """
            Config res tensor cache_write.
            """
            res_ub = sch.cache_write(res, cce_params.scope_ubuf)
            return res_ub

        def config_fixpipe_ub():
            """
            Config fixpipe_res tensor scope.
            """
            fixpipe_ub = tensor_map.get("fixpipe_op")
            sch[fixpipe_ub].set_scope(cce_params.scope_ubuf)
            return fixpipe_ub

        def config_fwc_ub():
            fmap = tensor_map.get("fmap")
            fwc_ub = sch.cache_write(fmap, cce_params.scope_ubuf)
            tensor_map.update(fwc_ub=fwc_ub)

        def config_clean_cache_ub():
            if self._not_align_flag:
                fifo_tensor = tensor_map.get("fifo_tensor")
                clean_cache_tensor = fifo_tensor.op.input_tensors[0]
                clean_cache_ub = sch.cache_read(clean_cache_tensor, cce_params.scope_ubuf, [fifo_tensor])
            else:
                fmap = tensor_map.get("fmap")
                fwc_ub = tensor_map.get("fwc_ub")
                clean_cache_tensor = fmap.op.input_tensors[0]
                clean_cache_ub = sch.cache_read(clean_cache_tensor, cce_params.scope_ubuf, [fwc_ub])
            tensor_map.update(clean_cache_ub=clean_cache_ub)

        def config_not_align_fwc():
            fmap = tensor_map.get("fmap")
            fifo_transpose_tensor = fmap.op.input_tensors[0]
            fifo_tensor = fifo_transpose_tensor.op.input_tensors[0]
            y_transpose_tensor = fifo_tensor.op.input_tensors[1]
            x_transpose_tensor = fifo_tensor.op.input_tensors[2]
            y_reshape_tensor = y_transpose_tensor.op.input_tensors[0]
            x_reshape_tensor = x_transpose_tensor.op.input_tensors[0]
            y_ub_tensor = y_reshape_tensor.op.input_tensors[0]
            x_ub_tensor = x_reshape_tensor.op.input_tensors[0]
            tensor_map.update(fifo_transpose_tensor=fifo_transpose_tensor, fifo_tensor=fifo_tensor,
                              y_transpose_tensor=y_transpose_tensor, x_transpose_tensor=x_transpose_tensor,
                              y_reshape_tensor=y_reshape_tensor, x_reshape_tensor=x_reshape_tensor,
                              y_ub_tensor=y_ub_tensor, x_ub_tensor=x_ub_tensor)
            for tensor in [fifo_transpose_tensor, fifo_tensor, y_transpose_tensor, x_transpose_tensor,
                           y_reshape_tensor, x_reshape_tensor, y_ub_tensor, x_ub_tensor]:
                sch[tensor].set_scope(cce_params.scope_ubuf)

        # ========set scope && cache_read && cache_write==========
        tensor_map = self._tensor_map
        sch = self._sch
        res = self._res

        if self._fixpipe_fusion.flag:
            conv_res = config_conv_res()
            res_ub = config_res()
        else:
            conv_res = config_res()
        aub = config_aub()
        bub = config_bub()
        bias_ub = config_bias()

        tensor_param = {"aub": aub, "bub": bub,
                        "bias_ub": bias_ub, "conv_res": conv_res}

        if self._fixpipe_fusion.flag:
            tensor_param.update({"res_ub": res_ub})

        if self._fwc_pre_fusion_flag:
            if self._not_align_flag:
                config_not_align_fwc()
            config_fwc_ub()
            config_clean_cache_ub()

        return tensor_param

    def special_process_pre(self, res, tensor_param):
        """
        Special process before tiling is parsed.
        """
        def fwc_reuse():
            if self._not_align_flag:
                fwc_in = self._tensor_map.get("x_ub_tensor").op.input_tensors[0]
            else:
                fwc_in = fwc_res.op.input_tensors[1]
            fwc_ub = self._tensor_map.get("fwc_ub")
            n, c1, h, w, c0 = fwc_in.shape
            # fwc_in size load to ub each time
            frame_size = n * c1 * h * w * c0

            sch[fwc_ub].reused_by(aub)
            sch[fwc_in].reused_by(fwc_res)
            # fillwindowcache need offset for reuse, which is frame_size
            sch[fwc_res].bind_buffer(fwc_res.op.axis[-1], 0, frame_size)

        sch = self._sch

        aub = tensor_param["aub"]
        _, k0_align_fm, _ = get_cube_mkn(self._fmap_dtype)
        sch[aub].buffer_align((1, 1), (1, 1), (1, 1), (1, 1), (k0_align_fm, k0_align_fm))
        if self._fwc_pre_fusion_flag:
            sch[aub].buffer_align((1, 1), (1, 1), (self._h_in, self._h_in),
                                  (self._w_in, self._w_in), (k0_align_fm, k0_align_fm))
            # reuse and compute with when fillwindowcache pre fusion flag true
            fwc_res = self._tensor_map.get("fmap")
            fwc_reuse()
            if self._not_align_flag:
                x_reshape_tensor = self._tensor_map.get("x_reshape_tensor")
                y_reshape_tensor = self._tensor_map.get("y_reshape_tensor")
                x_transpose_tensor = self._tensor_map.get("x_transpose_tensor")
                y_transpose_tensor = self._tensor_map.get("y_transpose_tensor")
                fifo_tensor = self._tensor_map.get("fifo_tensor")
                fifo_transpose_tensor = self._tensor_map.get("fifo_transpose_tensor")
                # reshape_tensor [n, c1, hw, c0], hw_axis need align to 16
                sch[x_reshape_tensor].compute_align(x_reshape_tensor.op.axis[NC1HWC0_HW], VNCHW_ALIGN)
                sch[y_reshape_tensor].compute_align(y_reshape_tensor.op.axis[NC1HWC0_HW], VNCHW_ALIGN)
                # transpose_tensor [n, c1, c0, hw], hw_axis need align to 16
                sch[x_transpose_tensor].compute_align(x_transpose_tensor.op.axis[NC1HWC0_C0], VNCHW_ALIGN)
                sch[y_transpose_tensor].compute_align(y_transpose_tensor.op.axis[NC1HWC0_C0], VNCHW_ALIGN)
                # fifo_tensor [n, c1, c0, hw], hw_axis need align to 16
                sch[fifo_tensor].compute_align(fifo_tensor.op.axis[NC1HWC0_C0], VNCHW_ALIGN)
                # fifo_transpose_tensor [n, c1, hw, c0], hw_axis need align to 16
                sch[fifo_transpose_tensor].compute_align(fifo_transpose_tensor.op.axis[NC1HWC0_HW], VNCHW_ALIGN)
                # all transpose relate tensor h_w_c0_size need align k0 * 16
                sch[x_reshape_tensor].storage_align(x_reshape_tensor.op.axis[NC1HWC0_C1], k0_align_fm * VNCHW_ALIGN, 0)
                sch[y_reshape_tensor].storage_align(y_reshape_tensor.op.axis[NC1HWC0_C1], k0_align_fm * VNCHW_ALIGN, 0)
                sch[x_transpose_tensor].storage_align(x_transpose_tensor.op.axis[NC1HWC0_C1],
                                                      k0_align_fm * VNCHW_ALIGN, 0)
                sch[y_transpose_tensor].storage_align(y_transpose_tensor.op.axis[NC1HWC0_C1],
                                                      k0_align_fm * VNCHW_ALIGN, 0)
                sch[fifo_tensor].storage_align(fifo_tensor.op.axis[NC1HWC0_C1], k0_align_fm * VNCHW_ALIGN, 0)
                sch[fifo_transpose_tensor].storage_align(fifo_transpose_tensor.op.axis[NC1HWC0_C1],
                                                         k0_align_fm * VNCHW_ALIGN, 0)
                sch[fifo_transpose_tensor].buffer_align((1, 1), (1, 1), (1, k0_align_fm), (1, 1))
            # when split batch, assure multi consumers right
            sch.compute_with([fwc_res, self._res], 1)

        # weight cin0 need align to k0_align (int16 * int8 hybird case):
        # weight c0 bigger than fmap c0, weight c0 could not be used in conv compute
        # weight cout0 need align to n0_align (weight: int8, res: int16 or float16):
        # res cout0 could less than weight cout0, weight cout0 could not be used in conv_fixpipe
        bub = tensor_param["bub"]
        _, k0_align_weight, n0_align = get_cube_mkn(self._weight_dtype)
        if not self._enable_depthwise_flag:
            sch[bub].buffer_align((1, 1), (1, 1), (n0_align, n0_align), (k0_align_weight, k0_align_weight))
        else:
            sch[bub].buffer_align((1, 1), (1, 1), (1, 1), (n0_align, n0_align))
        # compute inline for conv_op in fixpipe fusion
        if self._fixpipe_fusion.flag:
            self._fixpipe_fusion.special_process_pre(sch, res, tensor_param)


    def tile_attach_tensor(self, res, sch_list, tensor_param):
        """
        Split tensor axis and attach tensors.
        """
        def tile_tensor_cub():
            batch_idx, co1_c_idx, h_idx, w_idx, co0_c = range(len(cub.shape))
            # split batch to batch_out_ub, batch_inner_ub (only support one batch load in ub)
            cub_bout, cub_bin = sch[cub].split(cub.op.axis[batch_idx], 1)
            # ci1 support only full load yet
            if not self._normal_group_conv_flag:
                co1_cub_factor = co1_cub
                cub_coout, cub_coin = sch[cub].split(cub.op.axis[co1_c_idx], co1_cub_factor)
            else:
                co1_opt_factor = self._res.shape[1] // self._group_opt
                gr_co_opt, gr_co_in = sch[cub].split(cub.op.axis[co1_c_idx], co1_opt_factor)
                co1_cub_factor = co1_cub
                co1_outer, co1_inner = sch[cub].split(gr_co_in, co1_cub_factor)

            # split h to h_out_ub, h_inner_ub (only support h = 1 load in ub yet)
            cub_hout, cub_hin = sch[cub].split(cub.op.axis[h_idx], h_cub)
            # split w to w_out_ub, w_inner_ub
            cub_wout, cub_win = sch[cub].split(cub.op.axis[w_idx], w_cub)
            # ci1 support cut to kout, kin
            cub_kout = None
            if not self._enable_depthwise_flag:
                cin1_reduce_axis = sch[cub].op.reduce_axis[0]
                cub_kout, cub_kin = sch[cub].split(cin1_reduce_axis, factor=ci1_aub)
                # when split k ,aub and bub compute at to cub
                if ci1_aub < ceil_div(self._conv_param.cin_ori // self._group_opt, ci0_aub):
                    self._split_k_flag = True
                    attach_axis_dict.update({"aub_at_cub_axis": cub_kout,
                                             "bub_at_cub_axis": cub_kout})

            if self._normal_group_conv_flag:
                reorder_list = [gr_co_opt,
                                cub_bout,
                                co1_outer,
                                cub_hout,
                                cub_wout,
                                cub_kout,
                                cub_bin,
                                co1_inner,
                                cub_hin,
                                cub_win,
                                cub.op.axis[co0_c]]
            elif control_reorder_flag == 0:
                reorder_list = [cub_bout,
                                cub_hout,
                                cub_wout,
                                cub_coout,
                                cub_kout,
                                cub_bin,
                                cub_coin,
                                cub_hin,
                                cub_win,
                                cub.op.axis[co0_c]]
                # depthwise branch
                if cub_kout is None:
                    reorder_list.remove(cub_kout)
            else:
                reorder_list = [cub_bout,
                                cub_coout,
                                cub_hout,
                                cub_wout,
                                cub_kout,
                                cub_bin,
                                cub_coin,
                                cub_hin,
                                cub_win,
                                cub.op.axis[co0_c]]
                # depthwise branch
                if cub_kout is None:
                    reorder_list.remove(cub_kout)

            sch[cub].reorder(*reorder_list)

            cub_pragma_axis = cub_bin

            return cub_pragma_axis

        def bias_compute_at():
            if self._bias_flag:
                sch[bias].compute_at(sch[res], bias_at_res_axis)

        def tile_tensor_res():

            def fetch_base_axis():
                """
                Fetch axis of the res tensor.
                """
                res_n_axis, res_co1_axis, res_h_axis, res_w_axis, res_co0_axis = res.op.axis  # [n, co1, ho, wo, co0]
                return res_n_axis, res_co1_axis, res_h_axis, res_w_axis, res_co0_axis

            def split_batch_axis(batch_ub_factor):
                """
                Split out res_batchub_axis.
                """
                out2cub_loopbatch_axis, res_batchub_axis = sch[res].split(res_n_axis, factor=batch_ub_factor)
                return out2cub_loopbatch_axis, res_batchub_axis

            def split_co1_axis(co1_res_factor):
                """
                Split out res_nub_axis.
                """
                out2cub_loopn_axis, res_nub_axis = sch[res].split(res_co1_axis, factor=co1_res_factor)
                return out2cub_loopn_axis, res_nub_axis

            def split_ho_axis(ho_ub_factor):
                """
                Split out res_hub_axis.
                """
                out2cub_looph_axis, res_hub_axis = sch[res].split(res_h_axis, factor=ho_ub_factor)
                return out2cub_looph_axis, res_hub_axis

            def split_wo_axis(wo_ub_factor):
                """
                Split out res_wub_axis.
                """
                out2cub_loopw_axis, res_wub_axis = sch[res].split(res_w_axis, factor=wo_ub_factor)
                return out2cub_loopw_axis, res_wub_axis

            def get_res_attach_axis():
                """
                prepare res attach axis for compute_at
                """
                # get bub_at_res_axis
                bub_at_res_axis = out2cub_loopn_axis

                # get aub_at_res_axis
                aub_at_res_axis = out2cub_loopn_axis if (self._enable_depthwise_flag and control_reorder_flag == 0) \
                    else out2cub_loopw_axis

                # get cub_at_res_axis
                cub_at_res_axis = res_batchub_axis

                # get bias_at_res_axis
                bias_at_res_axis = out2cub_loopn_axis

                if self._normal_group_conv_flag:
                    bub_at_res_axis = res_nub_co_outer
                    bias_at_res_axis = res_nub_co_outer

                return [aub_at_res_axis, bub_at_res_axis, cub_at_res_axis, bias_at_res_axis]

            res_n_axis, res_co1_axis, res_h_axis, res_w_axis, res_co0_axis = fetch_base_axis()

            # split batch to batch_out and batch_in
            batch_ub_factor = batch_cub
            out2cub_loopbatch_axis, res_batchub_axis = split_batch_axis(batch_ub_factor)

            # split co1 to co1_out and co1_in, channel_split case co1_ub_factor = 2
            co1_res_factor = co1_cub
            out2cub_loopn_axis, res_nub_axis = split_co1_axis(co1_res_factor)

            # split res_h to res_h_out and res_h_in (only support h = 1 yet)
            ho_ub_factor = h_cub
            out2cub_looph_axis, res_hub_axis = split_ho_axis(ho_ub_factor)

            # split res_w to res_w_out and res_w_in
            wo_ub_factor = w_cub
            out2cub_loopw_axis, res_wub_axis = split_wo_axis(wo_ub_factor)

            if self._normal_group_conv_flag:
                co1_factor = self._conv_param.cout1_opt * n0_align // co0_cub
                res_nub_co_outer, res_nub_co_inner = sch[res].split(res_nub_axis, co1_factor)
                res_reorder_axis_list = [out2cub_loopn_axis,
                                         out2cub_loopbatch_axis,
                                         res_nub_co_outer,
                                         out2cub_looph_axis,
                                         out2cub_loopw_axis,
                                         res_batchub_axis,
                                         res_nub_co_inner,
                                         res_hub_axis,
                                         res_wub_axis,
                                         res_co0_axis]
            else:
                res_reorder_axis_list = [out2cub_loopbatch_axis,
                                        out2cub_loopn_axis,
                                        out2cub_looph_axis,
                                        out2cub_loopw_axis,
                                        res_batchub_axis,
                                        res_nub_axis,
                                        res_hub_axis,
                                        res_wub_axis,
                                        res_co0_axis]

            sch[res].reorder(*res_reorder_axis_list)

            # reorder mn according to tiling, control_reorder_flag = 0:hwn; 1:hnw
            if control_reorder_flag == 0 and not self._normal_group_conv_flag:
                sch[res].reorder(out2cub_looph_axis,
                                 out2cub_loopw_axis,
                                 out2cub_loopn_axis)

            res_axis_list = get_res_attach_axis()
            res_pragma_axis = res_hub_axis

            attach_axis_dict.update(
                {
                    "aub_at_res_axis": res_axis_list[0],
                    "bub_at_res_axis": res_axis_list[1],
                    "cub_at_res_axis": res_axis_list[2],
                    "batch_out_at_res_axis": out2cub_loopbatch_axis,
                    "batch_in_at_res_axis": res_batchub_axis,
                }
            )

            return res_axis_list, res_pragma_axis

        def cub_compute_at():
            sch[cub].compute_at(sch[res], attach_axis_dict.get("cub_at_res_axis"))

        def aub_compute_at():
            if self._split_k_flag:
                sch[aub].compute_at(sch[cub], attach_axis_dict.get("aub_at_cub_axis"))
            else:
                sch[aub].compute_at(sch[res], attach_axis_dict.get("aub_at_res_axis"))

        def bub_compute_at():
            if self._split_k_flag:
                sch[bub].compute_at(sch[cub], attach_axis_dict.get("bub_at_cub_axis"))
            else:
                sch[bub].compute_at(sch[res], attach_axis_dict.get("bub_at_res_axis"))

        def config_fixpipe_attach_axis():
            def get_fixpipe_fb_attach_axis():
                fixpipe_fullload_flag = self._fixpipe_fusion.get_fixpipe_fullload_flag(self._cout1)
                if self._tiling.get("INPUT_L1_FB_param") == "all" and fixpipe_fullload_flag:
                    return fixpipe_full_axis
                return fixpipe_slice_axis

            fixpipe_slice_axis = attach_axis_dict.get("batch_in_at_res_axis")
            fixpipe_full_axis = attach_axis_dict.get("batch_out_at_res_axis")
            fixpipe_fb_attach_axis = get_fixpipe_fb_attach_axis()
            fixpipe_ub_attach_axis = fixpipe_full_axis if self._tiling.get("INPUT_L1_FB_param") == "all" \
                else fixpipe_slice_axis

            return fixpipe_fb_attach_axis, fixpipe_ub_attach_axis

        def fwc_ub_compute_at(emit_insn_dict):
            fwc_res = self._tensor_map.get("fmap")
            fwc_res_batch_axis, fwc_res_c1_axis, *_ = fwc_res.op.axis
            if self._not_align_flag:
                fifo_transpose_tensor = self._tensor_map.get("fifo_transpose_tensor")
                fifo_tensor = self._tensor_map.get("fifo_tensor")
                y_transpose_tensor = self._tensor_map.get("y_transpose_tensor")
                x_transpose_tensor = self._tensor_map.get("x_transpose_tensor")
                y_reshape_tensor = self._tensor_map.get("y_reshape_tensor")
                x_reshape_tensor = self._tensor_map.get("x_reshape_tensor")
                y_ub_tensor = self._tensor_map.get("y_ub_tensor")
                x_ub_tensor = self._tensor_map.get("x_ub_tensor")
                fifo_not_align_scene_tensor_ist = [fifo_transpose_tensor, fifo_tensor, y_transpose_tensor,
                    x_transpose_tensor, y_reshape_tensor, x_reshape_tensor, y_ub_tensor, x_ub_tensor]
                for tensor in fifo_not_align_scene_tensor_ist:
                    sch[tensor].compute_at(sch[fwc_res], fwc_res_batch_axis)
                sch[y_reshape_tensor].compute_inline(instant=True)
                sch[x_reshape_tensor].compute_inline(instant=True)

            fwc_ub = self._tensor_map.get("fwc_ub")
            fwc_ub_batch_axis, *_ = fwc_ub.op.axis
            sch[fwc_ub].compute_at(sch[fwc_res], fwc_res_batch_axis)
            emit_insn_dict.update(fwc_ub_pragma_axis=fwc_ub_batch_axis,
                                  fwc_res_pragma_axis=fwc_res_c1_axis)

        sch = self._sch
        batch = self._batch
        weight_h = self._weight_h
        weight_w = self._weight_w
        dilate_h = self._dilate_h
        dilate_w = self._dilate_w
        group = self._group
        cin1_data = self._cin1_data
        cin1_weight = self._cin1_weight
        attach_axis_dict = {}
        _, _, n0_align = get_cube_mkn(self._weight_dtype)

        # ==========================parse tiling==================================
        aub_tiling = self._tiling["AUB_shape"]
        bub_tiling = self._tiling["BUB_shape"]
        cub_tiling = self._tiling["CUB_shape"]
        pingpong_buffer = self._tiling["manual_pingpong_buffer"]
        bias_slice_flag = self._tiling["INPUT_L1_BT_param"]
        control_reorder_flag = self._tiling["control_reorder_flag"]
        batch_aub, ci1_aub, ci0_aub = aub_tiling
        batch_cub, co1_cub, co0_cub, h_cub, w_cub = cub_tiling

        # ===========================split and compute at=========================================
        aub = tensor_param["aub"]
        bub = tensor_param["bub"]
        cub = tensor_param["res_ub"] if self._fixpipe_fusion.flag else tensor_param["conv_res"]
        bias = tensor_param["bias_ub"]

        # tile
        # ===================================tile res============================================
        res_axis_list, res_pragma_axis = tile_tensor_res()
        # ===================================tile res_ub=======================================
        cub_pragma_axis = tile_tensor_cub()
        # ===================================tile al0============================================
        aub_pragma_axis = aub.op.axis[0]

        aub_at_res_axis, bub_at_res_axis, cub_at_res_axis, bias_at_res_axis = res_axis_list

        # ===================================compute at==========================================
        cub_compute_at()
        aub_compute_at()
        bub_compute_at()
        bias_compute_at()

        # fixpipe
        fixpipe_fb_attach_axis, fixpipe_ub_attach_axis = config_fixpipe_attach_axis()
        self._fixpipe_fusion.fixpipe_inputs_compute_at(
            sch, res, fixpipe_fb_attach_axis, fixpipe_ub_attach_axis)

        emit_insn_dict = {"aub_pragma_axis": aub_pragma_axis,
                          "bub_pragma_axis": bub.op.axis[0],
                          "cub_pragma_axis": cub_pragma_axis,
                          "bias_pragma_axis": bias.op.axis[0] if self._bias_flag else None,
                          "res_pragma_axis": res_pragma_axis}

        if self._fwc_pre_fusion_flag:
            fwc_ub_compute_at(emit_insn_dict)

        return res, emit_insn_dict, attach_axis_dict

    def special_process_post(self, res, tensor_param):
        """
        Special process after tiling is parsed.
        """
        def set_tensor_bound():
            """
            cal tensor bound and set buffer size
            """
            sch = self._sch
            aub = tensor_param.get("aub")
            bub = tensor_param.get("bub")
            cub = tensor_param.get("res_ub")
            bias_ub = tensor_param.get("bias_ub")
            batch_a, ci1_a, ci0_a = self._tiling.get("AUB_shape")
            ci1_b, ci0_b, _, _ = self._tiling.get("BUB_shape")
            batch_c, co1_c, co0_c, h_c, w_c = self._tiling.get("CUB_shape")
            _, _, n0_align = get_cube_mkn(self._weight_dtype)
            # set fmap_ub, filter_ub, conv_op, out_tensor size
            hin_a = min(h_c * self._stride_h + self._hk_dilation, self._h_in)
            win_a = min(w_c * self._stride_w + self._wk_dilation, self._w_in)
            aub_bound = batch_a * ci1_a * ci0_a * hin_a * win_a
            bub_bound = co1_c * co0_c * ci1_b * ci0_b * self._weight_h * self._weight_w
            if self._enable_depthwise_flag:
                bub_bound = co1_c * co0_c * self._weight_h * self._weight_w
            cub_bound = batch_c * co1_c * co0_c * h_c * w_c
            # v350 must contain fixpipe fb due to only quant net support
            fixpipe_ub_bound = co1_c * co0_c if self._tiling.get("INPUT_L1_FB_param") == "slice" \
                else self._cout1 * n0_align
            fixpipe_fb_bound = co1_c * co0_c
            # set tensor bound
            sch[aub].set_buffer_size(aub_bound)
            sch[bub].set_buffer_size(bub_bound)
            sch[cub].set_buffer_size(cub_bound)
            # set fixpipe_param bound in ub and fb
            self._fixpipe_fusion.set_tensor_bound(sch, fixpipe_ub_bound, fixpipe_fb_bound)
            # when bias add to fixpipe_tmp_tensor(intermediate res) in split_k case:
            # bias cout task space in UB should be align to 16(which is N0)
            if self._bias_flag:
                bias_ub_size = ceil_div(co1_c * co0_c, n0_align) * n0_align
                sch[bias_ub].set_buffer_size(bias_ub_size)

        # set tensor buffer size
        set_tensor_bound()
        # use pass dynamic compile choice due to pass only support dynamic in future v350
        # 1. is_dynamic_shape: choose dynamic compile flag
        # 2. enable_branch_eliminator_else_case: main tail block merge flag, conv2d need to set false to optimize
        # 3. double_buffer_no_tial: control doublebuffer no tail block flag;
        build_config = {
            "is_dynamic_shape": True,
            "enable_branch_eliminator_else_case": False,
            "double_buffer_no_tial": True
        }
        set_fusion_buildcfg("conv2d", build_config)

    def double_buffer(self, tensor_param):
        """
        Enable pingpong buffer.
        """
        sch = self._sch
        pingpong_buffer = self._tiling["manual_pingpong_buffer"]

        aub = tensor_param["aub"]
        bub = tensor_param["bub"]
        if self._fixpipe_fusion.flag:
            cub = tensor_param["res_ub"]
        else:
            cub = tensor_param["conv_res"]

        pingpong_map = {
            "AUB_pbuffer": aub,
            "BUB_pbuffer": bub,
            "UB2OUT_pbuffer": cub,
        }

        for key, value in pingpong_buffer.items():
            if value == 2 and pingpong_map.get(key) is not None:
                sch[pingpong_map.get(key)].double_buffer()

        if self._fixpipe_fusion.flag:
            self._fixpipe_fusion.double_buffer(sch, pingpong_buffer)

    def map_insn(self, res, tensor_param, emit_insn_dict, attach_axis_dict):
        """
        Emit insn for each tensor.
        """
        def fwc_emit_insn():
            clean_cache_value = self._conv_param.offset_x
            if self._not_align_flag:
                fifo_transpose_tensor = self._tensor_map.get("fifo_transpose_tensor")
                fifo_tensor = self._tensor_map.get("fifo_tensor")
                y_transpose_tensor = self._tensor_map.get("y_transpose_tensor")
                x_transpose_tensor = self._tensor_map.get("x_transpose_tensor")
                y_reshape_tensor = self._tensor_map.get("y_reshape_tensor")
                x_reshape_tensor = self._tensor_map.get("x_reshape_tensor")
                y_ub_tensor = self._tensor_map.get("y_ub_tensor")
                x_ub_tensor = self._tensor_map.get("x_ub_tensor")
                transpose_attrs = {"is_trans_align": 1, "enable_vnchwconv_mode": 1}
                sch[fifo_transpose_tensor].emit_insn(fifo_transpose_tensor.op.axis[0], "vector_transpose",
                                                     attrs=transpose_attrs)
                sch[fifo_tensor].emit_insn(fifo_tensor.op.axis[0], "dma_copy")
                sch[y_transpose_tensor].emit_insn(y_transpose_tensor.op.axis[0], "vector_transpose",
                                                  attrs=transpose_attrs)
                sch[x_transpose_tensor].emit_insn(x_transpose_tensor.op.axis[0], "vector_transpose",
                                                  attrs=transpose_attrs)
                sch[y_reshape_tensor].emit_insn(y_reshape_tensor.op.axis[0], "phony_insn")
                sch[x_reshape_tensor].emit_insn(x_reshape_tensor.op.axis[0], "phony_insn")
                sch[y_ub_tensor].emit_insn(y_ub_tensor.op.axis[0], "dma_copy")
                sch[x_ub_tensor].emit_insn(x_ub_tensor.op.axis[0], "dma_copy")

            fwc_ub = self._tensor_map.get("fwc_ub")
            sch[fwc_ub].emit_insn(emit_insn_dict.get("fwc_ub_pragma_axis"), "dma_copy",
                                  attrs={"clean_cache_value": clean_cache_value})
            fwc_res = self._tensor_map.get("fmap")
            clean_cache_ub = self._tensor_map.get("clean_cache_ub")
            sch[clean_cache_ub].emit_insn(clean_cache_ub.op.axis[0], "dma_copy")
            sch[fwc_res].emit_insn(emit_insn_dict.get("fwc_res_pragma_axis"), "dma_copy")

        def aub_emit_insn():
            sch[aub].pragma(aub.op.axis[1], "loop_with_no_overlap_tensor")
            # set aub cin load ub actual value(because conv_ub_to_ub allow set k to no-align C0 value)
            _, cin1_aub_axis, _, _, cin0_aub_axis = aub.op.axis
            fmap_condition = tvm.all(
                tvm.any(cin1_aub_axis * get_cube_mkn(self._fmap_dtype)[C_INDEX] + cin0_aub_axis
                        < self._conv_param.cin_ori))
            sch[aub].set_store_predicate(fmap_condition, partition=True)
            emit_str = "dma_copy" if not self._fwc_pre_fusion_flag else "phony_insn"
            sch[aub].emit_insn(emit_insn_dict.get("aub_pragma_axis"), emit_str)

        def bub_emit_insn():
            sch[bub].emit_insn(emit_insn_dict.get("bub_pragma_axis"), "dma_copy")

        def bias_emit_insn():
            if self._bias_flag:
                sch[bias].emit_insn(emit_insn_dict.get("bias_pragma_axis"), "dma_copy")

        def cub_emit_insn(fixpipe_flag):
            pragma_axis = cub.op.axis[0] if fixpipe_flag else emit_insn_dict.get("cub_pragma_axis")
            sch[cub].emit_insn(pragma_axis, "conv")

        def res_emit_insn():
            sch[res].emit_insn(emit_insn_dict.get("res_pragma_axis"), "dma_copy")

        def fixpipe_emit_insn():
            if self._split_k_flag: 
                sch[fixpipe_ub].emit_insn(emit_insn_dict.get("cub_pragma_axis"), "fixpipe_op",
                                          attrs={"k_outer": attach_axis_dict["aub_at_cub_axis"]})
            else:
                sch[fixpipe_ub].emit_insn(emit_insn_dict.get("cub_pragma_axis"), "fixpipe_op")

        # =============================emit insn=========================================
        sch = self._sch
        aub = tensor_param["aub"]
        bub = tensor_param["bub"]
        bias = tensor_param["bias_ub"]
        if self._fixpipe_fusion.flag:
            cub = tensor_param["conv_res"]
            cub_emit_insn(self._fixpipe_fusion.flag)
            fixpipe_ub = tensor_param["res_ub"]
            fixpipe_emit_insn()
            self._fixpipe_fusion.fixpipe_inputs_emit_insn(sch)
        else:
            cub = tensor_param["conv_res"]
            cub_emit_insn(self._fixpipe_fusion.flag)

        if self._fwc_pre_fusion_flag:
            fwc_emit_insn()
        aub_emit_insn()
        bub_emit_insn()
        bias_emit_insn()
        res_emit_insn()


def conv_v350_schedule(res, spec_node_list, sch_list, op_graph):
    """
    Schedule for Conv2d v350.
    """
    schedule = Conv2dScheduleV350(res, spec_node_list, ConvParam, sch_list, op_graph)

    info_dict = schedule.fetch_info_dict()

    schedule.fetch_tiling(info_dict)

    tensor_param = schedule.config_scope()

    schedule.special_process_pre(res, tensor_param)

    res, emit_insn_dict, attach_axis_dict = schedule.tile_attach_tensor(res, sch_list, tensor_param)

    schedule.special_process_post(res, tensor_param)

    schedule.double_buffer(tensor_param)

    schedule.map_insn(res, tensor_param, emit_insn_dict, attach_axis_dict)

    return []
