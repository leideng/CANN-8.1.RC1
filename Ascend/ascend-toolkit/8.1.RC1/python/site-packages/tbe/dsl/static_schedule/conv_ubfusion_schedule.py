#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Schedule of conv2d ub fusion in v220/v300.
"""
from collections import deque
from te.platform import cce_params
from tbe import tvm
from tbe.dsl.static_schedule.conv_schedule_util import clear_suffix
from tbe.dsl.static_schedule.conv_schedule_util import get_fixpipe_tag_list
from tbe.dsl.static_schedule.conv_schedule_util import is_elewise
from tbe.dsl.static_schedule.conv_schedule_util import is_placeholder
from tbe.dsl.static_schedule.conv_schedule_util import is_shape_equal
from tbe.dsl.static_schedule.conv_schedule_util import get_dsl_insn
from tbe.dsl.static_schedule.conv_schedule_util import ceil_div
from tbe.dsl.static_schedule.conv_schedule_util import is_wino_res_tensor
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.dsl.static_schedule.conv_schedule_util import is_quantconv2d_eltwise_fusion_tensor
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe.common.utils.op_util.op_util_conv2d import UB_COEFF_CONVERT
from tbe.common.utils.op_util.op_util_conv2d import ConstValue
from tbe.common.utils import log

DOUBLE_BUFFER_DISABLED = 1
DOUBLE_BUFFER_ENABLED = 2


class EltwiseUBFusion:
    """
    Class of common eltwise op ub fusion.
    process cub and common eltwise ub tensors.
    """
    def __init__(self, sch, res, conv_param, mixl2_flag,
                 fixpipe_res_list, fixpipe_res_gm2ub_list,
                 pre_op_list, next_op_list):
        self.cub_tag_list = get_fixpipe_tag_list()
        self.conv_param = conv_param
        self.flag = self.check_ub_fusion_flag(res)
        self.multiout_flag = res.op.name == "conv_virtual_res"
        self.mixl2_flag = mixl2_flag
        self.res_cache_write_tensor = None
        self.cache_write_flag = is_elewise(res)
        self.inline_tensors = []
        self.ub_body_tensors = set()
        self.ub_wino_res_tensors = set()
        self.ub_input_placeholders = set()
        self.ub_input_broadcast_tensors = set()
        self.ub_vdup_tensors = set()
        self.ub_output_tensors = set()
        self.cache_read_tensors = []
        self.cache_write_tensors = []
        self.emit_insn_dict = {
            "elewise_single_relu": "vector_auto",
            "elewise_single_round_d": "vector_conv_round",
            "elewise_single_round": "vector_conv_rint",
            "elewise_single_ceil": "vector_conv_ceil",
            "elewise_single_VS_max": "vector_maxs",
            "elewise_single_VS_min": "vector_mins",
            "elewise_binary_div": "vector_div",
            "elewise_binary_vcmpv_gt": "vector_gt",
            "elewise_binary_vcmpv_ge": "vector_ge",
            "elewise_binary_vcmpv_lt": "vector_lt",
            "elewise_binary_vcmpv_le": "vector_le",
            "elewise_binary_vcmpv_eq": "vector_eq",
            "elewise_binary_vcmpv_ne": "vector_ne",
            "elewise_binary_cmpsel_gt": "vector_select_gt",
            "elewise_binary_cmpsel_ge": "vector_select_ge",
            "elewise_binary_cmpsel_lt": "vector_select_lt",
            "elewise_binary_cmpsel_le": "vector_select_le",
            "elewise_binary_cmpsel_eq": "vector_select_eq",
            "elewise_binary_cmpsel_ne": "vector_select_ne",
            "elewise_binary_cmpsel": "vector_cmpsel",
            "elewise_binary_add": "vector_add",
            "elewise_binary_sub": "vector_sub",
            "elewise_binary_mul": "vector_mul",
            "elewise_binary_min": "vector_min",
            "elewise_binary_max": "vector_max",
            "elewise_binary_or": "vector_or",
            "elewise_binary_and": "vector_and",
            "elewise_single_lrelu": "vector_auto",
            "elewise_binary_addrelu": "vector_addrelu",
            "elewise_binary_subrelu": "vector_subrelu",
            "elewise_multiple_sel": "vector_select_bool",
            "elewise_single_rec": "vector_rec",
            "emit_insn_elewise_binary_cmp": "elewise_binary_cmp",
            "emit_insn_elewise_multiple_sel": "elewise_multiple_sel",
            "elewise_single_VS_mul": "vector_auto",
            "elewise_single_VS_add": "vector_auto",
            "elewise_single_cast": "vector_auto",
            "elewise_single_exp": "vector_auto",
            "elewise_single_log": "vector_auto",
            "elewise_set_value_variable": "vector_dup",
            "elewise_set_value_const": "vector_dup",
            "unknown_broadcast": "vector_auto",
            "elewise_single_abs": "vector_abs",
            "dequant_remove_pad": "vector_auto",
            "one_shape_broadcast": "vector_auto",
        }
        self.pre_op_list = pre_op_list
        self.next_op_list = next_op_list
        self.cub_list = [] # cub is the fixpipe_res tensor in ub.
        self.cgm_list = [] # cgm is the fixpipe_res tensor in gm when mix_l2.
        self.fixpipe_res_list = fixpipe_res_list
        self.cache_read_blacklist = [
            # pooling fusion
            "max_pooling_pad_top",
            "max_pooling_pad_bottom",
            "max_pooling_pad_left",
            "max_pooling_pad_right",
            # avgpool_update fusion
            "mean_matrix_float16",
            "mean_matrix_float32"
        ]
        self.cache_map = {}  # Map tensor to its cache_read or cache_write tensor on UB
        self.parse_cub_cgm(res, mixl2_flag, fixpipe_res_list, fixpipe_res_gm2ub_list)
        if self.flag:
            self.parse_ub_tensors(res)
            self.process_ub_multioutput(sch, res)

    def check_ub_fusion_flag(self, res):
        """
        Check for ub fusion situations.
        Special fusion reset cub_tag_list to get the real cub.
        """
        if not (is_support_fixpipe() or
                (self.conv_param.binary_mode and res.op.input_tensors[0].op.tag != "5HD_TRANS_NCHW")):
            return False

        if self.conv_param.convbn1_flag:
            self.cub_tag_list = ["convolution_c_ub"]
            return True

        if clear_suffix(res.op.tag) in self.cub_tag_list:
            return False

        if res.op.name == "conv_virtual_res":
            for tensor in res.op.input_tensors:
                if clear_suffix(tensor.op.tag) not in self.cub_tag_list:
                    # enable ub fusion when any branch reaches ub.
                    return True
            return False

        return True

    def parse_cub_cgm(self, res, mixl2_flag, fixpipe_res_list, fixpipe_res_gm2ub_list):
        """
        Parse cub and cgm tensors.
        """
        if mixl2_flag:
            self.cub_list = fixpipe_res_gm2ub_list.copy()
            self.cgm_list = fixpipe_res_list.copy()
        else:
            self.cub_list = fixpipe_res_list.copy()

        if self.multiout_flag:
            for cub_tensor in self.cub_list:
                cub_next = self.next_op_list[self.pre_op_list.index(cub_tensor)]
                if res in cub_next:
                    self.cub_list.remove(cub_tensor)

    def parse_ub_tensors(self, res):
        """
        Parse the body tensors and input placeholders in eltwise ub fusion.
        """
        tensor_queue = deque()
        if self.multiout_flag:
            self.ub_output_tensors = list(i for i in res.op.input_tensors if i not in self.fixpipe_res_list)
        else:
            self.ub_output_tensors = [res]

        for tensor in self.ub_output_tensors:
            tensor_queue.extend(list(i for i in tensor.op.input_tensors
                                     if (i not in self.fixpipe_res_list and i not in self.ub_output_tensors)))

        while tensor_queue:
            src_tensor = tensor_queue.popleft()

            if is_placeholder(src_tensor):
                if src_tensor.op.tag == "broadcast":
                    self.ub_input_broadcast_tensors.add(src_tensor)
                elif src_tensor.op.tag in ("elewise_set_value_variable", "elewise_set_value_const"):
                    self.ub_vdup_tensors.add(src_tensor)
                elif src_tensor.op.name not in self.cache_read_blacklist:
                    self.ub_input_placeholders.add(src_tensor)
            elif src_tensor.op.tag == "broadcast_for_tensor":
                self.inline_tensors.append(src_tensor)
            elif is_elewise(src_tensor):
                self.ub_body_tensors.add(src_tensor)
            elif is_wino_res_tensor(src_tensor):
                # not include wino res as res or one of multiout res
                self.ub_wino_res_tensors.add(src_tensor)
            elif is_quantconv2d_eltwise_fusion_tensor(src_tensor):
                # qunat_conv ub fusion case
                self.ub_body_tensors.add(src_tensor)

            if src_tensor.op.input_tensors:
                for tensor in src_tensor.op.input_tensors:
                    if tensor not in self.fixpipe_res_list and tensor not in self.ub_output_tensors:
                        # avoid to add ub output tensor when an ub output is refered
                        # to the pre node of another ub output. eg: conv + sigmoid(1) + quant(2).
                        tensor_queue.append(tensor)

    def process_ub_multioutput(self, sch, res):
        """
        Process ub tensors in multioutput situation.
        """
        def clone_ub_output(ub_output, ub_output_cachewrite):
            """
            Clone ub output tensor.
            """
            virtual_res_next = None
            ub_next = []
            """
            special ub case(such as mish): single ub op has multi usage of pre node,
            cache_clone readers should be multi ub tensors.
            mish structure be like:
            xx -- softplus -- tanh -- mul --
             |                         |
             ---------------------------
            """
            for tensor in ub_output_next:
                if tensor == res:
                    virtual_res_next = tensor
                elif clear_suffix(tensor.op.tag) == "fixpipe":
                    err_man.raise_err_specific(
                        "conv2d",
                        "find fixpipe compute call after ub op compute, which is not supported!"
                        )
                else:
                    ub_next.append(tensor)

            if virtual_res_next is not None and ub_next is not None:
                # ori ir:
                # ub_output —> virtual_res
                #           —> ub_next —> virtual_res
                # final ir:
                # ub_output.local.UB —> ub_output —> virtual_res
                #                    —> [ub_output_clone] —> ub_next —> virtual_res
                ub_output_clone = sch.cache_clone(ub_output, cce_params.scope_gm, ub_next)
                sch[ub_output_clone].compute_inline()

        def is_wino_res_branch_node(node_tensor):
            """
            judge if node_tensor is both wino_res and branch node, need to do cache_write and cache_clone
            structure be like:
            fixpipe_res -> wino_res -> elt_ub -> virtual_res
                                    -> virtual_res
            """
            if not is_wino_res_tensor(node_tensor):
                return False
            node_tensors_next = self.next_op_list[self.pre_op_list.index(node_tensor)]
            tensor_next_count = 0
            for tensor_next in node_tensors_next:
                if tensor_next != res:
                    tensor_next_count = tensor_next_count + 1
            wino_res_branch_node_flag = False
            if tensor_next_count > 1:
                err_man.raise_err_specific(
                        "conv2d", "maximum support output is 2!")
            elif tensor_next_count == 1:
                wino_res_branch_node_flag = True
            return wino_res_branch_node_flag

        if self.multiout_flag:
            for ub_output in self.ub_output_tensors:
                wino_res_branch_node_flag = is_wino_res_branch_node(ub_output)
                if is_elewise(ub_output) or wino_res_branch_node_flag:
                    ub_output_cachewrite = sch.cache_write(ub_output, cce_params.scope_ubuf)
                    self.cache_write_tensors.append(ub_output_cachewrite)
                    # after cache_write, the inputs of ub_output must cache_read to ub ub_output_cachewrite.
                    for next_op in self.next_op_list:
                        if ub_output in next_op:
                            next_op.remove(ub_output)
                            next_op.append(ub_output_cachewrite)

                ub_output_next = self.next_op_list[self.pre_op_list.index(ub_output)]

                if is_elewise(ub_output) or wino_res_branch_node_flag:
                    clone_ub_output(ub_output, ub_output_cachewrite)

    def get_reuse_tensor(self, tensor):
        """
        Get a pre-tensor that can be reused by the given tensor.

        Parameters
        ----------
        tensor: tvm.Tensor
            The tensor in op graph

        Returns
        -------
        reuse_tensor: tvm.Tensor
            A pre-tensor that can be reused by the given tensor
            if no tensor can be reused, return None.
        """
        pre_tensor_list = []
        for pre_tensor in tensor.op.input_tensors:
            if pre_tensor in self.inline_tensors:
                append_list = list(i for i in pre_tensor.op.input_tensors)
                pre_tensor_list.extend(append_list)
            else:
                pre_tensor_list.append(pre_tensor)

        for pre_tensor in pre_tensor_list:
            # When fetch info_dict, real_pre_tensor is pre_tensor itself.
            # When schedule, real_pre_tensor is cache write/read tensor of pre_tensor.
            real_pre_tensor = self.cache_map.get(pre_tensor, pre_tensor)
            if len(self.next_op_list[self.pre_op_list.index(pre_tensor)]) == 1 and \
                    is_shape_equal(real_pre_tensor, tensor) and \
                    real_pre_tensor.dtype == tensor.dtype:
                return real_pre_tensor

        return None

    def is_memory_unique(self, tensor):
        """
        Check if cannot reuse the memory of pre tensor.
        """
        if self.conv_param.dynamic_flag and not self.conv_param.binary_mode:
            return False

        if isinstance(self.get_reuse_tensor(tensor), tvm.Tensor):
            return False

        return True

    def coeff_eltwise_cal(self):
        """
        Calculate the ub space coefficient.
        """
        eltwise_coeff = 0
        channelwise_coeff = 0
        scalar_num = 0

        if self.flag:
            #=================================cache_write tensors======================================
            for output_tensor in self.ub_output_tensors:
                if is_elewise(output_tensor) and self.is_memory_unique(output_tensor):
                    eltwise_coeff += UB_COEFF_CONVERT.get(output_tensor.dtype) # res.local.UB

            for cub_tensor in self.cub_list:
                eltwise_coeff += UB_COEFF_CONVERT.get(cub_tensor.dtype)  # C_UB

            #=================================body tensors======================================
            for ub_tensor in self.ub_body_tensors:
                if self.is_memory_unique(ub_tensor):
                    eltwise_coeff += UB_COEFF_CONVERT.get(ub_tensor.dtype)

            #=================================wino res tensors===================================
            for ub_tensor in self.ub_wino_res_tensors:
                eltwise_coeff += UB_COEFF_CONVERT.get(ub_tensor.dtype)

            #=================================input tensors======================================
            for input_tensor in list(self.ub_input_placeholders) + list(self.ub_input_broadcast_tensors) + \
                    list(self.ub_vdup_tensors):
                if len(input_tensor.shape) == 1:
                    if tvm.tvm.tir.analysis.expr_deep_equal(input_tensor.shape[0], 1):
                        scalar_num += 1
                    else:
                        channelwise_coeff += UB_COEFF_CONVERT.get(input_tensor.dtype)
                else:
                    eltwise_coeff += UB_COEFF_CONVERT.get(input_tensor.dtype)
            # =================================output nx1 process=================================
            if self.conv_param.v200_width_out_1_flag:
                # Such situation has a Tensor "remove_padded_column".
                # Its succeeded tensor size should be half of cc_to_ub mad tensor size.
                eltwise_coeff *= ConstValue.ONE_HALF

        return eltwise_coeff, channelwise_coeff, scalar_num

    def calc_ub_vector_utilize(self, res):
        """
        Calculate the number of vector operation on UB.

        Parameters
        ----------
        res: Tensor

        Returns
        -------
        vector_utilize: int
            The number of vector operation on UB
        """
        if not self.flag:
            return 0

        post_fusion_vec_utilize = 0
        for ub_tensor in self.ub_body_tensors:
            if get_dsl_insn(ub_tensor) in self.emit_insn_dict:
                post_fusion_vec_utilize += 1
        for input_tensor in list(self.ub_input_placeholders) + list(self.ub_input_broadcast_tensors) + \
                            list(self.ub_vdup_tensors):
            if get_dsl_insn(input_tensor) in self.emit_insn_dict:
                post_fusion_vec_utilize += 1

        return post_fusion_vec_utilize

    def ub_tensors_reuse(self, sch, res):
        """
        Specify the reuse of tensors on UB.

        Parameters
        ----------
        sch: tvm.Schedule
        """
        if not self.flag or not self.conv_param.dynamic_flag:
            return

        # Check result reuse
        cache_write_res = self.cache_map.get(res, None)  # Get res.local.UB
        tensor_reused_by_result = self.get_reuse_tensor(res)  # Here use res in op graph
        if isinstance(cache_write_res, tvm.Tensor) and isinstance(tensor_reused_by_result, tvm.Tensor):
            sch[tensor_reused_by_result].reused_by(cache_write_res)
            log.debug("Tensor {} is reused by Tensor {}".format(tensor_reused_by_result, cache_write_res))

        # Check UB body tensor reuse
        for ub_tensor in self.ub_body_tensors:
            reusing_tensor = self.get_reuse_tensor(ub_tensor)
            if isinstance(reusing_tensor, tvm.Tensor):
                sch[reusing_tensor].reused_by(ub_tensor)
                log.debug("Tensor {} is reused by Tensor {}".format(reusing_tensor, ub_tensor))

    def cub_set_scope(self, sch):
        """
        Set scope for cub body tensors.
        """
        if self.flag:
            for cub_tensor in self.cub_list:
                sch[cub_tensor].set_scope(cce_params.scope_ubuf)

            for tensor in list(self.ub_body_tensors) + list(self.ub_input_broadcast_tensors) + \
                    list(self.ub_vdup_tensors) + list(self.ub_wino_res_tensors):
                sch[tensor].set_scope(cce_params.scope_ubuf)

    def align_cub(self, sch, block_m0):
        if self.flag:
            for cub_tensor in self.cub_list:
                cub_axis_buffer_aligns = list()
                cub_total_axis = len(cub_tensor.shape)
                for i in range(cub_total_axis):
                    cub_axis_buffer_aligns.append((1, 1))
                _, _, m_idx, _ = range(cub_total_axis)
                cub_axis_buffer_aligns[m_idx] = (1, block_m0)
                sch[cub_tensor].buffer_align(*cub_axis_buffer_aligns)

    def inputs_cache_read(self, sch, op_graph):
        """
        Cache read for ub input placeholders.
        """
        for tensor in self.ub_input_placeholders:
            input_ub_next = self.next_op_list[self.pre_op_list.index(tensor)]
            input_ub = sch.cache_read(tensor, cce_params.scope_ubuf, input_ub_next)
            self.cache_read_tensors.append(input_ub)
            self.cache_map[tensor] = input_ub

    def res_cache_write(self, sch, res):
        """
        Cache write for the res tensor of eltwise operation in ub fusion.
        """
        if self.cache_write_flag:
            res_cache_write = sch.cache_write(res, cce_params.scope_ubuf)
            self.res_cache_write_tensor = res_cache_write
            self.cache_write_tensors.append(res_cache_write)
            self.cache_map[res] = res_cache_write

    def res_buffer_tile(self, sch, res, tiling_infos):
        if self.flag:
            conv_param = self.conv_param
            out_height = conv_param.h_out
            out_width = conv_param.w_out
            attach_axis_dict = tiling_infos.get("attach_axis_dict")
            al1_nparts = tiling_infos.get("al1_nparts")
            m_dim = tiling_infos.get("m_dim")
            m_bound = tiling_infos.get("ma_al0") * tiling_infos.get("block_m0")
            multi_m_al1 = tiling_infos.get("multi_m_al1")

            ho_dim_axis = attach_axis_dict.get("res_m_dim_axis")
            ho_out2l1_axis = attach_axis_dict.get("singlecore_out2al1_loopm_axis")
            ho_l12l0_axis = attach_axis_dict.get("al12al0_loopm_axis")
            split_w_out2cl0_loopm_axis = attach_axis_dict.get("split_w_out2cl0_loopm_axis")

            ho_dim_bound = ceil_div(al1_nparts[1], m_dim)
            ho_out2l1_bound = ceil_div(out_height, al1_nparts[1])
            ho_offset = ho_dim_axis.var * ho_dim_bound + ho_out2l1_axis.var * \
                        ho_out2l1_bound + ho_l12l0_axis.var * multi_m_al1
            wo_offset = split_w_out2cl0_loopm_axis.var * m_bound
            wo_bound = tvm.min(m_bound, out_width - split_w_out2cl0_loopm_axis.var * m_bound)
            m_offset = ho_offset * out_width + wo_offset
            buffer_tile_list = []
            ub_output_shape_size = len(self.ub_output_tensors[0].shape)
            for i in range(ub_output_shape_size):
                buffer_tile_list.append((None, None))

            _, _, m_idx, _ = range(ub_output_shape_size)
            buffer_tile_list[m_idx] = (m_offset, wo_bound)
            if self.cache_write_flag:
                sch[self.res_cache_write_tensor].buffer_tile(*buffer_tile_list)
            if self.multiout_flag:
                for tensor in self.ub_output_tensors:
                    sch[tensor].buffer_tile(*buffer_tile_list)

    def ub_tensors_inline(self, sch):
        """
        Compute inline for certain tensors in ub.
        """
        if self.flag:
            for tensor in self.inline_tensors:
                sch[tensor].compute_inline()

    def ub_tensors_attach(self, sch, res, cl0_at_res_axis, cub_slice_axis):
        """
        Attach for ub tensors.
        """
        # attach for fixpipe_res
        if self.mixl2_flag:
            for cgm_tensor in self.cgm_list:
                sch[cgm_tensor].compute_at(sch[res], cl0_at_res_axis)
            for cub_tensor in self.cub_list:
                sch[cub_tensor].compute_at(sch[res], cub_slice_axis)
        else:
            for fixpipe_res in self.fixpipe_res_list:
                if fixpipe_res != res:
                    sch[fixpipe_res].compute_at(sch[res], cub_slice_axis)

        # attach for ub output tensor in multi output
        if self.multiout_flag:
            for tensor in self.ub_output_tensors:
                if tensor != res:
                    sch[tensor].compute_at(sch[res], cub_slice_axis)

        if self.flag:
            for tensor in self.cache_read_tensors + self.cache_write_tensors + \
                    list(self.ub_body_tensors) + \
                    list(self.ub_input_broadcast_tensors) + \
                    list(self.ub_vdup_tensors) + \
                    list(self.ub_wino_res_tensors):
                sch[tensor].compute_at(sch[res], cub_slice_axis)

    def double_buffer(self, sch, pingpong_value):
        """
        Enable double buffer for ub tensors.
        """
        # L0C -> ddr/L1/ub pingpong
        if not self.flag:
            return

        if pingpong_value == DOUBLE_BUFFER_DISABLED:
            return
        tensor_list = self.cub_list + list(self.ub_body_tensors) + \
                      self.cache_write_tensors + list(self.ub_wino_res_tensors)
        if self.conv_param.binary_static_flag:
            tensor_list += self.cache_read_tensors
        if pingpong_value == DOUBLE_BUFFER_ENABLED:
            for tensor in tensor_list:
                sch[tensor].double_buffer()
            return
        elif isinstance(pingpong_value, tvm.tir.expr.Var):
            for tensor in tensor_list:
                sch[tensor].double_buffer(pingpong_value)
        else:
            log.debug("Unsupported input of double_buffer(), tvm.var is valid.")

    def ub_tensors_emit_insn(self, sch):
        """
        Emit insn for ub tensors.
        """
        if not self.flag:
            return
        for tensor in self.cache_read_tensors:
            sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")
        for tensor in self.cache_write_tensors:
            sch[tensor].emit_insn(tensor.op.axis[0],
                                    self.emit_insn_dict.get(get_dsl_insn(tensor), get_dsl_insn(tensor)))
        for tensor in self.ub_body_tensors:
            sch[tensor].emit_insn(tensor.op.axis[0],
                                    self.emit_insn_dict.get(get_dsl_insn(tensor), get_dsl_insn(tensor)))
        for tensor in self.ub_input_broadcast_tensors:
            # broadcast an immediate operand to tensor in ub by vector dup.
            sch[tensor].emit_insn(tensor.op.axis[0], "vector_dup")
        for tensor in self.ub_vdup_tensors:
            vdup_pragma_axis_index = -1 if tensor.op.tag == "elewise_set_value_variable" else 0
            sch[tensor].emit_insn(tensor.op.axis[vdup_pragma_axis_index],
                                    self.emit_insn_dict.get(get_dsl_insn(tensor), get_dsl_insn(tensor)))
        if self.multiout_flag:
            for tensor in self.ub_output_tensors:
                if is_wino_res_tensor(tensor):
                    continue
                sch[tensor].emit_insn(tensor.op.axis[0], "dma_copy")


class QuantFusion:
    """
    Class of Ascend_quant op fusion.
    Ascend_quant is always the last op when it appeared in int8/int4 conv2d ub fusion dataflow
    if there is no strided_write.
    """
    def __init__(self, res, op_graph):
        self.flag = False
        self.quant_res = self.parse_quant_fusion_flag(res)
        self.fusion_tensors = {}
        self.quant_tensor_dict = {}
        self.emit_insn_dict = {
            "input_ub": "dma_padding",
            "cast_f16_ub": "vector_auto",
            "reform_by_vadds": "vector_auto",
            "reform_by_vmuls": "vector_auto",
            "offset_ub": "vector_auto",
            "cast_i8_ub": "vector_conv",
            "cast_i4_ub": "vector_conv",
            "scale_sqrt_ub": "vector_auto",
        }
        self.quant_padding_flag = False
        self.reform_emit_insn_axis = None
        self.parse_quant_tensors(op_graph)

    def parse_quant_fusion_flag(self, res):
        """
        Check whether it is conv + (xxx) + quant fusion.
        """
        if res.op.tag == "quant":
            self.flag = True
            return res
        if res.op.name == "conv_virtual_res":
            for tensor in res.op.input_tensors:
                if tensor.op.tag == "quant":
                    self.flag = True
                    return tensor
        return None

    def parse_quant_tensors(self, op_graph):
        """
        Parse the tensors in Ascend_quant fusion compute.
        """
        if self.flag:
            for lop in op_graph.body_ops:
                if lop["op"] in self.emit_insn_dict:
                    self.quant_tensor_dict[lop["op"]] = lop["dst_buffer"]

            input_ub = self.quant_tensor_dict.get("input_ub")
            c_out = input_ub.op.attrs["c_out"].value
            c1_transform = input_ub.op.attrs["c1_transform"].value
            self.quant_padding_flag = c_out % c1_transform != 0

    def cal_quant_coeff(self):
        """
        Calculate ub space coefficient of the tensors in Ascend_quant fusion compute.
        """
        quant_coeff = 0
        if self.flag:
            for tensor_name, tensor in self.quant_tensor_dict.items():
                if tensor_name == "input_ub" and not self.quant_padding_flag:
                    continue
                if tensor_name == "offset_ub":
                    continue
                quant_coeff += UB_COEFF_CONVERT.get(tensor.dtype)

        return quant_coeff

    def inline_input_ub(self, sch):
        """
        Compute inline for input_ub when output channel is 32 aligned in Ascend_quant.
        """
        if self.flag and not self.quant_padding_flag:
            sch[self.quant_tensor_dict.get("input_ub")].compute_inline()
            del self.quant_tensor_dict["input_ub"]

    def quant_tensors_set_scope(self, sch):
        """
        Set scope for the tensors in Ascend_quant fusion compute.
        """
        if self.flag:
            for _, tensor in self.quant_tensor_dict.items():
                sch[tensor].set_scope(cce_params.scope_ubuf)

    def split_reform_axis(self, sch):
        """
        split the c0=32 axis of reform_by_vadds/reform_by_vmuls into 2 and c0=16 axis for emit insn.
        """
        for tensor_name, tensor in self.quant_tensor_dict.items():
            if "reform" in tensor_name:
                axis_list = sch[tensor].op.axis[: - 1]
                reform_c0_outer_axis, reform_c0_axis = sch[tensor].split(tensor.op.axis[-1], 16)
                sch[tensor].reorder(reform_c0_outer_axis, *axis_list)
                self.reform_emit_insn_axis = tensor.op.axis[2]
                # optimization to be added.

    def quant_tensors_attach(self, sch, res, cub_slice_axis):
        """
        Attach for the tensors in Ascend_quant fusion compute.
        """
        if self.flag:
            for _, tensor in self.quant_tensor_dict.items():
                sch[tensor].compute_at(sch[res], cub_slice_axis)

    def quant_tensors_emit_insn(self, sch):
        """
        Emit insn for the tensors in Ascend_quant fusion compute.
        """
        if self.flag:
            for tensor_name, tensor in self.quant_tensor_dict.items():
                if "reform" in tensor_name:
                    sch[tensor].emit_insn(self.reform_emit_insn_axis, self.emit_insn_dict.get(tensor_name))
                else:
                    sch[tensor].emit_insn(tensor.op.axis[0], self.emit_insn_dict.get(tensor_name))
