#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gemm layout manager
"""

from abc import abstractmethod
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils.errormgr import error_manager_cube


SCOPE_MAP = {
    "UB": tbe_platform_info.scope_ubuf,
    "L1": tbe_platform_info.scope_cbuf,
    "L0C": tbe_platform_info.scope_cc,
    "L0A": tbe_platform_info.scope_ca,
    "L0B": tbe_platform_info.scope_cb,
    "BT": tbe_platform_info.scope_bt
}


class GemmDataLayout:
    """
    GemmDataLayout
    """
    # pad-fusion
    PAD_B = 1
    PAD_A = 2
    PAD_AB = 3
    NZ_VEC_B = 1
    NZ_VEC_A = 2
    NZ_VEC_AB = 3

    def __init__(self, sch, container, status_controller, dynamic_para) -> None:
        self.sch = sch
        self.container = container
        self.status_controller = status_controller
        self.tensor_list = dynamic_para["tensor_list"]
        self.tensor_map, self.para_map = self.tensor_list[3:]
        self.matrix_mode = "none"
        self.optmt_a, self.optmt_b = self.para_map.get("a_dtype"), self.para_map.get("b_dtype")

    @staticmethod
    def _add_tensor_to_list(tensor, tensors_list_list):
        if tensor is None:
            return
        for tensors_list in tensors_list_list:
            if tensor not in tensors_list:
                tensors_list.append(tensor)

    @abstractmethod
    def set_data_layout(self):
        """
        set data layout
        """

    def _set_and_add_tensor_to_list(self, tensor_map, scope_name, name_list, tensors_list=None):
        """
        set scope for tensor and save to tensors_list
        """
        if len(name_list) == 1:
            base_name = save_name = name_list[0]
        else:
            base_name, save_name = name_list
        tensor = tensor_map.get(base_name)
        if save_name is None:
            save_name = base_name
        if tensor is not None:
            self.sch[tensor].set_scope(SCOPE_MAP.get(scope_name))
            self.container.tensor_map[save_name] = tensor
            if tensors_list is not None and tensor not in tensors_list:
                tensors_list.append(tensor)

    def _add_key_value(self, key, value):
        """
        add key-value to dict
        """
        buffer_reuse_dict = self.container.buffer_reuse_dict
        if (key is not None) and (value is not None):
            buffer_reuse_dict[key] = value

    def _find_key_by_mode(self, method_dict):
        key_list = list(method_dict.keys())
        for key in key_list:
            if self.matrix_mode in key:
                return key
        return ("none",)


class GemmDataLayoutAMatrix(GemmDataLayout):
    """
    set data layout for a matrix
    """

    def __init__(self, sch, container, status_controller, dynamic_para) -> None:
        super().__init__(sch, container, status_controller, dynamic_para)
        self.matrix_mode = self.para_map.get("a_matrix_mode", "none")

    def set_data_layout(self):
        """
        set_data_layout_a_matrix
        """
        tensors_in_aub = self.container.tensors_in_aub
        for tensor_name in ("a_ub_aligned", "a_ub_general", "a_int82fp16", "a_transpose", "a_ub_virtual_align"):
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", [tensor_name], tensors_in_aub)
        if self.matrix_mode in ("nd2Zz_vnchwconv", "nd_gevm"):
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["a_ub_fract"])
        self._set_and_add_tensor_to_list(self.tensor_list[0], "UB", ["tensor_a_aligned", "a_ub"])

        self._set_data_layout_by_mode()

        if self.tensor_map.get("a_ub_virtual_align") is not None:
            a_ub_virtual_align = self.container.tensor_map.get("a_ub_virtual_align")
            self.container.tensor_map["a_ub"] = self.sch.cache_read(
                self.container.tensor_map.get("a_placehold"),
                tbe_platform_info.scope_ubuf,
                [a_ub_virtual_align])
            self._add_key_value(a_ub_virtual_align, self.container.tensor_map.get("a_ub"))

        self._add_tensor_to_list(self.container.tensor_map.get("a_ub"), [tensors_in_aub])
        self._add_tensor_to_list(self.container.tensor_map.get("a_ub_fract"), [tensors_in_aub])

    def _set_data_layout_mode_default(self):
        self.container.tensor_map["a_l1"] = self.sch.cache_write(
            self.container.tensor_map.get("a_l0a"), tbe_platform_info.scope_cbuf)
        if self.matrix_mode == "Nz2Zz_int82fp32":
            # int82fp32 need cast to fp16
            self.container.tensor_map["a_ub"] = self.sch.cache_read(self.container.tensor_map.get("a_placehold"),
                tbe_platform_info.scope_ubuf, [self.container.tensor_map.get("a_int82fp16")])
            self.container.tensor_map["a_ub_fract"] = self.sch.cache_write(self.container.tensor_map.get("a_l1"),
                tbe_platform_info.scope_ubuf)

    def _set_data_layout_nz2zz(self):
        self.container.tensor_map["a_l1"] = self.sch.cache_read(
            self.container.tensor_map.get("a_placehold"),
            tbe_platform_info.scope_cbuf,
            [self.container.tensor_map.get("a_l0a")]
        )

    def _set_data_layout_nd2zz_vnchwconv(self):
        self.container.tensor_map["a_l1"] = self.sch.cache_read(
            self.container.tensor_map.get("a_ub_fract"),
            tbe_platform_info.scope_cbuf,
            [self.container.tensor_map.get("a_l0a")]
        )

    def _set_data_layout_nd_gemv(self):
        self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["a_l1"])
        if self.optmt_a == "float16":
            self.container.tensor_map["a_ub_fract"] = self.sch.cache_write(
                self.container.tensor_map.get("a_l1"), tbe_platform_info.scope_ubuf)

    def __set_data_layout_nd2zz_with_mix(self):
        if self.tensor_list[0].get("tensor_a_l1") is None:
            tensor_a_workspace = self.container.tensor_map.get("a_l0a")
            self.sch[tensor_a_workspace].set_scope("")
            self.container.tensor_map["a_l1"] = self.sch.cache_read(
                tensor_a_workspace, tbe_platform_info.scope_cbuf, [self.container.tensor_map.get("c_l0c")]
            )
            self.container.tensor_map["a_l0a"] = self.sch.cache_read(
                self.container.tensor_map.get("a_l1"), tbe_platform_info.scope_ca,
                [self.container.tensor_map.get("c_l0c")]
            )
            self.container.mix_workspace_tensor["tensor_a_workspace"] = tensor_a_workspace
        else:
            tensor_a_workspace = self.tensor_list[0].get("tensor_a_l1")
            self.container.tensor_map["a_l1"] = self.sch.cache_read(
                tensor_a_workspace, tbe_platform_info.scope_cbuf, self.container.tensor_map.get("a_l0a")
            )
            self.container.mix_workspace_tensor["tensor_a_workspace"] = tensor_a_workspace

        if self.optmt_a == "float16":
            self.container.tensor_map["a_ub_fract"] = self.sch.cache_write(
                self.container.mix_workspace_tensor.get("tensor_a_workspace"),
                tbe_platform_info.scope_ubuf)

    def __set_data_layout_nd2zz_without_mix(self):
        if self.tensor_list[0].get("tensor_a_l1") is None:
            self.container.tensor_map["a_l1"] = self.sch.cache_write(
                self.container.tensor_map.get("a_l0a"), tbe_platform_info.scope_cbuf)
        else:
            self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["a_l1"])

        if self.optmt_a == "float16":
            self.container.tensor_map["a_ub_fract"] = self.sch.cache_write(
                self.container.tensor_map.get("a_l1"), tbe_platform_info.scope_ubuf)

    def _set_data_layout_nd2zz(self):
        if self.status_controller.support_mix_l2_fusion:
            self.__set_data_layout_nd2zz_with_mix()
        else:
            self.__set_data_layout_nd2zz_without_mix()

    def _set_data_layout_nd2zz_trans_fusion(self):
        # milan pre transdata fusion or fc nd input
        if self.para_map.get("nz_fusion_flag", 0) not in [self.NZ_VEC_A, self.NZ_VEC_AB]:
            self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["a_l1"])
        if self.para_map.get("nz_fusion_flag", 0) in [self.NZ_VEC_A, self.NZ_VEC_AB]:
            self.container.tensor_map["a_l1"] = self.sch.cache_read(
                self.tensor_map.get("a_l1"),
                tbe_platform_info.scope_cbuf,
                [self.container.tensor_map.get("a_l0a")]
            )
            tensor_a_workspace = self.container.tensor_map.get("a_l1").op.input_tensors[0]
            self.container.mix_workspace_tensor["tensor_a_workspace"] = tensor_a_workspace
            self.container.tensor_map["a_ub_nz"] = self.sch.cache_write(
                self.container.mix_workspace_tensor.get("tensor_a_workspace"),
                tbe_platform_info.scope_ubuf
            )
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["a_ub_nd"])
        elif self.para_map.get("pad_flag", 0) in [self.PAD_A, self.PAD_AB]:
            tensor_a_workspace = self.tensor_map.get("a_l1").op.input_tensors[0]
            self.container.mix_workspace_tensor["tensor_a_workspace"] = tensor_a_workspace
            self.container.tensor_map["a_ub_pad"] = self.sch.cache_write(
                self.container.mix_workspace_tensor.get("tensor_a_workspace"),
                tbe_platform_info.scope_ubuf
            )

    def _set_data_layout_nhwc2zz(self):
         # milan fc pre transdata fusion
        self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["a_l1"])
        self.sch[self.tensor_map.get("a_l1_5hd")].compute_inline()

    def _set_data_layout_by_mode(self):
        method_dict = {
            ("none", "nd2Zz_int8", "nd_gevm", "Nz2Zz_int82fp32"): self._set_data_layout_mode_default,
            ("Nz2Zz", "fractal_gemv", "Zz_trans"): self._set_data_layout_nz2zz,
            ("nd2Zz_vnchwconv",): self._set_data_layout_nd2zz_vnchwconv,
            ("nd_gemv",): self._set_data_layout_nd_gemv,
            ("nd2Zz",): self._set_data_layout_nd2zz,
            ("nd2Zz_trans_fusion",): self._set_data_layout_nd2zz_trans_fusion,
            ("nhwc2Zz",): self._set_data_layout_nhwc2zz
        }
        method_key = self._find_key_by_mode(method_dict)
        if method_key not in method_dict:
            error_manager_cube.raise_err_specific("Matmul", f"a_matrix_mode[{self.matrix_mode}] is not supported")
        return method_dict.get(method_key)()


class GemmDataLayoutBMatrix(GemmDataLayout):
    """
    GemmDataLayoutBMatrix
    """

    def __init__(self, sch, container, status_controller, dynamic_para) -> None:
        super().__init__(sch, container, status_controller, dynamic_para)
        self.matrix_mode = self.para_map.get("b_matrix_mode", "none")

    def set_data_layout(self):
        """
        set_data_layout_b_matrix
        """
        tensors_in_bub = self.container.tensors_in_bub
        for tensor_name in ("b_ub_aligned", "b_ub_general", "b_int82fp16", "b_transpose"):
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", [tensor_name], tensors_in_bub)
        self._set_and_add_tensor_to_list(self.tensor_list[0], "UB", ["tensor_b_aligned", "b_ub"])
        self._set_data_layout_by_mode()
        self._add_tensor_to_list(self.container.tensor_map.get("b_ub_fract"), [tensors_in_bub])
        self._add_tensor_to_list(self.container.tensor_map.get("b_ub"), [tensors_in_bub])

    def _set_data_layout_mode_default(self):
        b_l0b = self.container.tensor_map.get("b_l0b")
        if "tile_L1_n" in b_l0b.op.attrs:
            self.container.tensor_map["compress_index"] = b_l0b.op.input_tensors[0]
            self.container.tensor_map["b_l1"] = self.sch.cache_write(b_l0b, tbe_platform_info.scope_cbuf)
        else:
            self.container.tensor_map["b_l1"] = self.sch.cache_read(
                self.container.tensor_map.get("b_placehold"), tbe_platform_info.scope_cbuf, [b_l0b])

    def _set_data_layout_nd_gemv(self):
        self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["b_l1"])
        if self.optmt_b == "float16":
            self.container.tensor_map["b_ub_fract"] = self.sch.cache_write(
                self.container.tensor_map.get("b_l1"),
                tbe_platform_info.scope_ubuf
            )

    def _set_data_layout_nd2zn_vnchwconv(self):
        self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["b_ub_fract"])
        self.container.tensor_map["b_l1"] = self.sch.cache_read(
            self.container.tensor_map.get("b_ub_fract"),
            tbe_platform_info.scope_cbuf,
            [self.container.tensor_map.get("b_l0b")]
        )

    def _set_data_layout_nd2nz_int8(self):
        self.container.tensor_map["b_l1"] = self.sch.cache_write(
            self.container.tensor_map.get("b_l0b"),
            tbe_platform_info.scope_cbuf
        )

    def _set_data_layout_nd_zn2zn_int82fp32(self):
        self.container.tensor_map["b_ub"] = self.sch.cache_read(
            self.container.tensor_map.get("b_placehold"),
            tbe_platform_info.scope_ubuf,
            [self.container.tensor_map.get("b_int82fp16")]
        )
        self.container.tensor_map["b_l1"] = self.sch.cache_write(
            self.container.tensor_map.get("b_l0b"),
            tbe_platform_info.scope_cbuf
        )
        self.container.tensor_map["b_ub_fract"] = self.sch.cache_write(
            self.container.tensor_map.get("b_l1"),
            tbe_platform_info.scope_ubuf
        )

    def __set_data_layout_mode_nd2zn_with_mix(self):
        if self.tensor_list[0].get("tensor_b_l1") is None:
            tensor_b_workspace = self.container.tensor_map.get("b_l0b")
            self.sch[tensor_b_workspace].set_scope("")
            self.container.tensor_map["b_l1"] = self.sch.cache_read(
                tensor_b_workspace, tbe_platform_info.scope_cbuf, [self.container.tensor_map.get("c_l0c")]
            )
            self.container.tensor_map["b_l0b"] = self.sch.cache_read(
                self.container.tensor_map.get("b_l1"), tbe_platform_info.scope_cb,
                [self.container.tensor_map.get("c_l0c")]
            )
            self.container.mix_workspace_tensor["tensor_b_workspace"] = tensor_b_workspace
        else:
            tensor_b_workspace = self.tensor_list[0].get("tensor_b_l1")
            self.container.tensor_map["b_l1"] = self.sch.cache_read(
                tensor_b_workspace, tbe_platform_info.scope_cbuf, [self.container.tensor_map.get("b_l0b")]
            )
            self.container.mix_workspace_tensor["tensor_b_workspace"] = tensor_b_workspace

        self.container.tensor_map["b_ub_fract"] = self.sch.cache_write(
            self.container.mix_workspace_tensor.get("tensor_b_workspace"),
            tbe_platform_info.scope_ubuf
        )

    def __set_data_layout_mode_nd2zn_without_mix(self):
        if self.tensor_list[0].get("tensor_b_l1") is None:
            self.container.tensor_map["b_l1"] = self.sch.cache_write(
                self.container.tensor_map.get("b_l0b"), tbe_platform_info.scope_cbuf)
        else:
            self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["b_l1"])
        self.container.tensor_map["b_ub_fract"] = self.sch.cache_write(
            self.container.tensor_map.get("b_l1"),  tbe_platform_info.scope_ubuf
        )

    def _set_data_layout_mode_nd2zn(self):
        if self.status_controller.support_mix_l2_fusion:
            self.__set_data_layout_mode_nd2zn_with_mix()
        else:
            self.__set_data_layout_mode_nd2zn_without_mix()

    def _set_data_layout_nd2zn_trans_fusion(self):
        if self.para_map.get("nz_fusion_flag", 0) not in [self.NZ_VEC_B, self.NZ_VEC_AB]:
            self._set_and_add_tensor_to_list(self.tensor_map, "L1", ["b_l1"])
        if self.para_map.get("nz_fusion_flag", 0) in [self.NZ_VEC_B, self.NZ_VEC_AB]:
            self.container.tensor_map["b_l1"] = self.sch.cache_read(
                self.tensor_map.get("b_l1"),
                tbe_platform_info.scope_cbuf,
                [self.container.tensor_map.get("b_l0b")]
            )
            tensor_b_workspace = self.container.tensor_map.get("b_l1").op.input_tensors[0]
            self.container.mix_workspace_tensor["tensor_b_workspace"] = tensor_b_workspace
            self.container.tensor_map["b_ub_nz"] = self.sch.cache_write(
                self.container.mix_workspace_tensor.get("tensor_b_workspace"),
                tbe_platform_info.scope_ubuf
            )
            self._set_and_add_tensor_to_list(self.tensor_map, "UB", ["b_ub_nd"])
        elif self.para_map.get("pad_flag", 0) in [self.PAD_B, self.PAD_AB]:
            tensor_b_workspace = self.tensor_map.get("b_l1").op.input_tensors[0]
            self.container.mix_workspace_tensor["tensor_b_workspace"] = tensor_b_workspace
            self.container.tensor_map["b_ub_pad"] = self.sch.cache_write(
                self.container.mix_workspace_tensor.get("tensor_b_workspace"),
                tbe_platform_info.scope_ubuf
            )

    def _set_data_layout_nz2zn(self):
        self.container.tensor_map["b_l1"] = self.sch.cache_read(
            self.container.tensor_map.get("b_placehold"),
            tbe_platform_info.scope_cbuf,
            [self.container.tensor_map.get("b_l0b")]
        )

    def _set_data_layout_by_mode(self):
        method_dict = {
            ("nd_gemv",): self._set_data_layout_nd_gemv,
            ("nd2Zn_vnchwconv",): self._set_data_layout_nd2zn_vnchwconv,
            ("nd2Zn_int8",): self._set_data_layout_nd2nz_int8,
            ("Zn2Zn_int82fp32",): self._set_data_layout_nd_zn2zn_int82fp32,
            ("nd2Zn",): self._set_data_layout_mode_nd2zn,
            ("nd2Zn_trans_fusion",): self._set_data_layout_nd2zn_trans_fusion,
            ("Nz2Zn", "Zn2Zn"): self._set_data_layout_nz2zn,
            ("none",): self._set_data_layout_mode_default
        }
        method_key = self._find_key_by_mode(method_dict)
        if not method_key:
            error_manager_cube.raise_err_specific("Matmul", "b_matrix_mode[{self.matrix_mode}] is not supported")
        return method_dict.get(method_key)()
