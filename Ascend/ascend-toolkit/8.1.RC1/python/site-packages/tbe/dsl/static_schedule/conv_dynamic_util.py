#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
from functools import reduce
from tbe import tvm
from tbe.common.utils import log
from tbe.common.context import get_context
from tbe.common.platform import CUBE_MKN
from tbe.common.platform import get_soc_spec
from tbe.common.utils.errormgr import error_manager_cube as err_man
from tbe.common.utils.op_util import op_util_conv2d
from tbe.common.utils.op_util.op_util_conv2d import TilingDataKey
from tbe.common.utils.op_util.op_util_conv2d import AttachMode
from tbe.common.utils.op_util.op_util_conv2d import is_support_fixpipe
from tbe.common.utils.op_util.op_util_conv2d import is_v300_soc, is_support_v220
from tbe.common.utils.op_util.op_util_conv2d import BinaryTilingKey
from tbe.common.utils.op_util.op_util_conv2d import Conv2dTensorName
from tbe.common.utils.op_util.op_util_conv2d import ConstValue
from tbe.dsl.static_schedule.conv_schedule_util import ceil, ceil_div
from tbe.dsl.static_schedule.conv_schedule_util import ConvIRSimplification
from tbe.dsl.base.operation import get_te_var
from te.platform import cce_params


def test_set_value(sch, cache_tiling):
    """
    set values to binary vars
    """
    contex = get_context()
    compile_test_flag = False
    test_tiling_data_map = contex.get_addition("test_tiling_data")
    if test_tiling_data_map is not None:
        compile_test_flag = True

    if compile_test_flag:
        for key in test_tiling_data_map.keys():
            sch.set_var_value(cache_tiling.get(key), test_tiling_data_map.get(key))
            log.warn("set_value: {} -> {}".format(key, test_tiling_data_map.get(key)))


class DynamicShape:
    """
    Class of dynamic shape.
    """
    def __init__(self, conv_param, weight_dtype, tiling_dict_flag, tiling_case, var_range):
        self.weight_dtype = weight_dtype
        if conv_param.dynamic_flag != conv_param.binary_mode:
            log.error("dynamic flag is not same as binary mode")
        self.flag = conv_param.dynamic_flag
        self.var_map = conv_param.dyn_var_map
        self.tiling_dict_flag = tiling_dict_flag
        self.var_range = var_range
        self.cache_tiling = self.init_cache_tiling(tiling_case)
        self.tiling_case = self.init_tiling_case(tiling_case, conv_param)

        #==========combined parameters==================
        self.h_dynamic = "fmap_h" in self.var_map
        self.w_dynamic = "fmap_w" in self.var_map
        self.hw_dynamic = self.h_dynamic or self.w_dynamic
        self.n_dynamic = "batch_n" in self.var_map
        self.conv_param = conv_param
        self.attach_flags = {}

    @staticmethod
    def dynamic_mode_im2col_v2(sch, conv_param, tensor_param, tiling_param,
                               emit_insn_dict, input_nd_flag, l0a_load2d_flag):
        """
        Use im2col_v2 in dynamic shape situation.
        """
        stride_h_update = tiling_param["stride_h_update"]
        fmap = tensor_param["fmap"]
        al1 = tensor_param["al1"]
        al0 = tensor_param["al0"]
        aub_dma = tensor_param["aub_dma"]
        al1_im2col = tensor_param["al1_im2col"]
        dynamic_al0_pragma_axis = emit_insn_dict["dynamic_al0_pragma_axis"]
        conv_fm_c0_l1 = al1.shape[-1]  # -1: the last axis is in_c0 of al1 shape

        im2col_attr = {
            'set_fmatrix': 1,
            'conv_kernel_h': conv_param.filter_h,
            'conv_kernel_w': conv_param.filter_w,
            'conv_padding_top': conv_param.padding[0],
            'conv_padding_bottom': conv_param.padding[1],
            'conv_padding_left': conv_param.padding[2],
            'conv_padding_right': conv_param.padding[3],
            'conv_stride_h': stride_h_update,
            'conv_stride_w': conv_param.stride_w,
            'conv_dilation_h': conv_param.dilate_h,
            'conv_dilation_w': conv_param.dilate_w,
            'conv_fm_c': fmap.shape[4]*fmap.shape[1],
            'conv_fm_c1': fmap.shape[1],
            'conv_fm_h': fmap.shape[2],
            'conv_fm_w': fmap.shape[3],
            'conv_fm_c0': fmap.shape[4],
        }

        if conv_param.c04_flag:
            im2col_attr["enable_dynamic_c04"] = 1

        im2col_attr_0 = {
            'set_fmatrix': 0,
            'conv_kernel_h': conv_param.filter_h,
            'conv_kernel_w': conv_param.filter_w,
            'conv_padding_top': conv_param.padding[0],
            'conv_padding_bottom': conv_param.padding[1],
            'conv_padding_left': conv_param.padding[2],
            'conv_padding_right': conv_param.padding[3],
            'conv_stride_h': stride_h_update,
            'conv_stride_w': conv_param.stride_w,
            'conv_dilation_h': conv_param.dilate_h,
            'conv_dilation_w': conv_param.dilate_w,
            'conv_fm_c': conv_fm_c0_l1*fmap.shape[1],
            'conv_fm_c1': fmap.shape[1],
            'conv_fm_h': fmap.shape[2],
            'conv_fm_w': fmap.shape[3],
            'conv_fm_c0': conv_fm_c0_l1,
            'group_flag': 1,
            'l1_group_flag': 1
        }

        if is_support_v220() or is_v300_soc():
            # For v220/v300 soc, in dynamic scene, conv_fm_c and conv_fm_c1 attr should be sizeChannel on L1.
            al1_cin1_opt_idx = 2
            cin1_on_l1 = tiling_param.get("k1_al1", al1.shape[al1_cin1_opt_idx])
            im2col_attr_0['conv_fm_c'] = conv_fm_c0_l1 * cin1_on_l1
            im2col_attr_0['conv_fm_c1'] = cin1_on_l1

        if conv_param.binary_static_flag:
            im2col_attr_0.update({"set_pad_value": 1})

        if l0a_load2d_flag:
            sch[al1].emit_insn(al1.op.axis[0], "dma_copy")
        elif input_nd_flag:
            im2col_attr.update({"layout_transform": "nd2nz"})
            sch[al1].emit_insn(al1.op.axis[2], "dma_copy", im2col_attr)
        else:
            sch[al1].emit_insn(al1.op.axis[0], "dma_copy", im2col_attr)

        if l0a_load2d_flag:
            sch[al0].emit_insn(dynamic_al0_pragma_axis, "dma_copy")
        else:
            sch[al0].emit_insn(dynamic_al0_pragma_axis, 'im2col_v2', im2col_attr_0)

    @staticmethod
    def double_buffer_dynamic(sch, pingpong_buffer, pingpong_map):
        for key, value in pingpong_buffer.items():
            if not isinstance(value, tvm.tir.expr.Var):
                log.debug("Unsupported input of double_buffer(), tvm.var is valid.")
                continue

            tensor_object_list = pingpong_map.get(key)
            if tensor_object_list is None:
                continue

            if not isinstance(tensor_object_list, list):
                tensor_object_list = [tensor_object_list]

            for tensor in tensor_object_list:
                sch[tensor].double_buffer(value)

    @staticmethod
    def handle_var_range_binary(sch, range_map):
        for var_name, var_range in range_map.items():
            sch.set_var_range(get_te_var(var_name).get_tvm_var(), *var_range)

    def init_cache_tiling(self, tiling_case):
        if not self.flag or not tiling_case:
            return {}

        cache_tiling = {}
        for _, key_name in op_util_conv2d.TILINGDATA_KEY_MAP.items():
            cache_tiling[key_name] = get_te_var(key_name).get_tvm_var()
        return cache_tiling

    def init_tiling_case(self, tiling, conv_param):
        if not self.flag or not tiling:
            return tiling

        attach_at_flag = tiling.get("attach_at_flag")
        tiling["block_dim"] = [self.cache_tiling.get(TilingDataKey.BATCH_DIM),
                               self.cache_tiling.get(TilingDataKey.N_DIM),
                               self.cache_tiling.get(TilingDataKey.M_DIM),
                               self.cache_tiling.get(TilingDataKey.GROUP_DIM)]
        tiling["AL0_matrix"][0] = self.cache_tiling.get(TilingDataKey.M_L0)
        tiling["AL0_matrix"][1] = self.cache_tiling.get(TilingDataKey.K_L0)
        tiling["CL0_matrix"][0] = self.cache_tiling.get(TilingDataKey.N_UB_L0C_FACTOR) * \
                                  self.cache_tiling.get(TilingDataKey.CUB_N1)
        tiling["CL0_matrix"][1] = self.cache_tiling.get(TilingDataKey.M_L0)
        tiling["CUB_matrix"][0] = self.cache_tiling.get(TilingDataKey.CUB_N1)
        tiling["CUB_matrix"][1] = self.cache_tiling.get(TilingDataKey.M_L0)
        tiling["AUB_shape"][0] = self.cache_tiling.get(TilingDataKey.K_AUB)
        tiling["AUB_shape"][1] = self.cache_tiling.get(TilingDataKey.M_AUB)
        tiling['manual_pingpong_buffer']['AL0_pbuffer'] = self.cache_tiling.get(TilingDataKey.AL0_PBUFFER)
        tiling['manual_pingpong_buffer']['BL0_pbuffer'] = self.cache_tiling.get(TilingDataKey.BL0_PBUFFER)
        tiling['manual_pingpong_buffer']['CL0_pbuffer'] = self.cache_tiling.get(TilingDataKey.CL0_PBUFFER)
        tiling['manual_pingpong_buffer']['AL1_pbuffer'] = self.cache_tiling.get(TilingDataKey.AL1_PBUFFER)
        tiling['manual_pingpong_buffer']['BL1_pbuffer'] = self.cache_tiling.get(TilingDataKey.BL1_PBUFFER)
        tiling['manual_pingpong_buffer']['AUB_pbuffer'] = self.cache_tiling.get(TilingDataKey.AUB_PBUFFER)
        tiling['manual_pingpong_buffer']['BUB_pbuffer'] = self.cache_tiling.get(TilingDataKey.BUB_PBUFFER)
        tiling['manual_pingpong_buffer']['CUB_pbuffer'] = self.cache_tiling.get(TilingDataKey.CUB_PBUFFER)
        tiling['manual_pingpong_buffer']['UBG_pbuffer'] = self.cache_tiling.get(TilingDataKey.UBG_PBUFFER)

        if attach_at_flag.get("bl0_attach_flag") == AttachMode.ATTACH_FULL_LOAD:
            tiling["BL0_matrix"] = []
        else:
            tiling["BL0_matrix"][0] = self.cache_tiling[TilingDataKey.K_L0]
            tiling["BL0_matrix"][1] = self.cache_tiling[TilingDataKey.N_UB_L0C_FACTOR] * \
                                      self.cache_tiling[TilingDataKey.CUB_N1]

        # L1 buffer tiling
        if conv_param.l0a_dma_flag:
            reduce_al1 = self.cache_tiling[TilingDataKey.K_H] * self.cache_tiling[TilingDataKey.K_W]
        else:
            reduce_al1 = ((self.cache_tiling[TilingDataKey.K_H] - 1) *
                        self.cache_tiling[TilingDataKey.DILATION_H] + 1) * \
                        ((self.cache_tiling[TilingDataKey.K_W] - 1) *
                        self.cache_tiling[TilingDataKey.DILATION_W] + 1)
        reduce_bl1 = self.cache_tiling[TilingDataKey.K_H] * self.cache_tiling[TilingDataKey.K_W]

        if conv_param.c04_flag:
            reduce_al1 = ceil_div(reduce_al1, ConstValue.C04_CONST)
            reduce_bl1 = ceil_div(reduce_bl1, ConstValue.C04_CONST)

        if attach_at_flag.get("al1_attach_flag") == AttachMode.ATTACH_FULL_LOAD:
            tiling["AL1_shape"] = []
        else:
            if conv_param.l0a_dma_flag:
                # case 1: same with common scene, KAL1_16 is ci1's cut * kh * kw * ci0 // 16
                # case 2: KAL1_16 is (ci1 * kh * kw)'s cut * ci0 // 16
                tiling["AL1_shape"][0] = self.cache_tiling[TilingDataKey.KAL1_16]
            else:
                # common scene, KAL1_16 is ci1's cut * khDilation * kwDilation * ci0 // 16
                tiling["AL1_shape"][0] = self.cache_tiling[TilingDataKey.KAL1_16] // reduce_al1
            tiling["AL1_shape"][1] = self.cache_tiling[TilingDataKey.M_AL1_FACTOR]

        if attach_at_flag.get("bl1_attach_flag") == AttachMode.ATTACH_PASS:
            tiling["BL1_shape"] = None
        elif attach_at_flag.get("bl1_attach_flag") == AttachMode.ATTACH_FULL_LOAD:
            tiling["BL1_shape"] = []
        else:
            tiling["BL1_shape"][0] = self.cache_tiling[TilingDataKey.KBL1_16] // reduce_bl1
            tiling["BL1_shape"][1] = self.cache_tiling[TilingDataKey.N_BL1_FACTOR]

        if is_support_fixpipe():
            tiling["INPUT_L1_BT_param"] = None
            tiling["control_reorder_flag"] = 0
            # DMA scene, al1 reused buffer need to keep same DB status
            tiling['manual_pingpong_buffer']['AL1_IM2COL_pbuffer'] = self.cache_tiling.get(TilingDataKey.AL1_PBUFFER)
            tiling['manual_pingpong_buffer']['AL1_ZERO_pbuffer'] = self.cache_tiling.get(TilingDataKey.AL1_PBUFFER)

        return tiling

    def fetch_tiling_case(self):
        """
        Fetch tiling case in dynamic shape.
        """
        return self.tiling_case

    def handle_var_value_binary(self, sch):
        for key, value in self.conv_param.src_shape_attr.items():
            sch.set_var_value(self.cache_tiling.get(key), value)

    def handle_var_range(self, sch):
        """
        Set var range for hi, ho, wi, wo, batch.
        """
        if self.flag:
            range_map = op_util_conv2d.TILINGDATA_KEY_RANGE_MAP
            if self.conv_param.binary_static_flag:
                range_map = self.conv_param.option_dict.get("CONST_BINARY_TILING_DATA", dict())
            self.handle_var_range_binary(sch, range_map)
            self.handle_var_value_binary(sch)
 
            return

        var_range = self.var_range
        var_map = self.var_map

        if self.h_dynamic:
            fmap_h_range = var_range['fmap_h']
            ho_range = var_range['ho']
            sch.set_var_range(var_map['fmap_h'], fmap_h_range[0], fmap_h_range[1])
            sch.set_var_range(var_map['ho'], ho_range[0], ho_range[1])

        if self.w_dynamic:
            fmap_w_range = var_range['fmap_w']
            wo_range = var_range['wo']
            sch.set_var_range(var_map['fmap_w'], fmap_w_range[0], fmap_w_range[1])
            sch.set_var_range(var_map['wo'], wo_range[0], wo_range[1])

        if self.n_dynamic:
            batch_range = var_range['batch_n']
            sch.set_var_range(var_map['batch_n'], batch_range[0], batch_range[1])

        return

    def check_dynamic_overhead_opt_flag(self, tiling, binary_flag):
        """
        Fmap overhead opti is not supported when hi or wi is dynamic.
        """
        if self.hw_dynamic and tiling["A_overhead_opt_flag"]:
            err_man.raise_err_value_or_format_invalid(
                "conv2d", 'tiling["A_overhead_opt_flag"]', "False", "when dynamic shape.")

    def disable_memory_reuse(self, sch, tensor_param):
        """
        Disable memory reuse in dynamic situation.
        """
        if self.flag:
            al1 = tensor_param["al1"]
            bl1 = tensor_param["bl1"]
            al0 = tensor_param["al0"]
            bl0 = tensor_param["bl0"]
            cl0 = tensor_param["cl0"]

            # sequential_malloc
            sch.sequential_malloc(cce_params.scope_cbuf)
            sch.sequential_malloc(cce_params.scope_ca)
            sch.sequential_malloc(cce_params.scope_cb)
            sch.sequential_malloc(cce_params.scope_cc)

            # mem_unique
            sch[al1].mem_unique()
            sch[al0].mem_unique()
            if bl1 is not None:
                sch[bl1].mem_unique()
            sch[bl0].mem_unique()
            sch[cl0].mem_unique()

    def get_al1_bound(self, conv_param, tiling_param):
        """
        Get al1 bound for dynamic shape.
        """
        if self.flag:
            case_info = {
                "n_dynamic": self.n_dynamic,
                "hw_dynamic": self.hw_dynamic
                }
            if self.conv_param.binary_static_flag:
                case_info.update({"static_shape": True})
            al1_bound_list = get_al1_bound_common(case_info, conv_param, tiling_param)
            return al1_bound_list
        return []

    def set_al1_bound(self, sch, al1, al1_bound_list):
        """
        Set al1 bound for dynamic shape.
        """
        if self.flag:
            set_al1_buffer_size_common(sch, al1, al1_bound_list)

    def set_bl1_bound(self, sch, bl1, tiling_param, group_opt):
        """
        for bl1 bound set for dynamic shape
        """
        if self.tiling_case.get("attach_at_flag").get("bl1_attach_flag") == AttachMode.ATTACH_PASS:
            return 0

        if self.tiling_case.get("attach_at_flag").get("bl1_attach_flag") == AttachMode.ATTACH_FULL_LOAD:
            tiny_weight_fractal_flag = self.conv_param.get_tiny_weight_fractal_flag()  # tiny weight for op Pooling
            k1, n1, n0, k0 = self._get_bl1_frac_z_shape(tiling_param, tiny_weight_fractal_flag)
            _, n_dim, _, _ = tiling_param.get("block_dim")
            n1 = (n1 + n_dim - 1) // n_dim
            # solve the problem of exceeding bl1_buffer_size
            if not tiny_weight_fractal_flag:
                k1 = (k1 + group_opt - 1) // group_opt
            bl1_bound_max_size = k1 * n1 * n0 * k0
        else:
            bl1_tiling = tiling_param["bl1_tiling"]
            bl0_tiling = tiling_param["bl0_tiling"]
            n_bound_max = bl1_tiling[1]*bl0_tiling[1]*CUBE_MKN[bl1.dtype]['mac'][2]
            k_bound_max = bl1_tiling[0]*self.cache_tiling[TilingDataKey.K_H]*self.cache_tiling[TilingDataKey.K_W] * \
                CUBE_MKN[bl1.dtype]['mac'][1]
            bl1_bound_max_size = n_bound_max * k_bound_max
        sch[bl1].set_buffer_size(bl1_bound_max_size)
        return bl1_bound_max_size

    def set_bl0_bound(self, sch, bl0, tiling_param, group_opt):
        """
        When bl0 is fully loaded, there is a problem with CCE address inference.
        """
        _, n_dim, _, _ = tiling_param.get("block_dim")
        if self.tiling_case.get("attach_at_flag").get("bl0_attach_flag") == AttachMode.ATTACH_FULL_LOAD:
            k1, n1, n0, k0 = tiling_param.get("weight_fracz_shape")
            bl0_bound_size = (k1 * n1 * n0 * k0) // group_opt // n_dim
            sch[bl0].set_buffer_size(bl0_bound_size)
        else:
            bl0_tiling = tiling_param.get("bl0_tiling")
            sch[bl0].set_buffer_size(reduce((lambda x, y: x*y), bl0_tiling))

    def set_bias_bound(self, sch, bias_load_tensor, bias_bt_tensor, bias_l1_tensors: list):
        """
        Set buffer size of bias.

        Parameters
        ----------
        sch: tvm.Schedule
        bias_load_tensor: tvm.Tensor
        bias_bt_tensor: tvm.Tensor
          bias tensor on BT
        bias_l1_tensors: tvm.Tensor
          bias tensors on L1 except of bias_virtual_add tensor (bias is expressed by several tensors on L1).

        Returns
        -------
        NA
        """
        if not self.flag or not isinstance(bias_load_tensor, tvm.Tensor):
            return
        bias_cout0 = CUBE_MKN[bias_load_tensor.dtype]['mac'][2]  # 2: N index
        if not self.tiling_case.get(op_util_conv2d.BinaryTilingKey.BIAS_CHANNEL_WISE_FLAG, False):
            # Calculate bias full load size
            cout1_opt = self.cache_tiling.get(TilingDataKey.COUT1_OPT)
            group_opt = self.cache_tiling.get(TilingDataKey.GROUP_OPT)
            n_dim = self.cache_tiling.get(TilingDataKey.N_DIM)
            group_dim = self.cache_tiling.get(TilingDataKey.GROUP_DIM)
            cout1_one_core = (cout1_opt + n_dim - 1) // n_dim
            groups_one_core = (group_opt + group_dim - 1) // group_dim

            bias_full_load_size = cout1_one_core * groups_one_core * bias_cout0
            if self.conv_param.groupopt_flag:
                # cout1_one_core --> the number of cout1 per group on one core (aligned)
                # cout1_one_core * nDim --> the number of cout1 per group (aligned)
                # When there are more than 1 group on one core,
                #     the [complete groups size on one core], that is [(cout1_one_core * n_dim) * groups_one_core *
                #     bias_cout0], should be allocated to keep accessing bias data correct.
                bias_space_multiple = tvm.select(groups_one_core != tvm.const(1),
                                                 n_dim, tvm.const(1))
                bias_full_load_size *= bias_space_multiple
            # Set bias full load size
            log.debug("Set bias full load size: {}".format(bias_full_load_size))
            sch[bias_load_tensor].set_buffer_size(bias_full_load_size)
            for bias_l1_tensor in bias_l1_tensors:
                if isinstance(bias_l1_tensor, tvm.Tensor):
                    sch[bias_l1_tensor].set_buffer_size(bias_full_load_size)
        else:
            # Calculate bias slice size
            bias_cout1 = self.cache_tiling.get(TilingDataKey.CUB_N1)
            bias_slice_size = bias_cout1 * bias_cout0
            # Set bias slice case buffer size
            log.debug("Set bias slice size: {}".format(bias_slice_size))
            sch[bias_load_tensor].set_buffer_size(bias_slice_size)
            for bias_l1_tensor in bias_l1_tensors:
                if isinstance(bias_l1_tensor, tvm.Tensor):
                    sch[bias_l1_tensor].set_buffer_size(bias_slice_size)

        if isinstance(bias_bt_tensor, tvm.Tensor):
            bias_bt_dtype_bype = 4
            bt_size = get_soc_spec("BT_SIZE")
            sch[bias_bt_tensor].set_buffer_size(bt_size // bias_bt_dtype_bype)

    def set_cl0_bound(self, sch, cl0, cl0_tiling):
        """
        Set storage bound for CL0 in dynamic shape
        to solve the memory allocate problem of using storage_align for CL0.
        """
        if self.flag:
            sch[cl0].set_buffer_size(reduce((lambda x, y: x*y), cl0_tiling))

    def res_hw_dynamic_pragma(self, sch, res, res_pragma_axis):
        """
        Pragma for res when hw dynamic.
        """
        if self.hw_dynamic:
            sch[res].pragma(res_pragma_axis, "gm_no_sync", 1)

    def cal_reuse_al1(self, tiling_case):
        """
        cal reuse al1 flag
        """
        reuse_al1 = False
        reuse_al1_attach_flags = [
            {BinaryTilingKey.BL0_ATTACH_FLAG: 1,
             BinaryTilingKey.AL1_ATTACH_FLAG: 0,
             BinaryTilingKey.BL1_ATTACH_FLAG: 3},
            {BinaryTilingKey.BL0_ATTACH_FLAG: 1,
             BinaryTilingKey.AL1_ATTACH_FLAG: 0,
             BinaryTilingKey.BL1_ATTACH_FLAG: 1},
            {BinaryTilingKey.BL0_ATTACH_FLAG: 1,
             BinaryTilingKey.AL1_ATTACH_FLAG: 2,
             BinaryTilingKey.BL1_ATTACH_FLAG: 3},
            {BinaryTilingKey.BL0_ATTACH_FLAG: 1,
             BinaryTilingKey.AL1_ATTACH_FLAG: 1,
             BinaryTilingKey.BL1_ATTACH_FLAG: 3},
            {BinaryTilingKey.BL0_ATTACH_FLAG: 1,
             BinaryTilingKey.AL1_ATTACH_FLAG: 1,
             BinaryTilingKey.BL1_ATTACH_FLAG: 1}
        ]
        self.attach_flags = tiling_case
        if tiling_case in reuse_al1_attach_flags:
            reuse_al1 = True

        return reuse_al1

    def ir_simplify(self, sch, tensor_param):
        if not self.flag:
            return

        is_no_simplify_scene = self.conv_param.l0a_load2d_flag or \
                               self.conv_param.l0a_dma_flag or \
                               self.conv_param.c04_flag or \
                               is_v300_soc()
        if is_no_simplify_scene:
            return

        ir_simplifier = ConvIRSimplification(sch, tensor_param, self.conv_param.al0boundcheck_flag)
        ir_simplifier.ir_simplify()

    def _get_bl1_frac_z_shape(self, tiling_param, tiny_weight_fractal_flag=False):
        if tiny_weight_fractal_flag:
            bl1_frac_z_tensor = self.conv_param.get_tensor_map().get(Conv2dTensorName.FILTER)
            if bl1_frac_z_tensor is not None:
                return bl1_frac_z_tensor.shape

        return tiling_param["weight_fracz_shape"]


def set_al1_buffer_size_common(sch, al1, al1_bound_list):
    """
    Set al1 buffer size in frontend.
    """
    batch_al1, m_al1, k1_al1, in_c0 = al1_bound_list
    al1_bound = batch_al1 * m_al1 * k1_al1 * in_c0
    sch[al1].set_buffer_size(al1_bound)


def get_al1_bound_common(case_info, conv_param, tiling_param):
    """
    Get al1 bound from tiling info.
    """
    def get_dma_al1_split_bound():
        """
        get al1 size in dma load3d scene(not full load)
        dma al1 is fragz shape. m_al1 = multiple*m_l0, k1_al1 = k_al1*k0
        """
        multi_m_al1 = tiling_param.get("multi_m_al1")
        k1_al1 = tiling_param.get("k1_al1")
        m_al1 = multi_m_al1 * tiling_param.get("al0_tiling")[0]
        batch_al1 = tiling_param.get("batch_al1")
        return [batch_al1, m_al1, k1_al1, in_c0]

    def get_dma_al1_full_bound():
        """
        get al1 size in dma load3d scene(full load)
        dma al1 is fragz shape. m_al1 = multiple*m_l0, k1_al1 = k_al1*k0
        """
        k1_al1 = in_c1 * kernel_h * kernel_w
        m_al1 = ceil(out_width*out_height, in_c0)
        batch_al1 = 1
        return [batch_al1, m_al1, k1_al1, in_c0]

    def modify_m_for_conv1d_split_w(m_al1):
        w_in_predicted = (m_al1 - 1) * stride_w + kernel_w_dilation
        w_in = tvm.min(w_in_predicted, in_width)
        # conv1d scene h_in=h_out=1
        h_in = 1
        return h_in*w_in

    def modify_m_for_split_w():
        # splitw scene L1:1*mc  --> L0C:mc
        # [DDR] ALL
        #    for All (DDR->L1)
        #      [L1] 1
        #         for 1 (L1 -> L0C)
        #           [L0C] m_c
        #               for m_c
        w_in_predicted = (m_cl0 - 1) * stride_w + kernel_w_dilation
        w_in = tvm.min(w_in_predicted, in_width)
        # splitw scene hout=1
        ho_len = 1
        hi_max = get_hi_max(ho_len)
        return hi_max*w_in

    def modify_m_for_load3d():
        ho_len = tvm.floordiv(m_al1, out_width) + additional_rows
        hi_max = get_hi_max(ho_len)
        return hi_max*in_width

    def get_hi_max(ho_len):
        in_height_opt = (in_height - kernel_h) // stride_h + 1 if strideh_opti_flag else in_height
        hi_max = tvm.min(kernel_h_dilation + (ho_len - 1) * stride_h_update, in_height_opt)
        return hi_max

    def get_m_al1_full_load(in_height, in_width, l0a_load2d_flag, strideh_opti_flag, cin_0):
        """
        Get m_al1 in fmap full load to L1 scene.
        In load2d mode, m_al1 need to be cin_0 aligned.
        """
        if conv_param.conv1d_split_w_flag:
            if conv_param.dynamic_flag:
                cube_unit = 16
                m_al1 = ceil(out_width, cube_unit)
                return modify_m_for_conv1d_split_w(m_al1)
            return in_width

        in_height = (in_height - 1) // stride_h + 1 if strideh_opti_flag else in_height
        m_al1_full_load = ceil(in_height * in_width, cin_0) if l0a_load2d_flag else in_height * in_width
        return m_al1_full_load

    _, in_c1, in_height, in_width, in_c0 = tiling_param.get("fmap_5hd_shape")
    al1_tiling = tiling_param.get("al1_tiling")
    stride_h_update = tiling_param.get("stride_h_update")
    m_cl0 = tiling_param.get("m_cl0")

    stride_h = conv_param.stride_h
    stride_w = conv_param.stride_w
    kernel_h = conv_param.filter_h
    kernel_w = conv_param.filter_w
    kernel_h_dilation = conv_param.filter_h_dilation
    kernel_w_dilation = conv_param.filter_w_dilation
    out_width = conv_param.w_out
    out_height = conv_param.h_out
    l0a_load2d_flag = conv_param.l0a_load2d_flag
    strideh_opti_flag = conv_param.strideh_opti_flag
    ci1_opt = conv_param.para_dict.get("c1_opt")

    if conv_param.c04_flag:
        in_c0 = ConstValue.C04_CONST

    if al1_tiling:
        if conv_param.l0a_dma_flag:
            al1_bound_list = get_dma_al1_split_bound()
            return al1_bound_list

        multi_m_al1 = tiling_param.get("multi_m_al1")
        k1_al1 = tiling_param.get("k1_al1")
        m_al1 = multi_m_al1 * m_cl0
        batch_al1 = tiling_param.get("batch_al1")

        if l0a_load2d_flag:
            # Hi*Wi = Ho*Wo
            m_al1 = tvm.min(m_al1, ceil(in_height * in_width, in_c0))
        elif conv_param.conv1d_split_w_flag:
            m_al1 = modify_m_for_conv1d_split_w(m_al1)
        elif conv_param.split_w_flag:
            m_al1 = modify_m_for_split_w()

        else:
            if case_info.get("hw_dynamic"):
                additional_rows = tvm.select(
                    tvm.floormod(m_al1, out_width) == 0,
                    0,
                    tvm.select(tvm.floormod(m_al1 * 2, out_width) == 0, 1, 2))
            elif case_info.get("n_dynamic") or case_info.get("static_shape"):
                if m_al1 % out_width == 0:
                    additional_rows = 0
                elif m_al1 * 2 % out_width == 0:
                    additional_rows = 1
                else:
                    additional_rows = 2
            m_al1 = modify_m_for_load3d()
        al1_bound_list = [batch_al1, m_al1, k1_al1, in_c0]
    else:
        # al1_tiling is [], which means loading single group_opt and single batch Fmap to L1.
        if conv_param.l0a_dma_flag:
            al1_bound_list = get_dma_al1_full_bound()
            return al1_bound_list
        m_al1 = get_m_al1_full_load(in_height, in_width, l0a_load2d_flag, strideh_opti_flag, in_c0)
        # pass c1 to al1_bound_list
        final_c1 = ci1_opt
        batch_al1 = tiling_param.get("batch_al1", 1)
        al1_bound_list = [batch_al1, m_al1, final_c1, in_c0]

    return al1_bound_list
