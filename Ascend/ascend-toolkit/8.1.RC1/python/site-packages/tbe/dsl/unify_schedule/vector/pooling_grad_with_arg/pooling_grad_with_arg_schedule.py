#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pooling grad schedule
"""
from tbe import tvm
from tbe.tvm import PlaceholderOp
from tbe.common import buildcfg
from tbe.dsl.base.operation import add_build_arg
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import var_inner

from .pooling_grad_helper import PoolingGradConstants
from .pooling_grad_helper import PoolingGradSchType
from .pooling_grad_helper import get_insn
from .pooling_grad_helper import judge_tvm_shape_equal
from .pooling_grad_info import PoolingGradComputeInfo
from .pooling_grad_info import PoolingGradSocInfo
from .pooling_grad_with_arg_tilingcase import PoolingGradTilingCase
from ... import util
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG
from ...constants import Pattern
from ...constants import PoolingGradPattern
from ...schedule import Schedule


class PGSchConstant:
    """
    constants of pooling grad schedule
    """
    RESERVE_SPACE = 1024
    MAX_COEXIST_NODE_NUM_FP16 = 2.5
    MAX_COEXIST_NODE_NUM_FP32 = 3.5
    UINT1_DTYPE_RATIO = 8
    FP16_DTYPE_RATIO = 2
    H_IDX = 3
    W_IDX = 4


class EntryPoolingGradWithArgSchedule(Schedule):
    """
    entrance to pooling grad schedule
    """

    def __init__(self, outs, tiling_case):
        self.outs = outs
        self.tiling_case = tiling_case

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return ["default"]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.POOLING_GRAD_WITH_ARG]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [PoolingGradPattern.PG_0]

    def do_schedule(self):
        current_compute = get_context().get_current_compute()
        compute_info = current_compute.get("_compute_info")
        if self.tiling_case.sch_type == PoolingGradSchType.COMMON:
            pooling_grad_with_arg_sch = PoolingGradWithArgCommonSchedule(
                compute_info, self.tiling_case, self.outs)
            return pooling_grad_with_arg_sch.do_schedule()
        elif self.tiling_case.sch_type == PoolingGradSchType.WINDOW_SPLIT:
            pooling_grad_with_arg_sch = PoolingGradWithArgWindowSplitSchedule(
                compute_info, self.tiling_case, self.outs)
            return pooling_grad_with_arg_sch.do_schedule()
        elif self.tiling_case.sch_type == PoolingGradSchType.STRIDE_1:
            pooling_grad_with_arg_sch = PoolingGradWithArgStrideOneSchedule(
                compute_info, self.tiling_case, self.outs)
            return pooling_grad_with_arg_sch.do_schedule()
        else:
            pooling_grad_with_arg_sch = PoolingGradWithArgLargeKernelSchedule(
                compute_info, self.tiling_case, self.outs)
            return pooling_grad_with_arg_sch.do_schedule()


class PoolingGradWithArgBaseSchedule:
    """
    pooling grad with arg base schedule
    """

    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGradTilingCase, outs):
        self._outs = outs
        self._sch = None
        self._scope = PoolingGradConstants.LOCAL_UB
        self._tiling_case = tiling_case

        self._compute_info = compute_info
        self._forward_compute_graph_map = compute_info.graph_info.tensor_consumers_map
        self._backward_compute_graph_map = compute_info.graph_info.tensor_producers_map
        self._mid_tensor_set = compute_info.graph_info.mid_tensor_set
        self._res_tensor = compute_info.base_info.res_tensor
        self._assist_tensor_set = compute_info.graph_info.assist_tensor_set
        self._reduce_window_tensor = compute_info.base_info.reduce_window_tensor
        self._real_output_set = self._compute_info.graph_info.real_pure_output_tensor_set

        self._cache_read_tensors = set()
        self._cache_read_buffer_and_tensor_map = {}
        self._cache_read_tensor_and_buffer_map = {}

        self._cache_write_tensors = set()
        self._cache_write_buffer_and_tensor_map = {}
        self._cache_write_tensor_and_buffer_map = {}

        self._window_reduce_tensor_list = []
        self._compute_inline_tensors = set()
        self._mem_unique_tensors = set()

        self._block_split_result = {}
        self._ub_split_result = {}
        self._window_split_result = {}
        self._tensor_set_before_reduce_node = set()

        self._reorder_map = {}
        self._multi_core_bind_axis = None
        self._storage_align_map = {}

        self._compute_at_map = {}
        self._emit_insn_map = {}
        self._buffer_size = None

        self.stride_h_outer = None
        self.stride_h_inner = None
        self.stride_w_outer = None
        self.stride_w_inner = None
        self.block = None


    def _calc_cache_read(self):
        for _tensor in self._compute_info.graph_info.input_tensor_set:
            if "grad" in _tensor.op.name:
                pass
            else:
                self._cache_read_tensors.update({_tensor})

    def _do_cache_read(self):
        for cache_read_tensor in self._cache_read_tensors:
            read_buffer = self._sch.cache_read(
                cache_read_tensor, self._scope, self._forward_compute_graph_map.get(cache_read_tensor))
            self._cache_read_buffer_and_tensor_map[read_buffer] = cache_read_tensor
            self._cache_read_tensor_and_buffer_map[cache_read_tensor] = read_buffer

    def _calc_cache_write(self):
        pass

    def _do_cache_write(self):
        for cache_write_tensor in self._cache_write_tensors:
            buffer_tensor = self._sch.cache_write(
                cache_write_tensor, self._scope)
            self._cache_write_buffer_and_tensor_map[buffer_tensor] = cache_write_tensor
            self._cache_write_tensor_and_buffer_map[cache_write_tensor] = buffer_tensor

    def _set_scope(self):
        for mid_tensor in self._mid_tensor_set - self._compute_info.graph_info.real_output_tensor_set:
            self._sch[mid_tensor].set_scope(self._scope)
        for assist_tensor in self._assist_tensor_set:
            self._sch[assist_tensor].set_scope(self._scope)

    def _do_mid_output_tensor_process(self):
        pass

    def _calc_compute_inline(self):
        pass

    def _do_compute_inline(self):
        for compute_inline_tensor in self._compute_inline_tensors:
            self._sch[compute_inline_tensor].compute_inline()

    def _do_rfactor(self):
        pass

    def _calc_mem_unique(self):
        pass

    def _do_mem_unique(self):
        for single_tensor in self._mem_unique_tensors:
            self._sch[single_tensor].mem_unique()

    def _calc_buffer_size(self):
        if list(self._real_output_set)[0].dtype == "float32":
            max_coexist_node_num = PGSchConstant.MAX_COEXIST_NODE_NUM_FP32  
        else:
            max_coexist_node_num =  PGSchConstant.MAX_COEXIST_NODE_NUM_FP16
        ub_size = PoolingGradSocInfo.get_ub_size() - PGSchConstant.RESERVE_SPACE
        coexist_node = max_coexist_node_num + len(self._mem_unique_tensors)
        if self._tiling_case.is_enable_db:
            coexist_node = coexist_node * 2
        self._buffer_size = ub_size // DTYPE_BYTE_MAPPING.get(
            self._compute_info.graph_info.max_type) // coexist_node

        buffer_size_list = get_compile_info().get("_buffer_size")
        if buffer_size_list is None:
            buffer_size_list = {}
            add_compile_info_inner("_buffer_size", buffer_size_list)
        if self._tiling_case.pattern_key not in buffer_size_list:
            buffer_size_list[self._tiling_case.pattern_key] = [
                0] * (len(PoolingGradSchType.__members__) * 2)
            buffer_size_list.get(self._tiling_case.pattern_key)[
                self._tiling_case.sch_key] = self._buffer_size
        else:
            buffer_size = buffer_size_list.get(self._tiling_case.pattern_key)
            if buffer_size[self._tiling_case.sch_key] != 0:
                buffer_size[self._tiling_case.sch_key] = min(
                    buffer_size[self._tiling_case.sch_key], self._buffer_size)
            else:
                buffer_size[self._tiling_case.sch_key] = self._buffer_size

    def _pre_check_sch(self):
        pass

    def _do_const_tiling(self):
        if self._tiling_case.is_const:
            self._tiling_case.calc_const_tiling()

    def _post_check_sch(self):
        return self._tiling_case.check_consistency()

    def _do_block_tiling(self):
        block_split_axis_index = self._tiling_case.block_split_axis_index
        block_factor = self._tiling_case.block_factor
        block_split_factor = block_factor if block_factor else var_inner(
            "_block_factor", (1, None))
        block_outer, block_inner = self._sch[self._res_tensor].split(
            self._res_tensor.op.axis[block_split_axis_index], factor=block_split_factor)

        self._block_split_result["index"] = block_split_axis_index
        self._block_split_result["outer_itervar"] = block_outer
        self._block_split_result["inner_itervar"] = block_inner
        self._block_split_result["factor"] = block_split_factor

    def _do_set_buffer_size(self):
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._window_reduce_tensor_list) \
                .union(self._assist_tensor_set):
            if single_tensor in self._compute_inline_tensors:
                continue
            storage_bound_value = int(self._buffer_size *
                                      DTYPE_BYTE_MAPPING.get(
                                          self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                              single_tensor.dtype))
            if single_tensor.dtype == "float16":
                self._sch[single_tensor].set_buffer_size(storage_bound_value // PGSchConstant.FP16_DTYPE_RATIO)
            elif single_tensor.dtype == "uint1":
                self._sch[single_tensor].set_buffer_size(storage_bound_value // PGSchConstant.UINT1_DTYPE_RATIO)
            else:
                self._sch[single_tensor].set_buffer_size(storage_bound_value)
        # set fake node buffer size
        storage_bound_value = int(self._buffer_size *
                                  DTYPE_BYTE_MAPPING.get(
                                      self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                          self._res_tensor.dtype))
        if self._res_tensor.dtype == "float16":
            real_res_bound = storage_bound_value // PGSchConstant.FP16_DTYPE_RATIO    
        else:
            real_res_bound = storage_bound_value
        self._sch[self._res_tensor].set_buffer_size(real_res_bound)

    def _do_reduce_reorder(self):
        pass

    def _do_reorder(self):
        for single_tensor, param in self._reorder_map.items():
            self._sch[single_tensor].reorder(*param)

    def _calc_storage_align(self):
        for tensor in self._cache_read_buffer_and_tensor_map.keys():
            if "argmax" in tensor.op.name and "local.UB" in tensor.op.name:
                self._sch[tensor].storage_align(tensor.op.axis[-3], 256, 0)

    def _do_storage_align(self):
        for single_tensor, param in self._storage_align_map.items():
            self._sch[single_tensor].storage_align(param[0], param[1], param[2])

    def _calc_multi_core(self):
        if self._tiling_case.multi_core:
            block_bind_axis = self._block_split_result.get("outer_itervar")
            fuse_axis_list = self._res_tensor.op.axis[:self._tiling_case.block_split_axis_index]
            fuse_axis_list.append(block_bind_axis)
            self._multi_core_bind_axis = self._sch[self._res_tensor].fuse(
                *fuse_axis_list)

    def _do_multi_core(self):
        if self._multi_core_bind_axis is not None:
            self.block = tvm.thread_axis(PoolingGradConstants.BLOCK_IDX)
            self._sch[self._res_tensor].bind(self._multi_core_bind_axis, self.block)

    def _do_set_constraint(self):
        pass

    def _calc_compute_at(self):
        pass

    def _do_compute_at(self):
        for single_tensor, param in self._compute_at_map.items():
            self._sch[single_tensor].compute_at(self._sch[param[0]], param[1])

    def _do_buffer_reuse(self):
        reuse_tensor = list(self._assist_tensor_set)[0]
        self._sch[reuse_tensor].reused_by(self._reduce_window_tensor)
        self._sch[self._reduce_window_tensor].reused_by(reuse_data=True)
        self._sch[self._reduce_window_tensor].remove_init()

    def _do_double_buffer(self):
        if not self._tiling_case.is_enable_db:
            return
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._window_reduce_tensor_list) \
                .union(self._assist_tensor_set):
            if single_tensor in self._compute_inline_tensors:
                continue
            self._sch[single_tensor].double_buffer()
        add_build_arg("double_buffer_non_reuse", True)

    def _calc_emit_insn(self):
        pass

    def _do_emit_insn(self):
        for single_tensor, param in self._emit_insn_map.items():
            if len(param) > 2:
                self._sch[single_tensor].emit_insn(
                    param[0], param[1], attrs=param[2])
            else:
                if param[1] == "vector_add":
                    self._sch[single_tensor].emit_insn(param[0], param[1], attrs={
                                                       "map_policy": "axis_shift_repeat_stride_no_overflow"})
                else:
                    self._sch[single_tensor].emit_insn(param[0], param[1])

    def _do_pragma(self):
        pass
    
    def _set_store_predict(self):
        ho = None
        wo = None
        for _tensor in self._compute_info.graph_info.input_tensor_set:
            if "grad" in _tensor.op.name:
                ho = _tensor.shape[-3]
                wo = _tensor.shape[-2]
        for single_tensor in self._mid_tensor_set:
            if single_tensor.op.name == "grad_ub":
                self._sch[single_tensor].set_store_predicate(single_tensor.op.axis[-2] < ho * wo)


class PoolingGradWithArgCommonSchedule(PoolingGradWithArgBaseSchedule):
    """
    pooling grad with arg common schedule
    """
    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGradTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)

    def do_schedule(self):
        """
        pooling grad with arg common schedule process
        """
        self._sch = tvm.create_schedule([self._res_tensor.op])

        self._sch.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_mid_output_tensor_process()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._do_rfactor()

        self._calc_mem_unique()
        self._do_mem_unique()

        self._calc_buffer_size()

        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._do_block_tiling()
        self._do_ub_tiling()

        self._do_reorder()
        self._do_reduce_reorder()

        self._do_set_buffer_size()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_multi_core()
        self._do_multi_core()

        self._do_set_constraint()
        self._set_store_predict()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_buffer_reuse()

        self._do_double_buffer()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_pragma()

        return self._sch

    def _do_ub_tiling(self):
        block_split_axis_index = self._tiling_case.block_split_axis_index
        ub_split_axis_index = self._tiling_case.ub_split_axis_index
        ub_factor = self._tiling_case.ub_factor
        ub_split_factor = ub_factor if ub_factor else var_inner(
            "_ub_factor", (1, None))

        if block_split_axis_index == ub_split_axis_index:
            ub_outer, ub_inner = self._sch[self._res_tensor].split(self._block_split_result.get("inner_itervar"),
                                                                   factor=ub_split_factor)
        else:
            ub_outer, ub_inner = self._sch[self._res_tensor].split(self._res_tensor.op.axis[ub_split_axis_index],
                                                                   factor=ub_split_factor)
        self._ub_split_result["index"] = ub_split_axis_index
        self._ub_split_result["outer_itervar"] = ub_outer
        self._ub_split_result["inner_itervar"] = ub_inner
        self._ub_split_result["factor"] = ub_split_factor

    def _do_reduce_reorder(self):
        k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
        s_d, s_h, s_w = self._compute_info.graph_info.strides_info
        self.stride_h_outer, self.stride_h_inner = self._sch[self._reduce_window_tensor].split(
            self._reduce_window_tensor.op.axis[-3], factor=s_h)
        self.stride_w_outer, self.stride_w_inner = self._sch[self._reduce_window_tensor].split(
            self._reduce_window_tensor.op.axis[-2], factor=s_w)

        self._sch[self._reduce_window_tensor].reorder(*[self._reduce_window_tensor.op.axis[0],
                                                        self._reduce_window_tensor.op.axis[1],
                                                        self._reduce_window_tensor.op.axis[2],
                                                        self._reduce_window_tensor.op.reduce_axis[0],
                                                        self.stride_h_inner, self.stride_w_inner,
                                                        self.stride_h_outer, self.stride_w_outer,
                                                        self._reduce_window_tensor.op.axis[5]])

    def _do_set_constraint(self):
        if self._tiling_case.is_const:
            return

    def _calc_compute_at(self):
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_tensor_and_buffer_map.keys()) \
                .union(self._window_reduce_tensor_list) \
                .union(self._assist_tensor_set) \
                .union(self._real_output_set):
            if single_tensor in self._compute_inline_tensors:
                continue
            self._compute_at_map[single_tensor] = [
                self._res_tensor, self._ub_split_result.get("outer_itervar")]

    def _calc_emit_insn(self):
        emit_insn_index = 0
        # cache read tensor
        for single_buffer, tensor in self._cache_read_buffer_and_tensor_map.items():
            if single_buffer in self._compute_inline_tensors:
                continue
            insn = PoolingGradConstants.PHONY_INSN \
                if tensor in self._compute_info.graph_info.mid_output_tensor_set \
                else PoolingGradConstants.DMA_COPY
            self._emit_insn_map[single_buffer] = [
                single_buffer.op.axis[emit_insn_index], insn]
        # mid tensor
        for single_tensor in self._mid_tensor_set - self._compute_inline_tensors -\
                self._compute_info.graph_info.real_output_tensor_set - {self._reduce_window_tensor}:
            insn = get_insn(single_tensor)
            if single_tensor.op.name == "grad_sel" and PoolingGradSocInfo.get_soc_version() in \
                    ["Ascend910B", "Ascend910_93"]:
                self._emit_insn_map[single_tensor] = [single_tensor.op.axis[-2], insn]
            else:
                self._emit_insn_map[single_tensor] = [single_tensor.op.axis[emit_insn_index], insn]
        # cache write node
        for single_buffer, tensor in self._cache_write_buffer_and_tensor_map.items():
            self._emit_insn_map[single_buffer] = [
                single_buffer.op.axis[emit_insn_index], get_insn(tensor)]
        # output node
        for out_tensor in self._compute_info.graph_info.real_output_tensor_set:
            if out_tensor == self._res_tensor:
                _emit_insn_axis = self._ub_split_result.get("inner_itervar")
            else:
                _emit_insn_axis = out_tensor.op.axis[emit_insn_index]
            self._emit_insn_map[out_tensor] = [
                _emit_insn_axis, PoolingGradConstants.DMA_COPY, {PoolingGradConstants.NO_OVERLAP: 0}]
        # assist node
        for assist_tensor in self._assist_tensor_set:
            _emit_insn_axis = assist_tensor.op.axis[emit_insn_index]
            self._emit_insn_map[assist_tensor] = [
                _emit_insn_axis, "vector_dup"]

        if self._res_tensor.op.tag == "fake_node":
            _emit_insn_axis = self._ub_split_result.get("inner_itervar")
            self._emit_insn_map[self._res_tensor] = [
                _emit_insn_axis, PoolingGradConstants.PHONY_INSN]

        # add predict condition to reduce node
        di = self._reduce_window_tensor.op.axis[1]
        hi = self._reduce_window_tensor.op.axis[3]
        wi = self._reduce_window_tensor.op.axis[4]
        reduce_window = self._reduce_window_tensor.op.reduce_axis[0]
        k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
        s_d, s_h, s_w = self._compute_info.graph_info.strides_info
        condition = tvm.all((wi - (reduce_window % (k_h * k_w)) % k_w) % s_w == 0,
                            (hi - (reduce_window % (k_h * k_w)) // k_w) % s_h == 0,
                            (di - reduce_window // (k_h * k_w)) % s_d == 0)
        self._sch[self._reduce_window_tensor].set_store_predicate(condition)

        self._emit_insn_map[self._reduce_window_tensor] = [
            self.stride_h_outer, get_insn(self._reduce_window_tensor)]


class PoolingGradWithArgWindowSplitSchedule(PoolingGradWithArgBaseSchedule):
    """
    pooling grad with arg window split schedule
    """

    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGradTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)

    def do_schedule(self):
        """
        pooling grad with arg window split schedule process
        """
        self._sch = tvm.create_schedule([self._res_tensor.op])
        self._sch.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_mid_output_tensor_process()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._do_rfactor()

        self._calc_mem_unique()
        self._do_mem_unique()

        self._calc_buffer_size()

        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._calc_tensor_set_before_reduce_node()

        self._do_block_tiling()
        self._do_ub_tiling()
        self._do_window_tiling()

        self._do_reorder()
        self._do_reduce_reorder()

        self._do_set_buffer_size()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_multi_core()
        self._do_multi_core()

        self._do_set_constraint()
        self._set_store_predict()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_buffer_reuse()

        self._do_double_buffer()

        if self._tiling_case.is_const:
            self._const_buffer_tile()

        self._calc_emit_insn()

        if get_compile_info().get("_unknown_rank"):
            self._set_buffer_tile()

        self._do_emit_insn()

        self._do_pragma()

        return self._sch

    def _do_ub_tiling(self):
        block_split_axis_index = self._tiling_case.block_split_axis_index
        ub_split_axis_index = self._tiling_case.ub_split_axis_index
        ub_factor = self._tiling_case.ub_factor
        ub_split_factor = ub_factor if ub_factor else var_inner(
            "_ub_factor", (1, None))

        if block_split_axis_index == ub_split_axis_index:
            ub_outer, ub_inner = self._sch[self._res_tensor].split(self._block_split_result.get("inner_itervar"),
                                                                   factor=ub_split_factor)
        else:
            ub_outer, ub_inner = self._sch[self._res_tensor].split(self._res_tensor.op.axis[ub_split_axis_index],
                                                                   factor=ub_split_factor)
        self._ub_split_result["index"] = ub_split_axis_index
        self._ub_split_result["outer_itervar"] = ub_outer
        self._ub_split_result["inner_itervar"] = ub_inner
        self._ub_split_result["factor"] = ub_split_factor

    def _do_window_tiling(self):
        window_factor = self._tiling_case.window_factor
        window_split_factor = window_factor if window_factor else var_inner(
            "_window_factor", (1, None))
        window_outer, window_inner = \
            self._sch[self._reduce_window_tensor].split(self._reduce_window_tensor.op.reduce_axis[0],
                                                        factor=window_split_factor)
        self._window_split_result["index"] = 0
        self._window_split_result["outer_itervar"] = window_outer
        self._window_split_result["inner_itervar"] = window_inner
        self._window_split_result["factor"] = window_split_factor

    def _do_reduce_reorder(self):
        k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
        s_d, s_h, s_w = self._compute_info.graph_info.strides_info
        self.stride_h_outer, self.stride_h_inner = self._sch[self._reduce_window_tensor].split(
            self._reduce_window_tensor.op.axis[-3], factor=s_h)
        self.stride_w_outer, self.stride_w_inner = self._sch[self._reduce_window_tensor].split(
            self._reduce_window_tensor.op.axis[-2], factor=s_w)

        self._sch[self._reduce_window_tensor].reorder(*[self._reduce_window_tensor.op.axis[0],
                                                        self._reduce_window_tensor.op.axis[1],
                                                        self._reduce_window_tensor.op.axis[2],
                                                        self._window_split_result.get("outer_itervar"),
                                                        self._window_split_result.get("inner_itervar"),
                                                        self.stride_h_inner, self.stride_w_inner,
                                                        self.stride_h_outer, self.stride_w_outer,
                                                        self._reduce_window_tensor.op.axis[5]])

    def _calc_tensor_set_before_reduce_node(self):
        def _is_placeholder(tensor):
            return isinstance(tensor.op, tvm.PlaceholderOp)

        def _crawl_tensor(res, end=None):
            queue = [res]
            visited = []
            while queue:
                head = queue.pop(0)
                for tensor in head.op.input_tensors:
                    if tensor in visited or _is_placeholder(tensor) or tensor.op.name == end:
                        continue
                    self._tensor_set_before_reduce_node.add(tensor)
                    visited.append(tensor)
                    queue.append(tensor)
        _crawl_tensor(self._reduce_window_tensor)

    def _do_set_constraint(self):
        if self._tiling_case.is_const:
            return

    def _calc_compute_at(self):
        for single_tensor in self._tensor_set_before_reduce_node \
                .union(self._cache_read_buffer_and_tensor_map.keys()):
            self._compute_at_map[single_tensor] = [
                self._reduce_window_tensor, self._window_split_result.get("outer_itervar")]

        for single_tensor in self._mid_tensor_set \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_tensor_and_buffer_map.keys()) \
                .union(self._window_reduce_tensor_list) \
                .union(self._real_output_set) \
                .union(self._assist_tensor_set) - self._tensor_set_before_reduce_node \
        - self._cache_read_buffer_and_tensor_map.keys():
            if single_tensor in self._compute_inline_tensors:
                continue
            self._compute_at_map[single_tensor] = [
                self._res_tensor, self._ub_split_result.get("outer_itervar")]

    def _calc_emit_insn(self):
        emit_insn_index = 0
        # cache read tensor
        for single_buffer, tensor in self._cache_read_buffer_and_tensor_map.items():
            if single_buffer in self._compute_inline_tensors:
                continue
            insn = PoolingGradConstants.PHONY_INSN \
                if tensor in self._compute_info.graph_info.mid_output_tensor_set \
                else PoolingGradConstants.DMA_COPY
            self._emit_insn_map[single_buffer] = [
                single_buffer.op.axis[emit_insn_index], insn]
        # mid tensor
        for single_tensor in self._mid_tensor_set - self._compute_inline_tensors -\
                self._compute_info.graph_info.real_output_tensor_set - {self._reduce_window_tensor}:
            insn = get_insn(single_tensor)
            if single_tensor.op.name == "grad_sel" and PoolingGradSocInfo.get_soc_version() in \
                    ["Ascend910B", "Ascend910_93"]:
                self._emit_insn_map[single_tensor] = [single_tensor.op.axis[-2], insn]
            else:
                self._emit_insn_map[single_tensor] = [single_tensor.op.axis[emit_insn_index], insn]
        # cache write node
        for single_buffer, tensor in self._cache_write_buffer_and_tensor_map.items():
            self._emit_insn_map[single_buffer] = [
                single_buffer.op.axis[emit_insn_index], get_insn(tensor)]
        # output node
        for out_tensor in self._compute_info.graph_info.real_output_tensor_set:
            if out_tensor == self._res_tensor:
                _emit_insn_axis = self._ub_split_result.get("inner_itervar")
            else:
                _emit_insn_axis = out_tensor.op.axis[emit_insn_index]
            self._emit_insn_map[out_tensor] = [
                _emit_insn_axis, PoolingGradConstants.DMA_COPY, {PoolingGradConstants.NO_OVERLAP: 0}]
        # assist node
        for assist_tensor in self._assist_tensor_set:
            _emit_insn_axis = assist_tensor.op.axis[emit_insn_index]
            self._emit_insn_map[assist_tensor] = [
                _emit_insn_axis, "vector_dup"]

        if self._res_tensor.op.tag == "fake_node":
            _emit_insn_axis = self._ub_split_result.get("inner_itervar")
            self._emit_insn_map[self._res_tensor] = [
                _emit_insn_axis, PoolingGradConstants.PHONY_INSN]

        # add predict condition to reduce node
        di = self._reduce_window_tensor.op.axis[1]
        hi = self._reduce_window_tensor.op.axis[3]
        wi = self._reduce_window_tensor.op.axis[4]
        reduce_window = self._reduce_window_tensor.op.reduce_axis[0]
        k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
        s_d, s_h, s_w = self._compute_info.graph_info.strides_info
        condition = tvm.all((wi - (reduce_window % (k_h * k_w)) % k_w) % s_w == 0,
                            (hi - (reduce_window % (k_h * k_w)) // k_w) % s_h == 0,
                            (di - reduce_window // (k_h * k_w)) % s_d == 0)
        self._sch[self._reduce_window_tensor].set_store_predicate(condition)

        self._emit_insn_map[self._reduce_window_tensor] = [
            self.stride_h_outer, get_insn(self._reduce_window_tensor)]

    def _set_buffer_tile(self):
        k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
        s_d, s_h, s_w = self._compute_info.graph_info.strides_info
        for single_tensor in self._mid_tensor_set:
            if single_tensor.op.name == "grad_sel_fp32" or single_tensor.op.name == "grad_split_hwo":
                ho_size = single_tensor.shape[-3]
                hi_size = self._res_tensor.shape[-3]
                if self._block_split_result.get("index") < PGSchConstant.H_IDX and \
                        self._ub_split_result.get("index") == PGSchConstant.H_IDX:
                    ub_offset = self._ub_split_result.get(
                        "outer_itervar") * self._ub_split_result.get("factor")
                    window_offset = tvm.floordiv(((self._window_split_result.get("factor") *
                                                   (self._window_split_result.get("outer_itervar") + 1)) - 1), k_w)
                    bottom_bound = tvm.min(tvm.max(tvm.floordiv(
                        (ub_offset - window_offset), s_h), 0), ho_size)
                    self._sch[single_tensor].buffer_tile((None, None),
                                                         (None, None),
                                                         (None, None),
                                                         (None, None),
                                                         (bottom_bound,
                                                          tvm.min(tvm.min(self._ub_split_result.get("factor"),
                                                                          tvm.truncdiv(
                                                              self._ub_split_result.get("factor"), s_h) + 2),
                                                              ho_size - bottom_bound)),
                                                         (None, None),
                                                         (None, None))
                if self._block_split_result.get("index") == PGSchConstant.H_IDX and \
                        self._ub_split_result.get("index") == PGSchConstant.H_IDX:
                    block_offset = tvm.floormod(self.block, tvm.floordiv(
                        hi_size - 1, self._block_split_result.get("factor")) + 1) * \
                        self._block_split_result.get("factor")
                    ub_offset = self._ub_split_result.get(
                        "outer_itervar") * self._ub_split_result.get("factor")
                    window_offset = tvm.floordiv(((self._window_split_result.get("factor") *
                                                   (self._window_split_result.get("outer_itervar") + 1)) - 1), k_w)
                    bottom_bound = tvm.min(tvm.max(tvm.floordiv(
                        block_offset + ub_offset - window_offset, s_h), 0), ho_size)
                    self._sch[single_tensor].buffer_tile((None, None),
                                                         (None, None),
                                                         (None, None),
                                                         (None, None),
                                                         (bottom_bound,
                                                          tvm.min(tvm.min(self._ub_split_result.get("factor"),
                                                                          tvm.truncdiv(
                                                              self._ub_split_result.get("factor"), s_h) + 2),
                                                              ho_size - bottom_bound)),
                                                         (None, None),
                                                         (None, None))
                if self._block_split_result.get("index") == PGSchConstant.H_IDX and \
                        self._ub_split_result.get("index") == PGSchConstant.W_IDX:
                    block_offset = tvm.floormod(self.block, tvm.floordiv(
                        hi_size - 1, self._block_split_result.get("factor")) + 1) * \
                        self._block_split_result.get("factor")
                    window_offset = tvm.floordiv(((self._window_split_result.get("factor") *
                                                   (self._window_split_result.get("outer_itervar") + 1)) - 1), k_w)
                    bottom_bound = tvm.min(tvm.max(tvm.floordiv(
                        block_offset + self._block_split_result.get("inner_itervar") - window_offset, s_h), 0), ho_size)
                    upper_bound = tvm.min(bottom_bound + 1, ho_size)
                    self._sch[single_tensor].buffer_tile((None, None),
                                                         (None, None),
                                                         (None, None),
                                                         (None, None),
                                                         (bottom_bound, upper_bound - bottom_bound),
                                                         (None, None),
                                                         (None, None))

    def _const_buffer_tile(self):
        """
        when hwo size > 255, need buffer tile to help inferbound
        """
        k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
        s_d, s_h, s_w = self._compute_info.graph_info.strides_info
        for single_tensor in self._mid_tensor_set:
            if single_tensor.op.name == "grad_split_hwo":
                wo_size = single_tensor.shape[-2]
        for single_tensor in self._mid_tensor_set:
            if single_tensor.op.name == "grad_ub" and \
                    PoolingGradSocInfo.get_soc_version() in ["Ascend910B", "Ascend910_93"] and \
                    self._ub_split_result.get("index") == PGSchConstant.H_IDX:
                self._set_buffer_tile()

    
class PoolingGradWithArgStrideOneSchedule(PoolingGradWithArgWindowSplitSchedule):
    """
    pooling grad with arg stride_1 schedule
    """
    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGradTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)

    def do_schedule(self):
        """
        pooling grad with arg stride_1 schedule process
        """
        self._sch = tvm.create_schedule([self._res_tensor.op])
        self._sch.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_mid_output_tensor_process()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._do_rfactor()

        self._calc_mem_unique()
        self._do_mem_unique()

        self._calc_buffer_size()

        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._calc_tensor_set_before_reduce_node()

        self._do_block_tiling()
        self._do_ub_tiling()
        self._do_window_tiling()

        self._do_reorder()
        self._do_reduce_reorder()

        self._do_set_buffer_size()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_multi_core()
        self._do_multi_core()

        self._do_set_constraint()
        self._set_store_predict()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_buffer_reuse()

        self._do_double_buffer()

        self._calc_emit_insn()

        if self._tiling_case.is_const:
            self._const_buffer_tile()

        if get_compile_info().get("_unknown_rank"):
            self._set_buffer_tile()

        self._do_emit_insn()

        if get_compile_info().get("_unknown_rank"):
            self._do_pragma()

        return self._sch

    def _do_pragma(self):
        for tensor in self._mid_tensor_set:
            if tensor.op.name == "res_fp32":
                id_val = tvm.call_extern("int32", "axis_group", 0, "append")
                self._sch[tensor].pragma(self.stride_w_outer, "axis_group", id_val)
                self._sch[tensor].pragma(tensor.op.axis[-1], "axis_group", id_val)


class PoolingGradWithArgLargeKernelSchedule(PoolingGradWithArgWindowSplitSchedule):
    """
    pooling grad with arg large kernel schedule
    """
    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGradTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)

    def do_schedule(self):
        """
        pooling grad with arg window split schedule process
        """
        self._sch = tvm.create_schedule([self._res_tensor.op])
        self._sch.tiling_key = self._tiling_case.tiling_key

        self._set_var()
        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_mid_output_tensor_process()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._do_rfactor()

        self._calc_mem_unique()
        self._do_mem_unique()

        self._calc_buffer_size()

        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._calc_tensor_set_before_reduce_node()

        self._do_block_tiling()
        self._do_ub_tiling()
        self._do_window_tiling()

        self._do_reorder()
        self._do_reduce_reorder()

        self._do_set_buffer_size()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_multi_core()
        self._do_multi_core()

        self._do_set_constraint()
        self._set_store_predict()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_buffer_reuse()

        self._do_double_buffer()

        self._calc_emit_insn()

        self._set_buffer_tile()

        self._do_emit_insn()

        self._do_pragma()

        return self._sch

    def _set_var(self):
        '''
        In this case, pad = 0, ho=wo=1, stride=ksize
        '''
        dynamic_vars = get_context().get_current_compute().get_vars()
        for _var in dynamic_vars:
            if "_window_1_value" in _var.get_name():
                kh = _var
            if "_window_2_value" in _var.get_name():
                kw = _var
        for _var in dynamic_vars:
            if "grad" in _var.get_name():
                self._sch.set_var_value(_var.get_tvm_var(), 1)

    def _do_window_tiling(self):
        window_factor = self._tiling_case.window_factor
        if get_compile_info().get("_unknown_rank"):
            dynamic_vars = get_context().get_current_compute().get_vars()
            for _var in dynamic_vars:
                if "_window_2_value" in _var.get_name():
                    kw = _var
            window_split_factor = kw
            window_outer, window_inner = \
                self._sch[self._reduce_window_tensor].split(self._reduce_window_tensor.op.reduce_axis[0],
                                                            factor=window_split_factor.get_tvm_var())
        else:
            k_d, k_h, k_w = self._compute_info.graph_info.ksize_info
            window_split_factor = window_factor if window_factor else k_w
            window_outer, window_inner = \
                self._sch[self._reduce_window_tensor].split(self._reduce_window_tensor.op.reduce_axis[0],
                                                            factor=window_split_factor)
        self._window_split_result["index"] = 0
        self._window_split_result["outer_itervar"] = window_outer
        self._window_split_result["inner_itervar"] = window_inner
        self._window_split_result["factor"] = window_split_factor

    def _do_reduce_reorder(self):
        self.stride_h_outer, self.stride_h_inner = self._sch[self._reduce_window_tensor].split(
            self._reduce_window_tensor.op.axis[-3], nparts=1)
        self.stride_w_outer, self.stride_w_inner = self._sch[self._reduce_window_tensor].split(
            self._reduce_window_tensor.op.axis[-2], nparts=1)

        self._sch[self._reduce_window_tensor].reorder(*[self._reduce_window_tensor.op.axis[0],
                                                        self._reduce_window_tensor.op.axis[1],
                                                        self._reduce_window_tensor.op.axis[2],
                                                        self._window_split_result.get(
                                                            "outer_itervar"),
                                                        self._window_split_result.get(
                                                            "inner_itervar"),
                                                        self.stride_h_inner, self.stride_w_inner,
                                                        self.stride_h_outer, self.stride_w_outer,
                                                        self._reduce_window_tensor.op.axis[5]])

    def _set_buffer_tile(self):
        for single_tensor in self._mid_tensor_set:
            if single_tensor.op.name == "grad_sel_fp32" or single_tensor.op.name == "grad_split_hwo":
                self._sch[single_tensor].buffer_tile((None, None),
                                                     (None, None),
                                                     (None, None),
                                                     (None, None),
                                                     (0, 1),
                                                     (0, 1),
                                                     (None, None))

    def _do_set_buffer_size(self):
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._window_reduce_tensor_list) \
                .union(self._assist_tensor_set):
            if single_tensor in self._compute_inline_tensors:
                continue
            storage_bound_value = int(self._buffer_size *
                                      DTYPE_BYTE_MAPPING.get(
                                        self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                            single_tensor.dtype))
            if single_tensor.op.name == "grad_ub":
                self._sch[single_tensor].set_buffer_size(256)
                continue
            if single_tensor.dtype == "float16":
                self._sch[single_tensor].set_buffer_size(storage_bound_value // PGSchConstant.FP16_DTYPE_RATIO)
            elif single_tensor.dtype == "uint1":
                self._sch[single_tensor].set_buffer_size(storage_bound_value // 2)
            else:
                self._sch[single_tensor].set_buffer_size(storage_bound_value)
        # set fake node buffer size
        storage_bound_value = int(self._buffer_size *
                                    DTYPE_BYTE_MAPPING.get(
                                        self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                            self._res_tensor.dtype))
        if self._res_tensor.dtype == "float16":
            real_res_bound = storage_bound_value // PGSchConstant.FP16_DTYPE_RATIO
        else:
            real_res_bound = storage_bound_value
        self._sch[self._res_tensor].set_buffer_size(real_res_bound)