#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pooling_with_arg_vars schedule
"""
from typing import Any
from functools import reduce

from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation

from ... import util
from ...constants import CompileInfo
from ...constants import PoolingWithArgPattern
from ...constants import Pattern
from ...constants import DTYPE_BYTE_MAPPING
from ...schedule import Schedule
from .pooling_with_arg_tilingcase import CEIL_MODE_MAP
from .pooling_with_arg_tilingcase import TilingStrategy
from .pooling_with_arg_tilingcase import PoolingWithArgCompileInfo
from .pooling_with_arg_tilingcase import unify_attr_info

# block size in D architecture
BLOCK_SIZE_BYTE = 32

# reserve space
RESERVE_SPACE = 1024

# begin tensor max size
BEGIN_TENSOR_MAX_SIZE = 32

DEFAULT = "default"
SCOPE_WORKSPACE = "global.workspace"
SCOPE_UB = "local.UB"
SCOPE_L1 = "local.L1"

MTE1 = "MTE1"
VECTOR = "VECTOR"
PATTERN_DCIT = {
    0: MTE1,
    1: VECTOR
}

NO_SPLIT = "NO_SPLIT"
SPLIT = "SPLIT"
ALL_SPLIT = "ALL_SPLIT"
C0_REORDER = "C0_REORDER"
SPLIT_DICT = {
    0: NO_SPLIT,
    1: SPLIT,
    2: ALL_SPLIT,
    3: C0_REORDER
}
FUSED_DICT = {
    0: False,
    1: True
}
UB_TEMP_BUFFER = 256


class PoolingWithArgDispatch(Schedule):
    """
    pooling_with_arg_vars schedule
    """
    def __init__(self, outs, tiling_case):
        self._output_tensors = outs
        self._tiling_case = tiling_case
        self._split_flag = self._tiling_case.get("split_flag", "unknown")
        self._calculate_mode = self._tiling_case.get("calculate_mode")

    @classmethod
    def get_instance(cls, outs, tiling_case):  # type: (list[Any], Any) -> "Schedule"
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):  # type: () -> list[str]
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):  # type: () -> list[str]
        return [Pattern.POOLINGWITHARG]

    @classmethod
    def get_supported_sub_pattern(cls):  # type: () -> list[str]
        return [PoolingWithArgPattern.SCHEDULE_5HD]

    def do_schedule(self):
        schedule_map = {
            NO_SPLIT: PoolingWithArgCommonSchedule,
            SPLIT: PoolingWithArgWorkSpaceSchedule,
            ALL_SPLIT: PoolingWithArgAllSplitSchedule,
            C0_REORDER: PoolingWithArgC0ReorderSchedule
        }
        current_schedule = schedule_map.get(self._split_flag)
        return current_schedule(self._output_tensors, self._tiling_case).do_schedule()


class PoolingWithArgScheduleBase():
    """
    pooling_with_arg_vars schedule
    """
    def __init__(self, outs, tiling_case):
        self._output_tensors = outs
        self._schedule_outs = []
        self._sch = None
        self._tiling_case = tiling_case
        self._tiling_strategy = self._tiling_case.get("tiling_strategy")
        self._tiling_key = self._tiling_case.get("key")
        self._calculate_mode = self._tiling_case.get("calculate_mode")
        self._insn_pattern = self._tiling_case.get("insn_pattern", "unknown")
        self._split_flag = self._tiling_case.get("split_flag", "unknown")
        self._fused_outer = self._tiling_case.get("fused_outer", False)
        self._valid_tiling = True

        self._n_axis = operation.get_context().get("_n_axis")
        self._c1_axis = operation.get_context().get("_c1_axis")
        self._khw_axis = operation.get_context().get("_khw_axis")
        self._fractal_axis = operation.get_context().get("_fractal_axis")
        self._fractal_lower_axis = operation.get_context().get("_fractal_lower_axis")

        self._block_factor = None
        self._ub_fractal_factor = None
        self._ub_khw_factor = None

        self._block_bind_axis = None
        self._compute_at_axis = None
        self._emit_insn_axis = None

        self._reverse_reduce_o = None
        self._reverse_reduce_i = None
        self._reverse_vand_ub_o = None
        self._reverse_vand_ub_i = None

        self._fractal_outer = None
        self._fractal_inner = None

        self._input_tensor = None
        self._pad_tensor = None
        self._fractal_tensor = None
        self._reduce_tensor = None
        self._depad_tensor = None
        self._vcmp_tensor = None
        self._init_not_tensor = None
        self._vand_tensor = None
        self._vand_tensor_ub = None
        self._vand_tensor_clone = None
        self._init_or_tensor = None
        self._vor_tensor = None
        self._vnot_tensor = None
        self._dummy_node = None

        # split tensor
        self._split_tensor = None

        self._in_out_map = {}
        self._middle_tensors = set()
        self._cache_read_tensor_l1 = set()
        self._cache_read_tensors_ub = set()
        self._cache_write_tensors = set()

        self._core_num = util.get_core_num()
        self._ub_size = util.get_ub_size() - UB_TEMP_BUFFER
        self._l1_size = util.get_l1_size()
        self._tensor_number = 3

    @staticmethod
    def _gen_load3d_attr(insn_pattern):
        ceil_mode = operation.get_context().get_current_compute().get("_compute_calculate_mode")
        shape_c1 = operation.get_context().get_current_compute().get("_fm_c1")
        shape_h = operation.get_context().get_current_compute().get("_fm_h")
        shape_w = operation.get_context().get_current_compute().get("_fm_w")
        shape_c0 = operation.get_context().get_current_compute().get("_fm_c0")
        kh = operation.get_context().get_current_compute().get("_kh")
        kw = operation.get_context().get_current_compute().get("_kw")
        stride_h = operation.get_context().get_current_compute().get("_stride_h")
        stride_w = operation.get_context().get_current_compute().get("_stride_w")
        pt = operation.get_context().get_current_compute().get("_padding_top")
        pb = operation.get_context().get_current_compute().get("_padding_bottom")
        pl = operation.get_context().get_current_compute().get("_padding_left")
        pr = operation.get_context().get_current_compute().get("_padding_right")
        dilate_h = operation.get_context().get_current_compute().get("_dilation_h")
        dilate_w = operation.get_context().get_current_compute().get("_dilation_w")
        if ceil_mode:
            pb = tvm.select(tvm.div(pt + shape_h + pb - 1 + stride_h - kh, stride_h) * stride_h >= shape_h + pt,
                            pb, pb + stride_h - 1)
            pr = tvm.select(tvm.div(pl + shape_w + pr - 1 + stride_w - kw, stride_w) * stride_w >= shape_w + pl,
                            pr, pr + stride_w - 1)
        if insn_pattern == MTE1:
            return {'set_fmatrix': 1, 'conv_kernel_h': kh, 'conv_kernel_w': kw, 'conv_padding_top': pt,
                    'conv_padding_bottom': pb, 'conv_padding_left': pl, 'conv_padding_right': pr,
                    'conv_stride_h': stride_h, 'conv_stride_w': stride_w, 'conv_dilation_h': dilate_h,
                    'conv_dilation_w': dilate_w, 'conv_fm_c': shape_c1 * shape_c0, 'conv_fm_c1': shape_c1,
                    'conv_fm_h': shape_h, 'conv_fm_w': shape_w, 'conv_fm_c0': shape_c0, 'ub_enable_zn': 1}
        else:
            vector_attr = {'img2col': 1, 'conv_kernel_h': kh, 'conv_kernel_w': kw, 'conv_padding_top': pt,
                           'conv_padding_bottom': pb, 'conv_padding_left': pl, 'conv_padding_right': pr,
                           'conv_stride_h': stride_h, 'conv_stride_w': stride_w, 'conv_dilation_h': dilate_h,
                           'conv_dilation_w': dilate_w, 'conv_fm_c': shape_c1 * shape_c0, 'conv_fm_c1': shape_c1,
                           'conv_fm_h': shape_h, 'conv_fm_w': shape_w, 'conv_fm_c0': shape_c0}
            vector_attr["ceil_mode"] = 1 if ceil_mode else 0
        return vector_attr

    def _construct_compute_graph(self):
        for _out in self._output_tensors:
            stage_pregram = util.get_dsl_insn(_out)
            if stage_pregram == "pooling_with_arg_depad":
                self._depad_tensor = _out
            else:
                self._vand_tensor = _out
                self._split_tensor = self._vand_tensor
                fractal_shape = util.shape_to_list(self._vand_tensor.shape)
                mask_shape = fractal_shape[-3:]
                self._init_or_tensor = tvm.compute(mask_shape,
                                                   lambda *i: (tvm.const(0, dtype="int8").astype("uint1")),
                                                   name="fake_init_or", tag="pooling_with_arg_vdep")
                self._vor_tensor = tvm.compute(fractal_shape,
                                               lambda _n, _c1, _khw, _f, _b, _c0:
                                               self._vand_tensor[_n, _c1, _khw, _f, _b, _c0] |
                                               self._init_or_tensor[_f, _b, _c0],
                                               name="fake_mask_or", tag="pooling_with_arg_vor")
                self._vnot_tensor = tvm.compute(fractal_shape,
                                                lambda *i: ~self._vor_tensor[i],
                                                name="fake_mask_not", tag="pooling_with_arg_vnot")
                self._dummy_node = tvm.compute(fractal_shape,
                                               lambda *i: self._vnot_tensor[i],
                                               name="dummy_node", tag="pooling_with_arg_dummy_node")
                self._middle_tensors.add(self._init_or_tensor)
                self._middle_tensors.add(self._vor_tensor)
                self._middle_tensors.add(self._vnot_tensor)

        self._schedule_outs.extend(self._output_tensors)
        self._schedule_outs.append(self._dummy_node)

    def _traversal_compute_graph(self):
        visited_tensors = set()

        for _out in self._output_tensors:
            self._dfs_sub_graph(_out, visited_tensors)

    def _calc_tiling_static(self):
        input_shape = util.shape_to_list(self._input_tensor.shape)
        input_dtype = self._input_tensor.dtype
        inputs = [{"shape": input_shape, "dtype": input_dtype}]
        outputs = [{"shape": input_shape, "dtype": input_dtype}]

        op_cores = util.get_core_num()
        op_ub_size = util.get_ub_size() - UB_TEMP_BUFFER
        op_l1_size = util.get_l1_size()
        op_input_bytes = operation.get_context().get(PoolingWithArgCompileInfo.INPUT_BYTES)
        op_input_format = operation.get_context().get(PoolingWithArgCompileInfo.INPUT_FORMAT)
        op_ceil_mode = operation.get_context().get(PoolingWithArgCompileInfo.CALCULATE_MODE)
        op_pooling_mode = 0 if operation.get_op_mode() == "dynamic" else 1
        base_info = [op_cores, op_l1_size, op_ub_size, op_input_bytes, op_input_format, CEIL_MODE_MAP.get(op_ceil_mode),
                     op_pooling_mode]

        op_window_axes = operation.get_context().get(PoolingWithArgCompileInfo.POOLING_AXES)
        op_attr_axes = operation.get_context().get(PoolingWithArgCompileInfo.ATTR_AXES)
        op_window_dimensions = operation.get_context().get(PoolingWithArgCompileInfo.POOLING_DIMENSIONS)
        op_window_strides = operation.get_context().get(PoolingWithArgCompileInfo.POOLING_STRIDES)
        op_window_dilations = operation.get_context().get(PoolingWithArgCompileInfo.POOLING_DILATIONS)
        op_window_pads = operation.get_context().get(PoolingWithArgCompileInfo.POOLING_PADS)

        op_context_axes = unify_attr_info(op_window_axes)
        op_context_attr_axes = unify_attr_info(op_attr_axes)
        op_context_dimensions = unify_attr_info(op_window_dimensions)
        op_context_strides = unify_attr_info(op_window_strides)
        op_context_dilations = unify_attr_info(op_window_dilations)
        op_context_pads = unify_attr_info(op_window_pads)
        op_context_info = [op_context_axes, op_context_attr_axes, op_context_dimensions, op_context_strides,
                           op_context_dilations, op_context_pads]

        insn_pattern = operation.get_context().get(PoolingWithArgCompileInfo.INSN_PATTERN)
        tmp_coex_node = {}
        for _key in insn_pattern:
            tmp_coex_node[_key] = self._tensor_number

        const_compile_info = {'_base_info': base_info,
                              '_coexisting_quantity': tmp_coex_node,
                              '_pattern': 'PoolingWithArg',
                              '_pooling_info': op_context_info}

        op_type = "AutoTiling"
        run_info = op_tiling.do_op_tiling(op_type, const_compile_info, inputs, outputs)
        tiling_format = {
            "key_pattern": "int",
            "key_split": "int",
            "key_fused": "int",
            "block_factor": "int",
            "ub_fractal_outer": "int",
            "ub_fractal_factor": "int",
            "ub_khw_outer": "int",
            "ub_khw_factor": "int"
        }

        tiling_data = op_tiling.decode(run_info.get("tiling_data"), tiling_format)
        key_pattern = tiling_data.get("key_pattern")
        key_split = tiling_data.get("key_split")
        key_fused = tiling_data.get("key_fused")
        self._block_factor = tiling_data.get("block_factor")
        self._ub_fractal_factor = tiling_data.get("ub_fractal_factor")
        self._ub_khw_factor = tiling_data.get("ub_khw_factor")

        if PATTERN_DCIT.get(key_pattern) == self._insn_pattern and SPLIT_DICT.get(key_split) == self._split_flag \
                and self._fused_outer == FUSED_DICT.get(key_fused):
            return
        self._valid_tiling = False

    def _add_compile_info(self):
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()

        # BASE INFO
        cpt_schedule.add(PoolingWithArgCompileInfo.POOLING_EXPAND_MODE, self._insn_pattern)
        cpt_schedule.add(CompileInfo.COEXISTING_QUANTITY, self._tensor_number)

    def _dfs_sub_graph(self, out, visited_tensors: set):
        for tensor_i in out.op.input_tensors:
            util.merge_value(self._in_out_map, tensor_i, out)
            if util.is_placeholder(tensor_i):
                self._input_tensor = tensor_i
            else:
                if tensor_i not in self._output_tensors:
                    self._middle_tensors.add(tensor_i)
                stage_pregram = util.get_dsl_insn(tensor_i)
                if stage_pregram == "pooling_with_arg_fractal":
                    self._fractal_tensor = tensor_i
                elif stage_pregram == "pooling_with_arg_pad":
                    self._pad_tensor = tensor_i
                elif stage_pregram == "pooling_with_arg_reduce":
                    self._reduce_tensor = tensor_i
                elif stage_pregram == "pooling_with_arg_vcmp":
                    self._vcmp_tensor = tensor_i
                elif stage_pregram == "pooling_with_arg_vdep":
                    self._init_not_tensor = tensor_i

            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self._dfs_sub_graph(tensor_i, visited_tensors)

    def _set_constraint(self):
        shape_h = operation.get_context().get_current_compute().get("_fm_h")
        shape_w = operation.get_context().get_current_compute().get("_fm_w")
        kh = operation.get_context().get_current_compute().get("_kh")
        kw = operation.get_context().get_current_compute().get("_kw")
        stride_h = operation.get_context().get_current_compute().get("_stride_h")
        stride_w = operation.get_context().get_current_compute().get("_stride_w")
        pt = operation.get_context().get_current_compute().get("_padding_top")
        pb = operation.get_context().get_current_compute().get("_padding_bottom")
        pl = operation.get_context().get_current_compute().get("_padding_left")
        pr = operation.get_context().get_current_compute().get("_padding_right")
        dilate_h = operation.get_context().get_current_compute().get("_dilation_h")
        dilate_w = operation.get_context().get_current_compute().get("_dilation_w")

        if self._calculate_mode == 'FLOOR':
            cond_h = tvm.div((((((pt + shape_h) + pb) + stride_h) - (dilate_h * (kh - 1))) - 1), stride_h) > 0
            cond_w = tvm.div((((((pl + shape_w) + pr) + stride_w) - (dilate_w * (kw - 1))) - 1), stride_w) > 0
        else:
            cond_h = tvm.div((((((pt + shape_h) + pb) + 2 * stride_h) - (dilate_h * (kh - 1))) - 2), stride_h) > 0
            cond_w = tvm.div((((((pl + shape_w) + pr) + 2 * stride_w) - (dilate_w * (kw - 1))) - 2), stride_w) > 0

        self._sch.set_constraint(cond_h)
        self._sch.set_constraint(cond_w)

        if self._tiling_strategy == TilingStrategy.DYNAMIC:
            cond_ho = tvm.expr.GT((((pt + shape_h) + pb) + stride_h) - kh, 0)
            cond_wo = tvm.expr.GT((((pl + shape_w) + pr) + stride_w) - kw, 0)
            self._sch.set_constraint(cond_ho)
            self._sch.set_constraint(cond_wo)


class PoolingWithArgCommonSchedule(PoolingWithArgScheduleBase):
    """
    pooling_with_arg_vars schedule
    """
    def __init__(self, outs, tiling_case):
        PoolingWithArgScheduleBase.__init__(self, outs, tiling_case)

    def do_schedule(self):
        self._construct_compute_graph()
        self._traversal_compute_graph()
        self._calc_tiling()
        if not self._valid_tiling:
            return None
        self._sch = tvm.create_schedule([_out.op for _out in self._schedule_outs])
        self._sch.tiling_key = self._tiling_key.tiling_key
        self._do_cache_read()
        self._do_cache_write()
        self._do_cache_clone()
        self._set_scope()
        self._do_compute_inline()
        self._do_memory_reuse()
        self._do_tiling()
        self._do_storage_bound()
        self._do_compute_at()
        self._do_emit_insn()
        self._add_compile_info()
        self._set_constraint()

        return self._sch

    def _calc_tiling(self):
        coex_node = operation.get_compile_info().get(CompileInfo.COEXISTING_QUANTITY, {})
        coex_node[self._insn_pattern] = self._tensor_number
        operation.add_compile_info_inner(CompileInfo.COEXISTING_QUANTITY, coex_node)
        funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                 TilingStrategy.STATIC: self._calc_tiling_static}

        funcs.get(self._tiling_strategy)()

    def _calc_tiling_dynamic(self):
        self._block_factor = operation.var_inner("_block_factor_0", (1, None))
        self._ub_fractal_factor = operation.var_inner("_ub_fractal_factor_0", (1, None))

    def _do_cache_read(self):
        if self._insn_pattern == MTE1:
            self._cache_read_tensor_l1 = self._sch.cache_read(self._input_tensor, SCOPE_L1, self._pad_tensor)

    def _do_cache_write(self):
        for _output_tensor in self._output_tensors:
            if _output_tensor != self._depad_tensor:
                self._vand_tensor_ub = self._sch.cache_write(_output_tensor, SCOPE_UB)
                self._cache_write_tensors.add(self._vand_tensor_ub)

    def _do_cache_clone(self):
        self._vand_tensor_clone = self._sch.cache_clone(self._vand_tensor, SCOPE_UB, self._vor_tensor)

    def _set_scope(self):
        for _tensor in self._middle_tensors:
            self._sch[_tensor].set_scope(SCOPE_UB)

    def _do_compute_inline(self):
        if self._insn_pattern == MTE1:
            self._sch[self._pad_tensor].compute_inline()
            self._middle_tensors.remove(self._pad_tensor)
        self._sch[self._vand_tensor_clone].compute_inline()

    def _do_memory_reuse(self):
        self._sch[self._init_or_tensor].reused_by(self._vor_tensor)
        self._sch[self._init_not_tensor].reused_by(self._vnot_tensor)
        self._sch[self._init_or_tensor].mem_unique()
        self._sch[self._init_not_tensor].mem_unique()
        self._sch[self._vand_tensor_ub].mem_unique()

    def _do_tiling(self):
        # n, c1, khkw, fractal, 16, c0
        fractal_o, fractal_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._fractal_axis], self._ub_fractal_factor)
        self._sch[self._split_tensor].reorder(self._sch[self._split_tensor].op.axis[0],
                                              self._sch[self._split_tensor].op.axis[1], fractal_o,
                                              self._sch[self._split_tensor].op.axis[2], fractal_i,
                                              self._sch[self._split_tensor].op.axis[4],
                                              self._sch[self._split_tensor].op.axis[5])
        self._emit_insn_axis = fractal_i

        if self._fused_outer:
            block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                                  self._sch[self._split_tensor].op.axis[1], fractal_o)
        else:
            block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                                  self._sch[self._split_tensor].op.axis[1])

        block_outer, block_inner = self._sch[self._split_tensor].split(block_outer_fuse, self._block_factor)
        self._sch[self._split_tensor].bind(block_outer, tvm.thread_axis("blockIdx.x"))
        self._compute_at_axis = block_inner if self._fused_outer else fractal_o

        if self._insn_pattern == VECTOR:
            vector_fractal_o, vector_fractal_i = self._sch[self._fractal_tensor].\
                split(self._fractal_tensor.op.axis[self._fractal_axis], self._ub_fractal_factor)
            self._fractal_inner = vector_fractal_i
            if self._fractal_tensor.dtype == "float32":
                # emit insn support float32 adapt
                c0_o, c0_i = self._sch[self._fractal_tensor].split(self._fractal_tensor.op.axis[5], 8)
                self._sch[self._fractal_tensor].reorder(self._sch[self._fractal_tensor].op.axis[0],
                                                        self._sch[self._fractal_tensor].op.axis[1], vector_fractal_o,
                                                        self._sch[self._fractal_tensor].op.axis[2],
                                                        c0_o, vector_fractal_i,
                                                        self._sch[self._fractal_tensor].op.axis[4],
                                                        c0_i)
            self._fractal_outer = fractal_o

        # reduce reverse compute at
        reduce_o, reduce_i = self._sch[self._reduce_tensor].split(self._reduce_tensor.op.axis[self._fractal_axis],
                                                                  self._ub_fractal_factor)
        self._reverse_reduce_o, self._reverse_reduce_i = self._sch[self._reduce_tensor].split(reduce_i, nparts=1)
        self._sch[self._reduce_tensor].reorder(self._reduce_tensor.op.axis[0], self._reduce_tensor.op.axis[1],
                                               self._reduce_tensor.op.axis[2],
                                               reduce_o,
                                               self._reverse_reduce_o,
                                               self._reduce_tensor.op.reduce_axis[0],
                                               self._reverse_reduce_i,
                                               self._reduce_tensor.op.axis[4], self._reduce_tensor.op.axis[5])

        # vand_ub reverse compute at
        _vand_tensor_ub_o, _vand_tensor_ub_i = self._sch[self._vand_tensor_ub].split(
            self._sch[self._vand_tensor_ub].op.axis[self._fractal_axis], self._ub_fractal_factor)
        self._reverse_vand_ub_o, self._reverse_vand_ub_i = self._sch[self._vand_tensor_ub].split(
            _vand_tensor_ub_i, nparts=1)

    def _do_storage_bound(self):
        if self._insn_pattern == MTE1:
            self._sch[self._cache_read_tensor_l1].set_buffer_size(
                self._l1_size // DTYPE_BYTE_MAPPING.get(self._cache_read_tensor_l1.dtype))

        max_dtype_count = self._ub_size // self._tensor_number // DTYPE_BYTE_MAPPING.get(self._input_tensor.dtype)

        total_ub_tensors = list(self._middle_tensors) \
                           + list(self._cache_write_tensors) \
                           + list(self._cache_read_tensors_ub)
        for _mid_tensor in total_ub_tensors:
            self._sch[_mid_tensor].set_buffer_size(max_dtype_count)

    def _do_compute_at(self):
        if self._insn_pattern == MTE1:
            self._sch[self._cache_read_tensor_l1].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        else:
            self._sch[self._pad_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._fractal_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._reduce_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._depad_tensor].reverse_compute_at(self._sch[self._reduce_tensor], self._reverse_reduce_o)
        self._sch[self._vcmp_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._init_not_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._vand_tensor_ub].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._init_or_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._vor_tensor].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)
        self._sch[self._vnot_tensor].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)
        self._sch[self._dummy_node].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)

    def _do_emit_insn(self):
        if self._insn_pattern == MTE1:
            self._sch[self._cache_read_tensor_l1].emit_insn(self._sch[self._cache_read_tensor_l1].op.axis[1],
                                                            "dma_copy")
            im2col_attr_0 = self._gen_load3d_attr(self._insn_pattern)
            self._sch[self._fractal_tensor].emit_insn(self._sch[self._fractal_tensor].op.axis[1], "im2col_v2",
                                                      attrs=im2col_attr_0)
        else:
            self._sch[self._pad_tensor].emit_insn(self._sch[self._pad_tensor].op.axis[3], "dma_copy")
            self._sch[self._pad_tensor].pragma(self._sch[self._pad_tensor].op.axis[2], "loop_with_no_overlap_tensor")
            vector_attr_0 = self._gen_load3d_attr(self._insn_pattern)
            vector_attr_0["ho_wo_outer"] = self._fractal_outer
            vector_attr_0["wo"] = self._fractal_tensor.op.attrs["wo"]
            vector_attr_0["ho"] = self._fractal_tensor.op.attrs["ho"]
            if util.is_v220():
                self._sch[self._fractal_tensor].emit_insn(self._fractal_inner,
                                                          "img2col_vector_shl", vector_attr_0)
            else:
                self._sch[self._fractal_tensor].emit_insn(self._fractal_inner,
                                                          "img2col_vector_or", vector_attr_0)
            hwo = self._fractal_tensor.op.attrs["hwo"]
            self._sch[self._fractal_tensor].set_value(lambda *i: (i[3]*16 + i[4]) > hwo - 1,
                                                      tvm.const(0.0, self._fractal_tensor.dtype))

        self._sch[self._reduce_tensor].emit_insn(self._reduce_tensor.op.reduce_axis[0], "vector_reduce_max")
        self._sch[self._depad_tensor].emit_insn(self._sch[self._depad_tensor].op.axis[1], "dma_copy")
        self._sch[self._vcmp_tensor].emit_insn(self._sch[self._vcmp_tensor].op.axis[1], "vector_auto")
        self._sch[self._init_not_tensor].emit_insn(self._sch[self._init_not_tensor].op.axis[0], "vector_dup")
        self._sch[self._vand_tensor_ub].emit_insn(self._reverse_vand_ub_i, "vector_auto")
        self._sch[self._vand_tensor].emit_insn(self._emit_insn_axis, "dma_copy")
        self._sch[self._init_or_tensor].emit_insn(self._sch[self._init_or_tensor].op.axis[0], "vector_dup")
        self._sch[self._vor_tensor].emit_insn(self._sch[self._vor_tensor].op.axis[3], "vector_auto")
        self._sch[self._vnot_tensor].emit_insn(self._sch[self._vnot_tensor].op.axis[3], "vector_auto")
        self._sch[self._dummy_node].emit_insn(self._sch[self._dummy_node].op.axis[3], "phony_insn")
        self._sch[self._vand_tensor].pragma(self._sch[self._vand_tensor].op.axis[2], "loop_with_no_overlap_tensor")


class PoolingWithArgWorkSpaceSchedule(PoolingWithArgScheduleBase):
    """
    pooling_with_arg_vars schedule
    """
    def __init__(self, outs, tiling_case):
        PoolingWithArgScheduleBase.__init__(self, outs, tiling_case)
        self._fractal_tensor_ub = None
        self._fractal_reduce_tensor_ub = None
        self._fractal_vcmp_tensor_ub = None
        self._no_overlap_axis = None
        self.reduce_axis_o = None
        self.reduce_axis_i = None
        self._workspace_emit_insn_axis = None
        self._workspace_compute_at_axis = None
        self._workspace_l1_compute_at_axis = None
        self._fractal_compute_at_axis = None
        self._reduce_compute_at_axis = None

    def do_schedule(self):
        self._construct_compute_graph()
        self._traversal_compute_graph()
        self._calc_tiling()
        if not self._valid_tiling:
            return None
        self._sch = tvm.create_schedule([_out.op for _out in self._schedule_outs])
        self._sch.tiling_key = self._tiling_key.tiling_key
        self._do_cache_read()
        self._do_cache_write()
        self._do_cache_clone()
        self._set_scope()
        self._do_compute_inline()
        self._do_memory_reuse()
        self._do_tiling()
        self._do_storage_bound()
        self._do_compute_at()
        self._do_emit_insn()
        self._add_compile_info()
        self._add_workspace_json()
        self._set_constraint()

        return self._sch

    def _calc_tiling(self):
        funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                 TilingStrategy.STATIC: self._calc_tiling_static}
        funcs.get(self._tiling_strategy)()

    def _calc_tiling_dynamic(self):
        self._block_factor = operation.var_inner("_block_factor_0", (1, None))
        self._ub_fractal_factor = operation.var_inner("_ub_fractal_factor_0", (1, None))
        self._ub_khw_factor = operation.var_inner("_ub_khw_factor_0", (1, None))

    def _do_cache_read(self):
        if self._insn_pattern == MTE1:
            self._cache_read_tensor_l1 = self._sch.cache_read(self._input_tensor, SCOPE_L1, self._pad_tensor)
        self._fractal_reduce_tensor_ub = self._sch.cache_read(self._fractal_tensor, SCOPE_UB, self._reduce_tensor)
        self._fractal_vcmp_tensor_ub = self._sch.cache_read(self._fractal_tensor, SCOPE_UB, self._vcmp_tensor)
        self._cache_read_tensors_ub.add(self._fractal_reduce_tensor_ub)
        self._cache_read_tensors_ub.add(self._fractal_vcmp_tensor_ub)

    def _do_cache_write(self):
        for _output_tensor in self._output_tensors:
            if _output_tensor != self._depad_tensor:
                self._vand_tensor_ub = self._sch.cache_write(_output_tensor, SCOPE_UB)
                self._cache_write_tensors.add(self._vand_tensor_ub)
        self._fractal_tensor_ub = self._sch.cache_write(self._fractal_tensor, SCOPE_UB)
        self._cache_write_tensors.add(self._fractal_tensor_ub)

    def _do_cache_clone(self):
        self._vand_tensor_clone = self._sch.cache_clone(self._vand_tensor, SCOPE_UB, self._vor_tensor)

    def _set_scope(self):
        for _tensor in self._middle_tensors:
            if _tensor != self._fractal_tensor:
                self._sch[_tensor].set_scope(SCOPE_UB)
            else:
                self._sch[_tensor].set_scope(SCOPE_WORKSPACE)

    def _do_compute_inline(self):
        if self._insn_pattern == MTE1:
            self._sch[self._pad_tensor].compute_inline()
            self._middle_tensors.remove(self._pad_tensor)
        self._sch[self._vand_tensor_clone].compute_inline()

    def _do_memory_reuse(self):
        self._sch[self._init_or_tensor].reused_by(self._vor_tensor)
        self._sch[self._init_not_tensor].reused_by(self._vnot_tensor)
        if self._insn_pattern == MTE1:
            self._sch[self._init_or_tensor].mem_unique()
            self._sch[self._init_not_tensor].mem_unique()
            self._sch[self._vand_tensor_ub].mem_unique()

    def _do_tiling(self):
        # n, c1, khkw, fractal, 16, c0
        fractal_gm_o, fractal_gm_i = self._sch[self._fractal_tensor]. \
            split(self._fractal_tensor.op.axis[self._fractal_axis], self._ub_fractal_factor)
        khw_gm_o, khw_gm_i = self._sch[self._fractal_tensor]. \
            split(self._fractal_tensor.op.axis[self._khw_axis], self._ub_khw_factor)
        self._sch[self._fractal_tensor].reorder(self._sch[self._fractal_tensor].op.axis[0],
                                                self._sch[self._fractal_tensor].op.axis[1], fractal_gm_o,
                                                khw_gm_o, khw_gm_i, fractal_gm_i,
                                                self._sch[self._fractal_tensor].op.axis[4],
                                                self._sch[self._fractal_tensor].op.axis[5])
        self._workspace_emit_insn_axis = khw_gm_i
        self._workspace_compute_at_axis = khw_gm_o
        self._workspace_l1_compute_at_axis = fractal_gm_o

        # n, c1, khkw, fractal, 16, c0
        fractal_o, fractal_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._fractal_axis], self._ub_fractal_factor)
        khw_o, khw_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._khw_axis], self._ub_khw_factor)
        self._sch[self._split_tensor].reorder(self._sch[self._split_tensor].op.axis[0],
                                              self._sch[self._split_tensor].op.axis[1], fractal_o,
                                              khw_o, khw_i, fractal_i,
                                              self._sch[self._split_tensor].op.axis[4],
                                              self._sch[self._split_tensor].op.axis[5])
        self._emit_insn_axis = khw_i
        self._no_overlap_axis = khw_o
        if self._fused_outer:
            block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                                  self._sch[self._split_tensor].op.axis[1], fractal_o)
        else:
            block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                                  self._sch[self._split_tensor].op.axis[1])

        block_outer, block_inner = self._sch[self._split_tensor].split(block_outer_fuse, self._block_factor)
        self._sch[self._split_tensor].bind(block_outer, tvm.thread_axis("blockIdx.x"))
        self._compute_at_axis = khw_o
        self._fractal_compute_at_axis = block_inner if self._fused_outer else fractal_o
        self._reduce_compute_at_axis = block_inner if self._fused_outer else fractal_o

        if self._insn_pattern == VECTOR:
            vector_fractal_o, vector_fractal_i = self._sch[self._fractal_tensor_ub]. \
                split(self._fractal_tensor_ub.op.axis[self._fractal_axis], self._ub_fractal_factor)
            if self._fractal_tensor_ub.dtype == "float32":
                c0_o, c0_i = self._sch[self._fractal_tensor_ub].split(self._fractal_tensor_ub.op.axis[5], 8)
                self._sch[self._fractal_tensor_ub].reorder(self._sch[self._fractal_tensor_ub].op.axis[0],
                                                           self._sch[self._fractal_tensor_ub].op.axis[1],
                                                           vector_fractal_o,
                                                           self._sch[self._fractal_tensor_ub].op.axis[2],
                                                           c0_o, vector_fractal_i,
                                                           self._sch[self._fractal_tensor_ub].op.axis[4],
                                                           c0_i)
            self._fractal_inner = vector_fractal_i
            self._fractal_outer = fractal_o

        # reduce reverse compute at
        reduce_o, reduce_i = self._sch[self._reduce_tensor].split(self._reduce_tensor.op.axis[self._fractal_axis],
                                                                  self._ub_fractal_factor)
        self.reduce_axis_o, self.reduce_axis_i = self._sch[self._reduce_tensor].split(
            self._reduce_tensor.op.reduce_axis[0],
            self._ub_khw_factor)
        self._reverse_reduce_o, self._reverse_reduce_i = self._sch[self._reduce_tensor].split(reduce_i, nparts=1)
        self._sch[self._reduce_tensor].reorder(self._reduce_tensor.op.axis[0], self._reduce_tensor.op.axis[1],
                                               self._reduce_tensor.op.axis[2],
                                               reduce_o,
                                               self._reverse_reduce_o,
                                               self.reduce_axis_o, self.reduce_axis_i,
                                               self._reverse_reduce_i,
                                               self._reduce_tensor.op.axis[4], self._reduce_tensor.op.axis[5])

        # vand_ub reverse compute at
        _vand_tensor_ub_o, _vand_tensor_ub_i = self._sch[self._vand_tensor_ub].split(
            self._sch[self._vand_tensor_ub].op.axis[self._fractal_axis], self._ub_fractal_factor)
        self._reverse_vand_ub_o, self._reverse_vand_ub_i = self._sch[self._vand_tensor_ub].split(
            _vand_tensor_ub_i, nparts=1)

    def _do_storage_bound(self):
        max_dtype_count = self._ub_size // self._tensor_number // DTYPE_BYTE_MAPPING.get(self._input_tensor.dtype)
        total_ub_tensors = list(self._middle_tensors) \
                           + list(self._cache_write_tensors) \
                           + list(self._cache_read_tensors_ub)
        if self._insn_pattern == MTE1:
            self._sch[self._cache_read_tensor_l1].set_buffer_size(
                self._l1_size // DTYPE_BYTE_MAPPING.get(self._cache_read_tensor_l1.dtype))
        else:
            self._sch[self._pad_tensor].set_buffer_size(max_dtype_count*2)
            total_ub_tensors.remove(self._pad_tensor)

        for _mid_tensor in total_ub_tensors:
            self._sch[_mid_tensor].set_buffer_size(max_dtype_count)

    def _do_compute_at(self):
        if self._insn_pattern == MTE1:
            self._sch[self._cache_read_tensor_l1].compute_at(self._sch[self._fractal_tensor],
                                                             self._workspace_l1_compute_at_axis)
        else:
            self._sch[self._pad_tensor].compute_at(self._sch[self._fractal_tensor], self._workspace_compute_at_axis)
        self._sch[self._fractal_tensor_ub].compute_at(self._sch[self._fractal_tensor], self._workspace_compute_at_axis)
        self._sch[self._fractal_tensor].compute_at(self._sch[self._split_tensor], self._fractal_compute_at_axis)
        self._sch[self._fractal_reduce_tensor_ub].compute_at(self._sch[self._reduce_tensor], self.reduce_axis_o)
        self._sch[self._fractal_vcmp_tensor_ub].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._reduce_tensor].compute_at(self._sch[self._split_tensor], self._reduce_compute_at_axis)
        self._sch[self._depad_tensor].reverse_compute_at(self._sch[self._reduce_tensor], self._reverse_reduce_o)
        self._sch[self._vcmp_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._init_not_tensor].compute_at(self._sch[self._split_tensor], self._fractal_compute_at_axis)
        self._sch[self._vand_tensor_ub].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._init_or_tensor].compute_at(self._sch[self._split_tensor], self._fractal_compute_at_axis)
        self._sch[self._vor_tensor].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)
        self._sch[self._vnot_tensor].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)
        self._sch[self._dummy_node].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)

    def _do_emit_insn(self):
        if self._insn_pattern == MTE1:
            self._sch[self._cache_read_tensor_l1].emit_insn(self._sch[self._cache_read_tensor_l1].op.axis[1],
                                                            "dma_copy")
            im2col_attr_0 = self._gen_load3d_attr(self._insn_pattern)
            self._sch[self._fractal_tensor_ub].emit_insn(self._sch[self._fractal_tensor_ub].op.axis[1], "im2col_v2",
                                                         attrs=im2col_attr_0)
        else:
            self._sch[self._pad_tensor].emit_insn(self._sch[self._pad_tensor].op.axis[3], "dma_copy")
            self._sch[self._pad_tensor].pragma(self._sch[self._pad_tensor].op.axis[2], "loop_with_no_overlap_tensor")
            vector_attr_0 = self._gen_load3d_attr(self._insn_pattern)
            vector_attr_0["ho_wo_outer"] = self._fractal_outer
            vector_attr_0["wo"] = self._fractal_tensor.op.attrs["wo"]
            vector_attr_0["ho"] = self._fractal_tensor.op.attrs["ho"]
            if util.is_v220():
                self._sch[self._fractal_tensor_ub].emit_insn(self._fractal_inner,
                                                             "img2col_vector_shl", vector_attr_0)
            else:
                self._sch[self._fractal_tensor_ub].emit_insn(self._fractal_inner,
                                                             "img2col_vector_or", vector_attr_0)
            hwo = self._fractal_tensor.op.attrs["hwo"]
            self._sch[self._fractal_tensor_ub].set_value(lambda *i: (i[3]*16 + i[4]) > hwo - 1,
                                                         tvm.const(0.0, self._fractal_tensor.dtype))
        self._sch[self._fractal_tensor].emit_insn(self._workspace_emit_insn_axis, "dma_copy")
        self._sch[self._fractal_reduce_tensor_ub].emit_insn(self._sch[self._fractal_reduce_tensor_ub].op.axis[1],
                                                            "dma_copy")
        self._sch[self._fractal_vcmp_tensor_ub].emit_insn(self._sch[self._fractal_vcmp_tensor_ub].op.axis[1],
                                                          "dma_copy")
        self._sch[self._reduce_tensor].emit_insn(self.reduce_axis_i, "vector_reduce_max")
        self._sch[self._depad_tensor].emit_insn(self._sch[self._depad_tensor].op.axis[1], "dma_copy")
        self._sch[self._vcmp_tensor].emit_insn(self._sch[self._vcmp_tensor].op.axis[1], "vector_auto")
        self._sch[self._init_not_tensor].emit_insn(self._sch[self._init_not_tensor].op.axis[0], "vector_dup")
        self._sch[self._vand_tensor_ub].emit_insn(self._reverse_vand_ub_i, "vector_auto")
        self._sch[self._vand_tensor].emit_insn(self._emit_insn_axis, "dma_copy")
        self._sch[self._init_or_tensor].emit_insn(self._sch[self._init_or_tensor].op.axis[0], "vector_dup")
        self._sch[self._vor_tensor].emit_insn(self._sch[self._vor_tensor].op.axis[3], "vector_auto")
        self._sch[self._vnot_tensor].emit_insn(self._sch[self._vnot_tensor].op.axis[3], "vector_auto")
        self._sch[self._dummy_node].emit_insn(self._sch[self._dummy_node].op.axis[3], "phony_insn")
        self._sch[self._vand_tensor].pragma(self._no_overlap_axis, "loop_with_no_overlap_tensor")

    def _add_workspace_json(self):
        self._output_tensors.append(self._fractal_tensor)
        if self._tiling_strategy == TilingStrategy.STATIC:
            fractal_shape = util.shape_to_list(self._fractal_tensor.shape)
            if self._fused_outer:
                fractal_outer_num = (fractal_shape[3] + self._ub_fractal_factor - 1) // self._ub_fractal_factor
                block_outer_num = (fractal_shape[0] * fractal_shape[1] * fractal_outer_num + self._block_factor - 1) \
                                  // self._block_factor
            else:
                block_outer_num = (fractal_shape[0] * fractal_shape[1] + self._block_factor - 1) // self._block_factor
            workspace_size = block_outer_num * fractal_shape[2] * self._ub_fractal_factor * fractal_shape[4] \
                             * fractal_shape[5] * DTYPE_BYTE_MAPPING.get(self._input_tensor.dtype)
            workspace_dict_in_json = {
                "num": 1,
                "size": [workspace_size, ],
                "type": [0]
            }
            operation.get_op_context().add_build_json_result("workspace", workspace_dict_in_json)


class PoolingWithArgC0ReorderSchedule(PoolingWithArgWorkSpaceSchedule):
    """
    pooling_with_arg_vars schedule
    """
    def _do_tiling(self):
        # n, c1, khkw, fractal, 16, c0
        fractal_gm_o, fractal_gm_i = self._sch[self._fractal_tensor]. \
            split(self._fractal_tensor.op.axis[self._fractal_axis], self._ub_fractal_factor)
        khw_gm_o, khw_gm_i = self._sch[self._fractal_tensor]. \
            split(self._fractal_tensor.op.axis[self._khw_axis], self._ub_khw_factor)
        c0_gm_o, c0_gm_i = self._sch[self._fractal_tensor].split(self._fractal_tensor.op.axis[5], 8)
        self._sch[self._fractal_tensor].reorder(self._sch[self._fractal_tensor].op.axis[0],
                                                self._sch[self._fractal_tensor].op.axis[1], fractal_gm_o, c0_gm_o,
                                                khw_gm_o, khw_gm_i, fractal_gm_i,
                                                self._sch[self._fractal_tensor].op.axis[4],
                                                c0_gm_i)
        self._workspace_emit_insn_axis = khw_gm_i
        self._workspace_compute_at_axis = khw_gm_o
        self._workspace_l1_compute_at_axis = fractal_gm_o

        vector_fractal_o, vector_fractal_i = self._sch[self._fractal_tensor_ub]. \
            split(self._fractal_tensor_ub.op.axis[self._fractal_axis], self._ub_fractal_factor)
        c0_o, c0_i = self._sch[self._fractal_tensor_ub].split(self._fractal_tensor_ub.op.axis[5], 8)
        self._sch[self._fractal_tensor_ub].reorder(self._sch[self._fractal_tensor_ub].op.axis[0],
                                                   self._sch[self._fractal_tensor_ub].op.axis[1],
                                                   vector_fractal_o, c0_o,
                                                   self._sch[self._fractal_tensor_ub].op.axis[2],
                                                   vector_fractal_i,
                                                   self._sch[self._fractal_tensor_ub].op.axis[4],
                                                   c0_i)
        self._fractal_inner = vector_fractal_i

        # n, c1, khkw, fractal, 16, c0
        fractal_o, fractal_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._fractal_axis], self._ub_fractal_factor)
        khw_o, khw_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._khw_axis], self._ub_khw_factor)
        self._sch[self._split_tensor].reorder(self._sch[self._split_tensor].op.axis[0],
                                              self._sch[self._split_tensor].op.axis[1], fractal_o,
                                              khw_o, khw_i, fractal_i,
                                              self._sch[self._split_tensor].op.axis[4],
                                              self._sch[self._split_tensor].op.axis[5])
        self._emit_insn_axis = khw_i
        self._no_overlap_axis = khw_o
        if self._fused_outer:
            block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                                  self._sch[self._split_tensor].op.axis[1], fractal_o)
        else:
            block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                                  self._sch[self._split_tensor].op.axis[1])
        self._fractal_outer = fractal_o
        block_outer, block_inner = self._sch[self._split_tensor].split(block_outer_fuse, self._block_factor)
        self._sch[self._split_tensor].bind(block_outer, tvm.thread_axis("blockIdx.x"))
        self._compute_at_axis = khw_o
        self._fractal_compute_at_axis = block_inner if self._fused_outer else fractal_o
        self._reduce_compute_at_axis = block_inner if self._fused_outer else fractal_o

        # reduce reverse compute at
        reduce_o, reduce_i = self._sch[self._reduce_tensor].split(self._reduce_tensor.op.axis[self._fractal_axis],
                                                                  self._ub_fractal_factor)
        self.reduce_axis_o, self.reduce_axis_i = self._sch[self._reduce_tensor].split(
            self._reduce_tensor.op.reduce_axis[0],
            self._ub_khw_factor)
        self._reverse_reduce_o, self._reverse_reduce_i = self._sch[self._reduce_tensor].split(reduce_i, nparts=1)
        self._sch[self._reduce_tensor].reorder(self._reduce_tensor.op.axis[0], self._reduce_tensor.op.axis[1],
                                               self._reduce_tensor.op.axis[2],
                                               reduce_o,
                                               self._reverse_reduce_o,
                                               self.reduce_axis_o, self.reduce_axis_i,
                                               self._reverse_reduce_i,
                                               self._reduce_tensor.op.axis[4], self._reduce_tensor.op.axis[5])

        # vand_ub reverse compute at
        _vand_tensor_ub_o, _vand_tensor_ub_i = self._sch[self._vand_tensor_ub].split(
            self._sch[self._vand_tensor_ub].op.axis[self._fractal_axis], self._ub_fractal_factor)
        self._reverse_vand_ub_o, self._reverse_vand_ub_i = self._sch[self._vand_tensor_ub].split(
            _vand_tensor_ub_i, nparts=1)


class PoolingWithArgAllSplitSchedule(PoolingWithArgScheduleBase):
    """
    pooling_with_arg_vars schedule
    """
    def __init__(self, outs, tiling_case):
        PoolingWithArgScheduleBase.__init__(self, outs, tiling_case)
        self._fractal_tensor_ub = None
        self._fractal_reduce_tensor_ub = None
        self._fractal_vcmp_tensor_ub = None
        self.reduce_axis_o = None
        self.reduce_axis_i = None
        self._workspace_emit_insn_axis = None
        self._workspace_compute_at_axis = None
        self._workspace_l1_compute_at_axis = None
        self._fractal_compute_at_axis = None
        self._reduce_compute_at_axis = None

    def do_schedule(self):
        self._construct_compute_graph()
        self._traversal_compute_graph()
        self._calc_tiling()
        if not self._valid_tiling:
            return None
        self._sch = tvm.create_schedule([_out.op for _out in self._schedule_outs])
        self._sch.tiling_key = self._tiling_key.tiling_key
        self._do_cache_read()
        self._do_cache_write()
        self._do_cache_clone()
        self._set_scope()
        self._do_compute_inline()
        self._do_memory_reuse()
        self._do_tiling()
        self._do_storage_bound()
        self._do_compute_at()
        self._do_emit_insn()
        self._add_compile_info()
        self._add_workspace_json()
        self._set_constraint()

        return self._sch

    def _calc_tiling(self):
        funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                 TilingStrategy.STATIC: self._calc_tiling_static}
        funcs.get(self._tiling_strategy)()

    def _calc_tiling_dynamic(self):
        self._block_factor = operation.var_inner("_block_factor_0", (1, None))
        self._ub_fractal_factor = 1
        self._ub_khw_factor = 1

    def _do_cache_read(self):
        self._fractal_reduce_tensor_ub = self._sch.cache_read(self._fractal_tensor, SCOPE_UB, self._reduce_tensor)
        self._fractal_vcmp_tensor_ub = self._sch.cache_read(self._fractal_tensor, SCOPE_UB, self._vcmp_tensor)
        self._cache_read_tensors_ub.add(self._fractal_reduce_tensor_ub)
        self._cache_read_tensors_ub.add(self._fractal_vcmp_tensor_ub)

    def _do_cache_write(self):
        for _output_tensor in self._output_tensors:
            if _output_tensor != self._depad_tensor:
                self._vand_tensor_ub = self._sch.cache_write(_output_tensor, SCOPE_UB)
                self._cache_write_tensors.add(self._vand_tensor_ub)
        self._fractal_tensor_ub = self._sch.cache_write(self._fractal_tensor, SCOPE_UB)
        self._cache_write_tensors.add(self._fractal_tensor_ub)

    def _do_cache_clone(self):
        self._vand_tensor_clone = self._sch.cache_clone(self._vand_tensor, SCOPE_UB, self._vor_tensor)

    def _set_scope(self):
        for _tensor in self._middle_tensors:
            if _tensor != self._fractal_tensor:
                self._sch[_tensor].set_scope(SCOPE_UB)

    def _do_compute_inline(self):
        self._sch[self._vand_tensor_clone].compute_inline()

    def _do_memory_reuse(self):
        self._sch[self._init_or_tensor].reused_by(self._vor_tensor)
        self._sch[self._init_not_tensor].reused_by(self._vnot_tensor)

    def _do_tiling(self):
        # n, c1, khkw, fractal, 16, c0
        fractal_gm_o, fractal_gm_i = self._sch[self._fractal_tensor]. \
            split(self._fractal_tensor.op.axis[self._fractal_lower_axis], self._ub_fractal_factor)
        khw_gm_o, khw_gm_i = self._sch[self._fractal_tensor]. \
            split(self._fractal_tensor.op.axis[self._khw_axis], self._ub_khw_factor)
        self._sch[self._fractal_tensor].reorder(self._sch[self._fractal_tensor].op.axis[0],
                                                self._sch[self._fractal_tensor].op.axis[1],
                                                self._sch[self._fractal_tensor].op.axis[3],
                                                fractal_gm_o, khw_gm_o, khw_gm_i, fractal_gm_i,
                                                self._sch[self._fractal_tensor].op.axis[5])
        self._workspace_emit_insn_axis = khw_gm_i
        self._workspace_compute_at_axis = khw_gm_o
        self._workspace_l1_compute_at_axis = fractal_gm_o

        # n, c1, khkw, fractal, 16, c0
        fractal_o, fractal_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._fractal_lower_axis], self._ub_fractal_factor)
        khw_o, khw_i = self._sch[self._split_tensor]. \
            split(self._split_tensor.op.axis[self._khw_axis], self._ub_khw_factor)
        self._sch[self._split_tensor].reorder(self._sch[self._split_tensor].op.axis[0],
                                              self._sch[self._split_tensor].op.axis[1],
                                              self._sch[self._split_tensor].op.axis[3],
                                              fractal_o, khw_o, khw_i, fractal_i,
                                              self._sch[self._split_tensor].op.axis[5])
        self._emit_insn_axis = khw_i

        block_outer_fuse = self._sch[self._split_tensor].fuse(self._sch[self._split_tensor].op.axis[0],
                                                              self._sch[self._split_tensor].op.axis[1],
                                                              self._sch[self._split_tensor].op.axis[3])

        block_outer, block_inner = self._sch[self._split_tensor].split(block_outer_fuse, self._block_factor)
        self._sch[self._split_tensor].bind(block_outer, tvm.thread_axis("blockIdx.x"))
        self._compute_at_axis = khw_o
        self._fractal_compute_at_axis = fractal_o
        self._reduce_compute_at_axis = fractal_o

        # reduce reverse compute at
        reduce_o, reduce_i = self._sch[self._reduce_tensor].split(self._reduce_tensor.op.axis[self._fractal_lower_axis],
                                                                  self._ub_fractal_factor)
        self.reduce_axis_o, self.reduce_axis_i = self._sch[self._reduce_tensor].split(
            self._reduce_tensor.op.reduce_axis[0],
            self._ub_khw_factor)
        self._reverse_reduce_o, self._reverse_reduce_i = self._sch[self._reduce_tensor].split(reduce_i, nparts=1)
        self._sch[self._reduce_tensor].reorder(self._reduce_tensor.op.axis[0],
                                               self._reduce_tensor.op.axis[1],
                                               self._reduce_tensor.op.axis[2],
                                               self._reduce_tensor.op.axis[3],
                                               reduce_o,
                                               self._reverse_reduce_o,
                                               self.reduce_axis_o, self.reduce_axis_i,
                                               self._reverse_reduce_i,
                                               self._reduce_tensor.op.axis[5])

        # vand_ub reverse compute at
        _vand_tensor_ub_o, _vand_tensor_ub_i = self._sch[self._vand_tensor_ub].split(
            self._sch[self._vand_tensor_ub].op.axis[self._fractal_lower_axis], self._ub_fractal_factor)
        self._reverse_vand_ub_o, self._reverse_vand_ub_i = self._sch[self._vand_tensor_ub].split(
            _vand_tensor_ub_i, nparts=1)

    def _do_storage_bound(self):
        max_dtype_count = self._ub_size // self._tensor_number // DTYPE_BYTE_MAPPING.get(self._input_tensor.dtype)

        total_ub_tensors = list(self._middle_tensors) \
                           + list(self._cache_write_tensors) \
                           + list(self._cache_read_tensors_ub)
        for _mid_tensor in total_ub_tensors:
            self._sch[_mid_tensor].set_buffer_size(max_dtype_count)

    def _do_compute_at(self):
        self._sch[self._pad_tensor].compute_at(self._sch[self._fractal_tensor], self._workspace_compute_at_axis)
        self._sch[self._fractal_tensor_ub].compute_at(self._sch[self._fractal_tensor], self._workspace_compute_at_axis)
        self._sch[self._fractal_tensor].compute_at(self._sch[self._split_tensor], self._fractal_compute_at_axis)
        self._sch[self._fractal_reduce_tensor_ub].compute_at(self._sch[self._reduce_tensor], self.reduce_axis_o)
        self._sch[self._fractal_vcmp_tensor_ub].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._reduce_tensor].compute_at(self._sch[self._split_tensor], self._reduce_compute_at_axis)
        self._sch[self._depad_tensor].reverse_compute_at(self._sch[self._reduce_tensor], self._reverse_reduce_o)
        self._sch[self._vcmp_tensor].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._init_not_tensor].compute_at(self._sch[self._split_tensor], self._fractal_compute_at_axis)
        self._sch[self._vand_tensor_ub].compute_at(self._sch[self._split_tensor], self._compute_at_axis)
        self._sch[self._init_or_tensor].compute_at(self._sch[self._split_tensor], self._fractal_compute_at_axis)
        self._sch[self._vor_tensor].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)
        self._sch[self._vnot_tensor].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)
        self._sch[self._dummy_node].reverse_compute_at(self._sch[self._vand_tensor_ub], self._reverse_vand_ub_o)

    def _do_emit_insn(self):
        self._sch[self._pad_tensor].emit_insn(self._sch[self._pad_tensor].op.axis[3], "dma_copy")
        self._sch[self._fractal_tensor_ub].emit_insn(self._sch[self._fractal_tensor_ub].op.axis[-1], "vector_or")
        self._sch[self._fractal_tensor].emit_insn(self._workspace_emit_insn_axis, "dma_copy")
        self._sch[self._fractal_reduce_tensor_ub].emit_insn(self._sch[self._fractal_reduce_tensor_ub].op.axis[1],
                                                            "dma_copy")
        self._sch[self._fractal_vcmp_tensor_ub].emit_insn(self._sch[self._fractal_vcmp_tensor_ub].op.axis[1],
                                                          "dma_copy")
        self._sch[self._reduce_tensor].emit_insn(self.reduce_axis_i, "vector_reduce_max")
        self._sch[self._depad_tensor].emit_insn(self._sch[self._depad_tensor].op.axis[-1], "dma_copy")
        self._sch[self._vcmp_tensor].emit_insn(self._sch[self._vcmp_tensor].op.axis[-1], "vector_auto")
        self._sch[self._init_not_tensor].emit_insn(self._sch[self._init_not_tensor].op.axis[-1], "vector_dup")
        self._sch[self._vand_tensor_ub].emit_insn(self._reverse_vand_ub_i, "vector_auto")
        self._sch[self._vand_tensor].emit_insn(self._emit_insn_axis, "dma_copy",
                                               {"no_overlap": "process_data_smaller_than_one_block_by_calcute_index"})
        self._sch[self._init_or_tensor].emit_insn(self._sch[self._init_or_tensor].op.axis[-1], "vector_dup")
        self._sch[self._vor_tensor].emit_insn(self._sch[self._vor_tensor].op.axis[-1], "vector_auto")
        self._sch[self._vnot_tensor].emit_insn(self._sch[self._vnot_tensor].op.axis[-1], "vector_auto")
        self._sch[self._dummy_node].emit_insn(self._sch[self._dummy_node].op.axis[-1], "phony_insn")

    def _add_workspace_json(self):
        self._output_tensors.append(self._fractal_tensor)
        if self._tiling_strategy == TilingStrategy.STATIC:
            fractal_shape = util.shape_to_list(self._fractal_tensor.shape)
            workspace_size = \
                reduce(lambda x, y: x * y, fractal_shape) * DTYPE_BYTE_MAPPING.get(self._input_tensor.dtype)
            workspace_dict_in_json = {
                "num": 1,
                "size": [workspace_size, ],
                "type": [0]
            }
            operation.get_op_context().add_build_json_result("workspace", workspace_dict_in_json)
