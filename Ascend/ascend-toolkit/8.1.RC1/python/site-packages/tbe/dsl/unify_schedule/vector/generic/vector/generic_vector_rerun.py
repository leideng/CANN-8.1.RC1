#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
generic vector rerun
"""
import tbe
from tbe import tvm
from tbe.common.context import op_context
from tbe.common.utils import shape_util
from tbe.dsl.base import operation
from tbe.dsl.base.record.records import tensor as record_tensor
from tbe.dsl.base.record.rerun import ReRun
from tbe.dsl.classifier import shape_classifier
from tbe.dsl.unify_schedule.constants import Pattern


class GenericVectorReRun(ReRun):
    def __init__(self, sch, config_map):
        super().__init__(sch, config_map)
        self._tensors = self._dfs_record_compute_graph()

    @classmethod
    def get_pattern(cls):
        return Pattern.GENERIC_VECTOR

    def rerun(self):
        """
        generic vector rerun
        """
        record_inputs_and_outputs = self._config_map.get("tensor_list")
        record_placeholders = [t for t in self._tensors if isinstance(t, record_tensor.Placeholder)]
        record_reduce_tensors = [t for t in self._tensors if isinstance(t, record_tensor.ReduceTensor)]

        # classify
        ori_inputs = [x.ori_input for x in record_placeholders]
        extra_params = {"reduce_axes": record_reduce_tensors[0].reduce_axes}
        classify_outputs = shape_classifier.classify(ori_inputs, "generic_vector", extra_params)

        schedules, tvm_tensors = [], []
        for classify_output in classify_outputs:
            inputs = classify_output[:-1]
            reduce_axis = classify_output[-1]
            with operation.compute():
                # variable shape
                variable_shapes = shape_util.variable_shape(inputs, op_mode="generic_vector")
                # create placeholder
                for shape_x, input_x, record_placeholder_x in zip(variable_shapes, inputs, record_placeholders):
                    tvm_placeholder_x = tvm.placeholder(shape_x, input_x.get("dtype"), record_placeholder_x.name)
                    record_placeholder_x.set_ori(tvm_placeholder_x)
                for reduce_tensor in record_reduce_tensors:
                    reduce_tensor.reduce_axes.set_ori(reduce_axis)
                # execute compute graph
                for action in operation.get_actions():
                    action.execute()
                # auto scheudle
                with tvm.target.cce():
                    tvm_outs = [t.get_ori() for t in self._outs]
                    sch = tbe.dsl.auto_schedule(tvm_outs)
                    schedules.append(sch)
                # collect tvm inputs and outputs
                tvm_tensors.append([t.get_ori() for t in record_inputs_and_outputs])

        if op_context.get_context().get_addition("_only_fusion_check"):
            return
        # build
        self._config_map["tensor_list"] = tvm_tensors
        tbe.dsl.build(schedules, self._config_map)

    def _dfs_record_compute_graph(self):
        def _dfs_record_compute_inner(_tensor):
            if _tensor in visited_tensors:
                return
            visited_tensors.append(_tensor)

            for _tensor_i in _tensor.input_tensors:
                _dfs_record_compute_inner(_tensor_i)

        visited_tensors = []
        for out in self._outs:
            _dfs_record_compute_inner(out)

        return visited_tensors
