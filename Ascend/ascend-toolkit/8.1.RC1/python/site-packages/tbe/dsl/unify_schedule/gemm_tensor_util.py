#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
schedule util
"""
from functools import reduce

from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.utils.errormgr import error_manager_cube
from tbe.common.utils.const import ComputeFlow
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.static_schedule.util import get_all_tensor
from tbe.dsl.static_schedule.util import get_value
from .cube_tensor import TensorCube

MM_LEN_ND = 2
BMM_LEN_ND = 3
BMM_LEN_NZ = 5
NZ_VEC_B = 1
NZ_VEC_A = 2
NZ_VEC_AB = 3


class TensorMatMul(TensorCube):
    """
    use to get tensor for matmul
    """
    def __init__(self, res_list):
        super().__init__(res_list)

    @staticmethod
    def _get_gemm_integrated_flag(tensors_map):
        """
        new integrated sch or old mmad sch
        """
        for tensor in tensors_map.values():
            if "is_gemm_new" in tensor.op.attrs:
                return True
        return False

    def get_tensor_and_para_map(self):
        """
        get the tensor and para for matmul
        """
        if not self._get_gemm_integrated_flag(self.compute_tensors) and \
            not tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
            return []
        self._get_single_cube_tensor()
        self._get_a_tensor()
        self._get_b_tensor()
        self._set_para_map()
        self._get_gemm_tensor()
        self._get_fusion_tensor()
        return [*self.all_tensor, self.tensor_map, self.para_map]

    def _process_multioutput(self):
        """
        handle the multi output for matmul, one of that with virtual_node
        """
        if len(self.res_list) > 1:
            self.para_map["multi_out"] = True
            if "virtual_res" in self.res_list[0].op.tag:
                multi_output_list = [output_tensor for output_tensor in self.res_list[0].op.input_tensors]
                self.tensor_map["multi_output_list"] = multi_output_list
                self.para_map["fusion_multi_output_flag"] = True
            else:
                self.para_map["matmul_multi_output_flag"] = True
        self.tensor_map["c_gm"] = self.res_list[0]
        return self.res_list[-1]

    def _get_tensor_by_placeholder_name(self, placeholder_name_dict, tensor_name):
        if tensor_name not in placeholder_name_dict:
            return None
        return self.placeholder_tensors.get(placeholder_name_dict[tensor_name])

    def _get_single_cube_tensor(self):
        """
        get tensor for mmad
        """
        super()._get_single_cube_tensor()
        # b_reshape
        self.tensor_map["tensor_b_reshape"] = self.compute_tensors.get("tensor_b_swap_c1_hw")
        # set tensor_map by placeholder_name
        placeholder_name = self.tensor_map.get("c_l0c").op.attrs["placeholder_name"]
        placeholder_name_list = ["a_placehold", "b_placehold", "alpha", "beta", "bias", "tensor_c"]
        for phd_name in placeholder_name_list:
            self.tensor_map[phd_name] = self._get_tensor_by_placeholder_name(placeholder_name, phd_name)
        if self.tensor_map.get("tensor_b_reshape") is not None:
            self.tensor_map["b_placehold"] = self.compute_tensors.get(placeholder_name["b_placehold"])

        if self.tensor_map.get("a_placehold") is None or self.tensor_map.get("b_placehold") is None:
            error_manager_cube.raise_err_message_cube("Don't support op + Gemm/MatMul/BatchMatMul ub_fusion")
        # fixpipe matmul is nd2nd for fc single node whose output format is NC1HWC0
        self.tensor_map["fixpipe_matmul"] = self.compute_tensors.get("fixpipe_matmul")

    def _set_base_para_map(self):
        """
        set the compute para from l0c for matmul
        """
        l0c_tensor = self.tensor_map.get("c_l0c")
        for para_name, default_value in (["ops_data_flow_mode", "fp162fp16"], ["only_use_gevm_gemv_flow", False],
                                         ["mmad_mode", "gemm"], ["int8_not_double_m", False],
                                         ["compress_flag", False], ["align_a", True], ["align_b", True],
                                         ["format_a", "FRACTAL_NZ"], ["format_b", "FRACTAL_NZ"],
                                         ["kernel_name", "matmul"], ["split_k", 0],
                                         ["custom_block_dim_m", None], ["custom_block_dim_k", None],
                                         ["custom_block_dim_n", None], ["cache_tiling_flag", False],
                                         ["unaligned_flag", False], ["pre_conv_mode", None],
                                         ["deq_vec_flag", False]):
            if para_name in l0c_tensor.op.attrs:
                self.para_map[para_name] = l0c_tensor.op.attrs[para_name]
            else:
                self.para_map[para_name] = default_value
        if self.para_map.get("custom_block_dim_m") is not None:
            self.para_map["custom_block_dim"] = [
                self.para_map.get("custom_block_dim_m"),
                self.para_map.get("custom_block_dim_k"),
                self.para_map.get("custom_block_dim_n")
            ]
        self.para_map["align"] = self.para_map.get("align_a") and self.para_map.get("align_b")
        if self.para_map.get("mmad_mode") in ("gevm", "gemv") and \
            not self.para_map.get("only_use_gevm_gemv_flow"):
            self.para_map["mad_pattern"] = tbe_platform.GEVM_MODE
        else:
            self.para_map["mad_pattern"] = tbe_platform.GEMM_MODE

    def _check_need_ab_ub(self, tensor_name):
        """
        check if need aub or bub
        """
        support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        support_out2l1_nd2nz = tbe_platform_info.intrinsic_check_support("Intrinsic_data_move_out2l1_nd2nz")
        if not in_dynamic() and support_out2l1_nd2nz:
            return False
        format_key = f"format_{tensor_name}"
        tensor_format = self.para_map.get(format_key)
        nd2nz_type = self.para_map.get("nd2nz_type")
        return tensor_format == "ND" and not (support_out2l1_nd2nz and nd2nz_type != ComputeFlow.mix_l2.value)

    def _set_para_map(self):
        """
        set the compute para for matmul
        """
        self._set_base_para_map()
        l0a_tensor, l0b_tensor, l0c_tensor = \
            self.tensor_map.get("a_l0a"), self.tensor_map.get("b_l0b"), self.tensor_map.get("c_l0c")
        # cal the format out
        format_out = "FRACTAL_NZ"
        res = self.compute_tensors.get("res")
        if res is not None:
            if res.op.attrs is None or "format" not in res.op.attrs:
                if len(res.shape) in (MM_LEN_ND, BMM_LEN_ND):
                    format_out = "ND"
            else:
                format_out = res.op.attrs["format"]
            if res.op.attrs is not None and "dtype_out" in res.op.attrs:
                dtype_out = res.op.attrs["dtype_out"]
                self.para_map["dtype_out"] = dtype_out
        self.para_map["format_out"] = format_out
        self.para_map["have_batch_a"] = len(l0a_tensor.shape) in (BMM_LEN_ND, BMM_LEN_NZ)
        self.para_map["have_batch_b"] = len(l0b_tensor.shape) in (BMM_LEN_ND, BMM_LEN_NZ)
        self.para_map["have_batch"] = len(l0c_tensor.shape) in (BMM_LEN_ND, BMM_LEN_NZ)
        self.para_map["zero_flag"] = self.tensor_map.get("c_gm").op.attrs.get("zero_flag", False)
        self.para_map["alg"] = self.tensor_map.get("b_l0b").op.attrs.get("alg", None)
        if "unaligned_flag" in res.op.attrs:
            self.para_map["unaligned_flag"] = res.op.attrs["unaligned_flag"].value
        if "pad_flag" in self.tensor_map.get("c_gm").op.attrs:
            self.para_map["pad_flag"] = self.tensor_map.get("c_gm").op.attrs.get("pad_flag")
        if "nz_fusion_flag" in self.tensor_map.get("c_gm").op.attrs:
            self.para_map["nz_fusion_flag"] = self.tensor_map.get("c_gm").op.attrs.get("nz_fusion_flag")
        if "nz_fusion_mode" in self.tensor_map.get("c_gm").op.attrs:
            self.para_map["nz_fusion_mode"] = self.tensor_map.get("c_gm").op.attrs.get("nz_fusion_mode")
        self.para_map["nd2nz_type"] = l0c_tensor.op.attrs["nd2nz_type"].value
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd"):
            self.para_map["trans_a"] = l0a_tensor.op.attrs["transpose_a"] == "true"
            self.para_map["trans_b"] = l0b_tensor.op.attrs["transpose_b"] == "true"
        else:
            self.para_map["trans_a"] = l0c_tensor.op.attrs["transpose_a"].value
            self.para_map["trans_b"] = l0c_tensor.op.attrs["transpose_b"].value

        if "ori_batch_shape" in l0a_tensor.op.attrs and "ori_batch_shape" in l0b_tensor.op.attrs:
            batch_a = [get_value(i) for i in l0a_tensor.op.attrs["ori_batch_shape"]]
            batch_b = [get_value(i) for i in l0b_tensor.op.attrs["ori_batch_shape"]]
            batch_broadcast_flag = (batch_a != batch_b)
            batch_broadcast_change_attach = batch_broadcast_flag
            batch_broadcast_change_attach &= (len(batch_a) > 0 and reduce(lambda x, y: x * y, batch_a) != 1)
            batch_broadcast_change_attach &= (len(batch_b) > 0 and reduce(lambda x, y: x * y, batch_b) != 1)
            self.para_map["batch_broadcast_flag"] = batch_broadcast_flag
            self.para_map["batch_broadcast_change_attach"] = batch_broadcast_change_attach
        self.para_map["need_aub"] = self._check_need_ab_ub("a")
        self.para_map["need_bub"] = self._check_need_ab_ub("b")

    def _get_a_tensor(self):
        """
        get the tensor of input as
        """
        get_a_matrix_mode = "none"
        if self.tensor_map.get("a_l0a") is None:
            self.tensor_map["a_l0a"] = self.tensor_map.get("a_placehold")
            self.para_map["a_l0a_cache_read"] = True
        self.para_map["a_dtype"] = self.tensor_map.get("a_l0a").dtype
        if "mode" in self.tensor_map.get("a_l0a").op.attrs:
            get_a_matrix_mode = self.tensor_map.get("a_l0a").op.attrs["mode"]
        self.para_map["a_matrix_mode"] = get_a_matrix_mode
        nz_fusion_flag = 0
        if "nz_fusion_flag" in self.tensor_map.get("c_gm").op.attrs:
            nz_fusion_flag = self.tensor_map.get("c_gm").op.attrs["nz_fusion_flag"]

        # virtual ub tensor is used in dynamic and ND
        self.tensor_map["a_ub"] = self.compute_tensors.get("tensor_a_aligned")
        self.tensor_map["a_ub_aligned"] = self.compute_tensors.get("tensor_a_already_aligned")
        self.tensor_map["a_ub_general"] = self.compute_tensors.get("tensor_a_do_align")
        self.tensor_map["a_int82fp16"] = self.compute_tensors.get("tensor_a_s82f16")
        self.tensor_map["a_transpose"] = self.compute_tensors.get("tensor_a_transpose")

        if get_a_matrix_mode in ("none", "nd2Zz_int8", "nd_gevm", "Nz2Zz_int82fp32"):
            self.tensor_map["a_l1"] = self.tensor_map.get("a_l0a")
        if get_a_matrix_mode == "nd2Zz_vnchwconv":
            self.tensor_map["a_ub_fract"] = self.compute_tensors.get("tensor_a_zz_fract_k")
            self.tensor_map["a_l1"] = self.tensor_map.get("a_ub_fract")
        if get_a_matrix_mode == "Nz2Zz_int82fp32":
            self.tensor_map["a_ub"] = self.tensor_map.get("a_placehold")
            self.tensor_map["a_ub_fract"] = self.tensor_map.get("a_l1")
        if get_a_matrix_mode in ("Nz2Zz", "fractal_gemv", "Zz_trans"):
            self.tensor_map["a_l1"] = self.tensor_map.get("a_placehold")
        if get_a_matrix_mode == "nd_gemv":
            self.tensor_map["a_l1"] = self.compute_tensors.get("tensor_a_nd2zz")
            if self.para_map.get("a_dtype") == "float16":
                self.tensor_map["a_ub_fract"] = self.compute_tensors.get("tensor_a_nd2zz")
        if get_a_matrix_mode == "nd_gevm":
            self.tensor_map["a_ub_fract"] = self.compute_tensors.get("tensor_a_nd2zz")
        if get_a_matrix_mode == "nd2Zz":
            self.tensor_map["a_l1"] = self.compute_tensors.get("tensor_a_l1")
            if self.tensor_map.get("a_l1") is None:
                self.tensor_map["a_l1"] = self.tensor_map.get("a_l0a")
            if self.para_map.get("a_dtype") == "float16":
                self.tensor_map["a_ub_fract"] = self.tensor_map.get("a_l1")
        if self.tensor_map.get("a_ub") is not None and "virtual_align" in self.tensor_map.get("a_ub").op.attrs:
            self.tensor_map["a_ub_virtual_align"] = self.tensor_map.get("a_ub")
            self.tensor_map["a_ub"] = self.tensor_map.get("a_placehold")
        if get_a_matrix_mode == "nd2Zz_trans_fusion":
            # milan pre transdata fusion
            self.tensor_map["a_l1"] = self.tensor_map.get("a_l0a").op.input_tensors[0]
            if nz_fusion_flag in [NZ_VEC_A, NZ_VEC_AB]:
                self.tensor_map["a_ub_nd"] = self.tensor_map.get("a_l1").op.input_tensors[0]
        if get_a_matrix_mode == "nhwc2Zz":
            # fc pre transdata fusion
            self.tensor_map["a_l1"] = self.tensor_map.get("a_l0a").op.input_tensors[0]
            self.tensor_map["a_l1_5hd"] = self.compute_tensors.get("tensor_a_5hd2nz").op.input_tensors[0]

    def _get_b_tensor(self):
        """
        get the tensor of input b
        """
        get_b_matrix_mode = "none"
        if self.tensor_map.get("b_l0b") is None:
            self.tensor_map["b_l0b"] = self.tensor_map.get("b_placehold")
            self.para_map["b_l0b_cache_read"] = True
        self.para_map["b_dtype"] = self.tensor_map.get("b_l0b").dtype
        if "mode" in self.tensor_map.get("b_l0b").op.attrs:
            get_b_matrix_mode = self.tensor_map.get("b_l0b").op.attrs["mode"]
        self.para_map["b_matrix_mode"] = get_b_matrix_mode
        nz_fusion_flag = 0
        if "nz_fusion_flag" in self.tensor_map.get("c_gm").op.attrs:
            nz_fusion_flag = self.tensor_map.get("c_gm").op.attrs["nz_fusion_flag"]

        # virtual ub tensor is used in dynamic and ND
        self.tensor_map["b_ub"] = self.compute_tensors.get("tensor_b_aligned")
        self.tensor_map["b_ub_aligned"] = self.compute_tensors.get("tensor_b_already_aligned")
        self.tensor_map["b_ub_general"] = self.compute_tensors.get("tensor_b_do_align")
        self.tensor_map["b_int82fp16"] = self.compute_tensors.get("tensor_b_s82f16")
        self.tensor_map["b_transpose"] = self.compute_tensors.get("tensor_b_transpose")

        if get_b_matrix_mode == "nd_gemv":
            self.tensor_map["b_l1"] = self.compute_tensors.get("tensor_b_nd2zz")
            if self.para_map.get("b_dtype") == "float16":
                self.tensor_map["b_ub_fract"] = self.tensor_map.get("b_l1")
        elif get_b_matrix_mode == "nd2Zn_vnchwconv":
            self.tensor_map["b_ub_fract"] = self.compute_tensors.get("tensor_b_zn_fract")
            self.tensor_map["b_l1"] = self.tensor_map.get("b_ub_fract")
        elif get_b_matrix_mode == "nd2Zn_int8":
            self.tensor_map["b_l1"] = self.tensor_map.get("b_l0b")
        elif get_b_matrix_mode == "Zn2Zn_int82fp32":
            self.tensor_map["b_ub"] = self.tensor_map.get("b_placehold")
            self.tensor_map["b_l1"] = self.tensor_map.get("b_l0b")
            self.tensor_map["b_ub_fract"] = self.tensor_map.get("b_l1")
        elif get_b_matrix_mode in ("Nz2Zn", "Nz2Zz", "fractal_gemv", "Zn_trans"):
            self.tensor_map["b_l1"] = self.tensor_map.get("b_placehold")
        elif get_b_matrix_mode == "nd2Zn":
            self.tensor_map["b_l1"] = self.compute_tensors.get("tensor_b_l1")
            if self.tensor_map.get("b_l1") is None:
                self.tensor_map["b_l1"] = self.tensor_map.get("b_l0b")
            self.tensor_map["b_ub_fract"] = self.tensor_map.get("b_l1")
        elif get_b_matrix_mode == "nd2Zn_trans_fusion":
            self.tensor_map["b_l1"] = self.tensor_map.get("b_l0b").op.input_tensors[0]
            if nz_fusion_flag in [NZ_VEC_B, NZ_VEC_AB]:
                self.tensor_map["b_ub_nd"] = self.tensor_map.get("b_l1").op.input_tensors[0]
        else:
            if "tile_L1_n" in self.tensor_map.get("b_l0b").op.attrs:
                self.tensor_map["b_l1"] = self.tensor_map.get("b_l0b")
            else:
                self.tensor_map["b_l1"] = self.tensor_map.get("b_placehold")

    def _get_gemm_tensor(self):
        """
        get tensor of gemm which is alpha*a*b + beta*c
        """
        # the first from l0c to ub
        support_out2l1_nd2nz = tbe_platform_info.intrinsic_check_support("Intrinsic_data_move_out2l1_nd2nz")
        support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        if support_l0c2out and not support_out2l1_nd2nz:
            self.tensor_map["c_ub_fract"] = self.compute_tensors.get("fixpipe_matmul_ub")
        else:
            self.tensor_map["c_ub_fract"] = self.compute_tensors.get("tensor_mmad_with_scale")
        # the c of gemm
        self.tensor_map["c_add_bias_ub"] = self.compute_tensors.get("tensor_gemm")
        if self.tensor_map.get("c_add_bias_ub") is not None:
            self.tensor_map["bias_ub"] = self.compute_tensors.get("tensor_c_aligned")
        self.tensor_map["bias_cast_to_fp32"] = self.compute_tensors.get("tensor_c_f162f32")
        # the beta of gemm
        self.tensor_map["beta_bias"] = self.compute_tensors.get("tensor_beta_c")
        self.tensor_map["beta_fp162fp32"] = self.compute_tensors.get("tensor_beta_f162f32")
        # the alpha of gemm
        self.tensor_map["alpha_c"] = self.compute_tensors.get("tensor_alpha_mmad")
        self.tensor_map["alpha_fp162fp32"] = self.compute_tensors.get("tensor_alpha_f162f32")
        # after gemm
        self.tensor_map["cast_to_fp16"] = self.compute_tensors.get("tensor_gemm_f16")
        self.tensor_map["cast_to_fp32"] = self.compute_tensors.get("tensor_gemm_f32")
        self.tensor_map["c_add_bias_ub_fp16"] = self.compute_tensors.get("tensor_mmad_with_bias_fp16")
        self.tensor_map["c_add_bias_ub_fp32"] = self.compute_tensors.get("tensor_mmad_with_bias_fp32")
        # nz to nd
        self.tensor_map["before_c_gm"] = self.compute_tensors.get("before_c_gm")
        self.tensor_map["bias_ub_drnn_cast_fp16"] = self.compute_tensors.get("bias_ub_drnn_cast_float16")
        self.tensor_map["bias_ub_drnn_cast_fp32"] = self.compute_tensors.get("bias_ub_drnn_cast_float32")
        self.tensor_map["nz_to_nd"] = self.compute_tensors.get("tensor_nz2nd")
        self.tensor_map["nz_to_nd_fp32"] = self.compute_tensors.get("tensor_nz2nd_fp32")
        self.tensor_map["tensor_out_fp16"] = self.compute_tensors.get("tensor_out_fp16")
        self.tensor_map["tensor_out_fp32"] = self.compute_tensors.get("tensor_out_fp32")
        self.tensor_map["tensor_virtual_res"] = self.compute_tensors.get("tensor_virtual_res")

        if self.tensor_map.get("cast_to_fp16") is not None and \
            self.tensor_map.get("cast_to_fp16").op.input_tensors[0] == self.tensor_map.get("c_ub_fract"):
            self.para_map["c_ub_fract_inline"] = True
