#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
common function for tiling
"""
import os
from functools import reduce
import math

from tbe import tvm
from tbe.common import platform as tbe_platform
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.tiling import get_tiling_type
from tbe.common.tiling import set_tiling_type
from tbe.common.tiling import tiling_api
from tbe.common.utils import log
from tbe.common.utils import decode
from tbe.common.utils import do_op_tiling
from tbe.common.utils.errormgr import error_manager_cube
from tbe.common.utils.errormgr import error_manager_util
from tbe.common.utils.op_util.op_util_cube import decode_tiling
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.boost_schedule_kit import Compare
from tbe.dsl.unify_schedule.cube_tilingcase import TilingUtils as utils
from tbe.dsl.static_schedule.util import align as int_ceil_align
from tbe.dsl.static_schedule.util import ceil as int_ceil_div
from tbe.dsl.static_schedule.util import CalculateMultiUB
from tbe.dsl.static_schedule.util import get_value
from tbe.dsl.static_schedule.util import shape_to_list
from tbe.dsl.static_schedule.util import get_op_impl_mode_enum
from tbe.dsl.compute.gemm_compute_util import GEMMComputeParam
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import debug
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import get_al1_m_fix_value
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import CacheTilingManager
from tbe.dsl.static_schedule.gemm_integrated_schedule_util import GemmTilingWork


BMM_LEN_ND = 3
BMM_LEN_NZ = 5
K_AXIS_ALIGN_FACTOR = 2
N_INDEX = -1
M_INDEX = -2
MC_FACTOR_INDEX = -3
NC_FACTOR_INDEX = -4
M_L0A_INDEX = -4
UNIT_LEN = 16
UNIT_LEN_32 = 32
ND_LENGTH = 2
NZ_MAX_IDX = 3
ALIGN_NUM_TWO = 2

DATA_SIZE = {
    "float16": 2, "bfloat16": 2, "int16": 2, "uint16": 2, "uint8": 1,
    "int8": 1, "float32": 4, "int32": 4, "uint64": 8, "int4": 0.5, "bool": 1
}
FIXPIPE_FLAG_BIT_OFFSET = {
    "quant_scale_0": 1,
    "relu_weight_0": 2,
    "relu_weight_1": 3,
    "quant_scale_1": 4,
    "eltwise_src": 5
}
MAD_TYPE = {
    "fp162fp16": "float32",
    "fp162fp32": "float32",
    "fp322fp32": "float32",
    "int82int32": "int32",
    "int42int32": "int32",
    "int82fp32": "float32",
    "bf162bf16": "float32"
}
DTYPE_WIDTH_MAP = {"uint64": 4, "float16": 1, "bfloat16": 1, "float32": 2, "int32": 2,
                   "int16": 1, "uint16": 1, "int8": 0.5, "uint8": 0.5,
                   "int4": 0.25, "bool": 0.5}


class CubeTiling:
    """
    use to get tiling for cube
    """
    def __init__(self, tensor_list):
        self.tensor_list = tensor_list
        self.compute_tensors, self.placeholder_tensors, self.output_tensors = tensor_list[:3]
        self.tensor_map, self.para_map = tensor_list[3], tensor_list[4]
        self.tiling_info_dict = None
        self.tiling = None
        self.dtype_a = "float16"
        self.dtype_b = "float16"
        self.dtype_c = "float16"
        self.res = self.tensor_map.get("c_gm")
        self.sparse_4to2_flag = bool(self.para_map.get("alg", None) == "weight_sparse_4_2")

    def _get_fixpipe_flag(self):
        """
        code the fixpipe
        :param tensor_map: tensor of matmul which after setscope
        :return: int, the flag of fixpipe
        """
        # the 0th bit of fixpipe_flag means is nz2nd or not
        fixpipe_flag = 1 if self.para_map.get("format_out") != "FRACTAL_NZ" else 0
        for fixpipe_input in self.tensor_map.get("fixpipe_input_name", []):
            fixpipe_flag_bit_offset = FIXPIPE_FLAG_BIT_OFFSET.get(fixpipe_input)
            if fixpipe_flag_bit_offset is not None:
                fixpipe_flag += 1 << fixpipe_flag_bit_offset
        return fixpipe_flag

    def _check_auto_tiling(self):
        """
        check tiling from auto_tiling is illegal
        """
        if self.tiling is not None and (self.para_map.get("format_a") == "ND" or self.para_map.get("format_b") == "ND"):
            size_aub = 0
            size_bub = 0
            if self.tiling.get("AUB_shape") is not None:
                size_aub = reduce(lambda x, y: x * y, self.tiling.get("AUB_shape")) * tbe_platform.BLOCK_REDUCE * \
                    DATA_SIZE.get(self.dtype_a)
            if self.tiling.get("BUB_shape") is not None:
                size_bub = reduce(lambda x, y: x * y, self.tiling.get("BUB_shape")) * tbe_platform.BLOCK_REDUCE * \
                    DATA_SIZE.get(self.dtype_b)
            size_cub = reduce(lambda x, y: x * y, self.tiling.get("CUB_matrix")) * DATA_SIZE.get(self.dtype_c)
            a_ub_fused_num = self.tiling_info_dict.get("padl")
            b_ub_fused_num = self.tiling_info_dict.get("padr")
            c_ub_fused_num = self.tiling_info_dict.get("fused_double_operand_num")
            # a_ub_fused_num and b_ub_fused_num is mutiple of 10 and is additional occupancy of ub
            base_num_aub = 1 if a_ub_fused_num else 0 # a_ub_fuse_num is 0 if aub is not used
            base_num_bub = 1 if b_ub_fused_num else 0 # b_ub_fuse_num is 0 if bub is not used
            tiling_size_ub = size_aub * (base_num_aub + a_ub_fused_num // 10) + \
                size_bub * (base_num_bub + b_ub_fused_num // 10) + size_cub * c_ub_fused_num
            if tiling_size_ub > tbe_platform_info.get_soc_spec("UB_SIZE"):
                self.tiling = None

    def _check_tiling(self):
        """
        check tiling illgal or not
        """
        if not self.tiling:
            args_dict = {"errCode": "E60114", "reason": "tiling is None", "value": "None"}
            raise RuntimeError(args_dict, error_manager_util.get_error_message(args_dict))
        if self.tiling.get("AL0_matrix") == [1, 1, 32, 16, 1, 1]:
            args_dict = {"errCode": "E60114", "reason": "tiling is illegal", "value": "None"}
            raise RuntimeError(args_dict, error_manager_util.get_error_message(args_dict))
        self._check_tiling_l1()
        self._check_tiling_l0()

    def _check_tiling_l1(self):
        """
        check tiling illgal or not
        """
        # use l0 tensorâ€˜s shape, not use L1 tensor shape, when L1 is vnchwconv scene, L1 is  (batch, m1, k, m0)
        al0_tensor = self.tensor_map["a_l0a"]
        bl0_tensor = self.tensor_map["b_l0b"]
        al0_shape = shape_to_list(al0_tensor.shape)
        bl0_shape = shape_to_list(bl0_tensor.shape)
        al0_dtype = al0_tensor.dtype
        bl0_dtype = bl0_tensor.dtype
        # the 3rd dim of block_dim is k_dim
        if self.tiling["AL1_shape"] == []:
            al1_size = reduce(lambda x, y: x * y, al0_shape[-4:]) // self.tiling["block_dim"][2] \
            // self.tiling["block_dim"][3]
        else:
            al1_size = self.tiling["AL1_shape"][0] * self.tiling["AL1_shape"][1] * \
                       self.tiling["CL0_matrix"][1] * tbe_platform.BLOCK_IN
            if self.tiling["manual_pingpong_buffer"].get("AL1_pbuffer") == 2:
                al1_size *= 2
        if self.tiling["BL1_shape"] == []:
            bl1_size = reduce(lambda x, y: x * y, bl0_shape[-4:]) // self.tiling["block_dim"][1] \
            // self.tiling["block_dim"][3]
        else:
            bl1_size = self.tiling["BL1_shape"][0] * self.tiling["BL1_shape"][1] * \
                       self.tiling["CL0_matrix"][0] * tbe_platform.BLOCK_OUT
            if self.tiling["manual_pingpong_buffer"].get("BL1_pbuffer") == 2:
                bl1_size *= 2
        l1_size_max = tbe_platform_info.get_soc_spec("L1_SIZE")
        al1_size_max = al1_size * DATA_SIZE.get(al0_dtype, 1)
        bl1_size_max = bl1_size * \
            DATA_SIZE.get(bl0_dtype, 1) if not self.sparse_4to2_flag else (bl1_size * \
            DATA_SIZE.get(bl0_dtype, 1) // 2)
        if (al1_size_max + bl1_size_max) > l1_size_max:
            args_dict = {
                "errCode": "E60114",
                "reason": "tiling size exceed L1 Buffer",
                "value": "tiling size = {}".format(
                    al1_size_max + bl1_size_max
                )
            }
            raise RuntimeError(args_dict, error_manager_util.get_error_message(args_dict))

    def _check_tiling_l0(self):
        """
        check tiling illgal or not
        """
        al0_dtype = self.tensor_map["a_l0a"].dtype
        bl0_dtype = self.tensor_map["b_l0b"].dtype
        cl0_dtype = self.tensor_map["c_l0c"].dtype
        l0a_shape = shape_to_list(self.tensor_map["a_l0a"].shape)
        l0b_shape = shape_to_list(self.tensor_map["b_l0b"].shape)
        mk_dim = l0a_shape[-3] if self.tiling["AL0_matrix"] == [] else self.tiling["AL0_matrix"][1]
        nk_dim = l0b_shape[-4] if self.tiling["BL0_matrix"] == [] else self.tiling["BL0_matrix"][0]
        self.tiling["AL0_matrix"] = [
            self.tiling["CL0_matrix"][1],
            mk_dim,
            tbe_platform.CUBE_MKN[al0_dtype]["mac"][0],
            tbe_platform.CUBE_MKN[al0_dtype]["mac"][1],
            1, 1
            ]
        self.tiling["BL0_matrix"] = [
            nk_dim,
            self.tiling["CL0_matrix"][0],
            tbe_platform.CUBE_MKN[bl0_dtype]["mac"][2],
            tbe_platform.CUBE_MKN[bl0_dtype]["mac"][1],
            1, 1
        ]
        for buffer_name, data_dtype in zip(["A", "B", "C"], [al0_dtype, bl0_dtype, cl0_dtype]):
            l0_buffer_name = "{}{}".format(buffer_name, "L0_matrix")
            l0_size = reduce(lambda x, y: x * y, self.tiling[l0_buffer_name][:4]) * DATA_SIZE.get(data_dtype)
            l0_size_max = tbe_platform_info.get_soc_spec("L0" + buffer_name + "_SIZE")
            if self.tiling["manual_pingpong_buffer"].get(buffer_name + "L0_pbuffer") == 2:
                l0_size *= 2
            if l0_size > l0_size_max:
                args_dict = {
                    "errCode": "E60114",
                    "reason": "tilling size exceed L0 Buffer of " + buffer_name,
                    "value": "tiling size is {} while l0_space is {}".format(l0_size, l0_size_max)
                }
                raise RuntimeError(args_dict, error_manager_util.get_error_message(args_dict))

    def _get_in_and_out_dtype(self):
        """
        get input and output dtype
        """
        dtype_out = self.para_map.get("dtype_out")
        self.dtype_c = dtype_out if dtype_out else self.res.dtype
        self.dtype_a = self.tensor_map.get("a_placehold").dtype
        self.dtype_b = self.tensor_map.get("b_placehold").dtype
        if self.para_map.get("mmad_mode") == "gemv":
            self.dtype_a, self.dtype_b = self.dtype_b, self.dtype_a


class MatMulTiling(CubeTiling):
    """
    use to get tiling for matmul
    """
    def __init__(self, tensor_list, out_list):
        super().__init__(tensor_list)
        self.fuse_num_group = None
        self._trans_a = self.para_map.get("trans_a", False)
        self._trans_b = self.para_map.get("trans_b", False)
        self._ops_data_flow_mode = self.para_map.get("ops_data_flow_mode")
        self.quantfy_fusion = self.para_map.get("quantify_fusion", False)
        self.eletwise_fusion = False
        self.dequant_eletwise_fusion = False
        self.matmul_end_tensor = self.tensor_map.get("tensor_c_gm")
        self.res_list = out_list

    @staticmethod
    def _handle_bool_env(env_name, default_value):
        """
        handle teh env of ENABLE_TUNE_BANK
        """
        str_env = str(os.environ.get(env_name, None)).lower()
        if str_env not in ('true', 'false'):
            return default_value
        return str_env == 'true'

    @staticmethod
    def _check_repository_tiling(tiling):
        """
        check the tiling is from repository
        """
        if all(value == 0 for value in tiling['AL0_matrix']):
            return False
        return True

    @staticmethod
    def _decode_tiling(tiling):
        """
        adjust tiling items for schedule
        """
        decode_tiling(tiling)
        exponent_base = 2
        if "special_optimize_flag" in tiling:
            tiling["binary_flag"] = tiling.get("special_optimize_flag") // exponent_base // exponent_base
        if tiling.get("AL0_matrix") == [0, 0, 32, 0, 0, 0]:
            tiling["AL0_matrix"] = [1, 1, 32, 16, 1, 1]

        l0c_output_matrix = tiling.get("L0C_OUTPUT_matrix")
        cub_matrix = tiling.get("CUB_matrix")
        tiling["CUB_matrix"] = \
            l0c_output_matrix if cub_matrix is None and l0c_output_matrix is not None else cub_matrix
        pingpong_buffer = tiling.get("manual_pingpong_buffer")
        l0c_output_pbuffer = pingpong_buffer.get("L0C_OUTPUT_pbuffer")
        cub_pbuffer = pingpong_buffer.get("CUB_pbuffer")
        tiling["manual_pingpong_buffer"]["CUB_pbuffer"] = \
            l0c_output_pbuffer if cub_pbuffer is None and l0c_output_pbuffer is not None else cub_pbuffer

    @staticmethod
    def _translate_dtype_bf16_to_fp16(data_type):
        return "float16" if (data_type == "bfloat16") else data_type

    def get_tiling_info_dict(self):
        """
        get the tiling input: tiling info dict
        """
        self._get_fusion_flag()
        self._get_in_and_out_dtype()
        a_shape, b_shape = self._get_input_shape()
        a_ub_fuse_num, b_ub_fuse_num, c_ub_fused_num = self._get_general_fuse_num()
        new_c_ub_fused_num, scalar_size = self._cal_new_cub_fuse_num(c_ub_fused_num)
        self.fuse_num_group = [a_ub_fuse_num, b_ub_fuse_num, new_c_ub_fused_num]
        self.para_map["fuse_num_group"] = self.fuse_num_group
        info_dict = {
            "op_type": "matmul",
            "A_shape": a_shape,
            "B_shape": b_shape,
            "C_shape": [0, 0, 0, 0, 0],
            "A_dtype": self._translate_dtype_bf16_to_fp16(self.dtype_a),
            "B_dtype": self._translate_dtype_bf16_to_fp16(self.dtype_b),
            "C_dtype": self._translate_dtype_bf16_to_fp16(self.dtype_c),
            "mad_dtype": MAD_TYPE.get(self._ops_data_flow_mode),
            "padl": a_ub_fuse_num,
            "padr": b_ub_fuse_num,
            "padu": int(self.para_map.get("mad_pattern") == tbe_platform.GEVM_MODE),
            "padd": int(self.para_map.get("int8_not_double_m") and not self._trans_a),
            "strideH":  self._get_op_type_flag(),
            "strideW": self._get_stride_w(),
            "strideH_expand": 1,
            "strideW_expand": 1,
            "dilationH": self._get_trans_flag(),
            "dilationW": int(not self.para_map.get("compress_flag")),
            "group": 1,
            "bias_flag": self.tensor_map.get("bias") is not None,
            "fused_double_operand_num": c_ub_fused_num,
            "batch_type": self._get_batch_flag(),
            "kernel_name": str(self.para_map.get("kernel_name")),
        }
        if in_dynamic():
            info_dict_dynamic = {
                "op_tag": "matmul",
                "dynamic_shape_flag": True,
                "trans_a": bool(self._trans_a),
                "trans_b": bool(self._trans_b),
                "unaligned_flag": bool(self.para_map.get("unaligned_flag")),
                "pad_flag": self.para_map.get("pad_flag", 0),
                "nz_fusion_flag": self.para_map.get("nz_fusion_flag", 0)
            }
            info_dict.update(info_dict_dynamic)
        else:
            info_dict_static = {
                "shape_a_align": self.para_map.get("align_a").value,
                "shape_b_align": self.para_map.get("align_b").value,
                "scalar_size": scalar_size,
                "reduce_fusion": int("reduce_sum" in self.tensor_map["c_gm"].op.tag)
            }
            info_dict.update(info_dict_static)
        self.tiling_info_dict = info_dict
        return self.tiling_info_dict

    def get_static_tiling(self, context):
        """
        get static tiling of different platform
        """
        current_tiling_type = get_tiling_type()
        self._update_info_dict(self.tiling_info_dict)
        old_tiling, new_tiling, old_tiling_from_repo, new_tiling_from_repo = \
            self._get_tiling_from_repository(current_tiling_type)
        # old fuse num get repository tiling
        if old_tiling_from_repo:
            self.tiling = old_tiling
        # the op gemm not use auto tiling
        elif self.tensor_map.get("alpha") is None:
            if new_tiling_from_repo:
                self.tiling = new_tiling
            else:
                # binary constant
                if self._support_binary_const() and not self.tiling and not self.res.op.attrs.get("zero_flag", False):
                    self.res.op.attrs["is_binary_constant"] = 1
                elif tbe_platform_info.intrinsic_check_support("Intrinsic_data_move_out2l1_nd2nz") or \
                     self.sparse_4to2_flag:
                    self.tiling = self._get_static_cache_tiling(context)
                elif not tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
                    self.tiling = auto_tiling(self.fuse_num_group, self.tensor_map, self.para_map)
                    self._check_auto_tiling()
        if not self.tiling:
            self.tiling_info_dict["fused_double_operand_num"] = self.fuse_num_group[2]
            self.tiling = tiling_api.get_tiling(self.tiling_info_dict)
            self._default_tiling_for_int8(context)
        self._decode_tiling(self.tiling)
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
            self._check_tiling()
        return self.tiling

    def _default_tiling_for_int8(self, context):
        if (self.tiling.get("AL0_matrix") == [0, 0, 32, 0, 0, 0, 0] and
            tbe_platform_info.intrinsic_check_support("Intrinsic_data_move_out2l1_nd2nz") and self.dtype_a == "int8"):
            self.tiling = self._get_static_cache_tiling(context)

    def _get_static_cache_tiling(self, context):
        """
        get cache_tiling result with static shape input
        """
        context.add_addition("static_cache_tiling", 1)
        context.add_addition("input_a_shape", shape_to_list(self.tensor_map.get("a_placehold").shape))
        context.add_addition("input_b_shape", shape_to_list(self.tensor_map.get("b_placehold").shape))
        context.add_addition("output_y_shape", shape_to_list(self.res.shape))

        m1, k1, m0, k0 = shape_to_list(self.tensor_map.get("a_l0a").shape)[NC_FACTOR_INDEX:]
        if tbe_platform.get_soc_spec("L0A_LAYOUT_IS_zN"):
            k1, m1, m0, k0 = shape_to_list(self.tensor_map.get("a_l0a").shape)[NC_FACTOR_INDEX:]
        _, n1, n0, _ = shape_to_list(self.tensor_map.get("b_l0b").shape)[NC_FACTOR_INDEX:]
        batch_shape = shape_to_list(self.tensor_map.get("c_l0c").shape)[:NC_FACTOR_INDEX]
        base_a_shape = [m1 * m0, k1 * k0]
        base_b_shape = [k1 * k0, n1 * n0]
        input_a_ori_shape = batch_shape + [base_a_shape[int(self._trans_a)], base_a_shape[1 - int(self._trans_a)]]
        input_b_ori_shape = batch_shape + [base_b_shape[int(self._trans_b)], base_b_shape[1 - int(self._trans_b)]]
        context.add_addition("input_a_ori_shape", input_a_ori_shape)
        context.add_addition("input_b_ori_shape", input_b_ori_shape)
        context.add_addition("output_y_ori_shape", [])

        context.add_addition("input_a_dtype", self.tensor_map.get("a_placehold").dtype)
        context.add_addition("input_b_dtype", self.tensor_map.get("b_placehold").dtype)
        context.add_addition("output_y_dtype", self.res.dtype)

        format_a, format_b = self.para_map.get("format_a"), self.para_map.get("format_b")
        a_placehold = self.tensor_map.get("a_placehold")
        b_placehold = self.tensor_map.get("b_placehold")
        if "src_format" in a_placehold.op.attrs:
            format_a = a_placehold.op.attrs["src_format"]
        if "src_format" in b_placehold.op.attrs:
            format_b = b_placehold.op.attrs["src_format"]
        context.add_addition("input_a_format", format_a)
        context.add_addition("input_b_format", format_b)
        context.add_addition("output_y_format", self.para_map.get("format_out"))

        context.add_addition("trans_a", bool(self._trans_a))
        context.add_addition("trans_b", bool(self._trans_b))

        bias = self.tensor_map.get("bias")
        if bias is not None:
            context.add_addition("bias_shape", shape_to_list(bias.shape))
            context.add_addition("bias_dtype", bias.dtype)
            context.add_addition("bias_format", self.para_map.get("format_out"))
            context.add_addition("bias_ori_shape", shape_to_list(bias.op.attrs["ori_shape"]))
        if self.tensor_map.get("fixpipe_input_name") is not None:
            fixpipe_op_dict = {
                "quant_scale_0": 0,
                "relu_weight_0": 0,
                "relu_weight_1": 0,
                "quant_scale_1": 0,
                "eltwise_src": 0
            }
            for fixpipe_input in self.tensor_map.get("fixpipe_input_name", []):
                if fixpipe_op_dict.get(fixpipe_input) is not None:
                    fixpipe_op_dict[fixpipe_input] = 1
            add_compile_info("fixpipe_op_dict", fixpipe_op_dict)
        op_type = "matmul"
        dynamic_mode = "dynamic_mkn"
        if ((len(self.tensor_map.get("a_placehold").op.attrs["ori_shape"]) > ND_LENGTH) and
            (not self.tensor_map.get("c_l0c").op.attrs["fc_flag"])):
            op_type = "batch_matmul"
            dynamic_mode = "dynamic_mknb"
        context.add_addition("binary_constant_type", op_type)
        # add_compile_info
        add_compile_info("binary_mode_flag", True)
        bias_flag = self.tiling_info_dict.get("bias_flag")
        nd_flag = format_a == "ND" and format_b == "ND"
        split_k_flag = bool(self.para_map.get("split_k"))
        zero_flag = bool(self.res.op.attrs.get("zero_flag", False))
        add_compile_info("binary_attrs", {"bias_flag": bias_flag,
                                          "nd_flag": nd_flag,
                                          "split_k_flag": split_k_flag,
                                          "zero_flag": zero_flag,
                                          "weight_nz": False})
        add_compile_info("sparse_4to2_flag", self.sparse_4to2_flag)
        add_compile_info("dynamic_mode", dynamic_mode)
        add_compile_info("block_dim", {"CORE_NUM": tbe_platform_info.get_soc_spec("CORE_NUM")})
        fused_double_operand_num = 0
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub"):
            fused_double_operand_num = self.tiling_info_dict.get("fused_double_operand_num")
        add_compile_info("fused_double_operand_num", fused_double_operand_num)
        tiling_case = transfer_tiling(context, self.sparse_4to2_flag)
        return tiling_case

    def _judge_format(self, is_quant):
        """
        judge format
        """
        format_a = self.para_map.get("format_a")
        format_b = self.para_map.get("format_b")
        a_placehold = self.tensor_map.get("a_placehold")
        b_placehold = self.tensor_map.get("b_placehold")
        if "src_format" in a_placehold.op.attrs or "src_format" in b_placehold.op.attrs:
            return False
        format_out = self.para_map.get("format_out")
        if format_a == "FRACTAL_NZ" and format_b == "FRACTAL_NZ" and format_out == "FRACTAL_NZ":
            return True
        elif format_a == "ND" and format_b == "ND" and format_out == "ND":
            return True
        if is_quant and format_a == "ND" and format_b == "ND" and format_out == "FRACTAL_NZ":
            return True
        return False

    def _unsupport_bias(self):
        """
        check bias unsupport binary const
        """
        if self.tensor_map.get("bias") is not None:
            format_out = self.para_map.get("format_out")
            shape_bias = self.tensor_map.get("bias").shape
            block_size = 16
            if list(shape_bias)[0] % block_size != 0 and format_out == "FRACTAL_NZ":
                return True
        return False

    def _support_binary_const(self):
        """
        check case support binary const or not
        """
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
            fixpipe_flag = self._get_fixpipe_flag()
            unsupport_bias_flag = self._unsupport_bias()
            if unsupport_bias_flag:
                return False
            if fixpipe_flag > 1 or tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub"):
                return False
            if self.tensor_map.get("bias") is not None:
                if self.tensor_map.get("bias").dtype not in ("float16", "float32"):
                    return False
            is_quant = self.dtype_a == "int8" and self.dtype_c == "int32"
            # check dtype is fp16 or fp32
            if (self.dtype_a in ("float16", "float32", "bfloat16") and
                self.dtype_b in ("float16", "float32", "bfloat16") and
                self.dtype_c in ("float16", "float32", "bfloat16")) or is_quant:
                # check format is NDND or NZNZ
                if self._judge_format(is_quant):
                    return True
        elif tbe_platform_info.get_soc_spec("pipev2_constraints") == "1":
            format_b = self.para_map.get("format_b")
            format_out = self.para_map.get("format_out")
            format_check = format_b == "FRACTAL_NZ" and format_out == "ND"
            if not self.para_map.get("have_batch") and format_check and not self.eletwise_fusion:
                return True
        return False

    def _update_info_dict(self, info_dict):
        """
        add fixpipe_flag and bias_dtype to info_dict and update input shape when inputs are float32
        """
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out"):
            if self.tensor_map.get("fixpipe_input_name") is not None:
                info_dict.update({"fixpipe_flag": self._get_fixpipe_flag()})
            elif self.para_map.get("format_out") != "FRACTAL_NZ":
                # the 0th bit of fixpipe_flag means is nz2nd or not
                info_dict.update({"fixpipe_flag": 1})
            if self.tensor_map.get("bias") is not None:
                tensor_bias = self.tensor_map.get("bias")
                info_dict.update({"bias_dtype": tensor_bias.dtype})
            # k_axis need align to 16 when k0 and m0(n0) need trans
            if self.dtype_a == "float32" and self.dtype_b == "float32" and (self._trans_a or not self._trans_b):
                # a_shape_dim: k1, m1, m0, k0
                info_dict["A_shape"][-4] = int_ceil_align(info_dict["A_shape"][-4], K_AXIS_ALIGN_FACTOR)
                # b_shape dim: K1*k0, n1, 1, 1, n0
                info_dict["B_shape"][0] = int_ceil_align(info_dict["B_shape"][0], tbe_platform.BLOCK_REDUCE)

    def _get_tiling_from_repository(self, current_tiling_type):
        """
        get tiling from repository use old and new fuse num
        """
        new_cub_fused_num = self.fuse_num_group[2]
        set_tiling_type("repository_tiling")
        old_tiling = tiling_api.get_tiling(self.tiling_info_dict)
        old_tiling_from_repo = False if self.sparse_4to2_flag else self._check_repository_tiling(old_tiling)
        self.tiling_info_dict["fused_double_operand_num"] = new_cub_fused_num
        new_tiling = tiling_api.get_tiling(self.tiling_info_dict)
        new_tiling_from_repo = self._check_repository_tiling(new_tiling)
        set_tiling_type(current_tiling_type)
        if self._support_binary_const() and tbe_platform_info.get_soc_spec("pipev2_constraints") == "1":
            format_a = self.para_map.get("format_a")
            format_b = self.para_map.get("format_b")
            format_out = self.para_map.get("format_out")
            if format_a == "FRACTAL_NZ" and format_b == "FRACTAL_NZ" and format_out == "ND":
                # -2 means m dim
                support_static = self.dtype_c == "float16"
                tensor_a = self.placeholder_tensors.get("tensor_a", None)
                if tensor_a is not None:
                    shape_a = tensor_a.op.attrs["ori_shape"]
                    m_dim = shape_a[-1] if self._trans_a else shape_a[-2]
                    support_static = support_static and m_dim % tbe_platform.BLOCK_IN == 0
                new_tiling_from_repo = new_tiling_from_repo and support_static
                old_tiling_from_repo = old_tiling_from_repo and support_static
        return [old_tiling, new_tiling, old_tiling_from_repo, new_tiling_from_repo]

    def _get_input_shape(self):
        """
        get the a_shape and b_shape, trans_flag
        """
        l0a_shape = [get_value(i) for i in self.tensor_map["a_l0a"].shape]
        l0b_shape = [get_value(i) for i in self.tensor_map["b_l0b"].shape]
        if self.para_map.get("mmad_mode") == "gemv":
            l0a_shape, l0b_shape = l0b_shape, l0a_shape

        if self._trans_a == self._trans_b and self.dtype_a == "float32" and self.dtype_b == "float32":
            # for some unaligned cases, shape_a=(2,4), shapeb_b=(4,16) for example, shape_a_l1 will be aligned as
            # (1,1,16,8) while shape_b_l1 is (2,1,16,8), the shapes on L0 are (1,1,16,8) and (2, 1, 16, 8), ka != kb
            l0a_shape[-3] = int_ceil_align(l0a_shape[-3], K_AXIS_ALIGN_FACTOR)
            l0b_shape[-4] = int_ceil_align(l0b_shape[-4], K_AXIS_ALIGN_FACTOR)
        block_reduce = tbe_platform.CUBE_MKN.get(self.tensor_map.get("a_l0a").dtype).get("mac")[1]
        # a_shape dim: batch_a, k1, m1, m0, k0
        a_shape = [1, l0a_shape[-3], l0a_shape[-4], tbe_platform.BLOCK_IN, block_reduce]
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            a_shape = [1, l0a_shape[-4], l0a_shape[-3], tbe_platform.BLOCK_IN, block_reduce]
        a_shape[0] = l0a_shape[0] if len(l0a_shape) == BMM_LEN_NZ else 1
        # b_shape dim: K1*k0, n1, 1, 1, n0
        b_shape = [2 * l0b_shape[-4] * block_reduce, l0b_shape[-3], 1, 1, tbe_platform.BLOCK_OUT] \
                  if self.sparse_4to2_flag else \
                  [l0b_shape[-4] * block_reduce, l0b_shape[-3], 1, 1, tbe_platform.BLOCK_OUT]
        # 2 is the index of m1
        fix_value = get_al1_m_fix_value(a_shape[2])
        a_shape[2] //= fix_value
        # when int82fp32, input is trans from int8 to fp16,
        if self._ops_data_flow_mode == "int82fp32":
            a_shape[1] = a_shape[1] // 2
            a_shape[1] = a_shape[1] if a_shape[1] != 0 else 1
            a_shape[4] *= 2
        return a_shape, b_shape

    def _get_trans_flag(self):
        """
        trans flag by trans_a and trans_b
        """
        trans_flag = 1
        if self._trans_a:
            trans_flag += 1
        if self._trans_b:
            trans_flag += 2
        return trans_flag

    def _get_op_type_flag(self):
        """
        get the op dtype flag
        """
        format_a = self.para_map.get("format_a")
        format_b = self.para_map.get("format_b")
        if self.para_map.get("mmad_mode")  == "gemv":
            format_a, format_b = format_b, format_a
        if format_a == "ND" and format_b == "ND":
            op_type_flag = 0
        elif format_a != "ND" and format_b == "ND":
            op_type_flag = 2
        elif format_a == "ND" and format_b != "ND":
            op_type_flag = 3
        else:
            op_type_flag = 1

        return op_type_flag

    def _get_batch_flag(self):
        batch_broadcase_flag = int(self.para_map.get("batch_broadcast_flag", False))
        have_batch_a = int(self.para_map.get("have_batch_a", False))
        have_batch_b = int(self.para_map.get("have_batch_b", False))
        batch_dtype = batch_broadcase_flag*4 + have_batch_a*2 + have_batch_b
        return batch_dtype

    def _get_general_fuse_num(self):
        """
        get a_ub_fuse_num, b_ub_fuse_num, and the old c_ub_fuse_num
        """
        if in_dynamic():
            a_ub_fuse_num = b_ub_fuse_num = c_ub_fuse_num = 0
            if self.para_map.get("format_a") == "ND":
                a_ub_fuse_num = 10
            if self.para_map.get("format_b") == "ND":
                b_ub_fuse_num = 10
            if self.para_map.get("format_out") == "ND":
                c_ub_fuse_num = 1
            return a_ub_fuse_num, b_ub_fuse_num, c_ub_fuse_num
        # the 10 is base number for aub or bub fuse_num
        a_ub_fuse_num = int(self._get_ub_fuse_num("aub") * 10)
        b_ub_fuse_num = int(self._get_ub_fuse_num("bub") * 10)
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd"):
            c_ub_fused_num = self._cal_old_cub_fuse_num()
        elif self.res == self.matmul_end_tensor:
            c_ub_fused_num = int(self._get_ub_fuse_num())
        else:
            c_ub = self.tensor_map.get("c_ub_fract")
            if self.tensor_map.get("cast_to_fp16") is not None:
                c_ub = self.tensor_map.get("cast_to_fp16")
            nz_to_nd = self.tensor_map.get("nz_to_nd")
            # 2 means double
            ub_byte = int(DTYPE_WIDTH_MAP.get(c_ub.dtype) * 2) if c_ub is not None else \
                      int(DTYPE_WIDTH_MAP.get(self.res.dtype) * 2)
            if nz_to_nd is not None:
                ub_byte = ub_byte * 2
            new_ub_byte = self._update_ub_byte(ub_byte)
            if ub_byte < new_ub_byte:
                ub_byte = new_ub_byte
            c_ub_fused_num = ub_byte // DTYPE_WIDTH_MAP.get(self.res.dtype) - 1
            for tensor_mem in self.compute_tensors.values():
                if "dropout_broadcast" in tensor_mem.op.tag:
                    c_ub_fused_num = 2.5
        return a_ub_fuse_num, b_ub_fuse_num, c_ub_fused_num

    def _get_ub_fuse_num(self, para_num="cub"):
        """
        get fuse num for ub tensor
        """
        ub_fuse_num = 0
        if para_num == "aub":
            tensor_list = ["a_ub", "a_int82fp16", "a_ub_fract", "a_transpose"]
            base_tensor = self.tensor_map.get("a_ub")
        elif para_num == "bub":
            tensor_list = ["b_ub", "b_int82fp16", "b_ub_fract", "b_transpose"]
            base_tensor = self.tensor_map.get("b_ub")
        else:
            if self.tensor_map.get("c_add_bias_ub") is not None:
                tensor_list = ["beta_bias", "c_ub_fract", "cast_to_fp16", "nz_to_nd", "before_c_gm"]
                if self._ops_data_flow_mode == "fp162fp16":
                    tensor_list.append("bias_ub")
            else:
                tensor_list = ["c_ub_fract", "cast_to_fp16", "nz_to_nd", "before_c_gm", "fixpipe_matmul_ub"]
                # inline the c_ub_fract
                if self.para_map.get("c_ub_fract_inline", False):
                    tensor_list = ["cast_to_fp16", "nz_to_nd", "before_c_gm"]
            base_tensor = self.res
        if base_tensor is not None:
            for ub_tensor_name in tensor_list:
                ub_tensor = self.tensor_map.get(ub_tensor_name)
                if ub_tensor is not None:
                    ub_fuse_num += DTYPE_WIDTH_MAP.get(ub_tensor.dtype)
            ub_fuse_num = ub_fuse_num / DTYPE_WIDTH_MAP.get(base_tensor.dtype) - 1
        return max(ub_fuse_num, 0)

    def _update_ub_byte(self, old_ub_byte):
        """
        update ub byte upon on the fusion tensor
        """
        # dequant_eletwise_fusion means some eletwise after dequant
        new_ub_byte = old_ub_byte
        if self.dequant_eletwise_fusion or self.eletwise_fusion:
            length = self._get_fusion_tensor_num()
            res_byte = DTYPE_WIDTH_MAP.get(self.res.dtype)
            if new_ub_byte < length * res_byte:
                new_ub_byte = length * res_byte
        return new_ub_byte

    def _get_fusion_flag(self):
        """
        cal the fusion flag
        """
        real_res = self.res.op.input_tensors[0] if "reduce_sum" in self.res.op.tag else self.res
        self.eletwise_fusion = not self.quantfy_fusion and real_res != self.matmul_end_tensor

        if self.para_map.get("dequant_fusion", False):
            self.dequant_eletwise_fusion = self.tensor_map.get("dequant_relu") is not None
            if real_res == self.tensor_map.get("quant_tensor"):
                # some eletwise bettween dequant and quant
                quant_input_tensor = self.tensor_map.get("input_ub").op.input_tensors[0]
                self.dequant_eletwise_fusion = self.dequant_eletwise_fusion or \
                    "dequant" not in quant_input_tensor.op.name
            else:
                # some eletwise after dequant
                self.dequant_eletwise_fusion = self.dequant_eletwise_fusion or "dequant" not in real_res.op.name

    def _get_fusion_tensor_num(self):
        """
        get teh fusion tensor numbers for fusion ub num
        """
        stack = [self.res]
        width = len(stack)
        visited_list = []
        tmp_stack = stack
        while tmp_stack:
            for tens in tmp_stack:
                if "broadcast" in tens.op.tag:
                    stack.remove(tens)
                    continue
                if tens in self.output_tensors:
                    width, visited_list, stack = self._calc_width_mid(tens, visited_list, stack, width)
                else:
                    width, visited_list, stack = self._calc_width_tail(tens, visited_list, stack, width)
            tmp_stack = []
            for ele in stack:
                tmp_stack.append(ele)
        return width

    def _calc_width_mid(self, tens, visited_list, stack, width):
        """
        cal the fusion tensor width in mid
        """
        all_out = True
        for out_ten in self.output_tensors.get(tens):
            if out_ten not in visited_list:
                all_out = False
        if all_out and (tens not in visited_list):
            visited_list.append(tens)
            stack.remove(tens)
            # the shape of deq_scale is very small
            if tens.op.tag not in ("dequant", "dequant_scale", "dequant_sqrt"):
                for in_ten in tens.op.input_tensors:
                    if in_ten not in stack and in_ten != self.matmul_end_tensor.op.input_tensors[0]:
                        stack.append(in_ten)
            else:
                stack.append(tens.op.input_tensors[0])
            width_local = 0
            cast_flag = False
            for ele in stack:
                width_local = width_local + DTYPE_WIDTH_MAP.get(ele.dtype)
                if DTYPE_WIDTH_MAP.get(ele.dtype) == 2:
                    cast_flag = True
            if width_local == 2 and cast_flag:
                width_local = 3
            if width_local > width:
                width = width_local
        return width, visited_list, stack

    def _calc_width_tail(self, tens, visited_list, stack, width):
        """
        cal the fusion tensor width in the tail
        """
        visited_list.append(tens)
        stack.remove(tens)
        for in_ten in tens.op.input_tensors:
            if in_ten not in stack and in_ten != self.matmul_end_tensor.op.input_tensors[0]:
                stack.append(in_ten)
        width_local = 0
        cast_flag = False
        for ele in stack:
            width_local = width_local + DTYPE_WIDTH_MAP.get(ele.dtype)
            if DTYPE_WIDTH_MAP.get(ele.dtype) == 2:
                cast_flag = True
        if width_local == 2 and cast_flag:
            width_local = 3
        if width_local > width:
            width = width_local
        return width, visited_list, stack

    def _cal_new_cub_fuse_num(self, old_fused_num):
        """
        cal the new cub fuse num
        """
        new_fused_num = old_fused_num
        scalar_size = 0
        compute_inline_list = self._get_inline_tensor()
        if (not in_dynamic() and not (tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
            and not tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2ub"))):
            not_count_list = []
            for tensor_item in compute_inline_list:
                if tensor_item not in self.placeholder_tensors:
                    not_count_list.append(tensor_item)
            new_fused_num = 0.0
            for out_tensor in self.res_list:
                multi_ub = CalculateMultiUB(self.tensor_map.get("c_ub_fract"), out_tensor, not_count_list)
                ub_res, scalar_size = multi_ub.calculate_start()
                new_fused_num = max(new_fused_num, ub_res / (DTYPE_WIDTH_MAP.get(out_tensor.dtype) * 2) - 1)
        return new_fused_num, scalar_size

    def _cal_old_cub_fuse_num(self):
        """
        cal the old cub_num
        """
        fuse_num = 0
        res_data_size = DATA_SIZE.get(self.res.dtype, 1)
        ub_data_size = res_data_size
        if self.tensor_map.get("eltwise_tensor"):
            fuse_num += 1
            for ub_eltwise_mem in self.tensor_map["eltwise_tensor"]:
                ub_data_size = max(ub_data_size, DATA_SIZE.get(ub_eltwise_mem, 1))
        if self.tensor_map.get("eltwise_input_tensor"):
            fuse_num += 1
            for ub_eltwise_input_mem in self.tensor_map["eltwise_input_tensor"]:
                ub_data_size = max(ub_data_size, DATA_SIZE.get(ub_eltwise_input_mem, 1))
        fuse_num *= int_ceil_div(ub_data_size, res_data_size)

        return fuse_num

    def _get_inline_tensor(self):
        compute_inline_list = []
        c_ub_fract = self.tensor_map.get("c_ub_fract")
        if self.quantfy_fusion or self.para_map.get("c_ub_fract_inline", False):
            compute_inline_list.append(c_ub_fract)
        if self.eletwise_fusion:
            for compute_tensor in self.compute_tensors.values():
                if ("broadcast" in compute_tensor.op.tag and
                    (not compute_tensor.op.attrs or 'broadcast_flag' not in compute_tensor.op.attrs)):
                    compute_inline_list.append(compute_tensor)
        if not self.para_map.get("multi_out", False) and self.matmul_end_tensor != self.res:
            compute_inline_list.append(self.matmul_end_tensor)
        for inline_tensor in compute_inline_list:
            if inline_tensor in self.placeholder_tensors.values():
                compute_inline_list.remove(inline_tensor)
        return compute_inline_list

    def _get_stride_w(self):
        """
        get the para for stride_w
        """
        format_out = self.para_map.get("format_out")
        format_a = self.para_map.get("format_a")
        split_k = int(self.para_map.get("split_k"))
        n_shape, n1_shape = self._cal_shape_n()
        n_shape_dynamic_flag = in_dynamic() and isinstance(n1_shape, (tvm.Var, tvm.tir.PrimExpr))
        if format_a == "ND" and format_out != "FRACTAL_NZ":
            if n_shape_dynamic_flag and format_out == "ND":
                stride_w = 0
            else:
                divide_factor = 32 if self._ops_data_flow_mode in ("int82int32", "int82fp32") else 16
                if 1 <= int(n_shape) <= divide_factor or int(n_shape) % divide_factor == 0:
                    stride_w = 1
                else:
                    stride_w = 0
        else:
            stride_w = 1
        stride_w |= split_k << 1
        return stride_w

    def _cal_shape_n(self):
        """
        cal the n shape
        """
        if not in_dynamic():
            n1_shape = self.tensor_map.get("b_l0b").shape[-3]
            n_shape = n1_shape * self.tensor_map.get("b_l0b").shape[-2]
        else:
            tensor_b = self.tensor_map.get("b_placehold")
            aligned_coeff = 1 if len(tensor_b.shape) == BMM_LEN_ND else tbe_platform.BLOCK_OUT
            batch_idx_offset = 1 if len(tensor_b.shape) in (BMM_LEN_ND, BMM_LEN_NZ) else 0
            n_dim = (batch_idx_offset + 1) if self._trans_b else batch_idx_offset
            n1_shape = tensor_b.shape[n_dim]
            n_shape = aligned_coeff * n1_shape
        return n_shape, n1_shape


def auto_tiling(fuse_num_group, tensor_map, para_map):
    """
    cal the tiling with formula calculator
    """
    a_l0a = tensor_map.get("a_l0a")
    b_l0b = tensor_map.get("b_l0b")
    c_l0c = tensor_map.get("c_l0c")
    tensor_a = tensor_map.get("a_placehold")
    l1_fusion_type = -1
    if "L1_fusion_type" in tensor_a.op.attrs:
        l1_fusion_type = tensor_a.op.attrs["L1_fusion_type"].value
    bytes_info = _get_bytes_info(fuse_num_group, tensor_map)
    block_reduce = tbe_platform.BLOCK_REDUCE_INT8 if para_map.get("ops_data_flow_mode") == "int82int32" else \
                   tbe_platform.BLOCK_REDUCE
    mkn_shape = (a_l0a.shape[-4].value * tbe_platform.BLOCK_IN,
                 a_l0a.shape[-3].value * block_reduce,
                 b_l0b.shape[-3].value * tbe_platform.BLOCK_OUT)
    schedule_info_dict = {}
    schedule_info_dict["l1_fusion_type"] = l1_fusion_type
    schedule_info_dict["dequant_fusion"] = para_map.get("dequant_fusion")
    schedule_info_dict["date_transfer_fusion"] = para_map.get("quant_fusion", False) or \
                                                 para_map.get("requant_fusion", False)
    schedule_info_dict["mmad_mode"] = para_map.get("mmad_mode")
    schedule_info_dict["is_b_nz"] = para_map.get("format_b") == "FRACTAL_NZ"
    schedule_info_dict["block_in"] = tbe_platform.BLOCK_IN
    schedule_info_dict["block_out"] = tbe_platform.BLOCK_OUT
    schedule_info_dict["block_reduce"] = block_reduce
    schedule_info_dict["b_trans"] = para_map.get("trans_b")
    if para_map.get("custom_block_dim") is not None:
        schedule_info_dict["custom_block_dim"] = para_map.get("custom_block_dim")

    batch_shape = get_value(c_l0c.shape[0]) if len(c_l0c.shape) == BMM_LEN_NZ else 0
    compute_tiling = ComputeTiling(bytes_info, mkn_shape, schedule_info_dict, batch_shape)
    return compute_tiling.compute_tiling_enter()


def _get_bytes_info(fuse_num_group, tensor_map):
    """
    get the dtype of ub, l1 and l0
    """
    aub_num, bub_num, cub_num = fuse_num_group
    ub_fused_num_multi = 10
    double_multi = 2
    a_ub_byte, b_ub_byte = 0, 0
    if tensor_map.get("a_ub") is not None:
        a_ub_byte = ((aub_num // ub_fused_num_multi + 1)
                * int(DTYPE_WIDTH_MAP.get(tensor_map.get("a_placehold").dtype) * double_multi))
    if tensor_map.get("b_ub") is not None:
        b_ub_byte = ((bub_num // ub_fused_num_multi + 1)
                * int(DTYPE_WIDTH_MAP.get(tensor_map.get("b_placehold").dtype) * double_multi))
    ub_res_byte = (cub_num + 1) * int(DTYPE_WIDTH_MAP.get(tensor_map.get("c_gm").dtype) * double_multi)
    l1a_byte = int(DTYPE_WIDTH_MAP.get(tensor_map.get("a_l1").dtype) * double_multi)
    l1b_byte = int(DTYPE_WIDTH_MAP.get(tensor_map.get("b_l1").dtype) * double_multi)
    l0a_byte = int(DTYPE_WIDTH_MAP.get(tensor_map.get("a_l0a").dtype) * double_multi)
    l0b_byte = int(DTYPE_WIDTH_MAP.get(tensor_map.get("b_l0b").dtype) * double_multi)
    l0c_byte = int(DTYPE_WIDTH_MAP.get(tensor_map.get("c_l0c").dtype) * double_multi)
    return [a_ub_byte, b_ub_byte, ub_res_byte, l1a_byte, l1b_byte, l0a_byte, l0b_byte, l0c_byte]


def get_tiling_key(context):
    """
    get static tiling key
    """
    support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
    tiling_data = []
    tiling_key = []
    if context.get_addition("is_binary_constant") or context.get_addition("static_cache_tiling"):
        debug_msg = "Enter binary constant" if context.get_addition("is_binary_constant") else "Static CacheTiling"
        log.debug(debug_msg)
        enable_pad = context.get_addition("enable_pad")
        enable_nz_fusion = context.get_addition("enable_nz_fusion")
        add_compile_info("enable_pad", enable_pad if enable_pad else 0)
        add_compile_info("enable_nz_fusion", True if enable_nz_fusion else False)
        inputs = []
        outputs = []
        inputs.append({"shape": context.get_addition("input_a_shape"),
                        "ori_shape": context.get_addition("input_a_ori_shape"),
                        "dtype": context.get_addition("input_a_dtype"),
                        "format": context.get_addition("input_a_format")})
        inputs.append({"shape": context.get_addition("input_b_shape"),
                        "ori_shape": context.get_addition("input_b_ori_shape"),
                        "dtype": context.get_addition("input_b_dtype"),
                        "format": context.get_addition("input_b_format")})
        if context.get_addition("bias_shape"):
            inputs.append({"shape": context.get_addition("bias_shape"),
                            "ori_shape": context.get_addition("bias_shape"),
                            "dtype": context.get_addition("bias_dtype"),
                            "format": context.get_addition("bias_format")})
        outputs.append({"shape": context.get_addition("output_y_shape"),
                        "ori_shape": context.get_addition("output_y_ori_shape"),
                        "dtype": context.get_addition("output_y_dtype"),
                        "format": context.get_addition("output_y_format")})
        tiling_format = []
        if context.get_addition("binary_constant_type") == "batch_matmul_fixpipe":
            inputs, outputs, attrs = context.get_addition("op_tiling_params")
            run_info = do_op_tiling("BatchMatmulFixpipe", get_compile_info(), inputs, outputs, None, None, attrs)
            tiling_format += [
                "k_ori", "m_ori", "n_ori", "k", "m", "n", "batch", "batch_a1", "batch_a2", "batch_a3", "batch_a4",
                "batch_b1", "batch_b2", "batch_b3", "batch_b4", "batch_c1", "batch_c2", "batch_c3", "batch_c4",
                "batch_single_core", "m_single_core", "n_single_core", "batch_dim", "n_dim", "m_dim", "k_dim", "m_al1",
                "n_bl1", "cub_n1", "m_l0", "k_l0", "n_ub_l0_time", "kal0_factor", "kbl0_factor", "kal1_factor",
                "kbl1_factor", "kal1_16", "kbl1_16", "kl1_times", "batch_l1_factor", "batch_ub_l0_time", "batch_cub",
                "out_branch_flag", "bias_flag", "hf32_flag", "datatype_bf16", "al1_db", "bl1_db", "l0c_db",
                "l2_cache_flag", "close_k_shift"
            ]
        elif context.get_addition("binary_constant_type") == "matmul":
            attrs = ({"name": "transpose_x1", "dtype": "bool", "value": context.get_addition("trans_a")},
                     {"name": "transpose_x2", "dtype": "bool", "value": context.get_addition("trans_b")},
                     {"name": "offset_x", "dtype": "int64", "value": 0},
                     {"name": "input_size", "dtype": "int64", "value": -1},
                     {"name": "hidden_size", "dtype": "int64", "value": -1},
                     {"name": "_op_impl_mode_enum", "dtype": "int", "value": get_op_impl_mode_enum("MatMul")},
                     {"name": "enable_pad", "dtype": "int", "value": 0})
            run_info = do_op_tiling("MatMulV2", get_compile_info(), inputs, outputs, None, None, attrs)
            tiling_format += ["k_ori", "m_ori", "n_ori", "k", "m", "n", "batch_single_core", "m_single_core",
                                "n_single_core", "batch_dim", "n_dim", "m_dim", "k_dim", "m_al1", "n_bl1", "cub_n1",
                                "m_l0", "k_l0", "n_ub_l0_time", "kal0_factor", "kbl0_factor", "kal1_factor",
                                "kbl1_factor", "kal1_16", "kbl1_16", "kl1_times", "batch_l1_factor",
                                "batch_ub_l0_time", "batch_cub", "out_branch_flag",
                                "bias_flag", "hf32_flag", "datatype_bf16", "al1_db", "bl1_db", "l0c_db",
                                "l2_cache_flag", "close_k_shift"]
            if context.get_addition('input_a_format') == "ND" and not support_l0c2out:
                tiling_format += ["m_aub", "n_bub", "k_aub", "k_bub", "batch_aub", "batch_bub", "multi_n_ub_l1",
                                    "multi_m_ub_l1", "multi_k_aub_l1", "multi_k_bub_l1", "multi_batch_aub_l1",
                                    "multi_batch_bub_l1", "a_align_value", "b_align_value", "aub_align_bound",
                                    "bub_align_bound"]
        elif context.get_addition("binary_constant_type") == "weight_quant_batchmatmul":
            attrs = ({"name": "adj_x1", "dtype": "bool", "value": context.get_addition("trans_a")},
                     {"name": "adj_x2", "dtype": "bool", "value": context.get_addition("trans_b")},
                     {"name": "_op_impl_mode_enum", "dtype": "int", "value": get_op_impl_mode_enum("MatMul")},
                     {"name": "enable_pad", "dtype": "int", "value": 0})
            run_info = do_op_tiling("WeightQuantBatchmatmul", get_compile_info(), inputs, outputs, None, None, attrs)
            tiling_format += ["k_ori", "m_ori", "n_ori", "k", "m", "n", "batch", "batch_a1", "batch_a2", "batch_a3",
                                "batch_a4", "batch_b1", "batch_b2", "batch_b3", "batch_b4", "batch_c1", "batch_c2",
                                "batch_c3", "batch_c4", "batch_single_core", "m_single_core", "n_single_core",
                                "batch_dim", "n_dim", "m_dim", "k_dim", "m_al1", "n_bl1", "cub_n1", "m_l0", "k_l0",
                                "n_ub_l0_time", "kal0_factor", "kbl0_factor", "kal1_factor", "kbl1_factor", "kal1_16",
                                "kbl1_16", "kl1_times", "batch_l1_factor", "batch_ub_l0_time", "batch_cub",
                                "out_branch_flag", "bias_flag", "hf32_flag", "datatype_bf16",
                                "al1_db", "bl1_db", "l0c_db", "l2_cache_flag", "close_k_shift"]
        else:
            attrs = ({"name": "adj_x1", "dtype": "bool", "value": context.get_addition("trans_a")},
                     {"name": "adj_x2", "dtype": "bool", "value": context.get_addition("trans_b")},
                     {"name": "offset_x", "dtype": "int64", "value": 0},
                     {"name": "_op_impl_mode_enum", "dtype": "int", "value": get_op_impl_mode_enum("MatMul")},
                     {"name": "enable_pad", "dtype": "int", "value": 0})
            run_info = do_op_tiling("BatchMatMulV2", get_compile_info(), inputs, outputs, None, None, attrs)
            tiling_format += ["k_ori", "m_ori", "n_ori", "k", "m", "n", "batch", "batch_a1", "batch_a2", "batch_a3",
                                "batch_a4", "batch_b1", "batch_b2", "batch_b3", "batch_b4", "batch_c1", "batch_c2",
                                "batch_c3", "batch_c4", "batch_single_core", "m_single_core", "n_single_core",
                                "batch_dim", "n_dim", "m_dim", "k_dim", "m_al1", "n_bl1", "cub_n1", "m_l0", "k_l0",
                                "n_ub_l0_time", "kal0_factor", "kbl0_factor", "kal1_factor", "kbl1_factor", "kal1_16",
                                "kbl1_16", "kl1_times", "batch_l1_factor", "batch_ub_l0_time", "batch_cub",
                                "out_branch_flag", "bias_flag", "hf32_flag", "datatype_bf16",
                                "al1_db", "bl1_db", "l0c_db", "l2_cache_flag", "close_k_shift"]
        if enable_pad:
            tiling_format += ["m_aub", "n_bub", "k_aub", "k_bub", "aub_dim", "bub_dim"]
            tiling_format = tiling_format[:6] + ["k_pad", "m_pad", "n_pad"] + tiling_format[6:]
        if enable_nz_fusion:
            tiling_format += [
                "m1_aub", "n1_bub", "k1_aub", "k1_bub", "m_aub_dim", "n_bub_dim", "k_aub_dim", "k_bub_dim"]
        tiling_format_dict = {name: "int" for name in tiling_format}
        tiling_data = decode(run_info.get("tiling_data"), tiling_format_dict)
        tiling_key = run_info.get("tiling_key")
        workspaces = run_info.get("workspaces", [])
        for idx, workspace in enumerate(workspaces):
            context.add_workspace(f"workspace_{idx}", size=workspace)

    return tiling_data, tiling_key


def _assembly_tiling(tiling_factors, double_buffers, block_sizes, sparse_4to2_flag=False):
    batch_factor, m_factor, n_factor, k_factor, m_l1_tile, k_al1_tile, k_bl1_tile, n_l1_tile, \
        m_l0_tile, k_l0_tile, n_l0_tile, cub_n1_tile, m_aub_tile, n_bub_tile, overhead_opti_flags = tiling_factors
    aub_pb, bub_pb, al1_pb, bl1_pb, al0_pb, bl0_pb, cl0_pb, cub_pb = double_buffers
    block_reduce, block_in, block_out = block_sizes
    aub_shape = [k_al1_tile * block_reduce, m_aub_tile, 1, 1] if m_aub_tile > 0 else None
    bub_shape = [k_bl1_tile * block_reduce, n_bub_tile, 1, 1] if n_bub_tile > 0 else None
    al1_shape = [k_al1_tile * block_reduce, m_l1_tile // m_l0_tile, 1, 1] if m_l1_tile > 0 else []
    bl1_shape = [k_bl1_tile * block_reduce, n_l1_tile // n_l0_tile, 1, 1] if n_l1_tile > 0 else []
    # in sparse_4to2 condition, ka must be a multiple of 2, cachetiling has been making k0 must be 2*x or 1,
    # so if ka is 1, change ka to 2.
    k_l0a_tile = 2 if (sparse_4to2_flag and k_l0_tile % 2 != 0) else k_l0_tile
    k_l0b_tile = (k_l0_tile // 2) if (sparse_4to2_flag and k_l0_tile != 1) else k_l0_tile
    tiling = {
        'AUB_shape': aub_shape,
        'BUB_shape': bub_shape,
        'AL1_shape': al1_shape,
        'BL1_shape': bl1_shape,
        'AL0_matrix': [m_l0_tile, k_l0a_tile, block_in, block_reduce, 1, 1],
        'BL0_matrix': [k_l0b_tile, n_l0_tile, block_out, block_reduce, 1, 1],
        'CL0_matrix': [n_l0_tile, m_l0_tile, block_in, block_out, 1, 1],
        'CUB_matrix': [cub_n1_tile, m_l0_tile, block_in, block_out, 1, 1],
        'block_dim': [batch_factor, n_factor, m_factor, k_factor],
        'n_bef_batch_flag': 0,
        'n_bef_group_flag': 0,
        'batch_bef_group_flag': 0,
        'A_overhead_opt_flag': overhead_opti_flags[0],
        'B_overhead_opt_flag': overhead_opti_flags[1],
        'AUB_channel_wise_flag': None,
        'BUB_channel_wise_flag': None,
        'CUB_channel_wise_flag': 0,
        'manual_pingpong_buffer':
        {'AUB_pbuffer': aub_pb,
            'BUB_pbuffer': bub_pb,
            'AL1_pbuffer': al1_pb,
            'BL1_pbuffer': bl1_pb,
            'AL0_pbuffer': al0_pb,
            'BL0_pbuffer': bl0_pb,
            'CL0_pbuffer': cl0_pb,
            'CUB_pbuffer': cub_pb,
            'UBG_pbuffer': 1,
            'INPUT_L1_BT_pbuffer': 1,
            'INPUT_L1_FB_pbuffer': 1,
            'INPUT_L1_eltwise_pbuffer': 1
        },
        'special_optimize_flag': 0,
        'tbe_compile_para': 0,
        'vector_block_num': 0,
        'binary_flag': 0,
        'INPUT_L1_BT_param': 'all',
        'INPUT_L1_FB_param': None,
        'INPUT_L1_eltwise_param': None,
        'INPUT_L1_sparse_index': None
    }
    return tiling


def transfer_tiling(context, sparse_4to2_flag=False, op_type="matmul"):
    """
    transfer tiling_data to tiling_case
    """
    tiling_data, tiling_key = get_tiling_key(context)
    if not tiling_data:
        return None
    block_reduce = int(UNIT_LEN_32 // DATA_SIZE.get(context.get_addition("input_a_dtype")))
    # get tiling value from tiling_data
    batch_dim = tiling_data.get("batch_dim")
    n_dim = tiling_data.get("n_dim")
    m_dim = tiling_data.get("m_dim")
    k_dim = tiling_data.get("k_dim")
    k_al1 = tiling_data.get("kal1_16")
    k_bl1 = tiling_data.get("kbl1_16")
    m_al1 = tiling_data.get("m_al1")
    n_bl1 = tiling_data.get("n_bl1")
    m_l0 = tiling_data.get("m_l0")
    k_l0 = tiling_data.get("k_l0")
    cub_n1 = tiling_data.get("cub_n1")
    n_l0 = cub_n1 * tiling_data.get("n_ub_l0_time")
    # get pb_flag from tiling_key
    if op_type == "weight_quant_batchmatmul":
        al1_pb = (tiling_key & 1) + 1
        tiling_key = tiling_key >> 1
        bl1_pb = (tiling_key & 1) + 1
        tiling_key = tiling_key >> 1
    else:
        al1_pb = tiling_data.get("al1_db") + 1
        bl1_pb = tiling_data.get("bl1_db") + 1
    l0c_pb = (tiling_key & 1) + 1
    tiling_key = tiling_key >> 3
    al1_attach_flag = (tiling_key & 3)
    tiling_key = tiling_key >> 2
    bl1_attach_flag = (tiling_key & 3)
    trans_a = context.get_addition("trans_a")
    trans_b = context.get_addition("trans_b")
    input_a_dtype = context.get_addition("input_a_dtype")
    input_b_dtype = context.get_addition("input_b_dtype")
    # full_load may cause nbl1 is split by nparts and offset is 16B align not 32B align
    x1_no_fullload = True if trans_a == True and input_a_dtype == "int8" else False
    x2_no_fullload = True if trans_b == False and input_b_dtype == "int8" else False
    m_al1 = m_al1 if (al1_attach_flag > 0 or op_type == "weight_quant_batchmatmul" or x1_no_fullload) else 0
    n_bl1 = n_bl1 if (bl1_attach_flag > 0 or op_type == "weight_quant_batchmatmul" or x2_no_fullload) else 0
    # in fp32 dtpe, block_result=8; when k is outer axis, k_al1=1/k_bl1==1 may result in emit_insn error
    if context.get_addition("input_a_dtype") == "float32":
        k_al1 += int(k_al1 == 1 and context.get_addition("trans_a"))
        k_bl1 += int(k_bl1 == 1 and not context.get_addition("trans_b"))
    # construct tiling_case
    tiling_factors = [batch_dim, m_dim, n_dim, k_dim,
                        m_al1 * m_l0, k_al1, k_bl1, n_bl1 * n_l0,
                        m_l0, k_l0, n_l0, cub_n1, 0, 0, [0, 0]]
    double_buffers = [1, 1, al1_pb, bl1_pb, utils.DB_ON, utils.DB_ON, l0c_pb, 1]
    block_sizes = [block_reduce, UNIT_LEN, UNIT_LEN]
    tiling_case = _assembly_tiling(tiling_factors, double_buffers, block_sizes, sparse_4to2_flag)
    return tiling_case


class ComputeTiling:
    """
    get tiling by compute
    """
    DOUBLE_VALUE = 2
    CORE_NUM_THRITY = 30
    CORE_NUM_THRITY_TWO = 32
    CORE_NUM_EIGHT = 8
    MKN_M_INDEX = 0
    MKN_K_INDEX = 1
    MKN_N_INDEX = 2

    def __init__(self, bytes_info, mkn_shape, schedule_info_dict, batch_shape=0):
        self.mkn_shape = mkn_shape
        self.batch_shape = batch_shape
        self.bytes_info = bytes_info
        self.l1_fusion_type = schedule_info_dict.get("l1_fusion_type")
        self.dequant_fusion = schedule_info_dict.get("dequant_fusion")
        self.date_transfer_fusion = schedule_info_dict.get("date_transfer_fusion")
        self.mmad_mode = schedule_info_dict.get("mmad_mode")
        self.is_b_nz = schedule_info_dict.get("is_b_nz")
        self.block_in = schedule_info_dict.get("block_in")
        self.block_out = schedule_info_dict.get("block_out")
        self.block_reduce = schedule_info_dict.get("block_reduce")
        self.b_trans = schedule_info_dict.get("b_trans")
        self.custom_block_dim = schedule_info_dict.get("custom_block_dim", [])

    @staticmethod
    def get_shape_map():
        """
        the knowledge of matmul schedule tiling
        """
        shape_map = {(1664, 4096, 1024, -1, 2): "176_320_176_176_80_176_2_2",
                    (1664, 4096, 1024, -1, 4): "176_320_176_176_80_176_2_2",
                    (1664, 1024, 4096, -1, 2): "240_512_128_240_64_128_2_2",
                    (1664, 1024, 4096, -1, 8): "240_512_64_240_64_64_2_2",
                    (1664, 16, 1024, -1, 2): "832_16_128_832_16_32_1_2",
                    (1664, 1024, 1024, -1, 2): "240_512_128_240_64_128_2_2",
                    (1664, 1024, 1024, -1, 4): "240_512_128_240_64_128_2_2",
                    (832, 4096, 1024, -1, 2): "176_320_176_176_80_176_2_2",
                    (832, 4096, 1024, -1, 4): "176_320_176_176_80_176_2_2",
                    (832, 1024, 4096, -1, 2): "240_512_128_240_64_128_2_2",
                    (832, 1024, 4096, -1, 8): "240_512_64_240_64_64_2_2",
                    (832, 1024, 1024, -1, 2): "240_512_128_240_64_128_2_2",
                    (832, 1024, 1024, -1, 4): "240_512_128_240_64_128_2_2",
                    (832, 16, 1024, -1, 2): "832_16_128_832_16_32_1_2",
                    (1280, 16, 768, -1, 2): "640_16_192_640_16_48_1_2",
                    (1280, 768, 768, -1, 2): "336_384_96_336_48_96_2_2",
                    (320, 64, 320, -1, 2): "320_64_192_320_48_96_1_2",
                    (1280, 768, 3072, -1, 2): "336_384_96_336_48_96_2_2",
                    (1280, 16, 768, -1, 2): "640_16_192_640_16_48_1_2",
                    (320, 64, 320, -1, 2): "320_64_192_320_48_96_1_2",
                    (1280, 768, 768, -1, 4): "320_384_96_320_48_96_2_2",
                    (16, -1, 4096, 0, 2): "16_16_1024_16_16_1024_2_2"
                    }

        return shape_map

    @staticmethod
    def get_mini_frac_shape_map():
        """
        the knowledge of matmul schedule tiling
        """
        shape_map = {(304, -1, 4096, -1, 2): "304_80_192_304_80_192_2_2",
                    (304, -1, 4096, -1, 4): "304_80_192_304_80_192_2_2",
                    (304, -1, 4096, -1, 6): "304_80_192_304_80_192_2_2"
                    }

        return shape_map

    @staticmethod
    def get_cloud_shape_map(core_num, core_num_thrity):
        """
        the knowledge of matmul schedule tiling
        """
        if core_num == core_num_thrity:
            shape_map = {(1024, 20480, 1024, -1, 4): "256_512_160_256_64_160_2_2",
                        (12288, 1024, 4096, -1, 2): "208_512_128_208_64_128_2_2",
                        (12288, 4096, 1024, -1, 2): "208_512_128_208_64_128_2_2",
                        (1024, 12288, 1024, -1, 4): "208_512_176_208_64_176_2_2",
                        (12288, 1024, 1024, -1, 2): "208_512_128_208_64_128_2_2",
                        (12288, 1024, 1024, -1, 4): "208_512_128_208_64_128_2_2"
                        }
        else:
            shape_map = {(18432, 1024, 1024, 1, 2): "144_512_256_144_64_256_2_2"
            }

        return shape_map

    @staticmethod
    def get_mdc_shape_map():
        """
        the knowledge of matmul schedule tiling
        """
        shape_map = {(1024, 768, 768, -1, 4): "256_768_384_128_256_128_2_2",
                    (1024, 768, 3072, 1, 6): "128_384_32_128_96_32_3_2",
                    (2048, 768, 3072, 1, 6): "128_384_32_128_96_32_3_2",
                    (1024, 768, 768, -1, 6): "256_768_384_128_256_128_2_2",
                    (512, 768, 768, 1, 2): "128_96_192_128_96_192_2_2"
                    }

        return shape_map

    @staticmethod
    def _get_l1fusion_device_core_num(is_l1fusion):
        """
        get the device core num
        :param is_l1fusion: is l1 fusion
        :return: device core num
        """
        if is_l1fusion:
            device_core_num = 1
        else:
            device_core_num = tbe_platform_info.get_soc_spec("CORE_NUM")
        return device_core_num

    @staticmethod
    def _get_core_map():
        """
        the knowledge of matmul schedule core tiling
        """
        shape_map = {(1024, 20480, 1024): (4, 7),
                    (4096, 20480, 1024): (7, 4),
                    (20480, 4096, 1024): (15, 2),
                    (1024, 20480, 4096): (4, 7),
                    (1024, 12288, 4096): (4, 7),
                    (4096, 12288, 1024): (7, 4)
                    }
        return shape_map

    @staticmethod
    def _get_special_l0_factor(src_shape, m_l0_shape, k_l0_shape, n_l0_shape):
        """
        get temp factors
        """
        m_shape = src_shape[0]
        k_shape = src_shape[1]
        n_shape = src_shape[2]
        if m_shape * n_shape * k_shape == m_l0_shape * n_l0_shape * k_l0_shape and \
                m_l0_shape != 1:
            m_l0_shape = int((m_l0_shape // 2))
            if int((m_l0_shape % 16)) != 0:
                m_l0_shape = int((m_l0_shape + 15) // 16 * 16)

        src_shape = [m_shape, k_shape, n_shape]
        if src_shape == [256, 64, 256]:
            m_l0_shape = 256
            k_l0_shape = 64
            n_l0_shape = 128
        elif src_shape == [256, 256, 64]:
            m_l0_shape = 64
            k_l0_shape = 256
            n_l0_shape = 64
        return m_l0_shape, k_l0_shape, n_l0_shape


    def compute_tiling_enter(self):
        """
        the enter of compute tiling
        Return: tiling, type dict
        """
        # compute the core num of batch, m and n
        is_l1fusion = self.l1_fusion_type in (0, 1)
        core_num = self._get_l1fusion_device_core_num(is_l1fusion)
        batch = self.batch_shape
        batch_factor = 1
        if batch > 1:
            batch_factor = min(batch, core_num)
        m_factors, n_factors = self._get_perfect_core_num(core_num // batch_factor)
        m_factors, n_factors = self._get_knowledge_core(self.mkn_shape, m_factors, n_factors)
        k_factors = 1
        if self.custom_block_dim != []:
            m_factors, k_factors, n_factors = self.custom_block_dim
        m_var = [self.mkn_shape[self.MKN_M_INDEX], m_factors]
        n_var = [self.mkn_shape[self.MKN_N_INDEX], n_factors]
        k_var = [self.mkn_shape[self.MKN_K_INDEX], k_factors]
        core_inner_m, core_inner_n, core_inner_k = self._get_core_inner_mkn(m_var, n_var, k_var)
        ub_reserve_buff = 0
        if self.dequant_fusion:
            # quant parameter is fixed float16, it's 2 bytes
            # just support scalar now, not support vector yet
            ub_reserve_buff = tbe_platform.BLOCK_OUT * 2

        n_cut_even = self._is_need_n_cut_even(core_inner_n)
        a_ub_byte, b_ub_byte, ub_res_byte, l1a_byte, l1b_byte, l0a_byte, l0b_byte, l0c_byte = self.bytes_info
        ub_res_byte = int(math.ceil(ub_res_byte))
        if self.mmad_mode != "gemm":
            core_inner_m = 1
        get_tiling_shape = tvm.get_global_func("cce.matmul_tiling_gen")
        tiling_shape = get_tiling_shape(int(core_inner_m), int(core_inner_k), int(core_inner_n),
                                        a_ub_byte, b_ub_byte, l1a_byte, l1b_byte, l0a_byte,
                                        l0b_byte, l0c_byte, ub_res_byte, ub_reserve_buff,
                                        n_cut_even, int(self.is_b_nz))

        m_shape, k_shape, n_shape = self.mkn_shape
        b_trans = self.b_trans
        shape_tiling_args = (m_shape, k_shape, n_shape, b_trans, ub_res_byte)
        tiling_shape = self._get_knowledge_tiling(shape_tiling_args, self.is_b_nz, tiling_shape)
        tiled_shape = tiling_shape.split('_')
        m_l1_shape, k_l1_shape, n_l1_shape, m_l0_shape, k_l0_shape, n_l0_shape = [int(i) for i in tiled_shape[:6]]

        after_multicore_shape = [core_inner_m, core_inner_k, core_inner_n]
        m_l0_shape, k_l0_shape, n_l0_shape = self._get_special_l0_factor(after_multicore_shape, m_l0_shape,
            k_l0_shape, n_l0_shape)
        m_l1_shape, k_l1_shape, n_l1_shape = m_l0_shape, k_l0_shape, n_l0_shape

        # compute L1 to L0 tiling params
        m_l0_tile = (m_l0_shape + self.block_in - 1) // self.block_in
        k_l0_tile = (k_l0_shape + self.block_reduce - 1) // self.block_reduce
        n_l0_tile = (n_l0_shape + self.block_out - 1) // self.block_out

        # compute GM to L1 tiling params
        m_l1_tile = (m_l1_shape + self.block_in - 1) // self.block_in
        k_l1_tile = (k_l1_shape + self.block_reduce - 1) // self.block_reduce
        n_l1_tile = (n_l1_shape + self.block_out - 1) // self.block_out

        tiling_factors = [
            batch_factor, m_factors, n_factors, k_factors,
            m_l1_tile, k_l1_tile, k_l1_tile, n_l1_tile,
            m_l0_tile, k_l0_tile, n_l0_tile, n_l0_tile, m_l1_tile, n_l1_tile, [1, 1]]
        double_buffers = self._get_double_buffer(tiling_factors, l0c_byte)
        block_sizes = [self.block_reduce, self.block_in, self.block_out]
        return _assembly_tiling(tiling_factors, double_buffers, block_sizes)

    def _get_double_buffer(self, tiling_factors, l0c_byte):
        m_shape, k_shape, n_shape = self.mkn_shape
        [_, _, _, _,
         m_l1_tile, k_l1_tile, _, n_l1_tile,
         m_l0_tile, _, n_l0_tile, _, _, _, _] = tiling_factors

        al1_db = 2
        if m_l1_tile * self.block_in == m_shape and k_l1_tile * self.block_reduce == k_shape:
            al1_db = 1
        bl1_db = 2
        if n_l1_tile * self.block_out == n_shape and k_l1_tile * self.block_reduce == k_shape:
            bl1_db = 1

        l0c_db = 2
        l0c_size = tbe_platform_info.get_soc_spec("L0C_SIZE")
        if m_l0_tile * n_l0_tile * self.block_in * self.block_out * l0c_byte * self.DOUBLE_VALUE > l0c_size:
            l0c_db = 1

        return [2, 2, al1_db, bl1_db, 2, 2, l0c_db, 2]

    def _get_perfect_core_num(self, core_num):
        """
        :param input_shape_1:the tensor_a shape
        :param input_shape_2:the tensor_b shape
        :return:core_num
        """
        m_shape, k_shape, n_shape = self.mkn_shape
        frac_size = self.block_in
        m_axis_outer = (m_shape + frac_size - 1) // frac_size
        if m_shape == 1:
            m_axis_outer = 1
            n_axis_outer = (n_shape + frac_size - 1) // frac_size
            if n_axis_outer > core_num:
                return 1, core_num
            return 1, 1

        m_axis_outer = m_shape // frac_size
        n_axis_outer = n_shape // frac_size
        if (m_axis_outer * n_axis_outer) <= core_num:
            return m_axis_outer, n_axis_outer
        tensor_a_size = m_shape * k_shape
        tensor_b_size = n_shape * k_shape
        min_copy_size = core_num * (tensor_a_size + tensor_b_size)
        m_factor = 1
        n_factor = 1

        for i in range(1, core_num + 1):
            # judge cur_factor
            if core_num % i != 0:
                continue

            cur_m_factor = i
            cur_n_factor = core_num // i
            if cur_m_factor > m_axis_outer or (m_axis_outer // cur_m_factor) == 0:
                continue
            if cur_n_factor > n_axis_outer or (n_axis_outer // cur_n_factor) == 0:
                continue

            cur_copy_size = cur_n_factor * tensor_a_size + cur_m_factor * \
                tensor_b_size
            temp_m_shape = m_shape
            temp_n_shape = n_shape
            if m_axis_outer % m_factor != 0:
                temp_m_shape = (((m_axis_outer // cur_m_factor) + 1) *
                                cur_m_factor) * frac_size

            if n_shape % n_factor != 0:
                temp_n_shape = (((n_axis_outer // cur_n_factor) + 1) *
                                cur_n_factor) * frac_size

            cur_copy_size = cur_n_factor * (temp_m_shape * k_shape) + \
                cur_m_factor * (temp_n_shape * k_shape)
            if cur_copy_size < min_copy_size:
                min_copy_size = cur_copy_size
                m_factor = cur_m_factor
                n_factor = cur_n_factor

        return m_factor, n_factor

    def _get_knowledge_core(self, shape_mkn_args, m_factors, n_factors):
        """
        get knowledge of core set

        Parameters
        ----------
        shape_mkn_args : list, shape info

        m_factors: core split in m_factor

        n_factors: core split in n_factors

        Returns
        -------
        m_factors, n_factors, value of m, n core split
        """
        shape_map = {}
        core_num = tbe_platform_info.get_soc_spec("CORE_NUM")
        if core_num == self.CORE_NUM_THRITY:
            shape_map = self._get_core_map()

        if shape_map.get(shape_mkn_args) is not None:
            m_factors, n_factors = shape_map[shape_mkn_args]

        return m_factors, n_factors

    def _get_core_inner_mkn(self, m_var, n_var, k_var):
        """
        get m/k/n shape in single core
        """
        m_shape = m_var[0]
        m_factors = m_var[1]
        n_shape = n_var[0]
        n_factors = n_var[1]
        core_inner_m = m_shape
        core_inner_n = n_shape
        core_inner_k = k_var[0]
        block_in = self.block_in
        block_out = self.block_out
        block_reduce = self.block_reduce
        if m_shape != 1:
            core_inner_m = (((m_shape + block_in - 1) // block_in + (m_factors - 1)) // m_factors) * block_in
        core_inner_n = (((n_shape + block_out - 1) // block_out + (n_factors - 1)) // n_factors) * block_out
        core_inner_k = (((k_var[0] + block_reduce - 1) // block_reduce + (k_var[1] - 1)) // k_var[1]) * block_reduce
        return [core_inner_m, core_inner_n, core_inner_k]

    def _is_need_n_cut_even(self, core_inner_n):
        if not self.date_transfer_fusion:
            return False
        if core_inner_n == 16:
            return False
        return True

    def _get_knowledge_tiling(self, shape_tiling_args, is_b_nz, tiling_shape):
        """
        get knowledge tiling for matmul schedule
        """
        m_shape, k_shape, n_shape, b_trans, ub_res_byte = shape_tiling_args
        b_trans_val = -1
        if b_trans is not None:
            b_trans_val = 1 if b_trans else 0
        shape_args = (m_shape, k_shape, n_shape, b_trans_val, ub_res_byte)

        shape_map = {}
        core_num = tbe_platform_info.get_soc_spec("CORE_NUM")
        if core_num == self.DOUBLE_VALUE:
            if is_b_nz:
                shape_map = self.get_shape_map()
            else:
                shape_map = self.get_mini_frac_shape_map()
        elif core_num in (self.CORE_NUM_THRITY, self.CORE_NUM_THRITY_TWO):
            shape_map = self.get_cloud_shape_map(core_num, self.CORE_NUM_THRITY)
        elif core_num == self.CORE_NUM_EIGHT:
            shape_map = self.get_mdc_shape_map()
        if shape_map.get(shape_args) is not None:
            tiling_shape = shape_map[shape_args]
        else:
            shape_args = (m_shape, k_shape, n_shape, -1, ub_res_byte)
            if shape_map.get(shape_args) is not None:
                tiling_shape = shape_map[shape_args]
            else:
                shape_args = (m_shape, -1, n_shape, b_trans_val, ub_res_byte)
                if shape_map.get(shape_args) is not None:
                    tiling_shape = shape_map[shape_args]
                else:
                    shape_args = (m_shape, -1, n_shape, -1, ub_res_byte)
                    if shape_map.get(shape_args) is not None:
                        tiling_shape = shape_map[shape_args]

        return tiling_shape


class ProcessTiling:
    """
    process tiling_case in tiling_cases and get attach_flag
    """
    ATTACH_MAP = {
        Compare.EQUAL: 1,
        Compare.LESS_EQ: 2,
        Compare.GREATE_EQ: 3
    }
    ATTACH_ORI_MAP = {
        Compare.EQUAL: 0,
        Compare.LESS_EQ: 1,
        Compare.GREATE_EQ: 0
    }
    ATTACH_ABUB_MAP = {
        Compare.EQUAL: 5,
        Compare.LESS_EQ: 4
    }
    FRACTAL_Z_KA_INDEX = -3
    FRACTAL_Z_KB_INDEX = -4

    def __init__(self, tiling_case):
        self.tiling = tiling_case.get("tiling_strategy")
        self.compute_param = tiling_case.get("compute_param")
        self.m_k_n_shape = tiling_case.get("m_k_n_shape")
        self.tensor_list = tiling_case.get("tensor_list")
        self.cache_tiling_mgr = CacheTilingManager(None, tiling_case)
        self.tiling_work = GemmTilingWork()
        self.dynamic_seed_shape = [1, 1, 1, 1]
        self.support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")

    @staticmethod
    def _prod(lst):
        return reduce(lambda x, y: x * y, lst)

    @staticmethod
    def _bit_width(dtype):
        return {'float16': 16, 'int8': 8, 'float32': 32, 'int4': 4}.get(dtype)

    def get_seed_shape(self):
        if not in_dynamic():
            return
        dynamic_m, dynamic_k, dynamic_n, dynamic_batch = 1, 1, 1, 1
        seed_shape = list(self.m_k_n_shape)
        if seed_shape and len(seed_shape) in (3, 4):
            dynamic_m, dynamic_k, dynamic_n = seed_shape[:3]
            if len(seed_shape) == 4:
                dynamic_batch = seed_shape[3]
        self.dynamic_seed_shape = [dynamic_m, dynamic_k, dynamic_n, dynamic_batch]

    def get_tiling(self):
        """
        get tiling from dynamic_para
        """
        # normal dynamic shape
        if in_dynamic() and "attach_at_flag" not in self.tiling:
            decode_tiling(self.tiling)
        tensor_map, para_map = self.tensor_list[3:]
        if self.tiling.get("binary_flag"):
            tensor_map.get("c_gm").op.attrs["is_binary_constant"] = 1
            self.tiling["AL0_matrix"] = [1, 1, 32, 16, 1, 1]
        # set init para
        self.tiling_work.block_reduce = GEMMComputeParam.get_block_reduce(para_map.get("ops_data_flow_mode"))
        load2d_transpose_a = (self.support_l0c2out and
                              tensor_map.get("a_l0a").dtype == "int8" and para_map.get("trans_a"))
        load2d_transpose_b = (self.support_l0c2out and
                              tensor_map.get("b_l0b").dtype == "int8" and (not para_map.get("trans_b")))
        if not in_dynamic() and tensor_map.get("c_gm").dtype in ("int8", "uint8") or load2d_transpose_b:
            n1_shape = get_value(tensor_map.get("b_l0b").shape[-3])
            para_map["requant_n_odd_flag"] = (n1_shape % 2 == 1)
            index_n = NC_FACTOR_INDEX if (para_map.get("format_out") != "ND") else N_INDEX
            para_map["requant_n_odd_flag_cub"] = (get_value(tensor_map.get("c_gm").shape[index_n]) %
                                                  ALIGN_NUM_TWO == 1)
        if not in_dynamic() and load2d_transpose_a:
            m1_shape = get_value(tensor_map.get("a_l0a").shape[M_L0A_INDEX])
            para_map["requant_m_odd_flag"] = (m1_shape % ALIGN_NUM_TWO == 1)
            index_m = MC_FACTOR_INDEX if (para_map.get("format_out") != "ND") else M_INDEX
            para_map["requant_m_odd_flag_cub"] = (get_value(tensor_map.get("c_gm").shape[index_m]) %
                                                  ALIGN_NUM_TWO == 1)
        # binary mode
        if in_dynamic() and "attach_at_flag" in self.tiling:
            self.cache_tiling_mgr.config_cache_tiling(self.compute_param, para_map.get("ops_data_flow_mode"))
            cache_tiling = self.cache_tiling_mgr.cache_tiling
            self.tiling = self.tiling_work.config_tiling(self.tiling, cache_tiling, para_map)
        self._no_solution_tiling()
        self._gemv_tiling()
        self._check_k_full_load()
        if not self.tiling:
            args_dict = {"errCode": "E60114", "reason": "tiling is None", "value": "None"}
            raise RuntimeError(args_dict, error_manager_util.get_error_message(args_dict))
        self._check_tiling_value()
        self.tiling_work.tiling = self.tiling

    def tiling_pre_process(self):
        """
        process the tiling_work tiling factors
        """
        self._tiling_l0_process()
        self._tiling_l1_process()
        self._tiling_ub_process()

    def get_tiling_attach_flag(self):
        # is binary
        if "attach_at_flag" in self.tiling_work.tiling.keys():
            return
        self.tiling_work.tiling["attach_at_flag"] = dict()
        c_ub_tiling_shape = self._get_cub_attach_flag()
        self._get_cl0_attach_flag(c_ub_tiling_shape)
        self._get_al0_attach_flag()
        self._get_bl0_attach_flag()
        self._get_al1_attach_flag()
        self._get_bl1_attach_flag()
        self._get_aub_attach_flag()
        self._get_bub_attach_flag()

    def _get_broad_batch_single_core(self, batch_shape, batch_input_list, batch_shape_list):
        batch_single_core = int_ceil_div(batch_shape, self.tiling_work.tiling.get("block_dim")[0])
        batch_input_idx = len(batch_input_list) - 1
        batch_idx = len(batch_shape_list) - 1
        batch_input_single_core = 1
        need_add_one = False
        while batch_single_core > 1 and batch_input_idx >= 0:
            if batch_single_core > batch_shape_list[batch_idx]:
                batch_single_core = int_ceil_div(batch_single_core, batch_shape_list[batch_idx])
                batch_input_single_core = batch_input_single_core * batch_input_list[batch_input_idx]
                batch_idx = batch_idx - 1
                batch_input_idx = batch_input_idx - 1
                need_add_one = batch_single_core % (reduce(lambda x, y : x * y, batch_shape_list[batch_idx:])) != 0
            else:
                if batch_shape_list[batch_idx] % batch_single_core == 0 or batch_idx == 0:
                    # batch_input_list[batch_input_idx] may is 1
                    batch_input_single_core *= min(batch_single_core + int(need_add_one),
                                                    batch_input_list[batch_input_idx])

                elif batch_input_idx == 0 or batch_input_list[batch_input_idx - 1] == 1:
                    batch_input_single_core *= batch_input_list[batch_input_idx]
                else:
                    batch_input_single_core *= min(batch_single_core, batch_input_list[batch_input_idx]) + \
                        int(need_add_one)
                return batch_input_single_core
        return batch_input_single_core

    def _get_batch_single_core(self, tensor_name):
        """
        calculate the amount of batch of input on a single core, especially for broadcast scene
        example:
        -----------------------
        batch_shape_list: [8, 13], batch_block_dim: 13
        if batch_input_list is [1, 13], batch_input_single_core: 13
        if batch_input_list is [8, 1], batch_input_single_core: 2

        """
        if in_dynamic():
            dynamic_batch = self.dynamic_seed_shape[-1]
            return int_ceil_div(dynamic_batch, self.tiling_work.tiling.get("block_dim")[0])
        tensor_map = self.tensor_list[3]
        attrs_input = tensor_map.get(tensor_name).op.attrs
        # for fully_connection ori_batch_shape is not in attrs
        batch_input_list = shape_to_list(attrs_input.get("ori_batch_shape", []))
        attr_dict = tensor_map.get("tensor_c_gm").op.attrs
        batch_shape_list = shape_to_list(attr_dict.get("batch_shape"))
        c_l0c = tensor_map.get("c_l0c")
        batch_shape =  get_value(c_l0c.shape[0]) if len(c_l0c.shape) == BMM_LEN_NZ else 0
        if not batch_input_list or batch_input_list == batch_shape_list:
            return int_ceil_div(batch_shape, self.tiling_work.tiling.get("block_dim")[0])
        else:
            return self._get_broad_batch_single_core(batch_shape, batch_input_list, batch_shape_list)

    def _get_bub_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        b_ub = tensor_map.get("b_ub")
        if b_ub in (None, []):
            return
        l1_kb = (self.tiling_work.bl1_tiling_k + self.tiling_work.bl0_tiling_k0 - 1) // self.tiling_work.bl0_tiling_k0
        bub_tiling_k0, bub_tiling_n0 = self.tiling_work.block_reduce, self.tiling_work.block_out
        ub_kb = (self.tiling_work.bub_tiling_k + bub_tiling_k0 - 1) // bub_tiling_k0
        b_ub_ori_shape = [get_value(i) for i in b_ub.shape]
        tiling_ori_bub = [self.tiling_work.bub_tiling_k, self.tiling_work.bub_tiling_n * bub_tiling_n0]
        tiling_ori_bl1 = [l1_kb, self.tiling_work.bl1_tiling_n,
                          self.tiling_work.bl0_tiling_n0, self.tiling_work.bl0_tiling_k0]
        tiling_ori_bub_with_l1 = [ub_kb, self.tiling_work.bub_tiling_n, bub_tiling_n0, bub_tiling_k0]
        if len(b_ub_ori_shape) in (2, 3):
            if para_map.get("trans_b"):
                b_ub_ori_shape[-2:] = [b_ub_ori_shape[-1], b_ub_ori_shape[-2]]
        if para_map.get("b_matrix_mode", "none") == "Zn2Zn_int82fp32":
            tiling_ori_bub = [ub_kb // 2, self.tiling_work.bub_tiling_n, bub_tiling_n0, bub_tiling_k0 * 2]
        elif para_map.get("b_matrix_mode", "none") == "Nz2Zn":
            tiling_ori_bub = [self.tiling_work.bub_tiling_n, ub_kb, bub_tiling_k0, bub_tiling_n0]
        b_ub_ori_shape, tiling_ori_bub = self._get_dynamic_bub_shape(b_ub_ori_shape, tiling_ori_bub)
        self._renew_bub_n(b_ub_ori_shape)
        bub_tiling_shape_with_lc0 = [self.tiling_work.bub_tiling_n, bub_tiling_n0,
            (self.tiling_work.bub_tiling_k + bub_tiling_k0 - 1) // bub_tiling_k0, bub_tiling_k0]
        cl0_tiling_shape = [self.tiling_work.cl0_tiling_nc, self.tiling_work.cl0_tiling_n0,
                            self.tiling_work.c_col_k1, self.tiling_work.c_col_k0]
        if para_map.get("have_batch_b"):
            tiling_ori_bub.insert(0, self.tiling_work.bub_tiling_batch)
            tiling_ori_bl1.insert(0, self.tiling_work.bl1_tiling_batch)
            tiling_ori_bub_with_l1.insert(0, self.tiling_work.bub_tiling_batch)
            bub_tiling_shape_with_lc0.insert(0, self.tiling_work.bub_tiling_batch)
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
        # get bub_attach_flag
        status_ori = Compare.compare(tiling_ori_bub, b_ub_ori_shape)
        status_l1 = Compare.compare(tiling_ori_bub_with_l1, tiling_ori_bl1)
        status_l0c = Compare.compare(bub_tiling_shape_with_lc0, cl0_tiling_shape)
        if para_map.get("batch_broadcast_change_attach") and para_map.get("format_b") == "ND":
            status_ori = Compare.LESS_EQ
        bub_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if bub_attach_flag == 1:
            bub_attach_flag = self.ATTACH_MAP.get(status_l1)
            if status_l1 == Compare.GREATE_EQ and status_l0c != Compare.GREATE_EQ:
                bub_attach_flag = self.ATTACH_ABUB_MAP.get(status_l0c)
        self.tiling_work.tiling["attach_at_flag"]["bub_attach_flag"] = bub_attach_flag

    def _renew_bub_n(self, b_ub_ori_shape):
        para_map = self.tensor_list[-1]
        index_offset = 1 if para_map.get("have_batch_b") else 0
        block_n = self.tiling_work.tiling.get("block_dim")[1]
        if para_map.get("format_b") == "ND":
            b_ub_ori_shape[1 + index_offset] = int_ceil_div(b_ub_ori_shape[1 + index_offset],
                block_n * self.tiling_work.block_out) * self.tiling_work.block_out
        elif para_map.get("format_b") == "FRACTAL_Z":
            b_ub_ori_shape[1 + index_offset] = int_ceil_div(b_ub_ori_shape[1 + index_offset], block_n)
        else:
            if para_map.get("trans_b"):
                b_ub_ori_shape[1 + index_offset] = int_ceil_div(b_ub_ori_shape[1 + index_offset], block_n)
            else:
                b_ub_ori_shape[index_offset] = int_ceil_div(b_ub_ori_shape[index_offset], block_n)
        if para_map.get("have_batch_b") and not in_dynamic():
            b_ub_ori_shape[0] = int_ceil_div(b_ub_ori_shape[0], self.tiling_work.tiling.get("block_dim")[0])

    def _get_dynamic_bub_shape(self, bub_shape, bub_tiling):
        if in_dynamic():
            _, dynamic_k, dynamic_n, _ = self.dynamic_seed_shape
            para_map = self.tensor_list[-1]
            # k1 * k0, n1
            bub_shape = [dynamic_k * self.tiling_work.block_reduce, dynamic_n]
            bub_tiling = [self.tiling_work.bub_tiling_k, self.tiling_work.bub_tiling_n]
            if para_map.get("have_batch_b"):
                bub_shape.insert(0, self.tiling_work.bub_tiling_batch)
        return bub_shape, bub_tiling

    def _get_aub_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        a_ub = tensor_map.get("a_ub")
        if a_ub in (None, []):
            return
        aub_tiling_k, aub_tiling_m = self.tiling_work.aub_tiling_k, self.tiling_work.aub_tiling_m
        aub_tiling_k0 = self.tiling_work.block_reduce
        aub_tiling_m0 = 1 if para_map.get("mmad_mode") == "gevm" else self.tiling_work.block_in
        al0_tiling_m0, al0_tiling_k0 = self.tiling_work.al0_tiling_m0, self.tiling_work.al0_tiling_k0
        l1_ma, al1_tiling_k = self.tiling_work.al1_tiling_m, self.tiling_work.al1_tiling_k
        l1_ka = (al1_tiling_k + al0_tiling_k0 - 1) // al0_tiling_k0
        ub_ka = (aub_tiling_k + aub_tiling_k0 - 1) // aub_tiling_k0
        # a_ub_ori_shape([m, k]) and tiling_ori_aub are used to compare full load (pass) or not
        tiling_ori_aub = [aub_tiling_m * aub_tiling_m0, ub_ka * aub_tiling_k0]
        a_ub_ori_shape = [get_value(i) for i in a_ub.shape]
        if para_map.get("trans_a") and len(a_ub_ori_shape) in (2, 3):
            a_ub_ori_shape[-2:] = [a_ub_ori_shape[-1], a_ub_ori_shape[-2]]
        # tiling_ori_al1, tiling_ori_aub_with_l1 are used to choose compute at tensor
        tiling_ori_al1 = [l1_ma, l1_ka, al0_tiling_m0, al0_tiling_k0]
        tiling_ori_aub_with_l1 = [aub_tiling_m, ub_ka, aub_tiling_m0, aub_tiling_k0]
        # in Nz2Zz_int82fp32 scene, ori_shape is 4d
        if para_map.get("a_matrix_mode", "none") == "Nz2Zz_int82fp32":
            tiling_ori_aub = [ub_ka // 2, aub_tiling_m, aub_tiling_m0, aub_tiling_k0 * 2]
        # in nd2Zz_vnchwconv scene L1 tensor is 3d
        elif para_map.get("a_matrix_mode", "none") == "nd2Zz_vnchwconv":
            tiling_ori_al1 = [l1_ma, l1_ka * al0_tiling_k0, al0_tiling_m0]
            tiling_ori_aub_with_l1 = [aub_tiling_m, ub_ka * aub_tiling_k0, aub_tiling_m0]
        elif para_map.get("a_matrix_mode", "none") in ("nd2Zz_int8", "nd2Zz", "nd_gemv", "nd_gevm"):
            tiling_ori_aub_with_l1 = [aub_tiling_m, ub_ka, al0_tiling_m0, aub_tiling_k0]
        a_ub_ori_shape, tiling_ori_aub = self._get_dynamic_aub_shape(a_ub_ori_shape, tiling_ori_aub)
        self._renew_aub_m(a_ub_ori_shape)
        aub_tiling_shape_with_lc0 = [aub_tiling_m, aub_tiling_m0,
                                     (aub_tiling_k + aub_tiling_k0 - 1) // aub_tiling_k0, aub_tiling_k0]
        cl0_tiling_shape = [self.tiling_work.cl0_tiling_mc, self.tiling_work.cl0_tiling_m0,
                            self.tiling_work.c_col_k1, self.tiling_work.c_col_k0]
        if para_map.get("have_batch_a"):
            tiling_ori_aub.insert(0, self.tiling_work.aub_tiling_batch)
            tiling_ori_al1.insert(0, self.tiling_work.al1_tiling_batch)
            tiling_ori_aub_with_l1.insert(0, self.tiling_work.aub_tiling_batch)
            aub_tiling_shape_with_lc0.insert(0, self.tiling_work.aub_tiling_batch)
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
        # get aub_attach_flag
        status_ori = Compare.compare(tiling_ori_aub, a_ub_ori_shape)
        status_l1 = Compare.compare(tiling_ori_aub_with_l1, tiling_ori_al1)
        status_l0c = Compare.compare(aub_tiling_shape_with_lc0, cl0_tiling_shape)
        if para_map.get("batch_broadcast_change_attach") and para_map.get("format_a") == "ND":
            status_ori = Compare.LESS_EQ
        aub_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if aub_attach_flag == 1:
            aub_attach_flag = self.ATTACH_MAP.get(status_l1)
            if status_l1 == Compare.GREATE_EQ and status_l0c != Compare.GREATE_EQ:
                aub_attach_flag = self.ATTACH_ABUB_MAP.get(status_l0c)
        self.tiling_work.tiling["attach_at_flag"]["aub_attach_flag"] = aub_attach_flag

    def _renew_aub_m(self, a_ub_ori_shape):
        para_map = self.tensor_list[-1]
        index_offset = 1 if para_map.get("have_batch_a") else 0
        if para_map.get("format_a") == "ND":
            a_ub_ori_shape[index_offset] = int_ceil_div(a_ub_ori_shape[index_offset],
                self.tiling_work.tiling.get("block_dim")[2] * self.tiling_work.block_in) * self.tiling_work.block_in
        else:
            if para_map.get("trans_a"):
                a_ub_ori_shape[index_offset] = int_ceil_div(a_ub_ori_shape[index_offset],
                    self.tiling_work.tiling.get("block_dim")[2])
            else:
                a_ub_ori_shape[1 + index_offset] = int_ceil_div(a_ub_ori_shape[1 + index_offset],
                    self.tiling_work.tiling.get("block_dim")[2])
        if para_map.get("have_batch_a") and not in_dynamic():
            a_ub_ori_shape[0] = int_ceil_div(a_ub_ori_shape[0], self.tiling_work.tiling.get("block_dim")[0])

    def _get_dynamic_aub_shape(self, aub_shape, aub_tiling):
        if in_dynamic():
            dynamic_m, dynamic_k, _, dynamic_batch = self.dynamic_seed_shape
            para_map = self.tensor_list[-1]
            # m1, k1*k0
            aub_shape = [dynamic_m, dynamic_k * self.tiling_work.block_reduce]
            aub_tiling = [self.tiling_work.aub_tiling_m, self.tiling_work.aub_tiling_k]
            if para_map.get("have_batch_a"):
                # tiling add batch at the same palce of static mode
                aub_shape.insert(0, dynamic_batch)
        return aub_shape, aub_tiling

    def _get_bl1_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        _, dynamic_k, dynamic_n, dynamic_batch = self.dynamic_seed_shape
        self._temporarily_enable_fullload_to_bind_multi_core_when_exceed_space()
        sparse_4to2_flag = para_map.get("alg", None)
        # get bl1_tiling_shape
        bl0_tiling_n0, bl0_tiling_k0 = self.tiling_work.bl0_tiling_n0, self.tiling_work.bl0_tiling_k0
        bl1_tiling_n, bl1_tiling_k = self.tiling_work.bl1_tiling_n, self.tiling_work.bl1_tiling_k
        l1_nb = bl1_tiling_n
        l1_kb = bl1_tiling_k // bl0_tiling_k0
        if not self.cache_tiling_mgr.cache_tiling:
            l1_kb = int_ceil_div(bl1_tiling_k, bl0_tiling_k0)
        kb_multi_factor = 2 if (sparse_4to2_flag and l1_kb != 1) else 1
        bl1_tiling_shape = [l1_nb, bl0_tiling_n0, l1_kb, bl0_tiling_k0]
        # get cl0_tiling_shape
        cl0_tiling_nc, cl0_tiling_n0 = self.tiling_work.cl0_tiling_nc, self.tiling_work.cl0_tiling_n0
        cl0_tiling_shape = [cl0_tiling_nc, cl0_tiling_n0, self.tiling_work.c_col_k1, self.tiling_work.c_col_k0]
        # get tiling_ori_bl1
        tiling_ori_bl1 = [l1_kb // kb_multi_factor, l1_nb]
        # get bl1_shape
        b_l1 = tensor_map.get("b_l1")
        if para_map.get("mmad_mode") == "gemv":
            b_l1 = tensor_map.get("a_l1")
        if in_dynamic():
            n_shape = dynamic_n
            k_shape = dynamic_k
        else:
            b_l0b = tensor_map.get("b_l0b")
            if para_map.get("mmad_mode") == "gemv":
                b_l0b = tensor_map.get("a_l0a")
            b_l0b_shape = [get_value(i) for i in b_l0b.shape]
            b_l0b_shape = self._update_fp32_l0_shape(b_l0b_shape, "b_l0b", b_l0b.dtype)
            n_shape = b_l0b_shape[-3]
            k_shape = b_l0b_shape[-4]
        bl1_shape = [k_shape, n_shape]
        if para_map.get("requant_n_odd_flag"):
            bl1_shape[-1] += 1
        if not self.tiling_work.bind_core_when_full_load_bl1:
            bl1_shape[1] = int_ceil_div(bl1_shape[1], self.tiling_work.tiling.get("block_dim")[1])
        # tiling_l1_nb may exceeds n1_shape in cache_tiling
        tiling_ori_bl1[1] = min(bl1_shape[1], tiling_ori_bl1[1]) if not in_dynamic() else tiling_ori_bl1[1]
        if para_map.get("have_batch_b"):
            bl1_tiling_shape.insert(0, self.tiling_work.bl1_tiling_batch)
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
            tiling_ori_bl1.insert(0, self.tiling_work.bl1_tiling_batch)
            batch_b_single_core = self._get_batch_single_core("b_l0b")
            bl1_shape.insert(0, batch_b_single_core)
        # get bl1_attach_flag
        status = Compare.compare(bl1_tiling_shape, cl0_tiling_shape)
        status_ori = Compare.compare(tiling_ori_bl1, bl1_shape)
        if para_map.get("batch_broadcast_change_attach") and para_map.get("format_b") == "ND":
            status_ori = Compare.LESS_EQ
        bl1_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if bl1_attach_flag == 1:
            bl1_attach_flag = self.ATTACH_MAP.get(status)
        self.tiling_work.tiling["attach_at_flag"]["bl1_attach_flag"] = bl1_attach_flag

    def _temporarily_enable_fullload_to_bind_multi_core_when_exceed_space(self):
        if self.support_l0c2out or in_dynamic():
            return
        tiling = self.tiling_work.tiling
        para_map = self.tensor_list[-1]
        soc_version = tbe_platform_info.get_soc_spec(tbe_platform_info.FULL_SOC_VERSION)
        size_l1 = tbe_platform_info.get_soc_spec(tbe_platform_info.L1_SIZE)
        status = (soc_version == "Ascend310P3" or soc_version == "Ascend310P5" or soc_version == "Ascend310P7") and \
            para_map.get("quantify_fusion", False)
        status = (status and para_map.get("a_matrix_mode", "none") == "Nz2Zz" and
                para_map.get("b_matrix_mode", "none") == "none")
        status = status and not tiling.get("AL1_shape") == [] and tiling.get("BL1_shape") == []
        status = status and (self._calc_b_l1(False) + self._calc_a_l1(False) > size_l1)
        status = status and (self._calc_b_l1(True) + self._calc_a_l1(False) <= size_l1)
        if status:
            self.tiling_work.bind_core_when_full_load_bl1 = True

    def _calc_b_l1(self, consider_multi_core_in_full_load=True):
        tensor_map, para_map = self.tensor_list[3:]
        tensor_bl1 = tensor_map.get("b_l1")
        tensor_bl0 = tensor_map.get("b_l0b")
        tiling_bl1 = self.tiling_work.tiling.get("BL1_shape")
        tiling_pb_bl1 = self.tiling_work.tiling.get("manual_pingpong_buffer").get("BL1_pbuffer")
        n_dim = self.tiling_work.tiling.get("block_dim")[1]
        *_, _, n1_bl0, n0_bl0, _ = (x.value for x in tensor_bl0.shape)
        if self.tiling_work.tiling.get("BL0_matrix") != []:
            *_, _, n1_bl0, n0_bl0, _ = self.tiling_work.tiling.get("BL0_matrix")
        if tiling_bl1 == []:
            size_shape = self._prod(x.value for x in tensor_bl1.shape)
            if consider_multi_core_in_full_load:
                # consistent with the implementation logic of schedule_agent, using nparts to split the axis
                # get_b_matrix_mode is none
                # cache_read tensor_b
                # k1, n1, k0, n0 with not trans_b
                *_, k1, n1, _, _ = (x.value for x in tensor_bl1.shape)
                if para_map.get("trans_b"):
                    k1, n1 = n1, k1
                n1_factor = math.ceil(n1 / n_dim)
                size_shape = size_shape // n1 * n1_factor
        else:
            size_shape = tiling_bl1[0] * tiling_bl1[1] * n1_bl0 * n0_bl0
        return size_shape * tiling_pb_bl1 * self._bit_width(tensor_bl1.dtype)

    def _calc_a_l1(self, consider_multi_core_in_full_load=True):
        tensor_map, para_map = self.tensor_list[3:]
        tensor_al1 = tensor_map.get("a_l1")
        tensor_al0 = tensor_map.get("a_l0a")
        tiling_al1 = self.tiling_work.tiling.get("AL1_shape")
        tiling_pb_al1 = self.tiling_work.tiling.get("manual_pingpong_buffer").get("AL1_pbuffer")
        m_dim = self.tiling_work.tiling.get("block_dim")[2]
        *_, m1_al0, _, m0_al0, _ = (x.value for x in tensor_al0.shape)
        if self.tiling_work.tiling.get("AL0_matrix") != []:
            *_, m1_al0, _, m0_al0, _ = self.tiling_work.tiling.get("AL0_matrix")
        if tiling_al1 == []:
            size_shape = self._prod(x.value for x in tensor_al1.shape)
            if consider_multi_core_in_full_load:
                # consistent with the implementation logic of schedule_agent, using nparts to split the axis
                # get_a_matrix_mode is Nz2Zz
                # cache_read tensor_a
                # k1, m1, m0, k0 with not trans_a
                *_, k1, m1, _, _ = (x.value for x in tensor_al1.shape)
                if para_map.get("trans_a"):
                    k1, m1 = m1, k1
                m1_factor = math.ceil(m1 / m_dim)
                size_shape = size_shape // m1 * m1_factor
        else:
            size_shape = tiling_al1[0] * tiling_al1[1] * m1_al0 * m0_al0
        return size_shape * tiling_pb_al1 * self._bit_width(tensor_al1.dtype)

    def _get_al1_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        dynamic_m, dynamic_k, _, dynamic_batch = self.dynamic_seed_shape
        # get al1_tiling_shape
        al1_tiling_m, al1_tiling_k = self.tiling_work.al1_tiling_m, self.tiling_work.al1_tiling_k
        al0_tiling_m0, al0_tiling_k0 = self.tiling_work.al0_tiling_m0, self.tiling_work.al0_tiling_k0
        l1_ma = al1_tiling_m
        l1_ka = al1_tiling_k // al0_tiling_k0
        if not self.cache_tiling_mgr.cache_tiling:
            l1_ka = int_ceil_div(al1_tiling_k, al0_tiling_k0)
        al1_tiling_shape = [l1_ma, al0_tiling_m0, l1_ka, al0_tiling_k0]
        # get cl0_tiling_shape
        cl0_tiling_mc, cl0_tiling_m0 = self.tiling_work.cl0_tiling_mc, self.tiling_work.cl0_tiling_m0
        cl0_tiling_shape = [cl0_tiling_mc, cl0_tiling_m0, self.tiling_work.c_col_k1, self.tiling_work.c_col_k0]
        # get tiling_ori_al1
        tiling_ori_al1 = [l1_ma, l1_ka]
        tiling_ori_al1[-2] = 1 if para_map.get("mmad_mode") in ("gevm", "gemv") else tiling_ori_al1[-2]
        # get al1_shape
        a_l1, a_l0a = tensor_map.get("a_l1"), tensor_map.get("a_l0a")
        if para_map.get("mmad_mode") == "gemv":
            a_l1, a_l0a = tensor_map.get("b_l1"), tensor_map.get("b_l0b")
        a_l0a_shape = [get_value(i) for i in a_l0a.shape]
        a_l0a_shape = self._update_fp32_l0_shape(a_l0a_shape, "a_l0a", a_l0a.dtype)
        m_shape = a_l0a_shape[-4]
        k_shape = a_l0a_shape[-3]
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            m_shape = a_l0a_shape[-3]
            k_shape = a_l0a_shape[-4]
        if in_dynamic():
            m_shape = dynamic_m
            k_shape = dynamic_k
        al1_shape = [m_shape, k_shape]
        if para_map.get("requant_m_odd_flag"):
            al1_shape[0] += 1
        al1_shape[0] = int_ceil_div(al1_shape[0], self.tiling_work.tiling.get("block_dim")[2])
        # tiling_l1_ma may exceeds m1_shape in cache_tiling
        tiling_ori_al1[0] = min(al1_shape[0], tiling_ori_al1[0]) if not in_dynamic() else tiling_ori_al1[0]
        if para_map.get("have_batch_a"):
            al1_tiling_shape.insert(0, self.tiling_work.al1_tiling_batch)
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
            tiling_ori_al1.insert(0, self.tiling_work.al1_tiling_batch)
            batch_a_single_core = self._get_batch_single_core("a_l0a")
            al1_shape.insert(0, batch_a_single_core)
        # get al1_attach_flag
        status = Compare.compare(al1_tiling_shape, cl0_tiling_shape)
        status_ori = Compare.compare(tiling_ori_al1, al1_shape)
        if para_map.get("batch_broadcast_change_attach") and para_map.get("format_a") == "ND":
            status_ori = Compare.LESS_EQ
        al1_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if al1_attach_flag == 1:
            al1_attach_flag = self.ATTACH_MAP.get(status)
        self.tiling_work.tiling["attach_at_flag"]["al1_attach_flag"] = al1_attach_flag

    def _get_bl0_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        sparse_4to2_flag = para_map.get("alg", None)
        # get tiling_ori_l0b
        tiling_ori_l0b = [self.tiling_work.bl0_tiling_kb, self.tiling_work.bl0_tiling_nb,
                          self.tiling_work.bl0_tiling_n0, self.tiling_work.bl0_tiling_k0]
        # get b_l0b_shape
        if para_map.get("mmad_mode") == "gemv":
            b_l0b = tensor_map.get("a_l0a")
        else:
            b_l0b = tensor_map.get("b_l0b")
        b_l0b_shape = [get_value(i) for i in b_l0b.shape]
        if para_map.get("requant_n_odd_flag"):
            b_l0b_shape[1] += 1
        b_l0b_shape = self._get_dynamic_l0b_shape(b_l0b_shape)
        b_l0b_shape = self._update_fp32_l0_shape(b_l0b_shape, "b_l0b", b_l0b.dtype)
        # get bl0_tiling_shape
        kb_multi_factor = 2 if sparse_4to2_flag else 1
        bl0_tiling_shape = [self.tiling_work.bl0_tiling_nb, self.tiling_work.bl0_tiling_n0,
                            self.tiling_work.bl0_tiling_kb * kb_multi_factor, self.tiling_work.bl0_tiling_k0]
        # get cl0_tiling_shape
        cl0_tiling_shape = [self.tiling_work.cl0_tiling_nc, self.tiling_work.cl0_tiling_n0,
                            self.tiling_work.c_col_k1, self.tiling_work.c_col_k0]
        if para_map.get("have_batch_b"):
            tiling_ori_l0b.insert(0, self.tiling_work.bl0_tiling_batch)
            bl0_tiling_shape.insert(0, self.tiling_work.bl0_tiling_batch)
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
        # get bl0_attach_flag
        status_ori = Compare.compare(tiling_ori_l0b, b_l0b_shape)
        status = Compare.compare(bl0_tiling_shape, cl0_tiling_shape)
        bl0_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if bl0_attach_flag == 1:
            bl0_attach_flag = self.ATTACH_MAP.get(status)
        self.tiling_work.tiling["attach_at_flag"]["bl0_attach_flag"] = bl0_attach_flag

    def _get_al0_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        sparse_4to2_flag = para_map.get("alg", None)
        # get tiling_ori_l0a
        tiling_ori_l0a = [self.tiling_work.al0_tiling_ma, self.tiling_work.al0_tiling_ka,
                          self.tiling_work.al0_tiling_m0, self.tiling_work.al0_tiling_k0]
        if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
            tiling_ori_l0a = [self.tiling_work.al0_tiling_ka, self.tiling_work.al0_tiling_ma,
                              self.tiling_work.al0_tiling_m0, self.tiling_work.al0_tiling_k0]
        # get a_l0a_shape
        a_l0a = tensor_map.get("a_l0a")
        if para_map.get("mmad_mode") == "gemv":
            a_l0a = tensor_map.get("b_l0b")
        else:
            a_l0a = tensor_map.get("a_l0a")
        a_l0a_shape = [get_value(i) for i in a_l0a.shape]
        if para_map.get("requant_m_odd_flag"):
            a_l0a_shape[0] += 1
        a_l0a_shape = self._get_dynamic_l0a_shape(a_l0a_shape)
        a_l0a_shape = self._update_fp32_l0_shape(a_l0a_shape, "a_l0a", a_l0a.dtype)
        # get al0_tiling_shape
        al0_tiling_shape = [self.tiling_work.al0_tiling_ma, self.tiling_work.al0_tiling_m0,
                            self.tiling_work.al0_tiling_ka, self.tiling_work.al0_tiling_k0]
        # get cl0_tiling_shape
        cl0_tiling_shape = [self.tiling_work.cl0_tiling_mc, self.tiling_work.cl0_tiling_m0,
                            self.tiling_work.c_col_k1, self.tiling_work.c_col_k0]
        if para_map.get("have_batch_a"):
            tiling_ori_l0a.insert(0, self.tiling_work.al0_tiling_batch)
            al0_tiling_shape.insert(0, self.tiling_work.al0_tiling_batch)
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
        if para_map.get("mad_pattern") == tbe_platform.GEVM_MODE:
            tiling_ori_l0a[-2] = 1
        # in this condition, ori_k less than 32, so ka must be changed to 2, for make them equal
        if (sparse_4to2_flag and a_l0a_shape[1] == 1):
            tiling_ori_l0a[1] = 1
        # get al0_attach_flag
        status_ori = Compare.compare(tiling_ori_l0a, a_l0a_shape)
        status = Compare.compare(al0_tiling_shape, cl0_tiling_shape)
        al0_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if al0_attach_flag == 1:
            al0_attach_flag = self.ATTACH_MAP.get(status)
        self.tiling_work.tiling["attach_at_flag"]["al0_attach_flag"] = al0_attach_flag

    def _update_fp32_l0_shape(self, shape, tensor_name, dtype):
        """
        update L0 tensor shape for fp32 input because in tiling info dict, som unaligned cases aligned by 2
        """
        para_map = self.tensor_list[-1]
        if dtype != "float32" or not (para_map.get("trans_a") or not para_map.get("trans_b")):
            return shape
        if tensor_name == "a_l0a":
            k_index = self.FRACTAL_Z_KA_INDEX
        else:
            k_index = self.FRACTAL_Z_KB_INDEX
        shape[k_index] = int_ceil_align(shape[k_index], K_AXIS_ALIGN_FACTOR)
        return shape

    def _get_dynamic_l0a_shape(self, l0a_shape):
        if in_dynamic():
            dynamic_m, dynamic_k, _, dynamic_batch = self.dynamic_seed_shape
            para_map = self.tensor_list[-1]
            if self.cache_tiling_mgr.cache_tiling:
                l0a_shape = [dynamic_m, dynamic_k] + l0a_shape[:-2]
            else:
                dynamic_m = int_ceil_div(dynamic_m, self.tiling_work.tiling.get("block_dim")[2])
                l0a_shape = [dynamic_m, dynamic_k, self.tiling_work.block_in, self.tiling_work.block_reduce]
            if para_map.get("have_batch_a"):
                dynamic_batch = int_ceil_div(dynamic_batch, self.tiling_work.tiling.get("block_dim")[0])
                l0a_shape.insert(0, dynamic_batch)
        return l0a_shape

    def _get_cl0_attach_flag(self, c_ub_tiling_shape):
        if c_ub_tiling_shape is None:
            return
        tensor_map, para_map = self.tensor_list[3:]
        # get cl0_tiling_shape
        cl0_tiling_nc, cl0_tiling_mc = self.tiling_work.cl0_tiling_nc, self.tiling_work.cl0_tiling_mc
        cl0_tiling_m0, cl0_tiling_n0 = self.tiling_work.cl0_tiling_m0, self.tiling_work.cl0_tiling_n0
        cl0_tiling_shape = [cl0_tiling_nc, cl0_tiling_mc, cl0_tiling_m0, cl0_tiling_n0]
        if para_map.get("have_batch"):
            cl0_tiling_shape.insert(0, self.tiling_work.cl0_tiling_batch)
        # get c_l0c_shape
        c_l0c = tensor_map.get("c_l0c")
        c_l0c_shape = [get_value(i) for i in c_l0c.shape]
        if para_map.get("requant_n_odd_flag"):
            c_l0c_shape[0] += 1
        if para_map.get("requant_m_odd_flag"):
            c_l0c_shape[1] += 1
        c_l0c_shape = self._get_dynamic_l0c_shape(c_l0c_shape, para_map.get("have_batch"))
        # get c_ub_tiling_shape
        if para_map.get("mmad_mode") in ("gemv", "gevm"):
            # add for dsl_mat_d_elm-eltwise-ut8_NZ_0054
            c_ub_tiling_shape[-2] = 16
        # get cl0_attach_flag
        status_ori = Compare.compare(cl0_tiling_shape, c_l0c_shape)
        status = Compare.compare(cl0_tiling_shape, c_ub_tiling_shape)
        cl0_attach_flag = self.ATTACH_ORI_MAP.get(status_ori)
        if cl0_attach_flag == 1:
            cl0_attach_flag = self.ATTACH_MAP.get(status)
        self.tiling_work.tiling["attach_at_flag"]["cl0_attach_flag"] = cl0_attach_flag

    def _get_dynamic_l0c_shape(self, l0c_shape, have_batch):
        if in_dynamic():
            dynamic_m, _, dynamic_n, dynamic_batch = self.dynamic_seed_shape
            if self.cache_tiling_mgr.cache_tiling:
                l0c_shape = [dynamic_n, dynamic_m] + l0c_shape[:-2]
            else:
                dynamic_n = int_ceil_div(dynamic_n, self.tiling_work.tiling.get("block_dim")[1])
                dynamic_m = int_ceil_div(dynamic_m, self.tiling_work.tiling.get("block_dim")[2])
                l0c_shape = [dynamic_n, dynamic_m, self.tiling_work.block_in, self.tiling_work.block_out]
            if have_batch:
                dynamic_batch = int_ceil_div(dynamic_batch, self.tiling_work.tiling.get("block_dim")[0])
                l0c_shape.insert(0, dynamic_batch)
        return l0c_shape

    def _get_cub_attach_flag(self):
        tensor_map, para_map = self.tensor_list[3:]
        c_ub_fract = tensor_map.get("c_ub_fract")
        if para_map.get("has_ub_fusion") and self.support_l0c2out:
            c_ub_fract = tensor_map.get("tensor_c_gm")
        if c_ub_fract is None:
            return None
        # get c_ub_tiling_shape
        cub_tiling = self.tiling_work.tiling.get("CUB_matrix")
        cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0, cub_tiling_batch, _ = cub_tiling
        self.tiling_work.cub_tiling_batch = cub_tiling_batch
        res = tensor_map.get("c_gm")
        if res.dtype == "int8":
            cub_tiling_nc_factor = int_ceil_div(cub_tiling_nc_factor, 2)
        # default is nz format
        c_ub_tiling_shape = [cub_tiling_nc_factor, cub_tiling_mc_factor, cub_tiling_m0, cub_tiling_n0]
        # get c_ub_shape
        c_ub_shape = [get_value(i) for i in c_ub_fract.shape]
        if tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_post_transform_nz2nd") \
                and para_map.get("format_out") == "ND":
            nc_factor_shape = int_ceil_div(c_ub_shape[N_INDEX], cub_tiling_n0)
            mc_factor_shape = int_ceil_div(c_ub_shape[M_INDEX], cub_tiling_m0)
            c_ub_shape_nz = [nc_factor_shape, mc_factor_shape, cub_tiling_m0, cub_tiling_n0]
            if para_map.get("have_batch"):
                c_ub_shape_nz.insert(0, cub_tiling_batch)
            c_ub_shape = c_ub_shape_nz
        if para_map.get("format_out") == "FRACTAL_NZ":
            self._fix_c_ub_shape(c_ub_shape)
        c_ub_shape = self._get_dynamic_cub_shape(c_ub_shape)
        if para_map.get("have_batch"):
            c_ub_tiling_shape.insert(0, cub_tiling_batch)
        if para_map.get("mmad_mode") in ("gemv", "gevm"):
            c_ub_tiling_shape[-2] = 1
        # get cub_attach_flag
        status = Compare.compare(c_ub_tiling_shape, c_ub_shape)
        self.tiling_work.tiling["attach_at_flag"]["cub_attach_flag"] = self.ATTACH_MAP.get(status)
        return c_ub_tiling_shape

    def _fix_c_ub_shape(self, c_ub_shape):
        # 4 is the index of para_map
        para_map = self.tensor_list[4]

        if para_map.get("requant_n_odd_flag_cub"):
            c_ub_shape[NC_FACTOR_INDEX] += 1
        if para_map.get("requant_m_odd_flag_cub"):
            c_ub_shape[MC_FACTOR_INDEX] += 1

    def _get_dynamic_cub_shape(self, cub_shape):
        if in_dynamic():
            dynamic_m, _, dynamic_n, dynamic_batch = self.dynamic_seed_shape
            para_map = self.tensor_list[-1]
            if self.cache_tiling_mgr.cache_tiling:
                cub_shape = [dynamic_n, dynamic_m] + cub_shape[:-2]
            else:
                dynamic_n = int_ceil_div(dynamic_n, self.tiling_work.tiling.get("block_dim")[1])
                dynamic_m = int_ceil_div(dynamic_m, self.tiling_work.tiling.get("block_dim")[2])
                cub_shape = [dynamic_n, dynamic_m, self.tiling_work.block_in, self.tiling_work.block_out]
            if para_map.get("have_batch"):
                dynamic_batch = int_ceil_div(dynamic_batch, self.tiling_work.tiling.get("block_dim")[0])
                cub_shape.insert(0, dynamic_batch)
        return cub_shape

    def _no_solution_tiling(self):
        """
        Determining that there is no solution to tilling and change tiling to default
        """
        tensor_map, para_map = self.tensor_list[3:]
        if self.tiling.get("AL0_matrix") == [1, 1, 32, 16, 1, 1]:
            multi_m, multi_n = 1, 1
            src_dtype = tensor_map.get("a_placehold").dtype
            dst_dtype = tensor_map.get("c_gm").dtype
            format_info = {"a": para_map.get("format_a"), "b": para_map.get("format_b"),
                           "out": para_map.get("format_out")}
            if src_dtype in ("uint8", "int8") and dst_dtype == "int32":
                multi_m, multi_n = 2, 2
            if para_map.get("int8_not_double_m") or format_info.get("a") != "ND":
                multi_m = 1
            if format_info.get("b") != "ND":
                multi_n = 1
            block_reduce = self.tiling_work.block_reduce
            block_in = self.tiling_work.block_in
            block_out = self.tiling_work.block_out
            self.tiling = {
                'AUB_shape': [block_reduce, multi_m, 1, 1],
                'BUB_shape': [block_reduce, multi_n, 1, 1],
                'AL1_shape': [block_reduce, 1, 1, 1],
                'BL1_shape': [block_reduce, 1, 1, 1],
                'AL0_matrix': [multi_m, 1, block_in, block_reduce, 1, 1],
                'BL0_matrix': [1, multi_n, block_out, block_reduce, 1, 1],
                'CL0_matrix': [multi_n, multi_m, block_in, block_out, 1, 1],
                'CUB_matrix': [multi_n, multi_m, block_in, block_out, 1, 1],
                'block_dim': [1, 1, 1, 1],
                'n_bef_batch_flag': 0, 'n_bef_group_flag': 0, 'batch_bef_group_flag': 0,
                'A_overhead_opt_flag': 0, 'B_overhead_opt_flag': 0,
                'AUB_channel_wise_flag': None, 'BUB_channel_wise_flag': None,
                'CUB_channel_wise_flag': 0,
                'manual_pingpong_buffer':
                    {'AUB_pbuffer': 1,
                    'BUB_pbuffer': 1,
                    'AL1_pbuffer': 1,
                    'BL1_pbuffer': 1,
                    'AL0_pbuffer': 1,
                    'BL0_pbuffer': 1,
                    'CL0_pbuffer': 1,
                    'CUB_pbuffer': 1,
                    'UBG_pbuffer': 2
                    },
                "attach_same_to_static": False
            }
            if para_map.get("mad_pattern") == tbe_platform.GEVM_MODE:
                self.tiling["AUB_shape"] = [block_reduce * block_in, 1, 1, 1]
                self.tiling["AL1_shape"] = [block_reduce * block_in, 1, 1, 1]
                self.tiling["BL1_shape"] = [block_reduce * block_in, 1, 1, 1]
                self.tiling["AL0_matrix"] = [1, block_in, 1, block_reduce, 1, 1]
                self.tiling["BL0_matrix"] = [block_in, 1, block_out, block_reduce, 1, 1]
            b_ub = tensor_map.get("b_ub")
            if (b_ub is not None) and b_ub.dtype in ("int8", "uint8"):
                self.tiling.get("BUB_shape")[1] = self.tiling.get("BUB_shape")[1] * 2

    def _gemv_tiling(self):
        para_map = self.tensor_list[-1]
        if para_map.get("mmad_mode") != "gemv":
            return
        self.tiling["AUB_shape"], self.tiling["BUB_shape"] = self.tiling.get("BUB_shape"), self.tiling.get("AUB_shape")
        self.tiling["AL1_shape"], self.tiling["BL1_shape"] = self.tiling.get("BL1_shape"), self.tiling.get("AL1_shape")
        block_dim = self.tiling.get("block_dim")
        block_dim[1] = 1
        self.tiling["block_dim"] = block_dim

    def _check_k_full_load(self):
        if not in_dynamic() or "attach_at_flag" in self.tiling:
            return
        block_reduce = self.tiling_work.block_reduce
        _, dynamic_k, _, _ = self.dynamic_seed_shape
        if self.tiling.get("AL1_shape") != []:
            al1_k = self.tiling.get("AL1_shape")[0] // block_reduce
            if al1_k != dynamic_k:
                self.tiling.get("AL1_shape")[1] = 1
        if self.tiling.get("BL1_shape") != []:
            bl1_k = self.tiling.get("BL1_shape")[0] // block_reduce
            if bl1_k != dynamic_k:
                self.tiling.get("BL1_shape")[1] = 1

    def _check_tiling_value(self):
        if len(self.tiling.get("block_dim")) != 4:
            error_manager_cube.raise_err_message_cube("block_dim should be 4 dim")
        batch_dim, n_dim, m_dim, reduce_dim = self.tiling.get("block_dim")
        is_valid = ((isinstance(batch_dim, int) and batch_dim < 1) or
                    (isinstance(n_dim, int) and n_dim < 1) or
                    (isinstance(m_dim, int) and m_dim < 1) or
                    (isinstance(reduce_dim, int) and reduce_dim < 1))
        if is_valid:
            error_manager_cube.raise_err_message_cube("block_dim cannot be less than 1")

    def _tiling_l0_process(self):
        tensor_map, para_map = self.tensor_list[3:]
        if para_map.get("mmad_mode") == "gemv":
            a_l0a = tensor_map.get("b_l0b")
            b_l0b = tensor_map.get("a_l0a")
        else:
            a_l0a = tensor_map.get("a_l0a")
            b_l0b = tensor_map.get("b_l0b")
        if self.tiling_work.tiling.get("BL0_matrix") != []:
            (self.tiling_work.bl0_tiling_kb,
             self.tiling_work.bl0_tiling_nb,
             self.tiling_work.bl0_tiling_n0,
             self.tiling_work.bl0_tiling_k0,
             self.tiling_work.bl0_tiling_batch,
             _) = self.tiling_work.tiling.get("BL0_matrix")
        else:
            b_l0b_shape = [get_value(i) for i in b_l0b.shape]
            b_l0b_shape = self._get_dynamic_l0b_shape(b_l0b_shape)
            (self.tiling_work.bl0_tiling_kb,
             self.tiling_work.bl0_tiling_nb,
             self.tiling_work.bl0_tiling_n0,
             self.tiling_work.bl0_tiling_k0) = b_l0b_shape[-4:]
            # full load only loads 1 batch
            self.tiling_work.bl0_tiling_batch = 1
            self.tiling_work.bl0_tiling_nb = self.tiling_work.tiling.get("CL0_matrix")[0]
            self.tiling_work.bl0_tiling_kb = self.tiling_work.tiling.get("AL0_matrix")[1]
        self.tiling_work.bl0_tiling_k0 = self.tiling_work.block_reduce
        (self.tiling_work.al0_tiling_ma,
         self.tiling_work.al0_tiling_ka,
         self.tiling_work.al0_tiling_m0,
         self.tiling_work.al0_tiling_k0,
         self.tiling_work.al0_tiling_batch,
         _) = self.tiling_work.tiling.get("AL0_matrix")
        self.tiling_work.al0_tiling_k0 = self.tiling_work.block_reduce
        (self.tiling_work.cl0_tiling_nc,
         self.tiling_work.cl0_tiling_mc,
         self.tiling_work.cl0_tiling_m0,
         self.tiling_work.cl0_tiling_n0,
         self.tiling_work.cl0_tiling_batch,
         _) = self.tiling_work.tiling.get("CL0_matrix")
        c_l0c = tensor_map.get("c_l0c")
        self._get_l0c_reduce_dims()
        self.tiling_work.al0_tiling_m0 = tbe_platform.CUBE_MKN[a_l0a.dtype]["mac"][0]
        self.tiling_work.al0_tiling_k0 = tbe_platform.CUBE_MKN[a_l0a.dtype]["mac"][1]
        self.tiling_work.bl0_tiling_k0 = tbe_platform.CUBE_MKN[b_l0b.dtype]["mac"][1]
        self.tiling_work.bl0_tiling_n0 = tbe_platform.CUBE_MKN[b_l0b.dtype]["mac"][2]
        self.tiling_work.cl0_tiling_m0 = tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][0]
        self.tiling_work.cl0_tiling_n0 = tbe_platform.CUBE_MKN[c_l0c.dtype]["mac"][2]
        # special handle
        if in_dynamic() and not self.cache_tiling_mgr.cache_tiling:
            # normal dynamic shape does not support multi batch
            self.tiling_work.al0_tiling_batch = 1
            self.tiling_work.bl0_tiling_batch = 1
            self.tiling_work.cl0_tiling_batch = 1

    def _get_dynamic_l0b_shape(self, l0b_shape):
        if in_dynamic():
            _, dynamic_k, dynamic_n, dynamic_batch = self.dynamic_seed_shape
            para_map = self.tensor_list[-1]
            if self.cache_tiling_mgr.cache_tiling:
                l0b_shape = [dynamic_k, dynamic_n, self.tiling_work.block_out, self.tiling_work.block_reduce]
            else:
                dynamic_n = int_ceil_div(dynamic_n, self.tiling_work.tiling.get("block_dim")[1])
                l0b_shape = [dynamic_k, dynamic_n, self.tiling_work.block_out, self.tiling_work.block_reduce]
            if para_map.get("have_batch_b"):
                dynamic_batch = int_ceil_div(dynamic_batch, self.tiling_work.tiling.get("block_dim")[0])
                l0b_shape.insert(0, dynamic_batch)
        return l0b_shape

    def _get_l0c_reduce_dims(self):
        tensor_map = self.tensor_list[3]
        c_l0c = tensor_map.get("c_l0c")
        _, dynamic_k, _, _ = self.dynamic_seed_shape
        self.tiling_work.c_col_k1, self.tiling_work.c_col_k0 = \
            [get_value(ax.dom.extent) for ax in c_l0c.op.reduce_axis]
        para_map = self.tensor_list[-1]
        if para_map.get("b_type") == "float32" and (para_map.get("trans_a") or not para_map.get("trans_b")):
            self.tiling_work.c_col_k1 = int_ceil_align(self.tiling_work.c_col_k1, K_AXIS_ALIGN_FACTOR)
        if in_dynamic():
            self.tiling_work.c_col_k1 = dynamic_k
        self.tiling_work.c_col_k1 = int_ceil_div(self.tiling_work.c_col_k1,
                                                 self.tiling_work.tiling.get("block_dim")[3])

    def _tiling_l1_process(self):
        self._tiling_al1_process()
        self._tiling_bl1_process()

    def _tiling_al1_process(self):
        tensor_map, para_map = self.tensor_list[3:]
        tiling = self.tiling_work.tiling
        dynamic_m, dynamic_k, _, dynamic_batch = self.dynamic_seed_shape
        al1_tiling_batch = 1
        if tiling.get("AL1_shape") != [] and (tiling.get("AL1_shape") is not None):
            al1_tiling_k, al1_tiling_m, al1_tiling_batch, _ = tiling.get("AL1_shape")
            al1_tiling_m *= self.tiling_work.al0_tiling_ma
        else:
            if in_dynamic():
                al1_tiling_k = dynamic_k * self.tiling_work.block_reduce
                al1_ma = dynamic_m
            else:
                if para_map.get("mmad_mode") != "gemv":
                    al0_shape = [get_value(i) for i in tensor_map.get("a_l0a").shape]
                    al0_shape = self._update_fp32_l0_shape(al0_shape, "a_l0a", tensor_map.get("a_l0a").dtype)
                    al1_tiling_k = al0_shape[-3] * al0_shape[-1]
                    al1_ma = al0_shape[-4]
                    if tbe_platform_info.get_soc_spec("L0A_LAYOUT_IS_zN"):
                        al1_tiling_k = al0_shape[-4] * al0_shape[-1]
                        al1_ma = al0_shape[-3]
                else:
                    al0_shape = [get_value(i) for i in tensor_map.get("b_l0b").shape]
                    al1_tiling_k = al0_shape[-4] * al0_shape[-1]
                    al1_ma = al0_shape[-3]
                al1_m_fix_value = get_al1_m_fix_value(get_value(al1_ma))
                al1_ma //= al1_m_fix_value
            al1_tiling_m = (al1_ma + tiling.get("block_dim")[2] - 1) // tiling.get("block_dim")[2]
            al1_tiling_k = (al1_tiling_k + tiling.get("block_dim")[3] - 1) // tiling.get("block_dim")[3]
            if para_map.get("have_batch_a"):
                al1_tiling_batch = dynamic_batch if in_dynamic() else al1_tiling_batch
        self.tiling_work.al1_tiling_batch = al1_tiling_batch
        if in_dynamic() and not self.cache_tiling_mgr.cache_tiling:
            # normal dynamic shape does not support multi batch
            self.tiling_work.al1_tiling_batch = 1
        self.tiling_work.al1_tiling_k = al1_tiling_k
        self.tiling_work.al1_tiling_m = al1_tiling_m
        al0_tiling_ma = self.tiling_work.al0_tiling_ma
        self.tiling_work.al0_tiling_ma = min(al0_tiling_ma, al1_tiling_m) if not in_dynamic() else al0_tiling_ma
        self.tiling_work.cl0_tiling_mc = self.tiling_work.al0_tiling_ma

    def _tiling_bl1_process(self):
        tensor_map, para_map = self.tensor_list[3:]
        tiling = self.tiling_work.tiling
        _, dynamic_k, dynamic_n, _ = self.dynamic_seed_shape
        bl1_tiling_batch = 1
        if tiling.get("BL1_shape") != [] and (tiling.get("BL1_shape") is not None):
            bl1_tiling_k, bl1_tiling_n, bl1_tiling_batch, _ = tiling.get("BL1_shape")
            bl1_tiling_n *= self.tiling_work.bl0_tiling_nb
        else:
            if in_dynamic():
                bl1_tiling_k = dynamic_k * self.tiling_work.block_reduce
                bl1_n = dynamic_n
            else:
                if para_map.get("mmad_mode") != "gemv":
                    bl0_shape = [get_value(i) for i in tensor_map.get("b_l0b").shape]
                    bl0_shape = self._update_fp32_l0_shape(bl0_shape, "b_l0b", tensor_map.get("b_l0b").dtype)
                    bl1_tiling_k = bl0_shape[-4] * bl0_shape[-1]
                    bl1_n = bl0_shape[-3]
                else:
                    bl0_shape = [get_value(i) for i in tensor_map.get("a_l0a").shape]
                    bl1_tiling_k = bl0_shape[-3] * bl0_shape[-1]
                    bl1_n = bl0_shape[-4]
            bl1_tiling_n = (bl1_n + tiling.get("block_dim")[1] - 1) // tiling.get("block_dim")[1]
            bl1_tiling_k = (bl1_tiling_k + tiling.get("block_dim")[3] - 1) // tiling.get("block_dim")[3]
        self.tiling_work.bl1_tiling_batch = bl1_tiling_batch
        if in_dynamic() and not self.cache_tiling_mgr.cache_tiling:
            # normal dynamic shape does not support multi batch
            self.tiling_work.bl1_tiling_batch = 1
        self.tiling_work.bl1_tiling_k = bl1_tiling_k
        self.tiling_work.bl1_tiling_n = bl1_tiling_n
        bl0_tiling_nb = self.tiling_work.bl0_tiling_nb
        self.tiling_work.bl0_tiling_nb = min(bl0_tiling_nb, bl1_tiling_n) if not in_dynamic() else bl0_tiling_nb
        self.tiling_work.cl0_tiling_nc = self.tiling_work.bl0_tiling_nb

    def _tiling_ub_process(self):
        para_map = self.tensor_list[-1]

        if para_map.get("need_aub") or para_map.get("ops_data_flow_mode") == "int82fp32":
            (self.tiling_work.aub_tiling_k, self.tiling_work.aub_tiling_m,
             self.tiling_work.aub_tiling_batch) = self.tiling_work.tiling.get("AUB_shape")[:3]
            if not self.cache_tiling_mgr.cache_tiling and (not self.tiling_work.aub_tiling_batch or in_dynamic()):
                # normal dynamic shape does not support multi batch
                self.tiling_work.aub_tiling_batch = 1
        else:
            self.tiling_work.aub_tiling_m, self.tiling_work.aub_tiling_k, self.tiling_work.aub_tiling_batch = 0, 0, 0
        if para_map.get("need_bub") or para_map.get("ops_data_flow_mode") == "int82fp32":
            (self.tiling_work.bub_tiling_k, self.tiling_work.bub_tiling_n,
             self.tiling_work.bub_tiling_batch) = self.tiling_work.tiling.get("BUB_shape")[:3]
            if not self.cache_tiling_mgr.cache_tiling and (not self.tiling_work.bub_tiling_batch or in_dynamic()):
                # normal dynamic shape does not support multi batch
                self.tiling_work.bub_tiling_batch = 1
        else:
            self.tiling_work.bub_tiling_k, self.tiling_work.bub_tiling_n, self.tiling_work.bub_tiling_batch = 0, 0, 0
