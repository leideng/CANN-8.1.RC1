#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
auto_schedule template, if user call auto_schedule, this file will choose a
corresponding schedule template for user's compute
"""
import copy
import functools
import json
import os
import stat
from typing import Any
from typing import Callable
from typing import Dict
from typing import List
from typing import Union

from tbe import tvm
from tbe.common import buildcfg
from tbe.common.buildcfg import build_config
from tbe.common.buildcfg import get_current_build_config
from tbe.common.platform import scope_cbuf_fusion
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.register import get_op_compute
from tbe.common.rl_bank import bank_manager
from tbe.common.rl_bank import search_bank
from tbe.common.utils import log
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.base import operation
from tbe.dsl.base.context import OperatorContext
from tbe.dsl.base.record.decorators import schedule as schedule_decorator
from tbe.dsl.base.var import AttrVarDesc
from tbe.dsl.base.var import Category
from tbe.dsl.base.var import Var
from tbe.dsl.classifier.elewise_classifier import ElewisePattern
from tbe.dsl.static_schedule.conv_schedule import check_dyn_quantfuse_doubleout
from tbe.dsl.static_schedule.conv_schedule import reget_tensor_list
from tbe.tvm.driver.cce_build_module import build_fatbin

from . import CompileInfo
from . import Pattern
from . import pattern_parser
from . import util
from .constants import VarAttrMode

CONST = "const"


def is_true(expr, dict_args):
    """
    :param expr: condition
    :param dict_args: error message
    :return: RuntimeError
    """
    if not expr:
        raise RuntimeError(dict_args, get_error_message(dict_args))


def try_match_rl_bank(context: Union[OperatorContext, None], original_outs: List, op_mode: str) -> List:
    """
    try to match rl bank
    :param context: OperatorContext
    :param original_outs: outs
    :param op_mode: op_mode
    :return: rl_schs
    """
    rl_schs = []
    if context is not None:
        try:
            _, rl_schs = search_bank.query_rl_bank(original_outs, op_mode=op_mode)
        except Exception as e:
            log.warn("rl bank switch exception: %s, pass!", e)
            rl_schs = []
        finally:
            pass
    return rl_schs


def _prolong_compute_context(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        operation.get_context().prolong_compute_context()
        try:
            ret = func(*args, **kwargs)
        finally:
            operation.get_context().end_prolonged_compute_context()
        return ret

    return wrapper


def _support_fusion(compute_):
    if compute_ is None:
        return False

    support_fusion_reg = compute_.if_support_fusion()
    if isinstance(support_fusion_reg, bool):
        return support_fusion_reg

    if isinstance(support_fusion_reg, Callable):
        supported = support_fusion_reg()
        if isinstance(supported, bool):
            return supported

    return False


def _write_workspace_info(sch, workspace_list, kernel_name):
    """
    write workspace info
    """

    def write_code(wkspace_dict, kernel_name_list):
        """
        write code
        """
        for kernel_name in kernel_name_list:
            fname = os.path.realpath(kernel_name)
            if not os.path.exists(fname):
                continue
            file_handle = os.open(fname, os.O_RDWR, stat.S_IWUSR | stat.S_IRUSR | stat.S_IRGRP)
            file_wrapper = os.fdopen(file_handle, "w+")
            load_dict = json.load(file_wrapper)
            os.lseek(file_handle, 0, 0)
            # Prepend workspace info for Mix-L2
            if (load_dict.get("workspace") and
                    load_dict.get("workspace").get("num") == len(load_dict.get("workspace").get(
                        "size", [])) == len(load_dict.get("workspace").get("type", []))):
                load_dict["workspace"]["num"] = wkspace_dict.get("workspace").get("num") + load_dict.get(
                    "workspace").get("num")
                load_dict["workspace"]["size"] = wkspace_dict.get("workspace").get("size") + load_dict.get(
                    "workspace").get("size")
                load_dict["workspace"]["type"] = wkspace_dict.get("workspace").get("type") + load_dict.get(
                    "workspace").get("type")
            else:
                load_dict.update(wkspace_dict)
            json.dump(load_dict, file_wrapper, sort_keys=True, indent=4, separators=(',', ':'))
            file_wrapper.close()

    num = len(workspace_list)
    if not num:
        return
    total_size = [-1] * num
    addr_type_list = []
    for tensor_w in workspace_list:
        if sch[tensor_w].scope == scope_cbuf_fusion:
            addr_type_list.append(1)
        else:
            addr_type_list.append(0)

    wkspace_dict = {
        "workspace": {"num": num,
                      "size": total_size,
                      "type": addr_type_list
                      }
    }

    tiling_key = sch.tiling_key
    kernel_meta_dir = get_current_build_config("kernel_meta_parent_dir") + f"/kernel_meta/{kernel_name}"
    kernel_name_ori = os.path.join(kernel_meta_dir, f"{kernel_name}_{tiling_key}.json")
    kernel_name_aic = os.path.join(kernel_meta_dir, f"{kernel_name}_mix_aic_{tiling_key}.json")
    kernel_name_aiv = os.path.join(kernel_meta_dir, f"{kernel_name}_mix_aiv_{tiling_key}.json")
    kernel_name_list = [kernel_name_ori, kernel_name_aic, kernel_name_aiv]
    write_code(wkspace_dict, kernel_name_list)


def _prebuild_process(pattern):
    # prebuild
    op_type = operation.get_context().get_op_type()
    compute = get_op_compute(op_type)

    if not _support_fusion(compute):
        fusion_pattern = Pattern.OPAQUE
    else:
        fusion_pattern = pattern

    if fusion_pattern == Pattern.ELEMWISE:
        classify_pattern = operation.get_context().get_current_compute().get(CompileInfo.PATTERN)
        maybe_broadcast_pattern = [ElewisePattern.BROADCAST_SCALAR, ElewisePattern.SCALAR_BROADCAST,
                                   ElewisePattern.ONE_RANK, ElewisePattern.CONST_SHAPE_DIFF]
        if classify_pattern in maybe_broadcast_pattern:
            fusion_pattern = Pattern.BROADCAST
    special_pattern = operation.get_context().get("_special_pattern")
    fusion_pattern = special_pattern if special_pattern else fusion_pattern
    operation.get_op_context().add_build_res("pattern", fusion_pattern)


def _get_single_process_tiling_ret(tiling_case_ret):
    op_context = operation.get_context()
    op_mpc = op_context.get_op_compile_process_vars()
    idx = op_mpc["idx"]
    num = op_mpc["num"]
    import math
    import sys
    chunk_size = max(math.ceil(len(tiling_case_ret) / num), 1)
    empty_process_num = (chunk_size * num - len(tiling_case_ret)) // chunk_size
    if empty_process_num >= 1:
        op_mpc["num"] = num - empty_process_num

    tiling_case_ret = tiling_case_ret[(idx * chunk_size): (idx * chunk_size + chunk_size)]
    if len(tiling_case_ret) == 0:
        if num > 1:
            raise RuntimeError("No schedule to build.")

    return tiling_case_ret, idx, chunk_size


def _check_int4_is_placeholder(outs):
    compute_type_size_map, compute_type_tensor_map = pattern_parser.get_dfs_compute(outs)
    int4_tensor_list = []
    for compute_type, tensors in compute_type_tensor_map.items():
        for tensor in tensors:
            data_type = tensor.dtype
            if data_type == "int4":
                int4_tensor_list.append(tensor)
    if int4_tensor_list and not all(isinstance(tensor.op, tvm.PlaceholderOp) for tensor in int4_tensor_list):
        raise RuntimeError("A tensor with data type int4 is not PLACEHOLDER")


@schedule_decorator.schedule
@_prolong_compute_context
def schedule_cce(outs, option=None):
    """
    :param outs:
    :param option:
    :return:
    """
    # rl set op res
    bank_manager.set_op_res(outs)

    original_outs = list(outs) if isinstance(outs, (list, tuple)) else [outs]
    original_outs = reget_tensor_list(original_outs)
    pattern = pattern_parser.get_pattern(outs)
    # set compute pattern to compile info because const tiling need pattern
    operation.add_compile_info_inner(CompileInfo.PATTERN, pattern)
    # set compute pattern
    operation.get_context().get_current_compute().set_pattern(pattern)
    _check_int4_is_placeholder(outs)

    schedules = []
    context = operation.get_context()
    if get_current_build_config("enable_op_prebuild"):
        prebuild_targets = set(operation.get_op_context().get_addition("prebuild_targets") or [])
        if "pattern" in prebuild_targets:
            _prebuild_process(pattern)
        if len(prebuild_targets - {"pattern"}) == 0:
            return None
        context.add("need_build_on_prebuild_process", True)
    else:
        # try to match rl bank
        op_mode = context.get_mode()
        rl_schs = try_match_rl_bank(context, original_outs, op_mode)
        if rl_schs and op_mode == "static":
            context.add("hit_static_rl_bank", True)
            return rl_schs
        elif rl_schs and op_mode == "dynamic":
            schedules.extend(rl_schs)

    tiling_case_func = operation.get_tiling_case(pattern)
    tiling_case_ret = tiling_case_func(original_outs, option)
    if not tiling_case_ret:
        log.warn("tiling_cases is empty!")
        return schedules

    tiling_case_ret, idx, chunk_size = _get_single_process_tiling_ret(tiling_case_ret)
    schedule_func = operation.get_schedule(pattern)
    for i, tiling_case in enumerate(tiling_case_ret):
        param_outs = original_outs.copy()
        with operation.schedule() as context:
            context.add("_sch_idx", i + idx * chunk_size)
            if Pattern.CONV2D == pattern:
                sch, real_outs = schedule_func(param_outs, tiling_case)
                ori_outs = real_outs
                param_outs = real_outs
            elif Pattern.MAT_MUL == pattern:
                sch, real_outs, spec_mid_list = schedule_func(param_outs, tiling_case)
                ori_outs = original_outs
                param_outs = real_outs
                util.add_sch_cce_special_entry(sch, "tensor_list", spec_mid_list)
                util.add_sch_cce_special_entry(sch, "real_out_tensor", real_outs)
                util.add_sch_cce_special_entry(sch, "original_outs", original_outs)
            else:
                sch = schedule_func(param_outs, tiling_case)
                ori_outs = original_outs
            if sch is not None:
                util.add_sch_additional_entry(sch, "original_outs", ori_outs)
                util.add_sch_additional_entry(sch, "real_outs", param_outs)
                util.add_sch_additional_entry(sch, "context", context)
        schedules.append(sch)

    return schedules


# 'pylint: disable=R0914
def build(schedules_list, config_map=None):
    """
    :param schedules_list:
    :param config_map:
    :return:
    """
    # rl set tensor list
    tensor_list = [] if config_map is None else config_map.get("tensor_list", [])
    bank_manager.set_tensor_list(tensor_list)

    # if CUBE_VECTOR_SPLIT is not empty, the prebuild process goes to the pass side
    if get_current_build_config("enable_op_prebuild") and \
            not operation.get_context().get("need_build_on_prebuild_process"):
        # to match schedule condition
        return

    def get_op_pattern_from_computes():
        """
        get pattern from compute contexts, broadcast and eletwise fix pattern
        use broadcast pattern
        :return:
        """

        # get computes
        op_computes = operation.get_context().get_computes()
        compute_patterns = set()
        for compute in op_computes:
            compute_patterns.add(compute.get_pattern())

        if {Pattern.ELEMWISE, Pattern.BROADCAST} == compute_patterns:
            return Pattern.BROADCAST

        return list(compute_patterns)[-1]

    pattern = get_op_pattern_from_computes()

    int4_index_list = []
    if pattern in ("ElemWise", "BroadCast"):
        tensors = tensor_list[0]
        for index, tensor in enumerate(tensors):
            if tensor.dtype == "int4":
                int4_index_list.append(index)
    if int4_index_list:
        operation.add_compile_info_inner("_int4_index_list", int4_index_list)
    # update op pattern
    operation.get_context().set_pattern(pattern)
    operation.add_compile_info_inner(CompileInfo.PATTERN, pattern)

    if operation.get_context().get("hit_static_rl_bank"):
        _build(schedules_list, config_map)
        return

    pointcut_func = operation.get_build_pointcut(pattern)
    if pointcut_func is not None:
        pointcut_func(_build, schedules_list, config_map)
    else:
        _build(schedules_list, config_map)


def _build(schedules_list, config_map):
    Builder(schedules_list, config_map).build()


class Builder:
    """
    class for build
    """

    def __init__(self, schedules_list, config_map):
        self.schedules_list = schedules_list
        self.config_map = config_map

        self.schedules = self._normalize_schedules()
        self.tensors = self._normalize_tensors()

        self.bank_info = {}

        self.te_vars_list = None
        self.ebv_list = None
        self.exclude_buildargs_vars_list = None
        self.attr_vars_desc_list = None
        self.sch_tensors_list = None
        self.block_sync_tiling_keys = []

    @staticmethod
    def _handle_special_tensors(sch):
        """
        get workspace tensor from schedule
        """
        if not hasattr(sch, "cce_special"):
            return []
        special_tensor_list = sch.cce_special.get("tensor_list", [])
        return special_tensor_list

    def build(self):
        """
        build all schedules and deal with some addition information
        """
        self._traverse_context()
        self._traverse_schedules()
        self._call_tvm_build()
        self._handle_workspace_info()
        op_context = operation.get_context()
        if op_context.get("_use_cache_tiling") is None or not op_context.get("_use_cache_tiling"):
            self._handle_compile_info()
        self._handle_addition()

    def _normalize_schedules(self):
        schedules = []
        for sub_schs in self.schedules_list:
            if isinstance(sub_schs, list):
                schedules.extend(sub_schs)
            else:
                schedules.append(sub_schs)

        return schedules

    def _normalize_tensors(self):
        op_context = operation.get_context()
        cpt_contexts = op_context.get_computes()
        tensors = list(self.config_map["tensor_list"])
        if len(cpt_contexts) == 1:
            if not (len(tensors) == 1 and isinstance(tensors[0], (tuple, list))):
                tensors = [tensors]

        is_true(len(cpt_contexts) == len(tensors),
                {"errCode": "E90001",
                 "detailed_cause": f"compute size {len(cpt_contexts)} and build tensors num {len(tensors)} not match!"})

        return tensors

    def _traverse_context(self):
        op_context = operation.get_context()

        op_vars = op_context.get_vars()
        op_ebv = op_context.get_exclude_bound_vars()
        op_ebav = op_context.get_exclude_buildargs_vars()
        op_attr_vars_desc = op_context.get_attr_vars_desc()

        # 'ebv' means 'exclude bound vars'
        te_vars_list, ebv_list, ebav_list, attr_vars_desc_list, sch_tensors_list = [], [], [], [], []
        bank_info = {}
        schedule_index = 0
        for i, cpt in enumerate(op_context.get_computes()):
            if cpt.get("_bank_info") is not None:
                bank_info.update(cpt.get("_bank_info"))
            cpt_vars = cpt.get_vars()
            cpt_ebv = cpt.get_exclude_bound_vars()
            cpt_ebav = cpt.get_exclude_buildargs_vars()
            cpt_attr_vars_desc = cpt.get_attr_vars_desc()
            for sch_context in cpt.get_schedules():
                sch_vars = sch_context.get_vars()
                sch_ebv = sch_context.get_exclude_bound_vars()
                sch_ebav = sch_context.get_exclude_buildargs_vars()
                sch_attr_vars_desc = sch_context.get_attr_vars_desc()

                te_vars_list.append(op_vars + cpt_vars + sch_vars)
                ebv_list.append(op_ebv + cpt_ebv + sch_ebv)
                ebav_list.append(op_ebav + cpt_ebav + sch_ebav)
                attr_vars_desc_list.append(op_attr_vars_desc + cpt_attr_vars_desc + sch_attr_vars_desc)
                sch_tensors_list.append(list(self.tensors[i]))

                if sch_context.get("_invalid_schedule") is True:
                    self.schedules[schedule_index] = None

                if sch_context.get("_block_sync") is not None:
                    self.block_sync_tiling_keys.append(sch_context.get("_block_sync"))

                schedule_index += 1

        lens = [len(self.schedules), len(te_vars_list), len(ebv_list), len(ebav_list)]
        is_true(len(set(lens)) == 1,
                {"errCode": "E90001",
                 "detailed_cause": f"The size of schedule, var, and var_bound does not match, they are {lens}."})

        self.bank_info = bank_info
        self.te_vars_list = te_vars_list
        self.ebv_list = ebv_list
        self.exclude_buildargs_vars_list = ebav_list
        self.attr_vars_desc_list = attr_vars_desc_list
        self.sch_tensors_list = sch_tensors_list

    def _traverse_schedules(self):
        args_list, tiling_keys, valid_schs = [], [], []
        compile_vars, compile_normal_vars, compile_attr_vars, compile_custom_vars = {}, {}, {}, {}

        for sch, te_vars, ebv, ebav, attr_vars_desc, sch_tensors in zip(self.schedules,
                                                                  self.te_vars_list,
                                                                  self.ebv_list,
                                                                  self.exclude_buildargs_vars_list,
                                                                  self.attr_vars_desc_list,
                                                                  self.sch_tensors_list):
            if sch is None:
                continue

            valid_schs.append(sch)
            real_sch_tensors = self._handle_tensors(sch_tensors, sch)
            special_tensor_list = self._handle_special_tensors(sch)
            var_groups = self._group_vars(te_vars)
            normal_vars = var_groups.get(Category.NORMAL, [])
            attr_vars = var_groups.get(Category.ATTR, [])
            custom_vars = var_groups.get(Category.CUSTOM, [])
            te_vars = normal_vars + attr_vars + custom_vars

            tvm_vars = [x.get_tvm_var() for x in te_vars]
            bounds = [x.get_bound() for x in te_vars]
            for tvm_var, bound in zip(tvm_vars, bounds):
                need_set_range = tvm_var not in ebv and bound is not None
                if need_set_range:
                    sch.set_var_range(tvm_var, *bound)

            tvm_vars_exclude_buildargs = [x.get_tvm_var() for x in te_vars if x not in ebav]
            args_list.append(real_sch_tensors + special_tensor_list + tvm_vars_exclude_buildargs)
            tiling_keys.append(sch.tiling_key)

            compile_vars[sch.tiling_key] = te_vars
            compile_normal_vars[sch.tiling_key] = normal_vars
            compile_attr_vars[sch.tiling_key] = attr_vars_desc
            compile_custom_vars[sch.tiling_key] = custom_vars

        self.args_list = args_list
        self.tiling_keys = tiling_keys
        self.valid_schs = valid_schs
        self.compile_vars = compile_vars
        self.compile_normal_vars = compile_normal_vars
        self.compile_attr_vars = compile_attr_vars
        self.compile_custom_vars = compile_custom_vars

    # noinspection PyMethodMayBeStatic
    def _group_vars(self, te_vars):
        # type: (List[Var]) -> Dict[Category, List[Var]]
        ret = {}
        for te_var in te_vars:
            ret.setdefault(te_var.get_category(), []).append(te_var)

        return ret

    # noinspection PyMethodMayBeStatic
    def _handle_tensors(self, sch_tensors, sch):
        real_sch_tensors = sch_tensors.copy()
        original_outs = util.get_sch_additional_entry(sch, "original_outs")
        real_outs = util.get_sch_additional_entry(sch, "real_outs")

        if original_outs != real_outs:
            # replace all original_outs with real_outs
            out_start_index = len(real_sch_tensors) - len(original_outs)
            out_end_index = len(real_sch_tensors)
            del real_sch_tensors[out_start_index: out_end_index]
            for tensor_i in real_outs:
                real_sch_tensors.append(tensor_i)
        real_sch_tensors = check_dyn_quantfuse_doubleout(real_sch_tensors, real_outs)
        return real_sch_tensors

    def _call_tvm_build(self):
        # build config use mode: 1 + m + n
        m_config_items = {}
        if operation.in_dynamic():
            m_config_items.update({"parse_ddr_args": True, "build_fatbin": True})
        if 'build_args' in self.config_map:
            m_config_items.update(self.config_map['build_args'])
        elif 'fusion_build_config' in self.config_map:
            m_config_items.update(self.config_map['fusion_build_config'])
        m_config_items.update(operation.get_build_args())

        dynamic_config = buildcfg.default_buildcfg.dynamic_build_config_dict
        with buildcfg.build_config(**dynamic_config):
            upper_config = buildcfg.get_current_build_config("all")
        build_configs = []

        for sch in self.valid_schs:
            dynamic_single_sch_build_config = tvm.deepcopy(upper_config)
            dynamic_single_sch_build_config.update(m_config_items)

            sch_context = util.get_sch_additional_entry(sch, "context")
            n_config_items = sch_context.get("_build_config")
            if n_config_items is not None:
                dynamic_single_sch_build_config.update(n_config_items)

            build_configs.append(build_config(**dynamic_single_sch_build_config))

        if operation.in_dynamic():
            log.info("operation in dynamic, start call build_fatbin")
            build_fatbin(build_config_list=build_configs,
                         schedule_list=self.valid_schs,
                         args_list=self.args_list,
                         rules=self.tiling_keys,
                         name=self.config_map["name"],
                         target_list="cce")
        else:
            log.info("operation in static, start call tvm.build")
            with build_configs[0]:
                tvm.build(self.valid_schs[0],
                          self.args_list[0],
                          target="cce",
                          name=self.config_map["name"]
                          )
        log.info("build end")

    def _handle_compile_info(self):
        def add_vars():
            # key: tiling_key, value: [var_name]
            value = {k: [x.get_name() for x in v] for k, v in self.compile_vars.items()}
            operation.add_compile_info_inner(CompileInfo.VARS, value)

        def add_normal_vars():
            # key: tiling_key, value: [var_name]
            value = {k: [x.get_name() for x in v] for k, v in self.compile_normal_vars.items()}
            operation.add_compile_info_inner(CompileInfo.NORMAL_VARS, value)

        def add_attr_vars():
            def is_all_attr_vars_the_same():
                first_item_flag = True
                first_attr_vars = []
                for k, v in self.compile_attr_vars.items():
                    if first_item_flag:
                        first_attr_vars = v
                        first_item_flag = False
                        continue
                    if len(v) != len(first_attr_vars):
                        return False, []
                    for i, j in enumerate(first_attr_vars):
                        if v[i].dtype != first_attr_vars[i].dtype or v[i].src_dtype != first_attr_vars[i].src_dtype \
                                or v[i].length != first_attr_vars[i].length or v[i].name != first_attr_vars[i].name:
                            return False, []
                return True, first_attr_vars

            def convert(attr_var):
                # type: (AttrVarDesc) -> Dict[str, Any]

                dtype, src_dtype, length = attr_var.dtype, attr_var.src_dtype, attr_var.length

                return {
                    "name": attr_var.name,
                    "index": attr_var.index,
                    "type": dtype,
                    "src_type": src_dtype,
                    "length": length or 1
                }

            all_the_same, single_var_attrs = is_all_attr_vars_the_same()
            if all_the_same:
                if len(single_var_attrs) == 0:
                    return
                else:
                    var_attr_mode = VarAttrMode.CONSISTENT
                    var_attr_value = [convert(x) for x in single_var_attrs]
            else:
                # key: tiling_key, value: [@see convert_attr_var()]
                var_attr_mode = VarAttrMode.INDEPENDENT
                var_attr_value = {k: [convert(x) for x in v] for k, v in self.compile_attr_vars.items()}

            operation.add_compile_info_inner(CompileInfo.VAR_ATTR_MODE, var_attr_mode)
            operation.add_compile_info_inner(CompileInfo.VAR_ATTRS, var_attr_value)

        def add_custom_vars():
            # key: tiling_key, value: [var_name]
            value = {k: [x.get_name() for x in v] for k, v in self.compile_custom_vars.items()}
            operation.add_compile_info_inner(CompileInfo.CUSTOM_VARS, value)

        def add_bank_info():
            """
            for dynamic rl tune, add bank_info into compile_info
            :return:
            """
            if self.bank_info:
                # key: '_bank_info', value: rl tune info
                operation.add_compile_info_inner(CompileInfo.RL_BANK_INFO, self.bank_info)

        def add_block_sync_tiling_keys():
            if self.block_sync_tiling_keys:
                operation.add_compile_info_inner("_block_sync_tiling_keys", self.block_sync_tiling_keys)

        add_vars()
        add_normal_vars()
        add_attr_vars()
        add_custom_vars()
        add_bank_info()
        add_block_sync_tiling_keys()

    def _handle_addition(self):
        operation.get_context().add("_tiling_keys", self.tiling_keys)

    def _handle_workspace_info(self):
        if get_current_build_config("enable_op_prebuild"):
            return
        for sch in self.valid_schs:
            if not hasattr(sch, "cce_special"):
                return
            special_tensor_list = []
            origin_out_tensors = sch.cce_special.get("original_outs")
            for tensor in sch.cce_special.get("tensor_list", []):
                if tensor not in origin_out_tensors and tensor not in self.tensors:
                    special_tensor_list.append(tensor)
            _write_workspace_info(sch, special_tensor_list, self.config_map.get("name"))
