#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
generic vector schedule
"""
from tbe import tvm
from tbe.common.utils import decode
from tbe.common.utils import do_op_tiling
from tbe.dsl.base.operation import add_build_arg
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import get_context

from . import generic_vector_tilingcase as tiling_case
from ..common import schedule_base
from ..common import constants
from ..common import helper
from ..common.info import buffer_size_info
from ..common.info import graph_info
from ..common.info import schedule_node_info
from ..common.info import soc_info
from .... import constants as unify_constants
from .... import util as unify_helper
from ....schedule import Schedule


class EntryGenericVectorSchedule(Schedule):
    """
    entrance to generic vector schedule
    """
    def __init__(self, outs, tiling_case_obj):
        self.outs = outs
        self.tiling_case = tiling_case_obj

    @classmethod
    def get_instance(cls, outs, tiling_case_obj):
        return cls(outs, tiling_case_obj)

    @classmethod
    def get_supported_soc(cls):
        return ["default"]

    @classmethod
    def get_supported_pattern(cls):
        return [unify_constants.Pattern.GENERIC_VECTOR]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [unify_constants.GenericVectorPattern.GV_0]

    def do_schedule(self):
        # Get Compute Graph Info
        current_compute = get_context().get_current_compute()
        compute_graph_info = current_compute.get(constants.ContextKey.COMPUTE_GRAPH_INFO)

        if compute_graph_info.compute_type == constants.ComputeType.REDUCE_AGGREGATE:
            generic_vector_sch = GenericVectorReduceAggregateSchedule(compute_graph_info, self.tiling_case, self.outs)
        else:
            generic_vector_sch = GenericVectorCommonSchedule(compute_graph_info, self.tiling_case, self.outs)

        real_schedule = generic_vector_sch.do_schedule()
        helper.report_fusion_check_result(check_result=real_schedule is not None)

        return real_schedule


class GenericVectorScheduleBase(schedule_base.ScheduleBase):
    """
    generic vector base schedule
    """
    def __init__(self, graph_info_obj: graph_info.GraphInfo, tiling_case_obj: tiling_case.GenericVectorTilingCase,
                 outs):
        schedule_base.ScheduleBase.__init__(self, graph_info_obj, outs)
        self._tiling_case: tiling_case.GenericVectorTilingCase = tiling_case_obj
        self._split_axes: schedule_base.ScheduleSplitAxes = schedule_base.ScheduleSplitAxes()
        self._is_enable_db = unify_helper.is_v220()
        self._max_buffer_size = -1
        self._sch.tiling_key = 0

    @staticmethod
    def _split_axis(stage_obj, split_axis, split_factor, is_post_split=True):
        if split_axis is None or split_factor is None:
            return None, None
        if split_factor < 0:
            return None, None

        actual_split_axis = \
            helper.AxesHelper.get_actual_split_axis(split_axis, stage_obj.split_info_list, is_post_split)

        return stage_obj.unify_split(actual_split_axis, factor=split_factor)

    @staticmethod
    def _do_build_arg_addition():
        add_build_arg("enable_loop_partition", True)
        add_build_arg("enable_precise_sync_in_emit_insn", False)
        add_build_arg("out_of_bound_sync_check", False)
        add_build_arg("double_buffer_non_reuse", True)

    def do_schedule(self):
        self._do_build_arg_addition()
        self._do_cache_read()
        self._do_cache_write()
        self._do_set_scope()
        self._do_mid_output_tensor_process()
        self._do_rfactor()
        self._do_compute_inline()
        self._do_pre_reorder()
        self._do_reused_by()
        self._do_compute_root()
        self._do_categorization()
        self._do_insn_info_selection()
        self._do_max_buffer_size_calculation()
        self._do_const_tiling_calculation()
        self._do_tiling()
        self._do_set_buffer_size()
        self._do_post_reorder()
        self._do_storage_align()
        self._do_double_buffer()
        self._do_compute_at()
        self._do_emit_insn()
        self._do_multi_core()

        return self._sch

    def _do_cache_read(self):
        for sch_node_obj in [n for n in self._sch_node_obj_list if n.tensor_obj.is_placeholder()]:
            self.unify_cache_read(sch_node_obj, constants.UB_SCOPE, sch_node_obj.consumers)

    def _do_cache_write(self):
        for sch_node_obj in [n for n in self._sch_node_obj_list if n.tensor_obj.is_real_non_middle_output()]:
            self.unify_cache_write(sch_node_obj, constants.UB_SCOPE)

    def _do_set_scope(self):
        for sch_node_obj in [n for n in self._sch_node_obj_list if n.tensor_obj.is_local_scope()]:
            sch_node_obj.stage_obj.unify_set_scope(constants.UB_SCOPE)

    def _do_mid_output_tensor_process(self):
        for sch_node_obj in [n for n in self._sch_node_obj_list if n.tensor_obj.is_real_middle_output()]:
            consumer_is_fake_node = sch_node_obj.consumers[0].tensor_obj.is_fake_node()
            cache_write_sch_node_obj = self.unify_cache_write(sch_node_obj, constants.UB_SCOPE)
            if consumer_is_fake_node:
                continue

            # sch_node_obj has been updated
            new_sch_node_obj = cache_write_sch_node_obj.consumers[0]
            cache_read_sch_node_obj = self.unify_cache_read(new_sch_node_obj, constants.UB_SCOPE,
                                                            new_sch_node_obj.consumers)
            cache_write_sch_node_obj.stage_obj.unify_reused_by(cache_read_sch_node_obj.tensor_obj)
            cache_read_sch_node_obj.stage_obj.unify_reused_by(reuse_data=True)
            cache_read_sch_node_obj.insn_obj.insn_name = constants.SpecialInsnName.PHONY_INSN

    def _do_rfactor(self):
        pass

    def _do_compute_inline(self):
        for broadcast_sch_node_obj in [n for n in self._sch_node_obj_list if n.is_broadcast()]:
            broadcast_sch_node_obj.insn_obj.set_all_insn_info()
            if broadcast_sch_node_obj.tensor_obj.is_real_output():
                continue
            if len(broadcast_sch_node_obj.consumers) != 1:
                continue
            consumer_sch_node_obj = broadcast_sch_node_obj.consumers[0]
            if consumer_sch_node_obj.is_reduce():
                continue
            is_support_inline = broadcast_sch_node_obj.stage_obj.is_support_inline()
            is_vbrcb_auto_broadcast = \
                broadcast_sch_node_obj.insn_obj.insn_strategy == constants.BroadcastStrategy.ENABLE_VBRCB and \
                consumer_sch_node_obj.insn_obj.insn_name in constants.ELEWISE_INSN_NAME_SUPPORT_AUTO_BROADCAST
            if is_support_inline or is_vbrcb_auto_broadcast:
                broadcast_sch_node_obj.stage_obj.unify_compute_inline()
                self.post_unify_compute_inline(broadcast_sch_node_obj)

    def _do_pre_reorder(self):
        if self._graph_info.union_reduce_axes_indices:
            self._do_res_pre_reorder()
            self._do_reduce_pre_reorder()

    def _do_res_pre_reorder(self):
        pass

    def _do_reduce_pre_reorder(self):
        for reduce_stage_obj in [n.stage_obj for n in self._sch_node_obj_list if n.is_reduce()]:
            reduce_reorder_perm = helper.AxesHelper.get_reorder_perm_according_to_reduce_axes(
                reduce_stage_obj.before_reduce_shape_length, reduce_stage_obj.reduce_axes_indices
            )
            reduce_reorder_axes = helper.AxesHelper.get_reduce_node_reorder_axes(
                reduce_stage_obj.tvm_stage, reduce_stage_obj.reduce_axes_indices,
                reduce_stage_obj.keep_dims, reduce_reorder_perm
            )
            reduce_stage_obj.unify_reorder(*reduce_reorder_axes)

    def _do_reused_by(self):
        for sch_node_obj in self._sch_node_obj_list:
            sch_node_obj.insn_obj.set_insn_sub_type()

        for sch_node_obj in self._sch_node_obj_list:
            # vector_0 -> cast_0 -> dma_copy_0...dma_copy_n -> cast_1
            # cast0: dtype A -> dtype B
            # cast1: dtype B -> dtype A
            # cast1 can reuse the date of vector_0
            if sch_node_obj.insn_obj.insn_sub_type == constants.CommonType.CAST:
                producer_sch_node_objs_in_ub = helper.traverse_and_select_obj(
                    sch_node_obj, lambda x: x.producers, lambda x: not x.is_data_move()
                )
                if not producer_sch_node_objs_in_ub:
                    continue
                producer_sch_node_obj_in_ub = producer_sch_node_objs_in_ub[0]
                if producer_sch_node_obj_in_ub.insn_obj.insn_sub_type != constants.CommonType.CAST:
                    continue
                reused_sch_node_obj = producer_sch_node_obj_in_ub.producers[0]
                reused_sch_node_obj.stage_obj.unify_reused_by(sch_node_obj.tensor_obj)
                sch_node_obj.stage_obj.unify_reused_by(reuse_data=True)
                sch_node_obj.insn_obj.insn_name = constants.SpecialInsnName.PHONY_INSN

            if sch_node_obj.insn_obj.insn_sub_type == constants.CommonType.SET_VALUE:
                reused_sch_node_obj = sch_node_obj.producers[0]
                reused_sch_node_obj.stage_obj.unify_reused_by(sch_node_obj.tensor_obj)
                sch_node_obj.stage_obj.unify_reused_by(reuse_data=True)
                sch_node_obj.stage_obj.unify_set_store_predicate(sch_node_obj.get_tvm_tensor().op.body[0].condition)

    def _do_compute_root(self):
        compute_root_sch_node_objs = []
        for sch_node_obj in self._sch_node_obj_list:
            if not hasattr(sch_node_obj.get_tvm_stage().op, "axis"):
                continue
            shape_product = helper.calc_iter_vars_extent_product(sch_node_obj.get_tvm_stage().op.axis)
            if shape_product != 1:
                continue
            helper.ListHelper.add(compute_root_sch_node_objs, sch_node_obj)

        for compute_root_sch_node_obj in compute_root_sch_node_objs:
            producer_sch_node_node_objs = helper.traverse_and_select_obj(
                compute_root_sch_node_obj, lambda x: x.producers, lambda x: x.stage_obj.is_ub_scope(),
                is_include_root=True
            )
            if helper.ListHelper.get_difference(producer_sch_node_node_objs, compute_root_sch_node_objs):
                continue
            compute_root_sch_node_obj.stage_obj.unify_compute_root()

    def _do_categorization(self):
        pass

    def _do_max_buffer_size_calculation(self):
        self._set_buffer_size_ratio()
        self._gen_buffer_size_info_obj()
        self._calc_max_buffer_size()

    def _do_insn_info_selection(self):
        self._pre_process_insn_info_for_broadcast()
        self._pre_process_insn_info_for_reduce()

        for sch_node_obj in self._sch_node_obj_list:
            sch_node_obj.insn_obj.set_all_insn_info()

    def _pre_process_insn_info_for_broadcast(self):
        # set rejected broadcast strategy
        broadcast_sch_node_objs = [n for n in self._sch_node_obj_list if n.is_broadcast()]
        for broadcast_sch_node_obj in broadcast_sch_node_objs:
            # broadcast_node -> broadcast_node
            producer_is_broadcast = helper.ListHelper.has_intersection(broadcast_sch_node_obj.producers,
                                                                       broadcast_sch_node_objs)
            consumer_is_broadcast = helper.ListHelper.has_intersection(broadcast_sch_node_obj.consumers,
                                                                       broadcast_sch_node_objs)
            for consumer_sch_node_obj in broadcast_sch_node_obj.consumers:
                consumer_compute_inlined_stage_obj = consumer_sch_node_obj.stage_obj.compute_inlined_stage_obj
                if consumer_compute_inlined_stage_obj and \
                        consumer_compute_inlined_stage_obj.node_type == constants.NodeType.BROADCAST:
                    consumer_is_broadcast = True

            if producer_is_broadcast or consumer_is_broadcast:
                broadcast_sch_node_obj.insn_obj.rejected_strategy_list.append(
                    constants.BroadcastStrategy.ENABLE_VNCHWCONV
                )

    def _pre_process_insn_info_for_reduce(self):
        for reduce_sch_node_obj in [n for n in self._sch_node_obj_list if n.is_reduce()]:
            producer_sch_node_obj = reduce_sch_node_obj.producers[0]
            if len(producer_sch_node_obj.consumers) > 1:
                reduce_sch_node_obj.insn_obj.maybe_appropriate_to_reuse_dst_tensor = False

    def _set_buffer_size_ratio(self):
        self._set_buffer_size_ratio_according_to_dtype()
        self._set_buffer_size_ratio_according_to_shape()
        self._set_buffer_size_ratio_for_scalar_tensor()
        self._update_buffer_size_ratio_for_reuse_tensor()

    def _set_buffer_size_ratio_according_to_dtype(self):
        max_dtype_bytes_num = helper.get_bytes_num(self._graph_info.max_dtype)
        for sch_node_obj in self._sch_node_obj_list:
            dtype_ratio = helper.get_bytes_num(sch_node_obj.tensor_obj.dtype) / max_dtype_bytes_num
            sch_node_obj.buffer_size_ratio *= dtype_ratio
            if sch_node_obj.insn_obj.insn_sub_type != constants.CommonType.CAST:
                sch_node_obj.extra_node_size_ratio *= dtype_ratio

    def _set_buffer_size_ratio_according_to_shape(self):
        processed_shape_list = []
        for reduce_sch_node_obj in [n for n in self._sch_node_obj_list if n.is_reduce()]:
            if reduce_sch_node_obj.insn_obj.is_aggregated:
                continue

            reduce_product = helper.calc_iter_vars_extent_product(reduce_sch_node_obj.get_tvm_stage().op.reduce_axis)
            if reduce_product <= 1:
                continue

            after_reduce_shape = reduce_sch_node_obj.tensor_obj.actual_shape
            if after_reduce_shape in processed_shape_list:
                continue
            processed_shape_list.append(after_reduce_shape)

            for sch_node_obj in self._sch_node_obj_list:
                if not helper.judge_the_equality_of_tvm_shape(sch_node_obj.tensor_obj.actual_shape,
                                                              after_reduce_shape):
                    continue
                if not sch_node_obj.insn_obj.insn_attrs.get(constants.InsnAttrsKey.REUSE_DST_TENSOR):
                    sch_node_obj.buffer_size_ratio /= reduce_product

    def _set_buffer_size_ratio_for_scalar_tensor(self):
        for sch_node_obj in self._sch_node_obj_list:
            if not hasattr(sch_node_obj.get_tvm_stage().op, "axis"):
                continue
            shape_product = helper.calc_iter_vars_extent_product(sch_node_obj.get_tvm_stage().op.axis)
            if shape_product == 1:
                if not sch_node_obj.insn_obj.insn_attrs.get(constants.InsnAttrsKey.REUSE_DST_TENSOR):
                    # it will be aligned to one block
                    sch_node_obj.buffer_size_ratio = 0

    def _update_buffer_size_ratio_for_reuse_tensor(self):
        for reused_sch_node_obj in [n for n in self._sch_node_obj_list if n.stage_obj.reuse_tensor_obj_list]:
            for reuse_tensor_obj in reused_sch_node_obj.stage_obj.reuse_tensor_obj_list:
                reuse_sch_node_obj = self._get_sch_node_obj(tensor_obj=reuse_tensor_obj)
                reuse_sch_node_obj.buffer_size_ratio = reused_sch_node_obj.buffer_size_ratio
                reuse_sch_node_obj.insn_obj.extra_node_size = reused_sch_node_obj.insn_obj.extra_node_size

    def _gen_buffer_size_info_obj(self):
        mem_unique_sch_node_objs = [n for n in self._sch_node_obj_list if n.stage_obj.flags.mem_unique_flag]
        compute_root_sch_node_objs = [n for n in self._sch_node_obj_list if n.stage_obj.flags.compute_root_flag]
        self._buffer_size_info_obj = buffer_size_info.BufferSizeInfo(
            self._res_sch_node_obj, mem_unique_sch_node_objs=mem_unique_sch_node_objs,
            compute_root_sch_node_objs_group=compute_root_sch_node_objs
        )

    def _calc_max_buffer_size(self):
        self._buffer_size_info_obj.calc_max_buffer_size()

        if self._buffer_size_info_obj.max_coexist_node_num > constants.COEXIST_NODE_NUM_UPPER_THRESHOLD_TO_ENABLE_DB:
            self._is_enable_db = False

        if self._is_enable_db:
            self._max_buffer_size = self._buffer_size_info_obj.max_db_buffer_size
        else:
            self._max_buffer_size = self._buffer_size_info_obj.max_buffer_size

    def _do_const_tiling_calculation(self):
        if self._is_enable_db:
            try:
                self._gen_tiling_result()
            except RuntimeError:
                self._is_enable_db = False
                self._max_buffer_size = self._buffer_size_info_obj.max_buffer_size
                self._gen_tiling_result()
        else:
            self._gen_tiling_result()

    def _gen_tiling_result(self):
        self._add_const_tiling_compile_info()
        self._obtain_tiling_data_and_run_info()
        self._check_tiling_and_report_error()

    def _add_const_tiling_compile_info(self):
        add_compile_info_inner(constants.CompileInfoKey.IS_CONST, True)
        add_compile_info_inner(constants.CompileInfoKey.MAX_SHAPE, self._graph_info.max_shape)
        add_compile_info_inner(constants.CompileInfoKey.ORI_REDUCE_AXES, self._graph_info.union_reduce_axes_indices)

        max_buffer_size_ratio = max(n.buffer_size_ratio for n in self._sch_node_obj_list)
        max_buffer_ele_num = int(self._max_buffer_size * max_buffer_size_ratio //
                                 helper.get_bytes_num(self._graph_info.max_dtype))
        add_compile_info_inner(constants.CompileInfoKey.BUFFER_SIZE,
                               {self._graph_info.compute_type.value: [max_buffer_ele_num]})

    def _obtain_tiling_data_and_run_info(self):
        inputs, outputs = [], []
        for tensor_obj in self._graph_info.tensor_obj_list:
            if tensor_obj.is_placeholder():
                inputs.append({constants.TensorDescKey.SHAPE: tensor_obj.shape,
                               constants.TensorDescKey.DTYPE: tensor_obj.dtype})
            elif tensor_obj.is_real_output():
                outputs.append({constants.TensorDescKey.SHAPE: tensor_obj.shape,
                                constants.TensorDescKey.DTYPE: tensor_obj.dtype})

        try:
            run_info = do_op_tiling(constants.AUTO_TILING, get_compile_info(), inputs, outputs)
        except RuntimeError:
            helper.report_fusion_check_result(check_result=False)
            helper.raise_error("currently, the tiling is not supported in generic_vector")

        tiling_format = {
            constants.TilingDataKey.BLOCK_AXIS: constants.DECODE_INT_FORMAT,
            constants.TilingDataKey.BLOCK_FACTOR: constants.DECODE_INT_FORMAT,
            constants.TilingDataKey.UB_AXIS: constants.DECODE_INT_FORMAT,
            constants.TilingDataKey.UB_FACTOR: constants.DECODE_INT_FORMAT,
            constants.TilingDataKey.IS_VALID_TILING: constants.DECODE_INT_FORMAT
        }

        tiling_data = decode(run_info.get(constants.RunInfoKey.TILING_DATA), tiling_format)
        if tiling_data.get(constants.TilingDataKey.IS_VALID_TILING) == 0:
            helper.report_fusion_check_result(check_result=False)
            helper.raise_error("currently, the tiling is not supported in generic_vector")

        self._tiling_case.block_dim = run_info.get(constants.RunInfoKey.BLOCK_DIM)

        self._tiling_case.block_split_axis_index = tiling_data.get(constants.TilingDataKey.BLOCK_AXIS)
        self._tiling_case.block_factor = tiling_data.get(constants.TilingDataKey.BLOCK_FACTOR)
        if self._tiling_case.block_split_axis_index < 0:
            self._tiling_case.block_split_axis_index = None
            self._tiling_case.block_factor = None

        self._tiling_case.ub_split_axis_index = tiling_data.get(constants.TilingDataKey.UB_AXIS)
        self._tiling_case.ub_factor = tiling_data.get(constants.TilingDataKey.UB_FACTOR)
        if self._tiling_case.ub_split_axis_index < 0:
            self._tiling_case.ub_split_axis_index = None
            self._tiling_case.ub_factor = None

    def _check_tiling_and_report_error(self):
        if self._check_tiling_default() and self._check_tiling_special():
            return
        helper.report_fusion_check_result(check_result=False)
        helper.raise_error("currently, the schedule is not supported in generic_vector")

    def _check_tiling_default(self):
        if helper.get_op_num() <= constants.OP_NUM_LOWER_THRESHOLD_TO_IGNORE_CORE_NUM:
            if self._tiling_case.block_dim < soc_info.SocInfo.get_core_num():
                return False

        last_dim_index = len(self._res_sch_node_obj.tensor_obj.shape) - 1
        if self._tiling_case.block_split_axis_index == last_dim_index:
            if self._tiling_case.block_factor % self._graph_info.max_block_ele_num != 0:
                return False

        is_unfavorable_mte = True
        for sch_node_obj in \
                [n for n in self._sch_node_obj_list if n.tensor_obj.is_placeholder() or n.tensor_obj.is_real_output()]:
            mte_bytes = helper.calc_mte_continuous_bytes(sch_node_obj.tensor_obj.shape,
                                                         self._tiling_case.ub_split_axis_index,
                                                         self._tiling_case.ub_factor)
            if mte_bytes >= soc_info.SocInfo.get_soc_mte_cache_line():
                is_unfavorable_mte = False

        return not is_unfavorable_mte

    def _check_tiling_special(self):
        return True

    def _do_tiling(self):
        self._do_block_tiling()
        self._do_ub_tiling()

    def _do_block_tiling(self):
        if self._tiling_case.block_split_axis_index is None:
            return
        # split res
        split_stage_obj = self._res_sch_node_obj.stage_obj
        self._split_axes.block_outer, self._split_axes.block_inner = \
            self._split_axis(split_stage_obj,
                             split_stage_obj.tvm_stage.op.axis[self._tiling_case.block_split_axis_index],
                             self._tiling_case.block_factor)

    def _do_ub_tiling(self):
        if self._tiling_case.ub_split_axis_index is None:
            return
        # split res
        split_stage_obj = self._res_sch_node_obj.stage_obj
        self._split_axes.ub_outer, self._split_axes.ub_inner = \
            self._split_axis(split_stage_obj, split_stage_obj.tvm_stage.op.axis[self._tiling_case.ub_split_axis_index],
                             self._tiling_case.ub_factor)

    def _do_post_reorder(self):
        if self._graph_info.union_reduce_axes_indices:
            self._do_res_post_reorder()
            self._do_reduce_post_reorder()

    def _do_res_post_reorder(self):
        pass

    def _do_reduce_post_reorder(self):
        pass

    def _do_set_buffer_size(self):
        for sch_node_obj in self._sch_node_obj_list:
            if sch_node_obj.tensor_obj.is_placeholder():
                continue
            weighted_buffer_size = int(self._max_buffer_size * sch_node_obj.buffer_size_ratio)
            cur_bytes_num = helper.get_bytes_num(sch_node_obj.tensor_obj.dtype)
            aligned_bytes = constants.BLOCK_SIZE
            if sch_node_obj.insn_obj.extra_node_size != 0:
                aligned_bytes = sch_node_obj.insn_obj.extra_node_size
            buffer_size_value = int(helper.value_aligned(weighted_buffer_size, aligned_bytes) // cur_bytes_num)
            sch_node_obj.stage_obj.unify_buffer_size(buffer_size_value)

            sch_node_obj.insn_obj.buffer_ele_num = buffer_size_value
            weighted_extra_node_size = int(self._max_buffer_size * sch_node_obj.extra_node_size_ratio)
            sch_node_obj.insn_obj.extra_node_ele_num = \
                int(helper.value_aligned(weighted_extra_node_size, constants.BLOCK_SIZE) // cur_bytes_num)
        # update insn
        self._do_insn_info_selection()

    def _do_storage_align(self):
        pass

    def _do_double_buffer(self):
        if not self._is_enable_db:
            return

        for sch_node_obj in self._sch_node_obj_list:
            if not sch_node_obj.stage_obj.is_ub_scope():
                continue
            sch_node_obj.stage_obj.unify_double_buffer()

    def _do_compute_at(self):
        for sch_node_obj in self._sch_node_obj_list:
            if sch_node_obj.tensor_obj.is_placeholder() or sch_node_obj == self._res_sch_node_obj:
                continue
            if sch_node_obj.stage_obj.flags.compute_root_flag:
                continue
            if self._split_axes.ub_outer is None:
                continue
            sch_node_obj.stage_obj.unify_compute_at(self._res_sch_node_obj.stage_obj, self._split_axes.ub_outer)

    def _do_emit_insn(self):
        if self._split_axes.ub_inner is not None:
            self._res_sch_node_obj.insn_obj.insn_axis = self._split_axes.ub_inner

        for sch_node_obj in self._sch_node_obj_list:
            if sch_node_obj.tensor_obj.is_placeholder():
                continue
            sch_node_obj.stage_obj.unify_emit_insn(sch_node_obj.insn_obj.insn_axis, sch_node_obj.insn_obj.insn_name,
                                                   sch_node_obj.insn_obj.insn_attrs)

    def _do_multi_core(self):
        res_leaf_iter_vars = list(self._res_sch_node_obj.get_tvm_stage().leaf_iter_vars)
        if self._split_axes.block_outer is not None:
            fuse_axes_list = res_leaf_iter_vars[:res_leaf_iter_vars.index(self._split_axes.block_outer) + 1]
            bind_axis = self._res_sch_node_obj.stage_obj.unify_fuse(*fuse_axes_list)
            self._res_sch_node_obj.stage_obj.unify_bind(bind_axis, tvm.thread_axis(constants.BLOCK_IDX))


class GenericVectorCommonSchedule(GenericVectorScheduleBase):
    """
    schedule for common
    """
    def __init__(self, graph_info_obj: graph_info.GraphInfo, tiling_case_obj: tiling_case.GenericVectorTilingCase,
                 outs):
        GenericVectorScheduleBase.__init__(self, graph_info_obj, tiling_case_obj, outs)

    def _do_rfactor(self):
        for reduce_sch_node_obj in [n for n in self._sch_node_obj_list if n.is_reduce()]:
            stage_obj = reduce_sch_node_obj.stage_obj
            need_rfactor = stage_obj.is_last_reduce() and stage_obj.is_discontinuous_reduce()
            if not need_rfactor:
                continue
            self.unify_rfactor(reduce_sch_node_obj, stage_obj.tvm_stage.op.reduce_axis[-1], -1)

    def _do_res_pre_reorder(self):
        res_stage_obj = self._res_sch_node_obj.stage_obj
        res_reorder_perm = helper.AxesHelper.get_reorder_perm_according_to_reduce_axes(
            len(res_stage_obj.tvm_stage.op.axis), self._graph_info.union_reduce_axes_indices
        )
        res_reorder_axes = helper.AxesHelper.get_common_node_reorder_axes(res_stage_obj.tvm_stage, res_reorder_perm)
        res_stage_obj.unify_reorder(*res_reorder_axes)

    def _check_tiling_special(self):
        last_dim_index = len(self._res_sch_node_obj.tensor_obj.shape) - 1
        if self._tiling_case.ub_split_axis_index == last_dim_index:
            if self._tiling_case.ub_factor % self._graph_info.max_block_ele_num != 0:
                return False

        return True

    def _do_res_post_reorder(self):
        res_stage_obj = self._res_sch_node_obj.stage_obj
        res_reorder_axes = helper.AxesHelper.fine_tune_common_node_reorder_axes_after_split(
            res_stage_obj.tvm_stage, self._graph_info.union_reduce_axes_indices, self._split_axes
        )
        res_stage_obj.unify_reorder(*res_reorder_axes)


class GenericVectorReduceAggregateSchedule(GenericVectorScheduleBase):
    """
    schedule for reduce aggregate
    """
    def __init__(self, graph_info_obj: graph_info.GraphInfo, tiling_case_obj: tiling_case.GenericVectorTilingCase,
                 outs):
        GenericVectorScheduleBase.__init__(self, graph_info_obj, tiling_case_obj, outs)
        self._last_reduce_sch_node_obj: schedule_node_info.ScheduleNode = None
        self._before_reduce_sch_node_obj_list = []

    def _do_categorization(self):
        reduce_sch_node_objs = helper.traverse_and_select_obj(self._res_sch_node_obj, lambda x: x.producers,
                                                              lambda x: x.is_reduce())
        self._last_reduce_sch_node_obj = reduce_sch_node_objs[0]

        helper.traverse_obj(self._last_reduce_sch_node_obj, self._before_reduce_sch_node_obj_list,
                            lambda x: x.producers)
        helper.ListHelper.remove(self._before_reduce_sch_node_obj_list, self._last_reduce_sch_node_obj)

    def _pre_process_insn_info_for_reduce(self):
        for reduce_sch_node_obj in [n for n in self._sch_node_obj_list if n.is_reduce()]:
            reduce_sch_node_obj.insn_obj.is_aggregated = True

    def _gen_buffer_size_info_obj(self):
        mem_unique_sch_node_objs = [n for n in self._sch_node_obj_list if n.stage_obj.flags.mem_unique_flag]
        compute_root_sch_node_objs = [n for n in self._sch_node_obj_list if n.stage_obj.flags.compute_root_flag]
        compute_root_sch_node_objs.append(
            [x
             for x in self._sch_node_obj_list
             if x not in self._before_reduce_sch_node_obj_list and x.stage_obj.is_ub_scope()
            ]
        )
        self._buffer_size_info_obj = buffer_size_info.BufferSizeInfo(
            self._res_sch_node_obj, mem_unique_sch_node_objs=mem_unique_sch_node_objs,
            compute_root_sch_node_objs_group=compute_root_sch_node_objs
        )

    def _check_tiling_special(self):
        last_dim_index = len(self._last_reduce_sch_node_obj.tensor_obj.shape) - 1
        if self._tiling_case.ub_split_axis_index == last_dim_index:
            if self._tiling_case.ub_factor % self._graph_info.max_block_ele_num != 0:
                return False

        return True

    def _do_ub_tiling(self):
        if self._tiling_case.ub_split_axis_index is None:
            return
        # split last reduce
        split_stage_obj = self._last_reduce_sch_node_obj.stage_obj
        real_split_axis = helper.AxesHelper.get_axis_of_reduce_according_to_index(
            split_stage_obj, self._tiling_case.ub_split_axis_index
        )
        self._split_axes.ub_outer, self._split_axes.ub_inner = \
            self._split_axis(split_stage_obj, real_split_axis, self._tiling_case.ub_factor)

    def _do_reduce_post_reorder(self):
        for reduce_stage_obj in [n.stage_obj for n in self._sch_node_obj_list if n.is_reduce()]:
            reduce_reorder_axes = helper.AxesHelper.fine_tune_reduce_node_reorder_axes_after_split(
                reduce_stage_obj.tvm_stage, self._split_axes
            )
            reduce_stage_obj.unify_reorder(*reduce_reorder_axes)

    def _do_compute_at(self):
        for sch_node_obj in self._sch_node_obj_list:
            if sch_node_obj.tensor_obj.is_placeholder():
                continue
            if sch_node_obj.stage_obj.flags.compute_root_flag:
                continue
            if sch_node_obj in self._before_reduce_sch_node_obj_list:
                if self._split_axes.ub_outer is None:
                    continue
                sch_node_obj.stage_obj.unify_compute_at(self._last_reduce_sch_node_obj.stage_obj,
                                                        self._split_axes.ub_outer)
            else:
                if sch_node_obj == self._res_sch_node_obj:
                    continue
                if self._split_axes.block_outer is None:
                    continue
                sch_node_obj.stage_obj.unify_compute_at(self._res_sch_node_obj.stage_obj, self._split_axes.block_outer)

    def _do_emit_insn(self):
        if self._split_axes.ub_inner is not None:
            self._last_reduce_sch_node_obj.insn_obj.insn_axis = self._split_axes.ub_inner
            # A A_ub_outer R R A_ub_inner A
            if not self._last_reduce_sch_node_obj.stage_obj.is_last_reduce() and \
                    not helper.judge_reduce_axis(self._split_axes.ub_inner):
                self._last_reduce_sch_node_obj.insn_obj.insn_axis = \
                    self._last_reduce_sch_node_obj.get_tvm_stage().op.reduce_axis[0]
        if self._split_axes.block_inner is not None:
            self._res_sch_node_obj.insn_obj.insn_axis = self._split_axes.block_inner

        for sch_node_obj in self._sch_node_obj_list:
            if sch_node_obj.tensor_obj.is_placeholder():
                continue
            sch_node_obj.stage_obj.unify_emit_insn(sch_node_obj.insn_obj.insn_axis, sch_node_obj.insn_obj.insn_name,
                                                   sch_node_obj.insn_obj.insn_attrs)
