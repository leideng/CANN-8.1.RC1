#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pooling info
"""
from typing import Any
from typing import Callable
from typing import Dict
from typing import Iterable
from typing import List
from typing import Optional
from typing import Set
from typing import Tuple
from typing import Union

from tbe.common.platform.platform_info import get_soc_spec
from tbe.dsl.base.operation import get_context
from tbe.tvm.tir import IntImm
from tbe.tvm import Var
from tbe.tvm import PlaceholderOp
from tbe.tvm import Tensor

from .pooling_helper import PoolingConstants
from ... import util
from ...constants import DTYPE_BYTE_MAPPING


class PoolingGraphInfo:
    """
    Operator Compute Graph Info collector and container
    """
    def __init__(self, output_tensors: Iterable[Tensor]):
        """
        Initialize and collect graph info
        """
        self.output_tensor_set: Optional[Set[Tensor]] = None
        # real_output_tensor_set: output set doesn't contain fake_node
        self.real_output_tensor_set: Optional[Set[Tensor]] = None
        # real_pure_output_tensor_set: output set doesn't contain fake_node and middle output
        self.real_pure_output_tensor_set: Optional[Set[Tensor]] = None
        self.tensor_consumers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_producers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_list: Optional[List[Tensor]] = None
        # extra info initialized by hooks
        self.reduce_window_tensor_set: Set[Tensor] = set()
        self.pad_tensor_set: Set[Tensor] = set()
        self.elewise_tensor_set: Set[Tensor] = set()
        self.input_tensor_set: Set[Tensor] = set()
        self.non_gm_input_tensor_set: Set[Tensor] = set()
        # extra info initialized after pre-initialization
        self.mid_output_tensor_set: Set[Tensor] = set()
        self.mid_tensor_set: Set[Tensor] = set()
        # res tensor
        self.endpoint_output_tensor: Tensor = None
        # max type and min type in graph
        self.max_type: Optional[str] = None
        self.min_type: Optional[str] = None
        # do info collection
        self.collect_info(output_tensors)
        self.gen_real_output_tensor_sets()
        self.fake_node()
        self.get_min_and_max_dtype()

    @staticmethod
    def dfs_compute_graph(root_tensor: Union[Iterable[Tensor], Tensor],
                          hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                             Callable[[Tensor], Any],
                                             Callable[[Tensor], Any]], ...]):
        """
        compute graph using dfs algorithm
        """
        def recursive_func(_root_tensor: Tensor,
                           _visited_list: Set[Tensor],
                           _tensor_consumers_map: Dict[Tensor, Set[Tensor]],
                           _tensor_producers_map: Dict[Tensor, Set[Tensor]],
                           _hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                               Callable[[Tensor], Any],
                                               Callable[[Tensor], Any]], ...]):
            _visited_list.add(_root_tensor)
            _tensor_producers_map.setdefault(_root_tensor, set())
            _tensor_consumers_map.setdefault(_root_tensor, set())
            for hook in hooks:
                if hook[0](_root_tensor):
                    hook[1](_root_tensor)
                else:
                    hook[2](_root_tensor)
            for in_tensor in _root_tensor.op.input_tensors:
                _tensor_consumers_map.setdefault(in_tensor, set())
                _tensor_consumers_map.get(in_tensor).add(_root_tensor)
                _tensor_producers_map.get(_root_tensor).add(in_tensor)
                recursive_func(in_tensor,
                               _visited_list,
                               _tensor_consumers_map,
                               _tensor_producers_map,
                               _hooks)

        visited_list = set()
        tensor_consumers_map = {}
        tensor_producers_map = {}
        if isinstance(root_tensor, (list, tuple, set)):
            for tensor in root_tensor:
                recursive_func(tensor, visited_list,
                               tensor_consumers_map,
                               tensor_producers_map,
                               hooks)
        elif isinstance(root_tensor, Tensor):
            recursive_func(root_tensor, visited_list,
                           tensor_consumers_map, tensor_producers_map,
                           hooks)

        return list(visited_list), tensor_consumers_map, tensor_producers_map

    def collect_info(self, output_tensors: Iterable[Tensor]):
        """
        Collect necessary information
        """
        self.output_tensor_set = set(output_tensors)
        self.tensor_list, self.tensor_consumers_map, self.tensor_producers_map = \
            self.dfs_compute_graph(self.output_tensor_set,
                                   (   # self.input_tensor_set hook
                                       (lambda _tensor: isinstance(_tensor.op, PlaceholderOp),
                                        lambda _tensor: self.input_tensor_set.add(_tensor),
                                        lambda _tensor: self.non_gm_input_tensor_set.add(_tensor)
                                        if not _tensor.op.input_tensors else None),
                                       # self.reduce_window_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find(PoolingConstants.REDUCE_WINDOW_TAG) != -1,
                                        lambda _tensor: self.reduce_window_tensor_set.add(_tensor),
                                        lambda _tensor: None),
                                       # self.pad_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find(PoolingConstants.PAD_TAG) != -1,
                                        lambda _tensor: self.pad_tensor_set.add(_tensor),
                                        lambda _tensor: None),
                                       # self.elewise_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find(PoolingConstants.ELEWISE_TAG) != -1,
                                        lambda _tensor: self.elewise_tensor_set.add(_tensor),
                                        lambda _tensor: None)
                                   ))
        # Initialize non-hookable info
        self.gen_mid_tensor_sets()
        # endpoint_output_tensor
        self.gen_endpoint_output_tensor()

    def gen_endpoint_output_tensor(self):
        """
        get endpoint tensor
        """
        for output_tensor in self.output_tensor_set:
            if not self.tensor_consumers_map.get(output_tensor):
                self.endpoint_output_tensor = output_tensor
                break

    def gen_mid_tensor_sets(self):
        """
        get mid tensors
        """
        # mid_output_tensor_set
        # mid_tensor_set
        for tensor in self.tensor_list:
            if tensor in self.output_tensor_set and self.tensor_consumers_map.get(tensor):
                # Tensor in output and has consumers is middle_out_tensor
                self.mid_output_tensor_set.add(tensor)
                self.mid_tensor_set.add(tensor)
            elif tensor not in self.output_tensor_set | self.input_tensor_set:
                self.mid_tensor_set.add(tensor)

    def gen_real_output_tensor_sets(self):
        """
        gen real output tensor sets
        """
        self.real_output_tensor_set = self.output_tensor_set
        self.real_pure_output_tensor_set = self.real_output_tensor_set - self.mid_output_tensor_set

    def fake_node(self):
        """
        do fake node
        """
        pass

    def get_min_and_max_dtype(self):
        """
        get min dtype and max dtype
        """
        self.max_type = self.tensor_list[0].dtype
        self.min_type = self.tensor_list[0].dtype

        for item in self.tensor_list:
            if DTYPE_BYTE_MAPPING.get(item.dtype) > DTYPE_BYTE_MAPPING.get(self.max_type):
                self.max_type = item.dtype
            elif DTYPE_BYTE_MAPPING.get(item.dtype) < DTYPE_BYTE_MAPPING.get(self.min_type):
                self.min_type = item.dtype


class PoolingBaseInfo:
    """
    class for pooling base info
    """
    def __init__(self, graph_info: PoolingGraphInfo):
        """
        init base info
        """
        self.graph_info: PoolingGraphInfo = graph_info
        self.reduce_window_tensor: Tensor = tuple(graph_info.reduce_window_tensor_set)[0]
        self.pad_tensor: Tensor = tuple(graph_info.pad_tensor_set)[0]

        self.window_axes: List[Var] = util.get_reduce_axes(self.reduce_window_tensor)
        self.window_indices: List[int] = util.shape_to_list(self.reduce_window_tensor.op.attrs["window_axes"])

        self.ori_input_shape: List[Union[Var, IntImm]] = list(self.pad_tensor.op.input_tensors[0].shape)
        self.pad_shape: List[Union[Var, IntImm]] = list(self.pad_tensor.shape)
        self.after_reduce_shape: List[Union[Var, IntImm]] = list(self.reduce_window_tensor.shape)

        self.is_reduce_last_axis = len(self.ori_input_shape) - 1 in self.window_indices

        self.strategy = None
        self.mode = None
        # list
        # idx 0 is window_dimensions
        # idx 1 is window_strides
        # idx 2 is window_dilations
        # idx 3 is padding_dimensions
        self.window_info = None
        self.get_info()

    def get_info(self):
        """
        get strategy, mode and window_info
        """
        current_compute = get_context().get_current_compute()
        if current_compute:
            self.strategy = current_compute.get("_strategy")
            self.mode = current_compute.get("_mode")
            self.window_info = current_compute.get("_window_info")


class PoolingComputeInfo:
    """
    class for pooling compute info
    """
    def __init__(self, output_tensors: Iterable[Tensor]):
        """
        init compute info
        """
        self.graph_info: PoolingGraphInfo = PoolingGraphInfo(output_tensors)
        self.base_info: PoolingBaseInfo = PoolingBaseInfo(self.graph_info)


class PoolingSocInfo:
    """
    class for pooling soc info
    """
    @classmethod
    def get_soc_version(cls):
        return get_soc_spec("SHORT_SOC_VERSION")

    @classmethod
    def get_core_num(cls):
        return get_soc_spec("CORE_NUM")

    @classmethod
    def get_ub_size(cls):
        return get_soc_spec("UB_SIZE")

    @classmethod
    def get_block_size(cls, dtype):
        return 32 // DTYPE_BYTE_MAPPING.get(dtype)
