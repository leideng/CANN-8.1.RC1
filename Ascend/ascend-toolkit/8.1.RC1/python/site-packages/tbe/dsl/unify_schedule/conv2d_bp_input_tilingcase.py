#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
conv2d backprop input tiling case
"""
import collections
import copy
import json
from collections import OrderedDict
from enum import Enum
from functools import reduce
from itertools import product
from typing import Any
from typing import Dict
from typing import Iterable
from typing import List
from typing import Union

import tbe.common.utils.log as log
import tvm
from tbe.common import platform
from tbe.common.context import get_context
from tbe.common.context import op_context
from tbe.common.platform import platform_info
from tbe.common.tiling import get_tiling
from tbe.common.tiling import get_tiling_type
from tbe.common.tiling import set_tiling_type
from tbe.common.tiling import tiling_api
from tbe.common.utils import decode
from tbe.common.utils import do_op_tiling
from tbe.common.utils.const import IS_CONV1D_SITUATION_STR
from tbe.common.utils.const import SPLIT_AXIS_MODE_STR
from tbe.common.utils.const import SplitAxisMode
from tbe.common.utils.const import QUANT_DTYPES
from tbe.common.utils.errormgr import raise_err_message_cube
from tbe.common.utils.op_util.op_util_cube import decode_tiling
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import get_context as op_get_context
from tbe.dsl.base.operation import get_te_var
from tbe.dsl.base.operation import is_unify
from tbe.dsl.base.operation import register_build_pointcut
from tbe.dsl.base.operation import register_tiling_case
from tbe.dsl.compute import cube_util
from tbe.dsl.compute.cube_util import BIT_RATIO_DICT
from tbe.dsl.compute.conv2d_backprop_input_compute import DynamicConv2dBpInputParams
from tbe.dsl.compute.conv2d_backprop_input_compute_util import CalL1Size
from tbe.dsl.compute.conv2d_backprop_input_general_compute import DeConvPattern
from tbe.dsl.compute.cube_util import BinaryMode
from tbe.dsl.static_schedule.conv_util import get_fixpipe_flag
from tbe.dsl.static_schedule.conv_util import get_inout_dtype
from tbe.dsl.static_schedule.conv_util import set_intrinsic_support
from tbe.dsl.static_schedule.util import align
from tbe.tvm.ir import PrimExpr

from . import util
from .constants import Pattern
from .cube_tilingcase import CubeTilingOp
from .cube_tilingcase import TilingSelection
from .cube_tilingcase import TilingUtils as utils
from .tilingcase_util import Conv2dBackpropParaProcess
from .tilingcase_util import construct_tiling_case
from .tilingcase_util import is_valid_tiling
from .tilingcase_util import parse_run_info


W_DELTA = 1
H_LEN = 400
W_LEN = 400
LARGE_M = 10000
MIN_STEP = 16
FUSED_DOUBLE_OPERAND_MUL = 100.0
DEFAULT_KERNEL_ID = None
DEFAULT_SPLIT_W_KERNEL_ID = 110000
NONALIGNED_FUSION_EXTRA_SIZE = 144
INTRINSTIC_FIX_PIPE_L0C2OUT = "Intrinsic_fix_pipe_l0c2out"
UINT32_MAX = 2 ^ 32 - 1


class ScopeDDR(Enum):
    DDR = 0
    L1 = 1


class L1FusionType(Enum):
    DISABLE = -1
    L1_WIDTH_FUSION = 0
    L1_BREADTH_FUSION = 1


class TilingIdOffset():
    # tiling_id offset
    def __init__(self):
        self.db_al1_offset = 15
        self.db_bl1_offset = 14
        self.db_l0c_offset = 13
        self.db_cub_offset = 12
        self.abkl1_attach_flag_offset = 10
        self.al1_attach_flag_offset = 8
        self.bl1_attach_flag_offset = 6
        self.conv1d_flag_offset = 5
        self.load3d_special_offset = 3
        self.min_k1_div_k0_is_1_offset = 18
        self.groups_gt_1_offset = 20
        self.split_w_flag_offset = 19
        # self.extend_tiling_offset must be placed at the end of this function
        self.extend_tiling_offset = 16


def parse_fuzz_build_range(info_list):
    """
    parse multiple range segment from json string

    Notice
    ----------
    for conv2d backprop input only parse input range

    Parameters
    ----------
    info_list: list support info
        [{
            "inputs": [{
                "index": 0,
                "tensor": [{
                    "range": [
                        [16, 32],
                        [3, 3],
                        [16, 32],
                        [16, 32]
                    ],
                    "shape": [-1, 3, -1, -1]
                }]
            }]
        }]

    Returns
    -------
    range_list: list of 4d range
    """
    invalid = (not isinstance(info_list, list)) or len(info_list) == 0
    if invalid:
        raise RuntimeError("invalid missing support info {}".format(str(info_list)))
    list_size = 4
    range_list = []
    op_type = DynamicConv2dBpInputParams.dynamic_para.get("op_type")
    if op_type in ["Conv2DBackpropInput", "depthwise_conv2d_backprop_input"]:
        target_index = 2
    elif op_type in ["Conv2DTranspose", "AvgPoolGrad"]:
        target_index = 1
    else:
        target_index = 0
    for item in info_list:
        inputs = item.get("inputs")
        invalid = (not isinstance(inputs, list)) or len(inputs) == 0
        if invalid:
            continue
        # >>> conv2d dx start: parse range from index [0] input
        for input_tensor in inputs:
            invalid = (not isinstance(input_tensor, dict)) or input_tensor.get("index") != target_index
            if invalid:
                continue
            invalid = (not isinstance(input_tensor.get("tensor"), list)) \
                      or len(input_tensor.get("tensor")) == 0 \
                      or (not isinstance(input_tensor.get("tensor")[0].get("range"), list)) \
                      or len(input_tensor.get("tensor")[0].get("range")) != list_size
            if invalid:
                raise RuntimeError("invalid support info input {}".format(str(input_tensor)))
            input_range = input_tensor.get("tensor")[0].get("range")
            for axis_range in input_range:
                invalid = (not isinstance(axis_range, list)) or len(axis_range) != 2 or axis_range[0] < 1 \
                          or axis_range[0] > axis_range[1]
                if invalid:
                    raise RuntimeError("invalid range {}".format(str(axis_range)))
            range_list.append(input_range)
            # <<< conv2d dx end: parse range from index [0] input
    return range_list


def gen_support_info(range_x, ori_tensors):
    """
    kernel list support info part

    Notice
    ------
    only need to set inputs with range

    Parameters
    ----------
    range_x: list
    input x range
    ori_tensors: dict
    orginal vaild tensors

    Returns
    -------
    support_info: dict
    """
    support_info = {}
    # >>> start: generate input shape and range
    inputs = []
    item = {}
    op_type = DynamicConv2dBpInputParams.dynamic_para.get("op_type")
    if op_type in ["Conv2DBackpropInput", "depthwise_conv2d_backprop_input"]:
        item["index"] = 2
    elif op_type in ["Conv2DTranspose", "AvgPoolGrad"]:
        item["index"] = 1
    else:
        item["index"] = 0
    item["tensor"] = []
    tensor_info = {}
    if op_type in ["Conv2DBackpropInput", "depthwise_conv2d_backprop_input", "AvgPoolGrad"]:
        ori_tensors_input = ori_tensors.get("out_backprop")
    else:
        ori_tensors_input = ori_tensors.get("x")
    ori_shape = ori_tensors_input.get("ori_shape")
    tensor_info["shape"] = ori_shape
    x_format = ori_tensors_input.get("ori_format")

    # get dy_range depends on dx_range
    conv_info = DynamicConv2dBpInputParams.tiling_info_dict
    dx_range = [range_x[0], [ori_shape[x_format.find("C")], ori_shape[x_format.find("C")]], range_x[1], range_x[2]]
    data_format = x_format
    ori_paras = {
        "filters": ori_tensors.get("filters"), "bias": None, "offset_w": None,
        "strides": (conv_info.get("strideH"), conv_info.get("strideW"), conv_info.get("strideH_expand"),
                    conv_info.get("strideW_expand")),
        "pads": (conv_info.get("padu"), conv_info.get("padd"), conv_info.get("padl"), conv_info.get("padr")),
        "dilations": (1, 1, 1, 1), "groups": conv_info.get("group"), "data_format": data_format,
        "output_padding": (0, 0, 0, 0), "offset_x": 0, "kernel_name": conv_info.get("kernel_name")
    }
    conv2d_backprop = Conv2dBackpropParaProcess(ori_paras)
    filter_shape_nchw = conv2d_backprop.get_input_nchw(ori_tensors.get("filters").get("ori_shape"),
                                                       ori_tensors.get("filters").get("ori_format"))
    dx_range_nchw = dx_range
    dy_range_nchw, _, _ = conv2d_backprop.get_output_range(filter_shape_nchw, dx_range_nchw)
    if op_type == "depthwise_conv2d_backprop_input":
        dy_range_nchw[1] = [filter_shape_nchw[0] * filter_shape_nchw[1], filter_shape_nchw[0] * filter_shape_nchw[1]]
    elif op_type == "AvgPoolGrad":
        dy_range_nchw[1] = dx_range_nchw[1]

    range_valid = [[0, 0]] * 4
    range_valid[x_format.find("N")] = list(dy_range_nchw[0])
    range_valid[x_format.find("C")] = list(dy_range_nchw[1])
    range_valid[x_format.find("H")] = list(dy_range_nchw[2])
    range_valid[x_format.find("W")] = list(dy_range_nchw[3])
    tensor_info["range"] = range_valid
    item.get("tensor").append(tensor_info)
    inputs.append(item)
    support_info["inputs"] = inputs
    # <<< end: generate input shape and range
    return support_info


def add_covered_shape_range(compile_info):
    """
    tiling_case func for dynamic shape conv2d backprop input

    Parameters
    ----------
    compile_info: dict
    tiling range info

    Returns
    -------
    info_list: dict
    support info and compile info pair
    max_id: int
    last kernel id
    """
    info_list = []
    id_list = list(compile_info["block_dim"].keys())
    id_list.sort()
    max_id = id_list[-1]
    # >>> start: add compute var for op tiling
    te_vars = []
    for cpt in op_get_context().get_computes():
        te_vars += cpt.get_vars()
    var_list = [var.get_name() for var in te_vars]

    # <<< end: add compute var for op tiling
    for kernel_id, _ in compile_info["block_dim"].items():
        new_compile = compile_info.copy()
        # >>> start: keep only one record
        for keys, value in new_compile.items():
            if isinstance(value, dict):
                value = {} if value.get(kernel_id) is None else {kernel_id: value[kernel_id]}
                new_compile[keys] = value
        # <<< end: keep only one record
        new_compile["kernelId"] = kernel_id
        new_compile["_vars"] = {kernel_id: var_list}
        range_x = new_compile["repo_range"].get(kernel_id) or new_compile["cost_range"].get(kernel_id)
        new_range = [range_x[:2], range_x[2:4], range_x[4:6]]
        ori_tensors = DynamicConv2dBpInputParams.ori_tensor
        new_support = gen_support_info(new_range, ori_tensors)
        info_list.append({"supportInfo": new_support, "compileInfo": new_compile})
    return info_list, max_id


@register_build_pointcut(pattern=Pattern.CONV2D_BACKPROP_INPUT)
def build_pointcut_conv2d_backprop_input(func, *args, **kwargs):
    """
    kernel info process before build

    Notice
    ------
    kernel_info: dict with support info and compile info
        {
            "supportInfo": {
                "inputs": [{
                    "index": 0,
                    "tensor": [{
                        "range": [
                            [16, 32],
                            [3, 3],
                            [64, 128],
                            [64, 128]
                        ],
                        "shape": [-1, 3, -1, -1]
                    }]
                }]
            },
            "compileInfo": {
                "_pattern": "Conv2d_backprop_input",
                "tiling_type": "dynamic_tiling",
                "repo_seeds": {},
                "repo_range": {},
                "cost_range": {
                    1: [16, 32, 64, 128, 64, 128]
                },
                "block_dim": {
                    1: 16
                },
                "_vars": {
                    1: ["batch_n", "dedy_h", "dx_h", "dedy_w", "dx_w"]
                }
            }
        }

    Parameters
    ----------
    func: funtions
        build process
    args: list
        function input args
    kwargs: dict
        function input args and value

    Returns
    -------
    None
    """
    fuzz_build = get_context().get_build_type() == "fuzzily_build" and not DynamicConv2dBpInputParams.binary_mode
    if fuzz_build:  # set kernel info
        info_list, max_id = add_covered_shape_range(get_compile_info())
        get_context().add_build_json_result("kernelList", info_list)
        get_context().add_build_json_result("maxKernelId", max_id)
    func(*args, **kwargs)


def _get_target_area(conv_info, tgt_list, var_names):
    shape_dict = {"batch_n": conv_info.get("C_shape")[0],
                  "dx_h": conv_info.get("C_shape")[2],
                  "dx_w": conv_info.get("C_shape")[3]}
    tgt_area = {}
    if DynamicConv2dBpInputParams.binary_mode:
        tgt_area = {
            "batch_n": (1, None),
            "dx_h": (1, None),
            "dx_w": (1, None)
        }
    else:
        for var_name in var_names:
            if get_te_var(var_name):
                tgt_area[var_name] = tuple(get_te_var(var_name).get_bound())
            else:
                tgt_area[var_name] = (int(shape_dict.get(var_name)), int(shape_dict.get(var_name)))
    tgt_list.append(tgt_area)


def _calc_min_load_size_exceed_l1(tensor_map, tensor_attr):
    # calculate whether the minimum l1 load of optiling_ordinary or optiling_split_w exceeds the l1 size
    # cal minimum l1 load, sliding window size is kh*kw
    m0, k_block_size, n0 = platform_info.CUBE_MKN.get(tensor_map['b_ddr'].dtype)["mac"]
    k_al1 = 2 if tensor_map['b_ddr'].dtype == "float32" else 1
    k_bl1 = k_al1
    tiling_dict = {"m_al1": 1, "m": 1, "m0": m0, "n_bl1": 1, "n": 1, "n0": n0, "k_al1": k_al1,
                   "k_bl1": k_bl1, "k_block_size": k_block_size, "db_al1": 1, "db_bl1": 1, "db_bias_l1": 1}
    optiling_default_flag_dict = {"binary_flag": True}
    input_dict = {}
    img_shape_g, filter_shape_g, output_shape_g = calc_inout_shape_g(tensor_map, tensor_attr)
    input_dict['dy_shape_nc1hwc0'] = img_shape_g
    input_dict['filter_shape_nchw'] = [filter_shape_g[0], filter_shape_g[1] * filter_shape_g[4]] + filter_shape_g[2:4]
    input_dict['dx_shape_nchw'] = [output_shape_g[0], output_shape_g[1] * output_shape_g[4]] + output_shape_g[2:4]
    if tensor_map.get('bias_bt') is not None:
        input_dict['bias'] = tensor_map.get('bias_bt')
        input_dict['bias_dtype'] =  tensor_map.get('bias_bt').dtype
    input_dict['dy_dtype'] = tensor_map['a_ddr'].dtype
    input_dict['filter_dtype'] = tensor_map['b_ddr'].dtype
    input_dict['strides'] = [tensor_attr['stride_h'], tensor_attr['stride_w']]
    input_dict['padding'] = calc_padding_for_auto_tiling(tensor_attr)
    input_dict['dilations'] = [0, 0] + tensor_attr['dilation']
    cal_size = CalL1Size(input_dict)
    al1_size, bl1_size, bias_l1_size = cal_size.cal_l1_size(optiling_default_flag_dict, tiling_dict)
    optiling_ordinary_min_load_size_flag = al1_size + bl1_size + bias_l1_size > platform_info.get_soc_spec("L1_SIZE")
    split_w_flag_dict =  {"binary_flag": True, "conv2d_split_w_flag": True}
    split_w_al1_size, split_w_bl1_size, split_w_bias_l1_size = cal_size.cal_l1_size(split_w_flag_dict, tiling_dict)
    optiling_split_w_min_load_size_flag = split_w_al1_size + split_w_bl1_size + \
        split_w_bias_l1_size > platform_info.get_soc_spec("L1_SIZE")
    return optiling_ordinary_min_load_size_flag and optiling_split_w_min_load_size_flag


def support_cache_tiling(tensor_map, tensor_attr, use_dynamic_schedule: bool):
    # common unsupported scenario
    # 0. not support l0c to out
    # 1. optimize schedule
    # 2. optiling_ordinary and optiling_split_w minimum load exceeds l1 size
    # 3. l0a layout is zN
    if not tensor_attr.get('support_l0c_to_out'):
        return False
    # performance consideration: only support general schedule
    if use_dynamic_schedule and tensor_map['deconv_res'].op.tag != 'conv2d_backprop_input':
        return False

    if _calc_min_load_size_exceed_l1(tensor_map, tensor_attr):
        return False

    if tensor_attr.get('l0a_layout_is_zN'):
        return False

    combinition_dtype = (tensor_map['a_ddr'].dtype, tensor_map['deconv_res'].dtype)
    if use_dynamic_schedule:
        if combinition_dtype in (('float16', 'float16'), ('float32', 'float32')):
            return True
    else:
        if combinition_dtype in (('bfloat16', 'bfloat16'),) and 'bias_ddr' not in tensor_map:
            return True

    return False


def get_default_tiling_static(info_dict, tensor_attr):

    def _get_factors(val, val_max):
        """
        get the factor of val that smaller than val_max
        """
        factor_max = min(val, val_max)
        factors = []
        for m_fac in range(factor_max, 0, -1):
            if val % m_fac == 0:
                factors.append(m_fac)
        return factors

    def _get_block_dim(batch, m, n, core_num):
        """
        Generate block dims for default tiling.
        """
        batch_dims = _get_factors(batch, core_num)
        n_dims = _get_factors(n, core_num)
        m_dims = _get_factors(m, core_num)
        block_dims = [batch_dims[0], 1, 1]
        if batch_dims[0] < core_num:
            n_dim = _get_factors(n, core_num // batch_dims[0])[0]
            m_dim = 1
            if n_dim * batch_dims[0] < core_num:
                m_dim = _get_factors(m, core_num // (batch_dims[0] * n_dim))[0]
            block_dims = [batch_dims[0], n_dim, m_dim]

        if reduce(lambda x, y: x * y, block_dims) < core_num:
            for comb in product(batch_dims, n_dims, m_dims):
                if reduce(lambda x, y: x * y, comb) == core_num:
                    block_dims = list(comb)
                    break

        return block_dims

    atype = info_dict['A_dtype']
    btype = info_dict['B_dtype']
    filter_shape = info_dict['B_shape']

    tiling = {"default_tiling": True}
    _, cin1, k_h, k_w, _ = filter_shape
    bit_dir = {
        "float32": 8, "int32": 16, "float16": 16,
        "int8": 32, "bfloat16": 16, "int4": 64,
    }
    if atype in bit_dir.keys():
        k_0 = bit_dir.get(atype)
        k_al1 = k_w * k_0
        if tensor_attr['split_w']:
            k_al1 = k_h * k_al1
    else:
        # defaut value 32
        k_al1 = 32
        k_0 = 32

    if btype in bit_dir.keys():
        k_bl1 = k_0 * k_w
    else:
        # defaut value 32
        k_bl1 = 32

    if tensor_attr.get("need_expand_stride"):
        tiling["AUB_shape"] = [k_w * bit_dir.get(atype), 1, 1, 1]
        tiling["BUB_shape"] = None
    else:
        tiling["AUB_shape"] = None
        tiling["BUB_shape"] = None

    n_l0 = 1
    group_l0 = 1
    if tensor_attr['l0c_multi_group_flag']:
        n_l0 = cin1
        group_l0 = 2

    core_num = platform_info.get_soc_spec("CORE_NUM")
    batch, _, fmap_h, _, _ = tensor_attr.get("output_shape")
    batch_dim, n_dim, m_dim = _get_block_dim(batch, fmap_h, cin1 // n_l0, core_num)
    ka_factor = kb_factor = 1
    tiling["AL1_shape"] = [k_al1 * ka_factor, 1, 1, 1]
    tiling["BL1_shape"] = [k_bl1 * kb_factor, 1, 1, 1]
    tiling["AL0_matrix"] = [1, ka_factor, 16, k_0, 1, 1]
    tiling["BL0_matrix"] = [kb_factor, n_l0, 16, k_0, 1, 1]
    tiling["CL0_matrix"] = [n_l0, 1, 16, 16, 1, group_l0]
    tiling["CUB_matrix"] = [n_l0, 1, 16, 16, 1, group_l0]
    tiling["block_dim"] = [batch_dim, n_dim, m_dim, 1]
    tiling["n_bef_batch_flag"] = 0
    tiling["n_bef_group_flag"] = 0
    tiling["batch_bef_group_fla"] = 0
    tiling["A_overhead_opt_flag"] = 0
    tiling["B_overhead_opt_flag"] = 0
    tiling["AUB_channel_wise_flag"] = None
    tiling["BUB_channel_wise_flag"] = None
    tiling["CUB_channel_wise_flag"] = None
    tiling["manual_pingpong_buffer"] = {
        "AUB_pbuffer": 1,
        "BUB_pbuffer": 1,
        "AL1_pbuffer": 1,
        "BL1_pbuffer": 1,
        "AL0_pbuffer": 1,
        "BL0_pbuffer": 1,
        "CL0_pbuffer": 1,
        "CUB_pbuffer": 1,
        "UBG_pbuffer": 1
    }
    return tiling


def is_valid_tiling_in_repository(tiling):
    return not all(x == 0 for x in tiling['AL0_matrix'])


def post_process_get_tiling(tiling, tensor_attr, tensor_map, dynamic_mode=False):
    split_w = tensor_attr['split_w']
    stride_h = tensor_attr['stride_h']
    stride_w = tensor_attr['stride_w']
    kernel_h = tensor_attr.get("kernel_h")
    kernel_w = tensor_attr.get("kernel_w")
    block_reduce = platform_info.CUBE_MKN.get(tensor_map['a_ddr'].dtype).get("mac")[1]

    # close overhead flag in dynamic mode
    # close overhead flag in v220 when stride > 1
    # close overhead flag in split_w
    disable_overhead_opt = dynamic_mode or split_w or (not tensor_attr.get("support_stride_expand_in_ub")
                                                       and (stride_h > 1 or stride_w > 1))
    if disable_overhead_opt:
        tiling['A_overhead_opt_flag'] = 0
        tiling['B_overhead_opt_flag'] = 0

    if not is_unify():
        # in split w scend, load3d cannot support load h > 1 and cin1 > 1 on L1 buffer at the same time
        if (split_w and tiling.get("AL1_shape") and tiling.get("AL1_shape")[1] > 1
                and tiling.get("AL1_shape")[0] > kernel_h * kernel_w * block_reduce):
            tiling["AL1_shape"][1] = 1


def get_static_tiling(tiling_info_dict, tensor_map, tensor_attr):
    curr_tiling_type = get_tiling_type()
    set_tiling_type("repository_tiling")
    tiling_from_repo = tiling_api.get_tiling(tiling_info_dict)
    set_tiling_type(curr_tiling_type)

    if is_valid_tiling_in_repository(tiling_from_repo):
        log.debug('tiling from repository in tilingcase')
        return tiling_from_repo
    if support_cache_tiling(tensor_map, tensor_attr, use_dynamic_schedule=True):
        op_context.get_context().add_addition("support_binary_constant", True)
        # 控制classify只编译一个模板
        op_context.get_context().add_addition("need_expand_stride", tensor_attr["need_expand_stride"])
        op_context.get_context().add_addition("split_w", tensor_attr["split_w"])
        log.debug("support binary_const")
    elif support_cache_tiling(tensor_map, tensor_attr, use_dynamic_schedule=False):
        log.debug("enter cache tiling with stc compute/schedule")
        add_compile_info("tiling_type", "binary")
        add_compile_info("binary_mode", True)
        add_compile_info("block_dim", {"CORE_NUM": util.get_core_num()})

        run_info = do_op_tiling(op_context.get_context().get_addition('params_do_op_tiling')['op_type'],
                                get_compile_info(),
                                op_context.get_context().get_addition('params_do_op_tiling')['inputs'],
                                op_context.get_context().get_addition('params_do_op_tiling')['outputs'],
                                None,
                                None,
                                op_context.get_context().get_addition('params_do_op_tiling')['attrs'])

        tiling_key, tiling_data = parse_run_info(run_info)
        block_reduce = platform_info.CUBE_MKN.get(tensor_map['a_ddr'].dtype).get("mac")[1]
        tiling_from_cache_tiling = TilingDx.convert_to_tiling_strategy(tiling_key, tiling_data, block_reduce,
                                                                       tensor_attr)
        return tiling_from_cache_tiling

    tiling = tiling_api.get_tiling(tiling_info_dict)
    if is_valid_tiling(tiling):
        log.debug('tiling from costmodel/repository in tilingcase')
        return tiling
    log.debug('default tiling in tilingcase')
    return get_default_tiling_static(tiling_info_dict, tensor_attr)


class Node:
    pass


class PatternNode(Node):
    def __init__(self, pattern, scope, alias) -> None:
        self.pattern = pattern
        self.scope = scope
        self.tensor = None
        self.alias = alias
        self.input_in_same_scope = False
        self.repeat_mode = None
        self.tensors = []
        self.not_verify_correctness_of_path = []

    def __str__(self):
        return f'{self.pattern} {self.scope}'

    def __repr__(self):
        return f'({self.pattern}, {self.scope}, {self.tensor}, {self.input_in_same_scope})'


class OriNode(Node):
    def __init__(self, name, tag, tensor) -> None:
        self.name = name
        self.tag = tag
        self.tensor = tensor
        self.alias = None
        self.input_in_same_scope = False
        self.scope = platform_info.scope_gm
        self.repeat_mode = None
        self.tensors = []
        self.not_verify_correctness_of_path = []

    def __str__(self):
        return f"({repr(self.name)}, {repr(self.tag)}, {self.scope}, {self.input_in_same_scope})"

    def __repr__(self):
        return f"({repr(self.name)}, {repr(self.tag)}, {self.scope}, {self.tensor} {self.input_in_same_scope})"


class Graph:
    def __init__(self, root) -> None:
        self.name = ''
        self.inputs = {}
        self.root = root
        self._additional_params = {}
        self.tensor_attr = {}
        self.tensor_map = {}
        self.post_process = None
        self.post_process_after_set_scope = None

    def __str__(self):
        res = []
        for item in self.iterate_all():
            res.append(str(item))
        return '\n'.join(res)

    def __repr__(self):
        res = [self.name]
        for item in self.iterate_all():
            res.append(f'{str(item)} <- {[str(x) for x in self.inputs.get(item, [])]}')
        return '\n'.join(res)

    def add_edge(self, dst, src):
        # add edge dst <- src
        if src not in self.inputs:
            self.inputs[src] = []
        if dst not in self.inputs:
            self.inputs[dst] = []
        self.inputs[dst].append(src)

    def insert_before(self, node, dst):
        # insert node before dst
        # s0 -> dst
        # s1 ->
        self.inputs[node] = self.inputs[dst]
        self.inputs[dst] = [node]

    def insert_after(self, node, dst):
        # insert node after dst
        # s0 -> dst
        # s1 ->
        for output_dst in self.output_nodes(dst):
            self.inputs[output_dst][self.inputs[output_dst].index(dst)] = node
        self.inputs[node] = [dst]

    def iterate_all(self):
        deque = collections.deque([self.root])
        while len(deque) != 0:
            node = deque.popleft()
            yield node
            deque.extend(self.inputs.get(node, {}))

    def iterate_by_layer(self):
        deque = collections.deque([self.root])
        while len(deque) != 0:
            yield [x for x in deque]
            size = len(deque)
            for _ in range(size):
                node = deque.popleft()
                deque.extend(self.inputs.get(node, {}))

    def update_tensor_attr(self, tensor_attr):
        if self.tensor_attr:
            tensor_attr.update(self.tensor_attr)

    def add_addition(self, key, value):
        self._additional_params[key] = value

    def input_nodes(self, node):
        return self.inputs.get(node, [])

    def output_nodes(self, node):
        nodes = []
        for dst_node, src_nodes in self.inputs.items():
            if node in src_nodes:
                nodes.append(dst_node)
        return nodes

    def print(self):
        for item in self.iterate_all():
            print(item)

    def find_node(self, node):
        for n in self.inputs.keys():
            if match_node(node, n):
                return n
        return None


def sync_node(on: OriNode, pn: PatternNode, og: Graph):
    on.alias = pn.alias
    on.input_in_same_scope = pn.input_in_same_scope
    on.scope = pn.scope
    on.repeat_mode = pn.repeat_mode
    on.tensors = pn.tensors
    on.not_verify_correctness_of_path = [og.find_node(x) for x in pn.not_verify_correctness_of_path]


def sync_graph(og: Graph, pg: Graph):
    og.tensor_attr = pg.tensor_attr
    for key, value in pg.tensor_map.items():
        if isinstance(value, Iterable):
            og.tensor_map[key] = [og.find_node(x) for x in value]
        else:
            og.tensor_map[key] = og.find_node(value)
    og.post_process = pg.post_process
    og.post_process_after_set_scope = pg.post_process_after_set_scope


def match_node(pn: Union[PatternNode, None], on: Union[OriNode, None]):
    # structure of pattern (on.name, on.tag)
    if pn is None and on is None:
        return True
    if pn is None or on is None:
        return False

    return pn.pattern[1] == on.tag


def mark_skip_unimportant_nodes():
    pass


def match_graph(og: Graph, pg: Graph):
    p_iter = pg.iterate_all()
    o_iter = og.iterate_all()

    while True:
        pn = next(p_iter, None)
        on = next(o_iter, None)

        if pn is not None and pn.repeat_mode:
            # repeat_mode *
            while pn.repeat_mode == '*' and not match_node(pn, on):
                pn = next(p_iter, None)
                mark_skip_unimportant_nodes()
            # repeat_mode + *
            if pn.repeat_mode is not None:
                while match_node(pn, on):
                    if pn is None or on is None:
                        return True
                    sync_node(on, pn, og)
                    pn.tensors.append(on.tensor)

                    # consume unimportant nodes
                    on = next(o_iter, None)
                    while on is not None and on.tag == '':
                        on = next(o_iter, None)
                pn = next(p_iter, None)

        if not match_node(pn, on):
            return False
        elif pn is None or on is None:
            return True
        sync_node(on, pn, og)


def post_process_of_fixpipe(pg: Graph, og: Graph, tensor_map: Dict[str, List[Node]], tensor_attr: Dict[str, any]):
    deconv_res = tensor_map["deconv_res"]
    tensor_attr['5HD_TO_NHWC_FP'] = tensor_map["fixpipe_tensor"] is not None and \
        len(cube_util.shape_to_list(deconv_res.shape)) == 3
    tensor_attr["5HD_TO_NCHW_DYN"] = "5HD_TRANS_NCHW" in deconv_res.op.tag
    tensor_attr["5HD_TO_NHWC_DYN"] = "5HD_to_NHWC_fusion" in deconv_res.op.tag
    tensor_attr["5HD_TO_4D_DYN"] = tensor_attr.get("5HD_TO_NCHW_DYN") or tensor_attr.get("5HD_TO_NHWC_DYN")


def post_process_fusion_type_of_quant(pg: Graph, og: Graph, tensor_map: Dict[str, List[Node]],
                                      tensor_attr: Dict[str, Any]):
    if tensor_map['a_ddr'].dtype in QUANT_DTYPES:
        tensor_attr['fusion_type'] = 2


class GeneralTemplates():
    @staticmethod
    def template_gen_stride_eq_1_with_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_eq_1_with_ub'
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True

        return pg

    @staticmethod
    def template_gen_stride_eq_1_with_ub_with_bias_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_add_vector = PatternNode(('bias_add_vector', ''), platform_info.scope_ubuf, 'bias_add_vector')
        bias_ddr = PatternNode(('bias', ''), platform_info.scope_gm, 'bias_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_eq_1_with_ub_with_bias_ub'
        pg.add_edge(c_ddr, bias_add_vector)
        pg.add_edge(bias_add_vector, c_ub)
        pg.add_edge(bias_add_vector, bias_ddr)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        bias_add_vector.input_in_same_scope = True

        return pg

    @staticmethod
    def template_gen_stride_eq_1_without_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_eq_1_without_ub'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant
        return pg

    @staticmethod
    def template_gen_stride_eq_1_without_ub_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_eq_1_without_ub_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant
        return pg

    @staticmethod
    def template_gen_stride_eq_1_without_ub_with_bias_bt():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_eq_1_without_ub_with_bias_bt'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col_before, a_ddr)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_gen_stride_eq_1_without_ub_with_bias_bt_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_eq_1_without_ub_with_bias_bt_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_col_before, a_ddr)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_gen_stride_eq_1_add():
        def post_process(pg: Graph, og: Graph, tensor_map: Dict[str, List[Node]], tensor_attr: Dict[str, any]):
            repeat_add = og.find_node(add_0)
            tensor_attr['fused_double_operand_num'] = min(2, len(repeat_add.tensors))

        add_0 = PatternNode(('add_0', 'elewise_binary_add'), platform_info.scope_ubuf, None)
        add_0.repeat_mode = '+'
        add = PatternNode(('add', ''), platform_info.scope_gm, None)
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_ubuf, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(add_0)
        pg.add_edge(add_0, c_ddr)
        pg.add_edge(add_0, add)
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'elewise_fuse': True, 'fusion_type': 3}
        pg.tensor_map = {'compute_inline_tensors': [c_ddr]}

        a_col_before.input_in_same_scope = True
        add_0.input_in_same_scope = True

        pg.post_process = post_process

        return pg

    @staticmethod
    def template_gen_stride_eq_1_add_n_relugrad_v2():
        def post_process(pg: Graph, og: Graph, tensor_map: Dict[str, List[Node]], tensor_attr: Dict[str, any]):
            repeat_add = og.find_node(add_0)
            tensor_attr['fused_double_operand_num'] = min(2, len(repeat_add.tensors)) + 1/16

        def post_process_after_set_scope(og: Graph, tensor_map: Dict[str, List[Node]], tensor_attr: Dict[str, any]):
            #   c_ub_cut                    tensor_vadd_1： addn_0
            #        |                       /
            #        |            tensor_vadd_1_ub： addn_0.local.UB
            #        |                     /
            #   tensor_inter_add_compute： add_0    tensor_vadd： addn_1
            #        |                               /
            #        |                      tensor_vadd_ub： addn_1.local.UB
            #        |                             /
            #   vadd_res： add_1
            #        |
            #   c_ub_drelu： sel_2.local.UB
            #        |
            #   deconv_res： sel_2
            c_ub_drelu = tensor_map['ub_list'][0]
            vadd_res = tensor_map['ub_list'][1] if len(tensor_map['ub_list']) > 1 else None
            tensor_inter_add_compute = tensor_map['ub_list'][2] if len(tensor_map['ub_list']) > 2 else None

            tensor_vadd_ub = tensor_map['input_tensor_list'][1] if len(tensor_map['input_tensor_list']) > 1 else None
            tensor_vadd_1_ub = tensor_map['input_tensor_list'][2] if len(tensor_map['input_tensor_list']) > 2 else None

            tensor_map['c_ub_drelu'] = c_ub_drelu
            tensor_map['vadd_res'] = vadd_res
            tensor_map['tensor_inter_add_compute'] = tensor_inter_add_compute
            tensor_map['tensor_vadd_ub'] = tensor_vadd_ub
            tensor_map['tensor_vadd_1_ub'] = tensor_vadd_1_ub

        sel_0 = PatternNode(('sel_0', 'emit_insn_elewise_multiple_sel|bool'), platform_info.scope_ubuf, None)
        bit_mask = PatternNode(('bit_mask', ''), platform_info.scope_gm, 'mask')
        add_0 = PatternNode(('add_0', 'elewise_binary_add'), platform_info.scope_ubuf, None)
        add_0.repeat_mode = '*'
        add = PatternNode(('add', ''), platform_info.scope_gm, None)
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_ubuf, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(sel_0)
        pg.add_edge(sel_0, bit_mask)
        pg.add_edge(sel_0, add_0)
        pg.add_edge(add_0, c_ddr)
        pg.add_edge(add_0, add)
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 4}
        pg.tensor_map = {'compute_inline_tensors': [c_ddr]}

        a_col_before.input_in_same_scope = True
        add_0.input_in_same_scope = True
        sel_0.input_in_same_scope = True

        pg.post_process = post_process
        pg.post_process_after_set_scope = post_process_after_set_scope

        return pg

    @staticmethod
    def template0_relu_grad_v2():
        sel_0 = PatternNode(('sel_0', 'emit_insn_elewise_multiple_sel|bool'), platform_info.scope_ubuf, None)
        bit_mask = PatternNode(('bit_mask', ''), platform_info.scope_gm, 'mask')
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_ubuf, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(sel_0)
        pg.add_edge(sel_0, bit_mask)
        pg.add_edge(sel_0, c_ddr)
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 8, 'fused_double_operand_num': 0.0625}
        pg.tensor_map = {'compute_inline_tensors': [c_ddr]}

        sel_0.input_in_same_scope = True
        a_col_before.input_in_same_scope = True

        return pg

    @staticmethod
    def template_gen_stride_eq_1_fixpipe():
        fixpipe_reform = PatternNode(('fixpipe_reform', 'fixpipe_reform'), platform_info.scope_ubuf, None)
        fixpipe = PatternNode(('fixpipe', 'fixpipe'), platform_info.scope_gm, 'fixpipe_tensor')
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_ubuf, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(fixpipe_reform)
        pg.add_edge(fixpipe_reform, fixpipe)
        pg.add_edge(fixpipe, c_ddr)
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_ddr)

        pg.tensor_attr = {'fusion_type': 0, 'fused_double_operand_num': 0}
        pg.tensor_map = {'compute_inline_tensors': [c_ddr]}

        a_col_before.input_in_same_scope = True

        return pg

    @staticmethod
    def template_gen_stride_gt_1_with_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_ubuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_ubuf, 'a_filling')
        a_l1 = PatternNode(('dy_l1', 'dy_l1'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_gt_1_with_ub'
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_l1)
        pg.add_edge(a_l1, a_filling)
        pg.add_edge(a_filling, a_ddr)
        pg.add_edge(a_filling, a_zero)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        a_filling.input_in_same_scope = True

        return pg

    @staticmethod
    def template_gen_stride_gt_1_with_ub_with_bias_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_ubuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_ubuf, 'a_filling')
        a_l1 = PatternNode(('dy_l1', 'dy_l1'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_add_vector = PatternNode(('bias_add_vector', ''), platform_info.scope_ubuf, 'bias_add_vector')
        bias_ddr = PatternNode(('bias', ''), platform_info.scope_gm, 'bias_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_gt_1_with_ub_with_bias_ub'
        pg.add_edge(c_ddr, bias_add_vector)
        pg.add_edge(bias_add_vector, c_ub)
        pg.add_edge(bias_add_vector, bias_ddr)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_l1)
        pg.add_edge(a_l1, a_filling)
        pg.add_edge(a_filling, a_ddr)
        pg.add_edge(a_filling, a_zero)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        a_filling.input_in_same_scope = True
        bias_add_vector.input_in_same_scope = True

        return pg

    @staticmethod
    def template_gen_stride_gt_1_without_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        a_l1 = PatternNode(('dy_l1', 'dy_l1'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_gt_1_without_ub'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_l1)
        pg.add_edge(a_l1, a_filling)
        pg.add_edge(a_filling, a_ddr)
        pg.add_edge(a_filling, a_zero)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        # dedy(gm)         -> dy_filling(l1)
        # dy_filling_i(l1) ->
        a_filling.not_verify_correctness_of_path = [a_zero]
        a_l1.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_gen_stride_gt_1_without_ub_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        a_l1 = PatternNode(('dy_l1', 'dy_l1'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_gt_1_without_ub_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_l1)
        pg.add_edge(a_l1, a_filling)
        pg.add_edge(a_filling, a_ddr)
        pg.add_edge(a_filling, a_zero)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        # dedy(gm)         -> dy_filling(l1)
        # dy_filling_i(l1) ->
        a_filling.not_verify_correctness_of_path = [a_zero]
        a_l1.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_gen_stride_gt_1_without_ub_with_bias_bt():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        a_l1 = PatternNode(('dy_l1', 'dy_l1'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_gt_1_without_ub_with_bias_bt'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_l1)
        pg.add_edge(a_l1, a_filling)
        pg.add_edge(a_filling, a_ddr)
        pg.add_edge(a_filling, a_zero)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        a_filling.not_verify_correctness_of_path = [a_zero]
        a_l1.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_gen_stride_gt_1_without_ub_with_bias_bt_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_l0a = PatternNode(('im2col_fractal', 'im2col_fractal'), platform_info.scope_ca, 'a_col')
        a_col_before = PatternNode(('im2col_row_major', 'im2col_row_major'), platform_info.scope_cbuf, 'a_col_before')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        a_l1 = PatternNode(('dy_l1', 'dy_l1'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_gen_stride_gt_1_without_ub_with_bias_bt_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_l0a)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_l0a, a_col_before)
        pg.add_edge(a_col_before, a_l1)
        pg.add_edge(a_l1, a_filling)
        pg.add_edge(a_filling, a_ddr)
        pg.add_edge(a_filling, a_zero)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_col_before.input_in_same_scope = True
        a_filling.not_verify_correctness_of_path = [a_zero]
        a_l1.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_gt_1_with_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_ubuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_ubuf, 'a_filling')
        dy_vn = PatternNode(('dy_vn', 'dy_vn'), platform_info.scope_ubuf, 'dy_vn')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_gt_1_with_ub'
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, dy_vn)
        pg.add_edge(dy_vn, a_zero)
        pg.add_edge(dy_vn, a_filling)
        pg.add_edge(a_filling, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}

        return pg

    @staticmethod
    def template_dyn_gen_stride_gt_1_with_ub_with_bias_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_ubuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_ubuf, 'a_filling')
        dy_vn = PatternNode(('dy_vn', 'dy_vn'), platform_info.scope_ubuf, 'dy_vn')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_add_vector = PatternNode(('bias_add_vector', ''), platform_info.scope_ubuf, 'bias_add_vector')
        bias_ddr = PatternNode(('bias', ''), platform_info.scope_gm, 'bias_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_gt_1_with_ub_with_bias_ub'
        pg.add_edge(c_ddr, bias_add_vector)
        pg.add_edge(bias_add_vector, c_ub)
        pg.add_edge(bias_add_vector, bias_ddr)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, dy_vn)
        pg.add_edge(dy_vn, a_zero)
        pg.add_edge(dy_vn, a_filling)
        pg.add_edge(a_filling, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        bias_add_vector.input_in_same_scope = True

        return pg

    @staticmethod
    def template_dyn_gen_stride_eq_1_with_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_eq_1_with_ub'
        pg.add_edge(c_ddr, c_ub)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}

        return pg

    @staticmethod
    def template_dyn_gen_stride_eq_1_with_ub_with_bias_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_ub = PatternNode(('c_ub', ''), platform_info.scope_ubuf, 'c_ub')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_add_vector = PatternNode(('bias_add_vector', ''), platform_info.scope_ubuf, 'bias_add_vector')
        bias_ddr = PatternNode(('bias', ''), platform_info.scope_gm, 'bias_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_eq_1_with_ub_with_bias_ub'
        pg.add_edge(c_ddr, bias_add_vector)
        pg.add_edge(bias_add_vector, c_ub)
        pg.add_edge(bias_add_vector, bias_ddr)
        pg.add_edge(c_ub, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        bias_add_vector.input_in_same_scope = True

        return pg

    @staticmethod
    def template_dyn_gen_stride_gt_1_without_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        dy_vn = PatternNode(('dy_vn', 'dy_vn'), platform_info.scope_cbuf, 'dy_vn')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_gt_1_without_ub'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, dy_vn)
        pg.add_edge(dy_vn, a_zero)
        pg.add_edge(dy_vn, a_filling)
        pg.add_edge(a_filling, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_l1.input_in_same_scope = True
        dy_vn.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_gt_1_without_ub_with_bias_bt():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        dy_vn = PatternNode(('dy_vn', 'dy_vn'), platform_info.scope_cbuf, 'dy_vn')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_gt_1_without_ub_with_bias_bt'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, dy_vn)
        pg.add_edge(dy_vn, a_zero)
        pg.add_edge(dy_vn, a_filling)
        pg.add_edge(a_filling, a_ddr)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_l1.input_in_same_scope = True
        dy_vn.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_eq_1_without_ub():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_eq_1_without_ub'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_eq_1_without_ub_with_bias_bt():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_eq_1_without_ub_with_bias_bt'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(b_l0b, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, a_ddr)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_gt_1_without_ub_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        dy_vn = PatternNode(('dy_vn', 'dy_vn'), platform_info.scope_cbuf, 'dy_vn')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_gt_1_without_ub_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, dy_vn)
        pg.add_edge(dy_vn, a_zero)
        pg.add_edge(dy_vn, a_filling)
        pg.add_edge(a_filling, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_l1.input_in_same_scope = True
        dy_vn.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_gt_1_without_ub_with_bias_bt_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_zero = PatternNode(('dy_filling_i', 'init_zero'), platform_info.scope_cbuf, 'a_zero')
        a_filling = PatternNode(('dy_filling', 'stride_filling'), platform_info.scope_cbuf, 'a_filling')
        dy_vn = PatternNode(('dy_vn', 'dy_vn'), platform_info.scope_cbuf, 'dy_vn')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_gt_1_without_ub_with_bias_bt_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, dy_vn)
        pg.add_edge(dy_vn, a_zero)
        pg.add_edge(dy_vn, a_filling)
        pg.add_edge(a_filling, a_ddr)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}
        a_l1.input_in_same_scope = True
        dy_vn.input_in_same_scope = True

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_eq_1_without_ub_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_eq_1_without_ub_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, a_ddr)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}

        pg.post_process = post_process_fusion_type_of_quant

        return pg

    @staticmethod
    def template_dyn_gen_stride_eq_1_without_ub_with_bias_bt_fp32():
        c_ddr = PatternNode(('c_ddr', 'conv2d_backprop_input'), platform_info.scope_gm, 'tensor_dx_gm')
        c_l0c = PatternNode(('C', 'mad'), platform_info.scope_cc, 'c_col')
        b_l0b = PatternNode(('w_col', 'inverse_trans_dma'), platform_info.scope_cb, 'b_col')
        b_l1 = PatternNode(('kernel_l1', ''), platform_info.scope_cbuf, 'b_l1')
        b_ddr = PatternNode(('filter', ''), platform_info.scope_gm, 'b_ddr')
        # a_col_before -> a_col
        a_col = PatternNode(('img2col_fractal_v2', 'im2col_fractal_v2'), platform_info.scope_ca, 'a_col')
        # None -> a_l1
        a_l1 = PatternNode(('dy_l1_6d_cut', 'dy_l1_6d_cut'), platform_info.scope_cbuf, 'a_l1')
        a_ddr = PatternNode(('dedy', ''), platform_info.scope_gm, 'a_ddr')
        bias_bt = PatternNode(('bias_bt', ''), platform_info.scope_bt, 'bias_bt')
        bias_ddr = PatternNode(('tensor_bias', ''), platform_info.scope_gm, 'bias_ddr')
        bias_ub = PatternNode(('bias_ub', ''), platform_info.scope_ubuf, 'bias_ub')
        bias_l1 = PatternNode(('bias_l1', ''), platform_info.scope_cbuf, 'bias_l1')
        bias_zero = PatternNode(('bias_zero', ''), platform_info.scope_cbuf, 'bias_zero')

        pg = Graph(c_ddr)
        pg.name = 'template_dyn_gen_stride_eq_1_without_ub_with_bias_bt_fp32'
        pg.add_edge(c_ddr, c_l0c)
        pg.add_edge(c_l0c, a_col)
        pg.add_edge(c_l0c, b_l0b)
        pg.add_edge(c_l0c, bias_bt)
        pg.add_edge(b_l0b, b_l1)
        pg.add_edge(b_l1, b_ddr)
        pg.add_edge(a_col, a_l1)
        pg.add_edge(a_l1, a_ddr)
        if (platform_info.intrinsic_check_support("Intrinsic_set_l1") and
            not platform_info.intrinsic_check_support("Intrinsic_set_l1", "f32")):
            pg.add_edge(bias_ub, bias_ddr)
            pg.add_edge(bias_l1, bias_ub)
        else:
            pg.add_edge(bias_l1, bias_ddr)
            pg.add_edge(bias_l1, bias_zero)
        pg.add_edge(bias_bt, bias_l1)

        pg.tensor_attr = {'fusion_type': 1, 'fused_double_operand_num': 0}

        pg.post_process = post_process_fusion_type_of_quant

        return pg


def templates():
    return [
        # binary constant
        GeneralTemplates.template_gen_stride_eq_1_with_ub(),
        GeneralTemplates.template_gen_stride_eq_1_with_ub_with_bias_ub(),
        GeneralTemplates.template_gen_stride_eq_1_without_ub(),
        GeneralTemplates.template_gen_stride_eq_1_without_ub_fp32(),
        GeneralTemplates.template_gen_stride_eq_1_without_ub_with_bias_bt(),
        GeneralTemplates.template_gen_stride_eq_1_without_ub_with_bias_bt_fp32(),
        GeneralTemplates.template_gen_stride_gt_1_with_ub(),
        GeneralTemplates.template_gen_stride_gt_1_with_ub_with_bias_ub(),
        GeneralTemplates.template_gen_stride_gt_1_without_ub(),
        GeneralTemplates.template_gen_stride_gt_1_without_ub_fp32(),
        GeneralTemplates.template_gen_stride_gt_1_without_ub_with_bias_bt(),
        GeneralTemplates.template_gen_stride_gt_1_without_ub_with_bias_bt_fp32(),
        GeneralTemplates.template_dyn_gen_stride_eq_1_with_ub(),
        GeneralTemplates.template_dyn_gen_stride_eq_1_with_ub_with_bias_ub(),
        GeneralTemplates.template_dyn_gen_stride_gt_1_with_ub(),
        GeneralTemplates.template_dyn_gen_stride_gt_1_with_ub_with_bias_ub(),
        GeneralTemplates.template_dyn_gen_stride_eq_1_without_ub(),
        GeneralTemplates.template_dyn_gen_stride_eq_1_without_ub_fp32(),
        GeneralTemplates.template_dyn_gen_stride_eq_1_without_ub_with_bias_bt(),
        GeneralTemplates.template_dyn_gen_stride_eq_1_without_ub_with_bias_bt_fp32(),
        GeneralTemplates.template_dyn_gen_stride_gt_1_without_ub(),
        GeneralTemplates.template_dyn_gen_stride_gt_1_without_ub_fp32(),
        GeneralTemplates.template_dyn_gen_stride_gt_1_without_ub_with_bias_bt(),
        GeneralTemplates.template_dyn_gen_stride_gt_1_without_ub_with_bias_bt_fp32(),
    ]


def construct_ori_graph(outs):
    def node(tensor, record):
        if tensor.name not in record:
            record[tensor.name] = OriNode(tensor.name, tensor.op.tag, tensor)
        return record[tensor.name]
    out = outs[0]
    # TASKS double out
    record = {}
    root_node = node(out, record)
    og = Graph(root_node)

    tensor_list = collections.deque([out])
    while len(tensor_list) != 0:
        tensor = tensor_list.popleft()
        curr_node = node(tensor, record)
        for prev_tensor in tensor.op.input_tensors:
            prev_node = node(prev_tensor, record)
            og.add_edge(curr_node, prev_node)

        tensor_list.extend(tensor.op.input_tensors)

    return og


def collect_attrs_from_compute_graph(out, tensor_attr):
    def convert_type_from_tvm_to_python(item):
        if isinstance(item, tvm.ir.container.Array):
            return cube_util.shape_to_list(item)
        elif isinstance(item, tvm.ir.container.Map):
            return cube_util.convert_tvm_map_to_dict(item)
        elif hasattr(item, 'value'):
            return item.value
        return item

    tensor_list = collections.deque([out])
    while len(tensor_list) != 0:
        tensor = tensor_list.popleft()
        tensor_attr.update({k: convert_type_from_tvm_to_python(v) for k, v in tensor.op.attrs.items()})
        tensor_list.extend(tensor.op.input_tensors)


def collect_tensor_from_pattern_graph(og, tensor_map):
    for node in og.iterate_all():
        if node.alias is not None:
            tensor_map[node.alias] = node.tensor


def set_special_attr(tensor_attr, tensor_map):
    # configuration of feature: dx+dequant+quant or dx+requant
    l0c_multi_group_flag = False
    ci1g = tensor_attr['group_dict']['dx_c1_extend']
    g_extend = tensor_attr['group_dict']['g_extend']
    if tensor_map['deconv_res'].dtype == "int8":
        if ci1g % 2 == 1 and g_extend > 1:
            l0c_multi_group_flag = True
    tensor_attr['l0c_multi_group_flag'] = l0c_multi_group_flag

    # configuration of feature: improve performance by correcting stride_h/w to 1
    _, _, hi, wi, _ = tensor_attr['output_shape']
    ho = tensor_attr['ho']
    wo = tensor_attr['wo']
    if hi == 1 and ho == 1:
        tensor_attr['stride_h'] = 1
    if wi == 1 and wo == 1:
        tensor_attr['stride_w'] = 1
    need_expand_stride = not (tensor_attr['stride_h'] == 1 and tensor_attr['stride_w'] == 1)
    tensor_attr['need_expand_stride'] = need_expand_stride

    # configuration of feature: split_w
    tensor_attr["split_w"] = SplitAxisMode(tensor_attr.get(SPLIT_AXIS_MODE_STR,
                                                           SplitAxisMode.split_hw.value)) == SplitAxisMode.split_w

    # configuration of feature: dma copy replace img2col
    # NOTE not support in tilingcase
    tensor_attr["no_ub_and_dma_copy_flag"] = False

    # configuration of feature: int4
    tensor_attr["support_stride_expand_in_ub"] = not tensor_map["b_ddr"].dtype == "int4" and not tensor_attr.get(
        "support_l0c_to_out")


def is_valid_tensor_attr(tensor_attr):
    list_key = ('need_expand_stride', 'output_shape', 'stride_h', 'stride_w', 'ho', 'wo', 'group_dict', 'padding',
                'dilation', 'kernel_name', IS_CONV1D_SITUATION_STR)
    fail_key = []
    for key in list_key:
        if key not in tensor_attr:
            fail_key.append(key)
    return len(fail_key) == 0, fail_key


def is_valid_tensor_map(tensor_map):
    list_key = ('deconv_res', 'a_ddr', 'b_ddr')
    fail_key = []
    for key in list_key:
        if key not in tensor_map:
            fail_key.append(key)
    return len(fail_key) == 0, fail_key


def parse_compute_graph(outs):
    og = construct_ori_graph(outs)

    support = False
    for pg in templates():
        if match_graph(og, pg):
            support = True
            log.debug(f'match graph {repr(pg)}')
            break
    if not support:
        return False, None, None, None

    sync_graph(og, pg)
    tensor_map = {}

    # TASKS double out tensor
    tensor_map['deconv_res'] = outs[0]
    collect_tensor_from_pattern_graph(og, tensor_map)
    tensor_attr = {}
    collect_attrs_from_compute_graph(outs[0], tensor_attr)
    set_intrinsic_support(tensor_attr)
    set_special_attr(tensor_attr, tensor_map)
    og.update_tensor_attr(tensor_attr)
    if og.post_process:
        og.post_process(pg, og, tensor_map, tensor_attr)

    res, fail_key = is_valid_tensor_attr(tensor_attr)
    if not res:
        raise_err_message_cube(f"cannot find {fail_key} in tensor_attr")
    return True, tensor_map, tensor_attr, og


def _set_filter_shape(tensor_map, tensor_attr):
    co1g = tensor_attr['group_dict'][cube_util.GroupDictKeys.co1g]
    ci1g = tensor_attr['group_dict'][cube_util.GroupDictKeys.ci1g]
    g_extend = tensor_attr['group_dict'][cube_util.GroupDictKeys.g_extend]
    hk = tensor_attr['kernel_h']
    wk = tensor_attr['kernel_w']
    channel_merge_ratio = tensor_attr.get("channel_merge_ratio")


    if tensor_attr.get("WEIGHT_NHWC_TRANS_FZ"):
        weight_fz_tensor = tensor_map['b_l1']
    else:
        weight_fz_tensor = tensor_map['b_ddr']
    if tensor_map['b_ddr'].dtype in QUANT_DTYPES:
        # GCout1HkWk, Cin1, Cin0, Cout0
        _, _, b_ddr_n0, b_ddr_k0 = list(i.value for i in weight_fz_tensor.shape)
    else:
        # GCin1HkWk, Cout1, Cout0, Cin0
        _, _, b_ddr_k0, b_ddr_n0 = cube_util.shape_to_list(weight_fz_tensor.shape)
    # G, Cout, Cin1, Hk, Wk, Cin0
    filter_shape_g = [co1g * b_ddr_k0, ci1g, hk, wk, b_ddr_n0]
    l0c_multi_group_flag = False
    if channel_merge_ratio is not None:
        if ci1g % channel_merge_ratio != 0 and g_extend > 1:
            l0c_multi_group_flag = True
        else:
            filter_shape_g[1] = align(filter_shape_g[1], channel_merge_ratio)

    return filter_shape_g, l0c_multi_group_flag


def calc_padding_for_auto_tiling(tensor_attr):
    if not tensor_attr.get('l0a_dma_flag'):
        return tensor_attr["padding"]
    if tensor_attr.get("need_expand_stride") or tensor_attr.get("no_ub_and_dma_copy_flag"):
        padu, padd, padl, padr = tensor_attr["dma_pad"]
        # if pad < 0, give tiling UINT32_MAX(pad in tiling is uint32), then tiling knows ubuf is used
        if padu + padd < 0:
            padu = UINT32_MAX
            padd = 0
        if padl + padr < 0:
            padl = UINT32_MAX
            padr = 0
        pad_modify = [0 if i < 0 else i for i in [padu, padd, padl, padr]]
        return pad_modify
    return (0, 0, 0, 0)


def calc_inout_shape_g(tensor_map, tensor_attr):
    # definition: real shape of out_backprop: n, co1, ho, wo, co0(16)
    # definition: tiling shape of out_backprop: n, co1g * co1g_factor, ho, wo, co0_reduce
    co1g_ddr_2_l0c = 2
    if tensor_map['a_col'] is None or tensor_map['a_col'].dtype != 'float32':
        co1g_ddr_2_l0c = 1

    if len(tensor_map['a_ddr'].shape) == 5:
        n, co1, ho, wo, co0 = cube_util.shape_to_list(tensor_map['a_ddr'].shape)
    elif len(tensor_map['a_ddr'].shape) == 4:
        # NOTE only support n, h, w, c
        n, ho, wo, co = cube_util.shape_to_list(tensor_map['a_ddr'].shape)
        # TASKS not support float32 int8
        co0 = platform_info.CUBE_MKN.get(tensor_map['a_ddr'].dtype)["mac"][1]
    else:
        raise_err_message_cube(f"not support length of {tensor_map['a_ddr']} is {len(tensor_map['a_ddr'])}")
    co1g = tensor_attr['group_dict']['dy_c1_extend']
    ci1g = tensor_attr['group_dict']['dx_c1_extend']
    img_shape_g = [n, co1g * co1g_ddr_2_l0c, ho, wo, co0]
    # definition: co1g * co0, ci1g, hk, hw, ci0
    filter_shape_g = _set_filter_shape(tensor_map, tensor_attr)[0]
    # definition: n, ci1g, hi, wi, ci0
    _, _, hi, wi, ci0 = tensor_attr['output_shape']
    output_shape_g = [n, ci1g, hi, wi, ci0]
    return img_shape_g, filter_shape_g, output_shape_g


def construct_info_dict(tensor_map, tensor_attr):
    img_shape_g, filter_shape_g, output_shape_g = calc_inout_shape_g(tensor_map, tensor_attr)
    padu, padd, padl, padr = calc_padding_for_auto_tiling(tensor_attr)
    dilation_h, dilation_w = tensor_attr['dilation']
    stride_h = tensor_attr['stride_h']
    stride_w = tensor_attr['stride_w']

    bias_flag = int(
        tensor_map.get('c_add_bias') is not None or tensor_map.get('bias_add_vector') is not None
        or tensor_map.get("bias_bt") is not None)

    # fusion_param
    fusion_para = DeConvPattern.fusion_para_map
    l1_fusion_type = L1FusionType(fusion_para.get("l1_fusion_type"))
    input_mem = ScopeDDR(fusion_para.get("input_memory_type"))
    # TASKS support l1_fusion
    out_mem = [0]
    fusion_type = tensor_attr['fusion_type']

    tiling_dtype = get_inout_dtype(tensor_map['a_ddr'], tensor_map['b_ddr'], tensor_map['tensor_dx_gm'],
                                   "Conv2DBackpropInput")

    info_dict = {
        "op_type": "conv2d_backprop_input",
        "A_shape": list(img_shape_g),
        "B_shape": list(filter_shape_g),
        "C_shape": list(output_shape_g),
        "A_dtype": str(tiling_dtype[0]),
        "B_dtype": str(tiling_dtype[1]),
        "C_dtype": str(tiling_dtype[2]),
        "mad_dtype": str(tiling_dtype[3]),
        "padl": padl,
        "padr": padr,
        "padu": padu,
        "padd": padd,
        "strideH": 1,
        "strideW": 1,
        "strideH_expand": stride_h,
        "strideW_expand": stride_w,
        "dilationH": dilation_h,
        "dilationW": dilation_w,
        "group": tensor_attr['group_dict'][cube_util.GroupDictKeys.g_extend],
        "bias_flag": bias_flag,
        "fused_double_operand_num": tensor_attr['fused_double_operand_num'],
        "kernel_name": str(tensor_attr['kernel_name']),
        "in_fm_memory_type": [input_mem.value],
        "out_fm_memory_type": out_mem,
        "l1_fusion_type": l1_fusion_type.value,
        "fusion_type": fusion_type,
        "general_flag": True,
        "split_axis_mode": tensor_attr['split_w']
    }

    if 'fixpipe_tensor' in tensor_map:
        info_dict['fixpipe_flag'] = get_fixpipe_flag(tensor_map['fixpipe_tensor'])
    if 'bias_bt' in tensor_map:
        info_dict['bias_dtype'] = tensor_map['bias_ddr'].dtype
    return info_dict


def extract_bits_from_int(num, offset, bits):
    return num >> offset & (2**bits - 1)


ItemTilingKey = collections.namedtuple('ItemTilingKey', 'name offset bits')

# | name                 | offset | bit |
# | -------------------- | ------ | --- |
# | groups_gt_1          | 20     | 1   |
# | split_w_flag         | 19     | 1   |
# | min_k1_div_k0_is_1   | 18     | 1   |
# | extend_tiling_offset | 16     | 2   |
# | db_al1               | 15     | 1   |
# | db_bl1               | 14     | 1   |
# | db_l0c               | 13     | 1   |
# | db_cub               | 12     | 1   |
# | abkl1_attach_flag    | 10     | 2   |
# | al1_attach_flag      | 8      | 2   |
# | bl1_attach_flag      | 6      | 2   |
# | conv1d_flag          | 5      | 1   |
# | load3d_special       | 3      | 2   |
# | kernel_mode          | 0      | 3   |
TILING_ITEMS_DX = (ItemTilingKey('kernel_mode', 0, 3),
                   ItemTilingKey('load3d_special', 3, 2),
                   ItemTilingKey('conv1d_flag', 5, 1),
                   ItemTilingKey('bl1_attach_flag', 6, 2),
                   ItemTilingKey('al1_attach_flag', 8, 2),
                   ItemTilingKey('abkl1_attach_flag', 10, 2),
                   ItemTilingKey('db_cub', 12, 1),
                   ItemTilingKey('db_l0c', 13, 1),
                   ItemTilingKey('db_bl1', 14, 1),
                   ItemTilingKey('db_al1', 15, 1),
                   ItemTilingKey('extend_tiling_offset', 16, 2),
                   ItemTilingKey('min_k1_div_k0_is_1', 18, 1),
                   ItemTilingKey('split_w_flag', 19, 1),
                   ItemTilingKey('groups_gt_1', 20, 1))


def decode_tiling_key(tiling_key: int, items: List[ItemTilingKey]) -> Dict[str, int]:
    return {item.name: extract_bits_from_int(tiling_key, item.offset, item.bits) for item in items}


class TilingDx:
    @staticmethod
    def convert_to_tiling_strategy(tiling_key: int, tiling_data: Dict[str, int], block_reduce: int, tensor_attr):
        flags = decode_tiling_key(tiling_key, TILING_ITEMS_DX)

        min_kl1 = tiling_data['min_kl1_div_kl0'] * tiling_data['k_l0'] * block_reduce
        max_kl1 = tiling_data['max_kl1_div_min_kl1'] * min_kl1

        abkl1_attach_flag = flags['abkl1_attach_flag']
        if abkl1_attach_flag == 0:
            k_bl1 = k_al1 = max_kl1
        elif abkl1_attach_flag == 1:
            k_al1 = max_kl1
            k_bl1 = min_kl1
        elif abkl1_attach_flag == 2:
            k_al1 = min_kl1
            k_bl1 = max_kl1
        else:
            raise_err_message_cube(f"not support abkl1_attach_flag is {abkl1_attach_flag}")

        tiling_al1_shape = [k_al1, tiling_data['m_al1'], 1, 1] if tiling_data['m_al1'] != 0 else []
        tiling_bl1_shape = [k_bl1, tiling_data['n_bl1'], 1, 1] if tiling_data['n_bl1'] != 0 else []

        tiling_strategy = {
            'block_dim': [tiling_data['batch_dim'], tiling_data['n_dim'],
                          tiling_data['m_dim'], tiling_data['group_dim']],
            'AL0_matrix': [tiling_data['m_l0'], tiling_data['k_l0'], utils.CUBE_SIZE, block_reduce, 1, 1],
            'BL0_matrix': [tiling_data['k_l0'], tiling_data['n_l0_div_ub'] * tiling_data['n_ub'],
                            utils.CUBE_SIZE, block_reduce, 1, 1],
            'CL0_matrix': [tiling_data['n_l0_div_ub'] * tiling_data['n_ub'], tiling_data['m_l0'],
                            utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
            'CUB_matrix': [tiling_data['n_ub'], tiling_data['m_l0'], utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
            'BUB_shape': [-1, -1, 1, 1],
            'AL1_shape': tiling_al1_shape,
            'BL1_shape': tiling_bl1_shape,
            'AUB_shape': [tiling_data['k_aub'], tiling_data['m_aub'], tiling_data['wo_aub'], 1],
            'n_bef_batch_flag': 0, 'n_bef_group_flag': 0, 'batch_bef_group_flag': 0,
            'A_overhead_opt_flag': 0, 'B_overhead_opt_flag': 0,
            'AUB_channel_wise_flag': None, 'BUB_channel_wise_flag': None, 'CUB_channel_wise_flag': None,
            'manual_pingpong_buffer': {'AUB_pbuffer': utils.DB_OFF, 'BUB_pbuffer': utils.DB_OFF,
                                       'AL1_pbuffer': flags['db_al1'] + 1, 'BL1_pbuffer': flags['db_bl1'] + 1,
                                       'AL0_pbuffer': utils.DB_ON, 'BL0_pbuffer': utils.DB_ON,
                                       'CL0_pbuffer': flags['db_l0c'] + 1,
                                       'CUB_pbuffer': flags['db_cub'] + 1, 'UBG_pbuffer': utils.DB_OFF},
            'attach_at_flag': {'cub_attach_flag': utils.ATTACH_LESS,
                               'cl0_attach_flag': utils.ATTACH_LARGE, 'al0_attach_flag': utils.ATTACH_LESS,
                               'bl0_attach_flag': utils.ATTACH_LESS,
                               'al1_attach_flag': flags['al1_attach_flag'],
                               'bl1_attach_flag': flags['bl1_attach_flag'],
                               'aub_attach_flag': utils.ATTACH_LESS,
                               'abkl1_attach_flag': abkl1_attach_flag, 'aub_multi_flag': -1,
                               'min_kl1_div_kl0_is_1_flag': flags['min_k1_div_k0_is_1']},
            'conv1d_flag': flags['conv1d_flag'],
            "load3d_special_flag": flags['load3d_special'],
            "simply_loop_mn_from_sc_to_l0_is_1": int(flags['extend_tiling_offset'] > 0),
            "co1g_ci1g_is_1": int(flags['extend_tiling_offset'] > 1),
            "split_axis_mode": flags['split_w_flag'],
            "groups_gt_1": flags['groups_gt_1']
        }
        return tiling_strategy


@register_tiling_case(pattern=Pattern.CONV2D_BACKPROP_INPUT)
def calc_conv2dbp_input(outs, option=None):
    """
    tiling_case func for dynamic shape conv2d_bp_input

    Parameters
    ----------
    outs : tvm tensor or list of tvm tensor, results for tvm compute

    Returns
    -------
    list of dict, each dict for a tiling case
    """
    context = op_context.get_context()

    res, tensor_map, tensor_attr, og = parse_compute_graph(outs)
    # static shape
    if not is_unify():
        if res:
            # 支持二进制模板的场景
            static_info_dict = construct_info_dict(tensor_map, tensor_attr)
            tiling = get_static_tiling(static_info_dict, tensor_map, tensor_attr)
            decode_tiling(tiling)
            post_process_get_tiling(tiling, tensor_attr, tensor_map)
            tiling_case = construct_tiling_case(tiling)
            tiling_case.update({'tensor_map': tensor_map, 'tensor_attr': tensor_attr, 'og': og})
            return tiling_case
        else:
            # 不支持二进制模板的场景
            return None

    if context.get_addition("enable_binary_constant"):
        add_compile_info("tiling_type", "binary")
        add_compile_info("binary_mode", DynamicConv2dBpInputParams.binary_mode)
        add_compile_info("block_dim", {"CORE_NUM": util.get_core_num()})

        run_info = do_op_tiling(context.get_addition('params_do_op_tiling')['op_type'], get_compile_info(),
                                context.get_addition('params_do_op_tiling')['inputs'],
                                context.get_addition('params_do_op_tiling')['outputs'],
                                None,
                                None,
                                context.get_addition('params_do_op_tiling')['attrs'])

        tiling_key, tiling_data = parse_run_info(run_info)
        log.debug(f"tiling_data {tiling_data}")
        tiling_strategies = Conv2dBpInputTiling.gen_binary_tiling_strategies(tensor_map["a_ddr"].dtype,
                                                                             tensor_attr["need_expand_stride"],
                                                                             DynamicConv2dBpInputParams.binary_mode,
                                                                             tensor_attr["split_axis_mode"])
        if tiling_key not in tiling_strategies:
            raise RuntimeError(f"tiling_strategies not match tiling_key {tiling_key}")
        tiling_cases = [construct_tiling_case(tiling_strategies[tiling_key], tiling_key, tiling_data)]
        for tiling_case in tiling_cases:
            tiling_case.update({'tensor_map': tensor_map, 'tensor_attr': tensor_attr, 'og': og})
        return tiling_cases

    var_names = ("batch_n", "dx_h", "dx_w")
    fuzz_build = get_context().get_build_type() == "fuzzily_build" and not DynamicConv2dBpInputParams.binary_mode
    conv_info = DynamicConv2dBpInputParams.tiling_info_dict
    if outs[-1].op.tag == "elewise_multiple_sel":
        conv_info["fused_double_operand_num"] = 1 / 16
        if "elewise_binary_add" in outs[-1].op.input_tensors[1].op.tag:
            conv_info["fused_double_operand_num"] += 1
            conv_info["fusion_type"] = 4
    tgt_list = []
    _get_target_area(conv_info, tgt_list, var_names)

    max_id = DEFAULT_KERNEL_ID
    if fuzz_build:  # parse input range
        # generate tgt_area by format
        ori_tensors = DynamicConv2dBpInputParams.ori_tensor
        op_type = DynamicConv2dBpInputParams.dynamic_para.get("op_type")
        if op_type in ["Conv2DBackpropInput", "depthwise_conv2d_backprop_input", "AvgPoolGrad"]:
            ori_tensors_input = ori_tensors.get("out_backprop")
        else:
            ori_tensors_input = ori_tensors.get("x")
        invalid = (not isinstance(ori_tensors, dict)) or (not isinstance(ori_tensors_input, dict))
        if invalid:
            raise RuntimeError("can't get input from para_dict")
        input_format = ori_tensors_input["ori_format"]
        # te fusion make sure that each range is within the range request
        range_str = get_context().get_addition("missing_support_info")
        range_list = []
        if len(range_str) > 0:
            range_list = parse_fuzz_build_range(json.loads(range_str))
        if len(range_list) > 0:
            tgt_list.clear()
            for item in range_list:
                # get dx_range depends on dy_range
                dy_range = copy.deepcopy(item)
                data_format = input_format
                ori_paras = {
                    "filters": ori_tensors.get("filters"), "bias": None, "offset_w": None,
                    "strides": (conv_info.get("strideH"), conv_info.get("strideW"), conv_info.get("strideH_expand"),
                                conv_info.get("strideW_expand")),
                    "pads": (
                        conv_info.get("padu"), conv_info.get("padd"), conv_info.get("padl"), conv_info.get("padr")),
                    "dilations": (1, 1, 1, 1), "groups": conv_info.get("group"), "data_format": data_format,
                    "output_padding": (0, 0, 0, 0), "offset_x": 0, "kernel_name": conv_info.get("kernel_name")
                }
                conv2d_backprop = Conv2dBackpropParaProcess(ori_paras)
                filter_shape_nchw = conv2d_backprop.get_input_nchw(ori_tensors.get("filters").get("ori_shape"),
                                                                   ori_tensors.get("filters").get("ori_format"))
                _, dy_range_nchw = conv2d_backprop.get_input_nchw((1, 1, 1, 1), data_format, dy_range)
                dx_range_nchw, _, _ = conv2d_backprop.get_input_range(filter_shape_nchw, dy_range_nchw)
                fuzz_area = {}
                fuzz_area["batch_n"] = tuple(dx_range_nchw[0])
                fuzz_area["dx_h"] = tuple(dx_range_nchw[2])
                fuzz_area["dx_w"] = tuple(dx_range_nchw[3])
                tgt_list.append(fuzz_area)
        # >>> start: get kernel id
        kernel_id = get_context().get_addition("max_kernel_id")
        valid = isinstance(kernel_id, int) and kernel_id > -2
        if valid:
            max_id = kernel_id + 1

    tiling_cases = []
    total_info = {}
    if conv_info.get("split_axis_mode", 0) == SplitAxisMode.split_w.value:
        max_id = DEFAULT_SPLIT_W_KERNEL_ID
    for tgt in tgt_list:
        new_info = copy.deepcopy(conv_info)
        tiling_op = Conv2dBpInputTiling(new_info)
        selector_dx = TilingSelection(tiling_op, max_id)
        tiling_cases += selector_dx.calc_tiling(tgt, var_names)
        # >>> start: gather compile_info process
        if fuzz_build:
            tgt_nhw = []
            for var_name in var_names:
                tgt_nhw.extend(tgt[var_name])
            current_info = get_compile_info().copy()
            id_list = list(current_info["block_dim"].keys())
            id_list.sort()
            max_id = id_list[-1] + 1
            # >>> conv2d dx start: make sure range is within tgt_nhw
            for range_key in ["repo_range", "cost_range"]:
                if isinstance(current_info.get(range_key), dict):
                    for kernel_id, range_x in current_info[range_key].items():
                        new_range = []
                        for index, dim_value in enumerate(range_x):
                            if index in (0, 2, 4):
                                new_range.append(tgt_nhw[index] if dim_value < tgt_nhw[index] else dim_value)
                            else:
                                new_range.append(tgt_nhw[index] if dim_value > tgt_nhw[index] else dim_value)
                        current_info[range_key][kernel_id] = new_range
            # <<< conv2d dx end: make sure range is within tgt_nhw
            if total_info:
                # >>> start: add new dict info
                for key, value in current_info.items():
                    need_update = isinstance(total_info.get(key), dict) \
                                  and isinstance(value, dict)
                    if need_update:
                        new_item = total_info.get(key)
                        new_item.update(value)
                        total_info[key] = new_item
                        add_compile_info(key, total_info.get(key))
            else:
                total_info = current_info
                # <<< end: add new dict info
        # <<< end: gather compile_info process
    return tiling_cases


class Conv2dBpInputTiling(CubeTilingOp):
    """
    get_tiling class for dynamic shape conv2d_bp_input
    """
    def __init__(self, tiling_info):
        super().__init__(tiling_info, None)
        self.a_info = self.tiling_info['A_shape']
        self.b_info = self.tiling_info['B_shape']
        self.c_info = self.tiling_info['C_shape']
        self.a_type = self.tiling_info["A_dtype"]
        self.c_type = self.tiling_info["C_dtype"]
        self.stride_h = self.tiling_info["strideH_expand"]
        self.binary_mode = DynamicConv2dBpInputParams.binary_mode
        self.var_map = DynamicConv2dBpInputParams.var_map
        self._get_calc_info()
        op_get_context().add('_use_cache_tiling', self.binary_mode)
        self.key = 'C_shape'
        self.op_type = "conv2d_backprop_input"
        self.split_axis_mode = self.tiling_info.get("split_axis_mode", 0)

    @staticmethod
    def check_template_valid(choice, split_axis_mode, support_l0c2out):
        """
        Check if the template is valid

        Returns
        -------
        bool: True, the template is valid
        """
        (al1_pb, bl1_pb, l0c_pb, _, abkl1_attach, al1_attach_flag, bl1_attach_flag,
         conv1d_flag, load3d_special, min_kl1_div_kl0_is_1_flag, groups_gt_1) = choice

        # The groups_gt_1 template is only used in scenarios with the unit_flag feature and groups > 1
        enable_unit_flag = support_l0c2out and l0c_pb == utils.DB_OFF
        invalid_choice = not enable_unit_flag and groups_gt_1

        # al1 full load
        invalid_choice = invalid_choice or (al1_attach_flag == utils.ATTACH_FULL_LOAD) and (
            (bl1_attach_flag in (utils.ATTACH_FULL_LOAD, utils.ATTACH_EQUAL) and abkl1_attach != 0) or
            (bl1_attach_flag == 2 and abkl1_attach != 1))

        invalid_choice = invalid_choice or (
            (al1_attach_flag == utils.ATTACH_FULL_LOAD and al1_pb == 2) or
            (bl1_attach_flag == utils.ATTACH_FULL_LOAD and bl1_pb == 2))

        # al1 attach at c_ddr
        invalid_choice = invalid_choice or (al1_attach_flag == utils.ATTACH_EQUAL and (
            (bl1_attach_flag in (utils.ATTACH_FULL_LOAD, utils.ATTACH_EQUAL) and abkl1_attach != 0) or
            (bl1_attach_flag == utils.ATTACH_LESS and abkl1_attach != 1)))

        # if al1 attach at l0c and full load in l1 buffer, there is no need to open double buffer
        invalid_choice = invalid_choice or (al1_attach_flag == utils.ATTACH_LESS and
                                            (bl1_attach_flag in (utils.ATTACH_FULL_LOAD, utils.ATTACH_EQUAL)
                                             and abkl1_attach != 2))

        invalid_choice = invalid_choice or (al1_attach_flag == utils.ATTACH_LARGE and abkl1_attach == utils.ATTACH_LESS)

        invalid_choice = invalid_choice or (
            min_kl1_div_kl0_is_1_flag == 1
            and not (al1_attach_flag == utils.ATTACH_LESS or bl1_attach_flag == utils.ATTACH_LESS))

        invalid_choice = invalid_choice or (split_axis_mode == SplitAxisMode.split_w.value and
                                            (al1_attach_flag == utils.ATTACH_FULL_LOAD))

        invalid_choice = invalid_choice or (split_axis_mode == SplitAxisMode.split_w.value and
                                            (conv1d_flag or load3d_special == 2))

        return invalid_choice

    @staticmethod
    def get_attach_choices():
        """
        generates all selections of 10 flags

        Returns
        -------
        list: all selections of flags
        """
        (al1_pb, bl1_pb, l0c_pb, cub_pb, abkl1_attach, al1_attach_flag,
         bl1_attach_flag, conv1d_flag, load3d_special_flag, min_kl1_div_kl0_is_1_flag, groups_gt_1) = (
            [utils.DB_OFF, utils.DB_ON], [utils.DB_OFF, utils.DB_ON],
            [utils.DB_OFF, utils.DB_ON], [utils.DB_OFF, utils.DB_ON],
            [utils.ATTACH_FULL_LOAD, utils.ATTACH_EQUAL, utils.ATTACH_LESS],
            [utils.ATTACH_FULL_LOAD, utils.ATTACH_EQUAL, utils.ATTACH_LESS, utils.ATTACH_LARGE],
            [utils.ATTACH_FULL_LOAD, utils.ATTACH_EQUAL, utils.ATTACH_LESS],
            [0, 1],
            [1, 2] if cube_util.is_load3d_constraint() else [1],
            [0, 1],
            [0, 1])
        attach_choices = list(
            product(al1_pb, bl1_pb, l0c_pb, cub_pb, abkl1_attach,
                    al1_attach_flag, bl1_attach_flag, conv1d_flag, load3d_special_flag, min_kl1_div_kl0_is_1_flag,
                    groups_gt_1))
        return attach_choices

    @staticmethod
    def modify_repo_tiling(tiling_mess):
        tiling = tiling_mess.get("tiling")
        n_size = tiling_mess.get("B_shape")[1]
        block_dim = tiling.get("block_dim")
        nc = tiling.get("CL0_matrix")[0]
        n_factor = utils.icd(n_size // block_dim[1], nc)
        block_dim[1] = n_size // nc // n_factor

    @staticmethod
    def get_kernel_mode(binary_mode, stride_expand_flag):
        '''
        --------------------------------------------------
        | kernel_mode |    format   | stride_expand_flag |
        --------------------------------------------------
        |      0      |  NC1HWC0    |          0         |
        |      1      |  NC1HWC0    |          1         |
        |      2      |     NCHW    |          0         |
        |      3      |     NCHW    |          1         |
        |      4      |     NHWC    |          0         |
        |      5      |     NHWC    |          1         |

        Returns
        ----------
        kernel_mode: int, kernel flag in binary
        '''
        kernel_mode = (binary_mode - 1) * 2 + stride_expand_flag
        return kernel_mode

    @staticmethod
    def extend_tiling(cache_tiling_all, offset_list):
        # structure of cache_tiling_all:
        # key: tiling_id] value: [[], cache_tiling, []]
        for ori_tiling_id in (8200, 8201):
            if ori_tiling_id not in cache_tiling_all:
                continue
            # In order to reduce the number of templates added, only the scenes where l0c can open double buffer in
            # MoileNetV2 is added:
            # kernel_mode 0/1 load3d_special 1 is_conv1d 0
            # bl1_attach 0 al1_attach 0 abkl1_attach 0
            # db_cub 0 db_l0c 1 db_bl1 0 db_al1 0

            # extend tiling id:
            # 00: original tiling
            # 01: simply_loop_mn_from_sc_to_l0_is_1
            # 10: simply_loop_mn_from_sc_to_l0_is_1 && co1g_ci1g_is_1
            cache_tiling = copy.copy(cache_tiling_all[ori_tiling_id])
            cache_tiling['simply_loop_mn_from_sc_to_l0_is_1'] = 1
            tiling_id = ori_tiling_id + (1 << offset_list[-1])
            cache_tiling_all[tiling_id] = cache_tiling

            cache_tiling = copy.copy(cache_tiling)
            cache_tiling['co1g_ci1g_is_1'] = 1
            tiling_id = ori_tiling_id + (2 << offset_list[-1])
            cache_tiling_all[tiling_id] = cache_tiling

    @staticmethod
    def cal_cache_tiling_id(choice, offset_list, kernel_mode, split_axis_mode):
        tiling_id = 0
        choice_extent = list(copy.deepcopy(choice))
        choice_extent.append(split_axis_mode)
        for idx, var_value in enumerate(choice_extent):
            if idx < 4:  # for saving bits, db value minus 1
                var_value -= 1
            tiling_id += var_value << offset_list[idx]
        tiling_id += kernel_mode
        return tiling_id

    @staticmethod
    def gen_binary_tiling_strategies(dtype_a, need_expand_stride, binary_mode, split_axis_mode):
        '''
        according to size in l1, generate 9 kind of templates, each subdivided into 132 different
        templates as follows templates according to size in l1 sub template
        --------------------------------------------|-----
        al1 @l0c and bl1 @l0c                       | 96
        al1 @l0c and bl1 @ddr                       | 32
        al1 @l0c and bl1 full load                  | 16
        al1 @ddr and bl1 @l0c                       | 32
        al1 @ddr and bl1 @ddr                       | 32
        al1 @ddr and bl1 full load                  | 16
        al1 full load and bl1 @l0c                  | 16
        al1 full load and bl1 @ddr                  | 16
        al1 full load and bl1 full load             | 8

        Returns
        ----------
        cache_tiling_all: list, include 132 different tiling templates
        '''
        def get_tvm_var(var_name):
            te_var = get_te_var(var_name)
            if te_var is None:
                raise_err_message_cube(f"cannot find tvm var with name {var_name}")
            return te_var.get_tvm_var()

        tiling_strategies = {}
        offset = TilingIdOffset()
        offset_list = list(vars(offset).values())
        block_reduce = platform_info.CUBE_MKN.get(dtype_a).get("mac")[1]
        kernel_mode = Conv2dBpInputTiling.get_kernel_mode(binary_mode, need_expand_stride)
        support_l0c2out = platform.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")

        for choice in Conv2dBpInputTiling.get_attach_choices():
            tiling_id = Conv2dBpInputTiling.cal_cache_tiling_id(choice, offset_list, kernel_mode, split_axis_mode)

            if Conv2dBpInputTiling.check_template_valid(choice, split_axis_mode, support_l0c2out):
                continue

            min_kl1 = get_tvm_var('min_kl1_div_kl0') * get_tvm_var('k_l0') * block_reduce
            max_kl1 = get_tvm_var('max_kl1_div_min_kl1') * min_kl1

            abkl1_attach_flag = choice[4]
            if abkl1_attach_flag == 0:
                k_bl1 = k_al1 = max_kl1
            elif abkl1_attach_flag == 1:
                k_al1 = max_kl1
                k_bl1 = min_kl1
            elif abkl1_attach_flag == 2:
                k_al1 = min_kl1
                k_bl1 = max_kl1
            else:
                raise_err_message_cube(f"not support abkl1_attach_flag is {abkl1_attach_flag}")

            tiling_strategy = {
                'block_dim': [get_tvm_var('batch_dim'), get_tvm_var('n_dim'),
                              get_tvm_var('m_dim'), get_tvm_var('group_dim')],
                'AL0_matrix': [get_tvm_var('m_l0'), get_tvm_var('k_l0'), utils.CUBE_SIZE, block_reduce, 1, 1],
                'BL0_matrix': [get_tvm_var('k_l0'), get_tvm_var('n_l0_div_ub') * get_tvm_var('n_ub'),
                               utils.CUBE_SIZE, block_reduce, 1, 1],
                'CL0_matrix': [get_tvm_var('n_l0_div_ub') * get_tvm_var('n_ub'), get_tvm_var('m_l0'),
                               utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
                'CUB_matrix': [get_tvm_var('n_ub'), get_tvm_var('m_l0'), utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
                'BUB_shape': [-1, -1, 1, 1], 'AL1_shape': [k_al1, get_tvm_var('m_al1'), 1, 1],
                'BL1_shape': [k_bl1, get_tvm_var('n_bl1'), 1, 1],
                'AUB_shape': [get_tvm_var('k_aub'), get_tvm_var('m_aub'), get_tvm_var('wo_aub'), 1],
                'n_bef_batch_flag': 0, 'n_bef_group_flag': 0, 'batch_bef_group_flag': 0,
                'A_overhead_opt_flag': 0, 'B_overhead_opt_flag': 0,
                'AUB_channel_wise_flag': None, 'BUB_channel_wise_flag': None, 'CUB_channel_wise_flag': None,
                'manual_pingpong_buffer': {'AUB_pbuffer': utils.DB_OFF, 'BUB_pbuffer': utils.DB_OFF,
                                           'AL1_pbuffer': choice[0], 'BL1_pbuffer': choice[1],
                                           'AL0_pbuffer': utils.DB_ON, 'BL0_pbuffer': utils.DB_ON,
                                           'CL0_pbuffer': choice[2],
                                           'CUB_pbuffer': choice[3], 'UBG_pbuffer': utils.DB_OFF},
                'attach_at_flag': {'cub_attach_flag': utils.ATTACH_LESS,
                                   'cl0_attach_flag': utils.ATTACH_LARGE, 'al0_attach_flag': utils.ATTACH_LESS,
                                   'bl0_attach_flag': utils.ATTACH_LESS,
                                   'al1_attach_flag': choice[5], 'bl1_attach_flag': choice[6],
                                   'aub_attach_flag': utils.ATTACH_LESS,
                                   'abkl1_attach_flag': abkl1_attach_flag, 'aub_multi_flag': -1,
                                   'min_kl1_div_kl0_is_1_flag': choice[9]},
                'conv1d_flag': choice[7],
                "load3d_special_flag": choice[8],
                "simply_loop_mn_from_sc_to_l0_is_1": 0,
                "co1g_ci1g_is_1": 0,
                "split_axis_mode": split_axis_mode,
                "groups_gt_1": choice[10]
            }

            tiling_strategies[tiling_id] = tiling_strategy
        Conv2dBpInputTiling.extend_tiling(tiling_strategies, offset_list)
        return tiling_strategies

    def get_repo_tiling(self):
        """
        get tiling from repository

        Returns
        -------
        tiling: shape and tiling retrieved from repository
        """

        tiling_list = get_tiling(self.tiling_info)
        res_list = []
        for tiling_mess in tiling_list:
            Conv2dBpInputTiling.modify_repo_tiling(tiling_mess)
            # in dx_opti, tiling's C_shape returned from repository is 0,
            # we calculate C_shape according to A_shape and stride
            if tiling_mess["C_shape"][0] == -1:
                tiling_mess["C_shape"][0] = tiling_mess["A_shape"][0]
            if tiling_mess["C_shape"][2] == 0:
                tiling_mess["C_shape"][2] = tiling_mess["A_shape"][2] * self.stride_h
            if tiling_mess["C_shape"][3] == 0:
                tiling_mess["C_shape"][3] = tiling_mess["A_shape"][3] * self.stride_w
            # pad set -1 to get tilings from repository, so we need to
            # check A_shape&C_shape to filter tilings not matched with
            # current kernel_info out
            t_h, t_w = self.get_output_h(tiling_mess["C_shape"][2]), \
                self.get_output_w(tiling_mess["C_shape"][3])
            if (t_h == tiling_mess["A_shape"][2] and t_w == tiling_mess["A_shape"][3]
                    and self.check_tiling_ub(tiling_mess)):
                res_list.append(tiling_mess)
        return res_list

    def get_ub_fusion_para(self):
        '''
        -----------------------------------------------------------------
        | format | stride_expand_flag |    aub_num     |    cub_num     |
        -----------------------------------------------------------------
        | NC1HWC0|          0         |        0       |        1       |
        | NC1HWC0|          1         |        1       |        1       |
        | NCHW   |          0         |        2       |        2       |
        | NCHW   |          1         |        1       |        2       |
        -----------------------------------------------------------------

        Returns
        ----------
        aub_num: int, The amount of space that aub needs to occupy
        bub_num: int, The amount of space that bub needs to occupy
        cub_num: int, The amount of space that cub needs to occupy
        ub_size: int, size of allocable ub

        '''
        stride_expand_flag = 0 if isinstance(self.stride_h, int) else 1
        aub_num = 0
        bub_num = 0
        cub_num = 1
        ub_size = platform_info.get_soc_spec("UB_SIZE")
        if self.binary_mode == BinaryMode.NC1HWC0:
            if stride_expand_flag == 1:
                aub_num = 1
        elif self.binary_mode == BinaryMode.NCHW or self.binary_mode == BinaryMode.NHWC:
            aub_num = 2
            cub_num = 2
            ub_size = ub_size - NONALIGNED_FUSION_EXTRA_SIZE
            if stride_expand_flag == 1:
                aub_num = 1
        result = {"aub_num": aub_num, "bub_num": bub_num, "cub_num": cub_num, "ub_size": ub_size}
        return result

    def get_cache_tiling(self):
        need_expand_stride = 0 if isinstance(self.stride_h, int) else 1
        tiling_strategies = Conv2dBpInputTiling.gen_binary_tiling_strategies(self.a_type, need_expand_stride,
                                                                             self.binary_mode, self.split_axis_mode)
        return [
            construct_tiling_case(tiling_strategy, tiling_id)
            for tiling_id, tiling_strategy in tiling_strategies.items()
        ]

    def check_tiling_ub(self, tiling_mess):
        """
        Check if tiling in repository ub space is legal

        Parameters
        ----------
        tiling_mess: shape and tiling retrieved from repository

        Returns
        -------
        tiling_valid_flag: If true means it's legal
        """
        if platform_info.intrinsic_check_support(INTRINSTIC_FIX_PIPE_L0C2OUT):
            return True
        tiling = tiling_mess.get('tiling')
        cub_db_flag = tiling.get("manual_pingpong_buffer").get("CUB_pbuffer")
        cub_dtype_bit = BIT_RATIO_DICT.get(self.c_type)
        cub_size = (reduce(lambda x, y: x * y, tiling.get("CUB_matrix")) * cub_db_flag * cub_dtype_bit)
        fused_double_operand_num = tiling_mess.get('fused_double_operand_num')
        fused_double_operand_num = fused_double_operand_num if fused_double_operand_num is not None else 0
        fused_double_operand_num /= FUSED_DOUBLE_OPERAND_MUL
        ub_size_limit = platform_info.get_soc_spec("UB_SIZE")
        bias_flag = 1 if tiling_mess.get('bias_flag') is True else 0
        bias_size = tiling_mess.get("C_shape")[1] * tiling_mess.get("C_shape")[-1] * bias_flag * cub_dtype_bit
        if (self.stride_h > 1 or self.stride_w > 1):
            if tiling.get("AUB_shape"):
                aub_tiling_k, aub_tiling_m, _, _ = tiling.get("AUB_shape")
                aub_co1 = aub_tiling_k // (self.b_info[2] * self.b_info[3] * utils.FP16_K)
                aub_w = tiling_mess["A_shape"][3] * self.stride_w
                aub_db = tiling.get("manual_pingpong_buffer").get("AUB_pbuffer")
                aub_bit = BIT_RATIO_DICT.get(self.a_type)
                aub_filling_size = aub_co1 * aub_tiling_m * aub_w * utils.FP16_K * aub_db * aub_bit
                cub_size *= (1 + fused_double_operand_num)
                if (cub_size + aub_filling_size + bias_size) > ub_size_limit:
                    return False
            elif self.k_h == 1 and self.k_w == 1:
                dedy_h, dedy_w = tiling_mess.get("A_shape")[2:4]
                dx_w = tiling_mess.get("C_shape")[3]
                nc_factor, mc_factor, m0, n0 = tiling.get("CUB_matrix")[:4]
                mc_from_tiling = mc_factor * m0
                max_n_is_hfactor = (ub_size_limit - cub_size) // (
                    nc_factor * n0 * cub_db_flag * cub_dtype_bit * self.stride_h) // dx_w
                if mc_from_tiling >= dedy_w:
                    if mc_from_tiling % dedy_w == 0 and (mc_from_tiling * self.stride_h * self.stride_w <=
                                                         utils.NHW_MAX) and dedy_h % (mc_from_tiling // dedy_w) == 0:
                        n_is_hfactor_val = mc_from_tiling // dedy_w
                    else:
                        n_is_hfactor_val = (mc_from_tiling - utils.FP16_M) // dedy_w
                else:
                    n_is_hfactor_val = (mc_from_tiling - utils.FP16_M) // dedy_w
                n_is_hfactor = min(n_is_hfactor_val, max_n_is_hfactor)
                dilate_l0c_m = dx_w * n_is_hfactor * self.stride_h
                cub_dilate_size = dilate_l0c_m * nc_factor * n0 * cub_db_flag * cub_dtype_bit
                cub_dilate_size *= (1 + fused_double_operand_num)
                if (cub_size + cub_dilate_size + bias_size) > ub_size_limit:
                    return False
        return True

    def check_tiling_al0(self, tiling_mess):
        """
        Check if tiling in repository al0 space is legal

        Parameters
        ----------
        tiling_mess: shape and tiling retrieved from repository
        Returns
        -------
        tiling_valid_flag: If true means it's legal
        """
        tiling_tmp = tiling_mess.get('tiling')
        l0_shape = "AL0_matrix"
        l0_space = platform_info.get_soc_spec("L0A_SIZE")
        row = tiling_tmp.get(l0_shape)[0]
        col = tiling_tmp.get(l0_shape)[1]
        group = tiling_tmp.get(l0_shape)[5]
        if row == 0 or col == 0:
            return False
        l0_dtype_bit = BIT_RATIO_DICT.get(self.a_type)
        data_amount_l0 = (
            row
            * col
            * tiling_tmp.get(l0_shape)[2]
            * tiling_tmp.get(l0_shape)[3]
            * group
            * l0_dtype_bit
        )
        if isinstance(data_amount_l0, int) and data_amount_l0 > l0_space:
            DynamicConv2dBpInputParams.dynamic_para["correct_range_flag"] = True
            return False
        return True

    def get_costmodel_tiling(self, shape):
        """
        get tiling using cost model

        Parameters
        ----------
        shape: specified shape to get tiling

        Returns
        -------
        tiling: tiling retrieved by cost model
        """

        if "batch_n" in self.var_map:
            self.a_info[0] = shape if isinstance(shape, int) else shape[0]
            self.c_info[0] = shape if isinstance(shape, int) else shape[0]
        if "dx_h" in self.var_map:
            self.c_info[2] = shape[1]
            self.a_info[2] = self.get_output_h(self.c_info[2])
        if "dx_w" in self.var_map:
            self.c_info[3] = shape[2]
            self.a_info[3] = self.get_output_w(self.c_info[3])
        self.tiling_info["tiling_type"] = "cost_model_tiling"
        for pad in ("padl", "padr", "padu", "padd"):
            self.tiling_info[pad] = 0
        if self.pad_mode == "FIX":
            _, _, dy_h, dy_w, _ = self.a_info
            new_hw = (dy_h * self.stride_h, dy_w * self.stride_w)
            new_pad_before = (
                (self.k_h - 1) * self.dilate_h - self.cur_pads[2],
                (self.k_w - 1) * self.dilate_w - self.cur_pads[0]
            )
            pad_up_before, pad_left_before = new_pad_before

            _, _, dx_h, dx_w, _ = self.c_info
            new_pad_after = tuple(
                i - o - pb + (k - 1) * d
                for i, o, pb, k, d in zip(
                    (dx_h, dx_w),
                    new_hw,
                    new_pad_before,
                    (self.k_h, self.k_w),
                    (self.dilate_h, self.dilate_w)
                )
            )
            pad_down_after, pad_right_after = new_pad_after

            pad_up_before = (pad_up_before + abs(pad_up_before)) // 2
            pad_left_before = (pad_left_before + abs(pad_left_before)) // 2
            pad_down_after = (pad_down_after + abs(pad_down_after)) // 2
            pad_right_after = (pad_right_after + abs(pad_right_after)) // 2

            self.tiling_info["padl"] = pad_left_before
            self.tiling_info["padr"] = pad_right_after
            self.tiling_info["padu"] = pad_up_before
            self.tiling_info["padd"] = pad_down_after
            if self.k_h == 1 and self.k_w == 1 and self.cur_pads == [0, 0, 0, 0]:
                self.tiling_info["general_flag"] = False
            else:
                self.tiling_info["general_flag"] = True

        cost_seeds = get_tiling(self.tiling_info)
        tiling_mess = self._check_and_set_default_tiling(cost_seeds[0])
        tiling_mess = self._modify_tiling_for_large_m(tiling_mess)

        return tiling_mess

    def get_tiling_range(self, tiling_in, c_shape):
        """
        get the covered area of a tiling

        Parameters
        ----------
        tiling_in : dict, result of tiling fetch

        c_shape : list, size of fmap_shape

        Returns
        -------
        list, range covered for tiling_in
        """
        def _modify_w_range():
            """
            modify w_range ensure that m_tiling - nw > 16
            """
            split_range_flag = False
            fmap_w_tiling = w_range_max

            if ("dx_w" in paras.get("var_map") and self.k_h == 1 and self.k_w == 1
                    and (self.pad_mode == "VAR" or sum(self.cur_pads) == 0)):
                dy_w_tiling = tiling_in.get("CL0_matrix")[1] * tiling_in.get("CL0_matrix")[2]
                dy_w = self.get_output_w(fmap_w)
                if self.pad_mode == "VAR":
                    fmap_w_tiling = min((dy_w_tiling - MIN_STEP - 1) * self.stride_w, fmap_w_tiling)
                else:
                    fmap_w_tiling = min(
                        (dy_w_tiling - MIN_STEP - 1 - self.k_w) * self.stride_w + self.k_w, fmap_w_tiling)
                if dy_w % 16 == 0 and dy_w > dy_w_tiling - MIN_STEP and dy_w_tiling > (MIN_STEP + self.k_w):
                    split_range_flag = True
                else:
                    fmap_w_tiling = max(fmap_w_tiling, fmap_w)
            return split_range_flag, fmap_w_tiling

        def _modify_max_range():
            """
            modify h_max and w_max according to the limit of ub buffer,
            ensure that aub + cub < ub buffer
            aub = ma * ka * db_flag * bit_num
            cub = mc * nc * m0 * n0 * db_flag * bit_num
            """
            if tiling_in.get("AUB_shape"):
                cub_buffer = (reduce(lambda x, y: x * y, tiling_in.get("CUB_matrix"))
                              * tiling_in.get("manual_pingpong_buffer").get("CUB_pbuffer")
                              * BIT_RATIO_DICT.get(self.c_type))
                tiling_k_aub = tiling_in.get("AUB_shape")[0] // (self.b_info[2] * self.b_info[3])
                m_aub_max = ((platform_info.get_soc_spec("UB_SIZE") - cub_buffer)
                             // BIT_RATIO_DICT.get(self.a_type)
                             // tiling_in.get("manual_pingpong_buffer").get("AUB_pbuffer")
                             // tiling_k_aub / (1 + 1 / self.stride_w))

                if tiling_in.get("AUB_shape")[1] >= 1:
                    w_range = min(w_range_max, max(m_aub_max // tiling_in.get("AUB_shape")[1], c_shape[3]))
                    return w_range
            return w_range_max

        def _get_perf_range(h_range_max, w_range_max):
            # modify range for curv performance line
            bool_check_case = utils.icd(
                utils.icd(utils.icd(fmap_h * fmap_w, tiling["block_dim"][2]), utils.FP16_M),
                tiling["AL0_matrix"][0]) <= tiling["AL1_shape"][1]

            if bool_check_case:
                range_max = tiling["AL1_shape"][1] * tiling["AL0_matrix"][0] * \
                            utils.FP16_M * tiling["block_dim"][2]
                if h_range_max * w_range_max > range_max:
                    return range_max // fmap_w, fmap_w
            return h_range_max, w_range_max

        tiling = self._preprocess_tiling(tiling_in)
        _, _, fmap_h, fmap_w, _ = c_shape

        paras = {
            "var_map": self.var_map,
            "k_h": self.k_h,
            "k_w": self.k_w,
            "pad_mode": self.pad_mode,
            "pads": self.cur_pads
        }
        n_range_min, n_range_max = self.get_batch_range(c_shape[0], paras)
        tiling_range = [n_range_min, n_range_max]
        # check tiling covering itself situation
        if not self.check_tiling_match(tiling, fmap_w, fmap_h) or fmap_h > utils.NHW_MAX or fmap_w > utils.NHW_MAX:
            return tiling_range + [0, 0, 0, 0]
        h_range_min, h_range_max = self.get_h_range(fmap_h, tiling, paras)
        tiling_range += [h_range_min, h_range_max]
        w_range_min, w_range_max = self.get_w_range(fmap_h, fmap_w, tiling, paras)
        split_range_flag, w_range_max = _modify_w_range()
        tiling_range += [w_range_min, w_range_max]
        if split_range_flag:
            tiling_range_self = tiling_range[:4] + [fmap_w, fmap_w]
            tiling_range_list = [tiling_range, tiling_range_self]

        if not tiling.get("AL1_shape"):
            if split_range_flag:
                return tiling_range_list
            return tiling_range

        tiling_range[-1] = _modify_max_range()
        tiling_range[3], tiling_range[5] = _get_perf_range(tiling_range[3], tiling_range[5])

        if split_range_flag:
            return tiling_range_list
        return tiling_range

    def assembly_case(self, tiling, coverage, cnt):
        """
        Configure dict of tiling strategy and coverage

        Parameters
        ----------
        tiling: dict, tiling from repository or cost model

        coverage: list of tuple, coverage of tiling

        cnt: serial number of tiling

        Returns
        -------
        dict: describe a tiling strategy
        """

        var_range = OrderedDict()
        if not self.binary_mode:
            if "batch_n" in self.var_map:
                var_range['batch_n'] = (utils.trans_to_int(coverage[0]), utils.trans_to_int(coverage[1]))
            if "dedy_h" in self.var_map:
                dx_h_low, dx_h_high = utils.trans_to_int(coverage[2]), utils.trans_to_int(coverage[3])
                dedy_h_low = self.get_output_h(dx_h_low)
                dedy_h_high = self.get_output_h(dx_h_high)
                var_range['dx_h'] = (dx_h_low, dx_h_high)
                var_range['dedy_h'] = (dedy_h_low, dedy_h_high)
            if "dedy_w" in self.var_map:
                dx_w_low, dx_w_high = utils.trans_to_int(coverage[4]), utils.trans_to_int(coverage[5])
                dedy_w_low = self.get_output_w(dx_w_low)
                dedy_w_high = self.get_output_w(dx_w_high)
                var_range['dx_w'] = (dx_w_low, dx_w_high)
                var_range['dedy_w'] = (dedy_w_low, dedy_w_high)
        correct_range_flag = DynamicConv2dBpInputParams.dynamic_para.get("correct_range_flag", False)

        return {"key": cnt, "tiling_strategy": tiling, "var_range": var_range, "correct_range_flag": correct_range_flag}

    def calculate_default_group_dim(self):
        """
        get default group dim for default tiling

        Returns
        -------
        int: default group dim for conv2d_bp_input tiling
        """
        group = self.tiling_info.get('group', 1)
        core_num = platform_info.get_soc_spec("CORE_NUM")
        group_dim = 1
        if group <= core_num:
            return group
        for candiate in range(core_num, 0, -1):
            if group % candiate == 0:
                group_dim = candiate
                break
        return group_dim

    def get_default_tiling(self, w_lower_bound=1):
        """
        get default tiling for unlimited range or special case

        Parameters
        ----------
        w_lower_bound: the min value of w when dynamic w

        Returns
        -------
        dict: default tiling for conv2d_bp_input
        """

        tiling = {}
        _, _, k_h, k_w, _ = self.b_info
        bit_dir = {
            "float32": 16,
            "int32": 16,
            "float16": 16,
            "int8": 32,
            "int4": 64,
        }
        support_l0c2out = platform_info.intrinsic_check_support(INTRINSTIC_FIX_PIPE_L0C2OUT)
        l1_space = platform_info.get_soc_spec("L1_SIZE")
        atype = self.tiling_info["A_dtype"]
        btype = self.tiling_info["B_dtype"]
        exceed_l1 = k_w * k_h * bit_dir.get(atype, MIN_STEP) * BIT_RATIO_DICT.get(atype) >= l1_space
        k_1 = k_w if (support_l0c2out and exceed_l1) else k_w * k_h
        if atype in bit_dir.keys():
            k_al1 = k_1 * 16
            k_al0 = bit_dir.get(atype)
        else:
            # default value 32
            k_al1 = 32
            k_al0 = 32
        # default to 32 if atype is not in bit_dir
        k_bl1 = bit_dir.get(btype, 32)
        k_bl0 = bit_dir.get(btype, 32)
        if self.split_axis_mode == SplitAxisMode.split_w.value:
            k_bl1 *= k_1
        k_aub = m_al0 = m_cl0 = 1
        if self.a_info[3] == -1:
            w_value = w_lower_bound
        else:
            w_value = self.a_info[3]
        if self.tiling_info["strideH_expand"] > 1 or self.tiling_info["strideW_expand"] > 1:
            if self.k_h == 1 and self.k_w == 1 and (self.pad_mode == "VAR" or sum(self.cur_pads) == 0):
                # when mmad, the min unit of M is a fmp's w
                if w_value % 16 == 0:
                    m_al0 = m_cl0 = w_value // utils.FP16_M
                else:
                    # add one is needed by buffer_tile of ub
                    m_al0 = m_cl0 = utils.icd(w_value, utils.FP16_M) + 1
            else:
                k_aub = k_1 * 16
        n_min = 1
        group_cl0 = 1
        group_dim = self.calculate_default_group_dim()

        tiling["AUB_shape"] = [k_aub, 1, 1, 1] if k_aub != 1 else None
        tiling["BUB_shape"] = None
        tiling["AL1_shape"] = [k_al1, 1, 1, 1]
        tiling["BL1_shape"] = [k_bl1, 1, 1, 1]
        tiling["AL0_matrix"] = [m_al0, 1, 16, k_al0, 1, 1]
        tiling["BL0_matrix"] = [1, n_min, 16, k_bl0, 1, 1]
        tiling["CL0_matrix"] = [n_min, m_cl0, 16, 16, 1, group_cl0]
        tiling["CUB_matrix"] = [n_min, m_cl0, 16, 16, 1, group_cl0]
        tiling["block_dim"] = [1, 1, 1, group_dim]
        tiling["n_bef_batch_flag"] = 0
        tiling["n_bef_group_flag"] = 0
        tiling["batch_bef_group_fla"] = 0
        tiling["A_overhead_opt_flag"] = 0
        tiling["B_overhead_opt_flag"] = 0
        tiling["CUB_channel_wise_flag"] = None
        tiling["AUB_channel_wise_flag"] = None
        tiling["BUB_channel_wise_flag"] = None
        tiling["default_tiling_flag"] = True
        tiling["manual_pingpong_buffer"] = {
            'AUB_pbuffer': 1,
            'BUB_pbuffer': 1,
            'AL1_pbuffer': 1,
            'BL1_pbuffer': 1,
            'AL0_pbuffer': 1,
            'BL0_pbuffer': 1,
            'CL0_pbuffer': 1,
            'CUB_pbuffer': 1,
            'UBG_pbuffer': 1,
        }
        return tiling

    def check_tiling_match(self, tiling, current_w, current_h):
        """
        check whether this tiling matches the shape

        Parameters
        ----------
        tiling : dict, result of tiling fetch

        current_size : int, size of h,w

        Returns
        -------
        bool, True: match
        False: do not match

        """

        if not tiling.get("AL1_shape"):
            return True

        # shape info
        fmap_h = current_h
        fmap_w = current_w

        # get M axis length in al1
        al1_bound = self._get_al1_bound(tiling, fmap_h, fmap_w)

        # fmap size in L1 ( M * K * db * 2byte)
        fmap_l1_size = utils.FP16_SIZE * al1_bound * tiling['AL1_shape'][0] * \
            utils.FP16_K * tiling['manual_pingpong_buffer']['AL1_pbuffer']

        # filter size
        if tiling['BL1_shape'] is None:
            # not using BL1
            filter_l1_size = 0
        elif len(tiling['BL1_shape']) == 0:
            # fully load in BL1
            filter_l1_size = utils.FP16_SIZE * self.k_cout * self.k_cin * self.k_h * \
                self.k_w // tiling['block_dim'][1]
        else:
            # fmap size in L1 ( K * N * db * 2byte)
            filter_l1_size = utils.FP16_SIZE * tiling['BL1_shape'][1] * \
                tiling['CL0_matrix'][0] * utils.FP16_N * tiling['BL1_shape'][0] * \
                utils.FP16_K * self.k_h * self.k_w * \
                tiling['manual_pingpong_buffer']['BL1_pbuffer']

        l1_buffer_flag = fmap_l1_size + filter_l1_size <= platform_info.get_soc_spec("L1_SIZE")

        a_shape = self.a_info[:3] + [self.get_output_w(fmap_w)] + [self.a_info[-1]]
        c_shape = self.c_info[:3] + [fmap_w] + [self.c_info[-1]]
        tiling_mess = {"tiling": tiling, "A_shape": a_shape, "B_shape": self.b_info, "C_shape": c_shape,
                       "fused_double_operand_num": self.tiling_info.get("fused_double_operand_num"),
                       "bias_flag": self.tiling_info.get("bias_flag")}
        ub_buffer_flag = self.check_tiling_ub(tiling_mess)

        return l1_buffer_flag and ub_buffer_flag

    def get_output_h(self, fmap_h, stride=None):
        """
        calculate output h
        """
        if not fmap_h:
            return fmap_h
        if not stride:
            stride = self.stride_h
        if self.pad_mode == "VAR":
            return utils.icd(fmap_h, stride)
        return (fmap_h + self.cur_pads[2] + self.cur_pads[3] - self.dilate_h * (self.k_h - 1) - 1) // stride + 1

    def get_output_w(self, fmap_w, stride=None):
        """
        calculate output w
        """
        if not fmap_w:
            return fmap_w
        if not stride:
            stride = self.stride_w
        if self.pad_mode == "VAR":
            return utils.icd(fmap_w, stride)
        return (fmap_w + self.cur_pads[0] + self.cur_pads[1] - self.dilate_w * (self.k_w - 1) - 1) // stride + 1

    def _modify_tiling_for_large_m(self, tiling_mess):
        """
        modify tiling for case with large m when stride > 1, only for kh == 1 and kw == 1

        Parameters
        ----------
        tiling_mess: tiling message with tiling and info

        Returns
        -------
        tiling_mess
        """
        tiling = tiling_mess.get("tiling")
        ori_m = tiling_mess.get("C_shape")[2] * tiling_mess.get("C_shape")[3]
        stride_above_one = self.stride_h > 1 or self.stride_w > 1
        if (stride_above_one and self.k_h == 1 and self.k_w == 1 and ori_m > LARGE_M and tiling["AL0_matrix"][0] == 1):
            max_core_num = platform_info.get_soc_spec("CORE_NUM")
            l0a_size = platform_info.get_soc_spec("L0A_SIZE") // BIT_RATIO_DICT.get(self.a_type)
            l0c_size = platform_info.get_soc_spec("L0C_SIZE") // BIT_RATIO_DICT.get(self.c_type)
            tiling_db = tiling.get("manual_pingpong_buffer")

            # use max core nums
            core_num = reduce(lambda x, y: x * y, tiling["block_dim"])
            if core_num < max_core_num:
                tiling["block_dim"][2] = max_core_num // (core_num // tiling["block_dim"][2])

            m_per_core = ori_m // tiling["block_dim"][2]

            # use max buffer size
            modified_flag = False
            while (self.check_tiling_ub(tiling_mess) and m_per_core > tiling["CUB_matrix"][1] * 16
                   and reduce(lambda x, y: x * y, tiling["AL0_matrix"]) * tiling_db.get("AL0_pbuffer") < l0a_size
                   and reduce(lambda x, y: x * y, tiling["CL0_matrix"]) * tiling_db.get("CL0_pbuffer") < l0c_size
                   and self.check_tiling_match(tiling,
                                               tiling_mess.get("C_shape")[3],
                                               tiling_mess.get("C_shape")[2])):
                tiling["CUB_matrix"][1] += 1
                tiling["AL0_matrix"][0] = tiling["CL0_matrix"][1] = tiling["CUB_matrix"][1]
                tiling_mess["tiling"] = tiling
                modified_flag = True
            if modified_flag:
                tiling["CUB_matrix"][1] -= 1
                tiling["AL0_matrix"][0] = tiling["CL0_matrix"][1] = tiling["CUB_matrix"][1]

        tiling_mess["tiling"] = tiling
        return tiling_mess

    def _check_and_set_default_tiling(self, tiling_in):
        if tiling_in.get("tiling").get("AL0_matrix")[2] == 32:
            tiling_in = {
                "tiling": self.get_default_tiling(),
                "A_shape": self.a_info,
                "B_shape": self.b_info,
                "C_shape": self.c_info
            }
        while not self.check_tiling_ub(tiling_in):
            if tiling_in.get("tiling").get("manual_pingpong_buffer").get("CUB_pbuffer") == 2:
                tiling_in.get("tiling").get("manual_pingpong_buffer")["CUB_pbuffer"] = 1
                continue
            _, mc_factor, m0, _ = tiling_in.get("tiling").get("CUB_matrix")[:4]
            if self.k_h == 1 and self.k_w == 1:
                if (mc_factor * m0 - utils.FP16_M) > self.a_info[3]:
                    tiling_in.get("tiling").get("CUB_matrix")[1] -= 1
                    tiling_in.get("tiling").get("CL0_matrix")[1] -= 1
                    tiling_in.get("tiling").get("AL0_matrix")[0] -= 1
                else:
                    return {"tiling": self.get_default_tiling(), "A_shape": self.a_info,
                            "B_shape": self.b_info, "C_shape": self.c_info}
            elif mc_factor > 1:
                tiling_in.get("tiling").get("CUB_matrix")[1] -= 1
                tiling_in.get("tiling").get("CL0_matrix")[1] -= 1
                tiling_in.get("tiling").get("AL0_matrix")[0] -= 1
            else:
                return {"tiling": self.get_default_tiling(), "A_shape": self.a_info,
                        "B_shape": self.b_info, "C_shape": self.c_info}
        return tiling_in

    def _get_calc_info(self):
        self._convert_type(self.a_info, self.b_info, self.c_info)
        self.k_h, self.k_w = self.b_info[2:4]
        self.k_cout = self.b_info[1] * self.b_info[4]
        self.k_cin = self.b_info[0]
        self.stride_h, self.stride_w = self.tiling_info["strideH_expand"], self.tiling_info["strideW_expand"]
        self.dilate_h, self.dilate_w = self.tiling_info["dilationH"], self.tiling_info["dilationW"]

        if isinstance(self.tiling_info["padl"], PrimExpr) or isinstance(self.tiling_info["padu"], PrimExpr):
            self.pad_mode = "VAR"
        else:
            self.pad_mode = "FIX"
        self.cur_pads = []
        for pad in ("padl", "padr", "padu", "padd"):
            self.cur_pads.append(self.tiling_info[pad])
            self.tiling_info[pad] = -1

    def _preprocess_tiling(self, tiling_in):
        """
        preprocess tiling for get tiling range
        """
        tiling = copy.deepcopy(tiling_in)
        kh_kw_k0 = self.k_h * self.k_w * utils.CUBE_SIZE
        if tiling["AL1_shape"]:
            tiling["AL1_shape"][0] = tiling["AL1_shape"][0] // kh_kw_k0
        if tiling.get("default_tiling_flag"):
            kh_kw_k0 = self.k_w * utils.CUBE_SIZE
        if tiling["BL1_shape"]:
            tiling["BL1_shape"][0] = tiling["BL1_shape"][0] // kh_kw_k0
        return tiling

    def _get_al1_bound(self, tiling, fmap_h, fmap_w):
        """
        get al1 bound info

        Parameters
        ----------
        tiling : dict, result of tiling fetch

        fmap_h : int, size of h

        fmap_w : int, size of w

        Returns
        -------
        int, al1_load_length (al1_bound)

        """

        # shape info
        h_i = self.get_output_h(fmap_h) * self.stride_h
        w_i = self.get_output_w(fmap_w) * self.stride_w

        if len(tiling['AL1_shape']) == 1:
            tiling['AL1_shape'].append(1)

        # M axis theorically loading length in al1
        al1_m_data = tiling['CL0_matrix'][1] * utils.FP16_M * tiling['AL1_shape'][1]

        # load2d instructions refer to data_mov with raw lens
        if (self.pad_mode == "VAR" or sum(self.cur_pads) == 0) and \
                self.k_h * self.k_w == 1:
            return al1_m_data

        # tiling load lens less than out_w, nned to load a full line
        if al1_m_data < fmap_w:
            l1_ho = 1 if fmap_w % al1_m_data == 0 else 2
        else:
            # load3d instructions refer to load extra lines with pad/stride/filter
            if al1_m_data % fmap_w == 0:
                # full line could load without extra lines
                extend_h = 0
            elif (al1_m_data * 2) % fmap_w == 0:
                # every 2 load3d covered only 1 extra line
                extend_h = 1
            else:
                # other situations need 2 extra lines in case
                extend_h = 2
            l1_ho = al1_m_data // fmap_w + extend_h

        # calculate input lines (hi) from output lines (ho)
        li_hi = min(self.k_h + (l1_ho - 1), h_i)

        return li_hi * w_i
