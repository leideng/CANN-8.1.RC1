#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright(C) 2022. Huawei Technologies Co., Ltd. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
transdata backward schedule
"""
from copy import copy
from tbe import tvm
from tbe.dsl.base.operation import var_inner
from tbe.dsl.unify_schedule.constants import TransdataCategory
from tbe.dsl.classifier.transdata.constants import STRIDE_2, NO_OVERLAP, INT64
from ..common.transdata_base_sch import TransdataBaseSch


class TransC04BackwardSchedule(TransdataBaseSch):
    """
    TransC04BackwardSchedule
    """

    def __init__(self, outs, tiling_case):
        TransdataBaseSch.__init__(self, outs, tiling_case)
        self.transpose_0_tensor = None
        self.transpose_1_tensor = None
        self.fuse_c_tensor = None
        self.depad_tensor = None
        self.split_tensor = None
        self.depad_c_tensor = None
        self.transpose_2_tensor = None
        self.transpose_3_tensor = None
        self.fuse_n_tensor = None
        self.depad_n_tensor = None
        self.ub2ub_0_tensor = None
        self.split_once = False

    @classmethod
    def get_supported_sub_pattern(cls):
        return TransdataCategory.GENERAL_C04_BACKWARD

    def do_schedule(self):
        """
        Process of schedule
        """
        self._create_schedule()
        self._do_cache_read()     # Create tensor after inputTensor
        self._do_set_scope()      # All scope set on UB
        self._do_cache_write()    # Create tensor before outputTensor
        self._init_tensors()

        self._calc_tiling()       # Do UB/Block tiling
        self._do_tiling()
        self._do_compute_align()  # Align extent
        self._do_storage_align()  # Align stride
        self._do_storage_bound()  # Set buffer_size(24192) calculated in tilingCase
        self._do_mem_reused()     # Memory reuse

        self._calc_compute_at()   # transdata_base_sch.py
        self._do_compute_at()     # transdata_base_sch.py

        self._calc_multi_core()   # transdata_base_sch.py Use fuse[0-BlockSplit-axis.out] bind multi-core
        self._do_multi_core()     # transdata_base_sch.py

        self._calc_emit_insn()
        self._do_emit_insn()
        self._do_pragma()         # Set label for Pass

        self.schedule.tiling_key = self.tiling_case.tiling_key
        return self.schedule

    def _init_tensors(self):
        self.mte2_tensor = self.child(list(self.graph_info.input_tensor_set)[0])

        # C1,N1,N0,C0 -> C1,N1,C0,N0
        self.transpose_0_tensor = self.child(self.mte2_tensor)
        # C1,N1,C0,N0 -> C1,C0,N1,N0 (UB2UB)
        self.transpose_1_tensor = self.child(self.transpose_0_tensor)
        # C1,C0,N1,N0 -> HC4_align16,N1,N0
        self.fuse_c_tensor = self.child(self.transpose_1_tensor)
        # HC4_align16,N1,N0 -> HC4,N1,N0
        self.depad_tensor = self.child(self.fuse_c_tensor)
        # HC4,N1,N0 -> H,C4,N1,N0
        self.split_tensor = self.child(self.depad_tensor)
        # H,C4,N1,N0 -> H,C,N1,N0
        self.depad_c_tensor = self.child(self.split_tensor)
        # H,C,N1,N0 -> C,H,N1,N0 (UB2UB)
        self.transpose_2_tensor = self.child(self.depad_c_tensor)
        # C,H,N1,N0 -> N1,N0,C,H(CH do storageAlign)
        self.transpose_3_tensor = self.child(self.transpose_2_tensor)
        # N1,N0,C,H(CH do storageAlign) -> Nx,C,H
        self.fuse_n_tensor = self.child(self.transpose_3_tensor)
        # Nx,C,H -> N,C,H
        self.depad_n_tensor = self.child(self.fuse_n_tensor)

        # Tiling
        self.tiling_tensor = list(self.graph_info.output_tensor_set)[0]

        # [UB]Tensors -> Reshape
        self.reshape_tensors = list(self.graph_info.s_reshape_tensor_set) + \
                               list(self.graph_info.f_reshape_tensor_set)
        # [UB]Tensors -> dePad
        self.depad_tensors = list(self.graph_info.de_pad_tensor_set)


    def _calc_tiling(self):
        self.tiling_axes = [x for x in self.tiling_tensor.op.axis]
        case = self.tiling_case
        if not case.ub_first_factor:
            case.ub_first_factor = var_inner("_ub_first_factor", (1, None), dtype=INT64)
        if not self.split_once and not case.ub_second_factor:
            case.ub_second_factor = var_inner("_ub_second_factor", (1, None), dtype=INT64)
        if not case.block_factor:
            case.block_factor = var_inner("_block_factor", (1, None), dtype=INT64)


    def _do_tiling(self):
        self._do_ub_tiling()     # transdata_base_sch.py
        self._do_block_tiling()  # transdata_base_sch.py
        self._do_reorder()
        self._do_fragments()     # transdata_base_sch.py


    def _do_reorder(self):
        """
        Regulation-0:
        [A.o, B.o, A.i, C, B.i]
        [A.o, B.o]: ub_outer
        [A.i, C, B.i]: ub_inner

        Regulation-1:
        Output is [N, C, H]. UB_first_split:axisH, UB_second_split:axisN
        After ub split, output is [N.o, N.i, C, H.o, H.i], reorder would change order is
        [N.o, H.o, N.i, C, H.i], not [H.o, N.o, N.i, C, H.i].

        Regulation-2:
        let N.o and H.o be block-split
        """
        self.axis_in_ub = copy(self.tiling_axes)
        self.axis_in_ub[0] = self.iter_ub_second_inner
        self.axis_in_ub[2] = self.iter_ub_first_inner

        if self.tiling_case.block_split_idx == 0:
            # split N.o
            self.axis_not_in_ub = [self.iter_block_outer, self.iter_block_inner, self.iter_ub_first_outer]
        else:
            # split H.o
            self.axis_not_in_ub = [self.iter_ub_second_outer, self.iter_block_outer, self.iter_block_inner]

        self.ub_outer = self.axis_not_in_ub[-1]
        self.ub_inner = self.axis_in_ub[0]
        self.reorder_list = self.axis_not_in_ub + self.axis_in_ub
        self.schedule[self.tiling_tensor].reorder(*self.reorder_list)


    def _do_storage_bound(self):
        for stage in self.forward_stage_graph_map:
            self.schedule[stage].set_buffer_size(self.tiling_case.buffer_size)


    def _do_storage_align(self):
        # Parameter Rules: The index of op.axis[*] should be in front of the Aligned root-axis.
        # When do HCN1N0 -> CHN1N0
        # need align transpose_2_tensor to H(8)*NiN0(ub_second_factor) - replace by computeAlign
        # When do CHN1N0 -> N1N0CH, need align transpose_3_tensor to C*H(8)
        self.schedule[self.transpose_3_tensor].storage_align(self.transpose_3_tensor.op.axis[-2], 8, 0)
        self.schedule[self.fuse_n_tensor].storage_align(self.fuse_n_tensor.op.axis[-2], 8, 0)
        # Tail axis alignment for datacopy
        self.schedule[self.depad_n_tensor].storage_align(self.depad_n_tensor.op.axis[-2], 8, 0)


    def _do_compute_align(self):
        # Parameter Rules: The index of op.axis[*] should be the index of Aligned root-axis.
        align_factor_hc4 = 16
        align_factor_c4 = 4
        # Keep axisN0 C0 Nx equal to 16, axisHC4 C4 equel to 4
        # C1,N1,C0,N0
        self.schedule[self.mte2_tensor].compute_align(self.mte2_tensor.op.axis[-1],
                                                      align_factor_hc4)
        self.schedule[self.mte2_tensor].compute_align(self.mte2_tensor.op.axis[-2],
                                                      align_factor_hc4)
        # C1,N1,C0,N0
        self.schedule[self.transpose_0_tensor].compute_align(self.transpose_0_tensor.op.axis[-1],
                                                             align_factor_hc4)
        self.schedule[self.transpose_0_tensor].compute_align(self.transpose_0_tensor.op.axis[-2],
                                                             align_factor_hc4)
        # C1,C0,N1,N0
        self.schedule[self.transpose_1_tensor].compute_align(self.transpose_1_tensor.op.axis[-1],
                                                             align_factor_hc4)
        self.schedule[self.transpose_1_tensor].compute_align(self.transpose_1_tensor.op.axis[-3],
                                                             align_factor_hc4)
        # HC4_align16,N1,N0
        self.schedule[self.fuse_c_tensor].compute_align(self.fuse_c_tensor.op.axis[-1],
                                                        align_factor_hc4)
        self.schedule[self.fuse_c_tensor].compute_align(self.fuse_c_tensor.op.axis[-3],
                                                        align_factor_hc4)
        # HC4,N1,N0
        self.schedule[self.depad_tensor].compute_align(self.depad_tensor.op.axis[-1],
                                                       align_factor_hc4)
        self.schedule[self.depad_tensor].compute_align(self.depad_tensor.op.axis[-3],
                                                       align_factor_c4)
        # H,C4,N1,N0
        self.schedule[self.split_tensor].compute_align(self.split_tensor.op.axis[-1],
                                                       align_factor_hc4)
        self.schedule[self.split_tensor].compute_align(self.split_tensor.op.axis[-3],
                                                       align_factor_c4)
        # H,C,N1,N0
        self.schedule[self.depad_c_tensor].compute_align(self.depad_c_tensor.op.axis[-1],
                                                         align_factor_hc4)
        # C,H,N1,N0
        self.schedule[self.transpose_2_tensor].compute_align(self.transpose_2_tensor.op.axis[-1],
                                                             align_factor_hc4)
        # When do HCN1N0 -> CHN1N0, storageAlign replace by computeAlign
        self.schedule[self.transpose_2_tensor].compute_align(self.transpose_2_tensor.op.axis[-3], 8)

        # N1,N0,C,H
        self.schedule[self.transpose_3_tensor].compute_align(self.transpose_3_tensor.op.axis[-3],
                                                             align_factor_hc4)
        # Nx,C,H
        self.schedule[self.fuse_n_tensor].compute_align(self.fuse_n_tensor.op.axis[-3],
                                                        align_factor_hc4)

        # Transpose_3_tensor aligned tailAxis to 8
        self.schedule[self.transpose_3_tensor].compute_align(self.transpose_3_tensor.op.axis[-1], 8)


    def _do_mem_reused(self):
        # Memory-reuse scenarios include fuse, split, no depad!
        self.schedule[self.transpose_1_tensor].reused_by(self.fuse_c_tensor)
        self.schedule[self.depad_tensor].reused_by(self.split_tensor)
        self.schedule[self.transpose_3_tensor].reused_by(self.fuse_n_tensor)


    def _calc_emit_insn(self):
        # Transpose tailAxis transform use vector_transpose
        # C1N1N0C0 -> C1N1C0N0 : 16*16 -> 16*16
        perm_tanspose0 = (0, 2, 1)
        perm_tanspose3 = (2, 3, 0, 1)
        # Parameter Rules: The index of op.axis[*] should be the index of the ubinner-axis.
        self.emit_insn_map[self.transpose_0_tensor] = {"scope": self.transpose_0_tensor.op.axis[0],
            "instruction": "vector_transpose",
            "attrs": {"src_in_dst_order": tvm.call_intrin("handle", 'tir.tvm_tuple', *perm_tanspose0),
            "trans_first_mode": tvm.call_intrin("handle", 'tir.tvm_tuple', 0, 1),
            "is_trans_align": 1, "enable_vnchwconv_b32": 1}}
        # CHN1N0 -> N1N0CH : 8*16 -> 16*8
        self.emit_insn_map[self.transpose_3_tensor] = {"scope": self.transpose_3_tensor.op.axis[0],
            "instruction": "vector_transpose",
            "attrs": {"src_in_dst_order": tvm.call_intrin("handle", 'tir.tvm_tuple', *perm_tanspose3),
            "trans_first_mode": tvm.call_intrin("handle", 'tir.tvm_tuple', 0, 1),
            "is_trans_align": 1, "enable_vnchwconv_b32": 3}}

        # Transpose UB2UB transform use dma_copy
        self.emit_insn_map[self.transpose_1_tensor] = {"scope": self.transpose_1_tensor.op.axis[0],
            "instruction": "dma_copy"}
        self.emit_insn_map[self.transpose_2_tensor] = {"scope": self.transpose_2_tensor.op.axis[0],
            "instruction": "dma_copy"}

        self.emit_insn_map[self.mte2_tensor] = {"scope": self.mte2_tensor.op.axis[0],
            "instruction": "dma_copy"}

        for tensor in self.reshape_tensors:
            self.emit_insn_map.update({tensor: {"scope": tensor.op.axis[0],
                "instruction": "phony_insn"}})

        for tensor in self.depad_tensors:
            self.emit_insn_map.update({tensor: {"scope": tensor.op.axis[0],
                "instruction": "dma_copy"}})

        self.emit_insn_map.update({self.depad_n_tensor: {"scope": self.depad_n_tensor.op.axis[0],
            "instruction": "dma_copy"}})

        self.emit_insn_map[self.tiling_tensor] = {"scope": self.ub_inner, "instruction": "dma_copy",
            "attrs": {NO_OVERLAP: 3, "no_overlap_malloc_buf_for_tail": 0}} 


    def _do_pragma(self):
        # Mark pragma_axis_group to avoid Pass problem
        axes = [self.transpose_3_tensor.op.axis[i] for i in [2, 3]]
        self._do_group(self.transpose_3_tensor, axes, number=0)

