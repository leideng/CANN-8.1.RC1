#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2019-2021. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
broadcast schedule for milan
"""
from functools import reduce
from operator import mul

from tbe import tvm
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.utils import para_check

from ... import util
from ...constants import BroadcastPattern
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG
from ...constants import Pattern
from ...constants import SUPPORT_SCALAR_INSNS
from ...constants import TERNARY_INSNS
from ...constants import COMPOSITE_INSNS_ALIGN_MAP
from ...schedule import Schedule
from ..storage_bound_util import EXTRA_NODE_B64
from ..storage_bound_util import EXTRA_NODE_B32
from ..storage_bound_util import EXTRA_TEMP_BUFFER
from ..storage_bound_util import EXTRA_NODE_COMPLEX
from ..storage_bound_util import EXTRA_TEMP_COMPLEX
from ..storage_bound_util import EXTRA_BLOCK
from ..storage_bound_util import base_op_complex_instructions
from ..storage_bound_util import cast_complex_instructions
from ..storage_bound_util import complex_instructions_b64_impl
from ..storage_bound_util import vsel_complex_instructions
from ..storage_bound_util import complex_calc_instructions
from ..storage_bound_util import base_single_instructions_b64
from ..storage_bound_util import powi_complex_instructions_b32
from ..storage_bound_util import cast_ints2ints_external_space
from ..storage_bound_util import cast_ints2ints_need_alignment
from .broadcast_schedule_base import BaseBroadcastSchedule
from .broadcast_tilingcase import TilingStrategy
from .broadcast_constants import BRC_ELE_INTERSECT_PURE_MIDDLE
from .broadcast_constants import BRC_ELE_INTERSECT_INPUT
from .broadcast_constants import BRC_ELE_INTERSECT_MIDDLE_OUT
from .broadcast_constants import CACHE_READ_MODE
from .broadcast_constants import CACHE_WRITE_MODE
from .broadcast_constants import SET_SCOPE_MODE
from .broadcast_constants import CACHE_CLONE_MODE
from .broadcast_constants import VECTOR_REDUCE
from .broadcast_constants import VCMP_INPUT_NUMBER
from .broadcast_constants import VSEL_INPUT_NUMBER
from .broadcast_constants import VCMPSEL_INPUT_NUMBER
from .broadcast_util import ScheduleUtil
from ..generic.common import helper


# 'pylint: disable=R0902, R0903
class BroadcastScheduleMl(BaseBroadcastSchedule, Schedule):
    """
    ElewiseSchedule
    """

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return [ASCEND_910B, ASCEND_910_93]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.BROADCAST]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [BroadcastPattern.B_0]

    def __init__(self, outs, tiling_case):
        super(BroadcastScheduleMl, self).__init__(outs, tiling_case)

        self._storage_align_map = {}

        self._remove_pad_tensors = set()

        self._ele_ub_depad_tensors = set()

        self._has_invalid_cast = False

    def _without_temp_buffer(self):
        return False

    def _calc_storage_bound(self):
        block_size_bytes = self._ub_block_size

        def _correct_ub_size_by_cmp_sel(_tensor):
            if util.is_vcmp_insn(_tensor):
                self._tmp_ub_size += block_size_bytes * (VCMP_INPUT_NUMBER - len(_tensor.op.input_tensors))
            if util.is_vsel_insn(_tensor):
                self._tmp_ub_size += block_size_bytes * (VSEL_INPUT_NUMBER - len(_tensor.op.input_tensors))
                if VSEL_INPUT_NUMBER == len(_tensor.op.input_tensors):
                    self._tmp_ub_size += block_size_bytes
            if util.is_vcmpsel_insn(_tensor):
                self._tmp_ub_size += block_size_bytes * (VCMPSEL_INPUT_NUMBER - len(_tensor.op.input_tensors))

        def _calc_current_space(_tensor):
            # one of the input of the ternary instruction must be reused with the output
            if util.get_dsl_insn(_tensor) in TERNARY_INSNS or _tensor in dependent_map:
                current_space = len(dependent_map)
            else:
                current_space = len(dependent_map) + 1
            for tensor_i in dependent_map:
                if tensor_i in self._absorbable_broadcast_tensors and \
                        len(tensor_i.op.input_tensors) == 1 and tensor_i.op.input_tensors[0] in dependent_map:
                    current_space -= 1
            # temporary plan: use a temp node
            if util.need_extent_node(_tensor) and not self._is_last_align(_tensor):
                current_space += 1
            if util.is_unified_broadcast(_tensor) and not self._is_last_align(_tensor) and \
                    self._broadcast_axis_num.get(_tensor, 0) > 1:
                current_space += 1

            # check if tensor is vsignbit calculation
            if _tensor.op.tag == "elewise_single_signbit" and util.is_v220():
                current_space += 1

            # check if tensor is complex calculation
            if complex_calc_instructions(_tensor):
                if util.is_v220():
                    op_tag = _tensor.op.tag.split('|')[0]
                    current_space += EXTRA_NODE_COMPLEX.get(op_tag, 0)
                    self._tmp_ub_size += EXTRA_TEMP_COMPLEX.get(op_tag, 0)

            extra_space, extra_ub = cast_ints2ints_external_space(_tensor)
            current_space += extra_space
            self._tmp_ub_size += extra_ub

            # check if tensor impl by complex instructions
            if complex_instructions_b64_impl(_tensor):
                current_space += EXTRA_NODE_B64.get(_tensor.op.tag)
                if util.is_v220() and \
                    _tensor.op.tag in ["elewise_binary_add", "elewise_binary_sub", "elewise_single_VS_add"]:
                    current_space += 1
                if cast_complex_instructions(_tensor):
                    # int64 cast to float16 need one more coexist node
                    if _tensor.dtype == "float16":
                        current_space += 1
                if base_op_complex_instructions(_tensor):
                    self._tmp_ub_size += EXTRA_TEMP_BUFFER.get(_tensor.op.tag)
            # check if tensor impl by vsel complex instructions
            if vsel_complex_instructions(_tensor):
                current_space += 3
            # single instruction support b64
            if base_single_instructions_b64(_tensor):
                current_space += EXTRA_NODE_B64.get(_tensor.op.tag)
            # power instruction support b32
            if powi_complex_instructions_b32(_tensor):
                current_space += EXTRA_NODE_B32.get(_tensor.op.tag)
                self._tmp_ub_size += EXTRA_TEMP_BUFFER.get(_tensor.op.tag)
            # 5hd need one more coexist node
            if self._5hd_actions is not None and len(self._5hd_actions) > 0:
                current_space += 1
            if _tensor.op.tag == "elewise_binary_gcd":
                current_space += 24 if _tensor.dtype == "int64" else 4

            if _need_external_space(_tensor):
                self._tmp_ub_size += block_size_bytes
            return current_space

        def _r_coexisting(_tensor):
            if _tensor in dependent_map and _tensor not in init_map:
                return len(dependent_map)
            _need_space = []
            for _tensor_i in _tensor.op.input_tensors:
                if _tensor in self._cst_compute_root_tensors | self._dyn_compute_root_tensors:
                    continue
                _need_space.append(_r_coexisting(_tensor_i))

            _current_space = _calc_current_space(_tensor)

            # correct ub size in vcmp or vsel or vcmpsel
            _correct_ub_size_by_cmp_sel(_tensor)

            _need_space.append(_current_space)
            _refresh_dependent(_tensor)
            if _tensor not in dependent_map:
                if _tensor in self._remove_pad_map:
                    _tensor = self._remove_pad_map[_tensor]
                dependent_map[_tensor] = self._in_out_map[_tensor].copy()
            elif _tensor in init_map:
                init_map.remove(_tensor)
            return max(_need_space)

        def _refresh_dependent(_tensor):
            for _tensor_i in _tensor.op.input_tensors:
                if _tensor_i not in dependent_map:
                    continue
                dependent_map[_tensor_i].discard(_tensor)
                if not dependent_map[_tensor_i]:
                    dependent_map.pop(_tensor_i)

        def _need_external_space(_tensor):
            exist_absorbable_broadcast = any(x in self._absorbable_broadcast_tensors
                                              for x in _tensor.op.input_tensors)
            if not exist_absorbable_broadcast:
                return False

            op_tag = util.get_dsl_insn(_tensor)
            if op_tag in set(SUPPORT_SCALAR_INSNS):
                return True

        coexisting_quantities = []
        dependent_map = {}
        init_map = set()
        all_producers = self._middle_tensors.copy()
        all_producers.update(self._out_tensors | self._input_tensors)
        for tensor_i in self._broadcast_store_predicate | self._store_predicate_common_tensors:
            dependent_map[tensor_i] = all_producers.copy()
            init_map.add(tensor_i)

        if self._ba_pattern_enable_reorder():
            for tensor_i in self._broadcast_tensors:
                dependent_map[tensor_i] = all_producers.copy()
                init_map.add(tensor_i)

        for tensor_i in self._out.op.input_tensors:
            if tensor_i in self._cst_compute_root_tensors | self._dyn_compute_root_tensors:
                continue
            coexisting_quantities.append(_r_coexisting(tensor_i))
        if self._out.op.tag != FAKE_NODE_TAG:
            _current_space = _calc_current_space(self._out)

            # correct ub size in vcmp or vsel or vcmpsel
            _correct_ub_size_by_cmp_sel(self._out)

            coexisting_quantities.append(_current_space)

        self._coexisting_quantity = max(coexisting_quantities)

        self._coexisting_quantity += \
            len(self._compute_root_helper.mem_unique_tensors) if self._enable_compute_root else 0

        if self._coexisting_quantity == 1:
            self._tmp_ub_size += block_size_bytes

        if self._enable_compute_root:
            self._tmp_ub_size += self._compute_root_helper.compute_root_tmp_ub_size

    def _calc_middle_broadcast(self, depad_broadcast_tensors):
        """
        calc middle depad broadcast tensors
        middle broadcast tensor don't need depad
        depad broadcast tensors: broadcast tensors except absorbable broadcast tensors
        @return:middle depad broadcast tensors
        """
        def dfs_middle_depad_broadcast(tensor):
            for input_i in tensor.op.input_tensors:
                if input_i in depad_broadcast_tensors:
                    middle_depad_broadcast_tensors.add(input_i)
                dfs_middle_depad_broadcast(input_i)

        middle_depad_broadcast_tensors = set()
        for _tensor in depad_broadcast_tensors:
            dfs_middle_depad_broadcast(_tensor)
        return middle_depad_broadcast_tensors

    def _calc_unify_remove_pad(self):

        self._calc_real_remove_pad()

    def _calc_ub_align(self):
        block_size_bytes = self._ub_block_size

        def _dsf_pre_tensors(_tensor):
            if _tensor in compute_align_tensors:
                return
            # calc dtype decline cast tensor, before broadcast tensor
            is_cast_tensor = util.is_cast_insn(_tensor)
            if is_cast_tensor:
                src_type_bytes = DTYPE_BYTE_MAPPING.get(_tensor.op.input_tensors[0].dtype)
                dst_type_bytes = DTYPE_BYTE_MAPPING.get(_tensor.dtype)
                self._has_invalid_cast = True if src_type_bytes > dst_type_bytes else self._has_invalid_cast

            compute_align_tensors.add(_tensor)
            for tensor_j in _tensor.op.input_tensors:
                if tensor_j in self._input_tensors:
                    shape_input = util.shape_to_list(tensor_j.shape)
                    shape_brc = util.shape_to_list(tensor_i.shape)
                    shape_input_all_one = all(_axis == 1 for _axis in shape_input)
                    has_last_brc = shape_input[-1] == 1 and shape_brc[-1] != 1 and not shape_input_all_one
                    is_const_mode = self._tiling_strategy == TilingStrategy.CONST
                    unsupported_cast = \
                        has_last_brc and self._has_invalid_cast and is_const_mode and ScheduleUtil.is_fusion_op()
                    error_message = "not support last axis broadcast, when compute graph exist cast to lower type"
                    util.is_true(not unsupported_cast, {"errCode": "E90001", "detailed_cause": error_message})
                    storage_align_tensors.add(tensor_j)
                else:
                    _dsf_pre_tensors(tensor_j)

        inline_tensors = self._absorbable_broadcast_tensors | self._compute_inline_tensors
        for tensor_i in inline_tensors:
            if tensor_i in self._remove_pad_map:
                self._remove_pad_tensors.remove(tensor_i)
                self._remove_pad_cache_read_buffer.remove(self._remove_pad_map.get(tensor_i))
                self._middle_tensors.remove(self._remove_pad_map.get(tensor_i))
                self._pure_middle_tensors.remove(self._remove_pad_map.get(tensor_i))
                self._compute_inline_tensors.add(self._remove_pad_map.get(tensor_i))

        compute_align_tensors = set()
        storage_align_tensors = set()
        for tensor_i in self._remove_pad_tensors:
            if not util.is_scalar_broadcast(tensor_i):
                self._has_invalid_cast = False
                _dsf_pre_tensors(tensor_i)

        for tensor_i in compute_align_tensors:
            tensor_ub = super(BroadcastScheduleMl, self)._get_ub_tensor(tensor_i)
            factor = int(block_size_bytes // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            axis = tensor_ub.op.axis[-1]
            self._compute_align_map[tensor_ub] = [axis, factor]

            if tensor_i in self._middle_out_tensors:
                if not self._enable_inplace_cache_clone:
                    middle_output_cache_read = self._gm_tensor_cache_read_buffer_map.get(tensor_i)
                    axis = middle_output_cache_read.op.axis[-1]
                    self._compute_align_map[middle_output_cache_read] = [axis, factor]

        offset = 0
        for tensor_i in storage_align_tensors:
            tensor_ub = super(BroadcastScheduleMl, self)._get_ub_tensor(tensor_i)
            factor = int(block_size_bytes // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            axis = tensor_ub.op.axis[-2]
            self._storage_align_map[tensor_ub] = [axis, factor, offset]

    def _calc_emit_insn(self):
        super(BroadcastScheduleMl, self)._calc_emit_insn()
        for tensor_i in self._remove_pad_cache_read_buffer:
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], VECTOR_REDUCE]

        for tensor_i in self._ele_ub_depad_tensors:
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], VECTOR_REDUCE]

    def _do_ub_align(self):
        sch = self._schedule
        for tensor_i, (axis, factor, offset) in self._storage_align_map.items():
            sch[tensor_i].storage_align(axis, factor, offset)

        for tensor_i, (axis, factor) in self._compute_align_map.items():
            sch[tensor_i].compute_align(axis, factor)

    def _do_storage_bound(self):
        self._do_inplace_graph_bound()
        block_size_bytes = self._ub_block_size
        sch = self._schedule
        tensors = set(graph_tensor.ub_tensor
                      for graph_tensor in self._ori_graph_unify_tensors
                      if graph_tensor.ub_mode in (CACHE_READ_MODE, CACHE_WRITE_MODE, SET_SCOPE_MODE, CACHE_CLONE_MODE))
        before_ub_tensor = set()
        for tensor_i in self._cst_compute_root_tensors:
            before_ub_tensor.add(self._get_ub_tensor(tensor_i))

        dyn_before_ub_tensor = self._get_all_ub_tensors(self._dyn_compute_root_tensors)
        for tensor_i in tensors:
            one_block = block_size_bytes // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
            if tensor_i in dyn_before_ub_tensor or tensor_i in self._dyn_compute_root_tensors:
                at_least_storage_bound = 0
                if complex_calc_instructions(tensor_i) and util.is_v220() and tensor_i.op.tag in EXTRA_BLOCK:
                    at_least_storage_bound = one_block * 32
                sch[tensor_i].set_buffer_size(at_least_storage_bound + self._compute_root_helper.compute_root_size)
                continue
            storage_bound = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            if tensor_i in before_ub_tensor or tensor_i in self._cst_compute_root_tensors:
                continue
            if self._tiling_strategy == TilingStrategy.CONST:
                dst_shape = util.shape_to_list(tensor_i.shape)
                ele_in_block = block_size_bytes // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)

                if tensor_i.dtype == "int64":
                    ele_in_block = COMPOSITE_INSNS_ALIGN_MAP.get(helper.get_insn(tensor_i), ele_in_block)

                dst_shape[-1] = (dst_shape[-1] + ele_in_block - 1) // ele_in_block * ele_in_block
                if self._need_do_block:
                    dst_shape[self._ub_split_axis] = 1 if dst_shape[self._ub_split_axis] == 1 else self._ub_factor
                real_storage_bound = reduce(mul, dst_shape[self._ub_split_axis:], 1)
                if real_storage_bound % ele_in_block != 0:
                    real_storage_bound = (real_storage_bound + ele_in_block - 1) // ele_in_block * ele_in_block

                if cast_ints2ints_need_alignment(tensor_i):
                    extra_ratio = DTYPE_BYTE_MAPPING.get(tensor_i.op.input_tensors[0].dtype) \
                                // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
                    real_storage_bound = extra_ratio * real_storage_bound

                storage_bound = min(storage_bound, int(real_storage_bound))
            if complex_calc_instructions(tensor_i) and util.is_v220():
                storage_bound += EXTRA_BLOCK.get(tensor_i.op.tag, 0) * one_block
            sch[tensor_i].set_buffer_size(storage_bound)

    def _calc_real_remove_pad(self):
        sch = self._schedule
        block_size_bytes = self._ub_block_size
        depad_broadcast_tensors = self._broadcast_tensors - self._absorbable_broadcast_tensors
        middle_broadcast_tensors = self._calc_middle_broadcast(depad_broadcast_tensors)
        outermost_broadcast_tensors = depad_broadcast_tensors - middle_broadcast_tensors

        # when schedule enable compute root in mv_align schedule, use vector_dup to make brc tensor aligned in ub
        last_axis_one = outermost_broadcast_tensors and \
                        all(util.shape_to_list(tensor_i.op.input_tensors[0].shape)[-1] == 1
                            for tensor_i in outermost_broadcast_tensors)
        self._enable_vdup_align = last_axis_one and self._enable_compute_root
        if self._enable_vdup_align:
            for tensor_i in outermost_broadcast_tensors:
                factor = int(block_size_bytes // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
                self._storage_align_map[self._get_ub_tensor(tensor_i)] = \
                    [self._get_ub_tensor(tensor_i).op.axis[-2], factor, 0]
                use_tensors = [super(BroadcastScheduleMl, self)._get_ub_tensor(_tensor) for
                               _tensor in self._in_out_map.get(tensor_i, [])]
                remove_pad_buffer = sch.cache_read(super(BroadcastScheduleMl, self)._get_ub_tensor(tensor_i),
                                                   self._scope, use_tensors)
                util.merge_value(self._in_out_map, remove_pad_buffer, use_tensors)
                self._in_out_map[tensor_i] = {remove_pad_buffer}
                self._remove_pad_map[tensor_i] = remove_pad_buffer
                self._remove_pad_cache_read_buffer.add(remove_pad_buffer)
                self._middle_tensors.add(remove_pad_buffer)
                self._pure_middle_tensors.add(remove_pad_buffer)
            return

        # ub has only one axis, no need to use pad
        if self._ub_split_axis == len(self._out.shape) - 1:
            return

        for tensor_i in outermost_broadcast_tensors:
            src_tensor = tensor_i.op.input_tensors[-1]
            last_axis_shape = util.shape_to_list(src_tensor.shape)[-1]
            align_factor = int(block_size_bytes // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            if isinstance(last_axis_shape, (tvm.Var, tvm.tir.PrimExpr)) or \
                    (isinstance(last_axis_shape, int) and last_axis_shape % align_factor != 0):
                self._remove_pad_tensors.add(tensor_i)
                use_tensors = [super(BroadcastScheduleMl, self)._get_ub_tensor(_tensor) for
                               _tensor in self._in_out_map.get(tensor_i, [])]
                if not use_tensors:
                    if tensor_i.op.input_tensors and tensor_i.op.input_tensors[0] in self._input_tensors:
                        use_tensors = list(self._in_out_map.get(tensor_i.op.input_tensors[0], []))
                remove_pad_buffer = sch.cache_read(super(BroadcastScheduleMl, self)._get_ub_tensor(tensor_i),
                                                   self._scope, use_tensors)
                util.merge_value(self._in_out_map, remove_pad_buffer, use_tensors)
                self._in_out_map[tensor_i] = {remove_pad_buffer}
                self._remove_pad_map[tensor_i] = remove_pad_buffer
                self._remove_pad_cache_read_buffer.add(remove_pad_buffer)
                self._middle_tensors.add(remove_pad_buffer)
                self._pure_middle_tensors.add(remove_pad_buffer)

        # broadcast and elementwise graph intersect tensor, need remove pad for elementwise graph
        if len(self._remove_pad_tensors) > 0:
            for gm_tensor in self._graph_analyzier.brc_ele_first_intersect_tensors:
                brc_ele_intersect_type = self._graph_analyzier.brc_ele_first_intersect_tensor_and_type_map.get(
                    gm_tensor)
                if brc_ele_intersect_type == BRC_ELE_INTERSECT_PURE_MIDDLE:
                    src_tensor = gm_tensor
                elif brc_ele_intersect_type in (BRC_ELE_INTERSECT_INPUT, BRC_ELE_INTERSECT_MIDDLE_OUT):
                    src_tensor = self._gm_tensor_cache_read_buffer_map.get(gm_tensor)
                else:
                    src_tensor = gm_tensor

                gm_consumers = self._graph_analyzier.intersect_in_and_ele_out_map.get(gm_tensor)
                # inplace graph has no need to remove pad
                if self._enable_inplace_cache_clone:
                    gm_consumers = set(tensor_i
                                       for tensor_i in gm_consumers
                                       if tensor_i not in self._cache_clone_helper.inplace_graph_tensors)
                    if not gm_consumers:
                        continue
                tvm_consumers = self._get_all_ub_tensors(gm_consumers)
                ele_ub_remove_pad_tensor = sch.cache_read(src_tensor, self._scope, tvm_consumers)
                self._ele_ub_depad_tensors.add(ele_ub_remove_pad_tensor)
                self._middle_tensors.add(ele_ub_remove_pad_tensor)
                self._pure_middle_tensors.add(ele_ub_remove_pad_tensor)
                if self._enable_compute_root and gm_tensor in self._compute_root_helper.dyn_compute_root_tensors:
                    self._compute_root_helper.dyn_compute_root_tensors.add(ele_ub_remove_pad_tensor)
                if gm_tensor in self._cst_compute_root_tensors:
                    self._cst_compute_root_tensors.add(ele_ub_remove_pad_tensor)
