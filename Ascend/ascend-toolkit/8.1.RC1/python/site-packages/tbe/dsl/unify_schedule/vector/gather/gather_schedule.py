#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
gather schedule
"""
from typing import Any
from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.utils import maths
from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import GatherPattern
from ...constants import Pattern
from ...schedule import Schedule
from .gather_tilingcase import TilingStrategy
from .gather_tilingcase import GatherCompileInfo


DEFAULT = "default"
SCOPE_UB = "local.UB"
DMA_COPY = "dma_copy"
VNCHWCONV = "vnchwconv"
VECTOR_TRANSPOSE = "vector_transpose"
VECTOR_REDUCE = "vector_reduce"
PHONY_INSN = "phony_insn"

# STORE AREA
PARAMS_STORE_GM = 0
PARAMS_STORE_UB = 1

# GATHER TYPE
GATHER = 0
GATHER_ND = 1

TRANSPOSE_FACTOR = 16


# 'pylint: disable=R0902, R0903
class GatherSchedule(Schedule):
    """
    gather schedule
    """

    @classmethod
    def get_instance(cls, outs, tiling_case):  # type: (list[Any], Any) -> "Schedule"
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):  # type: () -> list[str]
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):  # type: () -> list[str]
        return [Pattern.GATHER]

    @classmethod
    def get_supported_sub_pattern(cls):  # type: () -> list[str]
        return [GatherPattern.NORMAL_SCHEDULE]

    def __init__(self, outs, tiling_case):
        self._out_tensor = outs[0]
        self._schedule = None
        self._tiling_case = tiling_case
        self._tiling_strategy = self._tiling_case.get("tiling_strategy")
        self._tiling_key = self._tiling_case.get("key")

        self._special_pattern = (self._tiling_key % 10000) // 1000
        if self._tiling_key == 900018000:
            self._special_pattern = 80
        elif self._tiling_key == 900018001:
            self._special_pattern = 81
        elif self._tiling_key == 900018002:
            self._special_pattern = 82
        elif self._tiling_key == 900018004:
            self._special_pattern = 84
        elif self._tiling_key == 900018005:
            self._special_pattern = 85
        self._is_transpose = self._tiling_case.get("is_transpose", 0)
        self._is_trans_special = self._tiling_case.get("is_trans_special", False)
        self._is_vreduce = self._tiling_case.get("is_vreduce", False)
        self._last_row = self._tiling_case.get("last_row", 0)
        self._batch_dims = self._tiling_case.get("batch_dims", 0)
        # gather axis (real axis)
        self._axis = self._tiling_case.get("axis", 0)
        # last dim gather or no last dim gather splite no last dim
        self._is_need_align = self._tiling_case.get("is_need_align", False)
        self._store_area = self._tiling_case.get("store_area", PARAMS_STORE_GM)
        self._is_partial_store = self._tiling_case.get("partial_store", False)
        # is params align
        self._is_params_align = self._tiling_case.get("is_params_align", False)
        # whether to use malloc_buf point
        self._use_malloc_buf = self._tiling_case.get("use_malloc_buf", False)
        # DB
        self._is_db = self._tiling_case.get("is_db", False)
        self._scalar_mode = self._tiling_case.get("scalar_mode", False)
        self._remove_pad = self._tiling_case.get("remove_pad", False)
        self._tensor_swell = 4 if self._remove_pad else 1
        self._compile_info_swell = operation.get_context().get("_compile_info_swells")

        self._gather_compute_type = 0
        self._scope = SCOPE_UB
        self._input_tensors = set()
        self._params_gm_tensor = None
        self._indices_gm_tensor = None
        self._params_name = None
        self._indices_name = None

        self._dtypes = set()
        self._max_dtype_bytes = 4
        self._coexisting_quantity = 1
        self._tensor_space = None
        self._ub_size = util.get_ub_size()
        self._ub_block_size = util.get_ub_block_size()
        self._one_repeat = 128 if util.is_nano() else 256
        self._params_ub_size = self._ub_size // 2
        self._params_dtype_size = 4
        self._indices_dtype_size = 4
        self._ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(self._out_tensor.dtype)

        # input -> outputs mapping relations
        self._in_out_map = {}
        self._cache_write_tensor = None
        self._compute_at_map = {}
        
        # nano 
        self._ub_buffer_list = []
        self._compute_inline_tensors = []
        self._compute_align_map: Dict[List[Tuple[int, int]]] = {}
        self._in_reshape_map = {}
        self._out_reshape_map = {}
        self._in_reshape_tensors = set()
        self._out_transpose_tensors = []
        self._in_transpose_tensors = set()
        self._compute_align_factor = None
        self._is_v350_special = False

        # const tiling
        self._const_block_axis = -1
        self._const_ub_axis = 0
        self._const_block_factor = 1
        self._const_ub_factor = 1
        self._static = False

        self._block_tiling_vars = {}
        self._ub_tiling_vars = {}
        self._block_bind_axis = None
        self._compute_at_axis = None
        self._block_axis = None
        self._ub_axis = None
        self._emit_insn_map = {}
        self._out_shape = self._out_tensor.shape
        self._indices_storage_bound = 0
        self._gather_storage_bound = 0
        self._fake_schedule = False

        # define init value
        self._coexisting_quantity_gather = None
        self._coexisting_quantity_indices = None
        self._indices_ub_tensor = None
        self._params_inner_tensor = None
        self._removd_pad_tensor = None
        self._gather_ub_tensor = None
        self._gather_ub_tensor_t = None
        self._params_inner_t_tensor = None
        self._align_factor = None
        self._align_factor_v = None
        self._align_factor_t = None
        self.tensor_space = None
        self._params_storage_bound = 0
        self._gather_emit_at_axis = None
        self._gather_align_axis = None
        self._remove_pad_emit_at_axis = None
        self._res_emit_at_axis = None
        self._before_in_reshape_tensor = None
        self._in_reshape_tensors_copy = None
        self._gather_tensors = None
        self.is_310p = False
        if operation.get_context().get("_is_310p") is not None:
            self.is_310p = operation.get_context().get("_is_310p")

    def do_schedule(self):
        """
        schedule body
        :return:
        """
        self._construct_compute_graph()

        self._schedule = tvm.create_schedule(self._out_tensor.op)
        self._schedule.tiling_key = self._tiling_key

        self._cal_cache_read()
        self._do_cache_read()

        self._cal_cache_write()
        self._do_cache_write()

        self._do_reshape_transpose()
        self._do_cache_transpose()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._cal_storage_bound()
        self._do_storage_bound()

        self._calc_tiling()

        if self._fake_schedule:
            return None

        self._do_tiling()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_compute_align()
        self._do_compute_align()

        self._calc_compute_at()
        self._do_compute_at()

        self._calc_double_buffer()
        self._do_double_buffer()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_group_axis()

        self._add_compile_info()

        return self._schedule

    def _construct_compute_graph(self):
        visited_tensors = set()

        self.__dfs_sub_graph(self._out_tensor, visited_tensors)
        self._max_dtype_bytes = max(DTYPE_BYTE_MAPPING.get(dtype) for dtype in self._dtypes)

        # params gm and indices gm by name
        for one_input_tensor in self._input_tensors:
            if one_input_tensor.name == self._params_name:
                self._params_gm_tensor = one_input_tensor
            elif one_input_tensor.name == self._indices_name:
                self._indices_gm_tensor = one_input_tensor

        self._params_dtype_size = DTYPE_BYTE_MAPPING.get(self._params_gm_tensor.dtype)
        self._indices_dtype_size = DTYPE_BYTE_MAPPING.get(self._indices_gm_tensor.dtype)

    def _cal_cache_read(self):
        pass

    def _do_cache_read(self):
        # Only for the performance branch of the nano soc.
        self._is_v350_special = util.is_nano() and self._axis != 0 and DTYPE_BYTE_MAPPING.get(self._out_tensor.dtype)\
            == 2 and maths.prod(util.shape_to_list(self._out_shape[self._axis:])) <= self._ele_in_block and \
            maths.prod(util.shape_to_list(self._out_shape[:self._axis])) >= self._ele_in_block * 4 and \
                len(self._out_shape) == 4

        # indcies
        self._indices_ub_tensor = self._schedule.cache_read(self._indices_gm_tensor, self._scope,
                                                            self._in_out_map.get(self._indices_gm_tensor))

        # params in ub
        if self._store_area > 0 or self._is_v350_special:
            self._params_inner_tensor = self._schedule.cache_read(self._params_gm_tensor,
                                                                  self._scope,
                                                                  self._in_out_map.get(self._params_gm_tensor))

    def _cal_cache_write(self):
        self._cache_write_tensor = self._out_tensor

    def _do_cache_write(self):
        if self._remove_pad:
            # remove pad add one node
            self._removd_pad_tensor = self._schedule.cache_write(self._cache_write_tensor, self._scope)
            self._gather_ub_tensor = self._schedule.cache_write(self._removd_pad_tensor, self._scope)
        else:
            self._gather_ub_tensor = self._schedule.cache_write(self._cache_write_tensor, self._scope)

    def _transpose(self, tensor, permute):
        sch = self._schedule
        reorder_axis = []
        for axis in permute:
            reorder_axis.append(tensor.op.axis[axis])
        sch[tensor].reorder(*reorder_axis)
        transpose_tensor = sch.cache_write(tensor, SCOPE_UB)
        return transpose_tensor

    def _reshape(self, tensor, factors):
        sch = self._schedule
        axes = (tensor.op.axis[1], tensor.op.axis[1])
        for index, (action, value, axis_idx) in enumerate(factors):
            tail_strategy = "round_up"
            if index == 0:
                tail_strategy = "guard_with_if"
            split_axis = axes[axis_idx]
            if action == "nparts":
                axes = sch[tensor].split(split_axis, nparts=value, tail_strategy=tail_strategy)
            else:
                axes = sch[tensor].split(split_axis, factor=value, tail_strategy=tail_strategy)
        reshape_tensor = sch.cache_write(tensor, SCOPE_UB, "round_up")
        return reshape_tensor
    
    def _do_reshape_transpose(self):
        """
        ub tensors:
        self._params_inner_tensor -> self._in_reshape_tensors_copy -> self._in_transpose_tensors ->
        self._in_reshape_tensors(compute_inline) -> self._before_in_reshape_tensor (compute_inline) 
        -> self._gather_tensors -> self._out_transpose_tensors -> self._gather_ub_tensor
        """
        if self._is_v350_special:
            self._before_in_reshape_tensor = self._schedule.cache_read(self._params_inner_tensor, self._scope, \
                self._gather_ub_tensor)
            inner_factor = (self._out_shape[1] + self._ub_block_size - 1) // TRANSPOSE_FACTOR
            lcm = maths.lcm(8, maths.prod(util.shape_to_list(self._out_shape[2:])))
            update_inner_factor = ((maths.prod(util.shape_to_list(self._out_shape[2:]))) * inner_factor + lcm - 1) \
                // lcm * lcm // (maths.prod(util.shape_to_list(self._out_shape[2:])))
            self._compute_align_factor = TRANSPOSE_FACTOR * update_inner_factor
            reshape_factors = [("factor", self._compute_align_factor, 1)]
            reshape_factors.append(("nparts", TRANSPOSE_FACTOR, 1))
            self._in_reshape_map[self._before_in_reshape_tensor] = reshape_factors
            for tensor_i, factors in self._in_reshape_map.items():
                self._in_reshape_tensors = self._reshape(tensor_i, factors)
            self._in_reshape_tensors_copy = self._schedule.cache_write(self._in_reshape_tensors, self._scope)

            self._out_reshape_map[self._gather_ub_tensor] = reshape_factors
            for tensor_i, factors in self._out_reshape_map.items():
                self._out_transpose_tensors = self._reshape(tensor_i, factors)
            permute = [0, 3, 4, 5, 1, 2]
            self._in_transpose_tensors = self._transpose(self._in_reshape_tensors, permute)
            self._gather_tensors = self._transpose(self._out_transpose_tensors, permute)
            self._ub_buffer_list = [self._params_inner_tensor, self._in_reshape_tensors_copy, 
                                    self._in_transpose_tensors, self._in_reshape_tensors, 
                                    self._before_in_reshape_tensor, self._gather_tensors,
                                    self._out_transpose_tensors, self._gather_ub_tensor]

    def _do_cache_transpose(self):
        if self._is_v350_special:
            return

        if self._is_transpose and not self._is_vreduce:
            self._params_inner_t_tensor = self._schedule.cache_transpose(self._params_inner_tensor, [1, 0],
                                                                         self._scope, [self._gather_ub_tensor])
        if self._is_transpose and not self._is_trans_special and not self._is_vreduce:
            self._schedule[self._gather_ub_tensor].reorder(self._schedule[self._gather_ub_tensor].op.axis[1],
                                                           self._schedule[self._gather_ub_tensor].op.axis[0])
            self._gather_ub_tensor_t = self._schedule.cache_write(self._gather_ub_tensor, self._scope)
        else:
            pass

    def _calc_compute_inline(self):
        if self._is_v350_special:
            self._compute_inline_tensors.append(self._before_in_reshape_tensor)
            self._compute_inline_tensors.append(self._in_reshape_tensors_copy)
            self._compute_inline_tensors.append(self._in_reshape_tensors)
            self._compute_inline_tensors.append(self._gather_ub_tensor)

    def _do_compute_inline(self):
        sch = self._schedule
        for tensor_i in self._compute_inline_tensors:
            sch[tensor_i].compute_inline()

    def _cal_storage_bound(self):
        self._coexisting_quantity_gather = int(self._ub_block_size // self._indices_dtype_size)
        self._coexisting_quantity_indices = 1
        self._coexisting_quantity = self._coexisting_quantity_gather * self._tensor_swell \
                                    + self._coexisting_quantity_indices

    def _do_storage_bound(self):
        if not self._is_transpose:
            if self._store_area == PARAMS_STORE_UB:
                # params in ub
                self.tensor_space = (self._ub_size - self._params_ub_size) // self._coexisting_quantity \
                                    // self._ub_block_size * self._ub_block_size

                # set params ub storage bound
                self._params_storage_bound = int(self._params_ub_size // self._params_dtype_size)
                self._schedule[self._params_inner_tensor].set_buffer_size(self._params_storage_bound)
            else:
                self.tensor_space = self._ub_size // self._coexisting_quantity // \
                    self._ub_block_size * self._ub_block_size

                # PARAMS_STORE_GM
                if self._is_db:
                    # db
                    self.tensor_space = self.tensor_space // 2
            # indices buffer size
            self._indices_storage_bound = int(self.tensor_space // self._indices_dtype_size)
            self._schedule[self._indices_ub_tensor].set_buffer_size(self._indices_storage_bound)

            # gather buffer size
            self._gather_storage_bound = int(
                self.tensor_space * self._coexisting_quantity_gather // self._params_dtype_size)
            if self._use_malloc_buf:
                self._gather_storage_bound = self._gather_storage_bound // 2
            self._schedule[self._gather_ub_tensor].set_buffer_size(self._gather_storage_bound)

            # remove pad
            if self._remove_pad:
                self._schedule[self._removd_pad_tensor].set_buffer_size(self._gather_storage_bound)
        elif self._is_v350_special:
            self.tensor_space = ((self._ub_size - self._ub_block_size) // 2 // self._one_repeat) * self._one_repeat
            self._params_storage_bound = self.tensor_space // self._params_dtype_size
            self._indices_storage_bound = self._ub_block_size // self._indices_dtype_size
            self._schedule[self._params_inner_tensor].set_buffer_size(self._params_storage_bound)
            self._schedule[self._in_transpose_tensors].set_buffer_size(self._params_storage_bound)
            self._schedule[self._gather_tensors].set_buffer_size(self._params_storage_bound)
            self._schedule[self._out_transpose_tensors].set_buffer_size(self._params_storage_bound)
            self._schedule[self._indices_ub_tensor].set_buffer_size(self._indices_storage_bound)
        else:
            if self._is_trans_special and not self._is_vreduce:
                self.tensor_space = ((self._ub_size - self._ub_block_size) // 2 // self._one_repeat) * self._one_repeat
                self._params_storage_bound = int(self.tensor_space // self._params_dtype_size)
                self._gather_storage_bound = int(self.tensor_space // self._params_dtype_size)
                self._indices_storage_bound = self._ub_block_size // self._indices_dtype_size
                self._schedule[self._params_inner_tensor].set_buffer_size(self._params_storage_bound)
                self._schedule[self._params_inner_t_tensor].set_buffer_size(self._params_storage_bound)
                self._schedule[self._indices_ub_tensor].set_buffer_size(self._indices_storage_bound)
                self._schedule[self._gather_ub_tensor].set_buffer_size(self._gather_storage_bound)
            elif self._is_trans_special and self._is_vreduce:
                self._indices_storage_bound = self._ub_block_size // self._indices_dtype_size
                if self._last_row in (2, 4):
                    reg_buf = 64 if self.is_310p else 32
                    self.tensor_space = (self._ub_size - reg_buf) // 32 * 32
                    self._params_storage_bound = int(self.tensor_space // self._params_dtype_size)
                    self._gather_storage_bound = int(self.tensor_space // self._params_dtype_size)
                    self._schedule[self._params_inner_tensor].reused_by(self._gather_ub_tensor)
                else:
                    self.tensor_space = (((self._ub_size - 2080)*3) // 4) // 32 * 32
                    self._params_storage_bound = int(self.tensor_space // self._params_dtype_size)
                    self._gather_storage_bound = int(self.tensor_space // 3 // self._params_dtype_size)

                self._schedule[self._params_inner_tensor].set_buffer_size(self._params_storage_bound)
                self._schedule[self._indices_ub_tensor].set_buffer_size(self._indices_storage_bound)
                self._schedule[self._gather_ub_tensor].set_buffer_size(self._gather_storage_bound)
            else:
                self.tensor_space = self._ub_size // 8 // self._ub_block_size * self._ub_block_size
                self._params_storage_bound = (self.tensor_space * 2) // self._params_dtype_size
                self._gather_storage_bound = (self.tensor_space * 3) // self._params_dtype_size
                self._indices_storage_bound = self.tensor_space // self._indices_dtype_size
                self._schedule[self._params_inner_tensor].set_buffer_size(self._params_storage_bound)
                self._schedule[self._params_inner_t_tensor].set_buffer_size(self._params_storage_bound)
                self._schedule[self._indices_ub_tensor].set_buffer_size(self._indices_storage_bound)
                self._schedule[self._gather_ub_tensor].set_buffer_size(self._gather_storage_bound)
                self._schedule[self._gather_ub_tensor_t].set_buffer_size(self._gather_storage_bound)

    def _calc_tiling(self):
        funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                 TilingStrategy.STATIC: self._calc_tiling_static}

        funcs.get(self._tiling_strategy)()

    def _calc_tiling_dynamic(self):
        res = self._out_tensor
        shape = util.shape_to_list(res.shape)
        b_i = self._tiling_case["block_tiling_axis"]
        u_i = self._tiling_case["ub_tiling_axis"]
        self._block_axis = b_i
        self._ub_axis = u_i
        b_bound = (1, util.get_bound(shape[b_i])[1])
        u_bound = (1, util.get_bound(shape[u_i])[1])
        self._block_tiling_vars[b_i] = operation.var_inner("_block_factor_" + str(b_i), b_bound, "int64")
        self._ub_tiling_vars[u_i] = operation.var_inner("_ub_factor_" + str(u_i), u_bound, "int64")

    def _calc_tiling_static(self):
        tmp_output_shape = [i.value for i in self._out_shape]
        outputs = [{"shape": tmp_output_shape, "dtype": self._out_tensor.dtype}]

        tmp_params_shape = tuple(i.value for i in self._params_gm_tensor.shape)
        tmp_indices_shape = tuple(i.value for i in self._indices_gm_tensor.shape)

        inputs = [{"shape": tmp_params_shape, "dtype": self._params_gm_tensor.dtype},
                  {"shape": tmp_indices_shape, "dtype": self._indices_gm_tensor.dtype}]

        base_info = [util.get_core_num(), self._ub_size, self._gather_compute_type,
                     self._params_dtype_size, self._indices_dtype_size]
        if util.is_nano():
            base_info.append(self._ub_block_size)
        check_ids = 0
        move_pad = False
        if operation.get_context().get("_check_ids") is not None:
            check_ids = operation.get_context().get("_check_ids")
        if operation.get_context().get("_move_pad") is not None:
            move_pad = operation.get_context().get("_move_pad")
        custom_info = [int(self._params_ub_size // self._params_dtype_size),
                       self._batch_dims, False, self._batch_dims, check_ids, move_pad, self.is_310p]
        tensor_sizes = {self._special_pattern: [self._gather_storage_bound, self._indices_storage_bound,
                                                self._params_storage_bound]}
        if self._special_pattern != GatherCompileInfo.BASE_SCHEDULE_PATTERN:
            tensor_sizes[GatherCompileInfo.BASE_SCHEDULE_PATTERN] = \
                [self._gather_storage_bound, self._indices_storage_bound, self._params_storage_bound]
        const_compile_info = {
            CompileInfo.BASE_INFO: base_info,
            GatherCompileInfo.CUSTOM_INFO: custom_info,
            GatherCompileInfo.CONST_AXIS: self._axis,
            GatherCompileInfo.TENSOR_SIZES: tensor_sizes
        }
        const_compile_info.update(get_compile_info())
        op_type = "AutoTiling"
        run_info = op_tiling.do_op_tiling(op_type, const_compile_info, inputs, outputs)
        tiling_format = {
            "tiling_key": "int64",
            "block_axis": "int64",
            "block_factor": "int64",
            "ub_axis": "int64",
            "ub_factor": "int64"}
        self._static = True
        tiling_data = op_tiling.decode(run_info["tiling_data"], tiling_format)
        const_tiling_key = tiling_data.get("tiling_key")
        self._const_block_axis = tiling_data.get("block_axis")
        self._block_axis = self._const_block_axis
        self._const_block_factor = tiling_data.get("block_factor")
        self._const_ub_axis = tiling_data.get("ub_axis")
        self._ub_axis = self._const_ub_axis
        self._const_ub_factor = tiling_data.get("ub_factor")

        if operation.get_context().get(GatherCompileInfo.STATIC_SUCCESS) or const_tiling_key != self._tiling_key:
            operation.get_context().get_current_compute().get_current_schedule() \
                .add(GatherCompileInfo.FAKE_SCHEDULE, True)
            self._fake_schedule = True
        else:
            operation.get_context().add(GatherCompileInfo.STATIC_SUCCESS, True)
            if self._store_area == 1:
                operation.get_context().add(GatherCompileInfo.STATIC_CLOSE_PASS, False)
            else:
                operation.get_context().add(GatherCompileInfo.STATIC_CLOSE_PASS, True)

    def _do_tiling(self):
        funcs = {TilingStrategy.DYNAMIC: self._do_tiling_dynamic,
                 TilingStrategy.STATIC: self._do_tiling_static}
        funcs.get(self._tiling_strategy)()

    def _do_tiling_dynamic(self):
        b_idx = self._tiling_case["block_tiling_axis"]
        u_idx = self._tiling_case["ub_tiling_axis"]
        b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[b_idx],
                                                          factor=self._block_tiling_vars.get(b_idx))

        if b_idx == u_idx:
            u_o, u_i = self._schedule[self._out_tensor].split(b_i, factor=self._ub_tiling_vars.get(u_idx))
        else:
            u_o, u_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[u_idx],
                                                              factor=self._ub_tiling_vars.get(u_idx))

        self._block_bind_axis = b_o
        self._compute_at_axis = u_o
        self._gather_emit_at_axis = self._gather_ub_tensor.op.axis[-1]

        # gather align
        if self._is_need_align:
            self._gather_align_axis = self._gather_ub_tensor.op.axis[-2]

        # remove pad
        if self._remove_pad:
            self._remove_pad_emit_at_axis = self._removd_pad_tensor.op.axis[0]

        # res emit
        self._res_emit_at_axis = u_i

    def _do_tiling_static(self):
        if self._is_v350_special:
            self._compute_at_axis = self._out_tensor.op.axis[0]
            self._res_emit_at_axis = self._out_tensor.op.axis[1]
            return

        b_idx = self._const_block_axis
        u_idx = self._const_ub_axis

        b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[b_idx],
                                                          factor=self._const_block_factor)

        if b_idx == u_idx:
            u_o, u_i = self._schedule[self._out_tensor].split(b_i, factor=self._const_ub_factor)
        else:
            u_o, u_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[u_idx],
                                                              factor=self._const_ub_factor)
        self._block_bind_axis = b_o
        self._compute_at_axis = u_o
        self._gather_emit_at_axis = self._gather_ub_tensor.op.axis[-1]

        # gather align
        if self._is_need_align:
            self._gather_align_axis = self._gather_ub_tensor.op.axis[-2]

        # remove pad handle
        if self._remove_pad:
            self._remove_pad_emit_at_axis = self._removd_pad_tensor.op.axis[0]

        # res emit
        self._res_emit_at_axis = u_i

    def _calc_multi_core(self):
        pass

    def _do_multi_core(self):
        if self._is_v350_special:
            return
        if self._block_bind_axis is not None:
            block = tvm.thread_axis("blockIdx.x")
            self._schedule[self._out_tensor].bind(self._block_bind_axis, block)

    def _calc_storage_align(self):
        self._align_factor = self._ub_block_size // self._params_dtype_size
        if self._params_dtype_size == 1:
            self._align_factor_t = 32
            self._align_factor_v = 32
        elif self._params_dtype_size == 4:
            self._align_factor_v = 32
            self._align_factor_t = 16
        else:
            self._align_factor_v = 16
            self._align_factor_t = 16

    def _do_storage_align(self):
        """
        ub tensors:
        self._params_inner_tensor -> self._in_transpose_tensors -> self._gather_tensors -> self._out_transpose_tensors
        -> self._gather_ub_tensor 
        """
        if self._is_v350_special:
            self._schedule[self._params_inner_tensor].storage_align(self._params_inner_tensor.op.axis[0], 128, 0)
            self._schedule[self._params_inner_tensor].compute_align(self._params_inner_tensor.op.axis[1], \
                                                                    self._compute_align_factor)
            self._schedule[self._in_transpose_tensors].storage_align(
                self._in_transpose_tensors.op.axis[-4], TRANSPOSE_FACTOR, 0)
            self._schedule[self._gather_tensors].storage_align(
                self._gather_tensors.op.axis[-3], TRANSPOSE_FACTOR, 0)
            self._schedule[self._out_transpose_tensors].storage_align(
                self._out_transpose_tensors.op.axis[-4], TRANSPOSE_FACTOR // 2, 0)
            return

        if self._is_need_align:
            self._schedule[self._gather_ub_tensor].storage_align(self._gather_align_axis, self._align_factor, 0)
        if self._is_params_align and not self._is_transpose:
            self._schedule[self._params_inner_tensor].storage_align(self._params_inner_tensor.op.axis[-2],
                                                                    self._align_factor, 0)
        if self._is_transpose and self._is_trans_special and not self._is_vreduce:
            if not self._is_params_align:
                self._schedule[self._params_inner_tensor].storage_align(self._params_inner_tensor.op.axis[-2],
                                                                        self._align_factor, 0)
            self._schedule[self._params_inner_t_tensor].storage_align(self._params_inner_t_tensor.op.axis[-2],
                                                                    self._align_factor_t, 0)
        if self._is_transpose and not self._is_trans_special and not self._is_vreduce:
            if not self._is_params_align:
                self._schedule[self._params_inner_tensor].storage_align(self._params_inner_tensor.op.axis[-2],
                                                                        self._align_factor, 0)
                self._schedule[self._gather_ub_tensor_t].storage_align(self._gather_ub_tensor_t.op.axis[-2],
                                                                    self._align_factor, 0)
            self._schedule[self._gather_ub_tensor].compute_align(self._gather_ub_tensor.op.axis[-1],
                                                                 self._align_factor_t)
        
            self._schedule[self._params_inner_t_tensor].storage_align(self._params_inner_t_tensor.op.axis[-2],
                                                                    self._align_factor_t, 0)
        if self._is_trans_special and self._is_vreduce and self._last_row not in (2, 4):
            self._schedule[self._params_inner_tensor].storage_align(self._params_inner_tensor.op.axis[-2],
                                                                    self._align_factor_v, 0)

    def _calc_compute_align(self):
        if self._is_v350_special:
            def add_compute_align(tensor, axis, factor):
                if tensor in self._compute_align_map:
                    self._compute_align_map.get(tensor).append((axis, factor))
                else:
                    self._compute_align_map[tensor] = [(axis, factor)]
                
            def compute_align_list(tensor_i, factor_list):
                for axis_index, factor in factor_list:
                    add_compute_align(tensor_i, tensor_i.op.axis[axis_index], factor)

            in_align_list = [(-1, TRANSPOSE_FACTOR), (1, 4)]
            compute_align_list(self._in_transpose_tensors, in_align_list)
            compute_align_list(self._gather_tensors, in_align_list)
            out_align_list = [(2, TRANSPOSE_FACTOR), (-3, 4)]
            compute_align_list(self._out_transpose_tensors, out_align_list)

    def _do_compute_align(self):
        sch = self._schedule
        for tensor_i, params in self._compute_align_map.items():
            for align_value in params:
                sch[tensor_i].compute_align(*align_value)

    def _calc_compute_at(self):
        if self._is_v350_special:
            for tensor_i in self._ub_buffer_list:
                if tensor_i not in self._compute_inline_tensors:
                    self._compute_at_map[tensor_i] = [self._out_tensor, self._compute_at_axis]
            return
        # params indcies inputs
        if not self._is_transpose:
            for tensor_i in self._input_tensors:
                self._compute_at_map[tensor_i] = [self._out_tensor, self._compute_at_axis]

            # params ub
            if self._store_area > 0:
                local_compute_at_axis = self._block_bind_axis
                if self._is_partial_store:
                    store_at_axis = 1 if self._gather_compute_type == GATHER else 0
                    if self._ub_axis > store_at_axis:
                        if store_at_axis == self._block_axis:
                            local_compute_at_axis = self._compute_at_axis
                        else:
                            local_compute_at_axis = self._out_tensor.op.axis[store_at_axis]
                    elif self._ub_axis == store_at_axis:
                        local_compute_at_axis = self._compute_at_axis

                self._compute_at_map[self._params_inner_tensor] = [self._out_tensor, local_compute_at_axis]
            # indices ub
            self._compute_at_map[self._indices_ub_tensor] = [self._out_tensor, self._compute_at_axis]

        else:
            self._compute_at_map[self._params_inner_tensor] = [self._out_tensor, self._compute_at_axis]
            if not self._is_vreduce:
                self._compute_at_map[self._params_inner_t_tensor] = [self._out_tensor, self._compute_at_axis]
            self._compute_at_map[self._indices_ub_tensor] = [self._out_tensor, self._block_bind_axis]
            if not self._is_trans_special:
                self._compute_at_map[self._gather_ub_tensor_t] = [self._out_tensor, self._compute_at_axis]

        # gather ub
        self._compute_at_map[self._gather_ub_tensor] = [self._out_tensor, self._compute_at_axis]

        # remove pad
        if self._remove_pad:
            self._compute_at_map[self._removd_pad_tensor] = [self._out_tensor, self._compute_at_axis]

    def _do_compute_at(self):
        for tensor_i, param in self._compute_at_map.items():
            self._schedule[tensor_i].compute_at(self._schedule[param[0]], param[1])

    def _calc_double_buffer(self):
        pass

    def _do_double_buffer(self):
        if self._is_v350_special:
            return
        if self._is_db:
            self._schedule[self._indices_ub_tensor].double_buffer()
            self._schedule[self._gather_ub_tensor].double_buffer()

    def _calc_emit_insn(self):
        # indcies ub
        self._emit_insn_map[self._indices_ub_tensor] = [self._indices_ub_tensor.op.axis[0], DMA_COPY]

        if self._is_v350_special:
            transpose_attrs1 = {"is_trans_align": 1, "enable_vnchwconv_mode": 1, 
                                "src_in_dst_order": tvm.call_intrin("handle", "tir.tvm_tuple", *[1, 2, 3, 0])}
            transpose_attrs2 = {"is_trans_align": 1, "enable_vnchwconv_mode": 3, 
                                "src_in_dst_order": tvm.call_intrin("handle", "tir.tvm_tuple", *[2, 0, 1])}
            self._emit_insn_map[self._params_inner_tensor] = [self._params_inner_tensor.op.axis[0], DMA_COPY]
            self._emit_insn_map[self._in_transpose_tensors] = [self._in_transpose_tensors.op.axis[0], \
                VECTOR_TRANSPOSE, transpose_attrs1] 
            self._emit_insn_map[self._gather_tensors] = [self._gather_tensors.op.axis[0], DMA_COPY]
            self._emit_insn_map[self._out_transpose_tensors] = [self._out_transpose_tensors.op.axis[0], \
                VECTOR_TRANSPOSE, transpose_attrs2]
            self._emit_insn_map[self._out_tensor] = [self._res_emit_at_axis, DMA_COPY]
            return
        # params_ub need
        is_move_pad = False
        if tbe_platform_info.api_check_support("tik.data_move_pad"):
            is_move_pad = True
        if self._store_area == 1:
            # If criteria for removing copy gm to ub under dynamic shape
            if is_move_pad and not self._is_transpose:
                self._emit_insn_map[self._params_inner_tensor] = [self._params_inner_tensor.op.axis[0], DMA_COPY,
                                                                  {"map_policy": "1d"}]
            else:
                self._emit_insn_map[self._params_inner_tensor] = [self._params_inner_tensor.op.axis[0], DMA_COPY]
        if self._is_transpose and not self._is_vreduce:
            two_dim_in_ord = tvm.call_intrin("handle", "tir.tvm_tuple", 1, 0)
            transpose_attrs = {"src_in_dst_order": two_dim_in_ord, "is_trans_align": 1}
            if self._out_tensor.dtype in ("int32", "float32", "uint32"):
                transpose_attrs["enable_vnchwconv_b32"] = 1
            self._emit_insn_map[self._params_inner_t_tensor] = [self._params_inner_t_tensor.op.axis[0],
                                                                "vector_transpose", transpose_attrs]
            if self._is_trans_special:
                self._emit_insn_map[self._gather_ub_tensor] = [self._gather_ub_tensor.op.axis[0], DMA_COPY,
                                                               {"no_need_add_dup": 1}]
            else:
                self._emit_insn_map[self._gather_ub_tensor] = [self._gather_ub_tensor.op.axis[0],
                                                               "vector_transpose", transpose_attrs]
                self._emit_insn_map[self._gather_ub_tensor_t] = [self._gather_ub_tensor_t.op.axis[1], DMA_COPY,
                                                                 {"no_need_add_dup": 1}]
            self._emit_insn_map[self._out_tensor] = [self._res_emit_at_axis, DMA_COPY]
        elif self._is_transpose and self._is_vreduce:
            is_factor_on1 = self._static and (self._const_ub_factor == 1)
            if is_factor_on1:
                last_rows = (self._last_row + self._align_factor_v - 1) // self._align_factor_v * self._align_factor_v
                align_axes_value = tvm.call_intrin("handle", "tir.tvm_tuple", *[last_rows])
                self._emit_insn_map[self._gather_ub_tensor] = [self._gather_ub_tensor.op.axis[0], VECTOR_REDUCE,
                                                               {"enable_vreduce": 3,
                                                                "align_axes_value": align_axes_value}]
            else:
                if self._last_row == 2:
                    self._emit_insn_map[self._gather_ub_tensor] = [self._gather_ub_tensor.op.axis[0], VECTOR_REDUCE,
                                                                   {"enable_vreduce": 1}]
                elif self._last_row == 4:
                    self._emit_insn_map[self._gather_ub_tensor] = [self._gather_ub_tensor.op.axis[0], VECTOR_REDUCE,
                                                                   {"enable_vreduce": 2}]
                else:
                    self._emit_insn_map[self._gather_ub_tensor] = [self._gather_ub_tensor.op.axis[0], VECTOR_REDUCE,
                                                                   {"enable_vreduce": 3}]
            self._emit_insn_map[self._out_tensor] = [self._res_emit_at_axis, DMA_COPY]
        # gather ub
        if not self._is_transpose:
            if self._store_area == 1:
                if self._scalar_mode:
                    self._emit_insn_map[self._gather_ub_tensor] = [self._gather_emit_at_axis, "data_mov"]
                else:
                    if is_move_pad and not self._is_transpose:
                        self._emit_insn_map[self._gather_ub_tensor] = [self._gather_emit_at_axis, DMA_COPY,
                                                                       {"map_policy": "1d", "no_need_add_dup": 1}]
                    else:
                        self._emit_insn_map[self._gather_ub_tensor] = [self._gather_emit_at_axis, DMA_COPY,
                                                                       {"no_need_add_dup": 1}]

            else:
                if is_move_pad:
                    self._emit_insn_map[self._gather_ub_tensor] = [self._gather_emit_at_axis, DMA_COPY,
                                                                   {"map_policy": "1d", "no_need_add_dup": 1}]
                else:
                    self._emit_insn_map[self._gather_ub_tensor] = [self._gather_emit_at_axis, DMA_COPY,
                                                                   {"no_need_add_dup": 1}]

            self._schedule[self._gather_ub_tensor].pragma(self._gather_emit_at_axis, 'loop_with_no_overlap_tensor')

            # remove pad
            if self._remove_pad:
                self._emit_insn_map[self._removd_pad_tensor] = [self._remove_pad_emit_at_axis, "remove_pad"]

            # res
            if self._use_malloc_buf:
                self._emit_insn_map[self._out_tensor] = [self._res_emit_at_axis, DMA_COPY,
                                                         {"no_overlap": "process_unaliged_stride_with_malloc_buf",
                                                          "no_overlap_malloc_buf_for_tail": 0}]
            else:
                self._emit_insn_map[self._out_tensor] = [self._res_emit_at_axis, DMA_COPY, {"no_overlap": 2}]

    def _do_emit_insn(self):
        for tensor_i, param in self._emit_insn_map.items():
            if tensor_i.name == self._out_tensor.name and self._tiling_strategy == TilingStrategy.DYNAMIC:
                attr_dict = param[2] if len(param) > 2 else {}
                attr_dict["force_axis_group"] = 1
                if len(param) > 2:
                    param[2] = attr_dict
                else:
                    param.append(attr_dict)
            self._schedule[tensor_i].emit_insn(*param)

    def _do_group_axis(self):
        if self._is_v350_special:
            return 
        if self._store_area == 1 and not self._is_params_align and not self._is_transpose:
            group_id = tvm.call_extern("int32", "axis_group", 0, "append")
            self._schedule[self._params_inner_tensor].pragma(
                self._schedule[self._params_inner_tensor].op.axis[0], "axis_group", group_id)
            self._schedule[self._params_inner_tensor].pragma(
                self._schedule[self._params_inner_tensor].op.axis[1], "axis_group", group_id)
            self._schedule[self._params_inner_tensor].pragma(
                self._schedule[self._params_inner_tensor].op.axis[2], "axis_group", group_id)
            if self._gather_compute_type == 0:
                self._schedule[self._params_inner_tensor].pragma(
                    self._schedule[self._params_inner_tensor].op.axis[3], "axis_group", group_id)

    def _add_compile_info(self):
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()

        cpt_schedule.add(GatherCompileInfo.FAKE_SCHEDULE, False)

        # BASE INFO
        cpt_schedule.add(CompileInfo.CORE_NUM, util.get_core_num())
        cpt_schedule.add(CompileInfo.UB_SIZE, self._ub_size) 
        cpt_schedule.add(GatherCompileInfo.GATHER_TYPE, self._gather_compute_type)
        cpt_schedule.add(GatherCompileInfo.PARAMS_DTYPE_SIZE, self._params_dtype_size)
        cpt_schedule.add(GatherCompileInfo.INDICES_DTYPE_SIZE, self._indices_dtype_size)
        if util.is_nano():
            cpt_schedule.add(CompileInfo.UB_BLOCK_SIZE, self._ub_block_size)

        # CUSTOM INFO
        cpt_schedule.add(GatherCompileInfo.PARAMS_UB_NUM, int(self._params_ub_size // self._params_dtype_size))
        cpt_schedule.add(GatherCompileInfo.BATCH_DIMS, self._batch_dims)
        cpt_schedule.add(GatherCompileInfo.SPECIAL_PATTERN, self._special_pattern)
        cpt_schedule.add(GatherCompileInfo.PARAMS_NUM, self._gather_storage_bound)
        cpt_schedule.add(GatherCompileInfo.INDICES_NUM, self._indices_storage_bound)
        cpt_schedule.add(GatherCompileInfo.PARAMS_NUM1, self._params_storage_bound)

    def __dfs_sub_graph(self, out, visited_tensors: set):

        if len(out.op.attrs) > 0:
            _gather_op_name = operation.get_context().get("_gather_mode")
            self._gather_compute_type = 0 if _gather_op_name == "gather" else 1
            if _gather_op_name in ["gather", "gather_nd"]:
                if "params_name" in out.op.attrs:
                    self._params_name = out.op.attrs["params_name"]

                if "indices_name" in out.op.attrs:
                    self._indices_name = out.op.attrs["indices_name"]

        for tensor_i in out.op.input_tensors:
            util.merge_value(self._in_out_map, tensor_i, out)
            self._dtypes.add(tensor_i.dtype)

            if util.is_placeholder(tensor_i):
                self._input_tensors.add(tensor_i)

            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self.__dfs_sub_graph(tensor_i, visited_tensors)
