#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pooling grad nchw schedule
"""
from tbe import tvm
from tbe.tvm import PlaceholderOp
from tbe.common import buildcfg
from tbe.dsl.base.operation import add_build_arg
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import var_inner

from .pooling_grad_helper import PoolingGradConstants
from .pooling_grad_helper import PoolingGradSchType
from .pooling_grad_helper import get_insn
from .pooling_grad_helper import judge_tvm_shape_equal
from .pooling_grad_info import PoolingGradComputeInfo
from .pooling_grad_info import PoolingGradSocInfo
from .pooling_grad_with_arg_tilingcase import PoolingGrad4DTilingCase
from ... import util
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG
from ...constants import Pattern
from ...constants import PoolingGradPattern
from ...schedule import Schedule


class PGSchConstant:
    """
    constants of NCHW Schedule
    """
    RESERVE_SPACE = 1024
    MAX_COEXIST_NODE_NUM_FP16 = 4
    MAX_COEXIST_NODE_NUM_FP32 = 4
    ALIGN_C = 16
    ALIGN_K = 256
    HO_IDX = 0
    WO_IDX = 1
    NC_IDX = 2
    KH_IDX = 3
    KW_IDX = 4


class EntryPoolingGradWithArgSchedule(Schedule):
    """
    entrance to pooling grad NCHW schedule
    """

    def __init__(self, outs, tiling_case):
        self.outs = outs
        self.tiling_case = tiling_case

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return ["default"]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.POOLING_GRAD_WITH_ARG]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [PoolingGradPattern.PG_1]

    def do_schedule(self):
        current_compute = get_context().get_current_compute()
        compute_info = current_compute.get("_compute_info")
        if self.tiling_case.sch_type == PoolingGradSchType.COMMON:
            pooling_grad_with_arg_sch = PoolingGradWithArgNCHWCommonSchedule(
                compute_info, self.tiling_case, self.outs)
            return pooling_grad_with_arg_sch.do_schedule()
        else:
            return None


class PoolingGradWithArgNCHWBaseSchedule:
    """
    pooling grad with arg nchw base schedule
    """

    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGrad4DTilingCase, outs):
        self._outs = outs
        self._sch = None
        self._scope = PoolingGradConstants.LOCAL_UB
        self._tiling_case = tiling_case

        self._compute_info = compute_info
        self._forward_compute_graph_map = compute_info.graph_info.tensor_consumers_map
        self._backward_compute_graph_map = compute_info.graph_info.tensor_producers_map
        self._mid_tensor_set = compute_info.graph_info.mid_tensor_set
        self._res_tensor = compute_info.base_info.res_tensor
        self._assist_tensor_set = compute_info.graph_info.assist_tensor_set
        self._reduce_window_tensor = compute_info.base_info.reduce_window_tensor
        self._real_output_set = self._compute_info.graph_info.real_pure_output_tensor_set

        self._cache_read_tensors = set()
        self._cache_transpose_tensors = set()
        self._reorder_cache_write_tensors = set()
        self._cache_read_buffer_and_tensor_map = {}
        self._cache_read_tensor_and_buffer_map = {}

        self._cache_write_tensors = set()
        self._cache_write_buffer_and_tensor_map = {}
        self._cache_write_tensor_and_buffer_map = {}

        self._compute_inline_tensors = set()
        self._mem_unique_tensors = set()

        self._block_split_result = {}
        self._ub_split_result = {}
        self._tensor_set_before_reduce_node = set()
        self._tensor_set_after_reduce_node = set()

        self._reorder_map = {}
        self._multi_core_bind_axis = None
        self._multi_core_bind_map = None
        self._storage_align_map = {}

        self._compute_at_map = {}
        self._reverse_compute_at_map = {}
        self._emit_insn_map = {}
        self._buffer_size = None

        self.block = None
        self._reduce_cache_node = None
        self._indices_node = None
        self._indices_cache_trans_node = None
        self.f_nc_o = None
        self.f_nc_i = None
        self.r_nc_o = None
        self.r_nc_i = None

    def _calc_cache_read(self):
        for _tensor in self._compute_info.graph_info.input_tensor_set:
            self._cache_read_tensors.update({_tensor})

    def _do_cache_read(self):
        for _cache_read_tensor in self._cache_read_tensors:
            read_buffer = self._sch.cache_read(
                _cache_read_tensor, self._scope, self._forward_compute_graph_map.get(_cache_read_tensor))
            self._cache_read_buffer_and_tensor_map[read_buffer] = _cache_read_tensor
            self._cache_read_tensor_and_buffer_map[_cache_read_tensor] = read_buffer

    def _calc_cache_write(self):
        for _tensor in self._compute_info.graph_info.output_tensor_set:
            self._cache_write_tensors.update({_tensor})

    def _do_cache_write(self):
        for _cache_write_tensor in self._cache_write_tensors:
            write_buffer = self._sch.cache_write(
                _cache_write_tensor, self._scope)
            self._cache_write_buffer_and_tensor_map[write_buffer] = _cache_write_tensor
            self._cache_write_tensor_and_buffer_map[_cache_write_tensor] = write_buffer

    def _set_scope(self):
        for mid_tensor in self._mid_tensor_set - self._compute_info.graph_info.real_output_tensor_set:
            self._sch[mid_tensor].set_scope(self._scope)

    def _do_cache_transpose(self):
        """
        input tensor and indices tensor should be transposed from [nc,h,w] to [h,w,nc]
        """
        for _cache_read_buffer, _ori_tensor in self._cache_read_buffer_and_tensor_map.items():
            transpose_buffer = self._sch.cache_transpose(_cache_read_buffer, [1, 2, 0], self._scope, list(
                self._compute_info.graph_info.tensor_consumers_map.get(_ori_tensor)))
            if self._tiling_case.ub_split_axis_index == PGSchConstant.KW_IDX:
                self._sch[transpose_buffer].compute_align(transpose_buffer.op.axis[1], PGSchConstant.ALIGN_C)
            self._cache_transpose_tensors.update({transpose_buffer})
            self._tensor_set_before_reduce_node.add(_cache_read_buffer)
            self._tensor_set_before_reduce_node.add(transpose_buffer)
        for _tensor in self._mid_tensor_set:
            if _tensor.op.tag == PoolingGradConstants.INDICES_TAG:
                self._sch[_tensor].compute_align(_tensor.op.axis[0], PGSchConstant.ALIGN_C)
                transpose_buffer = self._sch.cache_transpose(_tensor, [1, 2, 0], self._scope, list(
                    self._compute_info.graph_info.tensor_consumers_map.get(_tensor)))
                self._cache_transpose_tensors.update({transpose_buffer})
                self._tensor_set_before_reduce_node.add(_tensor)
                self._tensor_set_before_reduce_node.add(transpose_buffer)
                self._sch[_tensor].compute_align(_tensor.op.axis[-1], PGSchConstant.ALIGN_C)
                self._sch[transpose_buffer].compute_align(transpose_buffer.op.axis[-1], PGSchConstant.ALIGN_C)
                self._sch[transpose_buffer].compute_align(transpose_buffer.op.axis[-2], PGSchConstant.ALIGN_C)
                self._indices_node = _tensor
                self._indices_cache_trans_node = transpose_buffer

    def _do_cache_wrtie_reorder_compute_inline(self):
        """
        mid tensors should do reorder and compute inline 
        """
        for _tensor in self._mid_tensor_set \
                .union(self._cache_write_buffer_and_tensor_map.keys()):
            if _tensor.op.tag == PoolingGradConstants.INDICES_TAG:
                continue
            # reorder
            if len(_tensor.op.axis[:]) == 3:  # nc,h,w --> h,w,nc
                self._sch[_tensor].reorder(
                    *_tensor.op.axis[1:], _tensor.op.axis[0])
            else:   # khw,nc,h,w --> khw,h,w,nc
                self._sch[_tensor].reorder(
                    _tensor.op.axis[0], *_tensor.op.axis[2:], _tensor.op.axis[1])
            # cache write
            if _tensor.op.tag == PoolingGradConstants.REDUCE_SUM_TAG:
                self._reduce_cache_node = self._sch.cache_write(
                    _tensor, self._scope)
                self._reorder_cache_write_tensors.update(
                    {self._reduce_cache_node})
            else:
                cache_tensor = self._sch.cache_write(_tensor, self._scope)
                self._reorder_cache_write_tensors.update({cache_tensor})
                if PoolingGradConstants.DEPAD_NODE in _tensor.op.name:
                    self._tensor_set_after_reduce_node.add(cache_tensor)
                    self._tensor_set_after_reduce_node.add(_tensor)
                    self._tensor_set_after_reduce_node.add(
                        self._cache_write_buffer_and_tensor_map.get(_tensor))
                else:
                    self._tensor_set_before_reduce_node.add(cache_tensor)
            # compute inline
            if _tensor in self._cache_write_buffer_and_tensor_map.keys():
                pass
            else:
                self._compute_inline_tensors.update({_tensor})
                self._sch[_tensor].compute_inline()

    def _calc_buffer_size(self):
        if list(self._real_output_set)[0].dtype == "float32":
            max_coexist_node_num = PGSchConstant.MAX_COEXIST_NODE_NUM_FP32
        else:
            max_coexist_node_num = PGSchConstant.MAX_COEXIST_NODE_NUM_FP16
        ub_size = PoolingGradSocInfo.get_ub_size() - PGSchConstant.RESERVE_SPACE
        coexist_node = max_coexist_node_num + len(self._mem_unique_tensors)
        if self._tiling_case.is_enable_db:
            coexist_node = coexist_node * 2
        self._buffer_size = ub_size // DTYPE_BYTE_MAPPING.get(
            self._compute_info.graph_info.max_type) // coexist_node

        buffer_size_list = get_compile_info().get("_buffer_size")
        if buffer_size_list is None:
            buffer_size_list = {}
            add_compile_info_inner("_buffer_size", buffer_size_list)
        if self._tiling_case.pattern_key not in buffer_size_list:
            buffer_size_list[self._tiling_case.pattern_key] = [0] * 2
            buffer_size_list.get(self._tiling_case.pattern_key)[
                self._tiling_case.sch_key] = self._buffer_size
        else:
            buffer_size = buffer_size_list.get(self._tiling_case.pattern_key)
            if buffer_size[self._tiling_case.sch_key] != 0:
                buffer_size[self._tiling_case.sch_key] = min(
                    buffer_size[self._tiling_case.sch_key], self._buffer_size)
            else:
                buffer_size[self._tiling_case.sch_key] = self._buffer_size

    def _pre_check_sch(self):
        pass

    def _do_const_tiling(self):
        if self._tiling_case.is_const:
            self._tiling_case.calc_const_tiling()

    def _post_check_sch(self):
        return self._tiling_case.check_consistency()

    def _do_fakenode_tiling(self):
        self.f_nc_o, self.f_nc_i = self._sch[self._res_tensor].split(
            self._res_tensor.op.axis[0], factor=PGSchConstant.ALIGN_C)

    def _do_block_tiling(self):
        """
        0: split ho
        1: split wo
        2: split nc
        """
        block_split_axis_index = self._tiling_case.block_split_axis_index
        block_factor = self._tiling_case.block_factor
        block_split_factor = block_factor if block_factor else var_inner("_block_factor", (1, None))
        if block_split_axis_index == PGSchConstant.NC_IDX:
            block_outer, block_inner = self._sch[self._res_tensor].split(
                self.f_nc_o, factor=block_split_factor)
            self._sch[self._res_tensor].reorder(
                block_outer, block_inner, *self._res_tensor.op.axis[1:], self.f_nc_i)
        else:
            self._sch[self._res_tensor].reorder(
                self.f_nc_o, *self._res_tensor.op.axis[1:], self.f_nc_i)
            block_outer, block_inner = self._sch[self._reduce_cache_node].split(
                self._reduce_cache_node.op.reduce_axis[block_split_axis_index + 2], factor=block_split_factor)

        self._block_split_result["index"] = block_split_axis_index
        self._block_split_result["outer_itervar"] = block_outer
        self._block_split_result["inner_itervar"] = block_inner
        self._block_split_result["factor"] = block_split_factor

    def _do_ub_tiling(self):
        """
        0: split ho
        1: split wo
        2: split nc
        3: split kh
        4: split kw
        """
        block_split_axis_index = self._tiling_case.block_split_axis_index
        ub_split_axis_index = self._tiling_case.ub_split_axis_index
        ub_factor = self._tiling_case.ub_factor
        ub_split_factor = ub_factor if ub_factor else var_inner("_ub_factor", (1, None))
        split_dict = {
            0: self._reduce_cache_node.op.reduce_axis[2],
            1: self._reduce_cache_node.op.reduce_axis[3],
            3: self._reduce_cache_node.op.reduce_axis[0],
            4: self._reduce_cache_node.op.reduce_axis[1],
            2: self._reduce_cache_node.op.axis[2]
        }
        if block_split_axis_index == PGSchConstant.NC_IDX:
            if ub_split_axis_index == PGSchConstant.NC_IDX:
                ub_outer, ub_inner = self._block_split_result.get("inner_itervar"), self.f_nc_i
                self.r_nc_o, self.r_nc_i = self._sch[self._reduce_cache_node].split(
                    self._reduce_cache_node.op.axis[-1], nparts=1)
            else:
                ub_outer, ub_inner = self._sch[self._reduce_cache_node].split(
                    split_dict.get(ub_split_axis_index), factor=ub_split_factor)
        else:
            if block_split_axis_index == ub_split_axis_index:
                ub_outer, ub_inner = self._sch[self._reduce_cache_node].split(
                    self._block_split_result.get("inner_itervar"), factor=ub_split_factor)
            else:
                ub_outer, ub_inner = self._sch[self._reduce_cache_node].split(
                    split_dict.get(ub_split_axis_index), factor=ub_split_factor)
        self._ub_split_result["index"] = ub_split_axis_index
        self._ub_split_result["outer_itervar"] = ub_outer
        self._ub_split_result["inner_itervar"] = ub_inner
        self._ub_split_result["factor"] = ub_split_factor

    def _do_set_buffer_size(self):
        for _tensor in self._reorder_cache_write_tensors \
                .union(self._cache_read_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._cache_transpose_tensors) \
                .union({self._indices_node}):
            if _tensor in self._compute_inline_tensors:
                continue
            if _tensor.op.tag == PoolingGradConstants.VCMP_TAG:
                storage_bound_value = int(self._buffer_size *
                                          DTYPE_BYTE_MAPPING.get(
                                              self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                              _tensor.op.input_tensors[0].dtype))
                self._sch[_tensor].set_buffer_size(storage_bound_value)
            else:
                storage_bound_value = int(self._buffer_size *
                                          DTYPE_BYTE_MAPPING.get(
                                              self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                              _tensor.dtype))
                self._sch[_tensor].set_buffer_size(storage_bound_value)
        # set fake node buffer size
        storage_bound_value = int(self._buffer_size *
                                  DTYPE_BYTE_MAPPING.get(
                                      self._compute_info.graph_info.max_type) // DTYPE_BYTE_MAPPING.get(
                                          self._res_tensor.dtype))
        self._sch[self._res_tensor].set_buffer_size(storage_bound_value)

    def _do_storage_align_and_compute_align(self):
        for _buffer, _tensor in \
            {**self._cache_read_buffer_and_tensor_map, **self._cache_write_buffer_and_tensor_map}.items():
            if PoolingGradConstants.DEPAD_NODE in _buffer.op.name:
                self._sch[_buffer].compute_align(_buffer.op.axis[-1], PGSchConstant.ALIGN_C)
                self._sch[_buffer].compute_align(_buffer.op.axis[0], PGSchConstant.ALIGN_C)
                continue
            self._sch[_buffer].storage_align(_buffer.op.axis[-3], PGSchConstant.ALIGN_C, 0)
        for _tensor in self._cache_transpose_tensors \
                .union(self._reorder_cache_write_tensors) - self._cache_write_tensor_and_buffer_map.keys():
            if _tensor.op.tag == PoolingGradConstants.INDICES_TAG:
                continue
            if PoolingGradConstants.DEPAD_NODE in _tensor.op.name:
                self._sch[_tensor].compute_align(_tensor.op.axis[-2], PGSchConstant.ALIGN_C)
                self._sch[_tensor].compute_align(_tensor.op.axis[-1], PGSchConstant.ALIGN_C)
                continue
            self._sch[_tensor].compute_align(_tensor.op.axis[-1], PGSchConstant.ALIGN_C)
            # mask0 node should align k
            if _tensor.op.tag == PoolingGradConstants.VCMP_TAG:
                self._sch[_tensor].storage_align(_tensor.op.axis[0], PGSchConstant.ALIGN_K, 0)

    def _do_reduce_reorder(self):
        reduce_axis_list = self._reduce_cache_node.op.reduce_axis[:]
        norm_axis_list = self._reduce_cache_node.op.axis[:]

        b_o = self._block_split_result.get("outer_itervar")
        b_i = self._block_split_result.get("inner_itervar")
        u_o = self._ub_split_result.get("outer_itervar")
        u_i = self._ub_split_result.get("inner_itervar")
        reorder_dict = {
            "00": [b_o, u_o, reduce_axis_list[0], reduce_axis_list[1], u_i, reduce_axis_list[3]] + norm_axis_list[:],
            "03": [b_o, b_i, u_o, u_i, reduce_axis_list[1], reduce_axis_list[3]] + norm_axis_list[:],
            "01": [b_o, b_i, reduce_axis_list[0], u_o, reduce_axis_list[1], u_i] + norm_axis_list[:],
            "04": [b_o, b_i, reduce_axis_list[0], reduce_axis_list[3], u_o, u_i] + norm_axis_list[:],
            "13": [reduce_axis_list[2], b_o, u_o, u_i, reduce_axis_list[1], b_i] + norm_axis_list[:],
            "11": [reduce_axis_list[2], b_o, reduce_axis_list[0], u_o, reduce_axis_list[1], u_i] + norm_axis_list[:],
            "14": [reduce_axis_list[2], b_o, reduce_axis_list[0], b_i, u_o, u_i] + norm_axis_list[:],
            "20": [u_o, reduce_axis_list[0], reduce_axis_list[1], u_i, reduce_axis_list[3]] + norm_axis_list[:],
            "23": [reduce_axis_list[2], u_o, u_i, reduce_axis_list[1], reduce_axis_list[3]] + norm_axis_list[:],
            "21": [reduce_axis_list[2], reduce_axis_list[0], u_o, reduce_axis_list[1], u_i] + norm_axis_list[:],
            "24": [reduce_axis_list[2], reduce_axis_list[0], reduce_axis_list[3], u_o, u_i] + norm_axis_list[:],
            "22": [self.r_nc_o] + reduce_axis_list + norm_axis_list[:-1] + [self.r_nc_i],
        }
        if self._tiling_case.block_split_axis_index > PGSchConstant.HO_IDX:
            block_ub_key = "{}".format(
                self._tiling_case.block_split_axis_index*10 + self._tiling_case.ub_split_axis_index)
        else:
            block_ub_key = "{}{}".format(
                self._tiling_case.block_split_axis_index*10, self._tiling_case.ub_split_axis_index)
        self._sch[self._reduce_cache_node].reorder(
            *reorder_dict.get(block_ub_key))

    def _calc_multi_core(self):
        reduce_axis_list = self._reduce_cache_node.op.reduce_axis[:]
        norm_axis_list = self._reduce_cache_node.op.axis[:]
        b_o = self._block_split_result.get("outer_itervar")
        b_i = self._block_split_result.get("inner_itervar")
        multi_core_dict = {
            "0": [self._reduce_cache_node, [b_o]],
            "1": [self._reduce_cache_node, [reduce_axis_list[2], b_o]],
            "2": [self._res_tensor, [b_o]]
        }
        block_key = str(self._tiling_case.block_split_axis_index)
        self._multi_core_bind_map = multi_core_dict.get(block_key)

    def _do_multi_core(self):
        if self._multi_core_bind_map is not None:
            self.block = tvm.thread_axis(PoolingGradConstants.BLOCK_IDX)
            if len(self._multi_core_bind_map[1]) > 1:
                multi_core_axis = self._sch[self._multi_core_bind_map[0]].fuse(
                    *self._multi_core_bind_map[1])
                self._sch[self._multi_core_bind_map[0]].bind(
                    multi_core_axis, self.block)
            else:
                self._sch[self._multi_core_bind_map[0]].bind(
                    self._multi_core_bind_map[1][0], self.block)

    def _do_set_constraint(self):
        pass

    def _calc_compute_at(self):
        pass

    def _calc_reverse_compute_at(self):
        pass

    def _do_compute_at(self):
        for _tensor, param in self._compute_at_map.items():
            self._sch[_tensor].compute_at(self._sch[param[0]], param[1])

    def _do_reverse_compute_at(self):
        for _tensor, param in self._reverse_compute_at_map.items():
            self._sch[_tensor].reverse_compute_at(
                self._sch[param[0]], param[1])

    def _do_double_buffer(self):
        if not self._tiling_case.is_enable_db:
            return
        for _tensor in self._reorder_cache_write_tensors \
                .union(self._cache_read_buffer_and_tensor_map.keys()) \
                .union(self._cache_write_buffer_and_tensor_map.keys()) \
                .union(self._cache_transpose_tensors) \
                .union({self._reduce_cache_node}):
            if _tensor in self._compute_inline_tensors:
                continue
            self._sch[_tensor].double_buffer()
        for _tensor in self._cache_read_buffer_and_tensor_map.keys():
            self._sch[_tensor].preload()

    def _calc_emit_insn(self):
        pass

    def _do_emit_insn(self):
        for _tensor, param in self._emit_insn_map.items():
            if len(param) > 2:
                self._sch[_tensor].emit_insn(
                    param[0], param[1], attrs=param[2])
            else:
                self._sch[_tensor].emit_insn(param[0], param[1])

    def _do_pragma(self):
        pass


class PoolingGradWithArgNCHWCommonSchedule(PoolingGradWithArgNCHWBaseSchedule):
    """
    pooling grad with arg nchw common schedule
    """

    def __init__(self, compute_info: PoolingGradComputeInfo, tiling_case: PoolingGrad4DTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)

    def do_schedule(self):
        self._sch = tvm.create_schedule(
            [self._res_tensor.op, list(self._real_output_set)[0].op])
        self._sch.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_cache_transpose()
        self._do_cache_wrtie_reorder_compute_inline()

        self._calc_buffer_size()
        self._do_set_buffer_size()

        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._do_storage_align_and_compute_align()

        self._do_fakenode_tiling()
        self._do_block_tiling()
        self._do_ub_tiling()

        self._do_reduce_reorder()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_compute_at()
        self._do_compute_at()

        self._calc_reverse_compute_at()
        self._do_reverse_compute_at()

        self._do_set_constraint()
        self._do_double_buffer()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_buffer_unreuse()
        self._do_pragma()

        if not self._tiling_case.is_const:
            self._do_pragma_dynamic()

        return self._sch

    def _calc_compute_at(self):
        for _tensor in self._tensor_set_before_reduce_node.union({self._indices_cache_trans_node}):
            if self._block_split_result.get("index") == PGSchConstant.NC_IDX and \
                    self._ub_split_result.get("index") == PGSchConstant.NC_IDX:
                # when split nc, all nodes should compute at to fakenode
                self._compute_at_map[_tensor] = [
                    self._res_tensor, self._block_split_result.get("inner_itervar")]
            else:
                # node before reduce_sum should compute at to reduce_sum
                self._compute_at_map[_tensor] = [
                    self._reduce_cache_node, self._ub_split_result.get("outer_itervar")]
        # reduce_sum should compute at to fakenode
        if self._block_split_result.get("index") == PGSchConstant.NC_IDX:
            self._compute_at_map[self._reduce_cache_node] = [
                self._res_tensor, self._block_split_result.get("inner_itervar")]
        else:
            self._compute_at_map[self._reduce_cache_node] = [
                self._res_tensor, self.f_nc_o]

    def _calc_reverse_compute_at(self):
        for _tensor in self._tensor_set_after_reduce_node:
            # node after reduce_sum should reverse compute at to reduce_sum
            if self._block_split_result.get("index") == PGSchConstant.NC_IDX and \
                    self._ub_split_result.get("index") == PGSchConstant.NC_IDX:
                self._reverse_compute_at_map[_tensor] = [
                    self._reduce_cache_node, self.r_nc_o]
            else:
                self._reverse_compute_at_map[_tensor] = [
                    self._reduce_cache_node, self._ub_split_result.get("outer_itervar")]

    def _calc_emit_insn(self):
        transpose_var = 2 if self._tiling_case.is_const else 1  # dynamic is 2, static is 1
        emit_insn_index = 0
        # cache_read_tensor
        for _cache_read_buffer, _ori_tensor in self._cache_read_buffer_and_tensor_map.items():
            insn = PoolingGradConstants.DMA_COPY
            self._emit_insn_map[_cache_read_buffer] = [
                _cache_read_buffer.op.axis[emit_insn_index], insn, {"pad_value": 0}]
        # cache_transpose tensors
        for _tensor in self._cache_transpose_tensors:
            three_dim_in_order = (1, 2, 0)
            three_dim_in_ord = tvm.call_intrin(
                'handle', 'tir.tvm_tuple', *tuple(three_dim_in_order))
            insn = "vector_transpose"
            self._emit_insn_map[_tensor] = [_tensor.op.axis[emit_insn_index], insn, {
                "src_in_dst_order": three_dim_in_ord, "is_trans_align": 1, "enable_vnchwconv_b32": transpose_var}]
        # mid tensors
        for _tensor in self._reorder_cache_write_tensors:
            insn = get_insn(_tensor)
            if PoolingGradConstants.PAD_NODE in _tensor.op.name:
                self._emit_insn_map[_tensor] = [
                    _tensor.op.axis[emit_insn_index], insn, {"pad_value": -1}]
            elif _tensor.op.tag == PoolingGradConstants.REDUCE_SUM_TAG:
                reduce_attrs = {
                    "reduce_mode": "conditional_reduce", "analysis_repeat_overlap": 1}
                if self._ub_split_result.get("index") in [PGSchConstant.KH_IDX, PGSchConstant.KW_IDX]:
                    self._emit_insn_map[_tensor] = [
                        self._ub_split_result.get("inner_itervar"), insn, reduce_attrs]
                elif self._ub_split_result.get("index") in [PGSchConstant.HO_IDX, PGSchConstant.NC_IDX]:
                    self._emit_insn_map[_tensor] = [
                        _tensor.op.reduce_axis[0], insn, reduce_attrs]
                else:
                    self._emit_insn_map[_tensor] = [
                        _tensor.op.reduce_axis[1], insn, reduce_attrs]
            elif _tensor.op.tag == PoolingGradConstants.IMG2COL_TAG:
                self._emit_insn_map[_tensor] = [
                    _tensor.op.axis[emit_insn_index], insn, {"auto_movedown_pragma": 1}]
            else:
                self._emit_insn_map[_tensor] = [
                    _tensor.op.axis[emit_insn_index], insn]
        # cache_write_tensor
        for _cache_write_buffer, _tensor in self._cache_write_buffer_and_tensor_map.items():
            three_dim_in_order_out = (2, 0, 1)
            three_dim_in_ord_out = tvm.call_intrin(
                'handle', 'tir.tvm_tuple', *tuple(three_dim_in_order_out))
            insn = "vector_transpose"
            self._emit_insn_map[_cache_write_buffer] = [_cache_write_buffer.op.axis[emit_insn_index], insn, {
                "src_in_dst_order": three_dim_in_ord_out, "is_trans_align": 1, "enable_vnchwconv_b32": transpose_var}]
            self._emit_insn_map[_tensor] = [
                _tensor.op.axis[emit_insn_index], get_insn(_tensor)]
        # indices node
        self._emit_insn_map[self._indices_node] = [
            self._indices_node.op.axis[emit_insn_index], PoolingGradConstants.INDICES_SEQUENCE]
        # fakenode
        self._emit_insn_map[self._res_tensor] = [
            self._res_tensor.op.axis[-1], PoolingGradConstants.PHONY_INSN]

    def _do_pragma(self):
        for _tensor in self._cache_transpose_tensors \
                .union(self._reorder_cache_write_tensors) - self._cache_write_buffer_and_tensor_map.keys():
            if _tensor.op.tag == PoolingGradConstants.VCMP_TAG:
                id_val = tvm.call_extern("int32", "axis_group", 0, "overwrite")
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-1], "axis_group", id_val)
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-2], "axis_group", id_val)
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-3], "axis_group", id_val)
            if _tensor.op.tag == PoolingGradConstants.VCMP_SEL_TAG:
                id_val = tvm.call_extern("int32", "axis_group", 1, "overwrite")
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-1], "axis_group", id_val)
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-2], "axis_group", id_val)
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-3], "axis_group", id_val)

    def _do_pragma_dynamic(self):
        for _tensor in self._cache_write_buffer_and_tensor_map.keys():
            if PoolingGradConstants.DEPAD_NODE in _tensor.op.name:
                id_val = tvm.call_extern("int32", "axis_group", 2, "overwrite")
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-1], "axis_group", id_val)
                self._sch[_tensor].pragma(
                    _tensor.op.axis[-2], "axis_group", id_val)
                
    def _do_set_constraint(self):
        if self._tiling_case.ub_split_axis_index == PGSchConstant.NC_IDX:
            for _tensor in self._mid_tensor_set:
                if _tensor.op.tag == PoolingGradConstants.INDICES_TAG:
                    self._sch[_tensor].set_store_predicate(self._ub_split_result.get("outer_itervar").var == 0)
            for _tensor in self._cache_transpose_tensors:
                if PoolingGradConstants.INDICES_T_TAG in _tensor.op.name:
                    self._sch[_tensor].set_store_predicate(self._ub_split_result.get("outer_itervar").var == 0)
                    self._sch[_tensor].mem_unique()

    def _do_buffer_unreuse(self):
        for _cache_tensor in self._reorder_cache_write_tensors:
            if PoolingGradConstants.PAD_NODE in _cache_tensor.op.name:
                buffer_unreuse = _cache_tensor
        for _tensor in self._reorder_cache_write_tensors:
            if _tensor.op.tag == PoolingGradConstants.IMG2COL_TAG:
                self._sch[buffer_unreuse].unreused_by(_tensor)