#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
scatter schedule base
"""
from abc import ABC
from typing import Any

from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform.platform_info import SHORT_SOC_VERSION
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import add_build_arg

from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import ScatterPattern
from ...constants import Pattern
from ...schedule import Schedule
from .scatter_tilingcase import TilingStrategy
from .scatter_tilingcase import ScatterCompileInfo
from .scatter_tilingcase import TilingThreshold
from .scatter_tilingcase import SPECIAL_PATTERN_THRESHOLD

DEFAULT = "default"

# block size in D architecture
BLOCK_SIZE_BYTE = 32

# STORE AREA
VAR_STORE_GM = 0
VAR_STORE_UB = 1
# Represents the axis that does not need to be split
DUMMY_DIM = -10
ATOMIC_PATTERN = 1
ATOMIC_PATTERN_WITH_PAD = 2
BASE_PATTERN = 5
BASE_PATTERN_WITH_PAD = 6
BASE_KEY = 880000000


# 'pylint: disable=R0902, R0903
class ScatterBaseSchedule(ABC):
    """
    scatter schedule
    """

    def __init__(self, outs, tiling_case):
        self._out_tensor = outs[0]
        self._schedule = None
        self._tiling_case = tiling_case
        self._tiling_strategy = self._tiling_case.get("tiling_strategy")
        self._tiling_key = self._tiling_case.get("key")
        self._op_type = self._tiling_case.get("op_type")

        self._special_pattern = (self._tiling_key % 10000) // 1000

        # scatter axis (real axis)
        self._scatter_axis = self._tiling_case.get("scatter_axis", 0)

        self._is_need_storage_align = self._tiling_case.get("is_need_storage_align", False)
        self._is_need_align_pad = self._tiling_case.get("is_need_align_pad", False)
        self._is_need_remove_pad = self._tiling_case.get("is_need_remove_pad", False)
        self._is_sparse_cache = self._tiling_case.get("is_sparse_cache", False)

        self._store_area = self._tiling_case.get("store_area", VAR_STORE_GM)
        self._is_atomic = self._tiling_case.get("is_atomic", False)

        # DB
        self._is_db = self._tiling_case.get("is_db", False)

        self._scatter_compute_type = 0
        self._scope = "local.UB"

        self._input_tensors = set()
        self._var_gm_tensor = None
        self._indices_gm_tensor = None
        self._update_gm_tensor = None

        self._var_name = None
        self._indices_name = None
        self._update_name = None

        self._max_dtype_bytes = 4
        self._coexisting_quantity = 1
        self._tensor_space = None
        self._ub_size = util.get_ub_size() - 32
        self._var_ub_size = self._ub_size // 2
        self._var_dtype_size = 4
        self._indices_dtype_size = 4
        self._update_dtype_size = 4

        # input -> outputs mapping relations
        self._cache_write_tensor = None
        self._in_out_map = {}
        self._reorder_map = {}
        self._compute_at_map = {}
        self._emit_insn_map = {}

        # const tiling
        self._const_block_axis = -1
        self._const_ub_norm_axis = 0
        self._const_ub_sparse_axis = 0
        self._const_block_factor = 1
        self._const_ub_norm_factor = 1
        self._const_ub_sparse_factor = 1

        self._indices_storage_bound = 0
        self._scatter_storage_bound = 0
        self._update_storage_bound = 0

        self._fake_schedule = False

        self._indices_ub = None
        self._var_ub = None
        self._update_ub = None
        self._scatter_ub = None
        self._var_align_pad_ub = None
        self._update_align_pad_ub = None
        self._remove_pad_ub = None

        self._align_factor = None

        self._block_tiling_vars = {}
        self._ub_norm_tiling_vars = {}
        self._ub_sparse_tiling_vars = {}
        self._block_bind_axis = None
        self._compute_at_norm_axis = None
        self._compute_at_sparse_axis = None
        self._compute_at_sparse_axis_out = None
        self._compute_at_sparse_axis_in = None
        self._emit_axis = None
        self._emit_scatter_axis = None
        self._reorder_axis = []
        self._is_split_last_axis = False
        self._block = None
        self.block_factor = None

    def do_schedule(self):
        """
        schedule body
        :return:
        """
        self._construct_compute_graph()

        self._schedule = tvm.create_schedule(self._out_tensor.op)
        self._schedule.tiling_key = self._tiling_key

        self._cal_cache_read()
        self._do_cache_read()

        self._cal_cache_write()
        self._do_cache_write()

        self._cal_storage_bound()
        self._do_storage_bound()

        self._cal_mem_use()
        self._do_mem_use()

        self._calc_tiling()

        if self._fake_schedule:
            return None

        self._do_tiling()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_reorder()
        self._do_reorder()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_set_store_predicate()

        self._calc_double_buffer()
        self._do_double_buffer()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_group_axis()

        self._add_compile_info()

        # to remove redundant pipev
        if self._tiling_strategy == TilingStrategy.STATIC:
            if self._special_pattern in [ATOMIC_PATTERN, ATOMIC_PATTERN_WITH_PAD]:
                add_build_arg("enable_loop_partition", True)

        return self._schedule

    def _is_scatter_update(self):
        return self._op_type in ["scatter_update", "scatter_nd_update"] and \
               self._special_pattern in [BASE_PATTERN, BASE_PATTERN_WITH_PAD]

    def _is_scatter_nd_op(self):
        return self._op_type == "scatter_nd"

    def _is_scatter_update_high_perf(self):
        impl_mode = operation.get_compile_info().get("impl_mode")
        return self._op_type in ["scatter_update", "scatter_nd_update"] and impl_mode == "high_performance"

    def _construct_compute_graph(self):
        visited_tensors = set()
        self.__dfs_sub_graph(self._out_tensor, visited_tensors)

        for one_input_tensor in self._input_tensors:
            if one_input_tensor.name == self._var_name:
                self._var_gm_tensor = one_input_tensor
            elif one_input_tensor.name == self._indices_name:
                self._indices_gm_tensor = one_input_tensor
            elif one_input_tensor.name == self._update_name:
                self._update_gm_tensor = one_input_tensor

        self._var_dtype_size = DTYPE_BYTE_MAPPING.get(self._var_gm_tensor.dtype)
        self._indices_dtype_size = DTYPE_BYTE_MAPPING.get(self._indices_gm_tensor.dtype)
        self._update_dtype_size = DTYPE_BYTE_MAPPING.get(self._update_gm_tensor.dtype)

    def _cal_cache_read(self):
        pass

    def _do_cache_read(self):
        self._indices_ub = self._schedule.cache_read(self._indices_gm_tensor, self._scope,
                                                     self._in_out_map.get(self._indices_gm_tensor))
        self._update_ub = self._schedule.cache_read(self._update_gm_tensor, self._scope,
                                                    self._in_out_map.get(self._update_gm_tensor))

        if self._store_area == VAR_STORE_UB:
            if not self._is_scatter_update() and not self._is_scatter_nd_op():
                self._var_ub = self._schedule.cache_read(self._var_gm_tensor, self._scope,
                                                         self._in_out_map.get(self._var_gm_tensor))
                if not self._is_sparse_cache and self._is_need_align_pad:
                    self._var_align_pad_ub = self._schedule.cache_read(self._var_ub, self._scope,
                                                                       self._in_out_map.get(self._var_gm_tensor))

        if self._is_need_align_pad:
            self._update_align_pad_ub = self._schedule.cache_read(self._update_ub, self._scope,
                                                                  self._in_out_map.get(self._update_gm_tensor))

        if self._is_scatter_nd_op():
            self._schedule[self._var_gm_tensor].set_scope("global")
            self._schedule[self._out_tensor].set_scope("global")

    def _cal_cache_write(self):
        self._cache_write_tensor = self._out_tensor

    def _do_cache_write(self):
        if self._is_need_remove_pad and not self._is_scatter_nd_op() and not self._is_scatter_update():
            self._remove_pad_ub = self._schedule.cache_write(self._cache_write_tensor, self._scope)
            self._scatter_ub = self._schedule.cache_write(self._remove_pad_ub, self._scope)
        elif not self._is_atomic and not self._is_scatter_nd_op() and not self._is_scatter_update():
            self._scatter_ub = self._schedule.cache_write(self._cache_write_tensor, self._scope)

    def _cal_storage_bound(self):
        _coexisting_quantity_var = 1 if self._store_area == VAR_STORE_UB else 0
        _coexisting_quantity_indices = 1
        _coexisting_quantity_update = 1
        _coexisting_quantity_scatter = 1 if not self._is_atomic and not self._is_scatter_update() else 0

        _coexisting_quantity_align_pad = 4 if self._is_need_align_pad else 1
        _coexisting_quantity_remove_pad = 4 if self._is_need_align_pad else 1
        self._coexisting_quantity = (_coexisting_quantity_var * _coexisting_quantity_align_pad +
                                     _coexisting_quantity_indices +
                                     _coexisting_quantity_update * _coexisting_quantity_align_pad +
                                     _coexisting_quantity_scatter * _coexisting_quantity_remove_pad)

    def _do_storage_bound(self):
        self.tensor_space = self._ub_size // self._coexisting_quantity // BLOCK_SIZE_BYTE * BLOCK_SIZE_BYTE
        if self._is_db:
            self.tensor_space = self.tensor_space // 2

        self._indices_storage_bound = int(self.tensor_space // self._indices_dtype_size)
        self._schedule[self._indices_ub].set_buffer_size(self._indices_storage_bound)

        self._scatter_storage_bound = int(self.tensor_space // self._var_dtype_size)
        self._update_storage_bound = int(self.tensor_space // self._update_dtype_size)
        self._schedule[self._update_ub].set_buffer_size(self._update_storage_bound)
        if self._store_area == VAR_STORE_UB and not self._is_scatter_update() and not self._is_scatter_nd_op():
            self._schedule[self._var_ub].set_buffer_size(self._scatter_storage_bound)
            if not self._is_sparse_cache and self._is_need_align_pad:
                self._schedule[self._var_align_pad_ub].set_buffer_size(self._scatter_storage_bound)

        if self._is_need_align_pad:
            self._schedule[self._update_align_pad_ub].set_buffer_size(self._update_storage_bound)
        if self._is_need_remove_pad:
            self._schedule[self._remove_pad_ub].set_buffer_size(self._scatter_storage_bound)
        if not self._is_atomic and not self._is_scatter_nd_op() and not self._is_scatter_update():
            self._schedule[self._scatter_ub].set_buffer_size(self._scatter_storage_bound)

    def _cal_mem_use(self):
        pass

    def _do_mem_use(self):
        if self._is_scatter_nd_op():
            self._schedule[self._out_tensor].reused_by(self._var_gm_tensor)
        elif not self._is_atomic and not self._is_scatter_update():
            if not self._is_sparse_cache and self._is_need_align_pad:
                self._schedule[self._var_align_pad_ub].reused_by(self._scatter_ub)
                self._schedule[self._scatter_ub].reused_by(reuse_data=True)
            else:
                self._schedule[self._var_ub].reused_by(self._scatter_ub)

            # Solve the problem of var_ub being incorrectly reused by update_ub in some cases
            if self._op_type in ["scatter_update", "scatter_nd_update"]:
                self._schedule[self._var_ub].unreused_by(self._update_ub)
        else:
            self._schedule[self._var_gm_tensor].reused_by(self._out_tensor)

    def _calc_tiling(self):
        if self._tiling_key != BASE_KEY:
            funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                     TilingStrategy.STATIC: self._calc_tiling_static}

            funcs.get(self._tiling_strategy)()
        else:
            pass

    def _calc_tiling_dynamic(self):
        output_shape = util.shape_to_list(self._out_tensor.shape)
        indices_shape = util.shape_to_list(self._indices_gm_tensor.shape)

        b_idx = self._tiling_case["block_tiling_norm_axis"]
        u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
        u_idx_sparse = self._tiling_case["ub_tiling_sparse_axis"]

        b_bound = (1, util.get_bound(output_shape[b_idx])[1])
        self._block_tiling_vars[b_idx] = operation.var_inner("_block_factor_" + str(b_idx),
                                                             b_bound, "int64")

        if u_idx_norm is not None:
            u_bound = (1, util.get_bound(output_shape[u_idx_norm])[1])
            self._ub_norm_tiling_vars[u_idx_norm] = operation.var_inner("_ub_norm_factor_" + str(u_idx_norm),
                                                                        u_bound, "int64")

        if u_idx_sparse is not None:
            u_bound = (1, util.get_bound(indices_shape[u_idx_sparse])[1])
            self._ub_sparse_tiling_vars[u_idx_sparse] = operation.var_inner("_ub_sparse_factor_" + str(u_idx_sparse),
                                                                            u_bound, "int64")

    def _calc_tiling_static(self):
        tmp_output_shape = tuple(i.value for i in self._out_tensor.shape)
        outputs = [{"shape": tmp_output_shape, "dtype": self._out_tensor.dtype}]

        tmp_var_shape = tuple(i.value for i in self._var_gm_tensor.shape)
        tmp_indices_shape = tuple(i.value for i in self._indices_gm_tensor.shape)
        tmp_update_shape = tuple(i.value for i in self._update_gm_tensor.shape)

        if self._is_scatter_nd_op():
            inputs = [{"shape": tmp_indices_shape, "dtype": self._indices_gm_tensor.dtype},
                      {"shape": tmp_update_shape, "dtype": self._update_gm_tensor.dtype},
                      {"shape": tmp_var_shape, "dtype": self._var_gm_tensor.dtype}]
        else:
            inputs = [{"shape": tmp_var_shape, "dtype": self._var_gm_tensor.dtype},
                      {"shape": tmp_indices_shape, "dtype": self._indices_gm_tensor.dtype},
                      {"shape": tmp_update_shape, "dtype": self._update_gm_tensor.dtype}]

        base_info = [util.get_core_num(), self._ub_size, self._scatter_compute_type,
                     self._var_dtype_size, self._indices_dtype_size, self._update_dtype_size]

        tensor_sizes = {self._special_pattern: [self._scatter_storage_bound,
                                                self._indices_storage_bound,
                                                self._update_storage_bound]}
        _scatter_nd_shape = {}
        _scatter_nd_shape["0"] = operation.get_context().get(ScatterCompileInfo.SCATTER_ND_SHAPE)
        if self._special_pattern != ScatterCompileInfo.BASE_SCHEDULE_PATTERN:
            tensor_sizes[ScatterCompileInfo.BASE_SCHEDULE_PATTERN] = [self._scatter_storage_bound,
                                                                      self._indices_storage_bound,
                                                                      self._update_storage_bound]
        if self._is_scatter_update_high_perf():
            global SPECIAL_PATTERN_THRESHOLD
            SPECIAL_PATTERN_THRESHOLD = 5

        const_compile_info = {
            CompileInfo.BASE_INFO: base_info,
            ScatterCompileInfo.CUSTOM_INFO: [int(self._is_atomic),
                                             SPECIAL_PATTERN_THRESHOLD,
                                             TilingThreshold.VAR_CACHE_THRESHOLD,
                                             TilingThreshold.TILING_TAIL_AXIS_THRESHOLD,
                                             int(self._is_scatter_nd_op())],
            ScatterCompileInfo.CONST_AXIS: self._scatter_axis,
            ScatterCompileInfo.TENSOR_SIZES: tensor_sizes,
            ScatterCompileInfo.SCATTER_ND_SHAPE: _scatter_nd_shape,
            ScatterCompileInfo.SOC_VERSION: get_soc_spec(SHORT_SOC_VERSION)
        }
        const_compile_info.update(get_compile_info())

        run_info = op_tiling.do_op_tiling("AutoTiling", const_compile_info, inputs, outputs)
        tiling_format = {
            "tiling_key": "int64",
            "block_axis": "int64",
            "block_factor": "int64",
            "ub_norm_axis": "int64",
            "ub_norm_factor": "int64",
            "ub_sparse_axis": "int64",
            "ub_sparse_factor": "int64"}

        tiling_data = op_tiling.decode(run_info["tiling_data"], tiling_format)
        const_tiling_key = tiling_data.get("tiling_key")
        self._const_block_axis = tiling_data.get("block_axis")
        self._const_block_factor = tiling_data.get("block_factor")
        self._const_ub_norm_axis = tiling_data.get("ub_norm_axis")
        self._const_ub_norm_factor = tiling_data.get("ub_norm_factor")
        self._const_ub_sparse_axis = tiling_data.get("ub_sparse_axis")
        self._const_ub_sparse_factor = tiling_data.get("ub_sparse_factor")

        if operation.get_context().get(ScatterCompileInfo.STATIC_SUCCESS) or const_tiling_key != self._tiling_key:
            operation.get_context().get_current_compute().get_current_schedule().add(ScatterCompileInfo.FAKE_SCHEDULE,
                                                                                     True)
            self._fake_schedule = True
        else:
            operation.get_context().add(ScatterCompileInfo.STATIC_SUCCESS, True)
            if self._store_area == 1:
                operation.get_context().add(ScatterCompileInfo.STATIC_CLOSE_PASS, False)
            else:
                operation.get_context().add(ScatterCompileInfo.STATIC_CLOSE_PASS, True)

    def _do_tiling(self):
        if self._tiling_strategy == TilingStrategy.DYNAMIC:
            b_idx = self._tiling_case["block_tiling_norm_axis"]
            u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
            u_idx_sparse = self._tiling_case["ub_tiling_sparse_axis"]

            self.block_factor = self._block_tiling_vars.get(b_idx)
            ub_norm_factor = self._ub_norm_tiling_vars.get(u_idx_norm)
            ub_sparse_factor = self._ub_sparse_tiling_vars.get(u_idx_sparse)
        else:
            b_idx = self._const_block_axis
            u_idx_norm = self._const_ub_norm_axis if self._const_ub_norm_axis != DUMMY_DIM else None
            u_idx_sparse = self._const_ub_sparse_axis

            self.block_factor = self._const_block_factor
            ub_norm_factor = self._const_ub_norm_factor
            ub_sparse_factor = self._const_ub_sparse_factor

        b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[b_idx],
                                                          factor=self.block_factor)

        if u_idx_norm is not None:
            self._is_split_last_axis = u_idx_norm == (len(self._out_tensor.shape) - 1)
            if u_idx_norm == b_idx:
                u_norm_o, u_norm_i = self._schedule[self._out_tensor].split(b_i,
                                                                            factor=ub_norm_factor)
                for i in range(len(self._out_tensor.op.axis)):
                    if i == b_idx:
                        self._reorder_axis.extend([b_o, u_norm_o, u_norm_i])
                    else:
                        self._reorder_axis.append(self._out_tensor.op.axis[i])
            else:
                u_norm_o, u_norm_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[u_idx_norm],
                                                                            factor=ub_norm_factor)
                for i in range(len(self._out_tensor.op.axis)):
                    if i == b_idx:
                        self._reorder_axis.append(b_o)
                    elif i == u_idx_norm:
                        self._reorder_axis.extend([u_norm_o, u_norm_i])
                    else:
                        self._reorder_axis.append(self._out_tensor.op.axis[i])
                self._reorder_axis.append(b_i)

            if self._is_split_last_axis:
                self._compute_at_sparse_axis_out = u_norm_o
                self._compute_at_sparse_axis_in = u_norm_o

        if u_idx_sparse is not None:
            u_sparse_o, u_sparse_i = self._schedule[self._out_tensor].split(
                self._out_tensor.op.sparse_axis[u_idx_sparse], factor=ub_sparse_factor)
            self._reorder_axis.insert(0, u_sparse_i)
            self._reorder_axis.insert(0, u_sparse_o)

        if u_idx_norm is None:
            for i in range(len(self._out_tensor.op.axis)):
                if i == b_idx:
                    self._reorder_axis.extend([b_o, b_i])
                else:
                    self._reorder_axis.append(self._out_tensor.op.axis[i])

        self._block_bind_axis = b_o
        self._compute_at_norm_axis = u_norm_o if u_idx_norm is not None else b_i
        if not self._is_split_last_axis:
            self._compute_at_sparse_axis_out = u_sparse_o if u_idx_sparse is not None else \
                self._out_tensor.op.sparse_axis[0]
            self._compute_at_sparse_axis_in = u_sparse_i if u_idx_sparse is not None else \
                self._out_tensor.op.sparse_axis[0]
        self._emit_axis = u_idx_sparse if u_idx_sparse is not None else -1
        if u_idx_norm is not None:
            self._emit_scatter_axis = u_norm_i
        else:
            self._emit_scatter_axis = b_i if b_idx == (len(self._out_tensor.shape) - 1) else \
                self._out_tensor.op.axis[-1]

    def _calc_multi_core(self):
        pass

    def _do_multi_core(self):
        if self._block_bind_axis is not None:
            self._block = tvm.thread_axis("blockIdx.x")
            self._schedule[self._out_tensor].bind(self._block_bind_axis, self._block)

    def _calc_reorder(self):
        if not self._is_atomic and not self._is_sparse_cache and not self._is_scatter_nd_op() \
                and not self._is_scatter_update():
            self._reorder_map[self._scatter_ub] = self._reorder_axis
        else:
            self._reorder_map[self._out_tensor] = self._reorder_axis

    def _do_reorder(self):
        for single_tensor, param in self._reorder_map.items():
            self._schedule[single_tensor].reorder(*param)

    def _calc_storage_align(self):
        self._align_factor = BLOCK_SIZE_BYTE // self._var_dtype_size

    def _do_storage_align(self):
        if self._is_need_storage_align and not self._is_need_align_pad:
            self._schedule[self._update_ub].storage_align(self._update_ub.op.axis[-2],
                                                          self._align_factor, 0)
        if self._is_need_align_pad:
            self._schedule[self._update_align_pad_ub].storage_align(self._update_align_pad_ub.op.axis[-2],
                                                                    self._align_factor, 0)

    def _calc_compute_at(self):
        self._compute_at_map[self._indices_ub] = [self._out_tensor, self._compute_at_sparse_axis_out]
        self._compute_at_map[self._update_ub] = [self._out_tensor, self._compute_at_sparse_axis_out]
        if not self._is_scatter_update() and not self._is_scatter_nd_op():
            self._compute_at_map[self._var_ub] = [self._out_tensor, self._compute_at_sparse_axis_in]

        if self._is_need_align_pad:
            self._compute_at_map[self._update_align_pad_ub] = [self._out_tensor, self._compute_at_sparse_axis_out]

        if not self._is_scatter_nd_op() and not self._is_scatter_update():
            self._compute_at_map[self._scatter_ub] = [self._out_tensor, self._compute_at_sparse_axis_in]

    def _do_compute_at(self):
        for tensor_i, param in self._compute_at_map.items():
            self._schedule[tensor_i].compute_at(self._schedule[param[0]], param[1])

    def _do_set_store_predicate(self):
        pass

    def _calc_double_buffer(self):
        pass

    def _do_double_buffer(self):
        pass

    def _calc_emit_insn(self):
        self._emit_insn_map[self._indices_ub] = [self._indices_ub.op.axis[0], "dma_copy"]
        self._emit_insn_map[self._update_ub] = [self._update_ub.op.axis[self._emit_axis], "dma_copy"]
        if self._is_need_align_pad:
            self._emit_insn_map[self._update_align_pad_ub] = [self._update_align_pad_ub.op.axis[self._emit_axis],
                                                              "align_pad"]

        dma_copy_attr = "process_data_smaller_than_one_block_by_calcute_index"
        if self._is_scatter_nd_op():
            self._emit_insn_map[self._var_gm_tensor] = [self._var_gm_tensor.op.axis[0], "phony_insn"]
            self._emit_insn_map[self._out_tensor] = [self._emit_scatter_axis, "dma_copy",
                                                     {"segment_atomic": 1}]
        elif self._is_scatter_update():
            self._emit_insn_map[self._out_tensor] = [self._emit_scatter_axis, "dma_copy",
                                                     {"no_overlap": dma_copy_attr}]
        else:
            self._emit_insn_map[self._var_ub] = [self._var_ub.op.axis[-1], "dma_copy"]
            self._emit_insn_map[self._scatter_ub] = [self._scatter_ub.op.axis[-1], "vector_auto"]
            self._emit_insn_map[self._out_tensor] = [self._emit_scatter_axis, "dma_copy",
                                                     {"no_overlap": dma_copy_attr}]
        self._schedule[self._out_tensor].pragma(self._emit_scatter_axis, "loop_with_no_overlap_tensor",
                                                pragma_tensor=self._out_tensor)

    def _do_emit_insn(self):
        for tensor_i, param in self._emit_insn_map.items():
            self._schedule[tensor_i].emit_insn(*param)

    def _do_group_axis(self):
        if not self._is_need_storage_align and not self._is_need_remove_pad and not self._is_need_align_pad:
            group_id = tvm.call_extern("int32", "axis_group", 0, "overwrite")
            self._schedule[self._update_ub].pragma(
                self._schedule[self._update_ub].op.axis[0], "axis_group", group_id)
            self._schedule[self._update_ub].pragma(
                self._schedule[self._update_ub].op.axis[1], "axis_group", group_id)

    def _add_compile_info(self):
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()

        cpt_schedule.add(ScatterCompileInfo.FAKE_SCHEDULE, False)

        # BASE INFO
        cpt_schedule.add(CompileInfo.CORE_NUM, util.get_core_num())
        cpt_schedule.add(CompileInfo.UB_SIZE, self._ub_size)
        cpt_schedule.add(ScatterCompileInfo.SCATTER_TYPE, self._scatter_compute_type)
        cpt_schedule.add(ScatterCompileInfo.VAR_DTYPE_SIZE, self._var_dtype_size)
        cpt_schedule.add(ScatterCompileInfo.INDICES_DTYPE_SIZE, self._indices_dtype_size)
        cpt_schedule.add(ScatterCompileInfo.UPDATE_DTYPE_SIZE, self._update_dtype_size)

        # CUSTOM INFO
        cpt_schedule.add(ScatterCompileInfo.IS_SUPPORT_ATOMIC, int(self._is_atomic))
        cpt_schedule.add(ScatterCompileInfo.SPECIAL_PATTERN, self._special_pattern)
        cpt_schedule.add(ScatterCompileInfo.VAR_NUM, self._scatter_storage_bound)
        cpt_schedule.add(ScatterCompileInfo.INDICES_NUM, self._indices_storage_bound)
        cpt_schedule.add(ScatterCompileInfo.UPDATE_NUM, self._update_storage_bound)
        cpt_schedule.add(ScatterCompileInfo.OP_TYPE, int(self._is_scatter_nd_op()))

    def __dfs_sub_graph(self, out, visited_tensors: set):
        if len(out.op.attrs) > 0:
            _scatter_mode = operation.get_context().get("_scatter_mode")
            self._scatter_compute_type = 0 if _scatter_mode == "scatter" else 1
            if _scatter_mode in ["scatter", "scatter_nd"]:
                if "var_name" in out.op.attrs:
                    self._var_name = out.op.attrs["var_name"]

                if "indices_name" in out.op.attrs:
                    self._indices_name = out.op.attrs["indices_name"]

                if "update_name" in out.op.attrs:
                    self._update_name = out.op.attrs["update_name"]

        for tensor_i in out.op.input_tensors:
            util.merge_value(self._in_out_map, tensor_i, out)
            self._max_dtype_bytes = max(self._max_dtype_bytes, DTYPE_BYTE_MAPPING.get(tensor_i.dtype))

            if util.is_placeholder(tensor_i):
                self._input_tensors.add(tensor_i)

            if self._is_scatter_nd_op() and tensor_i.name == self._var_name:
                self._input_tensors.add(tensor_i)

            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self.__dfs_sub_graph(tensor_i, visited_tensors)
