#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
insn info
"""
from typing import Type

from . import soc_info
from . import stage_info
from . import tensor_info
from .. import constants
from .. import helper
from ..... import constants as unify_constants
from ..... import util as unify_helper


def _get_strategy(strategy_funcs_map, preferred_strategy, rejected_strategy_list):
    if preferred_strategy is not None:
        if strategy_funcs_map.get(preferred_strategy)[0]():
            return preferred_strategy

    res_strategy = None
    for strategy, funcs_list in strategy_funcs_map.items():
        if strategy not in rejected_strategy_list and funcs_list[0]() and funcs_list[1]():
            res_strategy = strategy
            break

    helper.check_true(res_strategy is not None,
                      "func of get_strategy does not get an appropriate strategy, please check.")

    return res_strategy


class InsnBase:
    def __init__(self, tensor_obj, stage_obj):
        self._tensor_obj: Type[tensor_info.TensorBase] = tensor_obj
        self._stage_obj: Type[stage_info.StageBase] = stage_obj
        self._extra_temp_size = 0
        self._extra_node_num = 0
        self._extra_node_size = 0
        self._insn_axis = self._init_insn_axis()
        self._insn_name, self._dsl_insn_name = self._init_insn_name()
        self._insn_attrs = {}
        self._insn_strategy = None
        self._insn_sub_type = None
        self._buffer_ele_num = 0
        self._extra_node_ele_num = 0
        self._preferred_strategy = None
        self._rejected_strategy_list = []

    @property
    def extra_temp_size(self):
        return self._extra_temp_size

    @property
    def extra_node_num(self):
        return self._extra_node_num

    @property
    def extra_node_size(self):
        # node size should be block align and some instructions required special node size
        return self._extra_node_size

    @extra_node_size.setter
    def extra_node_size(self, value):
        self._extra_node_size = value

    @property
    def insn_axis(self):
        return self._insn_axis

    @insn_axis.setter
    def insn_axis(self, value):
        self._insn_axis = value

    @property
    def dsl_insn_name(self):
        return self._dsl_insn_name

    @property
    def insn_name(self):
        return self._insn_name

    @insn_name.setter
    def insn_name(self, value):
        self._insn_name = value

    @property
    def insn_attrs(self):
        return self._insn_attrs

    @property
    def insn_sub_type(self):
        return self._insn_sub_type

    @property
    def buffer_ele_num(self):
        return self._buffer_ele_num

    @buffer_ele_num.setter
    def buffer_ele_num(self, value):
        self._buffer_ele_num = value

    @property
    def extra_node_ele_num(self):
        return self._extra_node_ele_num

    @extra_node_ele_num.setter
    def extra_node_ele_num(self, value):
        self._extra_node_ele_num = value

    @property
    def insn_strategy(self):
        return self._insn_strategy

    @property
    def preferred_strategy(self):
        return self._preferred_strategy

    @preferred_strategy.setter
    def preferred_strategy(self, value):
        self._preferred_strategy = value

    @property
    def rejected_strategy_list(self):
        return self._rejected_strategy_list

    @rejected_strategy_list.setter
    def rejected_strategy_list(self, value):
        self._rejected_strategy_list = value

    def is_can_not_reuse_src_insn(self):
        if self._dsl_insn_name in unify_constants.DST_SRC_NO_REUSE_SET:
            return True

        if self._dsl_insn_name in unify_constants.BROADCAST_INSNS or \
                self._dsl_insn_name in unify_constants.REDUCE_INSNS:
            return True

        if self._stage_obj.compute_inlined_stage_obj is not None:
            return True

        return False

    def set_all_insn_info(self):
        self.set_insn_sub_type()
        self.set_insn_strategy()
        self.set_insn_attrs()
        self.set_extra_size_and_coexist_node()

    def set_insn_sub_type(self):
        self._insn_sub_type = constants.INSN_NAME_AND_SUB_TYPE_MAP.get(self._insn_name)

    def set_insn_strategy(self):
        pass

    def set_insn_attrs(self):
        pass

    def set_extra_size_and_coexist_node(self):
        pass

    def _init_insn_axis(self):
        if hasattr(self._stage_obj.tvm_stage, "op") and hasattr(self._stage_obj.tvm_stage.op, "axis"):
            return self._stage_obj.tvm_stage.op.axis[0]

        return None

    def _init_insn_name(self):
        insn_name = unify_helper.get_dsl_insn(self._stage_obj.tvm_stage)
        dsl_insn_name = insn_name
        if insn_name is None:
            return constants.SpecialInsnName.PHONY_INSN, dsl_insn_name

        return unify_constants.INSN_MAPPING.get(insn_name, insn_name), dsl_insn_name


class BroadcastInsn(InsnBase):
    def __init__(self, tensor_obj, stage_obj):
        InsnBase.__init__(self, tensor_obj, stage_obj)

    def set_insn_strategy(self):
        strategy_and_funcs_map = {
            constants.BroadcastStrategy.ENABLE_VBRCB:
                [self._is_support_vbrcb, self._is_appropriate_for_vbrcb],
            constants.BroadcastStrategy.ENABLE_VNCHWCONV:
                [self._is_support_vnchwconv, self._is_appropriate_for_vnchwconv],
            constants.BroadcastStrategy.ENABLE_SCALAR_EIGHT_DUP:
                [self._is_support_scalar_eight_dup, self._is_appropriate_for_scalar_eight_dup],
            constants.BroadcastStrategy.COMMON:
                [self._is_support_common, self._is_appropriate_for_common]
        }
        self._insn_strategy = _get_strategy(strategy_and_funcs_map, self._preferred_strategy,
                                            self._rejected_strategy_list)

    def set_insn_attrs(self):
        strategy_and_attrs_map = {
            constants.BroadcastStrategy.ENABLE_VBRCB:
                {constants.InsnAttrsKey.EBABLE_BRCB: True},
            constants.BroadcastStrategy.ENABLE_VNCHWCONV:
                {constants.InsnAttrsKey.ENABLE_VNCHWCONV: True,
                 constants.InsnAttrsKey.B32_TRANS_THRESHOLD: constants.InsnAttrsValue.B32_TRANS_THRESHOLD_VALUE},
            constants.BroadcastStrategy.ENABLE_SCALAR_EIGHT_DUP:
                {constants.InsnAttrsKey.ENABLE_ALIGN_EIGHT_DUP: True},
            constants.BroadcastStrategy.COMMON:
                {}
        }
        self._insn_attrs = strategy_and_attrs_map.get(self._insn_strategy)

    def set_extra_size_and_coexist_node(self):
        strategy_and_funcs_map = {
            constants.BroadcastStrategy.ENABLE_VBRCB:
                self._extra_size_and_coexist_node_for_others,
            constants.BroadcastStrategy.ENABLE_VNCHWCONV:
                self._extra_size_and_coexist_node_for_vnchwconv,
            constants.BroadcastStrategy.ENABLE_SCALAR_EIGHT_DUP:
                self._extra_size_and_coexist_node_for_others,
            constants.BroadcastStrategy.COMMON:
                self._extra_size_and_coexist_node_for_others
        }
        self._extra_temp_size, self._extra_node_num, self._extra_node_size = \
            strategy_and_funcs_map.get(self._insn_strategy)()

    def _is_support_vbrcb(self):
        if not soc_info.SocInfo.soc_support_vbrcb(self._tensor_obj.dtype):
            return False

        if not self._stage_obj.is_last_broadcast():
            return False

        if not self._stage_obj.align_info.actual_aligned:
            return False

        return True

    def _is_support_vnchwconv(self):
        if helper.get_bit_dtype(self._tensor_obj.dtype) not in (constants.BitDtype.B16, constants.BitDtype.B32):
            return False

        if not self._stage_obj.is_last_broadcast():
            return False

        if not self._stage_obj.align_info.enable_align and not self._stage_obj.align_info.actual_aligned:
            if self._buffer_ele_num > 0:
                entire_size = constants.DTYPE_AND_TRANSPOSE_ENTIRE_SIZE_MAP.get(self._tensor_obj.dtype)
                last_dim_upper_bound = self._buffer_ele_num // entire_size
                if helper.compare_iter_var_vs_value(self._stage_obj.tvm_stage.leaf_iter_vars[-1], last_dim_upper_bound,
                                                    constants.CmpOp.GT):
                    return False

        return True

    def _is_support_scalar_eight_dup(self):
        if not self._stage_obj.align_info.enable_align and not self._stage_obj.align_info.actual_aligned:
            return False

        if not self._stage_obj.is_last_broadcast():
            return False

        return True

    def _is_support_common(self):
        return True

    def _is_appropriate_for_vbrcb(self):
        return True

    def _is_appropriate_for_vnchwconv(self):
        if not self._stage_obj.is_single_broadcast():
            return False

        # broadcast + broadcast
        input_tensors = helper.get_tvm_input_tensors(self._tensor_obj.tvm_tensor)
        if input_tensors:
            if unify_helper.is_unified_broadcast(input_tensors[0]):
                return False

        return True

    def _is_appropriate_for_scalar_eight_dup(self):
        return True

    def _is_appropriate_for_common(self):
        return True

    def _extra_size_and_coexist_node_for_vnchwconv(self):
        extra_temp_size = 0
        extra_node_num = 0
        extra_node_size = 0

        if not self._stage_obj.align_info.enable_align and not self._stage_obj.align_info.actual_aligned:
            extra_node_num += constants.VNCHWCONV_UNALIGNED_BROADCAST_EXTRA_NODE
        else:
            bit_dtype = helper.get_bit_dtype(self._tensor_obj.dtype)
            if bit_dtype == constants.BitDtype.B16:
                extra_temp_size += constants.VNCHWCONV_B16_ALIGNED_BROADCAST_TEMP_SIZE + constants.BLOCK_SIZE
                extra_node_num += constants.VNCHWCONV_B16_ALIGNED_BROADCAST_EXTRA_NODE
            elif bit_dtype == constants.BitDtype.B32:
                extra_temp_size += constants.VNCHWCONV_B32_ALIGNED_BROADCAST_TEMP_SIZE + constants.BLOCK_SIZE

        return extra_temp_size, extra_node_num, extra_node_size

    def _extra_size_and_coexist_node_for_others(self):
        extra_temp_size = 0
        extra_node_num = 0
        extra_node_size = 0
        if not self._stage_obj.is_single_broadcast():
            extra_node_num += constants.MUITI_BROADCAST_AXES_EXTRA_NODE

        return extra_temp_size, extra_node_num, extra_node_size


class ReduceInsn(InsnBase):
    def __init__(self, tensor_obj, stage_obj: stage_info.ReduceStage):
        InsnBase.__init__(self, tensor_obj, stage_obj)
        self._storage_bound_attr_value = -1
        self._is_aggregated = False
        self._maybe_appropriate_to_reuse_dst_tensor = False
        self._enable_vc_flag = False
        self._enable_vcg_flag = False

    @property
    def storage_bound_attr_value(self):
        return self._storage_bound_attr_value

    @storage_bound_attr_value.setter
    def storage_bound_attr_value(self, value):
        self._storage_bound_attr_value = value

    @property
    def is_aggregated(self):
        return self._is_aggregated

    @is_aggregated.setter
    def is_aggregated(self, value):
        self._is_aggregated = value

    @property
    def maybe_appropriate_to_reuse_dst_tensor(self):
        return self._maybe_appropriate_to_reuse_dst_tensor

    @maybe_appropriate_to_reuse_dst_tensor.setter
    def maybe_appropriate_to_reuse_dst_tensor(self, value):
        self._maybe_appropriate_to_reuse_dst_tensor = value

    @property
    def enable_vc_flag(self):
        return self._enable_vc_flag

    @property
    def enable_vcg_flag(self):
        return self._enable_vcg_flag

    def set_insn_sub_type(self):
        self._insn_sub_type = constants.INSN_NAME_AND_SUB_TYPE_MAP.get(self._insn_name)
        self._enable_vc_flag = self._is_enable_vc()
        self._enable_vcg_flag = self._is_enable_vcg()

    def set_insn_strategy(self):
        strategy_and_funcs_map = {
            constants.ReduceStrategy.ENABLE_ENTIRE_REDUCE:
                [self._is_support_entire_reduce, self._is_appropriate_for_entire_reduce],
            constants.ReduceStrategy.ENABLE_NLAST_DICHOTOMY:
                [self._is_support_nlast_dichotomy, self._is_appropriate_for_nlast_dichotomy],
            constants.ReduceStrategy.ENABLE_TRANS:
                [self._is_support_trans, self._is_appropriate_for_trans],
            constants.ReduceStrategy.COMMON:
                [self._is_support_common, self._is_appropriate_for_common]
        }
        self._insn_strategy = _get_strategy(strategy_and_funcs_map, self._preferred_strategy,
                                            self._rejected_strategy_list)

    def set_insn_attrs(self):
        strategy_and_attrs_map = {
            constants.ReduceStrategy.ENABLE_ENTIRE_REDUCE:
                {constants.InsnAttrsKey.REDUCE_OPT_MODE: constants.InsnAttrsValue.ENTIRE_REDUCE},
            constants.ReduceStrategy.ENABLE_NLAST_DICHOTOMY:
                {constants.InsnAttrsKey.REDUCE_OPT_MODE: constants.InsnAttrsValue.DICHOTOMY_REDUCE_BIG_DIM,
                 constants.InsnAttrsKey.NLAST_REDUCE_DICHOTOMY: constants.InsnAttrsValue.NLAST_REDUCE_DICHOTOMY_VALUE},
            constants.ReduceStrategy.ENABLE_TRANS:
                {constants.InsnAttrsKey.REDUCE_TRANS: True},
            constants.ReduceStrategy.COMMON:
                {}
        }
        self._insn_attrs = strategy_and_attrs_map.get(self._insn_strategy)
        self._insn_attrs.update({constants.InsnAttrsKey.STORAGE_BOUND: self._extra_node_ele_num})
        if not self._is_aggregated:
            if self._insn_sub_type in (constants.ReduceType.MAX, constants.ReduceType.MIN):
                self._insn_attrs.update({constants.InsnAttrsKey.MAP_POLICY: constants.InsnAttrsValue.NO_OFFSET})
            if self._is_need_append_reuse_dst_tensor():
                self._insn_attrs.update({constants.InsnAttrsKey.REUSE_DST_TENSOR: True})
            if self._is_enable_vc and \
                    not soc_info.SocInfo.soc_support_vc_only_value(self._insn_sub_type, self._tensor_obj.dtype):
                self._insn_attrs.update({constants.InsnAttrsKey.ENOUGH_BUFFER: True})

    def set_extra_size_and_coexist_node(self):
        strategy_and_funcs_map = {
            constants.ReduceStrategy.ENABLE_ENTIRE_REDUCE:
                self._extra_size_and_coexist_node_for_entire_reduce,
            constants.ReduceStrategy.ENABLE_NLAST_DICHOTOMY:
                self._extra_size_and_coexist_node_for_nlast_dichotomy,
            constants.ReduceStrategy.ENABLE_TRANS:
                self._extra_size_and_coexist_node_for_trans,
            constants.ReduceStrategy.COMMON:
                self._extra_size_and_coexist_node_for_others
        }
        self._extra_temp_size, self._extra_node_num, self._extra_node_size = \
            strategy_and_funcs_map.get(self._insn_strategy)()

    def _is_need_append_reuse_dst_tensor(self):
        if self._is_aggregated:
            return False
        if self._insn_strategy not in (constants.ReduceStrategy.ENABLE_ENTIRE_REDUCE,
                                       constants.ReduceStrategy.ENABLE_NLAST_DICHOTOMY):
            return False
        if self._insn_strategy == constants.ReduceStrategy.ENABLE_ENTIRE_REDUCE:
            if self._enable_vc_flag and self._maybe_appropriate_to_reuse_dst_tensor:
                return False

        return True

    def _is_support_entire_reduce(self):
        if not self._stage_obj.is_last_reduce():
            return False

        if self._tensor_obj.dtype not in ("float16", "float32"):
            return False

        if self._stage_obj.is_discontinuous_reduce():
            return False

        if not self._stage_obj.is_single_reduce() and not self._stage_obj.align_info.actual_aligned:
            return False

        if self._insn_sub_type == constants.ReduceType.PROD:
            return False

        return True

    def _is_support_nlast_dichotomy(self):
        if self._is_aggregated:
            return False

        if self._stage_obj.is_last_reduce():
            return False

        if self._tensor_obj.dtype not in ("float16", "float32"):
            return False

        if self._stage_obj.is_discontinuous_reduce():
            return False

        if self._stage_obj.node_pattern[-2:] != "RA" and not self._stage_obj.align_info.actual_aligned:
            return False

        if self._insn_sub_type == constants.ReduceType.PROD:
            return False

        return True

    def _is_support_trans(self):
        if not self._stage_obj.is_last_reduce():
            return False

        if not self._stage_obj.is_single_reduce():
            return False

        if self._buffer_ele_num > 0:
            entire_size = constants.DTYPE_AND_TRANSPOSE_ENTIRE_SIZE_MAP.get(self._tensor_obj.dtype)
            last_dim_upper_bound = self._buffer_ele_num // entire_size
            if helper.compare_iter_var_vs_value(self._stage_obj.tvm_stage.leaf_iter_vars[-1], last_dim_upper_bound,
                                                constants.CmpOp.GT):
                return False

        return True

    def _is_support_common(self):
        return True

    def _is_appropriate_for_entire_reduce(self):
        last_dim_lower_bound = constants.ONE_REPEAT_SIZE // helper.get_bytes_num(self._tensor_obj.dtype)
        if helper.compare_iter_var_vs_value(self._stage_obj.tvm_stage.leaf_iter_vars[-1], last_dim_lower_bound,
                                            constants.CmpOp.LE):
            return False

        return True

    def _is_appropriate_for_nlast_dichotomy(self):
        if not self._stage_obj.align_info.enable_align and not self._stage_obj.align_info.actual_aligned:
            return False

        return True

    def _is_appropriate_for_trans(self):
        if self._stage_obj.align_info.actual_aligned:
            return False

        return True

    def _is_appropriate_for_common(self):
        return True

    def _extra_size_and_coexist_node_for_entire_reduce(self):
        extra_temp_size, extra_node_num, extra_node_size = \
            self._base_extra_size_and_coexist_node(is_entire_reduce=True)
        is_reuse_dst = self._insn_attrs.get(constants.InsnAttrsKey.REUSE_DST_TENSOR) is True
        if is_reuse_dst:
            if not self._enable_vc_flag:
                extra_node_num += constants.ENTIRE_REDUCE_WITHOUT_VC_AND_REUSE_DST_EXTRA_NODE
        else:
            if self._enable_vc_flag:
                extra_node_num += constants.ENTIRE_REDUCE_WITH_VC_NO_REUSE_DST_EXTRA_NODE
            else:
                extra_node_num += constants.ENTIRE_REDUCE_WITHOUT_VC_AND_NO_REUSE_DST_EXTRA_NODE

        return extra_temp_size, extra_node_num, extra_node_size

    def _extra_size_and_coexist_node_for_nlast_dichotomy(self):
        extra_temp_size, extra_node_num, extra_node_size = self._base_extra_size_and_coexist_node()
        is_reuse_dst = self._insn_attrs.get(constants.InsnAttrsKey.REUSE_DST_TENSOR) is True
        if not is_reuse_dst:
            extra_node_num += constants.NLAST_DICHOTOMY_REDUCE_NO_REUSE_DST_EXTRA_NODE

        return extra_temp_size, extra_node_num, extra_node_size

    def _extra_size_and_coexist_node_for_trans(self):
        extra_temp_size = constants.ONE_REPEAT_SIZE
        extra_node_num = constants.TRANS_REDUCE_EXTRA_NODE

        return extra_temp_size, extra_node_num, 0

    def _extra_size_and_coexist_node_for_others(self):
        return self._base_extra_size_and_coexist_node()

    def _base_extra_size_and_coexist_node(self, is_entire_reduce=False):
        extra_temp_size = 0
        extra_node_num = 0
        extra_node_size = 0
        if self._stage_obj.is_last_reduce():
            is_soc_support_vc_only_value = \
                soc_info.SocInfo.soc_support_vc_only_value(self._insn_sub_type, self._tensor_obj.dtype)
            if self._enable_vcg_flag:
                pass
            elif self._enable_vc_flag:
                last_dim_lower_bound = constants.ONE_REPEAT_SIZE // helper.get_bytes_num(self._tensor_obj.dtype)
                if helper.compare_iter_var_vs_value(self._stage_obj.tvm_stage.leaf_iter_vars[-1], last_dim_lower_bound,
                                                    constants.CmpOp.GT) and not is_entire_reduce:
                    extra_temp_size += constants.ONE_REPEAT_SIZE
                if not is_soc_support_vc_only_value:
                    extra_node_num += constants.REDUCE_VC_REMOVE_INDEX_EXTRA_NODE
                    # to use vcpadd
                    extra_node_size += constants.ONE_REPEAT_SIZE // 2
            else:
                extra_temp_size += constants.ONE_REPEAT_SIZE + constants.BLOCK_SIZE + constants.BLOCK_SIZE
        else:
            # R A
            # float16 need one repeat size temp buffer when A < 128 // 2
            # float32 need one repeat size temp buffer when A < 64 // 2
            # but A maybe split
            extra_temp_size += constants.ONE_REPEAT_SIZE

        return extra_temp_size, extra_node_num, extra_node_size

    def _is_enable_vc(self):
        if not self._stage_obj.is_last_reduce():
            return False

        return soc_info.SocInfo.soc_support_vc(self._insn_sub_type, self._tensor_obj.dtype)

    def _is_enable_vcg(self):
        if not self._stage_obj.is_last_reduce():
            return False

        if not soc_info.SocInfo.soc_support_vc(self._insn_sub_type, self._tensor_obj.dtype):
            return False

        last_dim_upper_bound = soc_info.SocInfo.get_block_ele_num(self._tensor_obj.dtype)
        if helper.compare_iter_var_vs_value(self._stage_obj.tvm_stage.leaf_iter_vars[-1], last_dim_upper_bound,
                                            constants.CmpOp.GT):
            return False

        return True


class DataMoveInsn(InsnBase):
    def __init__(self, tensor_obj, stage_obj):
        InsnBase.__init__(self, tensor_obj, stage_obj)
        self._insn_name = constants.SpecialInsnName.DMA_COPY


class CommonInsn(InsnBase):
    def __init__(self, tensor_obj, stage_obj):
        InsnBase.__init__(self, tensor_obj, stage_obj)
        self._is_last_broadcast_inline = False

    def set_extra_size_and_coexist_node(self):
        self._extra_temp_size, self._extra_node_num, self._extra_node_size = \
            self._cmp_sel_extra_size_and_coexist_node()

    def set_insn_attrs(self):
        if self._is_last_broadcast_inline:
            self._insn_attrs = {constants.InsnAttrsKey.STORAGE_BOUND: self._extra_node_ele_num}

    def _cmp_sel_extra_size_and_coexist_node(self):
        extra_temp_size = 0
        extra_node_num = 0
        extra_node_size = 0

        if isinstance(self.insn_sub_type, constants.CommonCmpType):
            input_tensors_num = len(helper.get_tvm_input_tensors(self._tensor_obj.tvm_tensor))
            extra_temp_size += constants.BLOCK_SIZE * (constants.VCMP_INPUT_NUMBER - input_tensors_num)
            extra_node_size += constants.ONE_REPEAT_SIZE
        if isinstance(self.insn_sub_type, constants.CommonCmpSelType):
            input_tensors_num = len(helper.get_tvm_input_tensors(self._tensor_obj.tvm_tensor))
            extra_temp_size += constants.BLOCK_SIZE * (constants.VCMPSEL_INPUT_NUMBER - input_tensors_num)
            extra_node_size += constants.ONE_REPEAT_SIZE
        if self.insn_sub_type == constants.CommonType.SEL:
            if not soc_info.SocInfo.soc_support_vsel_non_zero_mode():
                input_tensors_num = len(helper.get_tvm_input_tensors(self._tensor_obj.tvm_tensor))
                extra_temp_size += constants.BLOCK_SIZE * (constants.VSEL_INPUT_NUMBER - input_tensors_num)
            extra_node_size += constants.ONE_REPEAT_SIZE

        inlined_stage_obj = self._stage_obj.compute_inlined_stage_obj
        if inlined_stage_obj:
            if inlined_stage_obj.node_type == constants.NodeType.BROADCAST and inlined_stage_obj.is_last_broadcast():
                extra_node_num += 1
                self._is_last_broadcast_inline = True

        return extra_temp_size, extra_node_num, extra_node_size
