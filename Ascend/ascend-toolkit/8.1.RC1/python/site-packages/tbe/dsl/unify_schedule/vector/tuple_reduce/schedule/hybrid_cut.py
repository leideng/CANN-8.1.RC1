#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Reduce Cut Schedule
"""
# Standard Packages
from typing import List
# Local Packages
from tbe import tvm
from tbe.tvm import Tensor
from tbe.dsl.base.operation import var_inner
from tbe.dsl.base.operation import add_build_arg
from tbe.dsl.unify_schedule.util import is_v220
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.constants import DTYPE_BYTE_MAPPING
from tbe.dsl.unify_schedule.constants import TupleReducePattern
# Tuple-Reduce Packages
from ..common.constants import Options
from ..common.constants import ScheduleType
from ..common.schedule_helper import Schedule


class HybridCut:
    """
    Block cut on both normal and reduce axis
    """

    def __init__(self, outs, tiling_case):
        self.outs: List[Tensor] = outs
        self.tiling_case = tiling_case
        self.info = self.tiling_case.info
        self.tiling_key = self.tiling_case.tiling_key

        # SCHEDULE INFORMATION
        self.intermediate_output = []
        sch = Schedule(self.outs)
        tensors = sch.tensors
        for t in self.outs:
            input_tensors = []
            for ten in tensors:
                input_tensors = input_tensors + [tensor for tensor in ten.op.input_tensors]
            if t in input_tensors:
                self.intermediate_output.append(t)
        self.real_outs = [t for t in self.outs if t not in self.intermediate_output]
        self.sch = Schedule(self.real_outs)
        self.align_pad_stages = set()

        # Tiling
        self.ub_axis = self.tiling_case.ub_axis
        if self.info.is_const:
            self.blocka_factor = self.info.blocka_factor
            self.blockr_factor = self.info.blockr_factor
            self.ub_factor = self.info.ub_factor
        else:
            self.blocka_factor = var_inner("_blocka_factor", (1, None), dtype="int64")
            self.blockr_factor = var_inner("_blockr_factor", (1, None), dtype="int64")
            self.ub_factor = var_inner("_ub_factor", (1, None), dtype="int64")

        # align pad
        self.align_pad_stages = set()
        self.remove_pad_stages = set()

        # set switches
        self.info.buffer_size.switches.transpose_reduce = self.tiling_case.options == Options.TransposeReduce
        self.info.buffer_size.switches.align_pad = self.tiling_case.options == Options.UBAlignPad
        # buffer size options
        self.info.buffer_size.estimate(self.info.max_dtype_size, ScheduleType.HybridCut)

        # intermediate_output
        self.intermediate_output_stage = set()

        # init vars
        self.reduce_tensor = None
        self.reduce_stage = None
        self.ub_o = None
        self.ub_i = None
        self.normal_axis = None
        self.reduce_axis = None
        self.ax = None
        self.ax_o = None
        self.ax_i = None
        self.rx = None
        self.rx_o = None
        self.rx_i = None
        self.block_o = None
        self.reduce_rf = None
        self.reduce_rf_rf = None
        self.reduce_rf_stage = None
        self.reduce_rf_rf_stage = None
        

    def do_schedule(self):
        self._get_stages()
        self._cache_read()
        self._intermediate_output()
        self._align_pad()
        if self.info.reduce_axis_one_hot[self.ub_axis]:
            self._tile_ub()
            self._fuse_rx()
            self._tile_rx()
            self._rfactor_rx_o()
            self._rfactor_ub_r()
        else:
            self._fuse_rx()
            self._tile_rx()
            self._rfactor_rx_o()
            self._rfactor_ub_r()
            self._tile_ub()
        self._fuse_ax()
        self._tile_ax()
        self._fuse_ax_o_rx_o()
        self._reorder_reduce_rf_stage()
        self._reorder_reduce_rf_rf_stage()
        self._compute_at_rf_rf_stage()
        self._compute_at_reduce_stage()
        self._set_scope()
        self._bind_block()
        self._buffer_size()
        self._emit_insn()
        self._storage_align()
        self._compute_inline()
        self._compute_root()
        self._mem_unique()
        self._add_build_arg()

        return self.sch.sch

    def _get_stages(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        self.reduce_tensor = info.graph.reduce_tensor[0]
        self.reduce_stage = sch[self.reduce_tensor]

    def _cache_read(self):
        sch = self.sch
        for ph in sch.placeholder:
            consumers = sch.consumer(sch[ph])
            readers = [stage.origin_op for stage in consumers]
            sch.cache_read(ph, "local.UB", readers)
    
    def _intermediate_output(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        for tensor in self.intermediate_output:
            # cache write
            cache_write_ub_tensor = sch.sch.cache_write(tensor, "")
            cache_write_ub_stage = sch.get_stage(cache_write_ub_tensor)
            # cache read
            consumers = sch.consumer(sch[tensor])
            readers = [stage.origin_op for stage in consumers]
            cache_read_ub_tensor = sch.sch.cache_read(tensor, "local.UB", readers)
            cache_read_ub_stage = sch.get_stage(cache_read_ub_tensor)
            # mem reuse
            cache_write_ub_stage.reused_by(cache_read_ub_tensor)
            cache_read_ub_stage.reused_by(reuse_data=True)
            cache_read_ub_stage.emit_insn(cache_read_ub_stage.op.axis[0], "phony_insn")
            # add to intermediate_output_stage
            self.intermediate_output_stage.add(sch.get_stage(tensor))
    
    def _align_pad(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.align_pad:
            cache_read_stages_zero = set(sch.cache_read_stages)
            for stage in sch.broadcast_branch_roots:
                cache_read_stages_zero -= sch.poset(stage)

            for stage in cache_read_stages_zero:
                consumers = sch.consumer(stage)
                readers = [stage.origin_op for stage in consumers]
                t = sch.cache_read(sch.get_tensor(stage), "local.UB", readers)
                self.align_pad_stages.add(sch[t])
    
    def _tile_ub(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if self.info.reduce_axis_one_hot[self.ub_axis]:
            ub_split_axis_idx = sum(self.info.reduce_axis_one_hot[:self.ub_axis])
            ub_split_axis = self.reduce_stage.op.reduce_axis[ub_split_axis_idx]
        else:
            ub_split_axis = self.reduce_stage.op.axis[self.ub_axis]
        self.ub_o, self.ub_i = self.reduce_stage.split(ub_split_axis, factor=self.ub_factor)

    def _fuse_rx(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        ub_split_axis_idx = sum(self.info.reduce_axis_one_hot[:self.ub_axis])
        self.reduce_axis = sch.comm_reduce(self.reduce_stage)[:ub_split_axis_idx]
        if self.info.reduce_axis_one_hot[self.ub_axis]:
            self.reduce_axis = self.reduce_axis + [self.ub_o]
        self.rx = self.reduce_stage.fuse(*self.reduce_axis)
    
    def _tile_rx(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        rx_split_axis = sch.comm_reduce(self.reduce_stage)[0]
        self.rx_o, self.rx_i = self.reduce_stage.split(rx_split_axis, factor=self.blockr_factor)
    
    def _rfactor_rx_o(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        self.reduce_rf = sch.rfactor(self.reduce_tensor, self.rx_o, 0)[0]
        self.reduce_rf_stage = sch[self.reduce_rf]
        self.reduce_stage = sch.get_stage(self.reduce_tensor)
    
    def _rfactor_ub_r(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        rf_index = []
        rf_i_index = self.ub_axis + 2
        rx_i = sch.search_iter_by_name(self.reduce_rf_stage, self.rx_i.var.name)

        if self.info.reduce_axis_one_hot[self.ub_axis]:
            ub_i = sch.search_iter_by_name(self.reduce_rf_stage, self.ub_i.var.name)
            rf_axis = [ub_i] + [iter for iter in sch.comm_reduce(self.reduce_rf_stage) if iter not in [ub_i, rx_i]]
        else:
            rf_axis = [iter for iter in sch.comm_reduce(self.reduce_rf_stage) if iter not in [rx_i]]

        for i, v in enumerate(info.reduce_axis_one_hot[self.ub_axis:]):
            if v == 1:
                rf_index.append(rf_i_index + i)
        self.reduce_rf_rf = sch.rfactor(self.reduce_rf, rf_axis, rf_index)[0]
        self.reduce_rf_rf_stage = sch[self.reduce_rf_rf]
        self.reduce_rf_stage = sch.get_stage(self.reduce_rf)
        self.reduce_stage = sch.get_stage(self.reduce_tensor)

    def _fuse_ax(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if self.info.reduce_axis_one_hot[self.ub_axis]:
            self.normal_axis = sch.data_parallel_iteration(self.reduce_stage)[:self.ub_axis + 1]
        else:
            ub_o = sch.search_iter_by_name(self.reduce_stage, self.ub_o.var.name)
            self.normal_axis = sch.data_parallel_iteration(self.reduce_stage)[:self.ub_axis] + [ub_o]
        self.ax = self.reduce_stage.fuse(*self.normal_axis)

    def _tile_ax(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        self.ax_o, self.ax_i = self.reduce_stage.split(self.ax, factor=self.blocka_factor)
    
    def _fuse_ax_o_rx_o(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        rx_o = sch.search_iter_by_name(self.reduce_stage, self.rx_o.var.name)
        axis_order = [self.ax_o, rx_o]
        axis_order = axis_order + [iter for iter in self.reduce_stage.leaf_iter_vars if iter not in axis_order]
        self.reduce_stage.reorder(*axis_order)
        self.block_o = self.reduce_stage.fuse(self.ax_o, rx_o)
    
    def _reorder_reduce_rf_stage(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if self.info.reduce_axis_one_hot[-1] == 0:
            last_axis = sch.data_parallel_iteration(self.reduce_rf_stage)[-1]
            new_order = []
            for thisaxis in self.reduce_rf_stage.leaf_iter_vars:
                if thisaxis == last_axis:
                    continue
                new_order.append(thisaxis)
            new_order.append(last_axis)
            self.reduce_rf_stage.reorder(*new_order)
    
    def _reorder_reduce_rf_rf_stage(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        rx_i = sch.search_iter_by_name(self.reduce_rf_rf_stage, self.rx_i.var.name)
        rf_rf_order = [rx_i] + [iter for iter in self.reduce_rf_rf_stage.leaf_iter_vars if not iter == rx_i]
        self.reduce_rf_rf_stage.reorder(*rf_rf_order)
    
    def _compute_at_rf_rf_stage(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        ub_o = sch.comm_reduce(self.reduce_rf_rf_stage)[0]
        for stage in sch.poset(self.reduce_rf_rf_stage):
            stage.compute_at(self.reduce_rf_rf_stage, ub_o)
    
    def _compute_at_reduce_stage(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        ax_i = sch.search_iter_by_name(self.reduce_stage, self.ax_i.var.name)
        self.reduce_rf_rf_stage.compute_at(self.reduce_stage, ax_i)
        self.reduce_rf_stage.compute_at(self.reduce_stage, ax_i)
    
    def _set_scope(self):
        self.sch.stages_not_on_ub.add(self.reduce_stage)
        for stage in self.sch.stages_on_ub - self.intermediate_output_stage:
            stage.set_scope("local.UB")

    def _bind_block(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        block_o = sch.search_iter_by_name(self.reduce_stage, self.block_o.var.name)
        self.reduce_stage.bind(block_o, tvm.thread_axis("blockIdx.x"))
    
    def _emit_insn(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        self.reduce_stage.emit_insn(self.reduce_stage.leaf_iter_vars[2], "dma_copy")
        if not sch.comm_reduce(self.reduce_rf_stage):
            self.reduce_rf_stage.emit_insn(self.reduce_rf_stage.op.axis[0], "phony_insn")
        else:
            if self.info.reduce_axis_one_hot[-1] == 0:
                self.reduce_rf_stage.emit_insn(sch.comm_reduce(self.reduce_rf_stage)[0], "vector_reduce_sum")
            else:
                self.reduce_rf_stage.emit_insn(self.reduce_rf_stage.op.axis[0], "vector_reduce_sum")
        self.reduce_rf_rf_stage.emit_insn(self.reduce_rf_rf_stage.op.axis[0], "vector_reduce_sum")
        for stage in sch.cache_read_stages.keys() - self.intermediate_output_stage - self.align_pad_stages:
            stage.emit_insn(stage.op.axis[0], "dma_copy")
        for stage in self.align_pad_stages:
            stage.emit_insn(stage.op.axis[0], "align_pad")
        vector_with_broadcast_stages = []
        for stage in sch.broadcast_branch_roots:
            vector_with_broadcast_stages += sch.consumer(stage)
        for stage in vector_with_broadcast_stages:
            if info.reduce_axis_one_hot[-1] == 1:
                stage.emit_insn(stage.op.axis[0], info.get_insn(stage))
            else:
                stage.emit_insn(stage.op.axis[0], info.get_insn(stage), attrs={"use_ba_pattern_brc": 1})
        for stage in sch.stages_on_ub - {self.reduce_stage, self.reduce_rf_stage, self.reduce_rf_rf_stage} \
            - sch.cache_read_stages.keys() - self.intermediate_output_stage:
            if is_v220() and info.get_insn(stage) == 'vector_broadcast' and len(stage.op.axis) in (2, 3):
                if len(stage.op.axis) == 2:
                    stage.emit_insn(stage.op.axis[0], info.get_insn(stage),
                                    attrs={'enable_brc_block': 1, 'vbrcb_threshold': 200})
                elif len(stage.op.axis) == 3:
                    stage.emit_insn(stage.op.axis[0], info.get_insn(stage),
                                    attrs={'enable_brc_block': 1, 'vbrcb_threshold': 200, 'vbrcb_enough_buffer': True})
            else:
                stage.emit_insn(stage.op.axis[0], info.get_insn(stage))
    
    def _buffer_size(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        for stage in sch.stages_on_ub:
            if stage == self.reduce_rf_stage or \
                sch.cache_read_stages.get(stage) in info.buffer_size.short_tensors or \
                sch.get_ori_tensor(stage) in info.buffer_size.short_tensors:
                stage.set_buffer_size(info.buffer_size.short_buffer_size)
            else:
                stage.set_buffer_size(info.buffer_size.grande_buffer_size)

    def _storage_align(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.transpose_reduce:
            return
        stages_before_reduce_rf = sch.stages_on_ub.intersection(sch.poset(self.reduce_rf_stage))
        storage_align_stages = stages_before_reduce_rf
        for stage in self.align_pad_stages:
            storage_align_stages = storage_align_stages - sch.poset(stage)
        # stages in broadcast_branch do not need storage_align
        for stage in sch.broadcast_branch_roots:
            storage_align_stages -= sch.poset(stage)

        for stage in storage_align_stages:
            dtype_size = DTYPE_BYTE_MAPPING.get(sch.get_tensor(stage).dtype)
            if stage.op.attrs.get("_type") == "broadcast.tensor":
                # fix milan last broadcast emit insn not support storage align issue
                stage.compute_align(stage.op.axis[-1], info.soc.block_size // dtype_size)
            elif len(stage.op.axis) > 1:
                stage.storage_align(stage.op.axis[-2], info.soc.block_size // dtype_size, 0)
    
    def _compute_inline(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        for stage in sch.stages_on_ub.intersection(sch.broadcast_stages):
            if set(sch.consumer(stage)).intersection(sch.reduce_stages):
                continue
            if not sch.is_last_broadcast(stage) or sch.is_scalar_broadcast(stage):
                stage.compute_inline()

    def _compute_root(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.compute_root:
            for stage in sch.broadcast_branch.intersection(sch.stages_on_ub):
                stage.compute_root()

    def _mem_unique(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.mem_unique:
            for stage, tensor in sch.cache_read_stages.items():
                if tensor in info.buffer_size.unique_tensors:
                    stage.mem_unique()
    
    def _add_build_arg(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        add_build_arg("read_write_bank_conflict", True)
