#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
scatter schedule atomic
"""
from tbe.dsl.base import operation

from ... import util
from ...schedule import Schedule
from .scatter_tilingcase import TilingStrategy
from .scatter_schedule_base import ScatterBaseSchedule

# Represents the axis that does not need to be split
DUMMY_DIM = -10


# 'pylint: disable=R0902, R0903
class ScatterAtomicSchedule(ScatterBaseSchedule):
    """
    scatter schedule
    """

    def _calc_tiling_dynamic(self):
        output_shape = util.shape_to_list(self._out_tensor.shape)
        indices_shape = util.shape_to_list(self._indices_gm_tensor.shape)

        b_idx_sparse = self._tiling_case["block_tiling_sparse_axis"]
        u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
        u_idx_sparse = self._tiling_case["ub_tiling_sparse_axis"]

        b_bound = (1, util.get_bound(indices_shape[b_idx_sparse])[1])
        self._block_tiling_vars[b_idx_sparse] = operation.var_inner("_block_factor_" + str(b_idx_sparse),
                                                                    b_bound, "int64")

        if u_idx_norm is not None:
            u_bound = (1, util.get_bound(output_shape[u_idx_norm])[1])
            self._ub_norm_tiling_vars[u_idx_norm] = operation.var_inner("_ub_norm_factor_" + str(u_idx_norm),
                                                                        u_bound, "int64")

        if u_idx_sparse is not None:
            u_bound = (1, util.get_bound(indices_shape[u_idx_sparse])[1])
            self._ub_sparse_tiling_vars[u_idx_sparse] = operation.var_inner("_ub_sparse_factor_" + str(u_idx_sparse),
                                                                            u_bound, "int64")

    def _do_tiling(self):
        if self._tiling_strategy == TilingStrategy.DYNAMIC:
            b_idx_sparse = self._tiling_case["block_tiling_sparse_axis"]
            u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
            u_idx_sparse = self._tiling_case["ub_tiling_sparse_axis"]

            block_factor = self._block_tiling_vars.get(b_idx_sparse)
            ub_norm_factor = self._ub_norm_tiling_vars.get(u_idx_norm)
            ub_sparse_factor = self._ub_sparse_tiling_vars.get(u_idx_sparse)
        elif self._tiling_strategy == TilingStrategy.STATIC:
            b_idx_sparse = self._const_block_axis
            u_idx_norm = self._const_ub_norm_axis if self._const_ub_norm_axis != DUMMY_DIM else None
            u_idx_sparse = self._const_ub_sparse_axis

            block_factor = self._const_block_factor
            ub_norm_factor = self._const_ub_norm_factor
            ub_sparse_factor = self._const_ub_sparse_factor
        else:
            b_idx_sparse = 0
            u_idx_norm = None
            u_idx_sparse = 0
            block_factor = 1
            ub_norm_factor = 1
            ub_sparse_factor = 1

        b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.sparse_axis[b_idx_sparse],
                                                          factor=block_factor)

        if u_idx_sparse is not None:
            if u_idx_sparse == b_idx_sparse:
                u_sparse_o, u_sparse_i = self._schedule[self._out_tensor].split(b_i, factor=ub_sparse_factor)
                self._reorder_axis.extend([b_o, u_sparse_o, u_sparse_i])
            else:
                u_sparse_o, u_sparse_i = self._schedule[self._out_tensor].split(
                    self._out_tensor.op.sparse_axis[u_idx_sparse], factor=ub_sparse_factor)
                self._reorder_axis.extend([b_o, b_i, u_sparse_o, u_sparse_i])

        if u_idx_norm is not None:
            u_norm_o, u_norm_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[u_idx_norm],
                                                                        factor=ub_norm_factor)

            self._reorder_axis.extend([u_norm_o, u_norm_i])
            for i in range(len(self._out_tensor.op.axis) - 1):
                self._reorder_axis.append(self._out_tensor.op.axis[i])
        else:
            for i in range(len(self._out_tensor.op.axis)):
                self._reorder_axis.append(self._out_tensor.op.axis[i])

        self._block_bind_axis = b_o
        self._compute_at_norm_axis = u_norm_o if u_idx_norm else u_sparse_o
        self._compute_at_sparse_axis = u_sparse_o
        self._emit_axis = 1 if u_idx_norm else 0
        self._emit_scatter_axis = u_norm_i if u_idx_norm else self._out_tensor.op.axis[-1]

    def _calc_compute_at(self):
        if self._store_area == 1 and not self._is_scatter_nd_op():
            self._compute_at_map[self._var_ub] = [self._out_tensor, self._compute_at_norm_axis]
            if not self._is_sparse_cache and self._is_need_align_pad:
                self._compute_at_map[self._var_align_pad_ub] = [self._out_tensor, self._compute_at_norm_axis]

        self._compute_at_map[self._indices_ub] = [self._out_tensor, self._compute_at_sparse_axis]
        self._compute_at_map[self._update_ub] = [self._out_tensor, self._compute_at_norm_axis]
        if self._is_need_align_pad:
            self._compute_at_map[self._update_align_pad_ub] = [self._out_tensor, self._compute_at_norm_axis]

    def _calc_emit_insn(self):
        if self._store_area == 1 and not self._is_scatter_nd_op():
            self._emit_insn_map[self._var_ub] = [self._var_ub.op.axis[self._emit_axis], "dma_copy"]
            if not self._is_sparse_cache and self._is_need_align_pad:
                self._emit_insn_map[self._var_align_pad_ub] = [self._var_align_pad_ub.op.axis[self._emit_axis],
                                                               "align_pad"]

        self._emit_insn_map[self._indices_ub] = [self._indices_ub.op.axis[0], "dma_copy"]
        self._emit_insn_map[self._update_ub] = [self._update_ub.op.axis[self._emit_axis], "dma_copy"]
        if self._is_need_align_pad:
            self._emit_insn_map[self._update_align_pad_ub] = [self._update_align_pad_ub.op.axis[self._emit_axis],
                                                              "align_pad"]
        if self._is_scatter_nd_op():
            self._emit_insn_map[self._var_gm_tensor] = [self._var_gm_tensor.op.axis[0], "phony_insn"]
            self._emit_insn_map[self._out_tensor] = [self._emit_scatter_axis, "dma_copy",
                                                     {"segment_atomic": 1}]
        elif self._op_type in ["scatter_update", "scatter_nd_update"]:
            self._emit_insn_map[self._out_tensor] = [self._emit_scatter_axis, "dma_copy",
                                                     {"scatter_update": True}]
        else:
            self._emit_insn_map[self._out_tensor] = [self._emit_scatter_axis, "dma_copy",
                                                     {"no_overlap": 2, "no_init_atomic": 1}]
        self._schedule[self._out_tensor].pragma(self._emit_scatter_axis, "loop_with_no_overlap_tensor",
                                                pragma_tensor=self._out_tensor)
