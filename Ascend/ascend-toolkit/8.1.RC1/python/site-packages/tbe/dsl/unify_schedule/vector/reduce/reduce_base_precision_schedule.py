#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2024-2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
reduce base high precision schedule
"""

from tbe import tvm
from tbe.dsl.base.operation import get_context
from tbe.common.platform import get_block_size
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.base.operation import add_build_arg
from .reduce_tilingcase import ReduceTilingCase
from .reduce_tilingcase import SingleReduceInfo
from .vector_info import ComputeGraphInfo
from ...constants import INSN_MAPPING
from ... import util
from .reduce_branch_helper import BranchTensorInfo


LOCAL_UB = "local.UB"
PARENT = "parent"
SCOPE = "scope"
INSTRUCTION = "instruction"
ATTRS = "attrs"


class ReduceBasePrecisionSchedule:
    def __init__(self, graph_info: ComputeGraphInfo, reduce_info: SingleReduceInfo,
                 tiling_case: ReduceTilingCase, outs):
        self.graph_info = graph_info
        self.reduce_info = reduce_info
        self.tiling_case = tiling_case
        self.outs = outs
        self.sch = None

        # get last endpoint output tensor
        self.res_tensor = list(graph_info.endpoint_output_tensor_set)[0]
        self.reduce_tensor = None
        self.input_db_tensor = None

        # graph
        self.forward_graph_map = graph_info.tensor_consumers_map
        self.backward_graph_map = graph_info.tensor_producers_map
        self.forward_stage_graph_map = ComputeGraphInfo.set_map_deepcopy(self.forward_graph_map)
        self.backward_stage_graph_map = ComputeGraphInfo.set_map_deepcopy(self.backward_graph_map)

        self.branch_tensor_info: BranchTensorInfo = None
        self.tensors_before_reduce = None # not include reduce_tensor and rfactor tensors
        self.tensors_after_reduce = None # not include reduce_tensor nor res_tensor

        # mid tensor include pure mid tensor and non gm input tensor
        self.mid_tensor_set = graph_info.mid_tensor_set
        # real output except mid output tensor
        self.real_pure_output = graph_info.real_output_tensor_set - \
                                 graph_info.trunk_mid_output_tensor_set - \
                                 graph_info.branch_mid_output_tensor_set
        # for mid output tensor emit sn
        self.mid_output_tensor_cache_read_list = []

        # which means: a[tensor] is buffer
        self.cache_read_tensors_and_buffer_map = {}
        self.cache_write_tensors_and_buffer_map = {}

        self.block_split_result = {}
        self.block_split_result_2 = {} # block 二次切分
        self.ub_split_result = {}
        self.ub_split_result_2 = {} # ub 二次切分

        self.storage_align_map = {} # key: tensor; value: [axis, factor, offset]
        self.compute_align_map = {} # key: tensor; value: [axis, factor, pad]
        self.bind_buffer_map = {}   # key: tensor; value: [axis, stride, offset]
        self.compute_at_map = {}
        self.emit_insn_map = {}

        self.ar_reduce_attr = None
        self.ar_reduce_attr_scalar = None
        self.ara_reduce_attr = None
        self.ara_reduce_big_attr = None
        self._init_reduce_attr()

        self.block_size_byte = get_block_size()
        self.none_reduce_index_map = self._find_none_reduce_axis_map()

    @staticmethod
    def _get_insn(tensor):
        """
        get insn of tensor
        """
        tag = tensor.op.tag
        insn = tag.split("|")[0] if tensor.op.tag.find("|") != -1 else tag

        return INSN_MAPPING.get(insn, insn)

    @staticmethod
    def _add_build_args():
        # in v220, ffts multi-core synchronization case has ffts_addr in, so need Unified format build
        if util.is_v220() and in_dynamic():
            add_build_arg("enforce_mix_mode", True)

    @staticmethod
    def _add_block_sync_flag(tiling_key):
        # for batch mode flag when dynamic schedule
        get_context().get_current_compute().get_current_schedule().add("_block_sync", tiling_key)

    def get_all_producers_stages(self, tensor):
        """
        get all produce stages for current tensor
        """
        producers = set()
        for producer in self.backward_stage_graph_map[tensor]:
            producers.add(producer)
            producers.update(self.get_all_producers_stages(producer))
        return producers

    def _init_reduce_attr(self):
        # disable_reduce_tmp_buf 为了性能考虑，不使用scalar，直接vcadd到目的地址。
        # 因此，使用 disable_reduce_tmp_buf 需要满足只在目的地址上累加一次
        self.ar_reduce_attr = {
            "reduce_opt_mode": "entire_reduce",
            "storage_bound": self.tiling_case.tensor_ub_size_before_reduce,
            "big_dim_ub_factor": self.tiling_case.tensor_ub_size_before_reduce,
            "disable_reduce_tmp_buf": 1
            }
        self.ar_reduce_attr_scalar = {
            "reduce_opt_mode": "entire_reduce",
            "storage_bound": self.tiling_case.tensor_ub_size_before_reduce,
            "big_dim_ub_factor": self.tiling_case.tensor_ub_size_before_reduce
            }

        self.ara_reduce_attr = {
            "storage_bound": self.tiling_case.tensor_ub_size_before_reduce,
            "reduce_opt_mode": "dichotomy_reduce",
            "reuse_src_tensor": True,
            "nlast_reduce_dichotomy": 16
            }
        self.ara_reduce_big_attr = {
            "storage_bound": self.tiling_case.tensor_ub_size_before_reduce,
            "reduce_opt_mode": "dichotomy_reduce_big_dim",
            "reuse_src_tensor": True,
            "nlast_reduce_dichotomy": 16
            }

    def _create_schedule(self):
        self.sch = tvm.create_schedule([out.op for out in self.graph_info.schedule_tvm_out_set])

    def _do_cache_read(self):
        for cache_read_tensor in self.graph_info.input_tensor_set:
            read_buffer = self.sch.cache_read(
                cache_read_tensor, LOCAL_UB, self.forward_stage_graph_map.get(cache_read_tensor))
            self.update_stage(read_buffer, cache_read_tensor, False)
            self.cache_read_tensors_and_buffer_map[cache_read_tensor] = read_buffer

    def _do_cache_write(self):
        for cache_write_tensor in self.real_pure_output:
            write_buffer = self.sch.cache_write(cache_write_tensor, LOCAL_UB)
            self.update_stage(write_buffer, cache_write_tensor, True)
            self.cache_write_tensors_and_buffer_map[cache_write_tensor] = write_buffer

    def _do_mid_output_tensor_process(self):
        for single_mid_output_tensor in self.graph_info.trunk_mid_output_tensor_set | \
                                        self.graph_info.branch_mid_output_tensor_set:
            write_buffer = self.sch.cache_write(single_mid_output_tensor, LOCAL_UB)
            self.update_stage(write_buffer, single_mid_output_tensor, True)
            self.cache_write_tensors_and_buffer_map[single_mid_output_tensor] = write_buffer
            read_buffer = self.sch.cache_read(
                single_mid_output_tensor, LOCAL_UB, self.forward_stage_graph_map.get(single_mid_output_tensor))
            self.update_stage(read_buffer, single_mid_output_tensor, False)
            self.cache_read_tensors_and_buffer_map[single_mid_output_tensor] = read_buffer
            self.mid_output_tensor_cache_read_list.append(read_buffer)

            self.sch[write_buffer].reused_by(read_buffer)
            self.sch[read_buffer].reused_by(reuse_data=True)

    def _set_scope(self):
        for pure_mid_tensor in self.mid_tensor_set - self.graph_info.real_output_tensor_set:
            self.sch[pure_mid_tensor].set_scope(LOCAL_UB)

    def _init_branch_info(self):
        self.branch_tensor_info = BranchTensorInfo(self.graph_info,
                                                   self.sch,
                                                   self.cache_read_tensors_and_buffer_map,
                                                   self.cache_write_tensors_and_buffer_map)

    def _collect_tensors(self):
        self.reduce_tensor = self.cache_write_tensors_and_buffer_map.get(self.reduce_info.reduce_tensor,
                                                                         self.reduce_info.reduce_tensor)
        self.res_tensor = tuple(self.graph_info.endpoint_output_tensor_set)[0]

        trunk_tensor_before_reduce = self.get_all_producers_stages(self.reduce_tensor)
        self.tensors_before_reduce = trunk_tensor_before_reduce.union(
            self.branch_tensor_info.get_all_branch_stages(self.backward_stage_graph_map))
        all_trunk_tensors = self.get_all_producers_stages(self.res_tensor)
        self.tensors_after_reduce = all_trunk_tensors - trunk_tensor_before_reduce - {self.reduce_tensor}

    def _do_input_db(self):
        if self.graph_info.reduce_parents_list is None:
            return

        # when use entire_reduce, input.local.ub maybe reused by other tensor, use mem_unique to avoid reuse
        # vector_info.py 中 _calc_small_ubsize 已经为 reduce_parents_list 额外预留了存活节点
        for tensor in self.graph_info.reduce_parents_list:
            if tensor in self.cache_read_tensors_and_buffer_map.keys():
                cache_read_tensor = self.cache_read_tensors_and_buffer_map.get(tensor)
                self.sch[cache_read_tensor].mem_unique()

    def _do_reorder_ar(self, tensor):
        reordered_axis_list = []
        reordered_axis_list.extend([x for x in list(self.sch[tensor].op.axis)[:]])
        reordered_axis_list.extend([x for x in list(self.sch[tensor].op.reduce_axis)[:]])
        self.sch[tensor].reorder(*reordered_axis_list)

    def _do_set_ub_constraint_ar(self):
        ub_size = self.tiling_case.tensor_ub_size_before_reduce
        first_stage_shape_in_ub = 1
        for index, value in enumerate(self.reduce_info.shape_before_reduce):
            if index < self.tiling_case.ub_split_axis_index:
                continue
            if index == self.tiling_case.ub_split_axis_index:
                self.sch.set_constraint(self.ub_split_result.get("factor") <= ub_size)
                first_stage_shape_in_ub *= self.ub_split_result.get("factor")
            else:
                self.sch.set_constraint(value <= ub_size)
                first_stage_shape_in_ub *= value

        self.sch.set_constraint(first_stage_shape_in_ub <= ub_size)

    def _do_storage_align(self):
        for single_tensor, param in self.storage_align_map.items():
            self.sch[single_tensor].storage_align(param[0], param[1], param[2])

    def _do_compute_align(self):
        for single_tensor, param in self.compute_align_map.items():
            self.sch[single_tensor].compute_align(param[0], param[1], param[2])

    def _do_bind_buffer(self):
        for single_tensor, param in self.bind_buffer_map.items():
            self.sch[single_tensor].bind_buffer(param[0], param[1], param[2])

    def _do_compute_at(self):
        for stage in self.compute_at_map:
            parent_stage = self.compute_at_map.get(stage).get(PARENT)
            scope_iter_var = self.compute_at_map.get(stage).get(SCOPE)
            self.sch[stage].compute_at(parent_stage, scope_iter_var)

    def _do_reverse_compute_at(self):
        self.branch_tensor_info.do_reverse_compute_at()

    def _do_emit_insn(self):
        for stage in self.emit_insn_map:
            scope_iter_var = self.emit_insn_map.get(stage).get(SCOPE)
            instruction = self.emit_insn_map.get(stage).get(INSTRUCTION)
            attr_map = self.emit_insn_map.get(stage).get(ATTRS)

            if attr_map:
                self.sch[stage].emit_insn(scope_iter_var, instruction, attrs=attr_map)
            else:
                self.sch[stage].emit_insn(scope_iter_var, instruction)

    def update_stage(self, source_tensor, dst_tensor, before):
        """
        update  graph stage map by new tensor
        """
        if before:
            self.forward_stage_graph_map.setdefault(source_tensor, set())
            self.backward_stage_graph_map.setdefault(source_tensor, set())
            for producer in tuple(self.backward_stage_graph_map[dst_tensor]):
                self.forward_stage_graph_map[producer].remove(dst_tensor)
                self.forward_stage_graph_map[producer].add(source_tensor)
                self.backward_stage_graph_map[dst_tensor].remove(producer)
                self.backward_stage_graph_map[source_tensor].add(producer)
            self.forward_stage_graph_map[source_tensor].add(dst_tensor)
            self.backward_stage_graph_map[dst_tensor].add(source_tensor)
        else:
            self.forward_stage_graph_map.setdefault(source_tensor, set())
            self.backward_stage_graph_map.setdefault(source_tensor, set())
            for consumer in tuple(self.forward_stage_graph_map[dst_tensor]):
                self.forward_stage_graph_map[dst_tensor].discard(consumer)
                self.backward_stage_graph_map[consumer].discard(dst_tensor)
                self.backward_stage_graph_map[consumer].add(source_tensor)
                self.forward_stage_graph_map[source_tensor].add(consumer)
            self.forward_stage_graph_map[dst_tensor].add(source_tensor)
            self.backward_stage_graph_map[source_tensor].add(dst_tensor)

    def _find_none_reduce_axis_map(self):
        none_reduce_index_map = {}
        if self.reduce_info.keepdims:
            for i in range(0, len(self.reduce_info.shape_before_reduce)):
                if i not in self.reduce_info.reduce_axis_indexes:
                    none_reduce_index_map[i] = i
        else:
            count = 0
            for i in range(0, len(self.reduce_info.shape_before_reduce)):
                if i not in self.reduce_info.reduce_axis_indexes:
                    none_reduce_index_map[i] = count
                    count += 1

        return none_reduce_index_map
