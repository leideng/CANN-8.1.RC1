#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright(C) 2022. Huawei Technologies Co., Ltd. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Information containers for compute graph of transdata
"""
import operator
from functools import reduce
from typing import Dict, List, Set

from tbe import tvm
from tbe.tvm import Tensor
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.common.utils.errormgr import get_error_message
from tbe.common.platform.platform_info import get_soc_spec
from tbe.dsl.classifier.transdata.constants import DATA_TYPE_SIZE
from tbe.dsl.classifier.transdata.constants import DO_TRANSPOSE_PAD
from tbe.dsl.classifier.transdata.constants import COMMON_ALIGN_NEED_NODES
from tbe.dsl.classifier.transdata.constants import UB_CATEGORY_BH, UB_CATEGORY_BN
from tbe.dsl.classifier.transdata.constants import UB_CATEGORY_GENERAL, UB_CATEGORY_DATA_MOVE
from tbe.dsl.classifier.transdata.constants import UB_CATEGORY_DATA_MOVE_C04
from tbe.dsl.classifier.transdata.constants import UB_CATEGORY_REMOVE_SIZE_ONE_AXIS
from tbe.dsl.classifier.transdata.constants import GENERAL_BRANCH, DATA_MOVE_BRANCH
from tbe.dsl.classifier.transdata.constants import BN_BRANCH
from tbe.dsl.classifier.transdata.constants import RESERVED_SPACE
from tbe.dsl.classifier.transdata.constants import get_reshape
from .graph import Graph


class ComputeGraphInfo(Graph):
    """
    Operator Compute Graph Info collector and container
    """

    def __init__(self, output_tensors):
        Graph.__init__(self, output_tensors)
        self.c0 = None
        self.c1c0: List = list()
        self.x1x0: List = list()
        self.permute: List = list()
        self.soc_ub_size = get_soc_spec("UB_SIZE")

        self.reshape = None
        self.category = None
        self.is_forward = False
        self.is_last_transpose = False
        self.src_pad_mode = None
        self.src_pad_var = None
        self.tiling_tensor = None
        self.transpose_tensor_list = list(self.transpose_tensor_set)
        self._collect_transdata()

    @staticmethod
    def set_buffer_size(graph_info, tiling_case):
        def calc_value(_nodes):
            value = 0
            for v in _nodes:
                value += DATA_TYPE_SIZE[v]
            return value

        node_list = graph_info.exist_nodes
        max_value = max([calc_value(node) for node in node_list])
        soc_ub_size = graph_info.soc_ub_size - RESERVED_SPACE
        if tiling_case.ub_category in [UB_CATEGORY_GENERAL, UB_CATEGORY_DATA_MOVE,
                                       UB_CATEGORY_DATA_MOVE_C04, UB_CATEGORY_REMOVE_SIZE_ONE_AXIS]:
            # sch maybe choose storage_align (shape_type is 0)
            tiling_case.tensor_ub_size_list.append(soc_ub_size // max_value // 128 * 128)
            # sch maybe choose common_align (shape_type is 1)
            common_align = COMMON_ALIGN_NEED_NODES * DATA_TYPE_SIZE[node_list[0][0]]
            common_align = common_align if common_align > max_value else max_value
            tiling_case.tensor_ub_size_list.append(soc_ub_size // common_align // 128 * 128)
        elif tiling_case.ub_category in [UB_CATEGORY_BN, UB_CATEGORY_BH]:
            # sch only choose storage_align in bn\bh(shape_type is 0)
            tiling_case.tensor_ub_size_list.append(soc_ub_size // max_value // 128 * 128)
        else:
            dict_args = {"errCode": "E90001", "detailed_cause": "ub_category doesn't belong to [0,1,2,3]"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    @staticmethod
    def set_map_deepcopy(_map: Dict[Tensor, Set[Tensor]]) -> Dict[Tensor, Set[Tensor]]:
        """
        deep copy tensor map
        """
        return {key: _map[key].copy() for key in _map}

    def _collect_transdata(self):
        current_compute = get_context().get_current_compute()
        self.category = current_compute.get("_transdata_category")
        self.is_last_transpose = get_compile_info().get("_is_ori_last_transpose")
        self.is_forward = True if self.category.find("forward") != -1 else False

        if self.category in GENERAL_BRANCH + DATA_MOVE_BRANCH:
            self._general_collect()
        else:
            self._sort_transpose_op()
            self._bn_collect() if self.category in BN_BRANCH else self._bh_collect()

        # data-move c1c0 don't swaps that mode is 1.
        self.tiling_tensor = self.transpose_tensor_list[-1]
        self.src_pad_mode = current_compute.get("_src_pad_mode")
        self.src_pad_var = current_compute.get("_src_pad_var")
        if self.src_pad_mode and DO_TRANSPOSE_PAD in self.src_pad_mode:
            self.c0 = self.src_pad_var[self.src_pad_mode.index(DO_TRANSPOSE_PAD)]
        if self.category in DATA_MOVE_BRANCH:
            self.src_pad_mode = current_compute.get("_data_move_src_pad_mode")
            self.src_pad_var = current_compute.get("_data_move_src_pad_var")

    def _general_collect(self):
        """
        Info from general computation:
        1. permute: order of transpose.
        2. is_last_transpose: one kind of transpose.
        3. c1c0: index of c1c0 based on the tensor.
        """
        self.permute = [int(x) for x in list(self.transpose_tensor_list[-1].op.attrs["permute"])]
        self.is_last_transpose = self.permute[-1] != len(self.permute) - 1

        if self.is_forward:
            self.reshape = get_reshape(list(self.s_reshape_tensor_set)[0])
        else:
            self.reshape = get_reshape(list(self.f_reshape_tensor_set)[0])

        for item in self.reshape:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                self.c1c0 = item if not self.is_forward else [self.permute.index(x) for x in item]

    def _bn_collect(self):
        """
        Info from bn-computation:
        1. In backward, tiling would not split C (combined by c1 and c0), find C based on t2.
        2. In forward, tiling would not split c1 and c0, find c1\c0 based on t2.
        Backward: (N,H,C1*C0,16) -<f_reshape>- (N,H,C,16) -<transpose>- (N,16,H,C)
        Forward: (N,H,C1,C0,16) -<transpose>- (N,C1,H,C0,16) -<transpose>- (N,16,C1,H,C0)
        """
        t_0, t_1, t_2 = self.transpose_tensor_list
        if not self.is_forward:
            f_reshape = list(self.tensor_consumers_map.get(t_1, None))[0]
            self.c1c0 = find_reshape_point(f_reshape, is_forward=False)
            for tensor in [t_2, ]:
                perm = [int(x) for x in tensor.op.attrs["permute"]]
                self.c1c0 = [perm.index(x) for x in self.c1c0]
        else:
            s_reshape = list(self.tensor_producers_map.get(t_1, None))[0]
            self.c1c0 = find_reshape_point(s_reshape)
            for tensor in [t_1, t_2]:
                perm = [int(x) for x in tensor.op.attrs["permute"]]
                self.c1c0 = [perm.index(x) for x in self.c1c0]

        self.x1x0 = [0, 1]
        add_compile_info_inner("_bn_x1x0", self.x1x0)
        add_compile_info_inner("_bn_c1c0", self.c1c0)
        add_compile_info_inner("_bn_permute", [int(x) for x in t_2.op.attrs["permute"]])

    def _bh_collect(self):
        """
        Info from bh-computation:
        1. Forward: tiling would not split c1\c0\h0, find c1\c0\h1\h0 based on t2
        2. Backward: tiling would not split c\h0, find c, h1\h0 based on t2.
        """
        t_0, t_1, t_2 = self.transpose_tensor_list
        if self.is_forward:
            s_reshape = list(self.tensor_producers_map.get(t_1, None))[0]
            self.c1c0 = find_reshape_point(s_reshape)
            for tensor in [t_1, t_2]:
                perm = [int(x) for x in tensor.op.attrs["permute"]]
                self.c1c0 = [perm.index(x) for x in self.c1c0]

            f_reshape = list(self.tensor_consumers_map.get(t_2, None))[0]
            self.x1x0 = find_reshape_point(f_reshape)
        else:
            f_reshape = list(self.tensor_consumers_map.get(t_1, None))[0]
            self.c1c0 = find_reshape_point(f_reshape, is_forward=False)
            for tensor in [t_2, ]:
                perm = [int(x) for x in tensor.op.attrs["permute"]]
                self.c1c0 = [perm.index(x) for x in self.c1c0]
            for k, v in enumerate(perm):
                if k != v:
                    self.x1x0 = [k, k + 1]
                    break
        add_compile_info_inner("_bh_x1x0", self.x1x0)
        add_compile_info_inner("_bh_c1c0", self.c1c0)
        add_compile_info_inner("_bh_permute", [int(x) for x in t_2.op.attrs["permute"]])

    def _sort_transpose_op(self):
        # TransposeMode: last + n-last + last in BN\BH
        t_0, t_1, t_2 = None, None, None
        for _tensor in self.transpose_tensor_set:
            perm = [int(x) for x in _tensor.op.attrs["permute"]]
            if perm[-1] == len(perm) - 1:
                t_1 = _tensor
                break

        producers = self.get_all_producers_computes(t_1)
        for _tensor in self.transpose_tensor_set:
            if _tensor in producers:
                t_0 = _tensor
                break

        t_2 = list(self.transpose_tensor_set.difference({t_0, t_1}))[0]
        self.transpose_tensor_list = [t_0, t_1, t_2]


def choose_transpose_insn(perm):
    """
    According to the perm to decide use which instruction
    """
    emit_idx = 0
    if not perm:
        insn = "phony_insn"
    elif perm[-1] != len(perm) - 1:
        insn = "vector_transpose"
    else:
        insn = "dma_copy"
    return emit_idx, insn


def find_reshape_point(tensor_, is_forward=True):
    """
    Record point of reshape based on tensor_, just like c to [c1,c0] eg:
    -- [0, 1, [2, 3],   4] -- axes
    -- [N, H, [C1, C0], W] -- src
    -- [N, H,    C,     W] -- dst
    """
    result = []
    axes = get_reshape(tensor_)
    for k, v in enumerate(axes):
        if isinstance(v, (list, tuple)) and len(v) == 2:
            if is_forward:
                result.extend(v)
            else:
                result.append(k)
    return result


def ceil_div(dim, factor):
    # CeilDiv
    return tvm.floordiv(dim + factor - 1, factor)


def set_align(dim, factor):
    # Align dim by factor
    return ceil_div(dim, factor) * factor


def math_prod(iterable):
    # Prod for iter
    return reduce(operator.mul, iterable, 1)
