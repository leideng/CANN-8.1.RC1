#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
graph info
"""
from typing import Dict
from typing import Iterable
from typing import List
from typing import Optional
from typing import Type
from typing import Union

from tbe import tvm
from tbe.dsl.base.operation import get_op_context
from tbe.tvm import PlaceholderOp
from tbe.tvm import Tensor

from . import soc_info
from . import tensor_info
from .. import constants
from .. import helper
from ..... import constants as unify_constants
from ..... import util as unify_helper


def fake_node_compute(tensors: Iterable[Tensor]) -> Tensor:
    helper.check_true(
        helper.judge_iterable(tensors) and len(tensors) > 1,
        "tensors in func fake_node_compute must be a iterable container with more than 1 length, please check."
    )
    max_dtype = helper.get_tvm_tensor_max_dtype(tensors)
    max_shape = helper.get_tvm_tensor_max_shape(tensors)

    def _compute(*indexes):
        _res = tvm.const(1, max_dtype)
        for _tensor in tensors:
            _cur_indexes = []
            _diff_length = len(max_shape) - len(_tensor.shape)
            for _idx, _ in enumerate(_tensor.shape):
                _cur_indexes.append(indexes[_idx + _diff_length])
            _res *= tvm.expr.Cast(max_dtype, _tensor(*_cur_indexes))

        return _res

    with tvm.tag_scope(unify_constants.FAKE_NODE_TAG):
        res = tvm.compute(max_shape, _compute, name="fake_node")

    return res


class GraphInfo:
    def __init__(self, output_tensors: Union[Iterable[Tensor], Tensor]):
        self._tensor_obj_list: Optional[List[Type[tensor_info.TensorBase]]] = None
        self._res_tensor_obj: Optional[Type[tensor_info.TensorBase]] = None
        self._consumer_tensor_objs_map: Optional[Dict[Type[tensor_info.TensorBase], List]] = None
        self._producer_tensor_objs_map: Optional[Dict[Type[tensor_info.TensorBase], List]] = None

        self._collect_info(output_tensors)

        self._union_reduce_axes_indices = self._get_union_reduce_axes_indices()
        self._max_dtype = self._get_max_dtype()
        self._max_shape = self._get_max_shape()
        self._is_exist_output_after_reduce = self._judge_exist_output_after_reduce()
        self._max_block_ele_num, self._output_max_block_ele_num = self._get_max_block_ele_num()

        self._compute_type = self._parse_compute_type()
        self._check_graph()

    @property
    def tensor_obj_list(self):
        return self._tensor_obj_list

    @property
    def consumer_tensor_objs_map(self):
        return self._consumer_tensor_objs_map

    @property
    def producer_tensor_objs_map(self):
        return self._producer_tensor_objs_map

    @property
    def res_tensor_obj(self):
        return self._res_tensor_obj

    @property
    def union_reduce_axes_indices(self):
        return self._union_reduce_axes_indices

    @property
    def max_dtype(self):
        return self._max_dtype

    @property
    def max_shape(self):
        return self._max_shape

    @property
    def is_exist_output_after_reduce(self):
        return self._is_exist_output_after_reduce

    @property
    def max_block_ele_num(self):
        return self._max_block_ele_num

    @property
    def max_output_block_ele_num(self):
        return self._output_max_block_ele_num

    @property
    def compute_type(self):
        return self._compute_type

    @staticmethod
    def dfs_compute_graph(root_tensor_list):
        def _recursive_func(_root_tensor_obj, _visited_list, _consumer_tensor_objs_map, _producer_tensor_objs_map):
            helper.ListHelper.add(_visited_list, _root_tensor_obj)
            _consumer_tensor_objs_map.setdefault(_root_tensor_obj, [])
            _producer_tensor_objs_map.setdefault(_root_tensor_obj, [])
            for _in_tensor in _root_tensor_obj.tvm_tensor.op.input_tensors:
                _tensor_obj = tensor_info.gen_tensor_obj(_in_tensor, tensor_info.get_tensor_type(_in_tensor))
                _consumer_tensor_objs_map.setdefault(_tensor_obj, [])
                helper.ListHelper.add(_consumer_tensor_objs_map.get(_tensor_obj), _root_tensor_obj)
                helper.ListHelper.add(_producer_tensor_objs_map.get(_root_tensor_obj), _tensor_obj)
                _recursive_func(_tensor_obj, _visited_list, _consumer_tensor_objs_map, _producer_tensor_objs_map)

        visited_list = []
        consumers_tensor_obj_map = {}
        producers_tensor_obj_map = {}

        for tensor in root_tensor_list:
            tensor_obj = tensor_info.gen_tensor_obj(tensor, tensor_info.get_tensor_type(tensor))
            _recursive_func(tensor_obj, visited_list, consumers_tensor_obj_map, producers_tensor_obj_map)

        return visited_list, consumers_tensor_obj_map, producers_tensor_obj_map

    def _collect_info(self, output_tensors):
        if not helper.judge_iterable(output_tensors):
            output_tensors = [output_tensors]
        self._tensor_obj_list, self._consumer_tensor_objs_map, self._producer_tensor_objs_map = \
            self.dfs_compute_graph(output_tensors)

        for tensor_obj in self._tensor_obj_list:
            if isinstance(tensor_obj.tvm_tensor.op, PlaceholderOp):
                tensor_obj.flags.placeholder_flag = True
            if tensor_obj.tvm_tensor in output_tensors:
                tensor_obj.flags.real_output_flag = True
            if self._consumer_tensor_objs_map.get(tensor_obj) and not tensor_obj.flags.placeholder_flag:
                tensor_obj.flags.middle_flag = True

        real_non_middle_output_tensor_obj_list = [t for t in self._tensor_obj_list if t.is_real_non_middle_output()]

        if len(real_non_middle_output_tensor_obj_list) > 1:
            fake_out = fake_node_compute([t.tvm_tensor for t in real_non_middle_output_tensor_obj_list])
            fake_out_tensor_obj = tensor_info.gen_tensor_obj(fake_out, constants.NodeType.COMMON)
            fake_out_tensor_obj.flags.fake_node_flag = True
            # update
            self._tensor_obj_list.append(fake_out_tensor_obj)
            for tensor_obj in real_non_middle_output_tensor_obj_list:
                tensor_obj.flags.middle_flag = True
                self._consumer_tensor_objs_map[tensor_obj] = [fake_out_tensor_obj]
            self._producer_tensor_objs_map[fake_out_tensor_obj] = real_non_middle_output_tensor_obj_list
            self._consumer_tensor_objs_map[fake_out_tensor_obj] = []
            fake_out_tensor_obj.flags.endpoint_flag = True
            self._res_tensor_obj = fake_out_tensor_obj
        else:
            real_non_middle_output_tensor_obj_list[0].flags.endpoint_flag = True
            self._res_tensor_obj = real_non_middle_output_tensor_obj_list[0]

    def _get_union_reduce_axes_indices(self):
        union_reduce_axes_indices = []
        for reduce_tensor_obj in [t for t in self.tensor_obj_list if t.node_type == constants.NodeType.REDUCE]:
            union_reduce_axes_indices = \
                helper.ListHelper.get_union(union_reduce_axes_indices, reduce_tensor_obj.reduce_axes_indices)
        union_reduce_axes_indices.sort()

        return union_reduce_axes_indices

    def _get_max_dtype(self):
        return helper.get_tvm_tensor_max_dtype([t.tvm_tensor for t in self._tensor_obj_list])

    def _get_max_shape(self):
        return helper.get_tvm_tensor_max_shape([t.tvm_tensor for t in self._tensor_obj_list])

    def _judge_exist_output_after_reduce(self):
        is_exist_output_after_reduce = False
        after_reduce_shape_list = [t.shape for t in self.tensor_obj_list if t.node_type == constants.NodeType.REDUCE]
        for tensor_obj in self._tensor_obj_list:
            if not tensor_obj.is_real_output():
                continue
            if tensor_obj.shape in after_reduce_shape_list:
                is_exist_output_after_reduce = True

        return is_exist_output_after_reduce

    def _get_max_block_ele_num(self):
        max_block_ele_num = 1
        output_max_block_ele_num = 1
        for tensor_obj in self._tensor_obj_list:
            cur_block_ele_num = soc_info.SocInfo.get_block_ele_num(tensor_obj.dtype)
            if tensor_obj.is_real_output():
                output_max_block_ele_num = max(cur_block_ele_num, output_max_block_ele_num)
            max_block_ele_num = max(cur_block_ele_num, max_block_ele_num)

        return max_block_ele_num, output_max_block_ele_num

    def _parse_compute_type(self) -> constants.ComputeType:
        if self._is_reduce_aggregate():
            return constants.ComputeType.REDUCE_AGGREGATE

        return constants.ComputeType.COMMON

    def _is_reduce_aggregate(self):
        reduce_tensor_obj_list = [t for t in self.tensor_obj_list if t.node_type == constants.NodeType.REDUCE]
        if len(reduce_tensor_obj_list) != 1:
            return False

        reduce_tensor_obj = reduce_tensor_obj_list[0]
        if not helper.judge_the_equality_of_tvm_shape(reduce_tensor_obj.shape, self._res_tensor_obj.shape):
            return False

        after_reduce_tensor_obj_list = []
        helper.traverse_obj(reduce_tensor_obj, after_reduce_tensor_obj_list,
                            lambda x: self._consumer_tensor_objs_map.get(x))
        no_broadcast = not any(t.node_type == constants.NodeType.BROADCAST for t in after_reduce_tensor_obj_list)

        return no_broadcast

    def _check_graph(self):
        def _check_shape():
            is_last_dim_all_one = True
            for tensor_obj in self._tensor_obj_list:
                last_dim = tensor_obj.shape[-1]
                if last_dim == 1:
                    continue
                elif last_dim % self.max_block_ele_num == 0:
                    is_last_dim_all_one = False
                else:
                    return False

            if is_last_dim_all_one:
                return False

            return not is_last_dim_all_one

        def _check_reduce():
            before_reduce_tensor_shape = None
            after_reduce_tensor_shape = None
            reduce_tensor_obj_list = [t for t in self.tensor_obj_list if t.node_type == constants.NodeType.REDUCE]
            has_false_keep_dims = False
            for reduce_tensor_obj in reduce_tensor_obj_list:
                if not reduce_tensor_obj.keep_dims:
                    has_false_keep_dims = True
                if before_reduce_tensor_shape is None:
                    before_reduce_tensor_shape = tuple(reduce_tensor_obj.src_shapes[0])
                elif before_reduce_tensor_shape != tuple(reduce_tensor_obj.src_shapes[0]):
                    return False
                if after_reduce_tensor_shape is None:
                    after_reduce_tensor_shape = tuple(reduce_tensor_obj.shape)
                elif after_reduce_tensor_shape != tuple(reduce_tensor_obj.shape):
                    return False

            if not reduce_tensor_obj_list:
                return False

            if has_false_keep_dims:
                if self._compute_type == constants.ComputeType.COMMON:
                    return False
                else:
                    after_reduce_tensor_obj_list = []
                    helper.traverse_obj(reduce_tensor_obj, after_reduce_tensor_obj_list,
                                        lambda x: self._consumer_tensor_objs_map.get(x))

                    all_single_input = \
                        all(t.src_shapes and len(t.src_shapes) == 1 for t in after_reduce_tensor_obj_list)
                    if not all_single_input:
                        return False

            for output_tensor_obj in [t for t in self.tensor_obj_list if t.is_real_output()]:
                is_illegal_output_shape = not soc_info.SocInfo.soc_support_less_one_block_mte() and \
                    tuple(output_tensor_obj.shape) not in (before_reduce_tensor_shape, after_reduce_tensor_shape)
                if is_illegal_output_shape:
                    return False

            return tuple(self.max_shape) == before_reduce_tensor_shape

        def _check_dtype():
            return all(t.dtype != "int64" for t in self._tensor_obj_list)

        if not _check_shape():
            helper.report_fusion_check_result(check_result=False)
            helper.raise_error("currently, the shapes are not supported in generic_vector")

        if not _check_reduce():
            helper.report_fusion_check_result(check_result=False)
            helper.raise_error("currently, the reduce tensors are not supported in generic_vector")

        if not _check_dtype():
            helper.report_fusion_check_result(check_result=False)
            helper.raise_error("currently, the dtypes are not supported in generic_vector")

        if get_op_context() and get_op_context().get_op_mode() != "static":
            helper.report_fusion_check_result(check_result=False)
            helper.raise_error("currently, dynamic shape is not supported in generic_vector")
