#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2021. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
ascend_anti_quant tiling case
"""
from typing import Any
from typing import Set
from typing import Dict
from typing import Iterable
from typing import List
from typing import Optional
from typing import Tuple
from typing import Union
from typing import Callable
from tbe.tvm import Tensor
from tbe.tvm import PlaceholderOp
from tbe.dsl.base.operation import register_tiling_case
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import get_compile_info
from tbe.common.platform import get_soc_spec
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from . import util
from .constants import Pattern
from ...common.utils.errormgr import get_error_message


class QuantComputeGraphInfo:
    """
    Operator Compute Graph Info collector and container
    """

    def __init__(self, output_tensors: Iterable[Tensor], quant_type="AscendAntiQuant"):
        """
        Initialize containers and try to collect info
        """
        # Basic Info
        self.output_tensor_set: Optional[Set[Tensor]] = set(output_tensors)
        self.tensor_consumers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_producers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_list: Optional[List[Tensor]] = None

        # Extra info initialized by hooks
        self.elewise_tensor_set: Set[Tensor] = set()
        self.quant_tensor_set: Set[Tensor] = set()
        self.input_tensor_set: Set[Tensor] = set()
        self.broadcast_tensor_set: Set[Tensor] = set()
        self.non_gm_input_tensor_set: Set[Tensor] = set()

        # Extra info initialized after pre-initialization
        self.mid_output_tensor_set: Set[Tensor] = set()
        self.mid_tensor_set: Set[Tensor] = set()
        self.endpoint_output_tensor_set: Set[Tensor] = set()

        # antiquant fuse info
        self.input_ub_tensor_set: Set[Tensor] = set()
        self.buffer_align_tensor_set: Set[Tensor] = set()
        self.reorder_tensor_set: Set[Tensor] = set()

        self.quant_type = quant_type
        self.quant_fuse = False
        self.elewise_fuse = False

        # ub info
        self.max_single_tensor_ub_size: Optional[int] = None

        # Do info collection
        self.collect_info()
        self.gen_fuse_flag()
        self.init_max_ub_count()

    @staticmethod
    def dfs_compute_graph(root_tensor: Union[Iterable[Tensor], Tensor],
                          hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                             Callable[[Tensor], Any],
                                             Callable[[Tensor], Any]], ...]):
        def recursive_func(_root_tensor: Tensor,
                           _visited_list: Set[Tensor],
                           _tensor_consumers_map: Dict[Tensor, Union[Set[Tensor]]],
                           _tensor_producers_map: Dict[Tensor, Union[Set[Tensor]]],
                           _hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                               Callable[[Tensor], Any],
                                               Callable[[Tensor], Any]], ...]):
            _visited_list.add(_root_tensor)
            _tensor_producers_map.setdefault(_root_tensor, set())
            _tensor_consumers_map.setdefault(_root_tensor, set())

            for hook in hooks:
                if hook[0](_root_tensor):
                    hook[1](_root_tensor)
                else:
                    hook[2](_root_tensor)

            for in_tensor in _root_tensor.op.input_tensors:
                _tensor_consumers_map.setdefault(in_tensor, set())
                _tensor_consumers_map[in_tensor].add(_root_tensor)
                _tensor_producers_map[_root_tensor].add(in_tensor)
                recursive_func(in_tensor,
                               _visited_list,
                               _tensor_consumers_map,
                               _tensor_producers_map,
                               _hooks)

        visited_list = set()
        tensor_consumers_map = {}
        tensor_producers_map = {}
        if isinstance(root_tensor, (list, tuple, set)):
            for tensor in root_tensor:
                recursive_func(tensor,
                               visited_list,
                               tensor_consumers_map,
                               tensor_producers_map,
                               hooks)
        elif isinstance(root_tensor, Tensor):
            recursive_func(root_tensor,
                           visited_list,
                           tensor_consumers_map,
                           tensor_producers_map,
                           hooks)
        else:
            raise RuntimeError("dfs_compute_graph supports [list,tuple,Tensor]. Received %s" % str(type(root_tensor)))
        return list(visited_list), tensor_consumers_map, tensor_producers_map

    def gen_mid_tensor_sets(self):
        # mid_output_tensor_set
        # mid_tensor_set
        for tensor in self.tensor_list:
            if tensor in self.output_tensor_set and self.tensor_consumers_map.get(tensor):
                # Tensor in output and has consumers is middle_out_tensor
                self.mid_output_tensor_set.add(tensor)
                self.mid_tensor_set.add(tensor)
            elif tensor not in self.output_tensor_set | self.input_tensor_set | self.non_gm_input_tensor_set:
                self.mid_tensor_set.add(tensor)

    def gen_endpoint_output_tensor_set(self):
        for output_tensor in self.output_tensor_set:
            if not self.tensor_consumers_map.get(output_tensor):
                self.endpoint_output_tensor_set.add(output_tensor)

    def gen_quant_tensor_set(self):
        for tensor in self.tensor_list:
            # quant compute inline
            if tensor.op.tag in ["input_ub"]:
                self.input_ub_tensor_set.add(tensor)
            # antiquant buffer align
            if tensor.op.tag in ["anti_quant_input_ub", "anti_quant_cast_f16_ub"]:
                self.buffer_align_tensor_set.add(tensor)
            # quant reorder
            if tensor.op.tag in ["anti_quant_reform_by_vmuls", "reform_by_vmuls", "reform_by_vadds"]:
                self.reorder_tensor_set.add(tensor)

    def collect_info(self):
        self.tensor_list, self.tensor_consumers_map, self.tensor_producers_map = \
            self.dfs_compute_graph(self.output_tensor_set,
                                   (  # self.input_tensor_set hook
                                       (lambda _tensor: isinstance(_tensor.op, PlaceholderOp),
                                        lambda _tensor: self.input_tensor_set.add(_tensor),
                                        lambda _tensor: self.non_gm_input_tensor_set.add(_tensor)
                                        if not _tensor.op.input_tensors else None),

                                       # self.quant_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag == "quant",
                                        lambda _tensor: self.quant_tensor_set.add(_tensor),
                                        lambda _tensor: None),

                                       # self.broadcast_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find("broadcast") != -1,
                                        lambda _tensor: self.broadcast_tensor_set.add(_tensor),
                                        lambda _tensor: None),

                                       # self.elewise_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find("elewise") != -1,
                                        lambda _tensor: self.elewise_tensor_set.add(_tensor),
                                        lambda _tensor: None)
                                   ))

        self.gen_mid_tensor_sets()

        self.gen_endpoint_output_tensor_set()

        self.gen_quant_tensor_set()

    def gen_fuse_flag(self):
        if self.quant_tensor_set:
            self.quant_fuse = True
        if self.elewise_tensor_set:
            self.elewise_fuse = True

    def init_max_ub_count(self):
        soc_ub_size = get_soc_spec("UB_SIZE")
        soc_ub_size = soc_ub_size // 2

        total_width = 2
        if self.quant_type == "anti_quant" and self.elewise_fuse:
            total_width = 4

        max_bound = total_width * 128
        max_ub_count = int(soc_ub_size // max_bound * 128)

        self.max_single_tensor_ub_size = max_ub_count


class QuantTilingCase:
    def __init__(self):
        self.block_tiling_axis = None
        self.ub_tiling_axis = None
        self.multi_core = True
        self.is_split_ub = None
        self.tiling_key = None
        self.block_factor = None
        self.ub_factor = None
        self.is_fuse_block = None

    def __repr__(self):
        return "QUANT:" + "b_axis=" + str(self.block_tiling_axis) + ",u_axis=" + str(self.ub_tiling_axis) + \
               ",is_split_ub=" + str(self.is_split_ub) + ",tiling_key=" + str(self.tiling_key) + \
               ",b_factor=" + str(self.block_factor) + ",u_factor=" + str(self.ub_factor) + \
               ",is_fuse_block=" + str(self.is_fuse_block)

    def __hash__(self):
        return hash((self.block_tiling_axis, self.block_factor, self.ub_tiling_axis, self.ub_factor, self.multi_core))

    def __eq__(self, other):
        condition1 = other.block_tiling_axis == self.block_tiling_axis
        condition2 = other.block_factor == self.block_factor
        condition3 = other.ub_tiling_axis == self.ub_tiling_axis
        condition4 = other.ub_factor == self.ub_factor
        condition5 = other.multi_core == self.multi_core
        return type(other) == type(self) and condition1 and condition2 and condition3 and condition4 and condition5

    def __ne__(self, other):
        condition1 = other.block_tiling_axis != self.block_tiling_axis
        condition2 = other.block_factor != self.block_factor
        condition3 = other.ub_tiling_axis != self.ub_tiling_axis
        condition4 = other.ub_factor != self.ub_factor
        condition5 = other.multi_core != self.multi_core
        return type(other) != type(self) or condition1 or condition2 or condition3 or condition4 or condition5


def gen_tiling_case(b_start, b_end, u_end, is_fuse_block):
    """
    get all tiling cases
    """
    tiling_case_list = []
    for i in range(b_start, b_end):
        block_tiling_axis = i
        for j in range(i, u_end):
            ub_tiling_axis = j
            tiling_case = QuantTilingCase()
            tiling_case.block_tiling_axis = block_tiling_axis
            tiling_case.ub_tiling_axis = ub_tiling_axis
            tiling_case.multi_core = True
            tiling_case.is_fuse_block = is_fuse_block
            if ub_tiling_axis == block_tiling_axis:
                tiling_case.is_split_ub = False
            else:
                tiling_case.is_split_ub = True
            tiling_case_list.append(tiling_case)
    return tiling_case_list


def gen_const_tiling_case(graph_info):
    """
    get const tiling case
    """
    inputs = []
    tiling_case_list = []
    res_tensor = tuple(graph_info.endpoint_output_tensor_set)[0]
    ori_format = operation.get_context().get("ori_format")
    ori_shape = operation.get_context().get("ori_shape")
    input_format = operation.get_context().get("format")
    res_shape = util.shape_to_list(res_tensor.shape)
    res_dtype = res_tensor.dtype

    for _input in graph_info.input_tensor_set:
        input_shape = util.shape_to_list(_input.shape)
        inputs.append({"shape": input_shape, "dtype": _input.dtype, "format": input_format,
                       "ori_format": ori_format, "ori_shape": ori_shape})
    outputs = [{"shape": res_shape, "dtype": res_dtype}]

    run_info = op_tiling.do_op_tiling(graph_info.quant_type, get_compile_info(), inputs, outputs)
    tiling_format = {
        "block_factor": "int",
        "ub_factor": "int",
        "block_tiling_axis": "int",
        "ub_tiling_axis": "int",
        "is_fuse_block": "int"
    }
    tiling_data = op_tiling.decode(run_info["tiling_data"], tiling_format)
    tiling_case = QuantTilingCase()
    tiling_case.block_tiling_axis = tiling_data["block_tiling_axis"]
    tiling_case.ub_tiling_axis = tiling_data["ub_tiling_axis"]
    tiling_case.block_factor = tiling_data["block_factor"]
    tiling_case.ub_factor = tiling_data["ub_factor"]
    tiling_case.is_fuse_block = tiling_data["is_fuse_block"]
    tiling_case.multi_core = True
    if tiling_case.ub_tiling_axis == tiling_case.block_tiling_axis:
        tiling_case.is_split_ub = False
    else:
        tiling_case.is_split_ub = True
    tiling_case_list.append(tiling_case)

    return tiling_case_list


def calc_tiling_key(shape, tiling):
    block_tiling_axis = tiling.block_tiling_axis
    ub_tiling_axis = tiling.ub_tiling_axis
    is_fuse_block = tiling.is_fuse_block
    shape_type, double_buf = 0, 0
    tiling_key = _get_tiling_key(double_buf, is_fuse_block, shape_type, block_tiling_axis, ub_tiling_axis, shape)
    tiling.tiling_key = tiling_key


def gen_compile_info(graph_info, fuse_flag=0):
    mode = operation.get_context().get("quant_mode")
    var_index_list = operation.get_context().get_current_compute().get("var_index_list")

    max_ub_count = graph_info.max_single_tensor_ub_size
    core_num = get_soc_spec("CORE_NUM")
    common_info = [max_ub_count, core_num, fuse_flag]
    pre_compile_info = get_compile_info()
    if pre_compile_info:
        info_map = {"common_info": common_info, "mode": mode,
                    "var_index_list": var_index_list, "dsl_compile": False}
        for key in info_map.keys():
            if key not in pre_compile_info.keys():
                add_compile_info(key, info_map.get(key))
            else:
                if key != "common_info":
                    key_info = pre_compile_info.get(key)
                    key_info += info_map.get(key)
                    add_compile_info(key, key_info)
    else:
        raise RuntimeError("pre_compile_info is Null")


def _get_tiling_key(double_buf, is_fuse_block, shape_type, block_tiling_axis, ub_tiling_axis, shape):
    """
    get tiling key
    """

    def _check(idx, value):
        rule = [range(2), range(2), range(100), range(9), range(9), range(1000)]
        name = ["double_buf", "is_fuse_block", "shape_type", "block_tiling_axis", "ub_tiling_axis", "pattern"]
        if value not in rule[idx]:
            dict_args = dict()
            dict_args["errCode"] = "E90003"
            dict_args["detailed_cause"] = "%s should in %s, but is %d" % (name[idx], str(rule[idx]), value)
            raise RuntimeError(dict_args, get_error_message(dict_args))

    pattern = _get_pattern_key(shape, block_tiling_axis, ub_tiling_axis)
    pos = (double_buf, is_fuse_block, shape_type, block_tiling_axis, ub_tiling_axis, pattern)
    val = (10 ** 9, 10 ** 7, 10 ** 6, 10 ** 5, 10 ** 4, 10 ** 3)
    key = 0
    for item, value in enumerate(pos):
        _check(item, value)
        key += value * val[item]
    return key


def _get_pattern_key(shape, block_tiling_axis=0, ub_tiling_axis=0):
    pattern_key = 0
    length = len(shape)
    for i in range(length):
        pattern_key += 2 ** (length - i - 1)
    pattern_key += block_tiling_axis * 100 + ub_tiling_axis * 10
    return pattern_key


# noinspection PyUnusedLocal
@register_tiling_case(pattern=Pattern.ASCEND_ANTI_QUANT)
def calc_tiling_case(outs, option=None):
    """
    antiquant tiling case
    """
    outs = list(outs) if isinstance(outs, (list, tuple)) else [outs]
    out = outs[0]
    # dtype = out.dtype
    shape = util.shape_to_list(out.shape)
    mode = operation.get_context().get("quant_mode")
    graph_info = QuantComputeGraphInfo(outs)
    current_compute = operation.get_context().get_current_compute()
    current_compute.add("quant_graph_info", graph_info)

    fuse_flag = 1 if graph_info.elewise_fuse or graph_info.quant_fuse else 0
    gen_compile_info(graph_info, fuse_flag)

    tiling_case_list = []
    if mode == "const":
        tiling_case_list += gen_const_tiling_case(graph_info)
    else:
        tiling_case_list += gen_tiling_case(0, len(shape) - 1, len(shape) - 1, True)
        tiling_case_list += gen_tiling_case(1, len(shape) - 2, len(shape) - 1, False)

    for tiling_case in tiling_case_list:
        calc_tiling_key(shape, tiling_case)

    return tiling_case_list
