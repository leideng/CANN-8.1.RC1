#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pooling schedule
"""
from tbe import tvm
from tbe.dsl.base.operation import add_build_arg
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import var_inner

from .pooling_helper import PoolingConstants
from .pooling_helper import PoolingSchType
from .pooling_helper import get_insn
from .pooling_helper import judge_tvm_shape_equal
from .pooling_helper import reorder_reduce_window_shape
from .pooling_info import PoolingComputeInfo
from .pooling_info import PoolingSocInfo
from .pooling_tilingcase import PoolingTilingCase
from ... import util
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG
from ...constants import Pattern
from ...constants import PoolingPattern
from ...schedule import Schedule


class EntryPoolingSchedule(Schedule):
    """
    entrance to pooling schedule
    """
    def __init__(self, outs, tiling_case):
        self.outs = outs
        self.tiling_case = tiling_case

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return ["default"]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.POOLING]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [PoolingPattern.P_0]

    def do_schedule(self):
        current_compute = get_context().get_current_compute()
        compute_info = current_compute.get("_compute_info")
        if self.tiling_case.sch_type == PoolingSchType.COMMON:
            pooling_sch = PoolingCommonSchedule(compute_info, self.tiling_case, self.outs)
        else:
            pooling_sch = PoolingWindowSplitSchedule(compute_info, self.tiling_case, self.outs)

        return pooling_sch.do_schedule()


class PoolingBaseSchedule:
    """
    pooling base schedule
    """
    def __init__(self, compute_info: PoolingComputeInfo, tiling_case: PoolingTilingCase, outs):
        self._outs = outs
        self._sch = None
        self._scope = PoolingConstants.LOCAL_UB
        self._tiling_case = tiling_case

        self._compute_info = compute_info
        self._forward_compute_graph_map = compute_info.graph_info.tensor_consumers_map
        self._backward_compute_graph_map = compute_info.graph_info.tensor_producers_map
        self._mid_tensor_set = compute_info.graph_info.mid_tensor_set
        self._res_tensor = compute_info.graph_info.endpoint_output_tensor

        self._ori_reduce_window_buffer = compute_info.base_info.reduce_window_tensor

        self._cache_read_tensors = set()
        self._cache_read_buffer_and_tensor_map = {}
        self._cache_read_tensor_and_buffer_map = {}

        self._cache_write_tensors = set()
        self._cache_write_buffer_and_tensor_map = {}
        self._cache_write_tensor_and_buffer_map = {}

        self._window_reduce_tensor_list = []

        self._is_window_pad_using_dma = False
        self._compute_inline_tensors = set()

        self._mem_unique_tensors = set()

        self._block_split_result = {}
        self._ub_split_result = {}

        self._reorder_map = {}

        self._storage_align_map = {}

        self._compute_at_map = {}

        self._emit_insn_map = {}

        self._buffer_size = None

    def _calc_cache_read(self):
        self._cache_read_tensors.update(self._compute_info.graph_info.input_tensor_set)

    def _do_cache_read(self):
        for cache_read_tensor in self._cache_read_tensors:
            read_buffer = self._sch.cache_read(
                cache_read_tensor, self._scope, self._forward_compute_graph_map.get(cache_read_tensor))
            self._cache_read_buffer_and_tensor_map[read_buffer] = cache_read_tensor
            self._cache_read_tensor_and_buffer_map[cache_read_tensor] = read_buffer

    def _calc_cache_write(self):
        self._cache_write_tensors.update(self._compute_info.graph_info.real_pure_output_tensor_set)

    def _do_cache_write(self):
        for cache_write_tensor in self._cache_write_tensors:
            buffer_tensor = self._sch.cache_write(cache_write_tensor, self._scope)
            if cache_write_tensor == self._compute_info.base_info.reduce_window_tensor:
                self._ori_reduce_window_buffer = buffer_tensor
            self._cache_write_buffer_and_tensor_map[buffer_tensor] = cache_write_tensor
            self._cache_write_tensor_and_buffer_map[cache_write_tensor] = buffer_tensor

    def _set_scope(self):
        for mid_tensor in self._mid_tensor_set - self._compute_info.graph_info.real_output_tensor_set:
            self._sch[mid_tensor].set_scope(self._scope)

    def _do_mid_output_tensor_process(self):
        pass

    def _calc_compute_inline(self):
        for cache_read_tensor in self._cache_read_tensors:
            consumers = self._forward_compute_graph_map.get(cache_read_tensor)
            if consumers == {self._compute_info.base_info.pad_tensor}:
                read_buffer = self._cache_read_tensor_and_buffer_map.get(cache_read_tensor)
                self._compute_inline_tensors.add(read_buffer)
                self._is_window_pad_using_dma = True

    def _do_compute_inline(self):
        for compute_inline_tensor in self._compute_inline_tensors:
            self._sch[compute_inline_tensor].compute_inline()

    def _do_rfactor(self):
        pass

    def _calc_mem_unique(self):
        if len(self._compute_info.graph_info.input_tensor_set) > 1:
            return
        if self._is_window_pad_using_dma:
            self._mem_unique_tensors.add(self._compute_info.base_info.pad_tensor)
        else:
            single_input_tensor = tuple(self._compute_info.graph_info.input_tensor_set)[0]
            self._mem_unique_tensors.add(self._cache_read_buffer_and_tensor_map.get(single_input_tensor))

    def _do_mem_unique(self):
        for single_tensor in self._mem_unique_tensors:
            self._sch[single_tensor].mem_unique()

    def _calc_buffer_size(self):
        ub_size = PoolingSocInfo.get_ub_size() - 1024
        coexist_node = 2 + len(self._mem_unique_tensors)
        if self._tiling_case.is_enable_db:
            coexist_node = coexist_node * 2

        self._buffer_size = ub_size // DTYPE_BYTE_MAPPING.get(self._compute_info.graph_info.max_type) // coexist_node

        buffer_size_list = get_compile_info().get("_buffer_size")
        if buffer_size_list is None:
            buffer_size_list = {}
            add_compile_info_inner("_buffer_size", buffer_size_list)
        if self._tiling_case.pattern_key not in buffer_size_list:
            buffer_size_list[self._tiling_case.pattern_key] = [0] * (len(PoolingSchType.__members__) * 2)
            buffer_size_list.get(self._tiling_case.pattern_key)[self._tiling_case.sch_key] = self._buffer_size
        else:
            buffer_size = buffer_size_list.get(self._tiling_case.pattern_key)
            if buffer_size[self._tiling_case.sch_key] != 0:
                buffer_size[self._tiling_case.sch_key] = min(buffer_size[self._tiling_case.sch_key],
                                                             self._buffer_size)
            else:
                buffer_size[self._tiling_case.sch_key] = self._buffer_size

    def _get_window_product(self, start=0, end=None):
        window_product = 1
        window_axes = self._ori_reduce_window_buffer.op.reduce_axis
        window_info = self._compute_info.base_info.window_info
        if not window_info:
            return window_product
        window_dilations = window_info[2]
        local_end = len(window_axes) if end is None else end
        for idx, axis in enumerate(window_axes):
            if start > idx or idx >= local_end:
                continue
            axis_dom = axis.dom
            if hasattr(axis_dom.min, "value") and hasattr(axis_dom.extent, "value"):
                if axis_dom.min.value == 0:
                    window_product = ((axis_dom.extent.value - 1) * window_dilations[idx] + 1) * window_product

        return window_product

    def _pre_check_sch(self):
        pass

    def _do_const_tiling(self):
        if self._tiling_case.is_const:
            self._tiling_case.calc_const_tiling()

    def _post_check_sch(self):
        return self._tiling_case.check_consistency()

    def _do_ub_tiling(self):
        pass

    def _do_block_tiling(self):
        pass

    def _do_set_buffer_size(self):
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys())\
                .union(self._cache_write_buffer_and_tensor_map.keys())\
                .union(self._window_reduce_tensor_list):
            if single_tensor in self._compute_inline_tensors:
                continue
            storage_bound_value = self._buffer_size * \
                                  DTYPE_BYTE_MAPPING.get(self._compute_info.graph_info.max_type) // \
                                  DTYPE_BYTE_MAPPING.get(single_tensor.dtype)
            self._sch[single_tensor].set_buffer_size(storage_bound_value)

        if self._res_tensor.op.tag == FAKE_NODE_TAG:
            storage_bound_value = self._buffer_size * \
                                  DTYPE_BYTE_MAPPING.get(self._compute_info.graph_info.max_type) // \
                                  DTYPE_BYTE_MAPPING.get(self._res_tensor.dtype)
            self._sch[self._res_tensor].set_buffer_size(storage_bound_value)

    def _do_reorder(self):
        for single_tensor, param in self._reorder_map.items():
            self._sch[single_tensor].reorder(*param)

    def _calc_storage_align(self):
        pass

    def _do_storage_align(self):
        for single_tensor, param in self._storage_align_map.items():
            self._sch[single_tensor].storage_align(param[0], param[1], param[2])

    def _do_multi_core(self):
        pass

    def _do_set_constraint(self):
        pass

    def _calc_compute_at(self):
        pass

    def _do_compute_at(self):
        for single_tensor, param in self._compute_at_map.items():
            self._sch[single_tensor].compute_at(self._sch[param[0]], param[1])

    def _do_double_buffer(self):
        if not self._tiling_case.is_enable_db:
            return

        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys())\
                .union(self._cache_write_buffer_and_tensor_map.keys())\
                .union(self._window_reduce_tensor_list):
            if single_tensor in self._compute_inline_tensors:
                continue
            self._sch[single_tensor].double_buffer()

        add_build_arg("double_buffer_non_reuse", True)

    def _calc_emit_insn(self):
        pass

    def _do_emit_insn(self):
        for single_tensor, param in self._emit_insn_map.items():
            if len(param) > 2:
                self._sch[single_tensor].emit_insn(param[0], param[1], attrs=param[2])
            else:
                self._sch[single_tensor].emit_insn(param[0], param[1])

    def _do_pragma(self):
        pass


class PoolingCommonSchedule(PoolingBaseSchedule):
    """
    pooling common schedule
    """
    def __init__(self, compute_info: PoolingComputeInfo, tiling_case: PoolingTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)
        self._fuse_axes = None

    def do_schedule(self):
        """
        pooling common schedule process
        """
        self._sch = tvm.create_schedule([self._res_tensor.op])
        self._sch.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_mid_output_tensor_process()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._do_rfactor()

        self._calc_mem_unique()
        self._do_mem_unique()

        self._calc_buffer_size()

        if not self._pre_check_sch():
            return None
        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._do_ub_tiling()
        self._calc_res_reorder()
        self._calc_reduce_tensor_reorder()
        self._do_reorder()
        self._do_block_tiling()

        self._do_set_buffer_size()

        self._calc_storage_align()
        self._do_storage_align()

        self._do_multi_core()

        self._do_set_constraint()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_double_buffer()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_pragma()

        return self._sch

    def _do_rfactor(self):
        def __recursion(_cur_tensor, _cur_index):
            if _cur_index == 0:
                return
            _reduce_rf = self._sch.rfactor(_cur_tensor, _cur_tensor.op.reduce_axis[-1],
                                           mode=self._sch.RfactorModeNormal)
            self._sch[_reduce_rf].set_scope(PoolingConstants.LOCAL_UB)
            self._window_reduce_tensor_list.append(_reduce_rf)
            _cur_index -= 1
            __recursion(_reduce_rf, _cur_index)

        __recursion(self._ori_reduce_window_buffer, len(self._compute_info.base_info.window_indices) - 1)

    def _pre_check_sch(self):
        if self._tiling_case.is_enable_db:
            if len(self._compute_info.base_info.window_indices) > 2:
                return False

        window_product = self._get_window_product()
        # A after last window
        last_window_index = max(self._compute_info.base_info.window_indices)
        for idx, dim in enumerate(util.shape_to_list(self._compute_info.base_info.ori_input_shape)):
            if idx > last_window_index and isinstance(dim, int):
                window_product *= dim

        return window_product <= self._buffer_size

    def _do_ub_tiling(self):
        ub_split_axis_index = self._tiling_case.ub_split_axis_index
        ub_factor = self._tiling_case.ub_factor
        ub_split_factor = ub_factor if ub_factor else var_inner("_ub_factor", (1, None))
        ub_outer, ub_inner = self._sch[self._res_tensor].split(self._res_tensor.op.axis[ub_split_axis_index],
                                                               factor=ub_split_factor)

        self._ub_split_result["index"] = ub_split_axis_index
        self._ub_split_result["outer_itervar"] = ub_outer
        self._ub_split_result["inner_itervar"] = ub_inner
        self._ub_split_result["factor"] = ub_split_factor

    def _calc_res_reorder(self):
        def __calc_res_tensor_reorder_axis(_tensor):
            _ori_axis = _tensor.op.axis
            _ori_ub_axis = self._tiling_case.ub_split_axis_index
            _window_indices = self._compute_info.base_info.window_indices
            _is_reduce_last_axis = self._compute_info.base_info.is_reduce_last_axis
            _reduce_reorder_axis, _, _ori_to_reorder_axis_map = \
                reorder_reduce_window_shape(_ori_axis, _window_indices, _is_reduce_last_axis)
            reorder_axis = []
            for idx, axis in enumerate(_reduce_reorder_axis):
                if idx == _ori_to_reorder_axis_map.get(_ori_ub_axis):
                    reorder_axis.append(self._ub_split_result.get("outer_itervar"))
                    reorder_axis.append(self._ub_split_result.get("inner_itervar"))
                else:
                    reorder_axis.append(axis)

            return reorder_axis

        reorder_axis_list = __calc_res_tensor_reorder_axis(self._res_tensor)
        self._reorder_map[self._res_tensor] = reorder_axis_list

    def _calc_reduce_tensor_reorder(self):
        def __calc_reduce_tensor_reorder_axis(_tensor, _index):
            _reorder_axis = self._sch[_tensor].op.axis[:]
            _reorder_axis.insert(_index + 1, self._sch[_tensor].op.reduce_axis[0])

            return _reorder_axis

        # only nlast reduce mid tensor need reorder
        if not self._compute_info.base_info.is_reduce_last_axis:
            index = self._compute_info.base_info.window_indices[-1]
            reorder_axis_list = __calc_reduce_tensor_reorder_axis(self._ori_reduce_window_buffer, index)
            self._reorder_map[self._ori_reduce_window_buffer] = reorder_axis_list

        for count, single_tensor in enumerate(self._window_reduce_tensor_list):
            index = self._compute_info.base_info.window_indices[count]
            reorder_axis_list = __calc_reduce_tensor_reorder_axis(single_tensor, index)
            self._reorder_map[single_tensor] = reorder_axis_list

    def _do_block_tiling(self):
        res_reorder_axis = self._reorder_map.get(self._res_tensor)
        fuse_axis_list = res_reorder_axis[:res_reorder_axis.index(self._ub_split_result.get("outer_itervar")) + 1]
        self._fuse_axes = self._sch[self._res_tensor].fuse(*fuse_axis_list)

        block_factor = self._tiling_case.block_factor
        block_split_factor = block_factor if block_factor else var_inner("_block_factor", (1, None))
        block_outer, block_inner = self._sch[self._res_tensor].split(self._fuse_axes, factor=block_split_factor)

        self._block_split_result["outer_itervar"] = block_outer
        self._block_split_result["inner_itervar"] = block_inner
        self._block_split_result["factor"] = block_split_factor

    def _do_multi_core(self):
        if self._tiling_case.multi_core and self._block_split_result.get("outer_itervar") is not None:
            block = tvm.thread_axis(PoolingConstants.BLOCK_IDX)
            self._sch[self._res_tensor].bind(self._block_split_result.get("outer_itervar"), block)

    def _do_set_constraint(self):
        if self._tiling_case.is_const:
            return

        ori_shape = self._res_tensor.shape

        ub_split_inner = self._ub_split_result.get("factor")
        ori_ub_axis = self._tiling_case.ub_split_axis_index
        window_indices = self._compute_info.base_info.window_indices
        is_reduce_last_axis = self._compute_info.base_info.is_reduce_last_axis
        reduce_reorder_shape, _, ori_to_reorder_axis_map = reorder_reduce_window_shape(ori_shape,
                                                                                       window_indices,
                                                                                       is_reduce_last_axis)
        reorder_ub_axis = ori_to_reorder_axis_map.get(ori_ub_axis)
        shape_in_ub = ub_split_inner
        self._sch.set_constraint(ub_split_inner <= self._buffer_size)
        self._sch.set_constraint(ub_split_inner > 0)
        for i in range(reorder_ub_axis + 1, len(reduce_reorder_shape)):
            shape_in_ub *= reduce_reorder_shape[i]
            self._sch.set_constraint(reduce_reorder_shape[i] <= self._buffer_size)
            self._sch.set_constraint(reduce_reorder_shape[i] > 0)

        self._sch.set_constraint(shape_in_ub <= self._buffer_size)

        first_window_index = max(window_indices) + 1 - len(window_indices)
        window_product = self._get_window_product(end=reorder_ub_axis - first_window_index)
        self._sch.set_constraint(window_product * shape_in_ub <= self._buffer_size)

    def _calc_compute_at(self):
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys())\
                .union(self._cache_write_buffer_and_tensor_map.keys())\
                .union(self._window_reduce_tensor_list):
            if single_tensor in self._compute_inline_tensors:
                continue
            self._compute_at_map[single_tensor] = [self._res_tensor, self._block_split_result.get("inner_itervar")]

    def _calc_emit_insn(self):
        def __handle_reduce_tensor(_single_tensor, _insn):
            attr_storage_bound_value = self._buffer_size * \
                                       DTYPE_BYTE_MAPPING.get(self._compute_info.graph_info.max_type) // \
                                       DTYPE_BYTE_MAPPING.get(_single_tensor.dtype)
            self._emit_insn_map[_single_tensor] = [self._sch[_single_tensor].op.axis[emit_insn_axis_index],
                                                   _insn,
                                                   {PoolingConstants.STORAGE_BOUND: attr_storage_bound_value,
                                                    PoolingConstants.WINDOW: True}]

        emit_insn_axis_index = 0
        for single_buffer, tensor in self._cache_read_buffer_and_tensor_map.items():
            if single_buffer in self._compute_inline_tensors:
                continue
            # recache read buffer of mid output tensor does not do dma_copy
            insn = PoolingConstants.PHONY_INSN \
                   if tensor in self._compute_info.graph_info.mid_output_tensor_set \
                   else PoolingConstants.DMA_COPY
            self._emit_insn_map[single_buffer] = [single_buffer.op.axis[emit_insn_axis_index], insn]

        for single_tensor in self._mid_tensor_set - self._compute_inline_tensors -\
                self._compute_info.graph_info.real_output_tensor_set:
            insn = get_insn(single_tensor)
            if single_tensor == self._ori_reduce_window_buffer:
                __handle_reduce_tensor(single_tensor, get_insn(single_tensor))
                continue
            pad_tensor_with_dma_copy = \
                single_tensor == self._compute_info.base_info.pad_tensor and self._is_window_pad_using_dma
            if pad_tensor_with_dma_copy:
                insn = PoolingConstants.DMA_COPY
            self._emit_insn_map[single_tensor] = [single_tensor.op.axis[emit_insn_axis_index], insn]

        for single_buffer, tensor in self._cache_write_buffer_and_tensor_map.items():
            if single_buffer == self._ori_reduce_window_buffer:
                __handle_reduce_tensor(single_buffer, get_insn(tensor))
                continue
            self._emit_insn_map[single_buffer] = [single_buffer.op.axis[emit_insn_axis_index], get_insn(tensor)]

        for single_reduce_tensor in self._window_reduce_tensor_list:
            __handle_reduce_tensor(single_reduce_tensor, get_insn(self._compute_info.base_info.reduce_window_tensor))

        for out_tensor in self._compute_info.graph_info.real_output_tensor_set:
            if out_tensor == self._res_tensor:
                _emit_insn_axis = self._ub_split_result.get("inner_itervar")
            else:
                _emit_insn_axis = out_tensor.op.axis[emit_insn_axis_index]
            self._emit_insn_map[out_tensor] = [_emit_insn_axis, PoolingConstants.DMA_COPY,
                                               {PoolingConstants.NO_OVERLAP: 0}]

        if self._res_tensor.op.tag == FAKE_NODE_TAG:
            emit_insn_axis = self._ub_split_result.get("inner_itervar")
            self._emit_insn_map[self._res_tensor] = [emit_insn_axis, PoolingConstants.PHONY_INSN]


class PoolingWindowSplitSchedule(PoolingBaseSchedule):
    """
    pooling window split schedule
    """
    def __init__(self, compute_info: PoolingComputeInfo, tiling_case: PoolingTilingCase, outs):
        super().__init__(compute_info, tiling_case, outs)
        self._multi_core_bind_axis = None

    def do_schedule(self):
        """
        pooling window split schedule process
        """
        self._sch = tvm.create_schedule([self._res_tensor.op])
        self._sch.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._do_mid_output_tensor_process()

        self._calc_compute_inline()
        self._do_compute_inline()

        self._calc_mem_unique()
        self._do_mem_unique()

        self._calc_buffer_size()

        if not self._pre_check_sch():
            return None
        self._do_const_tiling()
        if not self._post_check_sch():
            return None

        self._do_block_tiling()
        self._do_ub_tiling()
        self._calc_reduce_tensor_reorder()
        self._do_reorder()

        self._do_set_buffer_size()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_multi_core()
        self._do_multi_core()

        self._do_set_constraint()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_double_buffer()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_pragma()

        return self._sch

    def _pre_check_sch(self):
        if self._tiling_case.is_enable_db:
            if len(self._ori_reduce_window_buffer.op.reduce_axis) > 2:
                return False

        window_info = self._compute_info.base_info.window_info
        if not window_info:
            return True

        window_product = 1
        has_unknown_window_size = False
        window_axes = self._ori_reduce_window_buffer.op.reduce_axis
        window_dilations = window_info[2]
        for idx, axis in enumerate(window_axes):
            axis_dom = axis.dom
            if hasattr(axis_dom.min, "value") and hasattr(axis_dom.extent, "value"):
                if axis_dom.min.value == 0:
                    window_product = ((axis_dom.extent.value - 1) * window_dilations[idx] + 1) * window_product
                    continue
            has_unknown_window_size = True

        if has_unknown_window_size:
            return True

        return window_product > PoolingConstants.WINDOW_SPLIT_THRESHOLD

    def _do_block_tiling(self):
        block_split_axis_index = self._tiling_case.block_split_axis_index

        block_factor = self._tiling_case.block_factor
        block_split_factor = block_factor if block_factor else var_inner("_block_factor", (1, None))
        block_outer, block_inner = self._sch[self._res_tensor].split(
            self._res_tensor.op.axis[block_split_axis_index], factor=block_split_factor)

        self._block_split_result["index"] = block_split_axis_index
        self._block_split_result["outer_itervar"] = block_outer
        self._block_split_result["inner_itervar"] = block_inner
        self._block_split_result["factor"] = block_split_factor

    def _do_ub_tiling(self):
        ub_split_axis_index = self._tiling_case.ub_split_axis_index
        ub_factor = self._tiling_case.ub_factor
        ub_split_factor = ub_factor if ub_factor else var_inner("_ub_factor", (1, None))

        ub_split_reduce_axis_index = self._compute_info.base_info.window_indices.index(ub_split_axis_index)
        ub_axis_var = self._ori_reduce_window_buffer.op.reduce_axis[ub_split_reduce_axis_index]
        ub_outer, ub_inner = \
            self._sch[self._ori_reduce_window_buffer].split(ub_axis_var, factor=ub_split_factor)

        self._ub_split_result["index"] = ub_split_axis_index
        self._ub_split_result["reduce_index"] = ub_split_reduce_axis_index
        self._ub_split_result["outer_itervar"] = ub_outer
        self._ub_split_result["inner_itervar"] = ub_inner
        self._ub_split_result["factor"] = ub_split_factor

    def _calc_reduce_tensor_reorder(self):
        # only nlast reduce mid tensor need reorder
        if not self._compute_info.base_info.is_reduce_last_axis:
            last_w_index = max(self._compute_info.base_info.window_indices)
            reorder_axis = list(self._ori_reduce_window_buffer.op.axis)
            insert_index = last_w_index - len(reorder_axis) + 1
            for idx, value in enumerate(self._ori_reduce_window_buffer.op.reduce_axis):
                if idx == self._ub_split_result.get("reduce_index"):
                    reorder_axis.insert(insert_index, self._ub_split_result.get("outer_itervar"))
                    reorder_axis.insert(insert_index, self._ub_split_result.get("inner_itervar"))
                else:
                    reorder_axis.insert(insert_index, value)

            self._reorder_map[self._ori_reduce_window_buffer] = reorder_axis

    def _calc_multi_core(self):
        if self._tiling_case.multi_core:
            block_bind_axis = self._block_split_result.get("outer_itervar")
            if self._res_tensor in self._reorder_map:
                reorder_axis = self._reorder_map.get(self._res_tensor)
                fuse_axis_list = reorder_axis[:reorder_axis.index(block_bind_axis) + 1]
            else:
                fuse_axis_list = self._res_tensor.op.axis[:self._tiling_case.block_split_axis_index]
                fuse_axis_list.append(block_bind_axis)
            self._multi_core_bind_axis = self._sch[self._res_tensor].fuse(*fuse_axis_list)

    def _do_multi_core(self):
        if self._multi_core_bind_axis is not None:
            block = tvm.thread_axis(PoolingConstants.BLOCK_IDX)
            self._sch[self._res_tensor].bind(self._multi_core_bind_axis, block)

    def _do_set_constraint(self):
        if self._tiling_case.is_const:
            return

        ub_split_inner = self._ub_split_result.get("factor")
        self._sch.set_constraint(ub_split_inner <= self._buffer_size)
        shape_in_ub = ub_split_inner

        window_axes = self._ori_reduce_window_buffer.op.reduce_axis
        for idx, axis in enumerate(window_axes):
            if idx <= self._ub_split_result.get("reduce_index"):
                continue
            axis_dom = axis.dom
            if hasattr(axis_dom.min, "value") and hasattr(axis_dom.extent, "value"):
                if axis_dom.min.value == 0:
                    shape_in_ub *= axis_dom.extent.value

        # A after last window
        last_window_index = max(self._compute_info.base_info.window_indices)
        for idx, dim in enumerate(util.shape_to_list(self._compute_info.base_info.ori_input_shape)):
            if idx > last_window_index:
                shape_in_ub *= dim

        self._sch.set_constraint(shape_in_ub <= self._buffer_size)

    def _calc_compute_at(self):
        for single_tensor in self._mid_tensor_set \
                .union(self._cache_read_buffer_and_tensor_map.keys())\
                .union(self._cache_write_buffer_and_tensor_map.keys()):
            if single_tensor in self._compute_inline_tensors or single_tensor == self._ori_reduce_window_buffer:
                continue
            if not judge_tvm_shape_equal(single_tensor.shape, self._compute_info.base_info.after_reduce_shape):
                self._compute_at_map[single_tensor] = [self._ori_reduce_window_buffer,
                                                       self._ub_split_result.get("outer_itervar")]
            else:
                self._compute_at_map[single_tensor] = [self._res_tensor,
                                                       self._sch[self._res_tensor].leaf_iter_vars[0]]
        self._compute_at_map[self._ori_reduce_window_buffer] = \
            [self._res_tensor, self._sch[self._res_tensor].leaf_iter_vars[0]]

    def _calc_emit_insn(self):
        def __handle_reduce_tensor(_single_tensor, _insn):
            attr_storage_bound_value = self._buffer_size * \
                                       DTYPE_BYTE_MAPPING.get(self._compute_info.graph_info.max_type) // \
                                       DTYPE_BYTE_MAPPING.get(_single_tensor.dtype)
            self._emit_insn_map[_single_tensor] = [self._ub_split_result.get("inner_itervar"),
                                                   _insn,
                                                   {PoolingConstants.STORAGE_BOUND: attr_storage_bound_value,
                                                    PoolingConstants.WINDOW: True}]

        emit_insn_axis_index = 0
        for single_buffer, tensor in self._cache_read_buffer_and_tensor_map.items():
            if single_buffer in self._compute_inline_tensors:
                continue
            # recache read buffer of mid output tensor does not do dma_copy
            insn = PoolingConstants.PHONY_INSN \
                   if tensor in self._compute_info.graph_info.mid_output_tensor_set \
                   else PoolingConstants.DMA_COPY
            self._emit_insn_map[single_buffer] = [single_buffer.op.axis[emit_insn_axis_index], insn]

        for single_tensor in self._mid_tensor_set - self._compute_inline_tensors -\
                self._compute_info.graph_info.real_output_tensor_set:
            insn = get_insn(single_tensor)
            if single_tensor == self._ori_reduce_window_buffer:
                __handle_reduce_tensor(single_tensor, get_insn(single_tensor))
                continue
            pad_tensor_with_dma_copy = \
                single_tensor == self._compute_info.base_info.pad_tensor and self._is_window_pad_using_dma
            if pad_tensor_with_dma_copy:
                insn = PoolingConstants.DMA_COPY
            self._emit_insn_map[single_tensor] = [single_tensor.op.axis[emit_insn_axis_index], insn]

        for single_buffer, tensor in self._cache_write_buffer_and_tensor_map.items():
            if single_buffer == self._ori_reduce_window_buffer:
                __handle_reduce_tensor(single_buffer, get_insn(tensor))
                continue
            self._emit_insn_map[single_buffer] = [single_buffer.op.axis[emit_insn_axis_index], get_insn(tensor)]

        for single_reduce_tensor in self._window_reduce_tensor_list:
            __handle_reduce_tensor(single_reduce_tensor, get_insn(self._compute_info.base_info.reduce_window_tensor))

        for out_tensor in self._compute_info.graph_info.real_output_tensor_set:
            if out_tensor == self._res_tensor:
                _emit_insn_axis = self._block_split_result.get("inner_itervar")
            else:
                _emit_insn_axis = out_tensor.op.axis[emit_insn_axis_index]
            self._emit_insn_map[out_tensor] = [_emit_insn_axis, PoolingConstants.DMA_COPY,
                                               {PoolingConstants.NO_OVERLAP: 0}]

        if self._res_tensor.op.tag == FAKE_NODE_TAG:
            emit_insn_axis = self._block_split_result.get("inner_itervar")
            self._emit_insn_map[self._res_tensor] = [emit_insn_axis, PoolingConstants.PHONY_INSN]
