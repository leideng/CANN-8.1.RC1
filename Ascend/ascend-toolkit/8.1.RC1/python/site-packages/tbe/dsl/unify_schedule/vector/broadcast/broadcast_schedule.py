#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
broadcast schedule
"""
from tbe.dsl.base import operation
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform import ASCEND_310P
from tbe.common.platform.platform_info import get_soc_spec
from ... import util
from ...constants import BroadcastPattern
from ...constants import FAKE_NODE_TAG
from ...constants import Pattern
from ...constants import SUPPORT_SCALAR_INSNS
from ...constants import TERNARY_INSNS
from ...schedule import Schedule
from ..storage_bound_util import EXTRA_NODE_B64
from ..storage_bound_util import EXTRA_NODE_B32
from ..storage_bound_util import EXTRA_TEMP_BUFFER
from ..storage_bound_util import base_op_complex_instructions
from ..storage_bound_util import cast_complex_instructions
from ..storage_bound_util import complex_instructions_b64_impl
from ..storage_bound_util import vsel_complex_instructions
from ..storage_bound_util import base_single_instructions_b64
from ..storage_bound_util import powi_complex_instructions_b32
from ..storage_bound_util import cast_ints2ints_external_space
from .broadcast_schedule_base import BaseBroadcastSchedule
from .broadcast_tilingcase import TilingStrategy
from .broadcast_tilingcase import get_pattern_key
from  .broadcast_constants import DEFAULT
from  .broadcast_constants import VCMP_INPUT_NUMBER
from  .broadcast_constants import VSEL_INPUT_NUMBER
from  .broadcast_constants import VCMPSEL_INPUT_NUMBER
from  .broadcast_constants import ENOUGH_BUFFER
from  .broadcast_util import ScheduleUtil


# 'pylint: disable=R0902, R0903
class BroadcastSchedule(BaseBroadcastSchedule, Schedule):
    """
    BroadcastSchedule
    """

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.BROADCAST]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [BroadcastPattern.B_0]

    def __init__(self, outs, tiling_case):
        super(BroadcastSchedule, self).__init__(outs, tiling_case)

    def _calc_emit_insn(self):
        super(BroadcastSchedule, self)._calc_emit_insn()
        if not self._tiling_case.is_storage_align:
            self._emit_insn_attr_map[ENOUGH_BUFFER] = False
        else:
            self._emit_insn_attr_map[ENOUGH_BUFFER] = True

    def _without_temp_buffer(self):
        if operation.get_context().get_op_type() not in ("Add", "Mul"):
            return False

        for _input in self._input_tensors:
            if _input.dtype in ("bool", "int8", "uint8", "int64"):
                return False

        if get_pattern_key(self._tiling_case.tiling_key) not in ('120', '210', '121'):
            return False

        if get_soc_spec(SHORT_SOC_VERSION) not in (ASCEND_310P,):
            return False

        return True

    def _calc_storage_bound(self):
        block_size_bytes = self._ub_block_size

        def _correct_ub_size_by_cmp_sel(_tensor):
            if util.is_vcmp_insn(_tensor):
                self._tmp_ub_size += block_size_bytes * (VCMP_INPUT_NUMBER - len(_tensor.op.input_tensors))
            if util.is_vsel_insn(_tensor):
                self._tmp_ub_size += block_size_bytes * (VSEL_INPUT_NUMBER - len(_tensor.op.input_tensors))
                if util.is_v200() and (VSEL_INPUT_NUMBER == len(_tensor.op.input_tensors)):
                    self._tmp_ub_size += block_size_bytes
            if util.is_vcmpsel_insn(_tensor):
                self._tmp_ub_size += block_size_bytes * (VCMPSEL_INPUT_NUMBER - len(_tensor.op.input_tensors))

        def _calc_current_space(__tensor):
            is_brc_tensor = __tensor in self._broadcast_tensors
            # one of the input of the ternary instruction must be reused with the output
            if util.get_dsl_insn(__tensor) in TERNARY_INSNS or \
                    __tensor in dependent_map or (self._is_pure_brc_common_db and is_brc_tensor):
                current_space = len(dependent_map)
            else:
                current_space = len(dependent_map) + 1
            for tensor_i in dependent_map:
                if tensor_i in self._absorbable_broadcast_tensors and \
                        len(tensor_i.op.input_tensors) == 1 and tensor_i.op.input_tensors[0] in dependent_map:
                    current_space -= 1
            if is_brc_tensor:
                # for vnchwconv extra space for not enough case.
                if not self._tiling_case.is_storage_align and not self._without_temp_buffer():
                    current_space += 1

                if len(__tensor.op.input_tensors) == 1 and (
                        len(self._in_out_map.get(__tensor.op.input_tensors[0])) > 1 or
                        __tensor.op.input_tensors[0] in self._cst_compute_root_tensors | self._dyn_compute_root_tensors):
                    self._broadcast_by_no_other_use[__tensor] = False
                else:
                    self._broadcast_by_no_other_use[__tensor] = True
            if util.need_extent_node(__tensor) and not self._is_last_align(__tensor):
                current_space += 1
                if is_brc_tensor and self._broadcast_by_no_other_use.get(__tensor):
                    current_space -= 1
                if self._is_no_store_align and not self._is_vnchwconv_align:
                    current_space += 1
            if (util.is_unified_broadcast(__tensor) and not self._is_last_align(__tensor)) or \
                    (self._is_pure_brc and not self._is_pure_brc_common_db):
                current_space += 1
                if is_brc_tensor and self._broadcast_by_no_other_use.get(__tensor):
                    current_space -= 1
                if (self._is_pure_brc or self._is_no_store_align) and not self._is_vnchwconv_align:
                    current_space += 1
            # check if tensor impl by vsel complex instructions
            if vsel_complex_instructions(__tensor):
                current_space += 3

            extra_space, extra_ub = cast_ints2ints_external_space(__tensor)
            current_space += extra_space
            self._tmp_ub_size += extra_ub

            # check if tensor impl by complex instructions
            if complex_instructions_b64_impl(__tensor):
                current_space += EXTRA_NODE_B64.get(__tensor.op.tag)
                if cast_complex_instructions(__tensor):
                    # int64 cast to float16 need one more coexist node
                    if __tensor.dtype == "float16":
                        current_space += 1
                if base_op_complex_instructions(__tensor):
                    self._tmp_ub_size += EXTRA_TEMP_BUFFER.get(__tensor.op.tag)
            # single instruction support b64
            if base_single_instructions_b64(__tensor):
                current_space += EXTRA_NODE_B64.get(__tensor.op.tag)
            # power instruction support b32
            if powi_complex_instructions_b32(__tensor):
                current_space += EXTRA_NODE_B32.get(__tensor.op.tag)
                self._tmp_ub_size += EXTRA_TEMP_BUFFER.get(__tensor.op.tag)
            # 5hd need one more coexist node
            if self._5hd_actions is not None and len(self._5hd_actions) > 0:
                current_space += 1
            if __tensor.op.tag == "elewise_binary_gcd":
                current_space += 24 if __tensor.dtype == "int64" else 4
                
            if util.need_temp_space(__tensor) or _need_external_space(__tensor):
                self._tmp_ub_size += block_size_bytes
            return current_space

        def _r_coexisting(_tensor):
            if _tensor in dependent_map and _tensor not in init_map:
                return len(dependent_map)
            _need_space = []
            for _tensor_i in _tensor.op.input_tensors:
                if _tensor_i in self._cst_compute_root_tensors | self._dyn_compute_root_tensors:
                    continue
                _need_space.append(_r_coexisting(_tensor_i))

            _current_space = _calc_current_space(_tensor)

            # correct ub size in vcmp or vsel or vcmpsel
            _correct_ub_size_by_cmp_sel(_tensor)

            _need_space.append(_current_space)
            _refresh_dependent(_tensor)
            if _tensor not in dependent_map:
                dependent_map[_tensor] = self._in_out_map[_tensor].copy()
            elif _tensor in init_map:
                init_map.remove(_tensor)
            return max(_need_space)

        def _refresh_dependent(_tensor):
            for _tensor_i in _tensor.op.input_tensors:
                if _tensor_i not in dependent_map:
                    continue
                dependent_map.get(_tensor_i, set()).discard(_tensor)
                if not dependent_map[_tensor_i]:
                    dependent_map.pop(_tensor_i)

        def _need_external_space(_tensor):
            exist_absorbable_broadcast = any(x in self._absorbable_broadcast_tensors
                                             for x in _tensor.op.input_tensors)
            if not exist_absorbable_broadcast:
                return False

            op_tag = util.get_dsl_insn(_tensor)
            support_vector_scalar_insns = ("elewise_binary_add", "elewise_binary_mul")
            if op_tag in set(SUPPORT_SCALAR_INSNS) - set(support_vector_scalar_insns):
                return True

            if util.is_v100() and op_tag in support_vector_scalar_insns and _tensor.dtype == "int32":
                return True

        coexisting_quantities = []
        dependent_map = {}
        init_map = set()
        all_producers = self._middle_tensors.copy()
        all_producers.update(self._out_tensors | self._input_tensors)
        for tensor_i in self._broadcast_store_predicate | self._store_predicate_common_tensors:
            dependent_map[tensor_i] = all_producers.copy()
            init_map.add(tensor_i)
        for tensor_i in self._out.op.input_tensors:
            if tensor_i in self._cst_compute_root_tensors | self._dyn_compute_root_tensors:
                continue
            coexisting_quantities.append(_r_coexisting(tensor_i))
        if not self._out.op.tag == FAKE_NODE_TAG:
            _current_space = _calc_current_space(self._out)

            # correct ub size in vcmp or vsel or vcmpsel
            _correct_ub_size_by_cmp_sel(self._out)

            coexisting_quantities.append(_current_space)

        self._coexisting_quantity = max(coexisting_quantities)
        self._coexisting_quantity += \
            len(self._compute_root_helper.mem_unique_tensors) if self._enable_compute_root else 0

        if self._coexisting_quantity == 1:
            self._tmp_ub_size += block_size_bytes
        brc_need_one_block = False
        for tensor_i in self._broadcast_tensors - self._absorbable_broadcast_tensors:
            if not tensor_i.op.input_tensors:
                continue
            if not self._is_last_align(tensor_i):
                brc_need_one_block = True
        if ((self._coexisting_quantity % 2 == 0 or self._tiling_strategy == TilingStrategy.CONST) and
            self._tmp_ub_size < block_size_bytes) and brc_need_one_block:
            self._tmp_ub_size += block_size_bytes

        if self._enable_compute_root:
            self._tmp_ub_size += self._compute_root_helper.compute_root_tmp_ub_size
