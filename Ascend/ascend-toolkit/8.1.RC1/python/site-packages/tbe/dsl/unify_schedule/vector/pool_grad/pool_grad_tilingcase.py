#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pool grad tiling case
"""

from dataclasses import dataclass
from enum import Enum

from tbe import tvm
from tbe.dsl.base import operation
from tbe.dsl.compute.constants import ComputeType
from tbe.dsl.unify_schedule import computation
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.constants import PoolGradPattern
from tbe.dsl.unify_schedule import util


class PoolGradComputation(computation.Computation):
    def __init__(self, outs, option):
        self._outs = list(outs) if isinstance(outs, (list, tuple)) else [outs]
        self._option = option
        self._graph = TensorGraph(self._outs)
        self._shape_p = None
        self._window_axes = None
        self._kernel_size = None
        self._strides = None

        self._init()

    @property
    def ah_len(self):
        return max(self._window_axes) - len(self._window_axes) + 1

    @property
    def at_len(self):
        return len(self._shape_p) - max(self._window_axes) - 1

    @property
    def ro_len(self):
        return len(self._window_axes)

    @property
    def rk_len(self):
        return len(self._window_axes)

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.POOL_GRAD]

    def do_tiling_case(self):
        if operation.in_static():
            return self._static_tiling_case()

        return self._dynamic_tiling_case()

    def get_sub_pattern(self):
        if len(self._shape_p) - 1 in self._window_axes:
            return PoolGradPattern.LAST_WINDOW_AXIS

        return PoolGradPattern.NLAST_WINDOW_AXIS

    def _init(self):
        tensor = self._graph.get_tensors(compute_type=ComputeType.COL2IMG)[0]
        self._shape_p = tensor.shape
        self._window_axes = util.shape_to_list(tensor.op.attrs["axes"])
        self._kernel_size = util.shape_to_list(tensor.op.attrs["sizes"])
        self._strides = util.shape_to_list(tensor.op.attrs["strides"])

    def _static_tiling_case(self):
        t_case = PoolGradTilingCase(mode=operation.get_op_mode())
        t_case.tiling_key = 0
        return [t_case]

    def _dynamic_tiling_case(self):
        if self.get_sub_pattern() == PoolGradPattern.NLAST_WINDOW_AXIS:
            strategies = {
                Strategy.UB_AH_BLOCK_A: {"ub_split_axes_len": self.ah_len},
                Strategy.UB_RO_BLOCK_A: {"ub_split_axes_len": self.ro_len},
                Strategy.UB_RO_BLOCK_R: {"ub_split_axes_len": self.ro_len},
                Strategy.UB_AT_BLOCK_A: {"ub_split_axes_len": self.at_len},
                Strategy.UB_AT_BLOCK_R: {"ub_split_axes_len": self.at_len},
                Strategy.UB_RK_BLOCK_A: {"ub_split_axes_len": self.rk_len},
                Strategy.UB_RK_BLOCK_R: {"ub_split_axes_len": self.rk_len},
            }
        else:
            strategies = {
                Strategy.UB_AH_BLOCK_A: {"ub_split_axes_len": self.ah_len},
                Strategy.UB_RO_BLOCK_A: {"ub_split_axes_len": self.ro_len},
                Strategy.UB_RO_BLOCK_R: {"ub_split_axes_len": self.ro_len},
                Strategy.UB_RK_BLOCK_A: {"ub_split_axes_len": self.rk_len},
                Strategy.UB_RK_BLOCK_R: {"ub_split_axes_len": self.rk_len},
            }

        cases = []
        for k, v in strategies.items():
            for i in range(v["ub_split_axes_len"]):
                t_case = PoolGradTilingCase(mode=operation.get_op_mode(),
                                            strategy=k,
                                            tiling_key=calc_tiling_key(k, i),
                                            ub_split_idx=i
                                            )
                cases.append(t_case)

        return cases


@dataclass
class PoolGradTilingCase:
    mode: str = None
    strategy: "Strategy" = None
    tiling_key: int = None
    ub_split_idx: int = None


class Strategy(Enum):
    # Tiling based on shape pattern:(a0, a1, .., am, r0, r1, .., rn, am+1, am+2, .., an)
    # - The head normal axis(a0, a1, .., am) called AH
    # - The tail normal axis(am+1, am+2, .., an) called AT
    # - The outer reduce axis called RO
    # - The kernel reduce axis called RK

    UB_AH_BLOCK_A = 10

    UB_RO_BLOCK_A = 11
    UB_RO_BLOCK_R = 12

    UB_AT_BLOCK_A = 13
    UB_AT_BLOCK_R = 14

    UB_RK_BLOCK_A = 15
    UB_RK_BLOCK_R = 16

    @classmethod
    def parse(cls, value_):
        for x in cls:
            if x.value == value_:
                return x
        return None


class TensorGraph:
    def __init__(self, outs):
        self._outs = outs
        self._tensors = []
        self._compute_type_map_tensors = {}

        self._build()

    @classmethod
    def _get_compute_type(cls, x):
        return x.op.attrs.get("_type")

    def get_tensors(self, compute_type=None):
        if compute_type is None:
            return self._tensors
        return self._compute_type_map_tensors.get(compute_type)

    def _build(self):
        def dfs(tensor):
            # type: (tvm.Tensor) -> None
            if tensor in self._tensors:
                return

            self._tensors.append(tensor)
            self._add_compute_type_map_tensor(tensor)

            for tensor_i in tensor.op.input_tensors:
                dfs(tensor_i)

        for out in self._outs:
            dfs(out)

    def _add_compute_type_map_tensor(self, tensor):
        compute_type = self._get_compute_type(tensor)
        tensors = self._compute_type_map_tensors.setdefault(compute_type, [])
        tensors.append(tensor)


def calc_tiling_key(strategy, ub_split_idx):
    compute_key = operation.get_context().get_current_compute().get("_key")

    return (compute_key*100 + strategy.value) * 100 + ub_split_idx


def parse_tiling_key(tiling_key):
    ub_split_idx = tiling_key % 100

    temp_key = tiling_key // 100
    strategy_key = temp_key % 100
    strategy =  Strategy.parse(strategy_key)

    return strategy, ub_split_idx
