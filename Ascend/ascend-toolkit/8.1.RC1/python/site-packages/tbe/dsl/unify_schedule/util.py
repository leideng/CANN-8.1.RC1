#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
schedule util
"""
from functools import reduce
from operator import mul
from typing import List
from typing import Union

from tbe import tvm
from tbe.common.platform import ASCEND_310
from tbe.common.platform import ASCEND_310P
from tbe.common.platform import ASCEND_610
from tbe.common.platform import ASCEND_910
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import BS9SX1A
from tbe.common.platform import HI3796CV300CS
from tbe.common.platform import HI3796CV300ES
from tbe.common.platform import SD3403
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.utils.errormgr import get_error_message
from tbe.dsl.base import operation
from tbe.dsl.base.expr_compare import expr_equal
from tbe.tvm import PlaceholderOp
from tbe.tvm import Tensor
from tbe.tvm import Var
from tbe.tvm.tir import Reduce
from tbe.tvm.tir import Div

from .constants import BROADCAST_INSNS
from .constants import DTYPE_BIT_MAPPING
from .constants import NEED_EXTENT_NODE_INSNS
from .constants import NEED_SPACE_WITH_DIFF_TYPE
from .constants import NEED_TEMP_SPACE_INSNS
from .constants import ONE_SHAPE_BROADCAST
from .constants import SUPPORT_SCALAR_INSNS
from .constants import TERNARY_INSNS
from .constants import VCMP_INSNS
from .constants import VCMPSEL_INSNS
from .constants import VSEL_INSNS

VAR_BOUND_LIMIT = 2147483647
NANO_UB_BLOCK_SIZE = 16
TINY_UB_SIZE = 30720


def is_true(expr, dict_args):
    """
    :param expr: condition
    :param dict_args: error message
    :return: RuntimeError
    """
    if not expr:
        raise RuntimeError(dict_args, get_error_message(dict_args))


def shape_to_list(shape):
    """
    :param shape:
    :return:
    """
    shape0 = []
    for i in shape:
        if isinstance(i, tvm.expr.ConstExpr):
            shape0.append(i.value)
        else:
            shape0.append(i)
    return shape0


def get_dsl_insn(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    tag = tensor.op.tag
    if tensor.op.tag.find("|") != -1:
        insn = tag.split("|")[0]
    else:
        insn = tag
    return insn


def support_scalar(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) in SUPPORT_SCALAR_INSNS


def is_v100():
    """
    :return:
    """
    return get_soc_spec(SHORT_SOC_VERSION) in (ASCEND_910, ASCEND_310)


def is_v200():
    """
    :return:
    """
    return get_soc_spec(SHORT_SOC_VERSION) in \
        (ASCEND_610, BS9SX1A, ASCEND_310P, HI3796CV300ES, HI3796CV300CS, SD3403)


def is_v220():
    """
    :return:
    """
    return get_soc_spec(SHORT_SOC_VERSION) in (ASCEND_910B, ASCEND_910_93)


def need_temp_space(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    op_tag = get_dsl_insn(tensor)
    return op_tag in NEED_TEMP_SPACE_INSNS or \
           (is_v100() and op_tag in NEED_SPACE_WITH_DIFF_TYPE and tensor.dtype == "int32")


def need_extent_node(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) in NEED_EXTENT_NODE_INSNS


def is_vcmp_insn(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) in VCMP_INSNS


def is_vsel_insn(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) == VSEL_INSNS


def is_vcmpsel_insn(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) in VCMPSEL_INSNS


def is_ternary_insn(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) in TERNARY_INSNS


def get_tensor_size(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    shape = shape_to_list(tensor.shape)
    if all(isinstance(i, int) for i in shape):
        return reduce(mul, shape_to_list(tensor.shape), 1)
    return -1


def is_vtranspose_broadcast(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    if not is_broadcast(tensor) or len(tensor.op.input_tensors) != 1:
        return False
    dtype_no_fp16 = tensor.dtype != "float16"
    compile_broadcast_no_last = is_unified_broadcast(tensor) and \
                                expr_equal(tensor.shape[-1], tensor.op.input_tensors[0].shape[-1]) and \
                                not expr_equal(tensor.shape[-1], 1)
    runtime_broadcast_no_last = expr_equal(tensor.shape[-1], tensor.op.input_tensors[0].shape[-1]) and \
                                isinstance(tensor.shape[-1], tvm.expr.ConstExpr) and \
                                isinstance(tensor.op.input_tensors[0].shape[-1], tvm.expr.ConstExpr) and \
                                not expr_equal(tensor.shape[-1], 1)
    return not (dtype_no_fp16 or compile_broadcast_no_last or runtime_broadcast_no_last)


def is_broadcast(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) in BROADCAST_INSNS


def is_one_shape_broadcast(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) == ONE_SHAPE_BROADCAST


def is_unknown_broadcast(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) == "unknown_broadcast"


def is_unified_broadcast(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) == "unified_broadcast"


def is_scalar_broadcast(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) == "broadcast"


def is_placeholder(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return isinstance(tensor.op, tvm.PlaceholderOp)


def is_cast_insn(tensor: tvm.Tensor):
    """
    :param tensor:
    :return:
    """
    return get_dsl_insn(tensor) == "elewise_single_cast"


def merge_value(map_: dict, key, value):
    """
    :param map_:
    :param key:
    :param value: value container is set
    :return:
    """
    if key not in map_:
        map_[key] = set()
    if isinstance(value, list):
        map_[key].update(value)
    elif isinstance(value, set):
        map_[key].union(value)
    else:
        map_[key].add(value)


def get_bound(expr):
    """
    :param expr:
    :return:
    """
    valid_types = (int, tvm.tir.PrimExpr)
    is_true(isinstance(expr, valid_types),
            {"errCode": "E90001",
            "detailed_cause": f"Only accept (int, expr), but now is {type(expr)}."
            })

    if isinstance(expr, int):
        return expr, expr
    if isinstance(expr, tvm.tir.IntImm):
        return expr.value, expr.value
    if isinstance(expr, tvm.Var):
        return operation.get_te_var(expr.name).get_bound()

    def _mul(_a, _b):
        if _a is None or _b is None:
            return None
        _bound = _a * _b
        return None if _bound > VAR_BOUND_LIMIT else _bound

    def _add(_a, _b):
        if _a is None or _b is None:
            return None
        _bound = _a + _b
        return None if _bound > VAR_BOUND_LIMIT else _bound

    def _sub(_a, _b):
        if _a is None or _b is None:
            return None
        _bound = _a - _b
        return None if _bound > VAR_BOUND_LIMIT else _bound

    def _floordiv(_a, _b):
        if _a is None and _b is None:
            return 1
        if _a is None:
            return None
        if _b is None:
            return 1
        if _a < 0 or _b < 0:
            dict_args = {
                "errCode": "E90001",
                "detailed_cause": "floordiv range cannot be negative"
            }
            raise RuntimeError(dict_args, get_error_message(dict_args))
        _bound = _a // _b
        return None if _bound > VAR_BOUND_LIMIT else _bound

    def _max(_a, _b):
        if _a is None or _b is None:
            return None
        return max(_a, _b)

    def _min(_a, _b):
        if _a is None or _b is None:
            return None
        return min(_a, _b)

    def _parse(_expr):
        if isinstance(_expr, tvm.expr.ConstExpr):
            return _expr.value, _expr.value
        elif isinstance(_expr, tvm.Var):
            bound = operation.get_te_var(_expr.name).get_bound()
            return bound[0], bound[1]
        elif isinstance(_expr, tvm.expr.Mul):
            left_lower, left_upper = _parse(_expr.a)
            right_lower, right_upper = _parse(_expr.b)
            _lower, _upper = _mul(left_lower, right_lower), _mul(left_upper, right_upper)
        elif isinstance(_expr, tvm.expr.Add):
            left_lower, left_upper = _parse(_expr.a)
            right_lower, right_upper = _parse(_expr.b)
            _lower, _upper = _add(left_lower, right_lower), _add(left_upper, right_upper)
        elif isinstance(_expr, tvm.expr.Max):
            left_lower, left_upper = _parse(_expr.a)
            right_lower, right_upper = _parse(_expr.b)
            _lower, _upper = _max(left_lower, right_lower), _max(left_upper, right_upper)
        elif isinstance(_expr, tvm.expr.Sub):
            left_lower, left_upper = _parse(_expr.a)
            right_lower, _ = _parse(_expr.b)
            _lower, _upper = _sub(left_lower, right_lower), _sub(left_upper, right_lower)
        elif isinstance(_expr, tvm.expr.FloorDiv):
            left_lower, left_upper = _parse(_expr.a)
            right_lower, right_upper = _parse(_expr.b)
            _lower, _upper = _floordiv(left_lower, right_upper), _floordiv(left_upper, right_lower)
        else:
            dict_args = {}
            dict_args["errCode"] = "E90001"
            dict_args["detailed_cause"] = "Only accept (ConstExpr, Var, Mul, Max, Add), but now " \
                                          "is [%s]" % type(_expr)
            raise RuntimeError(dict_args, get_error_message(dict_args))
        return _lower, _upper

    return _parse(expr)


def get_ub_size():
    """
    :return:
    """
    return int(get_soc_spec("UB_SIZE"))


def get_ub_block_size():
    """
    :return:
    """
    return int(get_soc_spec("ubblock_size"))


def get_l1_size():
    """
    :return:
    """
    return get_soc_spec("L1_SIZE")


def get_core_num():
    """
    :return:
    """
    return get_soc_spec("CORE_NUM")


def equals_one(_x):
    """
    :param _x:
    :return:
    """
    if isinstance(_x, tvm.expr.ConstExpr):
        return _x.value == 1
    if isinstance(_x, int):
        return _x == 1
    return False


def ceil_div(num, factor):
    """
    :param num:
    :param factor:
    :return:
    """
    return (num + factor - 1) // factor


def ceil_align(num, factor):
    """
    :param num:
    :param factor:
    :return:
    """
    return ceil_div(num, factor) * factor


def tvm_ceil_align(num, factor):
    """
    :param num:
    :param factor:
    :return:
    """
    return Div(num + factor - 1, factor) * factor


def add_sch_additional_entry(sch, k, v):
    """
    :param sch:
    :param k:
    :param v:
    :return:
    """
    if not hasattr(sch, "addition"):
        sch.addition = {}
    sch.addition[k] = v


def get_sch_additional_entry(sch, k):
    """
    :param sch:
    :param k:
    :return:
    """
    if not hasattr(sch, "addition"):
        return None
    return sch.addition.get(k)


def is_reduce_tensor(tensor: Tensor) -> bool:
    """
    Check if tensor contains reduce body
    """
    if isinstance(tensor.op, PlaceholderOp):
        return False
    if isinstance(tensor.op.body[0], Reduce):
        return True
    return False


def get_reduce_axes(reduce_tensor: Tensor) -> List[Union[Var, tvm.tir.IntImm]]:
    """
    Get reduce axes var of reduce tensor
    """
    if not is_reduce_tensor(reduce_tensor):
        raise RuntimeError("Cannot get reduce axes of non-reduce tensor!")
    reduce_tensor_body = reduce_tensor.op.body
    reduce_tensor_axes = list(reduce_tensor_body[0].axis)
    for idx, axis in enumerate(reduce_tensor_axes):
        reduce_tensor_axes[idx] = axis.var
    return reduce_tensor_axes


def get_reduce_all_axes(reduce_tensor: Tensor) -> List[Union[Var, tvm.tir.IntImm]]:
    """
    Get all axes var for reduce tensor
    indices: ARA for example, get [i0.c, k1, i2.c]
    """
    reduce_tensor_body = reduce_tensor.op.body
    return list(reduce_tensor_body[0].source[0].indices)


def get_reduce_axis_indexes(reduce_tensor: Tensor) -> List[int]:
    """
    Get all reduce axis index
    """
    return [get_reduce_all_axes(reduce_tensor).index(axis) for axis in get_reduce_axes(reduce_tensor)]


def is_keepdims(reduce_tensor: Tensor) -> bool:
    """
    Check if reduce tensor is keepdims
    """
    return len(reduce_tensor.shape) == len(get_reduce_all_axes(reduce_tensor))


def add_sch_cce_special_entry(sch, k, v):
    """
    add cce_special info
    :param sch:
    :param k:
    :param v:
    :return:
    """
    if not hasattr(sch, "cce_special"):
        sch.cce_special = {}
    sch.cce_special[k] = v


def get_align_factor_in_block(dtype):
    block_size_with_bit = 32*8
    return block_size_with_bit // DTYPE_BIT_MAPPING.get(dtype)


def is_nano():
    return int(get_soc_spec("ubblock_size")) == NANO_UB_BLOCK_SIZE


def is_tiny():
    return int(get_soc_spec("ub_size")) == TINY_UB_SIZE