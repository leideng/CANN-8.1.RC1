#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
storage bound util
"""

from tbe.dsl.unify_schedule import util
from tbe.dsl.compute.util import DTYPE_MAP, in_dynamic_and_static_unify
from tbe.common.platform import intrinsic_check_support
from tbe.common.platform import platform_info


EXTRA_NODE_COMPLEX = {
    "elewise_single_abs": 4,
    "elewise_single_cast": 2,
    "elewise_binary_mul": 2,
    "elewise_binary_div": 2,
    "elewise_binary_complex": 2,
}

EXTRA_TEMP_COMPLEX = {
    "elewise_single_abs": 256,
    "elewise_single_cast": 512,
    "elewise_single_VS_mul": 32,
    "elewise_binary_add": 32,
    "elewise_binary_sub": 32,
    "elewise_binary_mul": 512,
    "elewise_binary_div": 512,
    "elewise_binary_real": 512,
    "elewise_binary_complex": 512,
}

EXTRA_BLOCK = {
    "elewise_single_abs": 1,
    "elewise_single_VS_mul": 1,
    "elewise_single_cast": 1,
    "elewise_binary_div": 1,
    "elewise_binary_mul": 1,
    "elewise_binary_real": 1,
    "elewise_binary_complex": 1,
}

EXTRA_NODE_B32 = {
    "elewise_binary_powi": 10
}

EXTRA_NODE_B64 = {
    "elewise_binary_vcmpv_gt": 5,
    "elewise_binary_vcmpv_ge": 5,
    "elewise_binary_vcmpv_lt": 5,
    "elewise_binary_vcmpv_le": 5,
    "elewise_binary_vcmpv_eq": 3,
    "elewise_binary_vcmpv_ne": 3,
    "elewise_single_cast|not_auto_cast": 2,
    "elewise_single_VS_add": 2,
    "elewise_binary_add": 2,
    "elewise_binary_sub": 2,
    "elewise_binary_max": 5,
    "elewise_binary_min": 5,
    "elewise_single_abs": 5
}

EXTRA_TEMP_BUFFER = {
    "elewise_single_VS_add": 520,
    "elewise_binary_add": 520,
    "elewise_binary_sub": 520,
    "elewise_single_abs": 520,
    "elewise_binary_powi": 192
}

INSTRIC_SUPPORT_MAPPING = {
    "elewise_single_VS_add": "Intrinsic_vadds",
    "elewise_binary_add": "Intrinsic_vadd",
    "elewise_binary_sub": "Intrinsic_vsub",
    "elewise_single_abs": "Intrinsic_vabs"
}

VECTOR_SUPPORT_MAPPING = {
    "elewise_binary_max": "vector_max",
    "elewise_binary_min": "vector_min"
}

# u82s64, u82s32, s82s32, s322s64 and s642s32 will not adapt the pass instructions.
CAST_EXTRA_NODE_AND_UB = {
    "Ascend910B": {
        "s642s8": {"node": 1, "extra_ub": 8192}, "s642u8": {"node": 1, "extra_ub": 8192},
        # s322s8 has not adapted
        "s322u8": {"node": 1, "extra_ub": 4096},
        "u322u8": {"node": 1, "extra_ub": 4096}, "u322s8": {"node": 1, "extra_ub": 4096},
        "s162u8": {"node": 1, "extra_ub": 2048}, "s162s8": {"node": 1, "extra_ub": 2048},
        "u162u8": {"node": 1, "extra_ub": 2048}, "u162s8": {"node": 1, "extra_ub": 2048},
        "u322s64": {"node": 1, "extra_ub": 32},
        "u162s64": {"node": 2, "extra_ub": 32}, "s162s64": {"node": 1, "extra_ub": 0},
        "s82s64": {"node": 2, "extra_ub": 0},
        "s162s32": {"node": 1, "extra_ub": 0}, "s162u32": {"node": 1, "extra_ub": 0},
        "u162s32": {"node": 2, "extra_ub": 32}, "u162u32": {"node": 2, "extra_ub": 32},
        "s82u32": {"node": 2, "extra_ub": 0},
        "u82u32": {"node": 2, "extra_ub": 0},
        "u82s16": {"node": 1, "extra_ub": 0}, "s82s16": {"node": 1, "extra_ub": 0},
        "u82u16": {"node": 1, "extra_ub": 0}, "s82u16": {"node": 1, "extra_ub": 0}
    },
    "Ascend910_93": {
        "s642s8": {"node": 1, "extra_ub": 8192}, "s642u8": {"node": 1, "extra_ub": 8192},
        # s322s8 has not adapted
        "s322u8": {"node": 1, "extra_ub": 4096},
        "u322u8": {"node": 1, "extra_ub": 4096}, "u322s8": {"node": 1, "extra_ub": 4096},
        "s162u8": {"node": 1, "extra_ub": 2048}, "s162s8": {"node": 1, "extra_ub": 2048},
        "u162u8": {"node": 1, "extra_ub": 2048}, "u162s8": {"node": 1, "extra_ub": 2048},
        "u322s64": {"node": 1, "extra_ub": 32},
        "u162s64": {"node": 2, "extra_ub": 32}, "s162s64": {"node": 1, "extra_ub": 0},
        "s82s64": {"node": 2, "extra_ub": 0},
        "s162s32": {"node": 1, "extra_ub": 0}, "s162u32": {"node": 1, "extra_ub": 0},
        "u162s32": {"node": 2, "extra_ub": 32}, "u162u32": {"node": 2, "extra_ub": 32},
        "s82u32": {"node": 2, "extra_ub": 0},
        "u82u32": {"node": 2, "extra_ub": 0},
        "u82s16": {"node": 1, "extra_ub": 0}, "s82s16": {"node": 1, "extra_ub": 0},
        "u82u16": {"node": 1, "extra_ub": 0}, "s82u16": {"node": 1, "extra_ub": 0}
    },
    "Ascend310P": {
        # only adapt for s642u8, s322u8, s322s16, s162s32, s162s8, u82s8 
        "s642u8": {"node": 1, "extra_ub": 8192},
        "s322u8": {"node": 1, "extra_ub": 4096},
        "s162s8": {"node": 1, "extra_ub": 2048},
        "s162s32": {"node": 2, "extra_ub": 1056},
    }
}


def cast_complex_instructions(tensor):
    """
    Check if cast impl by complex instructions
    """
    if not tensor.op.input_tensors:
        return False
    op_tag = util.get_dsl_insn(tensor)
    if op_tag != "elewise_single_cast":
        return False

    complex_cast_support = ["s642s32", "s642f16", "s322s64"]
    src_dtype = tensor.op.input_tensors[0].dtype
    cur_dtype = tensor.dtype
    cast_type = f"{DTYPE_MAP.get(src_dtype)}2{DTYPE_MAP.get(cur_dtype)}"

    return cast_type in complex_cast_support and not intrinsic_check_support("Intrinsic_vconv", cast_type)


def cast_is_support_ints2ints(tensor, soc_ver):
    if not tensor.op.input_tensors:
        return False
    op_tag = util.get_dsl_insn(tensor)
    if op_tag != "elewise_single_cast":
        return False

    if soc_ver not in ("Ascend910B", "Ascend910_93", "Ascend310P"):
        return False

    return True


def cast_ints2ints_external_space(tensor):
    """
    Returns the extra space and coexist node required by the integer conversion.
    """
    soc_ver = platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if not cast_is_support_ints2ints(tensor, soc_ver):
        return 0, 0

    src_dtype = tensor.op.input_tensors[0].dtype
    cur_dtype = tensor.dtype
    cast_type = f"{DTYPE_MAP.get(src_dtype)}2{DTYPE_MAP.get(cur_dtype)}"

    support_ints_list = CAST_EXTRA_NODE_AND_UB.get(soc_ver, None)
    if not support_ints_list or cast_type not in support_ints_list:
        return 0, 0
    
    return support_ints_list[cast_type]['node'], support_ints_list[cast_type]['extra_ub'] 


def cast_ints2ints_need_alignment(tensor):
    """
    Check if ints to ints need to align
    """
    soc_ver = platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if not cast_is_support_ints2ints(tensor, soc_ver):    
        return False
    
    dtypes_need_alignment_map = {
        "Ascend910B": [
                    "s642s8", "s642u8",
                    # s322s8 has not adapted
                    "s322u8",
                    "u322u8", "u322s8",
                    "s162u8", "s162s8",
                    "u162u8", "u162s8"
        ],
        "Ascend910_93": [
                    "s642s8", "s642u8",
                    # s322s8 has not adapted
                    "s322u8",
                    "u322u8", "u322s8",
                    "s162u8", "s162s8",
                    "u162u8", "u162s8"
        ],
        "Ascend310P": ["s642u8", "s322u8", "s162s8"]
    }

    src_dtype = tensor.op.input_tensors[0].dtype
    cur_dtype = tensor.dtype
    cast_type = f"{DTYPE_MAP.get(src_dtype)}2{DTYPE_MAP.get(cur_dtype)}"
    
    dtypes_need_alignment = dtypes_need_alignment_map.get(soc_ver, None)
    return dtypes_need_alignment is not None and cast_type in dtypes_need_alignment


def vcmp_complex_instructions(tensor):
    """
    check if vcmp impl by complex instructions
    """
    if not tensor.op.input_tensors:
        return False
    op_tag = util.get_dsl_insn(tensor)
    if not op_tag.startswith("elewise_binary_vcmpv_"):
        return False

    src_dtype = tensor.op.input_tensors[0].dtype
    return src_dtype in ("int64", "uint64") and \
        not intrinsic_check_support(f"Intrinsic_vcmpv_{op_tag.split('_')[-1]}", tensor.dtype)


def vsel_complex_instructions(_tensor):
    """
    check if vsel impl by complex instructions
    """
    if not _tensor.op.input_tensors:
        return False
    if len(_tensor.op.input_tensors) < 2:
        return False
    op_tag = util.get_dsl_insn(_tensor)
    if not op_tag == "elewise_multiple_sel":
        return False

    condition_dtype = _tensor.op.input_tensors[0].dtype
    lhs_dtype = _tensor.op.input_tensors[1].dtype
    condition_dtype_support = condition_dtype in ("bool", "int8")
    lhs_dtype_support = lhs_dtype in ("int64", "uint64")
    return condition_dtype_support and lhs_dtype_support


def powi_complex_instructions_b32(_tensor):
    """
    check if powi impl by complex instructions
    """
    if not _tensor.op.input_tensors:
        return False
    if len(_tensor.op.input_tensors) < 2:
        return False
    op_tag = util.get_dsl_insn(_tensor)
    if not op_tag == "elewise_binary_powi":
        return False

    lhs_dtype = _tensor.op.input_tensors[1].dtype
    lhs_dtype_support = lhs_dtype == "int32"
    return lhs_dtype_support


def base_op_complex_instructions(tensor):
    """
    base op complex instructions
    """
    if not tensor.op.input_tensors:
        return False
    op_tag = util.get_dsl_insn(tensor)
    return tensor.dtype == "int64" and op_tag in INSTRIC_SUPPORT_MAPPING and \
        not intrinsic_check_support(INSTRIC_SUPPORT_MAPPING.get(op_tag), tensor.dtype)


def base_single_instructions_b64(tensor):
    """
    base op single instructions
    """
    if not tensor.op.input_tensors:
        return False
    op_tag = util.get_dsl_insn(tensor)
    return tensor.dtype == "int64" and op_tag in VECTOR_SUPPORT_MAPPING


def complex_instructions_b64_impl(tensor):
    """
    check if meet complex instructions which need extra coexist node and temp ub buffer
    """
    # placeholder no need calculate
    return cast_complex_instructions(tensor) or vcmp_complex_instructions(tensor) or \
           base_op_complex_instructions(tensor)


def complex_calc_instructions(tensor):
    """
    check if this tensor is complex calculation
    """
    if tensor.op.input_tensors:
        for in_tensor in tensor.op.input_tensors:
            if "complex" in in_tensor.dtype:
                return True
    return "complex" in tensor.dtype
