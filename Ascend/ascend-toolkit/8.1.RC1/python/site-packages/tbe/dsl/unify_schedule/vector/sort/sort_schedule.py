#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
sort schedule
"""
from typing import Any

from tbe import tvm
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform.platform_info import ASCEND_031
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform import ASCEND_310B
from tbe.common.platform import AS31XM1
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import ASCEND_610LITE
from tbe.common.platform import BS9SX2A
from tbe.common.platform import MC61AM21A
from tbe.dsl.base import operation

from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import SortPattern
from ...constants import Pattern
from ...schedule import Schedule
from .sort_tilingcase import TilingStrategy
from .sort_tilingcase import SortCompileInfo
from .sort_tilingcase import SORT
from .sort_tilingcase import SORT_MERGE
from .sort_tilingcase import SORT_SYNC_MERGE
from .sort_tilingcase import SORT_MERGE_SYNC_MERGE
from .sort_tilingcase import DB
from .sort_tilingcase import K_SMALL
from .sort_tilingcase import ONE_DIM
from .sort_tilingcase import WORKSPACE
from .sort_tilingcase import DEFAULT
from .sort_tilingcase import ceil_div

# block size in D architecture
BLOCK_SIZE_BYTE = 32
# reserve space
RESERVE_SPACE = 1024
SCOPE_UB = "local.UB"
DMA_COPY = "dma_copy"
VEC_SORT = "vector_sort"
VEC_TOPK = "vector_topk"
VEC_MERGE_SORT = "vector_merge_sort"
VEC_MERGE_TOPK = "vector_merge_topk"
CAST = "elewise_single_cast|not_auto_cast"
BF16 = "bfloat16"
FP32 = "float32"


class SortSchedule(Schedule):
    """
    sort schedule
    """
    def __init__(self, outs, tiling_case):
        self._outs = outs
        self._include_index = len(self._outs) > 1
        self._out_data = self._outs[0]
        self._out_index = self._outs[1] if self._include_index else None
        self._input_x_tensor = None
        self._sch = None
        self._scope = SCOPE_UB
        self._soc_version = get_soc_spec(SHORT_SOC_VERSION)
        self._op_mode = operation.get_context().get(SortCompileInfo.OP_MODE)
        self._coexisting_quantity = operation.get_context().get(SortCompileInfo.COEXISTING_NUM)
        self._proposal_num = operation.get_context().get(SortCompileInfo.PROPOSAL_NUM)
        self._tiling_case = tiling_case
        self._tiling_strategy = self._tiling_case.get("tiling_strategy")
        self._tiling_key = self._tiling_case.get("key")
        self._tiling_mode = self._tiling_case.get("mode")
        self._tiling_pattern = self._tiling_case.get("pattern")

        self._vmsort_ub_limit = 0
        self._sort_axis_num = 0
        self._x_dtype_bytes = DTYPE_BYTE_MAPPING.get(self._out_data.dtype if self._out_data.dtype != BF16 else FP32)
        self._tensor_storage_bound = None
        self._ub_size = util.get_ub_size() - RESERVE_SPACE
        self._block_idx = tvm.thread_axis("blockIdx.x")
        self._align_factor = 16
        self._specific_version = self._soc_version in [ASCEND_031, ASCEND_310B, ASCEND_910B, ASCEND_910_93,
            AS31XM1, ASCEND_610LITE, BS9SX2A, MC61AM21A]

        self._input_tensors = []
        self._compute_at_map = {}
        self._block_bind_axis = []
        self._compute_at_axis = []
        self._emit_insn_map = {}
        self._cache_read_tensor = None
        self._cache_write_tensor = []
        self._sort_x_ub_tensor = None
        self._sort_x_sf = None
        self._sort_x_sf_sf = None
        self._vsort_emit_at_axis = None
        self._vmsort_emit_at_axis = []
        self._res_emit_at_axis = None
        self._ub_factor_0 = 0
        self._ub_factor_1 = 0
        self._block_factor = 0
        self._start_addr = None
        self._block_stride_0 = None
        self._block_stride_1 = None
        self._workspace_size = None

        # static tiling
        self._const_block_axis = 0
        self._const_ub_axis = 0
        self._const_block_factor = 1
        self._const_ub_factor_0 = 1
        self._const_ub_factor_1 = 1
        # dynamic tiling
        self._block_tiling_vars = {}
        self._ub_tiling_vars = {}

        # bf16 cast tensor
        self._cast_tensor = None

    @classmethod
    def get_instance(cls, outs, tiling_case):  # type: (list[Any], Any) -> "Schedule"
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):  # type: () -> list[str]
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):  # type: () -> list[str]
        return [Pattern.SORT]

    @classmethod
    def get_supported_sub_pattern(cls):  # type: () -> list[str]
        return [SortPattern.NORMAL_SCHEDULE]

    def do_schedule(self):  # type: () -> TVM_Schedule
        """
        schedule body
        :return:
        """
        self._construct_compute_graph()

        self._calc_tiling()

        if self._include_index:
            self._sch = tvm.create_schedule([self._out_data.op, self._out_index.op])
        else:
            self._sch = tvm.create_schedule(self._out_data.op)
        self._sch.tiling_key = self._tiling_key

        self._do_tiling(before_cache=True)

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._do_tiling(before_cache=False)

        self._set_scope()

        self._do_store_predicate()

        self._do_double_buffer()

        self._calc_storage_bound()
        self._do_storage_bound()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_storage_align()
        self._do_storage_align()

        self._do_block_sync()

        self._calc_compute_at()
        self._do_compute_at()

        self._do_allocate_root()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._set_constraint()

        self._add_compile_info()

        self._add_block_sync_flag()

        return self._sch

    def _set_scope(self):
        if self._cast_tensor is not None:
            self._sch[self._cast_tensor].set_scope(self._scope)

    def _construct_compute_graph(self):
        visited_tensors = set()
        self.__dfs_sub_graph(self._out_data, visited_tensors)

    def __dfs_sub_graph(self, out, visited_tensors: set):
        for tensor_i in out.op.input_tensors:
            if util.is_placeholder(tensor_i):
                self._input_tensors.append(tensor_i)
            if tensor_i.op.tag == CAST:
                self._cast_tensor = tensor_i
            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self.__dfs_sub_graph(tensor_i, visited_tensors)

    def _calc_tiling(self):
        funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                 TilingStrategy.STATIC: self._calc_tiling_static}
        funcs.get(self._tiling_strategy)()

    def _calc_tiling_dynamic(self):
        shape = util.shape_to_list(self._input_tensors[0].shape)
        b_i = self._tiling_case.get("block_axis")
        u_i = self._tiling_case.get("ub_axis")
        b_bound = (1, util.get_bound(shape[b_i])[1])
        u_bound = (1, util.get_bound(shape[u_i])[1])
        self._block_tiling_vars[b_i] = operation.var_inner("_block_factor_" + str(b_i), b_bound, "int64")
        self._ub_tiling_vars[u_i] = [operation.var_inner("_ub_factor_0_" + str(u_i), u_bound, "int64")]
        if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
            self._ub_tiling_vars[u_i].append(operation.var_inner("_ub_factor_1_" + str(u_i), u_bound, "int64"))
        if self._tiling_mode == K_SMALL:
            self._start_addr = operation.var_inner("_block_start_addr", (1, None), "int64")
        if self._tiling_mode in [WORKSPACE, K_SMALL, ONE_DIM]:
            self._block_stride_0 = operation.var_inner("_block_stride_0", (1, None), "int64")
            if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
                self._block_stride_1 = operation.var_inner("_block_stride_1", (1, None), "int64")
            self._workspace_size = operation.var_inner("_workspace_size", (1, None), "int64")

    def _calc_tiling_static(self):
        self._const_block_axis = self._tiling_case.get("block_axis")
        self._const_block_factor = self._tiling_case.get("block_factor")
        self._const_ub_axis = self._tiling_case.get("ub_axis")
        self._const_ub_factor_0 = self._tiling_case.get("ub_factor_0")
        self._const_ub_factor_1 = self._tiling_case.get("ub_factor_1")
        self._start_addr = self._tiling_case.get("block_start_addr")
        self._block_stride_0 = self._tiling_case.get("block_stride_0")
        self._block_stride_1 = self._tiling_case.get("block_stride_1")
        self._workspace_size = self._tiling_case.get("workspace_size")

    def _do_tiling(self, before_cache):
        funcs = {TilingStrategy.DYNAMIC: self._do_tiling_dynamic,
                 TilingStrategy.STATIC: self._do_tiling_static}
        funcs.get(self._tiling_strategy)(before_cache)

    def _do_tiling_dynamic(self, before_cache):
        b_idx = self._tiling_case.get("block_axis")
        u_idx = self._tiling_case.get("ub_axis")
        ub_factor_1 = None
        if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
            ub_factor_1 = self._ub_tiling_vars.get(u_idx)[1]

        self._do_tiling_base(b_idx, self._block_tiling_vars.get(b_idx), self._ub_tiling_vars.get(u_idx)[0],
                             ub_factor_1, before_cache)

    def _do_tiling_static(self, before_cache):
        b_idx = self._const_block_axis
        self._do_tiling_base(b_idx, self._const_block_factor, self._const_ub_factor_0,
                             self._const_ub_factor_1, before_cache)

    def _do_tiling_base(self, b_idx, block_factor, ub_factor_0, ub_factor_1, before_cache):
        if before_cache:
            if self._tiling_pattern != SORT:
                u_o, u_i = self._sch[self._out_data].split(self._sch[self._out_data].op.sort_axis[0],
                                                           factor=ub_factor_0)
                self._sort_x_sf = self._sch.sfactor(self._out_data, u_o, add_dim=self._proposal_num)
                self._outs.append(self._sort_x_sf)
                if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
                    u_o_o, u_i_i = self._sch[self._sort_x_sf].split(self._sch[self._sort_x_sf].op.sort_axis[0],
                                                                    factor=ub_factor_1)
                    self._sort_x_sf_sf = self._sch.sfactor(self._sort_x_sf, u_o_o)
                    self._outs.append(self._sort_x_sf_sf)
        else:
            if self._tiling_pattern == SORT:
                b_o, b_i = self._sch[self._out_data].split(self._sch[self._out_data].op.axis[b_idx],
                                                           factor=block_factor)
                self._block_bind_axis.append(b_o)
                u_o, u_i = self._sch[self._out_data].split(b_i, factor=ub_factor_0)
                self._compute_at_axis.append(u_o)
                self._vsort_emit_at_axis = self._sch[self._sort_x_ub_tensor].op.axis[0]
                self._res_emit_at_axis = u_i
            else:
                self._ub_factor_0 = ub_factor_0
                if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
                    self._block_bind_axis.append(self._sch[self._out_data].op.axis[0])
                    self._block_bind_axis.append(self._sch[self._sort_x_sf].op.axis[1])
                    self._compute_at_axis.append(self._sch[self._sort_x_ub_tensor].op.axis[2])
                    self._compute_at_axis.append(self._sch[self._sort_x_sf_sf].op.axis[2])
                    self._compute_at_axis.append(self._sch[self._sort_x_sf].op.axis[1])
                    self._compute_at_axis.append(self._sch[self._out_data].op.axis[0])
                    self._vsort_emit_at_axis = self._sch[self._sort_x_ub_tensor].op.axis[3]
                    self._res_emit_at_axis = self._sch[self._sort_x_sf_sf].op.axis[3]
                    self._vmsort_emit_at_axis.append(self._sch[self._sort_x_sf].op.axis[2])
                    self._vmsort_emit_at_axis.append(self._sch[self._out_data].op.axis[-1])
                    self._block_factor = ceil_div(self._sort_axis_num, ub_factor_0)
                    self._ub_factor_1 = ub_factor_1
                else:
                    self._compute_at_axis.append(self._sch[self._sort_x_ub_tensor].op.axis[1])
                    self._compute_at_axis.append(self._sch[self._sort_x_sf].op.axis[1])
                    self._vsort_emit_at_axis = self._sch[self._sort_x_ub_tensor].op.axis[2]
                    self._res_emit_at_axis = self._sch[self._sort_x_sf].op.axis[2]
                    self._vmsort_emit_at_axis.append(self._sch[self._out_data].op.axis[-1])
                    if self._tiling_pattern == SORT_MERGE:
                        b_o, b_i = self._sch[self._out_data].split(self._sch[self._out_data].op.axis[b_idx],
                                                                   factor=block_factor)
                        self._block_bind_axis.append(b_o)
                        self._compute_at_axis.append(b_o)
                    else:
                        self._block_bind_axis.append(self._sch[self._out_data].op.axis[0])
                        self._block_bind_axis.append(self._sch[self._sort_x_sf].op.axis[1])
                        self._compute_at_axis.append(self._sch[self._out_data].op.axis[0])
                        self._block_factor = ceil_div(self._sort_axis_num, ub_factor_0)

    def _calc_cache_read(self):
        self._input_x_tensor = self._input_tensors[0]
        self._sort_axis_num = self._input_x_tensor.shape[-1]

    def _do_cache_read(self):
        if self._tiling_pattern == SORT:
            consumer_tensor = self._cast_tensor if self._cast_tensor is not None else self._out_data
            self._cache_read_tensor = self._sch.cache_read(self._input_x_tensor, self._scope, consumer_tensor)
        elif self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
            consumer_tensor = self._cast_tensor if self._cast_tensor is not None else self._sort_x_sf_sf
            self._cache_read_tensor = self._sch.cache_read(self._input_x_tensor, self._scope, consumer_tensor)
        else:
            consumer_tensor = self._cast_tensor if self._cast_tensor is not None else self._sort_x_sf
            self._cache_read_tensor = self._sch.cache_read(self._input_x_tensor, self._scope, consumer_tensor)

    def _calc_cache_write(self):
        if self._tiling_pattern == SORT:
            self._cache_write_tensor.append(self._out_data)
            if self._include_index:
                self._cache_write_tensor.append(self._out_index)
        elif self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
            self._cache_write_tensor.append(self._sort_x_sf_sf)
        else:
            self._cache_write_tensor.append(self._sort_x_sf)

    def _do_cache_write(self):
        self._sort_x_ub_tensor = self._sch.cache_write(self._cache_write_tensor, self._scope)[0]

    def _do_store_predicate(self):
        align = BLOCK_SIZE_BYTE // self._x_dtype_bytes
        sort_axis_align = ceil_div(self._sort_axis_num, align) * align
        stride_0 = sort_axis_align * self._proposal_num
        stride_1 = self._ub_factor_0 * self._proposal_num
        if self._tiling_mode in [WORKSPACE, K_SMALL, ONE_DIM]:
            stride_0 = self._block_stride_0
            stride_1 = self._block_stride_1

        if self._tiling_pattern in [SORT_MERGE, SORT_SYNC_MERGE]:
            self._sch[self._sort_x_sf].set_store_predicate(
                self._sch[self._sort_x_sf].op.axis[1] * self._ub_factor_0 +
                self._sch[self._sort_x_sf].op.axis[2] < self._sort_axis_num, True)
            self._sch[self._sort_x_sf].bind_buffer(self._sch[self._sort_x_sf].op.axis[0], stride=stride_0, offset=0)
        elif self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
            self._sch[self._sort_x_sf].set_store_predicate(
                [self._sch[self._sort_x_sf].op.axis[1] * self._ub_factor_0 +
                 self._sch[self._sort_x_sf].op.sort_axis[0] * self._ub_factor_1 +
                 self._sch[self._sort_x_sf].op.sort_axis[1] < self._sort_axis_num,
                 self._sch[self._sort_x_sf].op.axis[1] * self._ub_factor_0 +
                 self._sch[self._sort_x_sf].op.sort_axis[0] * self._ub_factor_1 < self._sort_axis_num], True)
            self._sch[self._sort_x_sf].bind_buffer(self._sch[self._sort_x_sf].op.axis[0], stride=stride_0, offset=0)

            self._sch[self._sort_x_sf_sf].set_store_predicate(
                [self._sch[self._sort_x_sf_sf].op.axis[2] * self._ub_factor_1 +
                 self._sch[self._sort_x_sf_sf].op.axis[3] < self._ub_factor_0,
                 self._sch[self._sort_x_sf_sf].op.axis[1] * self._ub_factor_0 +
                 self._sch[self._sort_x_sf_sf].op.axis[2] * self._ub_factor_1 +
                 self._sch[self._sort_x_sf_sf].op.axis[3] < self._sort_axis_num], True)
            self._sch[self._sort_x_sf_sf].bind_buffer(self._sch[self._sort_x_sf_sf].op.axis[1],
                                                      stride=stride_1, offset=0)
            self._sch[self._sort_x_sf_sf].bind_buffer(self._sch[self._sort_x_sf_sf].op.axis[0],
                                                      stride=stride_0, offset=0)

        if self._tiling_pattern in [SORT_SYNC_MERGE, SORT_MERGE_SYNC_MERGE]:
            self._sch[self._out_data].set_store_predicate(self._sch[self._sort_x_sf].op.axis[1] %
                                                          self._block_factor == 0)

    def _do_double_buffer(self):
        if self._tiling_mode == DB and self._tiling_pattern == SORT:
            self._sch[self._sort_x_ub_tensor].double_buffer()

    def _calc_storage_bound(self):
        tensor_space = self._ub_size // self._coexisting_quantity // BLOCK_SIZE_BYTE * BLOCK_SIZE_BYTE // \
                       self._x_dtype_bytes
        self._tensor_storage_bound = tensor_space // self._proposal_num
        if self._tiling_pattern == SORT:
            index_space = 2048
            self._tensor_storage_bound = (tensor_space - index_space) // self._proposal_num

    def _do_storage_bound(self):
        self._sch[self._cache_read_tensor].set_buffer_size(self._tensor_storage_bound)
        if self._cast_tensor is not None:
            self._sch[self._cast_tensor].set_buffer_size(self._tensor_storage_bound)
        if self._tiling_pattern == SORT:
            self._sch[self._sort_x_ub_tensor].set_buffer_size(self._tensor_storage_bound)
        else:
            self._sch[self._sort_x_ub_tensor].set_buffer_size(self._tensor_storage_bound * self._proposal_num)
            merge_channel = 4
            self._vmsort_ub_limit = self._tensor_storage_bound // merge_channel // 2 * 2

    def _calc_multi_core(self):
        pass

    def _do_multi_core(self):
        if self._tiling_pattern in [SORT, SORT_MERGE]:
            self._sch[self._out_data].bind(self._block_bind_axis[0], self._block_idx)
        else:
            self._sch.bind_axes(self._block_bind_axis, self._block_idx)

    def _calc_storage_align(self):
        if self._specific_version:
            self._align_factor = 32

    def _do_storage_align(self):

        if self._tiling_pattern == SORT:
            self._sch[self._cache_read_tensor].storage_align(self._cache_read_tensor.op.axis[0],
                                                             self._align_factor, 0)
            self._sch[self._sort_x_ub_tensor].storage_align(self._sort_x_ub_tensor.op.axis[0],
                                                            self._align_factor, 0)
            if self._cast_tensor is not None:
                self._sch[self._cast_tensor].storage_align(self._cast_tensor.op.axis[0],
                                                            self._align_factor, 0)
        elif self._soc_version in [ASCEND_031, ASCEND_310B, AS31XM1]:
            self._sch[self._cache_read_tensor].storage_align(self._cache_read_tensor.op.axis[0],
                                                             self._align_factor, 0)
            if self._cast_tensor is not None:
                self._sch[self._cast_tensor].storage_align(self._cast_tensor.op.axis[0],
                                                            self._align_factor, 0)

    def _do_block_sync(self):
        if self._tiling_pattern in [SORT_SYNC_MERGE, SORT_MERGE_SYNC_MERGE]:
            sync_axis = self._sch[self._sort_x_sf].leaf_iter_vars[1]
            sync_tensor = self._sch.create_block_sync()
            self._sch[self._sort_x_sf].wait_block_sync(sync_axis, tensor=sync_tensor, bottom=True)
            self._sch[self._sort_x_sf].set_block_sync(sync_axis, tensor=sync_tensor, bottom=True)

    def _calc_compute_at(self):
        if self._tiling_pattern == SORT:
            self._compute_at_map[self._sort_x_ub_tensor] = [self._out_data, self._compute_at_axis[0]]
            if self._cast_tensor is not None:
                self._compute_at_map[self._cast_tensor] = [self._out_data, self._compute_at_axis[0]]
            self._compute_at_map[self._cache_read_tensor] = [self._out_data, self._compute_at_axis[0]]
        else:
            if self._cast_tensor is not None:
                self._compute_at_map[self._cast_tensor] = [self._sort_x_ub_tensor, self._compute_at_axis[0]]
            self._compute_at_map[self._cache_read_tensor] = [self._sort_x_ub_tensor, self._compute_at_axis[0]]
            if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
                self._compute_at_map[self._sort_x_ub_tensor] = [self._sort_x_sf_sf, self._compute_at_axis[1]]
                self._compute_at_map[self._sort_x_sf_sf] = [self._sort_x_sf, self._compute_at_axis[2]]
                self._compute_at_map[self._sort_x_sf] = [self._out_data, self._compute_at_axis[3]]
            else:
                self._compute_at_map[self._sort_x_ub_tensor] = [self._sort_x_sf, self._compute_at_axis[1]]
                self._compute_at_map[self._sort_x_sf] = [self._out_data, self._compute_at_axis[2]]

    def _do_compute_at(self):
        for tensor_i, param in self._compute_at_map.items():
            self._sch[tensor_i].compute_at(self._sch[param[0]], param[1])

    def _do_allocate_root(self):
        if self._tiling_pattern != SORT:
            self._sch[self._sort_x_sf].allocate_root()
            if self._tiling_pattern == SORT_MERGE_SYNC_MERGE:
                self._sch[self._sort_x_sf_sf].allocate_root()

    def _calc_emit_insn(self):
        vsort_emit_insn = VEC_SORT if self._op_mode == 0 else VEC_TOPK
        vmsort_emit_insn = VEC_MERGE_SORT if self._op_mode == 0 else VEC_MERGE_TOPK
        first_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit}
        second_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit}
        if self._tiling_mode == K_SMALL:
            if self._soc_version in ["Ascend910B", "Ascend910_93"]:
                first_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit,
                                     "vmtopk_start_addr": 0,
                                     "sort_workspace_offset": self._workspace_size}
            else:
                first_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit,
                                     "vmtopk_start_addr": self._block_idx * self._start_addr,
                                     "sort_workspace_offset": self._workspace_size}
        if self._tiling_mode == ONE_DIM:
            first_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit, "vmtopk_output_align": True,
                                 "sort_workspace_offset": self._workspace_size}
            second_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit, "vmtopk_input_align": True,
                                  "sort_workspace_offset": self._workspace_size}
        if self._tiling_mode == WORKSPACE:
            first_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit,
                                 "sort_workspace_offset": self._workspace_size}
            second_vmsort_attr = {"vmsort_ub_limit": self._vmsort_ub_limit,
                                  "sort_workspace_offset": self._workspace_size}

        self._emit_insn_map[self._cache_read_tensor] = [self._cache_read_tensor.op.axis[0], DMA_COPY]
        if self._cast_tensor is not None:
            self._emit_insn_map[self._cast_tensor] = [self._cast_tensor.op.axis[0], "vector_conv"]
        if self._cast_tensor is not None and self._tiling_pattern == SORT:
            self._emit_insn_map[self._sort_x_ub_tensor] = [
                self._vsort_emit_at_axis, vsort_emit_insn, {"bf16_type": True}]
        else:
            self._emit_insn_map[self._sort_x_ub_tensor] = [self._vsort_emit_at_axis, vsort_emit_insn]
        if self._tiling_pattern == SORT:
            dma_attr = {"no_overlap": "process_data_smaller_than_one_block_by_calcute_index"}
            self._emit_insn_map[self._out_data] = [self._res_emit_at_axis, DMA_COPY, dma_attr]
        elif self._tiling_pattern in [SORT_MERGE, SORT_SYNC_MERGE]:
            if self._cast_tensor is not None:
                first_vmsort_attr["bf16_type"] = True
            self._emit_insn_map[self._sort_x_sf] = [self._res_emit_at_axis, DMA_COPY]
            self._emit_insn_map[self._out_data] = [self._vmsort_emit_at_axis[0], vmsort_emit_insn, first_vmsort_attr]
        else:
            if self._cast_tensor is not None:
                second_vmsort_attr["bf16_type"] = True
            self._emit_insn_map[self._sort_x_sf_sf] = [self._res_emit_at_axis, DMA_COPY]
            self._emit_insn_map[self._sort_x_sf] = [self._vmsort_emit_at_axis[0], vmsort_emit_insn, first_vmsort_attr]
            self._emit_insn_map[self._out_data] = [self._vmsort_emit_at_axis[1], vmsort_emit_insn, second_vmsort_attr]

    def _do_emit_insn(self):
        for tensor_i, param in self._emit_insn_map.items():
            self._sch[tensor_i].emit_insn(*param)

    def _set_constraint(self):
        cond = (self._sort_axis_num >= self._out_data.shape[-1])
        self._sch.set_constraint(cond)

    def _add_block_sync_flag(self):
        #for batch mode flag when dynamic schedule
        if self._tiling_pattern in ["sort_sync_merge", "sort_merge_sync_merge"]:
            operation.get_context().get_current_compute().get_current_schedule().add("_block_sync", self._tiling_key)

    def _add_compile_info(self):
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()

        # base info
        cpt_schedule.add(CompileInfo.CORE_NUM, util.get_core_num())
        cpt_schedule.add(CompileInfo.UB_SIZE, self._ub_size)
        cpt_schedule.add(SortCompileInfo.X_DTYPE_BYTES, self._x_dtype_bytes)
        cpt_schedule.add(SortCompileInfo.TILING_PATTERN, self._tiling_pattern)
