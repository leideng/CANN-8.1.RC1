#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pool grad schedule base
"""

import abc
from dataclasses import dataclass

from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from tbe.dsl.boost_schedule_kit import schedule_pro
from tbe.dsl.compute.constants import ComputeType
from tbe.dsl.unify_schedule import util
from tbe.dsl.unify_schedule.constants import DTYPE_BIT_MAPPING
from tbe.dsl.unify_schedule.constants import CompileInfo
from tbe.dsl.unify_schedule.vector.pool_grad import pool_grad_tilingcase
from tbe.dsl.unify_schedule.vector.pool_grad import tiling_core
from tbe.dsl.unify_schedule.vector.pool_grad.entity import pool_attr
from tbe.dsl.unify_schedule.vector.pool_grad.entity import split_info


class PoolGradSchedule:
    """
    Support nlast window axis pooling, contains:
    1. (n, h, w, c)
    2. (n, d, h, w, c)
    3. (n, c1, h, w, c0)
    4. (n, d, c1, h, w, c0)
    """

    def __init__(self, outs, tiling_case):
        self._outs = outs
        self._tiling_case = tiling_case

        self._inputs = InputTensors()
        self._inputs_local = InputTensors()

        self._dx = None
        self._dx_local = None

        self._dp = None
        self._fn = None

        self._sch = None

        self._pool_attr = pool_attr.PoolAttr()

        self._ub_split = split_info.SplitInfo()
        self._block_split = split_info.SplitInfo()

        self._tensor_size = 0

    @classmethod
    def _add_build_args(cls):
        operation.add_build_arg("enable_auto_atomic", ["", "", "", "add.float32"])
        operation.add_build_arg("bind_reduction_using_block", False)

    @classmethod
    def _get_transpose_factor(cls):
        # for h*w in last window axes
        transpose_factor = 16
        if util.is_v100() or util.is_v200():
            transpose_factor = 128
        return transpose_factor

    @classmethod
    def _get_k_align_factor(cls):
        return 256

    def do_schedule(self):
        self._do_fake_node()

        self._sch = schedule_pro.SchedulePro([self._fn.op, self._dx.op], self._outs)

        self._parse_base_info()
        self._calc_buffer_size()
        self._do_op_tiling()

        self._do_set_scope()
        self._do_cache_read()
        self._do_cache_write()
        self._do_transpose()

        self._do_tiling()

        self._do_compute_inline()
        self._do_align()
        self._do_set_buffer_size()
        self._do_bind_core()
        self._do_emit_insn()

        self._add_build_args()
        self._add_compile_info()

        t_sch = self._sch.get()
        t_sch.tiling_key = self._tiling_case.tiling_key

        return self._sch.get()

    def _do_fake_node(self):
        dx = self._outs[0]
        dp = dx.op.input_tensors[0]
        fn = tvm.compute(dp.shape, lambda *i: dp[i], name="fn", attrs={"_type": ComputeType.FAKE})

        self._fn, self._dx, self._dp = fn, dx, dp

    def _parse_base_info(self):
        col2img_stage = self._sch.get_stages(compute_type=ComputeType.COL2IMG)[0]
        axes = col2img_stage.op.attrs["axes"]

        self._pool_attr.ori_window_axes = axes

        # continue window axes
        # for example: (n, d, c1, h, w, c0), window axes: from [1, 3, 4] to [2, 3, 4]
        self._pool_attr.window_axes = [axes[-1]-i+1 for i in range(len(axes), 0, -1)]

        self._pool_attr.kernel = col2img_stage.op.attrs["sizes"]
        self._pool_attr.strides = col2img_stage.op.attrs["strides"]

        for x in self._sch.placeholders:
            input_name = x.op.attrs.get("_max_pool_grad_input_name")
            if input_name == "x":
                self._inputs.x = x
            elif input_name == "y":
                self._inputs.y = x
            elif input_name == "dy":
                self._inputs.dy = x

    def _calc_buffer_size(self):
        coexist = self._get_coexist()
        ub_size = util.get_ub_size()
        vcmp_temp_size = 256 # byte
        self._tensor_size = (ub_size//coexist-vcmp_temp_size)//32*32

    def _do_set_scope(self):
        is_placeholder = lambda x: isinstance(x.op, tvm.PlaceholderOp)
        out_stages = self._sch.to_stages(self._outs)
        is_out = lambda x: x in out_stages
        is_fake_node = lambda x: x.compute_type == ComputeType.FAKE

        stages = filter(lambda x: not(is_placeholder(x) or is_out(x) or is_fake_node(x)), self._sch.stages)
        for x in stages:
            x.set_scope("local.UB")

    def _do_cache_read(self):
        def handle_cache_read(x):
            x_ub = self._sch.cache_read(x, "local.UB", [a.op for a in self._sch[x].consumers])
            self._sch[x_ub].compute_type = ComputeType.DMA_IN
            self._sch[x_ub].tag = "dma_copy"
            return x_ub

        self._inputs_local.x = handle_cache_read(self._inputs.x)
        self._inputs_local.y = handle_cache_read(self._inputs.y)
        self._inputs_local.dy = handle_cache_read(self._inputs.dy)

    def _do_cache_write(self):
        self._dx_local = self._sch.cache_write(self._dx, "local.UB")

        dx_stage, dx_local_stage = self._sch[self._dx], self._sch[self._dx_local]

        dx_local_stage.compute_type = dx_stage.compute_type
        dx_local_stage.tag = dx_stage.tag

        dx_stage.compute_type = ComputeType.DMA_OUT
        dx_stage.tag = "dma_copy"

    def _do_transpose(self):
        pass

    def _do_op_tiling(self):
        if self._tiling_case.mode == "static":
            self._do_const_optiling()
        else:
            self._ub_split.factor = operation.var_inner("_ub_factor", (1, None))
            self._block_split.factor = operation.var_inner("_block_factor", (1, None))

    def _do_const_optiling(self):
        def build_dict(tensor_):
            return {
                "shape": util.shape_to_list(tensor_.shape),
                "dtyp": tensor_.dtype,
                "format": operation.get_context().get("_x_format"),
                "ori_format": operation.get_context().get("_x_ori_format")
            }

        def parse_inputs():
            return [build_dict(self._inputs.x),
                    build_dict(self._inputs.y),
                    build_dict(self._inputs.dy)]

        operation.add_compile_info_inner(CompileInfo.ONLY_CONST_TILING, True)
        self._add_compile_info_common()

        run_info = op_tiling.do_op_tiling("AutoTiling", operation.get_compile_info(), parse_inputs(), [])

        tiling_format = {
            "ub_factor": "int",
            "block_factor": "int",
        }
        tiling_data = op_tiling.decode(run_info.get('tiling_data'), tiling_format)

        self._ub_split.factor = tiling_data.get("ub_factor")
        self._block_split.factor = tiling_data.get("block_factor")

        self._tiling_case.tiling_key = run_info.get("tiling_key")
        self._tiling_case.strategy, self._tiling_case.ub_split_idx = \
                pool_grad_tilingcase.parse_tiling_key(self._tiling_case.tiling_key)

    def _do_tiling(self):
        tiling_core_obj = tiling_core.TilingCoreManager.build((self.get_supported_sub_pattern(),
                                                               self._tiling_case.strategy), self)
        tiling_core_obj.do_tiling()

    def _do_compute_inline(self):
        for x in self._sch.get_stages(compute_type=[ComputeType.BRC_TENSOR, ComputeType.RESHAPE]):
            x.compute_inline()

    def _do_set_buffer_size(self):
        buffer_size_bit = self._tensor_size*8
        for x in self._sch.get_stages(scope="local.UB"):
            x.set_buffer_size(buffer_size_bit//DTYPE_BIT_MAPPING.get(x.dtype))

    def _do_bind_core(self):
        block = tvm.thread_axis("blockIdx.x")
        self._block_split.stage.bind(self._block_split.outer, block)

    def _do_emit_insn(self):
        for x in self._sch.stages:
            if x.is_emited():
                continue

            if x.is_compute_inline() or x.is_placeholder() or x.is_scan():
                continue

            if x.dtype == "uint1" or x.tag == "elewise_multiple_sel":
                group_id = tvm.call_extern("int32", "axis_group", 0, "overwrite")
                for i in range(1, len(x.op.axis)):
                    x.pragma(x.op.axis[i], "axis_group", group_id)
                x.emit()
                continue

            x.emit()

    def _add_compile_info(self):
        operation.add_compile_info_inner(CompileInfo.ONLY_CONST_TILING, False)
        self._add_compile_info_common()

    def _add_compile_info_common(self):
        operation.add_compile_info_inner(CompileInfo.CORE_NUM, util.get_core_num())
        operation.add_compile_info_inner("_tensor_size", self._tensor_size)
        operation.add_compile_info_inner("_window_axes", self._pool_attr.ori_window_axes)

        if all(isinstance(x, int) for x in self._pool_attr.kernel):
            operation.add_compile_info_inner("_ksize", self._pool_attr.kernel)

        if all(isinstance(x, int) for x in self._pool_attr.strides):
            operation.add_compile_info_inner("_strides", self._pool_attr.strides)

        operation.add_compile_info_inner("_transpose_factor", self._get_transpose_factor())
        operation.add_compile_info_inner("_align_factor", self._get_align_factor())
        operation.add_compile_info_inner("_k_align_factor", self._get_k_align_factor())

    def _get_align_factor(self):
        # for c in nlast window axes, n*c in last
        if DTYPE_BIT_MAPPING.get(self._inputs.x.dtype) == 32:
            return 16
        return util.get_align_factor_in_block(self._inputs.x.dtype)

    @abc.abstractmethod
    def _do_align(self):
        pass

    @abc.abstractmethod
    def _get_coexist(self):
        pass


@dataclass
class InputTensors:
    x: tvm.Tensor = None
    y: tvm.Tensor = None
    dy: tvm.Tensor = None
