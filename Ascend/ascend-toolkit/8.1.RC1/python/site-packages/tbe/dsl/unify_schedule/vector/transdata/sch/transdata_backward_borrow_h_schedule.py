#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright(C) 2022. Huawei Technologies Co., Ltd. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
transdata backward borrow h
"""
from copy import copy
from tbe import tvm
from tbe.common.utils.errormgr import get_error_message
from tbe.common.utils.shape_util import shape_to_list
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import var_inner
from tbe.dsl.unify_schedule.constants import TransdataCategory
from tbe.dsl.classifier.transdata.constants import DATA_TYPE_SIZE
from tbe.dsl.classifier.transdata.constants import NO_OVERLAP, STRIDE_16, INT64
from ..common.transdata_base_sch import TransdataBaseSch
from ..common.graph.transdata_graph_info import choose_transpose_insn
from ..common.graph.transdata_graph_info import math_prod

LAST_T_BORROW = "last_transpose_borrow"
N_LAST_T_BORROW = "n_last_transpose_borrow"


class TransBBHSchedule(TransdataBaseSch):
    """
    TransBackwardBorrowHSchedule: backward + n_last_transpose + borrow H
    """

    def __init__(self, outs, tiling_case):
        TransdataBaseSch.__init__(self, outs, tiling_case)

        self.h1_index = None
        self.h0_index = None
        self.h_mapping_indexes = []
        self.c1_index = None
        self.c0_index = None
        self.c_mapping_indexes = []

        # tensors
        self.s_reshape_h_tensor = None
        self.f1_reshape_tensor = None
        self.f2_reshape_tensor = None

        self.transpose_0_tensor = None
        self.transpose_1_tensor = None
        self.transpose_2_tensor = None

        # tensors' attrs
        self.hi = None
        self.perm_2 = []
        self.perm_1 = []
        self.perm_0 = []

        self.branch = None
        self.shadow_t2_tensor = None
        self.ori_tiling_case = None

    @classmethod
    def get_supported_sub_pattern(cls):
        return TransdataCategory.BORROW_H_B8B16_BACKWARD

    def do_schedule(self):
        """
        Process of schedule
        """
        self._analysis_case()
        self._create_schedule()
        self._do_set_scope()
        self._init_tensors()

        self._calc_tiling()
        self._do_tiling()

        self._analysis_transpose()

        self._do_mem_reused()
        self._do_storage_bound()
        self._do_set_predicate()
        self._do_storage_align()
        self._do_compute_align()
        self._do_buffer_align()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_compute_at()
        self._do_compute_at()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_pragma()
        self.schedule.tiling_key = self.tiling_case.tiling_key
        return self.schedule

    def _analysis_case(self):
        # The schedule support n-last-transpose and last-transpose
        # Last and NLast have different streams
        self.branch = LAST_T_BORROW if self.graph_info.is_last_transpose else N_LAST_T_BORROW
        self.split_once = self.tiling_case.ub_split_second_idx == self.tiling_case.ub_split_first_idx
        if self.branch == N_LAST_T_BORROW and not self.split_once:
            dict_args = {"errCode": "E90003", "detailed_cause": "In the bh-sch, only support ub split once."}
            raise RuntimeError(dict_args, get_error_message(dict_args))

    def _init_tensors(self):
        """
        DataStream: pre-stream + main-stream + post-stream
            pre-stream:
                gm -> pad_tensor(H) -> s_reshape -> transpose_0
            main-stream:
                transpose_0 -> transpose_1 -> f_reshape -> depad
            post-stream:
                depad -> transpose_2 -> f_reshape -> res
        """
        self.mte2_tensor = self.child(list(self.graph_info.input_tensor_set)[0])
        self.s_reshape_h_tensor = list(self.graph_info.s_reshape_tensor_set)[0]
        self.transpose_0_tensor = self.child(self.s_reshape_h_tensor)

        for i in self.graph_info.f_reshape_tensor_set:
            if self.child(i) in self.graph_info.output_tensor_set:
                self.f2_reshape_tensor = i
                self.transpose_2_tensor = self.parent(self.f2_reshape_tensor)
            else:
                self.f1_reshape_tensor = i
                self.transpose_1_tensor = self.parent(self.f1_reshape_tensor)

        # depad tensors
        self.depad_tensors = list(self.graph_info.de_pad_tensor_set)
        self.depad_tensors.sort(key=lambda x: int(x.op.attrs["axes"]))
        self.depad_axis_list = [int(x.op.attrs["axes"]) for x in self.depad_tensors]
        self.tiling_tensor = list(self.graph_info.output_tensor_set)[0]

        if self.branch == LAST_T_BORROW and not self.split_once:
            # split NO and C\H need to do storage_align for MTE3
            tensor = self.parent(self.transpose_2_tensor)
            self.shadow_t2_tensor = self.single_cache_read(tensor)

    def _define_factor(self):
        self.hi = list(self.transpose_2_tensor.shape)[self.h0_index]
        if self.tiling_case.ub_first_factor:
            # hi is VarExpr from computeTransDataTilingCase that stored in ub_first_factor in const-model.
            # While const, hi is assigned a constant value.
            compute = get_context().get_current_compute().get_operator_context().get_current_compute()
            var = compute.get_var("_hi").get_tvm_var()
            self.schedule.set_var_range(var, self.tiling_case.ub_first_factor, self.tiling_case.ub_first_factor)
            compute.clear_vars()

        # define ub_sec
        if not self.tiling_case.ub_second_factor and not self.split_once:
            self.tiling_case.ub_second_factor = var_inner("_ub_second_factor", (1, None), dtype=INT64)
        self.tiling_case.ub_first_factor = self.align_factor

        if not self.tiling_case.block_factor:
            self.tiling_case.block_factor = var_inner("_block_factor", (1, None), dtype=INT64)

    def _calc_tiling(self):
        """
        The schedule has three regulations:
        1. only support split once in N_LAST_BORROW_X.
        2. shape of res is as same as transpose-2 that make ub_factor is constant, just like 16(fp16), 32(int8)
        3. hi is VarExpr that from compute, it should be assigned in const-model.
        """
        self.align_factor = self.block // DATA_TYPE_SIZE.get(self.tiling_tensor.dtype, 1)
        if self.align_factor < STRIDE_16:
            self.align_factor = STRIDE_16
        self.h_mapping_indexes, self.h1_index, self.h0_index = self.parses_f_reshape(self.f2_reshape_tensor)
        perm = [int(x) for x in list(self.transpose_2_tensor.op.attrs["permute"])]
        # Different From Forward
        self.parses_axis_type(perm)
        self._define_factor()
        self.parses_tiling_info(self.h_mapping_indexes, self.h1_index, self.h0_index, self.hi)
        self._last_calc_tiling()

    def _last_calc_tiling(self):
        # handle last-transpose-calc-tiling
        if self.branch == N_LAST_T_BORROW:
            return
        # while C not in ub-internal, make c0 in ub-internal
        self.ori_tiling_case = copy(self.tiling_case)
        c_idx = self.h_mapping_indexes[self.graph_info.c1c0[-1]]
        if self.branch == LAST_T_BORROW and c_idx not in self.axis_in_ub:
            c1, c0 = self.schedule[self.tiling_tensor].split(self.tiling_axes[c_idx], factor=self.graph_info.c0)
            self.tiling_axes[c_idx] = c0
            self.tiling_axes.insert(c_idx, c1)
            # update axis-index
            self.axis_in_ub = [i if i <= c_idx else i + 1 for i in self.axis_in_ub]
            self.axis_not_in_ub = [i if i <= c_idx else i + 1 for i in self.axis_not_in_ub]
            self.axis_in_ub.append(c_idx + 1)
            self.axis_in_ub.sort()
            if self.tiling_case.ub_split_first_idx > c_idx:
                self.tiling_case.ub_split_first_idx += 1
            if self.tiling_case.ub_split_second_idx > c_idx:
                self.tiling_case.ub_split_second_idx += 1
            if self.tiling_case.block_split_idx > c_idx:
                self.tiling_case.block_split_idx += 1

    def _do_buffer_align(self):
        def align(tensor, _axis_list, _factor):
            align_list = [[1, 1] for x in tensor.shape]
            for i in _axis_list:
                align_list[i] = [1, _factor]
            self.schedule[tensor].buffer_align(*align_list)

        self.c_mapping_indexes, self.c1_index, self.c0_index = self.parses_f_reshape(self.f1_reshape_tensor)

        if self.c1_index is not None:
            c_idx = self.c_mapping_indexes[self.c1_index]
            align(self.f1_reshape_tensor, [c_idx, ], self.graph_info.c0)

        if self.h1_index is not None:
            h_idx = self.h_mapping_indexes[self.h1_index]
            align(self.f2_reshape_tensor, [h_idx, ], self.align_factor * self.hi)

    def _do_set_predicate(self):
        self.schedule[self.mte2_tensor].set_store_predicate(tvm.expr.Not(self.mte2_tensor.op.body[0].condition))

    def _do_storage_align(self):
        if self.branch == N_LAST_T_BORROW:
            self._n_last_storage_align()
        else:
            self._last_storage_align()

    def _n_last_storage_align(self):
        # Use f0 to avoid bank-conflict that because hi is 16X
        # last-dim is C0, hi must be 16X
        i = self.infer_axes(self.transpose_2_tensor, self.s_reshape_h_tensor, self.graph_info.x1x0)[1]
        if i - 1 >= 0:
            var = self.hi + 1
            shape = shape_to_list(self.s_reshape_h_tensor.shape)
            factor = math_prod(shape[i + 1:]) * var
            self.schedule[self.s_reshape_h_tensor].bind_buffer(self.s_reshape_h_tensor.op.axis[i - 1], factor, 0)

    def _last_storage_align(self):
        sch = self.schedule
        if self.split_once:
            # hi must be 16
            self._n_last_storage_align()
        else:
            # hi must be 1 <pad-h avoid t1 bank-conflict>
            align_var = self.tiling_case.tensor_ub_size_list[self.tiling_case.shape_type]
            align_var = align_var // self.align_factor // self.align_factor
            align_var = (align_var if align_var % 2 != 0 else align_var - 1) * self.align_factor
            for tensor in self.get_all_producers_stages(self.transpose_0_tensor):
                if tensor in self.graph_info.input_tensor_set:
                    continue
                index = 0 if tensor in [self.mte2_tensor, self.s_reshape_h_tensor] else 1
                i = self.infer_axes(self.transpose_2_tensor, tensor, self.graph_info.x1x0)[index]
                sch[tensor].storage_align(tensor.op.axis[i], align_var, 0)

            # make mte3 correct
            # NO NI C H 2 <t2>
            tensor = self.transpose_2_tensor
            idx = self.ori_tiling_case.ub_split_second_idx
            idx = self.infer_axes(self.tiling_tensor, tensor, [idx, ])[0]
            if idx - 1 >= 0:
                sch[tensor].storage_align(tensor.op.axis[idx - 1], self.align_factor, 0)
            # N,C,H,2 <f2>
            tensor = self.f2_reshape_tensor
            idx = self.ori_tiling_case.ub_split_second_idx
            if idx - 1 >= 0:
                sch[tensor].storage_align(tensor.op.axis[idx - 1], self.align_factor, 0)
            # NI,C,H,2,NO <s_t2>
            tensor = self.shadow_t2_tensor
            idx = self.infer_axes(self.tiling_tensor, tensor, [idx, ])[0]
            sch[tensor].storage_align(tensor.op.axis[idx - 1], self.align_factor ** 2, 0)

            # avoid t2 conflict
            # NO NI C H 2 <t2>
            i = self.graph_info.x1x0[0]
            sch[self.transpose_2_tensor].storage_align(self.transpose_2_tensor.op.axis[i], align_var, 0)
            # N C H 2 <f2>
            sch[self.f2_reshape_tensor].storage_align(self.f2_reshape_tensor.op.axis[i], align_var, 0)

    def _do_compute_align(self):
        # make mte3_nov3 work in last-transpose
        if self.branch == N_LAST_T_BORROW or self.split_once:
            return

        if self.ori_tiling_case.ub_split_second_idx <= self.h1_index + 1:
            # split C, NI is 1, C.inner*H*2 is serial
            return

        # de_pad -> shadow_t2 -> transpose_2 -> f_reshape -> res
        # NI,C,H,2,NO -> NI,C,H,2,NO -> NO,NI,C,H,2 -> N,C,H,2(16,C,H,2)
        #                   | |
        #                   a s
        sch = self.schedule
        idx = self.ori_tiling_case.ub_split_second_idx
        factor = self.graph_info.src_pad_var[idx - 1]

        tensor = self.f2_reshape_tensor
        sch[tensor].compute_align(tensor.op.axis[idx - 1], factor)

        tensor = self.transpose_2_tensor
        sch[tensor].compute_align(tensor.op.axis[idx], factor)

    def _do_mem_reused(self):
        sch = self.schedule
        sch[self.transpose_1_tensor].reused_by(self.f1_reshape_tensor)
        sch[self.transpose_2_tensor].reused_by(self.f2_reshape_tensor)

    def _transpose_emit_insn(self, tensor, perm, stride_first=False):
        emit_idx, insn = choose_transpose_insn(perm)
        if insn in ["vector_transpose", "phony_insn"]:
            self.vnchwconv_insn_map(tensor, insn, tensor.op.axis[emit_idx], perm, stride_first)
        else:
            self.emit_insn_map[tensor] = {"scope": tensor.op.axis[0], "instruction": insn}

    def _calc_emit_insn(self):
        _map = self.emit_insn_map
        _map.clear()

        _map[self.mte2_tensor] = {"scope": self.mte2_tensor.op.axis[0], "instruction": "dma_copy"}

        if self.branch == LAST_T_BORROW and not self.split_once:
            self.schedule[self.s_reshape_h_tensor].reused_by(self.mte2_tensor)
            _map[self.s_reshape_h_tensor] = {"scope": self.s_reshape_h_tensor.op.axis[0], "instruction": "phony_insn"}
        else:
            _map[self.s_reshape_h_tensor] = {"scope": self.s_reshape_h_tensor.op.axis[0], "instruction": "dma_copy"}

        self._transpose_emit_insn(self.transpose_0_tensor, self.perm_0)
        self._transpose_emit_insn(self.transpose_1_tensor, self.perm_1)

        _map[self.f1_reshape_tensor] = {"scope": self.f1_reshape_tensor.op.axis[0], "instruction": "phony_insn"}

        for tensor in self.depad_tensors:
            _map[tensor] = {"scope": tensor.op.axis[0], "instruction": "dma_copy"}

        if self.shadow_t2_tensor is not None:
            _map[self.shadow_t2_tensor] = {"scope": self.shadow_t2_tensor.op.axis[0], "instruction": "dma_copy"}

        self._transpose_emit_insn(self.transpose_2_tensor, self.perm_2, stride_first=True)
        _map[self.f2_reshape_tensor] = {"scope": self.f2_reshape_tensor.op.axis[0], "instruction": "phony_insn"}

        if self.split_once:
            model, axes = 2, self.ub_inner
            _map[self.tiling_tensor] = {"scope": axes, "instruction": "dma_copy", "attrs": {NO_OVERLAP: model}}
        else:
            model, axes = 3, self.ub_inner
            _map[self.tiling_tensor] = {"scope": axes, "instruction": "dma_copy",
                                        "attrs": {NO_OVERLAP: model, "no_overlap_malloc_buf_for_tail": 0,
                                                  "gm_no_sync": 1, "no_overlap_constant": 1}}

    def _do_pragma(self):
        def _pragma(_tensor, begin, end):
            axes = [_tensor.op.axis[i] for i in range(begin, end)]
            self._do_group(_tensor, axes)

        self._do_transpose_pragma(self.transpose_1_tensor, self.perm_1)
        self._do_transpose_pragma(self.transpose_2_tensor, self.perm_2)
        for tensor in self.depad_tensors:
            _pragma(tensor, len(tensor.shape) - 2, len(tensor.shape))

        if self.shadow_t2_tensor is not None:
            tensor = self.shadow_t2_tensor
            idx = self.ori_tiling_case.ub_split_second_idx
            idx = self.infer_axes(self.tiling_tensor, tensor, [idx, ])[0]
            _pragma(tensor, idx, len(tensor.shape))

    def _analysis_transpose(self):
        # Func: judge transpose work or not
        self.perm_2 = self.update_ub_perm(self.transpose_2_tensor)
        self.perm_1 = self.update_ub_perm(self.transpose_1_tensor, transpose_work=self.tiling_case.transpose_work)
        self.perm_0 = self.update_ub_perm(self.transpose_0_tensor)
