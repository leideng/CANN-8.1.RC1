#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2022. All rights reserved.

This program is free software; you can redistribute it and/or modify
it under the terms of the Apache License Version 2.0.
You may not use this file except in compliance with the License.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
Apache License for more details at
http://www.apache.org/licenses/LICENSE-2.0

resize schedule
"""
from typing import Optional
from typing import List

from tbe import tvm
from tbe.dsl.base import operation
from tbe.common.utils import op_tiling
from tbe.dsl.base.operation import get_compile_info

from tbe.dsl.unify_schedule import util
from tbe.dsl.unify_schedule.schedule import Schedule
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.constants import CompileInfo
from tbe.dsl.unify_schedule.constants import ResizePattern
from tbe.dsl.unify_schedule.constants import DTYPE_BYTE_MAPPING
from .resize_tilingcase import TilingStrategy
from .resize_tilingcase import ResizeTilingCase
from .resize_tilingcase import ResizeCompileInfo
from .resize_tilingcase import ModeKey


class Insn:
    DMA_COPY = "dma_copy"
    PHONY_INSN = "phony_insn"
    VECTOR_ADDS = "vector_adds"
    VECTOR_RESIZE = "vector_resize"


# 'pylint: disable=R0902, R0903
class ResizeSchedule(Schedule):
    """
    ResizeSchedule
    """

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return ["default"]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.RESIZE]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [ResizePattern.R_0]

    class ComputeAt:
        """
        ResizeSchedule ComputeAt
        """

        def __init__(self):
            self._compute_at_axis = None
            self._compute_at_vice_axis = None

        @property
        def compute_at_axis(self):
            """
            :return: compute_at_axis
            """
            return self._compute_at_axis

        @compute_at_axis.setter
        def compute_at_axis(self, axis):
            """
            set compute_at_axis
            :param axis:
            :return:
            """
            self._compute_at_axis = axis

        @property
        def compute_at_vice_axis(self):
            """
            :return: compute_at_vice_axis
            """
            return self._compute_at_vice_axis

        @compute_at_vice_axis.setter
        def compute_at_vice_axis(self, axis):
            """
            set compute_at_vice_axis
            :param axis:
            :return:
            """
            self._compute_at_vice_axis = axis

    class EmitInsn:
        """
        ResizeSchedule EmitInsn Bean
        """

        def __init__(self):
            self._emit_insn_axis = None
            self._vice_emit_insn_axis = None

        @property
        def emit_insn_axis(self):
            """
            :return: emit_insn_axis
            """
            return self._emit_insn_axis

        @emit_insn_axis.setter
        def emit_insn_axis(self, axis):
            """
            :param axis:
            :return: emit_insn_axis
            """
            self._emit_insn_axis = axis

        @property
        def vice_emit_insn_axis(self):
            """
            :return: vice_emit_insn_axis
            """
            return self._vice_emit_insn_axis

        @vice_emit_insn_axis.setter
        def vice_emit_insn_axis(self, axis):
            """
            :param axis:
            :return: vice_emit_insn_axis
            """
            self._vice_emit_insn_axis = axis

    class Util:

        @staticmethod
        def is_const(strategy: TilingStrategy):
            return strategy == TilingStrategy.CONST

        @staticmethod
        def is_pure_copy(strategy: TilingStrategy):
            return strategy in [
                TilingStrategy.PURE_COPY, TilingStrategy.PURE_COPY_NONE_CUT, TilingStrategy.FIRST_HW,
                TilingStrategy.FIRST_HW_NONE_CUT
            ]

        @staticmethod
        def is_all_none_cut(strategy: TilingStrategy):
            return strategy in [
                TilingStrategy.BASE_NONE_CUT, TilingStrategy.PURE_COPY_NONE_CUT, TilingStrategy.BROADCAST_HW_NONE_CUT,
                TilingStrategy.FIRST_HW_NONE_CUT, TilingStrategy.SMALL_NC1_NONE_CUT
            ]

        @staticmethod
        def get_dsl_insn(tensor: tvm.Tensor):
            """
            get tensor insn
            """
            tag = tensor.op.tag
            if tag.find("|") != -1:
                out_insn, in_insn = tag.split("|")
                insn = in_insn if out_insn == "resize" else out_insn
            else:
                insn = tag
            return insn

        @staticmethod
        def reorder_axis(axis_list, base_axis, adjusted_axis_list):
            """
            reorder tensor axis
            """
            for axis in adjusted_axis_list:
                if (axis_list.index(base_axis) < axis_list.index(axis)):
                    axis_list.insert(axis_list.index(base_axis), axis_list.pop(axis_list.index(axis)))
                else:
                    axis_list.insert(axis_list.index(base_axis) - 1, axis_list.pop(axis_list.index(axis)))

        @staticmethod
        def is_vector_resize(tensor):
            return ResizeSchedule.Util.get_dsl_insn(tensor) == Insn.VECTOR_RESIZE

        @staticmethod
        def get_vector_resize(tensors):
            for tensor_i in tensors:
                if ResizeSchedule.Util.is_vector_resize(tensor_i):
                    return tensor_i
            return tensor_i

    def __init__(self, outs: List[tvm.Tensor], tiling_case):
        self._out: tvm.Tensor = outs[0]
        self._schedule: Optional[tvm.schedule] = None
        self._tiling_case: Optional[ResizeTilingCase] = tiling_case
        self._tiling_strategy: TilingStrategy = self._tiling_case.tiling_strategy
        self._ori_tiling_strategy = self._tiling_strategy
        self._enable_db: bool = self._tiling_case.enable_db
        # multi-core split axis
        self._block_split_axis: int = self._tiling_case.block_split_axis
        self._ub_split_axis: int = self._tiling_case.ub_split_axis
        self._vice_ub_split_axis: int = self._tiling_case.vice_ub_split_axis
        self._reused_cache_float32 = list()
        self._reused_cache_float16 = list()
        self._cast_to_count: int = 0
        self._none_cut = self.Util.is_all_none_cut(self._tiling_strategy)
        self._const_none_cut = False
        self._scope_ub = "local.UB"
        self._in_out_map = {}
        self._input_tensor = None
        self._input_tensors = set()
        self._out_tensors = set()
        self._middle_tensors = set()
        self._cache_read_tensors = set()
        self._cache_read_buffer_tensors_map = {}
        self._cache_write_tensors = set()
        self._cache_write_buffer_tensors_map = {}
        self._block_dims = 0
        self._coexisting_quantity = 1
        self._need_do_block = False
        self._block_factor = 1
        self._ub_factor = 1
        self._nc1_factor = 1
        self._vice_ub_factor = 1
        self._w_factor = 1
        self._c0_size = 16
        self._compute_inline_tensors = set()
        self._compute_at_map = {}
        self._compute_at_axis = self.ComputeAt()
        self._block_bind_axis = 0
        self._emit_insn_axis = self.EmitInsn()
        self._emit_insn_map = {}
        self._fused_hw_gate = 128
        self._vector_resize_coexist_ub_count = 32  # unit is byte
        self._saved_ub_for_tiling = 16 * 8 * 4
        self._small_nc1_gate = 2
        self._ub_size = util.get_ub_size()
        self._core_num = util.get_core_num()
        self._tensor_space: int = 0
        self._input_shape = list()
        self._output_shape = list()
        self._resize_method = 0

    def do_schedule(self):
        self._construct_compute_graph()
        self._get_resize_method()

        self._schedule = tvm.create_schedule(self._out.op)
        self._schedule.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()
        self._calc_cache_write()
        self._do_cache_write()
        self._set_scope()
        self._calc_reused_ub()
        self._calc_storage_bound()
        self._calc_tiling()
        self._calc_compute_inline()
        self._calc_compute_at()
        self._calc_multi_core()
        self._calc_double_buffer()
        self._calc_constraints()
        self._calc_emit_insn()

        self._do_storage_bound()
        self._do_tiling()
        self._do_compute_inline()
        self._do_compute_at()
        self._do_multi_core()
        self._do_double_buffer()
        self._do_constraints()
        self._do_emit_insn()

        self._add_compile_info()

        return self._schedule if self._check_tiling_factor() else None

    def _get_resize_method(self):
        if ResizeCompileInfo.RESIZE_METHOD in get_compile_info():
            self._resize_method = get_compile_info().get(ResizeCompileInfo.RESIZE_METHOD)

    def _construct_compute_graph(self):

        def _dfs_sub_graph(out):
            for tensor_i in out.op.input_tensors:
                util.merge_value(self._in_out_map, tensor_i, out)
                if util.is_placeholder(tensor_i):
                    self._input_tensor = tensor_i
                    self._input_tensors.add(tensor_i)
                else:
                    self._middle_tensors.add(tensor_i)
                _dfs_sub_graph(tensor_i)

        _dfs_sub_graph(self._out)
        self._out_tensors.add(self._out)
        self._input_shape.extend(util.shape_to_list(self._input_tensor.shape))
        self._output_shape.extend(util.shape_to_list(self._out.shape))

    def _calc_cache_read(self):
        self._cache_read_tensors.update(self._input_tensors)

    def _do_cache_read(self):
        for tensor_i in self._cache_read_tensors:
            buffer_tensor = self._schedule.cache_read(tensor_i, self._scope_ub, self._in_out_map.get(tensor_i))
            self._cache_read_buffer_tensors_map[buffer_tensor] = tensor_i

    def _calc_cache_write(self):
        self._cache_write_tensors.update(self._out_tensors)

    def _do_cache_write(self):
        for tensor_i in self._cache_write_tensors:
            buffer_tensor = self._schedule.cache_write(tensor_i, self._scope_ub)
            self._cache_write_buffer_tensors_map[buffer_tensor] = tensor_i

    def _set_scope(self):
        for tensor_i in self._middle_tensors:
            self._schedule[tensor_i].set_scope(self._scope_ub)

    def _calc_reused_ub(self):
        cache_read_set = self._cache_read_buffer_tensors_map.keys()
        cache_write_set = self._cache_write_buffer_tensors_map.keys()

        ub_tensors = self._middle_tensors.union(cache_read_set).union(cache_write_set)
        # only two float32 ub tensors
        for tensor_i in ub_tensors:
            if tensor_i.dtype == "float32":
                self._reused_cache_float32.append(tensor_i)

        self._reused_cache_float16.append(tuple(cache_read_set)[0])
        self._reused_cache_float16.append(tuple(cache_write_set)[-1])

    def _calc_storage_bound(self):
        """
        give parameters of storage usage scheme
        """
        # do the action in _calc_tiling_const
        if self._tiling_strategy == TilingStrategy.CONST:
            return

        self._cast_to_count = len(self._middle_tensors)
        no_cast = 0
        single_cast = 1
        pair_cast = 2

        # suppose ub dtype is float16
        if self._resize_method == 0:
            coexisting_ub_tensors_map = {no_cast: 6, single_cast: 7, pair_cast: 8}
            if self._tiling_strategy in (TilingStrategy.SMALL_NC1, TilingStrategy.SMALL_NC1_NONE_CUT):
                self._fused_hw_gate = 256
        else:
            # vector_resize no need addtional ub
            coexisting_ub_tensors_map = {no_cast: 4}
            self._fused_hw_gate = 512
            self._vector_resize_coexist_ub_count = 16
        saved_ub_for_src_pos = self._fused_hw_gate * self._vector_resize_coexist_ub_count * 4  # unit is byte
        self._coexisting_quantity = coexisting_ub_tensors_map.get(self._cast_to_count)
        self._ub_size -= (saved_ub_for_src_pos + self._saved_ub_for_tiling)

        # resize cache read reused by cache write when pure copy
        if self.Util.is_pure_copy(self._tiling_strategy):
            self._ub_size += saved_ub_for_src_pos
            if self._cast_to_count != 1:
                self._coexisting_quantity = 2
            else:
                self._coexisting_quantity -= 4

        # nearest_neighbor no need to convert float16 to float32 in process
        if self._input_tensor.dtype == "float16" and self._out.dtype == "float16" and self._resize_method == 1:
            self._coexisting_quantity = self._coexisting_quantity // 2

    def _calc_tiling(self):
        funcs = {
            TilingStrategy.PURE_COPY: self._calc_tiling_base,
            TilingStrategy.BROADCAST_HW: self._calc_tiling_base,
            TilingStrategy.FIRST_HW: self._calc_tiling_base,
            TilingStrategy.SMALL_NC1: self._calc_tiling_base,
            TilingStrategy.BASE_CUT: self._calc_tiling_base,
            TilingStrategy.PURE_COPY_NONE_CUT: self._calc_tiling_none_cut,
            TilingStrategy.BROADCAST_HW_NONE_CUT: self._calc_tiling_none_cut,
            TilingStrategy.FIRST_HW_NONE_CUT: self._calc_tiling_none_cut,
            TilingStrategy.SMALL_NC1_NONE_CUT: self._calc_tiling_none_cut,
            TilingStrategy.BASE_NONE_CUT: self._calc_tiling_none_cut,
            TilingStrategy.CONST: self._calc_tiling_const
        }
        funcs.get(self._tiling_strategy)()

    def _calc_tiling_base(self):
        """
        generate vars to access multi-core and ub-loop factors from tiling
        """
        res = self._out
        shape = util.shape_to_list(res.shape)
        b_i = self._block_split_axis
        u_i = self._ub_split_axis
        v_u_i = self._vice_ub_split_axis
        b_bound = (1, util.get_bound(shape[b_i])[1])
        u_bound = (1, util.get_bound(shape[u_i])[1])
        nc1_bound = (1, util.get_bound(shape[0])[1])
        v_u_bound = (1, util.get_bound(shape[v_u_i])[1])
        w_bound = (1, util.get_bound(shape[2])[1])
        self._block_factor = operation.var_inner("_block_factor_" + str(b_i), b_bound)
        self._ub_factor = operation.var_inner("_ub_factor_" + str(u_i), u_bound)
        self._nc1_factor = operation.var_inner("_nc1_factor_0", nc1_bound)
        self._vice_ub_factor = operation.var_inner("_vice_ub_factor" + str(v_u_i), v_u_bound)
        self._w_factor = operation.var_inner("_w_factor_2", w_bound)

    def _calc_tiling_none_cut(self):
        pass

    def _calc_best_strategy(self):
        """
        to select best performance tiling strategy for const case
        """
        nc1_idx = 0
        h_idx = 1
        w_idx = 2
        nc1, src_h, src_w = self._input_shape[nc1_idx], self._input_shape[h_idx], self._input_shape[w_idx]
        dst_h, dst_w = self._output_shape[h_idx], self._output_shape[w_idx]

        if src_h == dst_h and src_w == dst_w:
            return TilingStrategy.PURE_COPY, ModeKey.COPY_MODE.value
        if dst_h == 1 and dst_w == 1 and get_compile_info().get(ResizeCompileInfo.HALF_PIXEL_CENTERS, False) is False:
            return TilingStrategy.FIRST_HW, ModeKey.FIRST_HW_MODE.value
        if src_h == 1 and src_w == 1:
            return TilingStrategy.BROADCAST_HW, ModeKey.BROADCAST_MODE.value
        if nc1 <= self._small_nc1_gate:
            return TilingStrategy.SMALL_NC1, ModeKey.SMALL_NC1_MODE.value
        return TilingStrategy.BASE_CUT, ModeKey.BASE_MODE.value

    def _calc_tiling_const(self):
        """
        get tiling parameters to feed best performance dynamic strategy
        """
        self._tiling_strategy, const_mode = self._calc_best_strategy()
        self._calc_storage_bound()
        self._do_storage_bound()
        const_compile_info = get_compile_info()
        new_compile_info = {
            CompileInfo.CORE_NUM: self._core_num,
            ResizeCompileInfo.TENSOR_SPACE: self._tensor_space,
            ResizeCompileInfo.HW_GATE: self._fused_hw_gate,
            ResizeCompileInfo.NC1_GATE: self._small_nc1_gate,
            ResizeCompileInfo.ONLY_CONST_TILING: True,
            ResizeCompileInfo.IS_CONST: False,
            ResizeCompileInfo.COEXIST_UB_COUNT: {
                const_mode: self._coexisting_quantity
            }
        }
        const_compile_info.update(new_compile_info)

        op_type = "AutoTiling"
        inputs = [{"shape": self._input_shape, "dtype": self._input_tensor.dtype}]
        outputs = [{"shape": self._output_shape, "dtype": self._out.dtype}]
        run_info = op_tiling.do_op_tiling(op_type, const_compile_info, inputs, outputs)

        tiling_format = {
            "need_multi_core": "int64",
            "block_axis": "int64",
            "block_factor": "int64",
            "ub_axis": "int64",
            "ub_factor": "int64",
            "nc1_factor": "int64",
            "vice_ub_axis": "int64",
            "vice_ub_factor": "int64",
            "w_factor": "int64"
        }
        tiling_data = op_tiling.decode(run_info["tiling_data"], tiling_format)
        self._block_dims = run_info["block_dim"]
        self._need_do_block = True if tiling_data["need_multi_core"] > 0 else False
        if self._need_do_block:
            self._block_split_axis = tiling_data["block_axis"]
            self._block_factor = tiling_data["block_factor"]
            self._ub_split_axis = tiling_data["ub_axis"]
            self._ub_factor = tiling_data["ub_factor"]
            self._nc1_factor = tiling_data["nc1_factor"]
            self._vice_ub_split_axis = tiling_data["vice_ub_axis"]
            self._vice_ub_factor = tiling_data["vice_ub_factor"]
            self._w_factor = tiling_data["w_factor"]
        self._const_none_cut = not self._need_do_block

    def _calc_compute_inline(self):
        pass

    def _calc_multi_core(self):
        pass

    def _calc_compute_at(self):
        if self._none_cut or self._const_none_cut:
            return

        for tensor_i in self._cache_read_buffer_tensors_map:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

        for tensor_i in self._middle_tensors - self._compute_inline_tensors:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

        for tensor_i in self._cache_write_buffer_tensors_map:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

    def _calc_double_buffer(self):
        pass

    def _calc_constraints(self):
        pass

    def _calc_emit_insn(self):
        for source, _ in self._cache_read_buffer_tensors_map.items():
            if source not in self._compute_inline_tensors:
                self._emit_insn_map[source] = [source.op.axis[0], Insn.DMA_COPY]

        for tensor_i in (self._middle_tensors - self._compute_inline_tensors):
            # no need convert for case cast_to_float32 -> copy -> cast_to_float16
            if self.Util.is_pure_copy(self._tiling_strategy) and self._cast_to_count > 1:
                self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], Insn.PHONY_INSN]
            elif self.Util.is_vector_resize(tensor_i):
                self._emit_insn_map[tensor_i] = [self._emit_insn_axis, self.Util.get_dsl_insn(tensor_i)]
            else:
                self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], self.Util.get_dsl_insn(tensor_i)]

        for target, _ in (self._cache_write_buffer_tensors_map.items() - self._compute_inline_tensors):
            if self.Util.is_pure_copy(self._tiling_strategy) and self._cast_to_count > 1:
                self._emit_insn_map[target] = [target.op.axis[0], Insn.PHONY_INSN]
            elif self.Util.is_vector_resize(target):
                attr = {"fused_hw_size": self._fused_hw_gate}
                self._emit_insn_map[target] = [self._emit_insn_axis, self.Util.get_dsl_insn(self._out), attr]
            else:
                self._emit_insn_map[target] = [target.op.axis[0], self.Util.get_dsl_insn(self._out)]

        for tensor_i in self._out_tensors:
            self._emit_insn_map[tensor_i] = [self._emit_insn_axis, Insn.DMA_COPY]

    def _do_tiling(self):
        funcs = {
            TilingStrategy.PURE_COPY: self._do_tiling_pure_copy,
            TilingStrategy.FIRST_HW: self._do_tiling_pure_copy,
            TilingStrategy.BROADCAST_HW: self._do_tiling_broadcast_hw,
            TilingStrategy.SMALL_NC1: self._do_tiling_base,
            TilingStrategy.BASE_CUT: self._do_tiling_base,
            TilingStrategy.PURE_COPY_NONE_CUT: self._do_tiling_pure_copy_none_cut,
            TilingStrategy.FIRST_HW_NONE_CUT: self._do_tiling_pure_copy_none_cut,
            TilingStrategy.BROADCAST_HW_NONE_CUT: self._do_tiling_base_none_cut,
            TilingStrategy.SMALL_NC1_NONE_CUT: self._do_tiling_base_none_cut,
            TilingStrategy.BASE_NONE_CUT: self._do_tiling_base_none_cut,
            TilingStrategy.CONST: self._do_tiling_const
        }
        funcs.get(self._ori_tiling_strategy)()

    def _do_tiling_reuse_cache(self):
        if self._cast_to_count <= 1 and len(self._reused_cache_float32) != 0:
            self._schedule[self._reused_cache_float32[0]].reused_by(self._reused_cache_float32[-1])
        else:
            self._schedule[self._reused_cache_float16[0]].reused_by(self._reused_cache_float16[-1])

    def _do_tiling_pure_copy(self):
        sch = self._schedule
        res = self._out

        block_o, block_i = sch[res].split(res.op.axis[0], factor=self._block_factor)
        # multi-core axis and ub-split axis are same
        ub_o, ub_i = sch[res].split(block_i, factor=self._ub_factor)
        self._do_tiling_reuse_cache()

        self._compute_at_axis.compute_at_axis = ub_o
        self._compute_at_axis.compute_at_vice_axis = ub_o
        self._block_bind_axis = block_o
        self._emit_insn_axis.emit_insn_axis = ub_i

    def _do_tiling_pure_copy_none_cut(self):
        self._do_tiling_reuse_cache()
        res = self._out
        self._emit_insn_axis.emit_insn_axis = res.op.axis[0]

    def _do_tiling_broadcast_hw(self):
        sch = self._schedule
        res = self._out

        block_o, block_i = sch[res].split(res.op.axis[self._block_split_axis], factor=self._block_factor)
        if self._ub_split_axis == self._block_split_axis:
            ub_o, ub_i = sch[res].split(block_i, factor=self._ub_factor)
        else:
            ub_o, ub_i = sch[res].split(res.op.axis[self._ub_split_axis], factor=self._ub_factor)

        if self._ub_split_axis == 1:
            # HW is big, one c0 broadcast to hwc0
            if self._block_split_axis == 0:
                nc1_o, _ = sch[res].split(block_i, factor=self._tensor_space // self._c0_size)
            else:
                nc1_o, _ = sch[res].split(res.op.axis[0], factor=self._tensor_space // self._c0_size)
            self._compute_at_axis.compute_at_vice_axis = nc1_o
        else:
            # HW is small, nc1c0 broadcast to nc1hwc0
            self._compute_at_axis.compute_at_vice_axis = ub_o
        self._compute_at_axis.compute_at_axis = ub_o
        self._emit_insn_axis.emit_insn_axis = ub_i
        self._block_bind_axis = block_o

    def _do_tiling_base(self):
        sch = self._schedule
        res = self._out
        tensors = self._middle_tensors.union(self._cache_write_buffer_tensors_map.keys())
        resize_cache = self.Util.get_vector_resize(tensors)

        nc1, h, w = res.op.axis[:-1]
        axis_list = res.op.axis[:]

        block_o, block_i = sch[res].split(res.op.axis[self._block_split_axis], factor=self._block_factor)
        axis_list[self._block_split_axis:self._block_split_axis + 1] = [block_o, block_i]

        # to split nc1 in default, do nothing when vice split on nc1
        if self._block_split_axis == 0:
            nc1_o, nc1_i = sch[res].split(block_i, factor=self._nc1_factor)
            axis_list[axis_list.index(block_i):axis_list.index(block_i) + 1] = [nc1_o, nc1_i]
        else:
            nc1_o, nc1_i = sch[res].split(nc1, factor=self._nc1_factor)
            axis_list[0:1] = [nc1_o, nc1_i]

        if self._ub_split_axis == 0:
            ub_o, ub_i = sch[res].split(nc1_o, factor=self._ub_factor)
            axis_list[axis_list.index(nc1_o):axis_list.index(nc1_o) + 1] = [ub_o, ub_i]
        elif self._ub_split_axis == self._block_split_axis:
            ub_o, ub_i = sch[res].split(block_i, factor=self._ub_factor)
            axis_list[axis_list.index(block_i):axis_list.index(block_i) + 1] = [ub_o, ub_i]
        else:
            cur_axis = res.op.axis[self._ub_split_axis]
            ub_o, ub_i = sch[res].split(cur_axis, factor=self._ub_factor)
            axis_list[axis_list.index(cur_axis):axis_list.index(cur_axis) + 1] = [ub_o, ub_i]
        self.Util.reorder_axis(axis_list, ub_i, [nc1_i])

        self._compute_at_axis.compute_at_axis = ub_o
        self._compute_at_axis.compute_at_vice_axis = ub_o
        self._block_bind_axis = block_o
        self._emit_insn_axis.emit_insn_axis = nc1_i
        self._emit_insn_axis.vice_emit_insn_axis = resize_cache.op.axis[0]

        if self._ub_split_axis == 0:
            # the order is [h, w, ub_o, nc1_i, ub_i, c0]
            if self._block_split_axis == 0:
                self.Util.reorder_axis(axis_list, ub_o, [h, w])
            elif self._block_split_axis == 1:
                self.Util.reorder_axis(axis_list, ub_o, [block_o, block_i, w])
            elif self._block_split_axis == 2:
                self.Util.reorder_axis(axis_list, ub_o, [h, block_o, block_i])
            _, vice_ub_i = sch[resize_cache].split(resize_cache.op.axis[0], nparts=self._vice_ub_factor)
            self._emit_insn_axis.vice_emit_insn_axis = vice_ub_i
        elif self._vice_ub_split_axis == 1:
            # the order is [w_o, ub_o, nc1_i, ub_i, w_i, c0]
            if self._ub_split_axis == 2:
                w_o, w_i = sch[res].split(ub_i, factor=self._w_factor)
                axis_list[axis_list.index(ub_i):axis_list.index(ub_i) + 1] = [w_o, w_i]
            elif self._block_split_axis == 2:
                w_o, w_i = sch[res].split(block_i, factor=self._w_factor)
                axis_list[axis_list.index(block_i):axis_list.index(block_i) + 1] = [w_o, w_i]
            else:
                w_o, w_i = sch[res].split(w, factor=self._w_factor)
                axis_list[axis_list.index(w):axis_list.index(w) + 1] = [w_o, w_i]
            self.Util.reorder_axis(axis_list, ub_o, [w_o])
            vice_ub_o, vice_ub_i = sch[resize_cache].split(resize_cache.op.axis[1], factor=self._vice_ub_factor)
            sch[resize_cache].reorder(vice_ub_o, vice_ub_i, resize_cache.op.axis[0])
            self._emit_insn_axis.vice_emit_insn_axis = vice_ub_i
        elif self._vice_ub_split_axis == 2:
            # the order is [h, ub_o, nc1_i, ub_i, c0]
            vice_ub_o, vice_ub_i = sch[resize_cache].split(resize_cache.op.axis[2], factor=self._vice_ub_factor)
            sch[resize_cache].reorder(vice_ub_o, vice_ub_i, resize_cache.op.axis[0])
            self._emit_insn_axis.vice_emit_insn_axis = vice_ub_i
        sch[res].reorder(*axis_list)

    def _do_tiling_base_none_cut(self):
        res = self._out
        tensors = self._middle_tensors.union(self._cache_write_buffer_tensors_map.keys())
        resize_cache = self.Util.get_vector_resize(tensors)
        self._emit_insn_axis.emit_insn_axis = res.op.axis[0]
        self._emit_insn_axis.vice_emit_insn_axis = resize_cache.op.axis[0]

    def _do_tiling_const(self):
        const_tiling_case_map = {
            (True, TilingStrategy.PURE_COPY): self._do_tiling_pure_copy,
            (False, TilingStrategy.PURE_COPY): self._do_tiling_pure_copy_none_cut,
            (True, TilingStrategy.FIRST_HW): self._do_tiling_pure_copy,
            (False, TilingStrategy.FIRST_HW): self._do_tiling_pure_copy_none_cut,
            (True, TilingStrategy.BROADCAST_HW): self._do_tiling_broadcast_hw,
            (False, TilingStrategy.BROADCAST_HW): self._do_tiling_base_none_cut,
            (True, TilingStrategy.SMALL_NC1): self._do_tiling_base,
            (False, TilingStrategy.SMALL_NC1): self._do_tiling_base_none_cut,
            (True, TilingStrategy.BASE_CUT): self._do_tiling_base,
            (False, TilingStrategy.BASE_CUT): self._do_tiling_base_none_cut
        }
        const_tiling_case_map.get((self._need_do_block, self._tiling_strategy))()

    def _do_storage_bound(self):
        # do the action in _calc_tiling_const
        if self._tensor_space > 0:
            return

        sch = self._schedule
        tensors = self._middle_tensors.union(self._cache_read_buffer_tensors_map.keys()).union(
            self._cache_write_buffer_tensors_map.keys())

        tensor_space = self._ub_size // self._coexisting_quantity

        if self._enable_db:
            tensor_space = self._ub_size // 2 // self._coexisting_quantity
        block_bytes = 32
        self._tensor_space = tensor_space // block_bytes * block_bytes // DTYPE_BYTE_MAPPING.get("float16")

        # count coexisting_quantity in unit float16
        for tensor_i in tensors:
            sch[tensor_i].set_buffer_size(self._tensor_space)

    def _do_compute_inline(self):
        pass

    def _do_compute_at(self):
        sch = self._schedule

        def _is_broadcast_hw(tensor):
            vice_tensors = self._middle_tensors.union(self._cache_read_buffer_tensors_map.keys())
            is_read_cache_and_cast = self.Util.get_dsl_insn(tensor) != Insn.VECTOR_ADDS and tensor in vice_tensors
            return self._tiling_strategy == TilingStrategy.BROADCAST_HW and is_read_cache_and_cast

        for tensor_i, param in self._compute_at_map.items():
            if _is_broadcast_hw(tensor_i):
                sch[tensor_i].compute_at(sch[param[0]], param[1].compute_at_vice_axis)
            else:
                sch[tensor_i].compute_at(sch[param[0]], param[1].compute_at_axis)

    def _do_multi_core(self):
        if self._none_cut or self._const_none_cut:
            return

        sch = self._schedule
        res = self._out
        block = tvm.thread_axis("blockIdx.x")
        sch[res].bind(self._block_bind_axis, block)

    def _do_storage_align(self):
        pass

    def _do_double_buffer(self):
        if self._enable_db:
            sch = self._schedule

            tensors = self._middle_tensors.union(self._cache_read_buffer_tensors_map.keys()).union(
                self._cache_write_buffer_tensors_map.keys())

            for tensor_i in tensors:
                sch[tensor_i].double_buffer()

    def _do_constraints(self):
        pass

    def _do_emit_insn(self):
        sch = self._schedule
        tensors = self._middle_tensors.union(self._cache_write_buffer_tensors_map.keys())

        for tensor_i, param in self._emit_insn_map.items():
            emit_insn_axis = param[0]
            if isinstance(emit_insn_axis, self.EmitInsn):
                if self.Util.is_vector_resize(tensor_i) and tensor_i in tensors:
                    emit_insn_axis = emit_insn_axis.vice_emit_insn_axis
                else:
                    emit_insn_axis = emit_insn_axis.emit_insn_axis
            if len(param) > 2:
                sch[tensor_i].emit_insn(emit_insn_axis, param[1], param[2])
            else:
                sch[tensor_i].emit_insn(emit_insn_axis, param[1])

    def _add_compile_info(self):
        # the optiling will give block_dims directly for const case when running
        if self.Util.is_const(self._ori_tiling_strategy):
            operation.add_compile_info_inner(ResizeCompileInfo.IS_CONST, True)
            operation.add_compile_info_inner(ResizeCompileInfo.CONST_DIMS, self._block_dims)
            return

        if CompileInfo.CORE_NUM not in get_compile_info():
            operation.add_compile_info_inner(CompileInfo.CORE_NUM, self._core_num)
            operation.add_compile_info_inner(ResizeCompileInfo.TENSOR_SPACE, self._tensor_space)
            operation.add_compile_info_inner(ResizeCompileInfo.HW_GATE, self._fused_hw_gate)
            operation.add_compile_info_inner(ResizeCompileInfo.NC1_GATE, self._small_nc1_gate)
            operation.add_compile_info_inner(ResizeCompileInfo.IS_CONST, False)
            operation.add_compile_info_inner(ResizeCompileInfo.ONLY_CONST_TILING, False)

        input_shape_and_size = self._input_shape + self._output_shape[1:-1]
        axis_is_var = []
        for val in input_shape_and_size:
            if isinstance(val, int):
                axis_is_var.append(False)
            else:
                axis_is_var.append(True)
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()
        # the following will add to compile info at build_pointcut
        cpt_schedule.add(ResizeCompileInfo.RESIZE_AXIS_IS_VAR, axis_is_var)
        cpt_schedule.add(ResizeCompileInfo.TILING_KEY, self._tiling_case.tiling_key)
        if not self._tiling_case.pattern:
            cpt_schedule.add(ResizeCompileInfo.MODE, self._tiling_case.mode)
            cpt_schedule.add(ResizeCompileInfo.COEXIST_UB_COUNT, self._coexisting_quantity)

    def _check_tiling_factor(self):
        return True
