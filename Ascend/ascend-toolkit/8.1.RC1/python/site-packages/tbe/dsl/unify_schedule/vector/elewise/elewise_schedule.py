#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
elewise schedule
"""
from copy import deepcopy as deep_copy
from typing import Optional
from typing import List

from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from tbe.dsl.base.expr_compare import expr_equal
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.padding import padding
from tbe.dsl.padding.padding import Action
from tbe.dsl.padding.padding import ActionType
from tbe.dsl.padding.padding import ActionValueType

from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import ElewisePattern
from ...constants import FAKE_NODE_TAG
from ...constants import INSN_MAPPING
from ...constants import Pattern
from ...constants import SUPPORT_SCALAR_INSNS
from ...constants import DST_SRC_NO_REUSE_SET
from ...constants import TERNARY_INSNS
from ...constants import ELEWISE_BROADCAST_INSNS
from ...schedule import Schedule
from ..storage_bound_util import EXTRA_NODE_B64
from ..storage_bound_util import EXTRA_NODE_B32
from ..storage_bound_util import EXTRA_TEMP_BUFFER
from ..storage_bound_util import EXTRA_NODE_COMPLEX
from ..storage_bound_util import EXTRA_TEMP_COMPLEX
from ..storage_bound_util import EXTRA_BLOCK
from ..storage_bound_util import complex_calc_instructions
from ..storage_bound_util import base_op_complex_instructions
from ..storage_bound_util import cast_complex_instructions
from ..storage_bound_util import complex_instructions_b64_impl
from ..storage_bound_util import base_single_instructions_b64
from ..storage_bound_util import vsel_complex_instructions
from ..storage_bound_util import powi_complex_instructions_b32
from ..storage_bound_util import cast_ints2ints_external_space
from .elewise_tilingcase import TilingStrategy


DEFAULT = "default"

NUM_FOUR = 4
NUM_EIGHT = 8
COMMON_BLOCK_SIZE = 32
NANO_BLOCK_SIZE = 16
N_BUFFER_COEXISTING_QUANTITY = 5

# ONE_SHAPE_OPTIMIZATION tensor num
ONE_SHAPE_MAX_TENSORS = 32

CONST = "const"
ELEWISE_SCOPE = "local.UB"
VECTOR = "vector"

# vcmpsel constant
VCMP_INPUT_NUMBER = 2
VSEL_INPUT_NUMBER = 3
VCMPSEL_INPUT_NUMBER = 4

# mask tag
ELEWISE_MASK_FLAG = "ElewiseMaskFlag"
MASK = "Mask"


# 'pylint: disable=R0902, R0903
class ElewiseSchedule(Schedule):
    """
    ElewiseSchedule
    """

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.ELEMWISE]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [ElewisePattern.E_0]

    def __init__(self, outs, tiling_case):
        self._out = None  # type: Optional[tvm.Tensor]
        self._origin_outs = outs
        self._schedule = None
        self._tiling_case = tiling_case
        self._tiling_strategy = self._tiling_case.tiling_strategy
        self._is_db = self._tiling_case.enable_db
        self._is_nb = self._tiling_case.enable_nb
        self._is_one_dim = self._tiling_case.is_one_dim

        self._input_tensors = set()
        self._middle_tensors = set()
        self._pure_middle_tensors = set()
        self._middle_out_tensors = set()
        self._out_tensors = set()
        self._elewise_broadcast_tensors = set()
        self._absorbable_broadcast_tensors = set()
        self._compute_inline_tensors = set()

        self._dtypes = set()
        self._max_dtype_bytes = 4
        self._coexisting_quantity = 1
        self._tensor_space = None
        self._ub_size = util.get_ub_size()
        self._ub_block_size = util.get_ub_block_size()
        self._extra_ub_size = 0

        # input -> outputs mapping relations
        self._in_out_map = {}

        self._cache_read_tensors = set()
        self._cache_read_buffer_tensor_map = {}
        self._placeholder_tensor_map = {}
        self._middle_out_cache_read_buffer_map = {}

        self._cache_write_tensors = set()
        self._cache_write_buffer_tensor_map = {}
        self._cache_write_tensor_map = {}
        self._middle_out_cache_write_buffer_map = {}

        self._compute_at_map = {}
        self._copy_out_tensors = set()

        # just for const tiling
        self._need_do_block = False
        self._const_block_dims = 1
        self._block_factor = 1
        self._ub_split_axis = 0
        self._ub_factor = 1
        self._ub_factor_align = self._ub_block_size * NUM_FOUR

        self._block_tiling_vars = {}
        self._ub_tiling_vars = {}
        self._block_bind_axis = None
        self._compute_at_axis = None
        self._emit_insn_axis = None

        self._inner_shape = []
        self._mem_reuse_map = {}
        self._emit_insn_map = {}
        self._fractal_format_actions: List[Action] = None
        self._set_value_cache_read_map = []
        self._one_shape_tensors = set()
        self._is_one_shape_optimization = False

    @staticmethod
    def _is_one_shape_tensor(_tensor):
        cur_shape = util.shape_to_list(_tensor.shape)
        return all(dim_value == 1 for dim_value in cur_shape)

    def do_schedule(self):
        """
        :return:
        """
        self._construct_compute_graph()

        self._schedule = tvm.create_schedule(self._out.op)
        self._schedule.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._judge_one_shape_optimization()

        self._calc_storage_bound()
        self._calc_set_value()
        self._calc_tiling()
        self._calc_compute_inline()

        self._do_tiling()
        self._do_storage_bound()
        self._do_compute_inline()

        self._do_set_value()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_compute_at()
        self._do_compute_at()

        self._calc_double_buffer()
        self._do_double_buffer()
        self._do_n_buffer()

        self._calc_mem_reuse()
        self._do_mem_reuse()

        self._calc_emit_insn()
        self._do_emit_insn()
        self._add_compile_info()

        return self._schedule if self._check_tiling_case() else None

    def _construct_compute_graph(self):
        def _match_scalar_scene(_tensor):
            # condition:
            # 1. tensor -> tensor
            # 2. broadcast tensor is output
            # 3. next compute support scalar
            if _tensor.op.input_tensors and util.get_tensor_size(_tensor.op.input_tensors[0]) != 1:
                return False
            if _tensor in self._out_tensors:
                return False
            if all(util.support_scalar(tensor_i) for tensor_i in self._in_out_map.get(_tensor)):
                return True
            return False

        def _pre_handle_placeholder():
            for index, out in enumerate(self._origin_outs):
                if util.is_placeholder(out):
                    copy_out = _copy_node(out)
                    self._origin_outs[index] = copy_out
                    self._copy_out_tensors.add(copy_out)
                    self._out_tensors.add(copy_out)
                    self._out_tensors.remove(out)

        self._out_tensors = set(self._origin_outs)
        _pre_handle_placeholder()

        visited_tensors = set()
        for tensor_i in self._origin_outs:
            # get the broadcast tensors for compute inline
            if util.get_dsl_insn(tensor_i) in ELEWISE_BROADCAST_INSNS:
                self._elewise_broadcast_tensors.add(tensor_i)
            self.__dfs_sub_graph(tensor_i, visited_tensors)
            self._dtypes.add(tensor_i.dtype)
        byte_len = [DTYPE_BYTE_MAPPING.get(dtype) for dtype in self._dtypes]
        self._max_dtype_bytes = max(byte_len)

        self._pure_middle_tensors = self._middle_tensors - self._out_tensors
        self._middle_out_tensors = self._middle_tensors.intersection(self._out_tensors)

        pure_out_tensors = [_tensor for _tensor in self._origin_outs if _tensor not in list(self._middle_out_tensors)]
        if len(pure_out_tensors) > 1:
            self._out = _fake_node(pure_out_tensors)
            self.__dfs_sub_graph(self._out, visited_tensors)
        else:
            self._out = pure_out_tensors[0]

        for tensor_i in self._elewise_broadcast_tensors:
            if _match_scalar_scene(tensor_i):
                self._absorbable_broadcast_tensors.add(tensor_i)

    def _calc_cache_read(self):
        self._cache_read_tensors.update(self._input_tensors)
        self._cache_read_tensors.update(self._middle_out_tensors)

    def _do_cache_read(self):
        for tensor_i in self._cache_read_tensors:
            buffer_tensor = self._schedule.cache_read(tensor_i, ELEWISE_SCOPE, self._in_out_map.get(tensor_i))
            self._cache_read_buffer_tensor_map[buffer_tensor] = tensor_i
            self._placeholder_tensor_map[tensor_i] = buffer_tensor

            if tensor_i in self._middle_out_tensors:
                self._middle_out_cache_read_buffer_map[tensor_i] = buffer_tensor

    def _calc_cache_write(self):
        self._cache_write_tensors.update(self._out_tensors - self._copy_out_tensors)

    def _do_cache_write(self):
        for tensor_i in self._cache_write_tensors:
            buffer_tensor = self._schedule.cache_write(tensor_i, ELEWISE_SCOPE)
            self._cache_write_buffer_tensor_map[buffer_tensor] = tensor_i
            self._cache_write_tensor_map[tensor_i] = buffer_tensor

            if tensor_i in self._middle_out_tensors:
                self._middle_out_cache_write_buffer_map[tensor_i] = buffer_tensor

    def _set_scope(self):
        sch = self._schedule
        for tensor_i in self._pure_middle_tensors:
            sch[tensor_i].set_scope(ELEWISE_SCOPE)

    def _judge_one_shape_optimization(self):
        block_size_byte = self._ub_block_size
        all_tensors = self._pure_middle_tensors \
            .union(self._cache_read_buffer_tensor_map.keys()) \
            .union(self._cache_write_buffer_tensor_map.keys()) \
            .union(self._placeholder_tensor_map.keys()) \
            .union(self._cache_write_tensor_map.keys())
        for tensor_i in all_tensors:
            if self._is_one_shape_tensor(tensor_i):
                self._one_shape_tensors.add(tensor_i)
        # special instructions disable one shape optimization
        self._is_one_shape_optimization = len(self._one_shape_tensors) < ONE_SHAPE_MAX_TENSORS and \
            all(not complex_instructions_b64_impl(_one) for _one in self._one_shape_tensors)
        # one shape optimization will need extra block size without any coexstiling_quantity
        if self._is_one_shape_optimization:
            self._ub_size -= len(self._one_shape_tensors) * block_size_byte

    def _calc_tiling(self):
        # when return mask,mask shape return 1/8 about output, ub_factor should use block*8
        mask_flag = operation.get_context().get(ELEWISE_MASK_FLAG)
        if mask_flag is not None and mask_flag == MASK:
            self._ub_factor_align = self._ub_block_size * NUM_EIGHT
        funcs = {TilingStrategy.ALL_FUSE: self._calc_tiling_all_fuse,
                 TilingStrategy.CONST: self._calc_tiling_const,
                 TilingStrategy.DISABLE_FUSE: self._calc_tiling_disable_fuse,
                 TilingStrategy.STATIC: self._calc_tiling_static
                 }
        funcs[self._tiling_strategy]()

    def _calc_tiling_all_fuse(self):
        dound_factor = self._ub_block_size / COMMON_BLOCK_SIZE
        # TYPE DOUNDS
        type_dounds = {
            1: (1, int(32767 * dound_factor)),
            2: (1, int(32767 * dound_factor)),
            4: (1, int(16383 * dound_factor)),
            8: (1, int(8191 * dound_factor)),
        }
        result = self._out
        result_shape = util.shape_to_list(result.shape)
        block_bound = (1, util.get_bound(result_shape[0])[1])
        ub_bound = type_dounds.get(self._max_dtype_bytes)
        ub_bound = (1, util.get_bound(result_shape[0])[1]) if ub_bound is None else ub_bound
        self._block_tiling_vars[0] = operation.var_inner_adaptive("_block_factor_0", block_bound)
        self._ub_tiling_vars[0] = operation.var_inner_adaptive("_ub_factor_0", ub_bound)

    def _calc_tiling_disable_fuse(self):
        res = self._out
        shape = util.shape_to_list(res.shape)
        b_i = self._tiling_case.block_split_axis
        u_i = self._tiling_case.ub_split_axis
        b_bound = (1, util.get_bound(shape[b_i])[1])
        u_bound = self._tiling_case.ub_factor_bound
        if u_bound is None:
            u_bound = (1, util.get_bound(shape[u_i])[1])
        self._block_tiling_vars[b_i] = operation.var_inner_adaptive(f"_block_factor_{b_i}", b_bound)
        self._ub_tiling_vars[u_i] = operation.var_inner_adaptive(f"_ub_factor_{u_i}", u_bound)
        self._block_factor = self._block_tiling_vars.get(b_i)
        self._ub_factor = self._ub_tiling_vars.get(u_i)

    def _calc_tiling_static(self):
        res = self._out
        shape = util.shape_to_list(res.shape)
        b_i = self._tiling_case.block_split_axis
        u_i = self._tiling_case.ub_split_axis
        b_bound = (1, util.get_bound(shape[b_i])[1])
        self._block_tiling_vars[b_i] = operation.var_inner_adaptive(f"_block_factor_{b_i}", b_bound)
        self._ub_tiling_vars[u_i] = self._tiling_case.ub_factor_bound
        self._block_factor = self._block_tiling_vars.get(b_i)
        self._ub_factor = self._ub_tiling_vars.get(u_i)

    def _calc_tiling_const(self):
        block_size_byte = self._ub_block_size
        res = self._out
        output_shape_list = util.shape_to_list(res.shape)
        if output_shape_list == [0]:
            self._const_block_dims = 1
            self._need_do_block = False
            return

        # construct const tiling inputs and outs
        inputs = []
        for _input in self._input_tensors:
            input_shape = util.shape_to_list(_input.shape)
            inputs.append({"shape": input_shape, "dtype": _input.dtype})
        outputs = [{"shape": output_shape_list, "dtype": res.dtype}]
        if len(inputs) == 0:
            inputs = deep_copy(outputs)

        # collect const tiling compile info
        max_available_ub = ((((self._ub_size - self._extra_ub_size) // self._coexisting_quantity) // block_size_byte) *
                            block_size_byte) // self._max_dtype_bytes 
        max_available_ub_db = ((((self._ub_size - 2 * self._extra_ub_size) // 2 // self._coexisting_quantity)
                                // block_size_byte) * block_size_byte) // self._max_dtype_bytes
        
        base_info = {"000": [util.get_core_num(), self._max_dtype_bytes, max_available_ub, max_available_ub_db]} 

        # const tiling is not sensitive with is_const_shape, adding for simplify tiling parser
        const_compile_info = {
            CompileInfo.BASE_INFO: base_info,
            CompileInfo.IS_CONST_SHAPES: False,
            CompileInfo.MAX_OUT_DTYPE_NUM: block_size_byte // DTYPE_BYTE_MAPPING.get(res.dtype),
            CompileInfo.ONLY_CONST_TILING: True,
            CompileInfo.UB_FACTOR_ALIGN: self._ub_factor_align,
            CompileInfo.INT64_MODE: operation.is_int64_mode(),
            CompileInfo.UB_BLOCK_SIZE: self._ub_block_size
        }

        const_compile_info.update(get_compile_info())
        op_type = "AutoTiling"
        run_info = op_tiling.do_op_tiling(op_type, const_compile_info, inputs, outputs)
        tiling_type = "int64" if operation.is_int64_mode() else "int"
        tiling_format = {
            "need_multi_core": tiling_type,
            "block_axis": tiling_type,
            "block_factor": tiling_type,
            "ub_axis": tiling_type,
            "ub_factor": tiling_type,
            "is_need_db": tiling_type,
            "is_need_nb": tiling_type}
        tiling_data = op_tiling.decode(run_info.get('tiling_data'), tiling_format)
        self._const_block_dims = run_info.get("block_dim")
        self._need_do_block = tiling_data.get("need_multi_core") > 0
        if self._need_do_block:
            self._block_factor = tiling_data.get("block_factor")
            self._ub_split_axis = tiling_data.get("ub_axis")
            self._ub_factor = tiling_data.get("ub_factor")

        self._is_db = tiling_data.get("is_need_db", 0) == 1
        self._is_nb = tiling_data.get("is_need_nb", 0) == 1

    def _calc_compute_inline(self):
        self._compute_inline_tensors = self._absorbable_broadcast_tensors.copy()

    def _do_tiling(self):
        funcs = {TilingStrategy.ALL_FUSE: self._do_tiling_all_fuse,
                 TilingStrategy.DISABLE_FUSE: self._do_tiling_disable_fuse,
                 TilingStrategy.STATIC: self._do_tiling_static,
                 TilingStrategy.CONST: self._do_tiling_const,
                 }
        funcs.get(self._tiling_strategy)()

    def _do_tiling_ub_cut_first(self, u_idx, b_factor, u_factor):
        sch = self._schedule
        res = self._out
        shape = util.shape_to_list(res.shape)
        block_axes = []
        ub_axes = []
        inner_axes = []

        # step 1: split ub axis
        u_o, u_i = sch[res].split(res.op.axis[u_idx], factor=u_factor)
        ub_axes.append([u_o, u_idx])
        inner_axes.append([u_i, u_idx])
        self._inner_shape.append([u_factor, u_idx])
        for i in range(u_idx):
            block_axes.append([res.op.axis[i], i])
        block_axes.append([u_o, u_idx])

        # step 2: fuse all block axis
        block_fuse_axis = sch[res].fuse(*[x[0] for x in block_axes])

        # step 3: split block axis
        b_o, b_i = sch[res].split(block_fuse_axis, factor=b_factor)

        for i in range(u_idx + 1, len(res.op.axis)):
            inner_axes.append([res.op.axis[i], i])
            self._inner_shape.append([shape[i], i])
        self._block_bind_axis = b_o
        self._compute_at_axis = b_i
        self._emit_insn_axis = inner_axes[0][0]

    def _do_tiling_all_fuse(self):
        sch = self._schedule
        res = self._out
        block_axes = []
        ub_axes = []
        inner_axes = []
        b_o, b_i = sch[res].split(res.op.axis[0], factor=self._block_tiling_vars.get(0))
        block_axes.append([b_o, 0])

        u_o, u_i = sch[res].split(b_i, factor=self._ub_tiling_vars.get(0))
        ub_axes.append([u_o, 0])
        inner_axes.append([u_i, 0])
        self._inner_shape.append([self._ub_tiling_vars.get(0), 0])

        self._block_bind_axis = sch[res].fuse(*[x[0] for x in block_axes])
        self._compute_at_axis = u_o
        self._emit_insn_axis = u_i
        if res in self._cache_write_tensor_map:
            extent_ub_factor = self._ub_tiling_vars.get(0)
            extent_ub_tail = self._block_tiling_vars.get(0) - (self._compute_at_axis * self._ub_tiling_vars.get(0))
            extent_tail_ub_tail = res.shape[0] - b_o * self._block_tiling_vars.get(0) - \
                self._compute_at_axis * self._ub_tiling_vars.get(0)
            sch[self._cache_write_tensor_map[res]].buffer_tile((None, tvm.min(extent_ub_factor, \
                tvm.min(extent_ub_tail, extent_tail_ub_tail))))
        else:
            for tensor_i in res.op.input_tensors:
                extent_ub_factor = self._ub_tiling_vars.get(0)
                extent_ub_tail = self._block_tiling_vars.get(0) - (self._compute_at_axis * self._ub_tiling_vars.get(0))
                extent_tail_ub_tail = tensor_i.shape[0] - b_o * self._block_tiling_vars.get(0) - \
                       self._compute_at_axis * self._ub_tiling_vars.get(0)
                sch[tensor_i].buffer_tile((None, tvm.min(extent_ub_factor, tvm.min(extent_ub_tail, \
                    extent_tail_ub_tail))))

    def _do_tiling_disable_fuse(self):
        b_idx = self._tiling_case.block_split_axis
        u_idx = self._tiling_case.ub_split_axis
        block_factor = self._block_tiling_vars.get(b_idx)
        ub_factor = self._ub_tiling_vars.get(u_idx)
        return self._do_tiling_ub_cut_first(u_idx, block_factor, ub_factor)

    def _do_tiling_static(self):
        self._do_tiling_disable_fuse()

    def _do_tiling_const_all_fuse(self):
        sch = self._schedule
        res = self._out
        block_axes = []
        if self._need_do_block:
            b_o, b_i = sch[res].split(res.op.axis[0], factor=self._block_factor)
            block_axes.append([b_o, 0])
            self._block_bind_axis = sch[res].fuse(*[x[0] for x in block_axes])
            u_o, u_i = sch[res].split(b_i, factor=self._ub_factor)
            self._compute_at_axis = u_o
            self._emit_insn_axis = u_i
        else:
            self._emit_insn_axis = res.op.axis[0]

    def _do_tiling_const(self):
        disable_fuse = operation.get_context().get("_elewise_disable_fuse")
        is_fractal_format = operation.get_context().get_current_compute().get("_is_fractal_format")
        disable_all_fuse = not self._is_one_dim or is_fractal_format or disable_fuse
        if not disable_all_fuse:
            self._do_tiling_const_all_fuse()
        else:
            u_idx = self._ub_split_axis
            block_factor = self._block_factor
            ub_factor = self._ub_factor
            if self._need_do_block:
                self._do_tiling_ub_cut_first(u_idx, block_factor, ub_factor)
            else:
                self._emit_insn_axis = self._out.op.axis[0]

    def _calc_multi_core(self):
        pass

    def _do_multi_core(self):
        if self._block_bind_axis is not None:
            block = tvm.thread_axis("blockIdx.x")
            self._schedule[self._out].bind(self._block_bind_axis, block)

    def _calc_compute_at(self):
        if self._tiling_strategy == TilingStrategy.CONST and not self._need_do_block:
            self._compute_at_map.clear()
            return

        # traverse all tensors without compute_inline tensors
        for tensor_i in self._input_tensors:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

        for tensor_i in self._middle_tensors - self._compute_inline_tensors:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

        for tensor_i in self._cache_read_buffer_tensor_map:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

        for tensor_i in self._cache_write_buffer_tensor_map:
            self._compute_at_map[tensor_i] = [self._out, self._compute_at_axis]

    def _do_compute_at(self):
        sch = self._schedule
        for tensor_i, param in self._compute_at_map.items():
            if self._is_one_shape_optimization and tensor_i in self._one_shape_tensors:
                sch[tensor_i].compute_root()
            else:
                sch[tensor_i].compute_at(sch[param[0]], param[1])

    def _calc_emit_insn(self):
        def get_insn(tensor_):
            tag = tensor_.op.tag
            if tensor_.op.tag.find("|") != -1:
                insn = tag.split("|")[0]
            else:
                insn = tag
            return INSN_MAPPING.get(insn, insn)

        for source, target in self._cache_read_buffer_tensor_map.items():
            if target in self._middle_out_tensors:
                self._emit_insn_map[source] = [source.op.axis[0], "phony_insn"]
            else:
                self._emit_insn_map[source] = [source.op.axis[0], "dma_copy"]

        for tensor_i in self._pure_middle_tensors - self._compute_inline_tensors:
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], get_insn(tensor_i)]

        for source, target in self._cache_write_buffer_tensor_map.items() - self._compute_inline_tensors:
            self._emit_insn_map[source] = [source.op.axis[0], get_insn(target)]

        if len(self._out_tensors) > 1:
            for tensor_i in self._out_tensors:
                self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], "dma_copy"]
            if len(self._out_tensors) - len(self._middle_out_tensors) > 1:
                self._emit_insn_map[self._out] = [self._emit_insn_axis, "phony_insn"]
            else:
                self._emit_insn_map[self._out] = [self._emit_insn_axis, "dma_copy"]
        else:
            for tensor_i in self._out_tensors:
                self._emit_insn_map[tensor_i] = [self._emit_insn_axis, "dma_copy"]

    def _do_emit_insn(self):
        sch = self._schedule
        for tensor_i, param in self._emit_insn_map.items():
            if param[1] == "unknown_broadcast":
                buffer_size = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
                attrs = {"storage_bound": [buffer_size], "dynamic_fuse": False, "dynamic_split": False}
                sch[tensor_i].emit_insn(param[0], param[1], attrs)
            elif tensor_i.dtype == "int64" and \
                 (param[1] in ["vector_min", "vector_max"] or
                 (util.is_v220() and \
                 param[1] in ["vector_add", "vector_sub"])):
                sch[tensor_i].emit_insn(param[0], param[1], attrs={"parallel_int64": 1})
            elif tensor_i in self._out_tensors and self._is_one_dim:
                sch[tensor_i].emit_insn(param[0], param[1], attrs={"no_overlap": 0})
            elif param[1] == "vector_conv_rint" and tensor_i.op.attrs.get("decimals") is not None:
                sch[tensor_i].emit_insn(param[0], param[1], attrs={"decimals": tensor_i.op.attrs.get("decimals")})
            elif param[1] == "vector_select_bool" and self._ub_block_size == NANO_BLOCK_SIZE:
                sch[tensor_i].emit_insn(param[0], param[1], attrs={"mode": tensor_i.op.attrs.get("_mode")})
            else:
                sch[tensor_i].emit_insn(param[0], param[1])

    def _calc_double_buffer(self):
        pass

    def _do_double_buffer(self):
        if self._is_db:
            sch = self._schedule

            # get all tensors including placeholders, pure_middle tensors and pure_out tensors
            tensors = self._pure_middle_tensors \
                .union(self._cache_read_buffer_tensor_map.keys()) \
                .union(self._cache_write_buffer_tensor_map.keys())

            for tensor_i in tensors:
                sch[tensor_i].double_buffer()

    def _do_n_buffer(self):
        if self._is_nb:
            sch = self._schedule

            # get all tensors including placeholders, pure_middle tensors and pure_out tensors
            tensors = self._cache_read_buffer_tensor_map.keys()

            for tensor_i in tensors:
                sch[tensor_i].n_buffer(4)

    def _calc_mem_reuse(self):
        # one of the input of the ternary instruction must be reused with the output, and the reuse location is 
        # determined by the ternary_reuse_map.
        # consider "vmadd": A=A*A+B, output reuse the second A, input_tensors is 2, which need to be completed to 3
        ternary_reuse_map = {
            "elewise_binary_scalar_axpy": 1,
            "elewise_multiple_mla": 2,
            "elewise_multiple_madd": 1,
            "elewise_multiple_maddrelu": 1,
        }

        def __get_reuse_ub_tensor(_input_tensor, _output_tensor):
            if _input_tensor in self._placeholder_tensor_map:
                _input_tensor = self._placeholder_tensor_map.get(_input_tensor, None)
            if _output_tensor in self._cache_write_tensor_map:
                _output_tensor = self._cache_write_tensor_map.get(_output_tensor, None)
            return _input_tensor, _output_tensor

        # compute_inline_tensors no need to reuse memory
        for tensor_i in self._out_tensors | self._pure_middle_tensors - self._compute_inline_tensors:
            insn = util.get_dsl_insn(tensor_i)
            args = ""
            if tensor_i.op.tag.find("|") != -1:
                args = tensor_i.op.tag.split("|")[1].split(",")
            if insn in TERNARY_INSNS:
                src_tensors = []
                index = 0
                first_same_tensor = None
                for i in args:
                    if i == "1":
                        if first_same_tensor not in src_tensors:
                            first_same_tensor = tensor_i.op.input_tensors[index]
                            index += 1
                        src_tensors.append(first_same_tensor)
                    else:
                        src_tensors.append(tensor_i.op.input_tensors[index])
                        index += 1
                reuse_index = ternary_reuse_map.get(insn)
                src_tensor = src_tensors[reuse_index]
                src_tensor, dst_tensor = __get_reuse_ub_tensor(src_tensor, tensor_i)
                util.merge_value(self._mem_reuse_map, src_tensor, dst_tensor)
        for tensor_i, write_buffer in self._middle_out_cache_write_buffer_map.items():
            util.merge_value(self._mem_reuse_map,
                             self._middle_out_cache_read_buffer_map.get(tensor_i),
                             write_buffer)

    def _do_mem_reuse(self):
        sch = self._schedule
        for _a, _b in self._mem_reuse_map.items():
            for b_i in _b:
                sch[_a].reused_by(b_i)

    def _calc_storage_bound(self):
        block_size_byte = self._ub_block_size
        special_factor_align = block_size_byte * NUM_EIGHT

        def _correct_ub_size_by_cmp_sel(_tensor):
            # vcmp/vsel/vcmpsel only first input must be tensor, others can be scalar that each needs one block ub space
            if util.is_vcmp_insn(_tensor):
                self._extra_ub_size += block_size_byte * (VCMP_INPUT_NUMBER - len(_tensor.op.input_tensors))
            if util.is_vsel_insn(_tensor):
                self._extra_ub_size += block_size_byte * (VSEL_INPUT_NUMBER - len(_tensor.op.input_tensors))
                if (util.is_v200() or util.is_v220()) and (VSEL_INPUT_NUMBER == len(_tensor.op.input_tensors)):
                    self._extra_ub_size += block_size_byte
            if util.is_vcmpsel_insn(_tensor):
                self._extra_ub_size += block_size_byte * (VCMPSEL_INPUT_NUMBER - len(_tensor.op.input_tensors))

        def _dst_can_not_reuse_src(_tensor):
            # place hold tensor need one in ub
            is_place_hold = util.is_placeholder(_tensor)
            # check by insn if dst can reuse src
            dst_no_reuse_src_insn = util.get_dsl_insn(_tensor) in DST_SRC_NO_REUSE_SET
            # if inputs(src) are all in dependent after refresh dependent, dst can not reuse inputs
            dst_no_src_dependent = set(_tensor.op.input_tensors).issubset(set(dependent_map.keys()))

            return is_place_hold or dst_no_reuse_src_insn or dst_no_src_dependent

        def _match_broadcast_inline(_in_tensor):
            # broadcast compute inline must match three scenes:
            # 1. current tensor match scalar scene
            # 2. current tensor is not placeholder and can not gen from scalar broadcast
            # 3. input tensor of current tensor must be in the dependent_map
            if _in_tensor in self._absorbable_broadcast_tensors and _in_tensor.op.input_tensors and \
                    _in_tensor.op.input_tensors[0] in dependent_map:
                return True
            return False

        def _match_ub_size_correct(_tensor):
            if util.need_temp_space(_tensor) or _need_external_space(_tensor):
                return True
            return False

        def _calc_current_coexist_node(_tensor):
            # one of the input of the ternary instruction must be reused with the output
            block_size_byte = self._ub_block_size
            _current_coexist_node = len(dependent_map)

            # broadcast_inline can reduce the coexist node
            for tensor_i in dependent_map:
                if _match_broadcast_inline(tensor_i):
                    _current_coexist_node -= 1

            # check if tensor is vsignbit calculation
            if _tensor.op.tag == "elewise_single_signbit" and util.is_v220():
                _current_coexist_node += 1

            # check if tensor is complex calculation
            if complex_calc_instructions(_tensor):
                if util.is_v220():
                    op_tag = _tensor.op.tag.split('|')[0]
                    _current_coexist_node += EXTRA_NODE_COMPLEX.get(op_tag, 0)
                    self._extra_ub_size += EXTRA_TEMP_COMPLEX.get(op_tag, 0)

            extra_node, extra_ub = cast_ints2ints_external_space(_tensor)
            _current_coexist_node += extra_node
            self._extra_ub_size += extra_ub

            # check if tensor impl by complex instructions
            if complex_instructions_b64_impl(_tensor):
                _current_coexist_node += EXTRA_NODE_B64.get(_tensor.op.tag)
                if util.is_v220() \
                    and _tensor.op.tag in ["elewise_binary_add", "elewise_binary_sub", "elewise_single_VS_add"]:
                    _current_coexist_node += 1
                if cast_complex_instructions(_tensor):
                    # int64 cast to float16 need one more coexist node
                    if _tensor.dtype == "float16":
                        _current_coexist_node += 1
                    # complex impl of cast from int32 to int64 requires ub_factor be multiplies of 256
                    if _tensor.dtype == "int64":
                        self._ub_factor_align = max(special_factor_align, self._ub_factor_align)
                if base_op_complex_instructions(_tensor):
                    self._extra_ub_size += EXTRA_TEMP_BUFFER.get(_tensor.op.tag)
            
            # check if tensor impl by vsel complex instructions
            if vsel_complex_instructions(_tensor):
                _current_coexist_node += 1

            # single instruction support b64
            if base_single_instructions_b64(_tensor):
                _current_coexist_node += EXTRA_NODE_B64.get(_tensor.op.tag)

            # power instruction support b32
            if powi_complex_instructions_b32(_tensor):
                _current_coexist_node += EXTRA_NODE_B32.get(_tensor.op.tag)
                self._extra_ub_size += EXTRA_TEMP_BUFFER.get(_tensor.op.tag)

            # 5hd pad need extra coexist node
            if self._fractal_format_actions is not None and len(self._fractal_format_actions) > 0:
                _current_coexist_node += 1
            
            if _tensor.op.tag == "elewise_binary_gcd":
                _current_coexist_node += 24 if _tensor.dtype == "int64" else 3
            
            if "|with_decimals" in _tensor.op.tag and _tensor.dtype == "float16":
                _current_coexist_node += 2
                self._extra_ub_size += block_size_byte
            
            if "|with_decimals" in _tensor.op.tag and _tensor.dtype == "float32":
                self._extra_ub_size += block_size_byte

            # check if all src be used later
            if _dst_can_not_reuse_src(_tensor):
                _current_coexist_node += 1

            # one_rank_broadcast/unknown_broadcast needs one more node
            if util.need_extent_node(_tensor):
                _current_coexist_node += 1

            # correct ub size in broadcast absorb
            if _match_ub_size_correct(_tensor):
                self._extra_ub_size += block_size_byte

            # correct ub size in vcmp or vsel or vcmpsel
            _correct_ub_size_by_cmp_sel(_tensor)

            return _current_coexist_node

        def _r_coexisting(_tensor):
            if _tensor in dependent_map:
                return len(dependent_map)
            _need_coexist_node = []
            for _tensor_i in _tensor.op.input_tensors:
                _need_coexist_node.append(_r_coexisting(_tensor_i))

            _current_coexist_node = _calc_current_coexist_node(_tensor)

            _need_coexist_node.append(_current_coexist_node)

            # after calculate current node then refresh its coexist relation
            _refresh_dependent(_tensor)

            if _tensor not in dependent_map:
                # one shape with one_shape_optimization will not add into dependent map;
                # all out shape out cur shape must be greater than one will cur shape is greater than one
                if not (self._is_one_shape_tensor(_tensor) and self._is_one_shape_optimization):
                    dependent_map[_tensor] = self._in_out_map.get(_tensor).copy()
            return max(_need_coexist_node)

        def _refresh_dependent(_tensor):
            for _tensor_i in _tensor.op.input_tensors:
                if _tensor_i not in dependent_map:
                    continue
                dependent_map[_tensor_i].remove(_tensor)
                if not dependent_map.get(_tensor_i):
                    dependent_map.pop(_tensor_i)

        def _need_external_space(_tensor):
            # absorb broadcast needs tmp ub size
            if any(x in self._absorbable_broadcast_tensors for x in _tensor.op.input_tensors):
                return True
            op_tag = util.get_dsl_insn(_tensor)
            if op_tag in ELEWISE_BROADCAST_INSNS:
                return True
            support_vector_scalar_insns = ("elewise_binary_add", "elewise_binary_mul")
            if op_tag in set(SUPPORT_SCALAR_INSNS) - set(support_vector_scalar_insns):
                return True

            if util.is_v100() and op_tag in support_vector_scalar_insns and _tensor.dtype == "int32":
                return True
            return False

        coexisting_quantities = []
        dependent_map = {}
        for tensor_i in self._out.op.input_tensors:
            coexisting_quantities.append(_r_coexisting(tensor_i))
        if not self._out.op.tag == FAKE_NODE_TAG:
            # last node cal current node
            current_coexist_node = _calc_current_coexist_node(self._out)
            coexisting_quantities.append(current_coexist_node)

        if self._is_nb:
            self._coexisting_quantity = N_BUFFER_COEXISTING_QUANTITY
        else:
            self._coexisting_quantity = max(coexisting_quantities)

        if self._coexisting_quantity == 1:
            self._extra_ub_size += block_size_byte

        # in order to improve performance, add one node
        if len(self._input_tensors) >= 2:
            self._coexisting_quantity += 1

    def _do_storage_bound(self):
        block_size_byte = self._ub_block_size
        one_dim_align = block_size_byte * NUM_FOUR
        double_buffer_factor = 2 if self._is_db else 1
        nb_factor = 4 if self._is_nb else 1
        # delete tmp size
        self._ub_size -= max(double_buffer_factor, nb_factor) * self._extra_ub_size
        if self._is_nb:
            tensor_space = self._ub_size // N_BUFFER_COEXISTING_QUANTITY
        else:
            tensor_space = self._ub_size // self._coexisting_quantity // double_buffer_factor
        self._tensor_space = tensor_space // block_size_byte * block_size_byte

        sch = self._schedule
        tensors = self._pure_middle_tensors \
            .union(self._cache_read_buffer_tensor_map.keys()) \
            .union(self._cache_write_buffer_tensor_map.keys())

        # one shape optimization tensor will use one block size, and other all tensors equally share ub size
        for tensor_i in tensors:
            one_block = int(block_size_byte // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            if self._is_one_shape_optimization and self._is_one_shape_tensor(tensor_i):
                if complex_calc_instructions(tensor_i) and util.is_v220() and tensor_i.op.tag in EXTRA_BLOCK:
                    sch[tensor_i].set_buffer_size(one_block * 32)
                else:
                    sch[tensor_i].set_buffer_size(one_block)
            else:
                max_buffer_size = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
                if "elewise_binary_vcmpv" in tensor_i.op.tag:
                    max_buffer_size = int(self._tensor_space // 
                    DTYPE_BYTE_MAPPING.get(tensor_i.op.input_tensors[0].dtype))
                if complex_calc_instructions(tensor_i) and util.is_v220():
                    max_buffer_size += EXTRA_BLOCK.get(tensor_i.op.tag, 0) * one_block
                sch[tensor_i].set_buffer_size(max_buffer_size)

    def _do_compute_inline(self):
        sch = self._schedule
        for tensor_i in self._compute_inline_tensors:
            sch[tensor_i].compute_inline()

    def _check_tiling_case(self):
        lower_bound = 1
        for item in self._inner_shape[::-1]:
            cur_bound = util.get_bound(item[0])[0]
            if cur_bound is None:
                return False
            lower_bound *= cur_bound
        if not self._tensor_space // self._max_dtype_bytes >= lower_bound:
            return False
        return True

    def _add_compile_info(self):
        block_size_bytes = self._ub_block_size
        special_factor_align = block_size_bytes * NUM_EIGHT
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()
        mode = operation.get_context().get_current_compute().get("_mode")
        if mode == CONST:
            # const shape: one compute, one schedule
            cpt_compute.add("_elewise_const_block_dim", self._const_block_dims)
        else:
            cpt_schedule.add(CompileInfo.MAX_DTYPE, self._max_dtype_bytes)
            if self._is_nb:
                cpt_schedule.add(CompileInfo.COEXISTING_QUANTITY, N_BUFFER_COEXISTING_QUANTITY)
            else:
                cpt_schedule.add(CompileInfo.COEXISTING_QUANTITY, self._coexisting_quantity)
            cpt_schedule.add(CompileInfo.UB_SIZE, self._ub_size)
            cpt_schedule.add(CompileInfo.CORE_NUM, util.get_core_num())
            cpt_schedule.add("_tiling_key", self._schedule.tiling_key)

        # add max_out_dtype_num for simplify runtime tiling calculation
        max_out_dtype_num = \
            block_size_bytes // max(DTYPE_BYTE_MAPPING.get(_tensor.dtype) for _tensor in self._out_tensors)
        operation.add_compile_info_inner(CompileInfo.MAX_OUT_DTYPE_NUM, max_out_dtype_num)

        # add ub_block_size 
        operation.add_compile_info_inner(CompileInfo.UB_BLOCK_SIZE, self._ub_block_size)

        # out uint1 dtype needs ub_factor align to 256
        out_dtypes = {_tensor.dtype for _tensor in self._out_tensors}
        if "uint1" in out_dtypes:
            self._ub_factor_align = max(self._ub_factor_align, special_factor_align)
        operation.add_compile_info_inner(CompileInfo.UB_FACTOR_ALIGN, self._ub_factor_align)

        if self._is_nb:
            max_available_ub_nb = ((((self._ub_size - 4 * self._extra_ub_size) // N_BUFFER_COEXISTING_QUANTITY)
                                    // block_size_bytes) * block_size_bytes) // self._max_dtype_bytes
            operation.add_compile_info_inner("_max_available_ub_nb", max_available_ub_nb)


    def _get_ub_tensor(self, tensor_i):
        if tensor_i in self._placeholder_tensor_map:
            return self._placeholder_tensor_map.get(tensor_i)
        if tensor_i in self._cache_write_tensor_map:
            return self._cache_write_tensor_map.get(tensor_i)
        return tensor_i

    def __dfs_sub_graph(self, out, visited_tensors: set):
        for tensor_i in out.op.input_tensors:
            util.merge_value(self._in_out_map, tensor_i, out)
            self._dtypes.add(tensor_i.dtype)

            if util.is_placeholder(tensor_i):
                self._input_tensors.add(tensor_i)
            else:
                self._middle_tensors.add(tensor_i)
                if util.get_dsl_insn(tensor_i) in ELEWISE_BROADCAST_INSNS:
                    self._elewise_broadcast_tensors.add(tensor_i)

            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self.__dfs_sub_graph(tensor_i, visited_tensors)

    def _calc_set_value(self):
        if operation.get_context().get_current_compute().get('_is_fractal_format'):
            self._fractal_format_actions = padding.calc_padding(self._origin_outs)

        if self._fractal_format_actions:
            operation.add_compile_info_inner(CompileInfo.CONTAINS_NEED_PAD_COMPUTE, True)

    def _do_set_value(self):
        if self._fractal_format_actions:
            for action in self._fractal_format_actions:
                action_type = action.get_action_type()
                tensor = action.get_tensor()
                condition = action.get_condition()
                value = action.get_value()
                target_tensors = action.get_target_tensors()
                value_type = action.get_value_type()

                if value_type == ActionValueType.TENSOR:
                    value = value(self._get_ub_tensor(tensor))
                if action_type == ActionType.SET_VALUE:
                    self._schedule[self._get_ub_tensor(tensor)].set_value(condition, value)
                elif action_type == ActionType.CACHE_READ_AND_SET_VALUE:
                    set_value_cache_read_buffer = self._schedule.cache_read(tensor, ELEWISE_SCOPE, target_tensors)
                    self._emit_insn_map[set_value_cache_read_buffer] = \
                        [set_value_cache_read_buffer.op.axis[0], 'dma_copy']
                    if self._set_value_cache_read_map[tensor] is None:
                        set_value_cache_read_buffer_set = {set_value_cache_read_buffer}
                        self._set_value_cache_read_map[tensor] = set_value_cache_read_buffer_set
                    self._set_value_cache_read_map[tensor].add(set_value_cache_read_buffer)


def _fake_node(tensors):
    dtype = tensors[0].dtype
    max_len = max(len(t.shape) for t in tensors)
    shape = [1] * max_len
    for tensor_i in tensors:
        if DTYPE_BYTE_MAPPING.get(tensor_i.dtype) > DTYPE_BYTE_MAPPING.get(dtype):
            dtype = tensor_i.dtype
        shape_i = util.shape_to_list(tensor_i.shape)
        diff_len = max_len - len(shape_i)
        shape_i = [1] * diff_len + shape_i
        for j in range(diff_len, max_len):
            if util.equals_one(shape[j]):
                shape[j] = shape_i[j]
            elif not expr_equal(shape[j], shape_i[j]) and not util.equals_one(shape_i[j]):
                shape[j] = tvm.max(shape_i[j], shape[j])

    def _fake_compute(*indices):
        res_ = tvm.const(1, dtype)
        for tensor in tensors:
            cur_indices = []
            for idx, dim in enumerate(tensor.shape):
                cur_indices.append(0 if util.equals_one(dim) else indices[idx])
            res_ *= tvm.expr.Cast(dtype, tensor(*cur_indices))
        return res_

    with tvm.tag_scope(FAKE_NODE_TAG):
        res = tvm.compute(shape, _fake_compute, name="fake_node")

    return res


def _copy_node(tensor):
    shape = tensor.shape
    with tvm.tag_scope("dma_copy"):
        res = tvm.compute(shape, lambda *index: tensor(*index), name="copy_node")
    return res
