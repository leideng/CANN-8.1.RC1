#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
broadcast helper
"""
import functools
from functools import reduce
from operator import mul

from tbe.dsl.base import operation
from ... import util
from ...constants import DTYPE_BYTE_MAPPING

from .broadcast_constants import TENSOR_VOLUME_STR_KEY
from .broadcast_constants import BRC_ELE_INTERSECT_INPUT
from .broadcast_constants import BRC_ELE_INTERSECT_MIDDLE_OUT
from .broadcast_constants import BRC_ELE_INTERSECT_PURE_MIDDLE
from .broadcast_constants import INPLACE_INPUT_STR_KEY
from .broadcast_constants import MEM_UNIQUE_STR_KEY
from .broadcast_constants import COMPUTE_ROOT_GRAPH_SIZE_COEFFICIENT
from .broadcast_constants import COMPUTE_ROOT_FAKE_NODE
from .broadcast_constants import INPLACE_FAKE_NODE
from .broadcast_util import GraphUtil
from .broadcast_util import ScheduleUtil


class ComputeRootGraphHelper:

    def __init__(self, outs, is_const):
        self._init_properties(outs, is_const)
        self._deal_compute_root(outs)

    def _init_properties(self, outs, is_const):
        self._outs = outs
        self._is_const = is_const
        self._op_compute_root_end_tensors = set()
        self._op_inplace_out_tensors = set()
        self._inplace_out_size = 1
        self._compute_root_tensor_and_tensor_before_map = {}
        self._compute_root_in_out_map = {}

        self.dyn_compute_root_tensors = set()
        self.compute_root_size = 1
        self.compute_root_unique_tensors = set()
        self.compute_root_max_byte = 2
        self.block_one_out_tensors = set()
        self.compute_root_max_coexisting_quantity = 1
        self.compute_root_out_index = set()
        self.mem_unique_tensors = set()
        self.enable_compute_root = False
        self.only_has_tensor_volume = False
        self.compute_root_fake_node = None
        self.compute_root_tmp_ub_size = 0

    def _deal_compute_root(self, outs):
        """
        build infomation for compute root
        @param outs:
        """
        # get tensor tag by traveling compute graph
        self._travel_compute_graph(outs)

        if self.enable_compute_root:
            self.dyn_compute_root_tensors = GraphUtil.calc_all_graph_tensors(self._op_compute_root_end_tensors |
                                                                             self._op_inplace_out_tensors)
            self._calc_compute_root_graph()
            self._calc_compute_root_tensor_size()
            self._calc_compute_root_unique()
            self._calc_compute_root_max_dtype()
            self._calc_compute_block_one_tensors()
            self._calc_compute_root_coexisting_quantity_size()
            self._calc_compute_root_fake_node()
            self._calc_compute_root_temp_ub_size()

    def _travel_compute_graph(self, outs):
        visited_tensors = set()
        for out in outs:
            self._calc_special_attr_tensor(out)
            self._dfs_sub_graph(out, visited_tensors)

        # compute root need op tag
        self.enable_compute_root = \
            len(self._op_compute_root_end_tensors) > 0 or len(self._op_inplace_out_tensors) > 0
        if not self.enable_compute_root:
            return
        # when compute graph only has tensor volume tag, enable compute at when size is bigger than tensor volume
        self.only_has_tensor_volume = \
            len(self._op_compute_root_end_tensors) > 0 and len(self._op_inplace_out_tensors) == 0

        # disable compute root when tensor shape size is bigger than given tensor volume in const mode
        if self._is_const:
            tensor_size_set = set()
            for _tensor in (self._op_compute_root_end_tensors | self._op_inplace_out_tensors):
                shape_list = util.shape_to_list(_tensor.shape)
                shape_size = reduce(lambda x, y: x * y, shape_list or [1])
                tensor_size_set.add(shape_size)
            self.enable_compute_root = all(tensor_size <= self.compute_root_size for tensor_size in tensor_size_set)

    def _calc_special_attr_tensor(self, tensor):
        if INPLACE_INPUT_STR_KEY in tensor.op.attrs and TENSOR_VOLUME_STR_KEY in tensor.op.attrs:
            self._op_inplace_out_tensors.add(tensor)
            self._inplace_out_size = max(tensor.op.attrs[TENSOR_VOLUME_STR_KEY].value, self._inplace_out_size)
            for tensor_before in GraphUtil.calc_all_graph_tensors({tensor}):
                util.merge_value(self._compute_root_tensor_and_tensor_before_map, tensor, tensor_before)

        if INPLACE_INPUT_STR_KEY not in tensor.op.attrs and TENSOR_VOLUME_STR_KEY in tensor.op.attrs:
            self._op_compute_root_end_tensors.add(tensor)
            self.compute_root_size = max(tensor.op.attrs[TENSOR_VOLUME_STR_KEY].value, self.compute_root_size)
            for tensor_before in GraphUtil.calc_all_graph_tensors({tensor}):
                util.merge_value(self._compute_root_tensor_and_tensor_before_map, tensor, tensor_before)

        if MEM_UNIQUE_STR_KEY in tensor.op.attrs and operation.in_dynamic() and util.is_v220():
            self.mem_unique_tensors.add(tensor.op.input_tensors[0])

    def _dfs_sub_graph(self, out, visited_tensors):
        for tensor_i in out.op.input_tensors:
            if tensor_i in visited_tensors:
                continue
            self._calc_special_attr_tensor(tensor_i)
            visited_tensors.add(tensor_i)
            self._dfs_sub_graph(tensor_i, visited_tensors)

    def _calc_compute_root_graph(self):
        def dfs_construct_compute_root_graph(tensor, visited_tensors):
            for input_i in tensor.op.input_tensors:
                if input_i in visited_tensors:
                    continue

                if input_i in self._outs:
                    self.compute_root_out_index.add(self._outs.index(input_i))

                util.merge_value(self._compute_root_in_out_map, input_i, tensor)
                visited_tensors.add(input_i)
                dfs_construct_compute_root_graph(input_i, visited_tensors)

        visited_tensors = set()
        for tensor_i in self.dyn_compute_root_tensors:
            if tensor_i in visited_tensors:
                continue

            if tensor_i in self._outs:
                self.compute_root_out_index.add(self._outs.index(tensor_i))
            visited_tensors.add(tensor_i)
            dfs_construct_compute_root_graph(tensor_i, visited_tensors)

    def _calc_compute_root_coexisting_quantity_size(self):
        all_in_out_map = dict()
        root_end_tensors = self._op_inplace_out_tensors | self._op_compute_root_end_tensors
        for tensor_i in root_end_tensors:
            if not all_in_out_map:
                all_in_out_map = GraphUtil.get_in_out_map(tensor_i)
                continue
            cur_in_out_map = GraphUtil.get_in_out_map(tensor_i)
            for t_in, t_outs in cur_in_out_map.items():
                util.merge_value(all_in_out_map, t_in, t_outs)

        for tensor_i in root_end_tensors:
            coexisting_quantity = GraphUtil.max_live_node(tensor_i, all_in_out_map, False)
            self.compute_root_max_coexisting_quantity = max(self.compute_root_max_coexisting_quantity,
                                                            coexisting_quantity)

    def _calc_compute_root_tensor_size(self):
        # use compute_root size from op in dynamic context.
        if not self._is_const:
            self.compute_root_size = max(self.compute_root_size, self._inplace_out_size)
            return

        max_tensor_size = 0
        for tensor_i in self._op_compute_root_end_tensors | self._op_inplace_out_tensors:
            shape_list = util.shape_to_list(tensor_i.shape)
            shape_size = reduce(lambda x, y: x * y, shape_list or [1])
            max_tensor_size = shape_size if shape_size > max_tensor_size else max_tensor_size

        self.compute_root_size = max_tensor_size

    def _calc_compute_root_max_dtype(self):
        compute_root_max_byte = set(DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
                                    for tensor_i in self.dyn_compute_root_tensors)
        self.compute_root_max_byte = max(compute_root_max_byte)

    def _calc_compute_block_one_tensors(self):
        """
        calc tensors , calculated only in first core
        @return:
        """
        for out_i in self._op_inplace_out_tensors:
            self.block_one_out_tensors.add(out_i)

    def _calc_compute_root_unique(self):
        def dfs_unique_tensor(tensor, visited_tensors):
            for tensor_i in tensor.op.input_tensors:
                if tensor_i in visited_tensors:
                    continue

                if tensor_i in self.dyn_compute_root_tensors:
                    self.compute_root_unique_tensors.add(tensor_i)
                    continue
                visited_tensors.add(tensor_i)
                dfs_unique_tensor(tensor_i, visited_tensors)

        visited_tensors = set()
        for out_i in set(self._outs) - self.dyn_compute_root_tensors:
            if out_i in visited_tensors:
                continue
            visited_tensors.add(out_i)
            dfs_unique_tensor(out_i, visited_tensors)

    def _calc_compute_root_fake_node(self):
        if self.compute_root_unique_tensors:
            self.compute_root_fake_node = ScheduleUtil.fake_node(list(self.compute_root_unique_tensors),
                                                                 COMPUTE_ROOT_FAKE_NODE)
            self.dyn_compute_root_tensors.add(self.compute_root_fake_node)

    def _calc_compute_root_temp_ub_size(self):
        self.compute_root_tmp_ub_size += self.compute_root_max_coexisting_quantity * \
                                         self.compute_root_max_byte * \
                                         self.compute_root_size


class GraphAnalysisHelper:
    def __init__(self, outs):
        self._init_properties(outs)
        self._analysis_graph() if self._enable_graph_analysis else None

    def _init_properties(self, outs):
        self._outs = set(outs)
        self._broadcast_graph_outs = set()
        self._elewise_graph_outs = set()
        self._out_graph_tensors_map = dict()
        self._broadcast_graph_tensors = set()
        self._elewise_graph_tensors = set()
        self._in_out_map = dict()
        self._enable_graph_analysis = ScheduleUtil.is_mov_align_sch()
        self._middle_tensors = set()
        self._input_tensors = set()

        self.intersect_in_and_brc_out_map = dict()
        self.intersect_in_and_ele_out_map = dict()
        self.brc_ele_first_intersect_tensors = set()
        self.brc_ele_first_intersect_tensor_and_type_map = dict()

    def _analysis_graph(self):
        def _dfs_graph(out):
            for _tensor in out.op.input_tensors:
                if util.is_placeholder(_tensor):
                    self._input_tensors.add(_tensor)
                util.merge_value(self._in_out_map, _tensor, out)
                self._middle_tensors.add(_tensor)
                _dfs_graph(_tensor)

        def _calc_brc_ele_first_intersect(tensor):
            consumers = self._in_out_map.get(tensor, set())
            elewise_consumers = set()
            broadcast_comsumers = set()
            for _comsumer in consumers:
                if _comsumer in self._broadcast_graph_tensors:
                    broadcast_comsumers.add(_comsumer)
                elif _comsumer in self._elewise_graph_tensors:
                    elewise_consumers.add(_comsumer)
            if len(consumers) > len(elewise_consumers):
                self.brc_ele_first_intersect_tensors.add(tensor)
                self.brc_ele_first_intersect_tensor_and_type_map[tensor] = self._calc_intersect_tensor_type(tensor)
                util.merge_value(self.intersect_in_and_brc_out_map, tensor, list(broadcast_comsumers))
                util.merge_value(self.intersect_in_and_ele_out_map, tensor, list(elewise_consumers))
                return True
            return False

        def _dfs_brc_ele_graph(out):
            for tensor in out.op.input_tensors:
                if tensor in visited_tensors:
                    continue
                nonlocal is_elewise_graph
                is_elewise_graph = is_elewise_graph and ScheduleUtil.is_elewise_or_brc_inline(tensor)
                cur_out_graph_tensors.add(tensor)
                visited_tensors.add(tensor)
                _dfs_brc_ele_graph(tensor)

        def _dfs_intersect_tensor(tensor):
            for _tensor in tensor.op.input_tensors:
                if _calc_brc_ele_first_intersect(_tensor):
                    continue
                _dfs_intersect_tensor(_tensor)

        # build in_out_map, middle tensors
        for out in self._outs:
            _dfs_graph(out)

        for out in self._outs - self._middle_tensors:
            cur_out_graph_tensors = {out}
            visited_tensors = {out}
            is_elewise_graph = ScheduleUtil.is_elewise_or_brc_inline(out)
            _dfs_brc_ele_graph(out)
            self._elewise_graph_outs.add(out) if is_elewise_graph else self._broadcast_graph_outs.add(out)
            if is_elewise_graph:
                self._elewise_graph_tensors.update(cur_out_graph_tensors)
            else:
                self._broadcast_graph_tensors.update(cur_out_graph_tensors)

            self._out_graph_tensors_map[out] = cur_out_graph_tensors

        for elewise_out in self._elewise_graph_outs:
            _dfs_intersect_tensor(elewise_out)

    def _calc_intersect_tensor_type(self, tensor):
        if tensor in self._input_tensors:
            return BRC_ELE_INTERSECT_INPUT
        elif tensor in self._middle_tensors and tensor in self._outs:
            return BRC_ELE_INTERSECT_MIDDLE_OUT
        else:
            return BRC_ELE_INTERSECT_PURE_MIDDLE


class CacheCloneHelper:

    def __init__(self, outs, is_const_mode):

        self._outs = outs
        self._enable_cache_clone = False
        self._inplace_out = None
        self._inplace_coexisting_quantity = None
        self._inplace_max_dtype_bytes = 4
        self._inplace_graph_has_brc = False
        self._is_const_mode = is_const_mode

        self._inplace_outs = list()
        self._cache_clone_tensors = list()
        self._cache_clone_start_tensors = list()
        self._cache_clone_middle_tensors = list()
        self._cache_clone_input_tensors = list()

        self._inplace_out_indexes = set()
        self._inplace_graph_out_indexes = set()
        self._inplace_graph_tensors = set()
        self._inplace_input_tensors = set()
        self._inplace_graph_out_tensors = set()
        self._inplace_middle_tensors = set()
        self._inplace_graph_dtypes = set()
        self._pure_inplace_graph_tensors = set()
        self._original_graph_inputs = set()
        self._original_graph_middle_tensors = set()
        self._original_middle_out_inplace_pure_out_tensors = set()

        self._inplace_graph_in_out_map = {}
        self._original_graph_in_out_map = {}
        self._cache_clone_producer_consumer_map = {}

        self._calc_inplace_outs()
        if self._enable_cache_clone:
            self._build_original_and_inplace_graph()
            self._calc_cache_clone_start_tensor()
            self._calc_cache_clone_producer_consumer_map()
            self._calc_inplace_fake_node()
            self._calc_inplace_graph_coexisting_quantity()

    @property
    def cache_clone_middle_tensors(self):
        return self._cache_clone_middle_tensors

    @property
    def cache_clone_input_tensors(self):
        return self._cache_clone_input_tensors

    @property
    def inplace_graph_tensors(self):
        return self._inplace_graph_tensors

    @property
    def inplace_graph_in_out_map(self):
        return self._inplace_graph_in_out_map

    @property
    def inplace_outs(self):
        return self._inplace_outs

    @property
    def inplace_coexisting_quantity(self):
        return self._inplace_coexisting_quantity

    @property
    def inplace_max_dtype_bytes(self):
        return self._inplace_max_dtype_bytes

    @property
    def inplace_out_indexes(self):
        return self._inplace_out_indexes

    @property
    def inplace_middle_tensors(self):
        return self._inplace_middle_tensors

    @property
    def inplace_input_tensors(self):
        return self._inplace_input_tensors

    @property
    def inplace_graph_out_tensors(self):
        return self._inplace_graph_out_tensors

    @property
    def pure_inplace_graph_tensors(self):
        return self._pure_inplace_graph_tensors

    @property
    def enable_cache_clone(self):
        return self._enable_cache_clone

    @property
    def inplace_out(self):
        return self._inplace_out

    @property
    def cache_clone_tensors(self):
        return self._cache_clone_tensors

    @property
    def cache_clone_producer_consumer_map(self):
        return self._cache_clone_producer_consumer_map

    @property
    def inplace_graph_out_indexes(self):
        return self._inplace_graph_out_indexes

    @property
    def inplace_out_volume(self):
        return self._inplace_out_volume

    def _calc_inplace_outs(self):
        """
        enable cache clone:
        1. out  tensor contains 'inplace_input' attr
        2. inplace graph not include tensor broadcast
        """
        def _dfs_graph(out):
            for _tensor in out.op.input_tensors:
                if util.is_broadcast(_tensor):
                    self._inplace_graph_has_brc = True

        inplace_out_volumes = set()
        is_inplace_out_volume_unknown = False
        is_inplace_out_shape_size_bigger_than_volume = False
        is_inplace_out_shape_size_unknown = not self._is_const_mode
        for _out_idx, _out in enumerate(self._outs):
            if INPLACE_INPUT_STR_KEY in _out.op.attrs:
                if TENSOR_VOLUME_STR_KEY not in _out.op.attrs:
                    is_inplace_out_volume_unknown = is_inplace_out_volume_unknown | True
                else:
                    inplace_out_op_volume = int(_out.op.attrs[TENSOR_VOLUME_STR_KEY])
                    inplace_out_volumes.add(inplace_out_op_volume)
                    inplace_out_shape_size = \
                        - 1 if is_inplace_out_shape_size_unknown else reduce(mul, util.shape_to_list(_out.shape), 1)
                    is_inplace_out_shape_size_bigger_than_volume = is_inplace_out_shape_size_bigger_than_volume | (
                                inplace_out_shape_size > inplace_out_op_volume)
                self._inplace_outs.append(_out)
                self._inplace_out_indexes.add(_out_idx)
                if util.is_broadcast(_out):
                    self._inplace_graph_has_brc = True
                _dfs_graph(_out)

        # inplace out shape size is unknown
        # inplace out shape size bigger than op given tensor volume in const mode
        # inplace out volume from op compute is unknown
        is_inplace_out_size_satisfied = is_inplace_out_volume_unknown or \
                                        is_inplace_out_shape_size_bigger_than_volume or \
                                        is_inplace_out_shape_size_unknown
        self._inplace_out_volume = max(inplace_out_volumes) if len(inplace_out_volumes) > 0 else 0
        self._enable_cache_clone = len(self._inplace_outs) > 0 and \
                                   not self._inplace_graph_has_brc and \
                                   is_inplace_out_size_satisfied

    def _build_original_and_inplace_graph(self):
        """
        calc original and inplace graph producer and consumer map
        """
        def _dfs_original_graph(out):
            for _tensor in out.op.input_tensors:
                if util.is_placeholder(_tensor):
                    self._original_graph_inputs.add(_tensor)
                else:
                    self._original_graph_middle_tensors.add(_tensor)
                util.merge_value(self._original_graph_in_out_map, _tensor, out)
                _dfs_original_graph(_tensor)

        def _dfs_inplace_graph(out):
            for _tensor in out.op.input_tensors:
                if util.is_placeholder(_tensor):
                    self._inplace_input_tensors.add(_tensor)
                else:
                    self._inplace_middle_tensors.add(_tensor)

                if _tensor in self._outs:
                    self._inplace_graph_out_tensors.add(_tensor)
                    self._inplace_graph_out_indexes.add(self._outs.index(_tensor))

                util.merge_value(self._inplace_graph_in_out_map, _tensor, out)
                self._inplace_graph_tensors.add(_tensor)
                self._inplace_graph_dtypes.add(_tensor.dtype)
                _dfs_inplace_graph(_tensor)

        # build original graph
        for _out in self._outs:
            _dfs_original_graph(_out)

        # build inplace graph
        for _out in self._inplace_outs:
            self._inplace_graph_dtypes.add(_out.dtype)
            self._inplace_graph_tensors.add(_out)
            self._inplace_graph_out_indexes.add(self._outs.index(_out))
            self._inplace_graph_out_tensors.add(_out)
            _dfs_inplace_graph(_out)

        original_middle_out_tensors = self._original_graph_middle_tensors.intersection(set(self._outs))
        self._original_middle_out_inplace_pure_out_tensors = \
            set(_tensor for _tensor in original_middle_out_tensors if _tensor not in self._inplace_middle_tensors)

    def _calc_cache_clone_start_tensor(self):
        """
        1. cache clone should be done from bottom to top of the graph
        2. the start tensor of cache_clone is first intersection of inplace graph and original graph
        """
        def _dfs_inplace_and_ori_intersect_tensor(out):
            consumers = self._original_graph_in_out_map.get(out)
            # first tensor used by inplace graph and original graph
            if not consumers.issubset(self._inplace_graph_tensors):
                self._cache_clone_start_tensors.append(out)
                return

            for _tensor in out.op.input_tensors:
                _dfs_inplace_and_ori_intersect_tensor(_tensor)

        for _out in self._inplace_outs:
            if _out in self._original_graph_middle_tensors:
                _dfs_inplace_and_ori_intersect_tensor(_out)
            else:
                for _tensor in _out.op.input_tensors:
                    _dfs_inplace_and_ori_intersect_tensor(_tensor)

        # bottommost tensor should do cache_clone first
        self._cache_clone_start_tensors.sort(key=functools.cmp_to_key(GraphUtil.calc_tensor_dependency))

    def _calc_cache_clone_producer_consumer_map(self):
        """
        1. tensor do cache clone for original tensors
        2. cache clone tensor consumer should not contain pure inplace graph tensors
        """

        def _dfs_cache_clone_consumer_producer(out):
            for tensor_i in out.op.input_tensors:
                if tensor_i not in self._cache_clone_tensors:
                    self._cache_clone_tensors.append(tensor_i)
                util.merge_value(self._cache_clone_producer_consumer_map, tensor_i, out)
                _dfs_cache_clone_consumer_producer(tensor_i)

        for _tensor in self._cache_clone_start_tensors:
            if _tensor not in self._cache_clone_tensors:
                self._cache_clone_tensors.append(_tensor)

            if _tensor in self._cache_clone_producer_consumer_map:
                pre_consumer = self._cache_clone_producer_consumer_map.get(_tensor)
                pure_ori_graph_consumer = self._original_graph_in_out_map.get(_tensor) - \
                                          self._inplace_graph_in_out_map.get(_tensor)
                self._cache_clone_producer_consumer_map[_tensor] = pre_consumer.union(pure_ori_graph_consumer)
            else:
                self._cache_clone_producer_consumer_map[_tensor] = \
                    self._original_graph_in_out_map.get(_tensor) - self._inplace_graph_in_out_map.get(_tensor)

            _dfs_cache_clone_consumer_producer(_tensor)

        self._cache_clone_tensors.sort(key=functools.cmp_to_key(GraphUtil.calc_tensor_dependency))
        self._cache_clone_middle_tensors = \
            [_tensor for _tensor in self.cache_clone_tensors if _tensor not in self._original_graph_inputs]
        self._cache_clone_input_tensors = \
            [_tensor for _tensor in self.cache_clone_tensors if _tensor in self._original_graph_inputs]

    def _calc_inplace_fake_node(self):
        if len(self._inplace_outs) == 1:
            self._inplace_out = self._inplace_outs[0]
        else:
            self._inplace_out = ScheduleUtil.fake_node(self._inplace_outs, INPLACE_FAKE_NODE)

        for _tensor in self._inplace_outs:
            self._inplace_graph_in_out_map[_tensor] = {self._inplace_out}

    def _calc_inplace_graph_coexisting_quantity(self):
        self._inplace_coexisting_quantity = GraphUtil.max_live_node(self._inplace_out,
                                                                    self._inplace_graph_in_out_map, True)


class UnifyGraphTensor:
    def __init__(self, ub_tensor):
        self._ub_tensor = ub_tensor
        self._gm_tensor = None

        # original graph, inplace sub graph
        self._graph_type = None

        # input, pure_middle, middle_out, pure_out_tensor, fake_node_tensor
        self._tensor_type = None

        # cache_read, cache_write, set_scope
        self._ub_mode = None

    def __repr__(self):
        return 'ub_tensor: %s, gm_tensor: %s, graph_type: %d, tensor_type: %d, ub_mode: %d' %\
               (str(self.ub_tensor), str(self.gm_tensor), self.graph_type, self.tensor_type, self.ub_mode)

    @property
    def ub_tensor(self):
        return self._ub_tensor

    @property
    def gm_tensor(self):
        return self._gm_tensor

    @property
    def graph_type(self):
        return self._graph_type

    @property
    def tensor_type(self):
        return self._tensor_type

    @property
    def ub_mode(self):
        return self._ub_mode

    @gm_tensor.setter
    def gm_tensor(self, value):
        self._gm_tensor = value

    @graph_type.setter
    def graph_type(self, value):
        self._graph_type = value

    @tensor_type.setter
    def tensor_type(self, value):
        self._tensor_type = value

    @ub_mode.setter
    def ub_mode(self, value):
        self._ub_mode = value

    def build_tensor(self, gm_tensor, graph_type, tensor_type, ub_mode):
        self._gm_tensor = gm_tensor
        self._graph_type = graph_type
        self._tensor_type = tensor_type
        self._ub_mode = ub_mode

