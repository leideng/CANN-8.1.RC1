#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
pooling grad with arg info
"""

from typing import Any
from typing import Callable
from typing import Dict
from typing import Iterable
from typing import List
from typing import Optional
from typing import Set
from typing import Tuple
from typing import Union
from tbe import tvm
from tbe.common.platform.platform_info import get_soc_spec
from tbe.dsl.base.operation import get_context
from tbe.tvm.tir import IntImm
from tbe.tvm import Var
from tbe.tvm import PlaceholderOp
from tbe.tvm import Tensor

from .pooling_grad_helper import PoolingGradConstants
from ... import util
from ...constants import DTYPE_BYTE_MAPPING


class PoolingGradGraphInfo:
    """
    operation Compute Graph Info collector and container
    """

    def __init__(self, output_tensors: Iterable[Tensor]):
        """
        initialize and collect graph info
        """
        self.output_tensor_set: Optional[Set[Tensor]] = None
        # real_output_tensor_set: output set which doesn't contain fake_node
        self.real_output_tensor_set: Optional[Set[Tensor]] = None
        # real_pure_output_tensor_set: output set which doesn't contain fake_node and middle output
        self.real_pure_output_tensor_set: Optional[Set[Tensor]] = None
        self.tensor_consumers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_producers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_list: Optional[List[Tensor]] = None
        self.reduce_window_tensor_set: Set[Tensor] = set()
        self.input_tensor_set: Set[Tensor] = set()
        self.assist_tensor_set: Set[Tensor] = set()
        self.non_gm_input_tensor_set: Set[Tensor] = set()
        self.mid_output_tensor_set: Set[Tensor] = set()
        self.mid_tensor_set: Set[Tensor] = set()
        self.endpoint_output_tensor: Tensor = None
        self.max_type: Optional[str] = None
        self.min_type: Optional[str] = None

        self.collect_info(output_tensors)
        self.gen_real_output_tensor_sets()
        self.fake_node()
        self.get_min_and_max_dtype()

    @staticmethod
    def dfs_compute_graph(root_tensor: Union[Iterable[Tensor], Tensor],
                          hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                             Callable[[Tensor], Any],
                                             Callable[[Tensor], Any]], ...]):
        """
        compute graph using dfs algorithm
        """
        def recursive_func(_root_tensor: Tensor,
                           _visited_list: Set[Tensor],
                           _tensor_consumers_map: Dict[Tensor, Set[Tensor]],
                           _tensor_producers_map: Dict[Tensor, Set[Tensor]],
                           _hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                               Callable[[Tensor], Any],
                                               Callable[[Tensor], Any]], ...]):
            _visited_list.add(_root_tensor)
            _tensor_producers_map.setdefault(_root_tensor, set())
            _tensor_consumers_map.setdefault(_root_tensor, set())
            for hook in hooks:
                if hook[0](_root_tensor):
                    hook[1](_root_tensor)
                else:
                    hook[2](_root_tensor)
            for in_tensor in _root_tensor.op.input_tensors:
                _tensor_consumers_map.setdefault(in_tensor, set())
                _tensor_consumers_map.get(in_tensor).add(_root_tensor)
                _tensor_producers_map.get(_root_tensor).add(in_tensor)
                recursive_func(in_tensor,
                               _visited_list,
                               _tensor_consumers_map,
                               _tensor_producers_map,
                               _hooks)

        visited_list = set()
        tensor_consumers_map = {}
        tensor_producers_map = {}
        if isinstance(root_tensor, (list, tuple, set)):
            for tensor in root_tensor:
                recursive_func(tensor, visited_list,
                               tensor_consumers_map,
                               tensor_producers_map,
                               hooks)
        elif isinstance(root_tensor, Tensor):
            recursive_func(root_tensor, visited_list,
                           tensor_consumers_map,
                           tensor_producers_map,
                           hooks)
        return list(visited_list), tensor_consumers_map, tensor_producers_map

    def collect_info(self, output_tensors: Iterable[Tensor]):
        """
        collect necessary information
        """
        self.output_tensor_set = set(output_tensors)
        self.tensor_list, self.tensor_consumers_map, self.tensor_producers_map = \
            self.dfs_compute_graph(self.output_tensor_set,
                                   (
                                       # self.input_tensor_set hook
                                       (lambda _tensor: isinstance(_tensor.op, PlaceholderOp),
                                        lambda _tensor: self.input_tensor_set.add(
                                            _tensor),
                                        lambda _tensor: self.non_gm_input_tensor_set.add(
                                            _tensor)
                                        if not _tensor.op.input_tensors else None),
                                       # self.reduce_window_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find(
                                           PoolingGradConstants.REDUCE_WINDOW_TAG) != -1 or \
                                           _tensor.op.tag.find(
                                               PoolingGradConstants.REDUCE_SUM_TAG) != -1,
                                        lambda _tensor: self.reduce_window_tensor_set.add(
                                            _tensor),
                                        lambda _tensor: None)
                                   ))
        self.gen_mid_tensor_set()
        self.gen_endpoint_output_tensor()

    def gen_endpoint_output_tensor(self):
        """
        get endpoint tensor
        """
        for output_tensor in self.output_tensor_set:
            if not self.tensor_consumers_map.get(output_tensor):
                self.endpoint_output_tensor = output_tensor
                break

    def gen_mid_tensor_set(self):
        """
        gen mid tensor: mid output tensor and mid tensor
        """
        for tensor in self.tensor_list:
            if tensor in self.output_tensor_set and self.tensor_consumers_map.get(tensor):
                self.mid_output_tensor_set.add(tensor)
                self.mid_tensor_set.add(tensor)
            elif tensor not in self.output_tensor_set | self.input_tensor_set:
                self.mid_tensor_set.add(tensor)

    def gen_real_output_tensor_sets(self):
        """
        gen real output tensor sets
        """
        self.real_output_tensor_set = self.output_tensor_set
        self.real_pure_output_tensor_set = self.real_output_tensor_set - \
            self.mid_output_tensor_set

    def fake_node(self):
        """
        add fake node
        """
        def _fake_output(src1, src2, depad_shape, pads):
            def _add_pad(indices, src, pad_value, depad_shape, pads):
                d_in, h_in, w_in = depad_shape[1], depad_shape[3], depad_shape[4]
                p_0, p_1, p_2, p_3, p_4, p_5 = pads
                _n, _di, _c1, _hi, _wi, _c0 = indices

                _in_n = _n
                _in_di = _di - p_0
                _in_c1 = _c1
                _in_hi = _hi - p_2
                _in_wi = _wi - p_4
                _in_c0 = _c0

                cond = tvm.any(_di < p_0, _di > d_in + p_0 - 1,
                               _hi < p_2, _hi > h_in + p_2 - 1,
                               _wi < p_4, _wi > w_in + p_4 - 1,)
                return tvm.select(cond, pad_value, src[_in_n, _in_di, _in_c1, _in_hi, _in_wi, _in_c0])
            dst_shape = src1.shape
            res = tvm.compute(dst_shape, lambda * i: src1(*i) + _add_pad(i, src2, tvm.const(
                0.0, dtype="float16"), depad_shape, pads).astype("float32"), name="fake_node", tag="fake_node")
            return res
        current_compute = get_context().get_current_compute()
        if current_compute:
            self.pad_info = current_compute.get("_pad_info")
            self.ksize_info = current_compute.get("_kernel_info")
            self.strides_info = current_compute.get("_strides_info")
        if current_compute.get("_format_info") == 0:
            res_init_shape = list(self.output_tensor_set)[
                0].op.input_tensors[0].shape
            res_init_fp32 = tvm.compute(
                res_init_shape, lambda * i: tvm.const(0.0, dtype="float32"), name="res_init_fp32")
            real_output_tensor = list(self.output_tensor_set)[0]
            dummy_node = _fake_output(
                res_init_fp32, real_output_tensor, real_output_tensor.shape, self.pad_info)

            self.assist_tensor_set.add(res_init_fp32)
            self.fake_output_node = dummy_node
        else:
            fakenode_shape = list(self.output_tensor_set)[0].op.input_tensors[0].shape
            self.fake_output_node = tvm.compute(fakenode_shape, 
                                                lambda *i: list(self.output_tensor_set)[0].op.input_tensors[0][i], 
                                                name="fakenode")

    def get_min_and_max_dtype(self):
        """
        get min dtype and max dtype
        """
        self.max_type = self.tensor_list[0].dtype
        self.min_type = self.tensor_list[0].dtype

        for item in self.tensor_list:
            if DTYPE_BYTE_MAPPING.get(item.dtype) > DTYPE_BYTE_MAPPING.get(self.max_type):
                self.max_type = item.dtype
            elif DTYPE_BYTE_MAPPING.get(item.dtype) < DTYPE_BYTE_MAPPING.get(self.min_type):
                self.min_type = item.dtype


class PoolingGradBaseInfo:
    """
    class for pooling grad base info
    """

    def __init__(self, graph_info: PoolingGradGraphInfo):
        """
        init base info
        """
        self.graph_info: PoolingGradGraphInfo = graph_info
        self.reduce_window_tensor: Tensor = tuple(
            graph_info.reduce_window_tensor_set)[0]
        self.window_axes: List[Var] = util.get_reduce_axes(
            self.reduce_window_tensor)
        if graph_info.fake_output_node is not None:
            self.res_tensor: Tensor = graph_info.fake_output_node
        else:
            self.res_tensor: Tensor = list(graph_info.output_tensor_set)[0]
        self.res_shape: List[Union[Var, IntImm]] = list(self.res_tensor.shape)

        self.strategy = None
        self.mode = None
        self.window_info = None
        self.pad_info = None
        self.get_info()

    def get_info(self):
        """
        get strategy, mode and window_info
        """
        current_compute = get_context().get_current_compute()
        if current_compute:
            self.strategy = current_compute.get("_strategy")
            self.mode = current_compute.get("_mode")
            self.pad_info = current_compute.get("_pad_info")
            self.window_info = current_compute.get("_window_info")


class PoolingGradComputeInfo:
    """
    class for pooling grad compute info
    """

    def __init__(self, output_tensors: Iterable[Tensor]):
        """
        init compute info
        """
        self.graph_info: PoolingGradGraphInfo = PoolingGradGraphInfo(
            output_tensors)
        self.base_info: PoolingGradBaseInfo = PoolingGradBaseInfo(
            self.graph_info)


class PoolingGradSocInfo:
    """
    class for pooling grad soc info
    """
    @classmethod
    def get_soc_version(cls):
        return get_soc_spec("SHORT_SOC_VERSION")

    @classmethod
    def get_core_num(cls):
        return get_soc_spec("CORE_NUM")

    @classmethod
    def get_ub_size(cls):
        return get_soc_spec("UB_SIZE")

    @classmethod
    def get_block_size(cls, dtype):
        return 32 // DTYPE_BYTE_MAPPING.get(dtype)
