#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

import functools

from itertools import product
from tbe.common.utils import decode
from tbe.common.utils import do_op_tiling
from tbe.common.utils import log
from tbe.common.utils.errormgr import error_manager_cube
from tbe.dsl.compute.util import shape_to_list
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import register_tiling_case
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.static_schedule.util import get_all_tensor
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.cube_tilingcase import TilingUtils as utils

OFFSET_ONE = 1
BATCH_MATMUL_LEN_NZ = 5


def is_static(placeholder_tensor):
    for tensor in placeholder_tensor.values():
        tensor_shape = shape_to_list(tensor.shape)
        if not all(isinstance(s, int) for s in tensor_shape):
            return False
    return True


@register_tiling_case(pattern=Pattern.MATMUL_UB_TO_UB)
def calc_ub_matmul(outs, option=None):
    """
    tiling_case func for dynamic shape matmul

    Parameters
    ----------
    outs: tvm tensor or list of tvm tensor, results for tvm compute

    Returns
    -------
    list of dict, each dict for a tiling case
    """
    tensor_matmul = TensorMatMul(outs)
    tensor_list, param_map = tensor_matmul.get_tensor_and_param_map()
    tiling_matmul = TilingMatMul(param_map)
    tiling_cases = []
    if param_map.get("is_static"):
        inputs, outputs = tensor_matmul.get_input_output_para_list()
        attrs = tensor_matmul.get_attrs()
        tiling_cases = tiling_matmul.get_tilingcase_static(inputs, outputs, attrs)
    else:
        tiling_cases = tiling_matmul.get_cache_tiling()
    for tiling_case in tiling_cases:
        tiling_case["tensor_list"] = tensor_list
        tiling_case["param_map"] = param_map
    return tiling_cases


class TensorMatMul():
    """
    use to get tensor for matmul
    """
    def __init__(self, res_list):
        self.res_list = res_list
        self.is_batch_matmul = False
        self.param_map = {}
        self.compute_tensor = {}
        self.placeholder_tensor = []
        self.tensor_dst_node = {}
        self.batch = 1

    def get_fifo_fusion_flag(self):
        # 1 means input_fifo, 2 means weight_fifo,
        # 3 means input_fifo not align, 4 means weight_fifo not align, 0 means no fifo
        if self.compute_tensor.get("fwc_res") is None:
            return 0
        tensor_a = self.compute_tensor.get("tensor_a_nz")
        if self.param_map.get("trans_a"):
            tensor_a = self.compute_tensor.get("tensor_a_ub")
        if tensor_a.op.input_tensors:
            if tensor_a.op.input_tensors[0].op.name in ["fwc_res"]:
                if self.compute_tensor.get("x_reshape_tensor") is not None:
                    # 3 means input_fifo not align
                    return 3
                # 1 means input_fifo align
                return 1
        tensor_b = self.compute_tensor.get("tensor_b_ub")
        if self.param_map.get("trans_b"):
            tensor_b = self.compute_tensor.get("tensor_b_zn")
        if tensor_b.op.input_tensors:
            if tensor_b.op.input_tensors[0].op.name in ["fwc_res"]:
                if self.compute_tensor.get("x_reshape_tensor") is not None:
                    # 4 means weight_fifo not align
                    return 4
                # 2 means weight_fifo align
                return 2
        return 0

    def get_tensor_and_param_map(self):
        self.compute_tensor, self.placeholder_tensor, self.tensor_dst_node = get_all_tensor(self.res_list[-1])
        self.param_map["is_static"] = is_static(self.placeholder_tensor)
        self.param_map["trans_a"] = self.compute_tensor.get("tensor_a_nz").op.attrs["trans_a"]
        self.param_map["trans_b"] = self.compute_tensor.get("tensor_b_zn").op.attrs["trans_b"]
        self.param_map["fifo_fusion_flag"] = self.get_fifo_fusion_flag()
        fixpipe_op = self.compute_tensor.get("fixpipe", None)
        if fixpipe_op is not None:
            self.param_map["fixpipe_vector_params"] = fixpipe_op.op.attrs["vector_params"]
            self.param_map["fixpipe_vector_tensors"] = fixpipe_op.op.attrs["vector_tensors"]
            self.param_map["fixpipe_nz2nd_flag"] = fixpipe_op.op.attrs["nz2nd_flag"]
            self.param_map["fixpipe_anti_quant_flag"] = fixpipe_op.op.attrs["nz2nd_flag"]
            self.param_map["fixpipe_op_dict"] = fixpipe_op.op.attrs["op_dict"]
        matmul_op = self.compute_tensor.get("matmul_op")
        self.is_batch_matmul = len(matmul_op.shape) == BATCH_MATMUL_LEN_NZ
        if self.is_batch_matmul:
            matmul_shape = shape_to_list(matmul_op.shape)
            # format_nz batch include [:-4]
            self.param_map["batch"] = functools.reduce(lambda x, y: x * y, matmul_shape[:-4])
        block_dict = {"block_m0": int(matmul_op.op.attrs["block_m0"]),
                      "block_n0": int(matmul_op.op.attrs["block_n0"]),
                      "block_a_k0": int(matmul_op.op.attrs["block_a_k0"]),
                      "block_b_k0": int(matmul_op.op.attrs["block_b_k0"])}
        self.param_map["bm_fusion_flag"] = matmul_op.op.attrs["bm_fusion_flag"]
        self.param_map["block_dict"] = block_dict
        self.param_map["op_type"] = "MatMulV2" if not self.is_batch_matmul else "BatchMatMulV2"
        self.param_map["a_dtype"] = self.compute_tensor.get("tensor_a_nz").dtype
        self.param_map["b_dtype"] = self.compute_tensor.get("tensor_b_zn").dtype
        self.param_map["bias_flag"] = self.compute_tensor.get("tensor_bias_align", None) is not None
        return [self.compute_tensor, self.placeholder_tensor], self.param_map

    def get_input_output_para_list(self):
        inputs = []
        for tensor in self.placeholder_tensor.values():
            tensor_shape = shape_to_list(tensor.shape)
            tensor_ori_shape = shape_to_list(tensor.op.attrs["ori_shape"])
            if self.param_map["bm_fusion_flag"] and tensor.op.name == "params_0":
                new_m_dim = functools.reduce(lambda x, y: x * y, tensor_ori_shape[:-1])
                # -4 means k1, -2 means m0, n0
                tensor_shape = [1, tensor_shape[-4], new_m_dim, *tensor_shape[-2:]]
                tensor_ori_shape = [1, new_m_dim, tensor_ori_shape[-1]]
            tensor_para = {"shape": tensor_shape,
                           "ori_shape": tensor_ori_shape,
                           "dtype": tensor.dtype,
                           "format":tensor.op.attrs["format"]}
            inputs.append(tensor_para)
        outputs = []
        if not self.param_map.get("fifo_fusion_flag") == 0:
            tensor = self.compute_tensor.get("fwc_res")
            tensor_shape = shape_to_list(tensor.shape)
            tensor_ori_shape = shape_to_list(tensor.op.attrs["ori_shape"])
            # te_fusion use name "format_" when tensor is output
            tensor_format = tensor.op.attrs["format_"] if tensor.op.attrs.get("format_") else \
                            tensor.op.attrs.get("format")
            tensor_para = {"shape": tensor_shape,
                           "ori_shape": tensor_ori_shape,
                           "dtype": tensor.dtype,
                           "format": tensor_format}
            outputs.append(tensor_para)
        for tensor in self.res_list:
            tensor_shape = shape_to_list(tensor.shape)
            tensor_ori_shape = shape_to_list(tensor.op.attrs["ori_shape"])
            if self.param_map["bm_fusion_flag"]:
                new_m_dim = functools.reduce(lambda x, y: x * y, tensor_ori_shape[:-1])
                # -4 means k1, -2 means m0, n0
                tensor_shape = [1, tensor_shape[-4], new_m_dim, *tensor_shape[-2:]]
                tensor_ori_shape = [1, new_m_dim, tensor_ori_shape[-1]]
            tensor_para = {"shape": tensor_shape,
                           "ori_shape": tensor_ori_shape,
                           "dtype": tensor.dtype,
                           "format":tensor.op.attrs["format"]}
            outputs.append(tensor_para)
        return inputs, outputs

    def get_attrs(self):
        if self.param_map.get("op_type") in ("MatMulV2", "MatMul"):
            attrs = ({"name": "transpose_x1", "dtype": "bool", "value": bool(self.param_map.get("trans_a"))},
                    {"name": "transpose_x2", "dtype": "bool", "value": bool(self.param_map.get("trans_b"))})
        else:
            attrs = ({"name": "adj_x1", "dtype": "bool", "value": bool(self.param_map.get("trans_a"))},
                    {"name": "adj_x2", "dtype": "bool", "value": bool(self.param_map.get("trans_b"))})
        return attrs


class TilingMatMul():

    def __init__(self, param_map):
        self.param_map = param_map
        self.batch = self.param_map.get("batch", 1)
        self.block_m0 = self.param_map.get("block_dict").get("block_m0")
        self.block_n0 = self.param_map.get("block_dict").get("block_n0")

    @staticmethod
    def _valid_template(choice):
        """
        check the template is valid
        """
        _, _, _, k_aub_full_load, k_bub_full_load, a_b_kub_compare, reorder_mn_flag = choice
        # k_aub full load, a_b_kub_compare must be 1
        template_invalid = (k_aub_full_load == 1 and a_b_kub_compare == 0)
        template_invalid = template_invalid or (k_aub_full_load == 0 and k_bub_full_load == 1
                                                and a_b_kub_compare == 1)
        # the order of n, m axis default to n, m
        template_invalid = template_invalid or (k_aub_full_load == 0 and k_bub_full_load == 0
                                                and reorder_mn_flag == 1)
        # make sure there is no repeat loading for k_axis full load tensor
        template_invalid = template_invalid or (k_aub_full_load == 1 and
                                                k_bub_full_load == 0 and reorder_mn_flag == 0)
        template_invalid = template_invalid or (k_aub_full_load == 0 and
                                                k_bub_full_load == 1 and reorder_mn_flag == 1)
        return not template_invalid

    @staticmethod
    def _get_tiling_id(choice):
        """
        calculate tiling_id according template choice
        """
        (a_pb, b_pb, c_pb, k_aub_full_load, k_bub_full_load, a_b_kub_compare, reorder_mn_flag) = choice
        tiling_id = 0
        tiling_id = (tiling_id << OFFSET_ONE) + (a_pb - 1)
        tiling_id = (tiling_id << OFFSET_ONE) + (b_pb - 1)
        tiling_id = (tiling_id << OFFSET_ONE) + (c_pb - 1)
        tiling_id = (tiling_id << OFFSET_ONE) + k_aub_full_load
        tiling_id = (tiling_id << OFFSET_ONE) + k_bub_full_load
        tiling_id = (tiling_id << OFFSET_ONE) + a_b_kub_compare
        tiling_id = (tiling_id << OFFSET_ONE) + reorder_mn_flag
        return tiling_id

    @staticmethod
    def _set_tiling_value(cache_tiling, tiling_data):
        """
        set cache_tiling value for static scene
        """
        cache_tiling["block_dim"] = [1, tiling_data.get("block_n"), tiling_data.get("block_m")]
        cache_tiling["aub_matrix"][0:2] = [tiling_data.get("k_aub"), tiling_data.get("m_ub")]
        cache_tiling["bub_matrix"][0:2] = [tiling_data.get("k_bub"), tiling_data.get("n_ub")]
        cache_tiling["cub_matrix"][0:2] = [tiling_data.get("n_mmad"), tiling_data.get("m_mmad")]
        log.debug("Tiling from op_tiling is {}".format(cache_tiling))

    def get_tilingcase_static(self, inputs, outputs, attrs):
        tiling_cases = self.get_cache_tiling()
        run_info = self._get_op_tiling(inputs, outputs, attrs)
        tiling_key = run_info.get("tiling_key")
        new_tiling_cases = []
        for idx, _ in enumerate(tiling_cases):
            if tiling_cases[idx]["key"] == tiling_key:
                self._set_tiling_value(tiling_cases[idx]["tiling_strategy"], run_info.get("tiling_data"))
                new_tiling_cases.append(tiling_cases[idx])
        tiling_cases = new_tiling_cases
        if new_tiling_cases == []:
            error_manager_cube(f"invalid tiling_id {tiling_key}")
        return tiling_cases

    def get_cache_tiling(self):
        """
        get all tiling template of matmul
        ---------------------------------
        cache_tiling variable:
        block_dim: [block_batch, block_n, block_m]
        aub_matrix: [k_aub which unit is 1, m1_aub which unit is m0, m0, aub_batch]
        bub_matrix: [k_bub which unit is 1, n1_bub which unit is n0, n0, bub_batch]
        cub_matrix: [n_mmad which unit is n0, m_mmad which unit is m0, m0, n0, cub_batch]

        return list of tilingcase
        """
        add_compile_info("dynamic_mode", "dynamic_mkn")
        add_compile_info("binary_mode_flag", True)
        add_compile_info("binary_attrs", {"bias_flag": self.param_map.get("bias_flag"),
                                          "nd_flag": False,
                                          "split_k_flag": False,
                                          "zero_flag": False,
                                          "weight_nz": False})
        add_compile_info("block_dim", {"CORE_NUM":1})
        add_compile_info("block_dict", self.param_map.get("block_dict"))
        add_compile_info("bm_fusion_flag", bool(self.param_map.get("bm_fusion_flag")))
        add_compile_info("is_batch_matmul", self.param_map["op_type"] == "BatchMatMulV2")
        fifo_fusion_flag = "fifo_fusion_flag"
        if self.param_map.get(fifo_fusion_flag) is not None:
            add_compile_info(fifo_fusion_flag, self.param_map.get(fifo_fusion_flag))
        if self.param_map.get("fixpipe_op_dict", None) is not None:
            fixpipe_op_dict = {
                "pre_conv": str(self.param_map.get("fixpipe_op_dict").get("pre_conv")),
                "pre_activation": str(self.param_map.get("fixpipe_op_dict").get("pre_activation")),
                "post_anti_quant": str(self.param_map.get("fixpipe_op_dict").get("post_anti_quant")),
                "post_eltwise": str(self.param_map.get("fixpipe_op_dict").get("post_eltwise")),
                "post_activation": str(self.param_map.get("fixpipe_op_dict").get("post_activation")),
                "post_quant": str(self.param_map.get("fixpipe_op_dict").get("post_quant")),
                "post_transform": str(self.param_map.get("fixpipe_op_dict").get("post_transform"))
            }
            add_compile_info("fixpipe_op_dict", fixpipe_op_dict)
        # get cache_tiling
        tilingcases = []
        template_choices = self._get_attach_choices()
        for choice in template_choices:
            cache_tiling = {
                'block_dim': [1, 1, 1],
                'aub_matrix': [-1, -1, self.block_m0, 1],
                'bub_matrix': [-1, -1, self.block_n0, 1],
                'cub_matrix': [-1, -1, self.block_m0, self.block_n0, self.batch],
                'manual_pingpong_buffer': {'AUB_pbuffer': choice[0], 'BUB_pbuffer': choice[1],
                'CUB_pbuffer': choice[2]},
                'attach_at_flag': {"k_aub_full_load": choice[3], "k_bub_full_load": choice[4],
                                   "a_b_kub_compare": choice[5], "reorder_mn_flag": choice[6]}}
            one_tilingcase = {"key": self._get_tiling_id(choice),
                              "tiling_strategy": cache_tiling}
            tilingcases.append(one_tilingcase)
        return tilingcases

    def _get_op_tiling(self, inputs, outputs, attrs):
        run_info = do_op_tiling(self.param_map.get("op_type"), get_compile_info(), inputs, outputs, None, None, attrs)
        tiling_var = {"block_m": "int", "block_n": "int", "m_ub": "int", "n_ub": "int",
                      "k_aub": "int", "k_bub": "int", "k_mmad": "int", "m_mmad": "int", "n_mmad": "int"}
        tiling_data = decode(run_info.get("tiling_data"), tiling_var)
        run_info["tiling_data"] = tiling_data
        return run_info

    def _get_attach_choices(self):
        """
        generates all selections of l0 flags

        Returns
        -------
        list: all selections of flags
        """
        # Performance Mode 使用 0 模式描述FP32進FP32出 和FP16模式下的FP16進FP32出
        (a_pb, b_pb, c_pb, k_aub_full_load, k_bub_full_load, a_b_kub_compare, reorder_mn_flag) = (
            [utils.DB_ON, utils.DB_OFF], [utils.DB_ON, utils.DB_OFF], [utils.DB_ON, utils.DB_OFF],
            [utils.ABUB_FULL_LOAD - 1, utils.ABUB_NOT_FULL_LOAD],
            [utils.ABUB_FULL_LOAD - 1, utils.ABUB_NOT_FULL_LOAD],
            [1, 0], [1, 0])

        choice_list = [a_pb, b_pb, c_pb, k_aub_full_load, k_bub_full_load, a_b_kub_compare, reorder_mn_flag]
        template_choices = list(product(*choice_list))
        template_choices = list(filter(self._valid_template, template_choices))
        return template_choices