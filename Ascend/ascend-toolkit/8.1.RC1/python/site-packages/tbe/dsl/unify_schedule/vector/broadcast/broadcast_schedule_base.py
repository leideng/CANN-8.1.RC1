#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2019-2022. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
base broadcast schedule
"""
import copy
from typing import Optional
from functools import reduce
from operator import mul

from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from tbe.dsl.base.expr_compare import expr_equal
from tbe.dsl.base.operation import get_compile_info
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import ASCEND_310P
from tbe.common.platform import ASCEND_910
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform.platform_info import L2_SIZE
from tbe.common.utils import para_check

from tbe.dsl.padding import padding
from tbe.dsl.padding.padding import ActionValueType
from tbe.dsl.padding.padding import ActionType

from .broadcast_constants import PURE_BRC, NUM_FOUR, BLOCK_NUM, NUM_EIGHT, CONST_BRC_INLINE_LIMIT, \
    CONST_AHEAD_DB_FACTOR, CONST_DB_MIDDLE_NODES_LIMIT, BROADCAST, MAX_EXTEND_NODE_NUM, INT32_MAX, GM_MODE, \
    FAKE_NODE_TYPE, COMPUTE_ROOT_GRAPH_TENSOR, ORI_GRAPH_TENSOR, CACHE_CLONE_MODE, PURE_MIDDLE_TYPE, INPUT_TYPE, \
    MIDDLE_OUT_TYPE, PURE_OUT_TYPE, SET_SCOPE_MODE, CACHE_READ_MODE, INPLACE_GRAPH_TENSOR, CACHE_WRITE_MODE, \
    TILING_BASIC_FORMAT, TILING_BASIC_FORMAT_INT64, TILING_WITH_INPLACE_EXTRA_FORMAT_INT64, \
    TILING_WITH_INPLACE_EXTRA_FORMAT, TILING_DATA, BLOCK_DIM, NEED_TILING_CUT, IS_STORE_ALIGN, BLOCK_AXIS, \
    BLOCK_FACTOR, UB_AXIS, UB_FACTOR, IS_NEED_DB, INPLACE_BLOCK_AXIS, INPLACE_BLOCK_FACTOR, INPLACE_UB_AXIS, \
    INPLACE_UB_FACTOR, PHONY_INSN, DMA_COPY, ROW_LIMIT, VECTOR_BROADCAST, UNKNOWN_BROADCAST, INPLACE_BLOCK_FACTOR_PRE, \
    INPLACE_UB_FACTOR_PRE, BRC_LAST_DUP_LIMIT_VDUP_ALIGN, BRC_LAST_DUP_LIMIT, LAST_LOOP_REG_MOV, \
    MISSALIGN_STRIDE_WHITH_MALLOC_BUF, LAST_LOOP_COEXISTING_QUANTITY_LIMIT, \
    MISSALIGN_STRIDE_COEXISTING_QUANTITY_LIMIT, CONST_MODE, ENOUGH_BUFFER
from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import DTYPE_BRC_ALIGN_MAPPING
from ...constants import INSN_MAPPING
from ...constants import TERNARY_INSNS
from ..storage_bound_util import cast_ints2ints_need_alignment
from .broadcast_tilingcase import TilingStrategy
from .broadcast_tilingcase import BroadcastTilingCase
from .broadcast_helper import UnifyGraphTensor
from .broadcast_helper import GraphAnalysisHelper
from .broadcast_util import ScheduleUtil
from .broadcast_util import GraphUtil


# 'pylint: disable=R0902, R0903
class BaseBroadcastSchedule:
    """
    BaseBroadcastSchedule
    """

    class ComputeAt:
        """
        BaseBroadcast ComputeAt
        """

        def __init__(self):
            self._compute_at_axis = None

        @property
        def compute_at_axis(self):
            """
            return compute_at_axis
            """
            return self._compute_at_axis

        @compute_at_axis.setter
        def compute_at_axis(self, axis):
            self._compute_at_axis = axis

    class EmitInsn:
        """
        BaseBroadcast EmitInsn Bean
        """

        def __init__(self):
            self._emit_insn_axis = None

        @property
        def emit_insn_axis(self):
            """
            return emit_insn_axis
            """
            return self._emit_insn_axis

        @emit_insn_axis.setter
        def emit_insn_axis(self, axis):
            self._emit_insn_axis = axis

    def __init__(self, outs, tiling_case):
        # init compute graph variable
        self._init_compute_graph(outs)

        # init tiling variable
        self._init_tiling(tiling_case)

        # init schedule helper
        self.init_helper()

        #  init schedule variable
        self._init_schedule()

    def _init_compute_graph(self, outs):
        self._out: Optional[tvm.Tensor] = None
        self._outs = outs
        self._input_tensors = set()
        self._middle_tensors = set()
        self._pure_middle_tensors = set()
        self._middle_out_tensors = set()
        self._out_tensors = set()
        self._broadcast_tensors = set()
        self._one_shape_broadcast_tensors = set()
        self._absorbable_broadcast_tensors = set()
        self._before_broadcast_tensors = set()
        self._elewise_graph_tensors = set()
        self._broadcast_axis_num = {}
        self._dtypes = set()
        self._outs_dtypes = set()
        self._max_dtype_bytes = 4
        self._max_ele_in_block = 2
        self._max_brc_bytes = 2
        self._coexisting_quantity = 1
        self._in_out_map = {}
        self._cst_compute_root_tensors = set()
        self._cst_compute_root_last_brc_cut_last = False
        self._is_vnchwconv_align = True
        self._brc_avoid_bank_conflict = False
        self._pure_out_tensors = list()

    def _init_tiling(self, tiling_case):
        self._tiling_case: Optional[BroadcastTilingCase] = tiling_case
        self._tiling_strategy = self._tiling_case.tiling_strategy
        self._is_pure_brc = operation.get_context().get_current_compute().get("_mode") == PURE_BRC
        self._is_no_store_align = True
        self._is_store_align = self._tiling_case.is_storage_align
        self._need_do_block = False
        self._block_dims = 1
        self._block_split_axis = \
            -1 if self._tiling_case.block_split_axis is None else self._tiling_case.block_split_axis
        self._block_factor = 1
        self._ub_split_axis = 0 if self._tiling_case.ub_split_axis is None else self._tiling_case.ub_split_axis
        self._ub_factor = 1
        self._ub_factor_is_one = False
        self._block_tiling_vars = {}
        self._ub_tiling_vars = {}
        self._one_dim_align = util.get_ub_block_size() * NUM_FOUR
        self._ub_factor_align = self._one_dim_align
        self._one_repeat_bytes = util.get_ub_block_size() * BLOCK_NUM
        self._special_factor_align = util.get_ub_block_size() * NUM_EIGHT
        self._inplace_graph_block_factor = None
        self._inplace_graph_block_axis = self._tiling_case.inplace_block_axis
        self._inplace_graph_ub_factor = None
        self._inplace_graph_ub_axis = self._tiling_case.inplace_ub_axis
        self._inplace_emit_insn = self.EmitInsn()
        self._inplace_compute_at = self.ComputeAt()
        self._inplace_bind_axis = None

    def _init_schedule(self):
        self._schedule = None
        self._enable_db = self._tiling_case.enable_db
        self._is_one_dim = self._tiling_case.is_one_dim
        self._mode = operation.get_context().get_current_compute().get("_mode")
        self._is_pure_brc_common_db = self._is_pure_brc and self._enable_db and \
            (operation.get_context().get("_pure_brc_common") or False)
        self._scope = "local.UB"
        self._block_bind_axis = None
        self._unfold_brd_compute_at_axis = None
        self._unfold_broadcast_tensors = set()
        self._compute_at_axis = self.ComputeAt()
        self._compute_at_axis_idx = None
        self._emit_insn_axis = self.EmitInsn()
        self._tensor_space = None
        self._ub_size = util.get_ub_size()
        self._ub_block_size = util.get_ub_block_size()
        self._correct_factor = 2 if self._enable_db else 1
        self._tmp_ub_size = 0
        self._compute_root_tmp_ub_size = 0
        self._min_storage_bound = -1
        self._broadcast_by_no_other_use = {}
        self._cache_read_tensors = set()
        self._cache_read_buffer_tensor_map = {}
        self._gm_tensor_cache_read_buffer_map = {}
        self._gm_tensor_cache_read_buffers_map = {}
        self._middle_out_cache_read_buffer_map = {}
        self._middle_out_cache_read_buffers_map = {}
        self._gm_tensor_cache_read_brc_map = {}
        self._gm_tensor_cache_read_ele_map = {}
        self._cache_write_tensors = set()
        self._cache_write_buffer_tensor_map = {}
        self._gm_tensor_cache_write_buffer_map = {}
        self._middle_out_cache_write_buffer_map = {}
        self._gm_tensor_multi_cache_read_buffer_map = {}
        self._broadcast_store_predicate = set()
        self._store_predicate_common_tensors = set()
        self._all_pre_node_broadcast = set()
        self._compute_inline_tensors = set()
        self._const_brc_inline_tensor = set()
        self._store_align_tensors = set()
        self._before_broadcast_axis_group_tensors = set()
        self._compute_at_map = {}
        self._copy_out_tensors = set()
        self._remove_out_tensors = set()
        self._inner_shape = []
        self._constraints = set()
        self._mem_reuse_map = {}
        self._data_reuse_map = {}
        self._emit_insn_map = {}
        self._emit_insn_attr_map = {}
        self._compute_align_map = {}
        self._remove_pad_map = {}
        self._5hd_actions = set()
        self._set_value_cache_read_map = {}
        self._elewise_axis_group_tensors = set()
        self._block_var = tvm.thread_axis("blockIdx.x")
        self._inplace_block_var = tvm.thread_axis("blockIdx.x")
        self._remove_pad_cache_read_buffer = set()
        self._multi_cache_read_gm_tensors = set()
        self._cache_read_to_brc_tensors = set()
        self._cache_read_to_ele_tensors = set()
        self._cache_read_to_brc_and_consumer_map = dict()
        self._cache_read_to_ele_and_consumer_map = dict()
        self._enable_vdup_align = False
        self._cache_clone_buffers = set()
        self._gm_tensor_and_clone_buffer_map = {}
        self._enable_inplace_cache_clone = self._tiling_case.is_inplace_cache_clone_mode
        self._enable_compute_root = self._tiling_case.is_inplace_compute_root_mode
        self._dyn_compute_root_tensors = \
            self._compute_root_helper.dyn_compute_root_tensors if self._enable_compute_root else set()
        self._only_has_tensor_volume = self._tiling_case.only_has_tensor_volume
        self._ori_graph_unify_tensors = set()
        self._inplace_graph_unify_tensors = set()

    def init_helper(self):
        self._cache_clone_helper = self._tiling_case.cache_clone_helper
        self._compute_root_helper = self._tiling_case.compute_root_helper
        self._graph_analyzier = GraphAnalysisHelper(self._outs)

    def do_schedule(self):
        """
        :return:
        """
        self._construct_compute_graph()

        self._schedule = self._create_schedule()
        self._schedule.tiling_key = self._tiling_case.tiling_key

        self._do_cache_clone()

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()
        self._set_scope()

        self._calc_unify_remove_pad()
        self._calc_set_value()
        self._calc_unify_tiling()
        self._calc_unify_graph_tensors()
        self._calc_compute_inline()
        self._calc_storage_align()
        self._calc_multi_core()
        self._calc_ub_align()
        self._calc_compute_at()
        self._calc_double_buffer()
        self._calc_mem_reuse()
        self._calc_constraints()
        self._calc_emit_insn()
        self._calc_axis_group()

        self._do_tiling()
        self._do_storage_bound()
        self._do_compute_inline()
        self._do_set_value()
        self._do_multi_core()
        self._do_storage_align()
        self._do_ub_align()
        self._do_remove_pad()
        self._do_compute_at()
        self._do_store_predicate()
        self._do_double_buffer()
        self._do_mem_reuse()
        self._do_constraints()
        self._do_emit_insn()
        self._do_axis_group()
        self._do_mem_unique()
        self._do_build_args()

        self._add_compile_info()

        return self._schedule if self._check_tiling_case() else None


    def _do_build_args(self):
        double_page_ops_white_list = (
            "Add",
            "Mul",
        )

        double_page_soc_white_list = (ASCEND_910B, ASCEND_910_93, ASCEND_310P)

        if operation.get_context().get_op_type() not in double_page_ops_white_list:
            return

        if get_soc_spec(SHORT_SOC_VERSION) not in double_page_soc_white_list:
            return

        if self._tiling_strategy == TilingStrategy.CONST:
            self._do_build_args_const()
        else:
            self._do_build_args_dyn()


    def _do_build_args_dyn(self):
        dst_shape = util.shape_to_list(self._out.shape)
        max_brd_shape_size = reduce(mul, dst_shape, 1)

        max_brd_number = tvm.const(0)
        for input_tensor in self._input_tensors:
            cur_shape = util.shape_to_list(input_tensor.shape)
            cur_shape_size = reduce(mul, cur_shape, 1)

            max_brd_number = max_brd_number + tvm.select(cur_shape_size == max_brd_shape_size, 1, 0)

        l2_cache_size = tvm.div(get_soc_spec(L2_SIZE), max_brd_number)

        sch = self._schedule
        for input_tensor in self._input_tensors:
            cur_shape = util.shape_to_list(input_tensor.shape)
            cur_shape_size = reduce(mul, cur_shape, 1)
            condition = tvm.all(tvm.expr.EQ(cur_shape_size, max_brd_shape_size), \
                tvm.expr.GT(DTYPE_BYTE_MAPPING.get(input_tensor.dtype) * cur_shape_size, l2_cache_size))
            sch.bind_args(input_tensor, condition, tvm.BindArgPurpose.DisableL2Cache)

        for out_tensor in self._out_tensors:
            condition = tvm.expr.GT(
                max_brd_shape_size * DTYPE_BYTE_MAPPING.get(out_tensor.dtype), get_soc_spec(L2_SIZE))
            sch.bind_args(out_tensor, condition, tvm.BindArgPurpose.DisableL2Cache)


    def _do_build_args_const(self):
        dst_shape = util.shape_to_list(self._out.shape)
        max_brd_shape_size = reduce(mul, dst_shape, 1)

        max_brd_number = 0
        for input_tensor in self._input_tensors:
            cur_shape = util.shape_to_list(input_tensor.shape)
            cur_shape_size = reduce(mul, cur_shape, 1)

            if cur_shape_size == max_brd_shape_size:
                max_brd_number = max_brd_number + 1

        l2_cache_size = tvm.div(get_soc_spec(L2_SIZE), max_brd_number)

        sch = self._schedule
        for input_tensor in self._input_tensors:
            cur_shape = util.shape_to_list(input_tensor.shape)
            cur_shape_size = reduce(mul, cur_shape, 1)
            if cur_shape_size == max_brd_shape_size and \
                cur_shape_size * DTYPE_BYTE_MAPPING.get(input_tensor.dtype) > l2_cache_size:
                sch.bind_args(input_tensor, True, tvm.BindArgPurpose.DisableL2Cache)

        for out_tensor in self._out_tensors:
            if max_brd_shape_size * DTYPE_BYTE_MAPPING.get(out_tensor.dtype) > get_soc_spec(L2_SIZE):
                sch.bind_args(out_tensor, True, tvm.BindArgPurpose.DisableL2Cache)


    def _ba_pattern_enable_reorder(self):
        if self._mode in (para_check.UNFOLD_A, para_check.UNFOLD_B) and self._ub_split_axis == 1:
            return True
        return False

    def _get_ub_tensor(self, tensor_i, is_ori_graph=True):
        """
        get single ub tensor for gm tensor
        for middle out tensor, return cache_write tensor
        @param tensor: gm tensor
        @return: ub tensor
        """
        if (not self._enable_inplace_cache_clone) or (self._enable_inplace_cache_clone and not is_ori_graph):
            if tensor_i in self._middle_out_tensors:
                return self._gm_tensor_cache_write_buffer_map.get(tensor_i)
            if tensor_i in self._gm_tensor_cache_read_buffer_map:
                return self._gm_tensor_cache_read_buffer_map.get(tensor_i)
            if tensor_i in self._gm_tensor_cache_write_buffer_map:
                return self._gm_tensor_cache_write_buffer_map.get(tensor_i)
            return tensor_i

        # ori_graph return cache clone tensor for intersect tensor
        if self._enable_inplace_cache_clone and is_ori_graph:
            if tensor_i in self._cache_clone_helper.inplace_graph_tensors:
                return self._gm_tensor_and_clone_buffer_map.get(tensor_i)
            if tensor_i in self._middle_out_tensors:
                return self._gm_tensor_cache_write_buffer_map.get(tensor_i)
            if tensor_i in self._gm_tensor_cache_read_buffer_map:
                return self._gm_tensor_cache_read_buffer_map.get(tensor_i)
            if tensor_i in self._gm_tensor_cache_write_buffer_map:
                return self._gm_tensor_cache_write_buffer_map.get(tensor_i)
            return tensor_i

        return None

    def _get_ub_tensors(self, tensor, is_ori_graph=True):
        """
        get all ub tensors for gm tensor
        for middle out tensor, return both cache_read and cache_write tensor
        @param tensor: gm tensor
        @return: ub tensor
        """
        if (not self._enable_inplace_cache_clone) or (self._enable_inplace_cache_clone and not is_ori_graph):
            ub_tensors = set()
            if tensor in self._gm_tensor_cache_read_buffer_map:
                ub_tensors.add(self._gm_tensor_cache_read_buffer_map.get(tensor))
            if tensor in self._gm_tensor_cache_write_buffer_map:
                ub_tensors.add(self._gm_tensor_cache_write_buffer_map.get(tensor))
            if tensor not in self._gm_tensor_cache_read_buffer_map and \
                    tensor not in self._gm_tensor_cache_write_buffer_map:
                ub_tensors.add(tensor)
            if tensor in self._gm_tensor_cache_read_buffers_map:
                ub_tensors.update(self._gm_tensor_cache_read_buffers_map.get(tensor, set()))
            return ub_tensors
        if self._enable_inplace_cache_clone and is_ori_graph:
            ub_tensors = set()
            if tensor in self._cache_clone_helper.inplace_graph_tensors:
                ub_tensors.add(self._gm_tensor_and_clone_buffer_map.get(tensor))
                return ub_tensors
            if tensor in self._gm_tensor_cache_read_buffer_map:
                ub_tensors.add(self._gm_tensor_cache_read_buffer_map.get(tensor))
            if tensor in self._gm_tensor_cache_write_buffer_map:
                ub_tensors.add(self._gm_tensor_cache_write_buffer_map.get(tensor))
            if tensor not in self._gm_tensor_cache_read_buffer_map and \
                    tensor not in self._gm_tensor_cache_write_buffer_map:
                ub_tensors.add(tensor)
            if tensor in self._gm_tensor_cache_read_buffers_map:
                ub_tensors.update(self._gm_tensor_cache_read_buffers_map.get(tensor, set()))
            return ub_tensors

        return set()

    def _get_all_ub_tensors(self, tensors, is_ori_graph=True):
        """
        get all ub tensors set for gm tensor set
        @param tensors: gm tensor set
        @return: all ub tensors set
        """
        ub_tensors = set()
        for tensor_i in tensors:
            ub_tensors.update(self._get_ub_tensors(tensor_i))
        return ub_tensors

    def _calc_unify_tiling(self):
        one_repeat_bytes = self._one_repeat_bytes

        def enable_brc_inline():
            for _tensor in self._broadcast_tensors - self._compute_inline_tensors:
                shapes = util.shape_to_list(_tensor.shape)
                ele_in_block = int(self._ub_block_size // DTYPE_BYTE_MAPPING.get(_tensor.dtype))
                one_repeat = int(one_repeat_bytes // DTYPE_BYTE_MAPPING.get(_tensor.dtype))
                if isinstance(shapes[-1], int) and one_repeat % shapes[-1] == 0 and \
                   ele_in_block < shapes[-1] <= CONST_BRC_INLINE_LIMIT * ele_in_block:
                    return True
            return False

        def broadcast_is_align():
            for _tensor in self._broadcast_tensors - self._compute_inline_tensors:
                src_shapes = util.shape_to_list(_tensor.shape)
                ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(_tensor.dtype)
                if not isinstance(src_shapes[-1], int) or src_shapes[-1] % ele_in_block != 0:
                    return False
            return True

        self._brc_avoid_bank_conflict = not broadcast_is_align()
        operation.add_compile_info_inner("_brc_avoid_bank_conflict", self._brc_avoid_bank_conflict)
        if self._tiling_strategy == TilingStrategy.CONST:
            enable_ahead_calc = not self._enable_compute_root and self._ahead_calc()
            if enable_ahead_calc:
                middle_num = len(self._middle_tensors - self._cst_compute_root_tensors)
                input_num = len(self._input_tensors - self._cst_compute_root_tensors)
                self._enable_db = False
                if input_num * CONST_AHEAD_DB_FACTOR >= middle_num:
                    self._enable_db = True
                if enable_brc_inline():
                    self._tmp_ub_size += one_repeat_bytes
                self._calc_tiling()
            else:
                self._get_const_storage_bound()
                self._enable_db = True
                if self._coexisting_quantity > CONST_DB_MIDDLE_NODES_LIMIT or self._enable_compute_root:
                    self._enable_db = False
                for tensor_i in self._broadcast_tensors - self._compute_inline_tensors:
                    dst_shape = util.shape_to_list(tensor_i.shape)
                    ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
                    if dst_shape[-1] % ele_in_block != 0:
                        self._enable_db = False
                        break
                if enable_brc_inline():
                    self._tmp_ub_size += one_repeat_bytes
                self._calc_tiling()
                self._calc_store_predicate()
        else:
            self._calc_store_predicate()
            self._calc_storage_bound()
            if self._is_store_align and self._ub_split_axis != len(self._out.shape) - 1:
                predicate_nodes = len(self._broadcast_store_predicate.union(self._store_predicate_common_tensors))
                if self._coexisting_quantity - predicate_nodes < 2:
                    self._coexisting_quantity += 1
            if enable_brc_inline():
                self._tmp_ub_size += one_repeat_bytes
            self._calc_tiling()

    def _ahead_calc(self):
        # step 0: find all brc
        brc_nodes = GraphUtil.find_outermost_brc(self._out)
        # step 1: grouping
        groups_map = GraphUtil.brc_grouping(brc_nodes, self._max_dtype_bytes)
        GraphUtil.update_groups_by_out(groups_map, self._max_dtype_bytes, self._outs)
        groups = sorted(groups_map.items(), key=lambda x: x[0])
        # step 2: clac nodes
        self._calc_storage_bound()
        max_available_ub_bytes = (((self._ub_size - self._tmp_ub_size) // self._coexisting_quantity) //
                                  self._ub_block_size) * self._ub_block_size
        max_available_ub = max_available_ub_bytes // self._max_dtype_bytes
        is_only_last_brc = GraphUtil.only_last_brc(self._input_tensors)
        ele_in_block = self._ub_block_size // self._max_dtype_bytes
        ahead_size = 0
        cst_compute_root_last_brc_cut_last = True
        for src_size, brc_group in groups:
            for tensor_i in brc_group:
                before_brc_shape = util.shape_to_list(tensor_i.op.input_tensors[0].shape)
                dst_shape = util.shape_to_list(tensor_i.shape)
                # cond 1: last align
                # keep original branch for v220 for after smoke performance drop.
                if util.is_v220():
                    is_last_align = before_brc_shape[-1] % ele_in_block == 0
                else:
                    is_last_align = before_brc_shape[-1] % self._max_ele_in_block == 0

                # cond 2: cut last axis, dividing by 2 may be too risky,
                # so take a conservative strategy get max_abailable_ub
                is_last_brc_cut_last = is_only_last_brc and before_brc_shape[-1] == 1 and \
                                       dst_shape[-1] > max_available_ub and not ScheduleUtil.is_mov_align_sch()
                if not (is_last_align or is_last_brc_cut_last):
                    continue
                cst_compute_root_last_brc_cut_last = \
                    cst_compute_root_last_brc_cut_last and is_last_brc_cut_last and not is_last_align
                if tensor_i in self._cst_compute_root_tensors:
                    continue
                is_out = tensor_i in self._outs and tensor_i not in self._broadcast_tensors
                tensor_root = tensor_i
                if not is_out:
                    tensor_root = tensor_i.op.input_tensors[0]
                all_nodes = GraphUtil.get_all_nodes(tensor_root)
                in_out_maps = GraphUtil.get_in_out_map(tensor_root)
                coexisting_quantity = GraphUtil.max_live_node(tensor_root, in_out_maps, True)
                max_dtype_bytes = max(DTYPE_BYTE_MAPPING.get(_tensor.dtype) for _tensor in all_nodes)
                cur_max_size = coexisting_quantity * src_size * max_dtype_bytes
                if ahead_size + cur_max_size <= max_available_ub_bytes:
                    self._cst_compute_root_tensors.update(all_nodes)
                    if not is_out:
                        ahead_size += (src_size * DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
        self._cst_compute_root_last_brc_cut_last = cst_compute_root_last_brc_cut_last
        dst_shape = util.shape_to_list(self._out.shape)
        dst_size = reduce(mul, dst_shape, 1)
        if not self._cst_compute_root_tensors or dst_size <= max_available_ub:
            self._tmp_ub_size = 0
            self._coexisting_quantity = 1
            return False

        self._tmp_ub_size = 0
        self._coexisting_quantity = 1
        self._calc_storage_bound()
        self._tmp_ub_size += ahead_size
        self._compute_root_tmp_ub_size = ahead_size
        return True

    def _visit_before_broadcast_node(self):
        def dfs_graph(_out):
            for tensor_i in _out.op.input_tensors:
                if tensor_i in visited_tensors:
                    continue
                visited_tensors.add(tensor_i)
                self._before_broadcast_tensors.add(tensor_i)
                dfs_graph(tensor_i)

        out_brc = GraphUtil.find_outermost_brc(self._out, self._compute_inline_tensors)
        visited_tensors = set()
        for out in out_brc:
            dfs_graph(out)

    def _visit_elewise_graph(self):
        def _dfs_brc_ele_graph(out):
            for tensor in out.op.input_tensors:
                if tensor in visited_tensors:
                    continue
                nonlocal is_elewise_graph
                is_elewise_graph = is_elewise_graph and ScheduleUtil.is_elewise_or_brc_inline(tensor)
                cur_out_graph_tensors.add(tensor)
                visited_tensors.add(tensor)
                _dfs_brc_ele_graph(tensor)

        for out in self._out_tensors - self._middle_out_tensors:
            cur_out_graph_tensors = {out}
            visited_tensors = {out}

            is_elewise_graph = ScheduleUtil.is_elewise_or_brc_inline(out)
            _dfs_brc_ele_graph(out)
            if is_elewise_graph:
                self._elewise_graph_tensors.update(cur_out_graph_tensors)

    def _construct_compute_graph(self):
        def match_scalar_scene(tensor_):
            # condition:
            # 1. tensor --> tensor
            # 2. broadcast tensor is output
            # 3. next compute support scalar
            if len(tensor_.op.input_tensors) != 0 and \
                    util.get_tensor_size(tensor_.op.input_tensors[0]) != 1:
                return False
            if tensor_ in self._out_tensors:
                return False
            if all(util.support_scalar(tensor_o) for tensor_o in self._in_out_map.get(tensor_)):
                return True
            return False

        def _no_broadcast(_src_shapes, _dst_shapes):
            _src_shapes = util.shape_to_list(_src_shapes)
            _dst_shapes = util.shape_to_list(_dst_shapes)
            broadcast_num = 0
            for x, y in zip(_src_shapes, _dst_shapes):
                if not expr_equal(x, y):
                    broadcast_num += 1
            return broadcast_num

        def _pre_handle_placeholder():
            for index, out in enumerate(self._outs):
                if util.is_placeholder(out):
                    copy_out = ScheduleUtil.copy_node(out)
                    self._outs[index] = copy_out
                    self._copy_out_tensors.add(copy_out)
                    self._out_tensors.add(copy_out)
                    self._out_tensors.remove(out)

        def _build_graph_fake_node():
            self._pure_out_tensors = [_tensor for _tensor in self._outs if _tensor not in self._middle_out_tensors]

            # compute root graph produce a fake_node, compute root graph fake_node should compute at original graph
            if self._enable_compute_root and self._compute_root_helper.compute_root_fake_node is not None:
                self._pure_out_tensors.append(self._compute_root_helper.compute_root_fake_node)

            # inplace graph outs produce a fake_nde, inplace graph compute independently
            if self._enable_inplace_cache_clone:
                self._pure_out_tensors = [_tensor
                                          for _tensor in self._pure_out_tensors
                                          if _tensor not in self._cache_clone_helper.inplace_outs]
            if len(self._pure_out_tensors) > 1:
                self._out = ScheduleUtil.fake_node(self._pure_out_tensors)
                self.__dfs_sub_graph(self._out, visited_tensors)
            else:
                self._out = self._pure_out_tensors[0]

        def dfs_graph_inner(_out):
            for tensor_i in _out.op.input_tensors:
                if tensor_i in visited_tensors:
                    continue
                visited_tensors.add(tensor_i)
                self._unfold_broadcast_tensors.add(tensor_i)
                dfs_graph_inner(tensor_i)

        self._out_tensors = set(self._outs)
        _pre_handle_placeholder()

        visited_tensors = set()
        for out in self._out_tensors:
            if util.is_broadcast(out):
                self._broadcast_tensors.add(out)
            if util.is_one_shape_broadcast(out):
                self._one_shape_broadcast_tensors.add(out)
            self.__dfs_sub_graph(out, visited_tensors)
            self._dtypes.add(out.dtype)
            self._outs_dtypes.add(out.dtype)
        byte_len = [DTYPE_BYTE_MAPPING.get(dtype) for dtype in self._dtypes]
        self._max_dtype_bytes = max(byte_len)
        self._max_ele_in_block = int(self._ub_block_size // min(byte_len))
        # out uint1 dtype needs ub_factor align to 256
        if "uint1" in self._outs_dtypes:
            self._ub_factor_align = max(self._ub_factor_align, self._special_factor_align)

        self._pure_middle_tensors = self._middle_tensors - self._out_tensors
        self._middle_out_tensors = self._middle_tensors.intersection(self._out_tensors)

        _build_graph_fake_node()

        for tensor_i in self._broadcast_tensors:
            if match_scalar_scene(tensor_i):
                self._absorbable_broadcast_tensors.add(tensor_i)
        for tensor_i in self._one_shape_broadcast_tensors:
            self._absorbable_broadcast_tensors.add(tensor_i)

        if self._broadcast_tensors:
            self._max_brc_bytes = max(DTYPE_BYTE_MAPPING.get(tensor_i.dtype) for tensor_i in self._broadcast_tensors)

        ub_idx = self._ub_split_axis
        for tensor_i in self._broadcast_tensors - self._absorbable_broadcast_tensors:
            if util.get_dsl_insn(tensor_i) != BROADCAST:
                src_shapes = tensor_i.op.input_tensors[0].shape[ub_idx:]
                dst_shapes = tensor_i.shape[ub_idx:]
                self._broadcast_axis_num[tensor_i] = _no_broadcast(src_shapes, dst_shapes)

        if self._ba_pattern_enable_reorder():
            visited_tensors = set()
            for tensor_i in self._broadcast_tensors:
                dfs_graph_inner(tensor_i)

    def _calc_cache_read(self):
        self._cache_read_tensors.update(self._input_tensors)
        self._cache_read_tensors.update(self._middle_out_tensors)

    def _do_cache_read(self):
        for tensor_i in self._cache_read_tensors:
            if self._enable_inplace_cache_clone and tensor_i in self._cache_clone_helper.inplace_graph_tensors:
                read_buffer_consumer = self._cache_clone_helper.inplace_graph_in_out_map.get(tensor_i)
            else:
                read_buffer_consumer = self._in_out_map.get(tensor_i)

            buffer_tensor = self._schedule.cache_read(tensor_i, self._scope, read_buffer_consumer)
            self._cache_read_buffer_tensor_map[buffer_tensor] = tensor_i
            self._gm_tensor_cache_read_buffer_map[tensor_i] = buffer_tensor
            if tensor_i in self._middle_out_tensors:
                self._middle_out_cache_read_buffer_map[tensor_i] = buffer_tensor

    def _calc_cache_write(self):
        self._cache_write_tensors.update(self._out_tensors - self._copy_out_tensors)

    def _do_cache_write(self):
        for tensor_i in self._cache_write_tensors:
            buffer_tensor = self._schedule.cache_write(tensor_i, self._scope)
            self._cache_write_buffer_tensor_map[buffer_tensor] = tensor_i
            self._gm_tensor_cache_write_buffer_map[tensor_i] = buffer_tensor

            if tensor_i in self._middle_out_tensors:
                self._middle_out_cache_write_buffer_map[tensor_i] = \
                    buffer_tensor

    def _set_scope(self):
        sch = self._schedule
        for tensor_i in self._pure_middle_tensors:
            sch[tensor_i].set_scope(self._scope)

    def _get_const_storage_bound(self):
        res = self._out
        output_shape = util.shape_to_list(res.shape)
        const_tmp_ub_size = 0
        const_coexisting_quantity = 1
        for i in range(len(output_shape) - 1, -1, -1):
            self._ub_split_axis = i
            self._tmp_ub_size = 0
            self._coexisting_quantity = 1
            self._need_do_block = True
            self._calc_store_predicate()
            self._need_do_block = False
            self._calc_storage_bound()
            const_tmp_ub_size = max(const_tmp_ub_size, self._tmp_ub_size)
            const_coexisting_quantity = max(const_coexisting_quantity, self._coexisting_quantity)
            self._broadcast_store_predicate.clear()
            self._all_pre_node_broadcast.clear()
            self._store_predicate_common_tensors.clear()

        self._tmp_ub_size = const_tmp_ub_size
        self._coexisting_quantity = const_coexisting_quantity
        self._ub_split_axis = 0

    def _calc_store_predicate(self):
        def _dfs_cur_tensor(tensor_i):
            for _tensor in tensor_i.op.input_tensors:
                all_pre_node.add(_tensor)
                _dfs_cur_tensor(_tensor)

        def _is_only_calc_one(tensor_i):
            if len(tensor_i.op.input_tensors) != 1:
                return False
            if self._tiling_strategy == TilingStrategy.ONE_CUT or \
                    (self._tiling_strategy == TilingStrategy.CONST and self._need_do_block):
                u_i = self._ub_split_axis
                src_shape = tensor_i.op.input_tensors[0].shape
                dst_shape = tensor_i.shape
                return not expr_equal(src_shape[u_i], dst_shape[u_i])
            return False

        def _has_ternary_insns():
            for tensor_i in self._out_tensors | self._pure_middle_tensors:
                insn = util.get_dsl_insn(tensor_i)
                if insn in TERNARY_INSNS:
                    return True
            return False

        if _has_ternary_insns():
            return

        for tensor_i in self._broadcast_tensors:
            if not _is_only_calc_one(tensor_i):
                continue
            cur_tensor = tensor_i
            pre_tensor = tensor_i
            all_pre_node = set()
            if tensor_i in self._absorbable_broadcast_tensors:
                cur_tensor = tensor_i.op.input_tensors[0]
                pre_tensor = cur_tensor
            elif tensor_i in self._remove_pad_map:
                cur_tensor = self._remove_pad_map.get(tensor_i)
                all_pre_node.add(tensor_i)
            self._broadcast_store_predicate.add(cur_tensor)
            _dfs_cur_tensor(pre_tensor)
            self._all_pre_node_broadcast.update(all_pre_node)

        disable_store_predicate = False
        for tensor_i in self._all_pre_node_broadcast:
            #1. for graph input0 (-1, 16, -1, -1）
            #             input1 (1,) --> brc3     --> div --> brc5
            #             input2 (-1, 1, 1, -1)    --> brc4          --> add --> output
            # solution: from brc5 see input1 as a pre node. input1 set store predicate but not mem unique.
            #           To avoid such fusion graph set store predicate in the following condition:
            #               1. in dymamic mode
            #               2. in fusion mode
            #               3. exists pre node's(without broadacst_store_predicate) consumer is one shape broadcast
            if operation.in_dynamic() and ScheduleUtil.is_fusion_op():
                common_tensor_without_broadcast = self._in_out_map.get(tensor_i) - self._broadcast_store_predicate
                if len(common_tensor_without_broadcast) > 0 and \
                    (common_tensor_without_broadcast & self._one_shape_broadcast_tensors):
                    disable_store_predicate = True
                    break

            common_tensor = self._in_out_map.get(tensor_i) - \
                            (self._all_pre_node_broadcast | self._broadcast_store_predicate)
            if len(common_tensor) > 0:
                # common in multi output
                if tensor_i in self._out_tensors:
                    disable_store_predicate = True
                    break
                self._store_predicate_common_tensors.add(tensor_i)
        extend_node_num = len(self._broadcast_store_predicate) + len(self._store_predicate_common_tensors)
        if disable_store_predicate or extend_node_num > MAX_EXTEND_NODE_NUM:
            self._broadcast_store_predicate.clear()
            self._all_pre_node_broadcast.clear()
            self._store_predicate_common_tensors.clear()

    def _without_temp_buffer(self):
        pass

    def _calc_storage_bound(self):
        pass

    def _calc_tiling(self):
        funcs = {TilingStrategy.ALL_CUT: self._calc_tiling_all_cut,
                 TilingStrategy.NONE_CUT: self._calc_tiling_none_cut,
                 TilingStrategy.ONE_CUT: self._calc_tiling_one_cut,
                 TilingStrategy.STATIC: self._calc_tiling_static,
                 TilingStrategy.CONST: self._calc_tiling_const,
                 }
        funcs.get(self._tiling_strategy)()

    def _calc_tiling_all_cut(self):
        res = self._out
        for _i, _x in enumerate(res.shape):
            bound = (1, util.get_bound(_x)[1])
            self._block_tiling_vars[_i] = operation.var_inner_adaptive("_block_factor_" + str(_i), bound)
            self._ub_tiling_vars[_i] = operation.var_inner_adaptive("_ub_factor_" + str(_i), bound)

    def _calc_tiling_none_cut(self):
        pass

    def _calc_tiling_one_cut(self):

        def _calc_inplace_tiling_one_cut():
            if self._enable_inplace_cache_clone:
                var_bound = (1, INT32_MAX)
                block_axis = self._tiling_case.inplace_block_axis
                ub_axis = self._tiling_case.inplace_ub_axis
                self._inplace_graph_block_factor = operation.var_inner_adaptive(INPLACE_BLOCK_FACTOR_PRE, var_bound)
                self._inplace_graph_ub_factor = operation.var_inner_adaptive(INPLACE_UB_FACTOR_PRE, var_bound)

        _calc_inplace_tiling_one_cut()

        res = self._out
        shape = util.shape_to_list(res.shape)
        b_i = self._block_split_axis
        u_i = self._ub_split_axis
        b_bound = (1, INT32_MAX)
        u_bound = self._tiling_case.ub_factor_bound
        if u_bound is None:
            u_bound = (1, util.get_bound(shape[u_i])[1])
        self._block_tiling_vars[b_i] = operation.var_inner_adaptive("_block_factor_" + str(b_i), b_bound)
        self._ub_tiling_vars[u_i] = operation.var_inner_adaptive("_ub_factor_" + str(u_i), u_bound)
        self._block_factor = self._block_tiling_vars.get(b_i)
        self._ub_factor = self._ub_tiling_vars.get(u_i)

    def _calc_tiling_static(self):
        res = self._out
        shape = util.shape_to_list(res.shape)
        b_i = self._block_split_axis
        u_i = self._ub_split_axis
        b_bound = (1, util.get_bound(shape[b_i])[1])
        self._block_tiling_vars[b_i] = operation.var_inner_adaptive("_block_factor_" + str(b_i), b_bound)
        self._ub_tiling_vars[u_i] = self._tiling_case.ub_factor_bound
        self._block_factor = self._block_tiling_vars.get(b_i)
        self._ub_factor = self._ub_tiling_vars.get(u_i)

    def _calc_tiling_const(self):
        def _get_const_tiling_info():
            input_shapes = []
            inputs = []
            outputs = []
            max_dim_length = len(output_shape)
            for _input in self._input_tensors:
                input_shape = util.shape_to_list(_input.shape)
                input_shapes.append([1] * (max_dim_length - len(input_shape)) + input_shape)
                inputs.append({"shape": input_shape, "dtype": _input.dtype})
            for out_i in self._outs:
                outputs.append({"shape": util.shape_to_list(out_i.shape), "dtype": out_i.dtype})

            if not inputs:
                inputs = [{"shape": output_shape, "dtype": self._out.dtype}]
                max_dim_length = 0

            input_shapes = list(map(list, zip(*input_shapes)))
            for i in range(max_dim_length - 1, -1, -1):
                if any(input_shapes[i][0] != s for s in input_shapes[i]):
                    broadcast_axis[i] = True

            max_available_ub = ((((self._ub_size - self._tmp_ub_size) // self._coexisting_quantity) //
                                 self._ub_block_size) * self._ub_block_size) // self._max_dtype_bytes
            # compute_root tensor tmp ub size, don't need double in db
            db_tmp_ub_size = self._compute_root_tmp_ub_size + 2 * (self._tmp_ub_size - self._compute_root_tmp_ub_size)
            max_available_ub_db = ((((self._ub_size - db_tmp_ub_size) // 2 // self._coexisting_quantity) //
                                    self._ub_block_size) * self._ub_block_size) // self._max_dtype_bytes
            if self._enable_db:
                max_available_ub = max_available_ub_db
            base_info = {"000": [util.get_core_num(), self._max_dtype_bytes,
                                 max_available_ub, max_available_ub_db, self._max_brc_bytes]}

            only_const_tiling = True
            is_const_shapes = False
            support_broadcast = operation.get_context().get("_support_broadcast")
            use_special_pattern = False
            const_compile_info = {
                CompileInfo.ONLY_CONST_TILING: only_const_tiling,
                CompileInfo.IS_CONST_SHAPES: is_const_shapes,
                CompileInfo.SUPPORT_BROADCAST: support_broadcast,
                CompileInfo.USE_SPECIAL_PATTERN: use_special_pattern,
                CompileInfo.BASE_INFO: base_info,
                CompileInfo.SOC_VERSION: get_soc_spec(SHORT_SOC_VERSION),
                CompileInfo.BROADCAST_AXIS: broadcast_axis,
                CompileInfo.UB_FACTOR_ALIGN: self._ub_factor_align,
                CompileInfo.IS_VNCHWCONV_ALIGN: self._is_vnchwconv_align,
                CompileInfo.CONST_TILING_OUT: output_shape,
                CompileInfo.INT64_MODE: operation.is_int64_mode(),
                CompileInfo.MAX_ELE_IN_BLOCK: self._max_ele_in_block,
            }
            const_compile_info.update(get_compile_info())
            const_compile_info.update(self._calc_inplace_clone_compiler_info())
            const_compile_info.update(self._calc_compute_root_compiler_info())

            op_type = "AutoTiling"
            return op_tiling.do_op_tiling(op_type, const_compile_info, inputs, outputs)

        def _decode_const_tiling_data(tiling_info):
            tiling_format = TILING_BASIC_FORMAT.copy()
            tiling_extra_format = dict()
            is_tiling_data_int64_mode = operation.is_int64_mode()

            if is_tiling_data_int64_mode:
                tiling_format = TILING_BASIC_FORMAT_INT64.copy()
                if self._enable_inplace_cache_clone:
                    tiling_extra_format = TILING_WITH_INPLACE_EXTRA_FORMAT_INT64
            else:
                if self._enable_inplace_cache_clone:
                    tiling_extra_format = TILING_WITH_INPLACE_EXTRA_FORMAT

            tiling_format.update(tiling_extra_format)
            tiling_data = op_tiling.decode(tiling_info.get(TILING_DATA), tiling_format)
            self._block_dims = tiling_info.get(BLOCK_DIM)
            self._need_do_block = tiling_data.get(NEED_TILING_CUT) > 0
            self._is_store_align = tiling_data.get(IS_STORE_ALIGN) > 0
            if self._need_do_block:
                self._block_split_axis = tiling_data.get(BLOCK_AXIS)
                self._block_factor = tiling_data.get(BLOCK_FACTOR)
                self._ub_split_axis = tiling_data.get(UB_AXIS)
                self._ub_factor = tiling_data.get(UB_FACTOR)
                if self._enable_inplace_cache_clone:
                    self._inplace_graph_block_axis = tiling_data.get(INPLACE_BLOCK_AXIS)
                    self._inplace_graph_block_factor = tiling_data.get(INPLACE_BLOCK_FACTOR)
                    self._inplace_graph_ub_axis = tiling_data.get(INPLACE_UB_AXIS)
                    self._inplace_graph_ub_factor = tiling_data.get(INPLACE_UB_FACTOR)
            self._enable_db = self._enable_db or (True if tiling_data.get("is_need_db", 0) == 1 else False)

        output_shape = util.shape_to_list(self._out.shape)
        broadcast_axis = [False] * len(output_shape)
        if output_shape == [0]:
            self._block_dims = 1
            self._need_do_block = False
            return

        tiling_result = _get_const_tiling_info()
        _decode_const_tiling_data(tiling_result)

    def _is_last_align(self, _tensor):
        if operation.get_context().get_current_compute().get("_is_fractal_format"):
            return False
        src_shapes = util.shape_to_list(_tensor.op.input_tensors[0].shape)
        ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(_tensor.dtype)
        if isinstance(src_shapes[-1], int) and src_shapes[-1] % ele_in_block == 0 and \
                _tensor not in self._out_tensors:
            use_tensors = list(self._in_out_map.get(_tensor, []))
            for u_tensor in use_tensors:
                # BA_broadcast_inline: broadcast consumer must not be spceial dsl
                if util.is_vcmp_insn(u_tensor) or util.is_vsel_insn(u_tensor) or \
                        util.is_vcmpsel_insn(u_tensor) or util.is_ternary_insn(u_tensor):
                    return False
                # BA_broadcast_inline: broadcast consumer must be aligned
                u_last_axis = 0
                if util.shape_to_list(u_tensor.shape):
                    u_last_axis = util.shape_to_list(u_tensor.shape)[-1]
                u_ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(u_tensor.dtype)
                if isinstance(u_last_axis, int) and u_last_axis % u_ele_in_block != 0:
                    return False
                if ScheduleUtil.is_b64_vmax_vmin(u_tensor):
                    return False
            return True
        return False

    def _calc_compute_inline(self):
        def no_broadcast(_src_shapes, _dst_shapes):
            _src_shapes = util.shape_to_list(_src_shapes)
            _dst_shapes = util.shape_to_list(_dst_shapes)
            for x, y in zip(_src_shapes, _dst_shapes):
                if not expr_equal(x, y):
                    return False
            return True

        def update_store_predicate(_tensor):
            if _tensor in self._broadcast_store_predicate:
                self._broadcast_store_predicate.remove(_tensor)
                self._broadcast_store_predicate.add(_tensor.op.input_tensors[0])

                # as one pre node broadcast tensor may concat with two broadcast tensor
                # and both broadcast tensors can be inline, so should add conditon when
                # remove tensor in all_pre_node_broadcast set in case of it has already
                # been removed by another broadcast tensor.
                if _tensor.op.input_tensors[0] in self._all_pre_node_broadcast:
                    self._all_pre_node_broadcast.remove(_tensor.op.input_tensors[0])

        # ternary instruct out must be memory reused with input, so ternary instruct input can not do compute_inline
        def is_ternary_ins_input(_tensor):
            if _tensor not in self._in_out_map.keys():
                return False
            for tensor_out in self._in_out_map.get(_tensor):
                tensor_out_insn = util.get_dsl_insn(tensor_out)
                if tensor_out_insn in TERNARY_INSNS:
                    return True
            return False

        def is_const_compute_inline(_tensor):
            if util.get_dsl_insn(_tensor) == BROADCAST or is_ternary_ins_input(_tensor):
                return False
            src_shapes = _tensor.op.input_tensors[0].shape[ub_idx:]
            dst_shapes = _tensor.shape[ub_idx:]
            if no_broadcast(src_shapes, dst_shapes):
                return True
            return False

        def is_dynamic_compute_inline(_tensor):
            return self._broadcast_axis_num.get(_tensor) == 0 \
                   and not is_ternary_ins_input(_tensor)

        def is_output_tag_gcd(_tensor):
            if _tensor not in self._in_out_map.keys():
                return False
            for tensor_out in self._in_out_map.get(_tensor):
                tensor_out_insn = util.get_dsl_insn(tensor_out)
                if tensor_out_insn == "elewise_binary_gcd":
                    return True
            return False

        def is_output_tag_powi(_tensor):
            if _tensor not in self._in_out_map.keys():
                return False
            for tensor_out in self._in_out_map.get(_tensor):
                tensor_out_insn = util.get_dsl_insn(tensor_out)
                if tensor_out_insn == "elewise_binary_powi":
                    return True
            return False

        self._compute_inline_tensors = self._absorbable_broadcast_tensors.copy()
        if self._tiling_strategy == TilingStrategy.CONST:
            ub_idx = self._ub_split_axis
            for tensor_i in self._broadcast_tensors - self._absorbable_broadcast_tensors:
                if is_const_compute_inline(tensor_i):
                    update_store_predicate(tensor_i)
                    self._compute_inline_tensors.add(self._get_ub_tensor(tensor_i))
        else:
            for tensor_i in self._broadcast_axis_num:
                if is_dynamic_compute_inline(tensor_i):
                    self._compute_inline_tensors.add(self._get_ub_tensor(tensor_i))

        for tensor_i in self._broadcast_tensors - self._absorbable_broadcast_tensors:
            if not tensor_i.op.input_tensors:
                continue
            if is_output_tag_powi(tensor_i):
                continue
            if self._is_last_align(tensor_i) and not is_output_tag_gcd(tensor_i):
                update_store_predicate(tensor_i)
                self._compute_inline_tensors.add(self._get_ub_tensor(tensor_i))
                use_tensors = set(self._in_out_map.get(tensor_i, set()))
                self._const_brc_inline_tensor.update(use_tensors)

        if self._is_pure_brc_common_db:
            for tensor_i in self._broadcast_tensors:
                if tensor_i in self._gm_tensor_cache_write_buffer_map:
                    self._compute_inline_tensors.add(self._gm_tensor_cache_write_buffer_map.get(tensor_i))
                else:
                    self._compute_inline_tensors.add(tensor_i)

        self.__calc_tensor_space()

        for tensor_i in self._broadcast_tensors - self._compute_inline_tensors:
            need_update_var_range = False
            if util.get_dsl_insn(tensor_i) != BROADCAST:
                ub_under_shapes = util.shape_to_list(tensor_i.op.input_tensors[0].shape[self._ub_split_axis + 1:])
                ub_under_size = reduce(lambda x, y: x * y, ub_under_shapes or [1])
                if (self._min_storage_bound // ub_under_size) == 1 and isinstance(self._ub_factor, tvm.Var):
                    self._compute_inline_tensors.add(self._get_ub_tensor(tensor_i))
                    update_store_predicate(tensor_i)
                    need_update_var_range = True
            if need_update_var_range:
                self._constraints.add(tvm.expr.EQ(self._ub_factor, 1))
                self._ub_factor_is_one = False
                out_ub_shapes = util.shape_to_list(self._out.shape[self._ub_split_axis + 1:])
                for in_var, out_var in zip(ub_under_shapes, out_ub_shapes):
                    if isinstance(out_var, tvm.Var):
                        self._constraints.add(tvm.expr.EQ(out_var, in_var))

    def _calc_storage_align(self):
        if not self._is_store_align:
            return

        self._visit_before_broadcast_node()
        self._visit_elewise_graph()
        for _graph_tensor in self._ori_graph_unify_tensors:
            if _graph_tensor.ub_mode in (CACHE_READ_MODE, CACHE_WRITE_MODE, SET_SCOPE_MODE, CACHE_CLONE_MODE) or \
                    _graph_tensor.tensor_type in (MIDDLE_OUT_TYPE,):
                self._store_align_tensors.add(_graph_tensor.ub_tensor)

    def _calc_multi_core(self):
        if self._tiling_strategy == TilingStrategy.NONE_CUT:
            self._block_bind_axis = None

    def _calc_unify_remove_pad(self):
        pass

    def _calc_ub_align(self):
        pass

    def _calc_compute_at(self):

        def _calc_inplace_compute_at():
            """
            inplace sub_graph compute at sub_graph fake_node
            """
            inplace_compute_at_map = {}
            if self._enable_inplace_cache_clone:
                for _graph_tensor in self._inplace_graph_unify_tensors:
                    if _graph_tensor.ub_tensor != self._cache_clone_helper.inplace_out:
                        inplace_compute_at_map[_graph_tensor.ub_tensor] = [self._cache_clone_helper.inplace_out,
                                                                           self._inplace_compute_at]
            return inplace_compute_at_map

        def _calc_original_graph_compute_at():
            """
            calc original graph compute at map, except for pure inplace graph tensor and compute_root tensor
            1. except for inplace graph tensor and compute_root tensor
            2. for original and inplace graph common tensor, use cache clone buffer do compute_at
            """
            ori_graph_compute_at_map = {}
            for _graph_tensor in self._ori_graph_unify_tensors:
                if _graph_tensor.graph_type == COMPUTE_ROOT_GRAPH_TENSOR:
                    continue
                if _graph_tensor.ub_tensor == self._out:
                    continue
                if _graph_tensor.ub_tensor in self._compute_inline_tensors:
                    continue
                ori_graph_compute_at_map[_graph_tensor.ub_tensor] = [self._out, self._compute_at_axis]
            return ori_graph_compute_at_map

        if self._tiling_strategy == TilingStrategy.NONE_CUT or \
                (self._tiling_strategy == TilingStrategy.CONST and not self._need_do_block):
            self._compute_at_map.clear()
            return

        self._compute_at_map.update(_calc_inplace_compute_at())
        self._compute_at_map.update(_calc_original_graph_compute_at())

    def _calc_double_buffer(self):
        pass

    def _calc_mem_reuse(self):
        ternary_reuse_map = {
            "elewise_binary_scalar_axpy": 1,
            "elewise_multiple_mla": 2,
            "elewise_multiple_madd": 1,
            "elewise_multiple_maddrelu": 1,
        }

        # one of the input of the ternary instruction must be reused with the output, refer to "ternary_reuse_map"
        # consider "vmadd": A=A*A+B, output reuse the second A, input_tensors is 2, which need to be completed to 3
        for tensor_i in self._out_tensors | (self._pure_middle_tensors - self._compute_inline_tensors):
            insn = util.get_dsl_insn(tensor_i)
            args = ""
            if tensor_i.op.tag.find("|") != -1:
                args = tensor_i.op.tag.split("|")[1].split(",")
            if insn in TERNARY_INSNS:
                src_tensors = []
                index = 0
                first_same_tensor = None
                for i in args:
                    if i == "1":
                        if first_same_tensor not in src_tensors:
                            first_same_tensor = tensor_i.op.input_tensors[index]
                            index += 1
                        src_tensors.append(first_same_tensor)
                    else:
                        src_tensors.append(tensor_i.op.input_tensors[index])
                        index += 1
                reuse_index = ternary_reuse_map.get(insn)
                src_tensor = src_tensors[reuse_index]
                src_tensor = self._get_ub_tensor(src_tensor)
                if src_tensor in self._remove_pad_map:
                    src_tensor = self._remove_pad_map.get(src_tensor)
                dst_tensor = self._get_ub_tensor(tensor_i)
                util.merge_value(self._mem_reuse_map,
                                 src_tensor,
                                 dst_tensor)
        for tensor_i, write_buffer in self._middle_out_cache_write_buffer_map.items():
            if tensor_i in self._middle_out_cache_read_buffer_map.keys():
                util.merge_value(self._mem_reuse_map, self._middle_out_cache_read_buffer_map.get(tensor_i),
                                 write_buffer)
                continue

    def _calc_constraints(self):
        def add_condition(condition):
            if isinstance(condition, tvm.tir.PrimExpr):
                self._constraints.add(condition)

        def complete_shape(shapes):
            max_len = max([len(shape) for shape in shapes])
            complete_shape_list = []
            for shape in shapes:
                shape_len = len(shape)
                complete_shape_list.append([tvm.const(1)] * (max_len - shape_len) + list(shape))
            return complete_shape_list

        def add_elewise_axis_equal_condition(complete_shapes):
            transpose_shapes = list(map(list, zip(*complete_shapes)))
            for shape in transpose_shapes:
                pre_axis = None
                for index, axis in enumerate(shape):
                    if index == 0:
                        pre_axis = axis
                        continue
                    if not expr_equal(pre_axis, axis):
                        add_condition((pre_axis == axis).asobject())
                    pre_axis = axis

        def add_elewise_constraint():
            pure_elewise_tensors = GraphUtil.calc_elewise_tensors(self._outs)
            for tensor_i in pure_elewise_tensors:
                shapes = [list(input_i.shape) for input_i in tensor_i.op.input_tensors]
                shapes.append(util.shape_to_list(tensor_i.shape))
                complete_shapes = complete_shape(shapes)
                add_elewise_axis_equal_condition(complete_shapes)

        for tensor_i in self._broadcast_tensors:
            if util.is_unknown_broadcast(tensor_i):
                src_shapes = util.shape_to_list(tensor_i.op.input_tensors[0].shape)
                dst_shapes = util.shape_to_list(tensor_i.shape)
                for src_shape, dst_shape in zip(src_shapes, dst_shapes):
                    if not expr_equal(src_shape, dst_shape):
                        add_condition(src_shape <= dst_shape)

        shapes = util.shape_to_list(self._out.shape)
        ub_shapes = shapes[self._ub_split_axis + 1:]
        for shape in ub_shapes:
            add_condition(shape <= self._min_storage_bound)
        if self._tiling_strategy != TilingStrategy.NONE_CUT:
            ub_shapes.insert(0, self._ub_factor)
            shape_size = reduce(lambda x, y: x * y, ub_shapes)
            add_condition(shape_size <= self._min_storage_bound)
            add_condition(self._ub_factor <= self._min_storage_bound)
        add_elewise_constraint()

    def _calc_emit_insn(self):
        def get_insn(tensor_):
            tag = tensor_.op.tag
            if tensor_.op.tag.find("|") != -1:
                insn = tag.split("|")[0]
            else:
                insn = tag
            return INSN_MAPPING.get(insn, insn)

        def _calc_inplace_emit_insn():
            """
            calc inplace graph tensor emit insn
            @return:
            """
            inplace_emit_insn_map = {}
            if self._enable_inplace_cache_clone:
                for _graph_tensor in self._inplace_graph_unify_tensors:
                    if _graph_tensor.tensor_type == INPUT_TYPE:
                        inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0], DMA_COPY]
                    elif _graph_tensor.tensor_type == PURE_MIDDLE_TYPE and \
                            _graph_tensor.ub_tensor not in self._compute_inline_tensors:
                        inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0],
                                                                          get_insn(_graph_tensor.gm_tensor)]
                    elif _graph_tensor.tensor_type == MIDDLE_OUT_TYPE:
                        if _graph_tensor.ub_mode == CACHE_READ_MODE:
                            inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0],
                                                                              PHONY_INSN]
                        elif _graph_tensor.ub_mode == CACHE_WRITE_MODE:
                            inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0],
                                                                              get_insn(_graph_tensor.gm_tensor)]
                        else:
                            inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0],
                                                                              DMA_COPY]
                    elif _graph_tensor.tensor_type == PURE_OUT_TYPE:
                        if _graph_tensor.ub_mode == CACHE_WRITE_MODE:
                            inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0],
                                                                              get_insn(_graph_tensor.gm_tensor)]
                        elif _graph_tensor.ub_tensor == self._cache_clone_helper.inplace_out:
                            inplace_emit_insn_map[_graph_tensor.ub_tensor] = [self._inplace_emit_insn, DMA_COPY]
                        else:
                            inplace_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0],
                                                                              DMA_COPY]
                    elif _graph_tensor.tensor_type == FAKE_NODE_TYPE:
                        inplace_emit_insn_map[_graph_tensor.ub_tensor] = [self._inplace_emit_insn, PHONY_INSN]
            return inplace_emit_insn_map

        def _calc_original_emit_insn():
            ori_graph_emit_insn_map = {}
            for _graph_tensor in self._ori_graph_unify_tensors:
                if _graph_tensor.tensor_type == INPUT_TYPE:
                    ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0], DMA_COPY]
                elif _graph_tensor.tensor_type == PURE_MIDDLE_TYPE and \
                        _graph_tensor.ub_tensor not in self._compute_inline_tensors:
                    ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = \
                        [_graph_tensor.ub_tensor.op.axis[0], get_insn(_graph_tensor.gm_tensor)]
                elif _graph_tensor.tensor_type == MIDDLE_OUT_TYPE:
                    if _graph_tensor.ub_mode == CACHE_READ_MODE:
                        ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = \
                            [_graph_tensor.ub_tensor.op.axis[0], PHONY_INSN]
                    elif _graph_tensor.ub_mode == CACHE_WRITE_MODE:
                        ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = \
                            [_graph_tensor.ub_tensor.op.axis[0], get_insn(_graph_tensor.gm_tensor)]
                    else:
                        ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = \
                            [_graph_tensor.ub_tensor.op.axis[0], DMA_COPY]
                elif _graph_tensor.tensor_type == PURE_OUT_TYPE:
                    if _graph_tensor.ub_mode == CACHE_WRITE_MODE:
                        ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = \
                            [_graph_tensor.ub_tensor.op.axis[0], get_insn(_graph_tensor.gm_tensor)]
                    elif _graph_tensor.ub_tensor == self._out:
                        ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = [self._emit_insn_axis, DMA_COPY]
                    else:
                        ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = \
                            [_graph_tensor.ub_tensor.op.axis[0], DMA_COPY]
                elif _graph_tensor.tensor_type == FAKE_NODE_TYPE and \
                        _graph_tensor.graph_type != COMPUTE_ROOT_GRAPH_TENSOR:
                    ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = [self._emit_insn_axis, PHONY_INSN]
                elif _graph_tensor.tensor_type == FAKE_NODE_TYPE and \
                        _graph_tensor.graph_type == COMPUTE_ROOT_GRAPH_TENSOR:
                    ori_graph_emit_insn_map[_graph_tensor.ub_tensor] = [_graph_tensor.ub_tensor.op.axis[0], PHONY_INSN]
            return ori_graph_emit_insn_map

        self._emit_insn_map.update(_calc_inplace_emit_insn())
        self._emit_insn_map.update(_calc_original_emit_insn())

    def _do_tiling(self):
        funcs = {TilingStrategy.ALL_CUT: self._do_tiling_all_cut,
                 TilingStrategy.NONE_CUT: self._do_tiling_none_cut,
                 TilingStrategy.ONE_CUT: self._do_tiling_one_cut,
                 TilingStrategy.STATIC: self._do_tiling_static,
                 TilingStrategy.CONST: self._do_tiling_const,
                 }
        funcs.get(self._tiling_strategy)()

    def _do_tiling_all_cut(self):
        sch = self._schedule
        res = self._out
        block_axes = []
        ub_axes = []
        inner_axes = []
        for _i, _x in enumerate(res.op.axis):
            x_o, x_i = sch[res].split(_x, factor=self._block_tiling_vars.get(_i))
            x_io, x_ii = sch[res].split(x_i, factor=self._ub_tiling_vars.get(_i))
            block_axes.append([x_o, _i])
            ub_axes.append([x_io, _i])
            inner_axes.append([x_ii, _i])
            self._inner_shape.append([self._ub_tiling_vars.get(_i), _i])
        ir_axes = block_axes + ub_axes + inner_axes
        ordered_axes = [x[0] for x in ir_axes]
        sch[res].reorder(*ordered_axes)
        self._block_bind_axis = sch[res].fuse(*[x[0] for x in block_axes])
        self._compute_at_axis.compute_at_axis = ub_axes[-1][0]
        self._compute_at_axis_idx = ub_axes[-1][1]
        self._emit_insn_axis.emit_insn_axis = inner_axes[0][0]

    def _do_tiling_none_cut(self):
        def _do_inplace_tiling_none_cut():
            if self._enable_inplace_cache_clone:
                inplace_out = self._cache_clone_helper.inplace_out
                inplace_out_axes = inplace_out.op.axis
                self._inplace_emit_insn.emit_insn_axis = inplace_out_axes[0]

        _do_inplace_tiling_none_cut()
        res = self._out
        shape = util.shape_to_list(res.shape)
        for _i, _x in enumerate(res.op.axis):
            self._inner_shape.append([shape[_i], _i])
        self._emit_insn_axis.emit_insn_axis = res.op.axis[0]

    def _do_tiling_one_cut(self):
        self._do_inplace_tiling()
        sch = self._schedule
        res = self._out
        shape = util.shape_to_list(res.shape)
        b_idx = self._block_split_axis
        u_idx = self._ub_split_axis
        block_axes = []
        ub_axes = []
        inner_axes = []

        # 1. split ub first
        u_o, u_i = sch[res].split(res.op.axis[u_idx], factor=self._ub_tiling_vars.get(u_idx))
        ub_axes.append([u_o, u_idx])
        inner_axes.append([u_i, u_idx])
        self._inner_shape.append([self._ub_tiling_vars[u_idx], u_idx])
        for i in range(u_idx):
            block_axes.append([res.op.axis[i], i])

        # unfold mode and _ub_split_axis == 1, no need to fuse axis
        if not self._ba_pattern_enable_reorder():
            block_axes.append([u_o, u_idx])

        # 2. fuse block axis
        block_fuse_axis = sch[res].fuse(*[x[0] for x in block_axes])

        # 3. spilt block
        b_o, b_i = sch[res].split(block_fuse_axis, factor=self._block_tiling_vars.get(b_idx))

        # 4. reorder
        if self._ba_pattern_enable_reorder():
            sch[res].reorder(b_o, u_o, b_i, u_i)

        for i in range(u_idx + 1, len(res.op.axis)):
            inner_axes.append([res.op.axis[i], i])
            self._inner_shape.append([shape[i], i])

        if self._ba_pattern_enable_reorder():
            self._unfold_brd_compute_at_axis = u_o

        self._block_bind_axis = b_o
        self._compute_at_axis.compute_at_axis = b_i
        self._compute_at_axis_idx = u_idx
        self._emit_insn_axis.emit_insn_axis = inner_axes[0][0]

    def _do_tiling_static(self):
        self._do_tiling_one_cut()

    def _do_tiling_const(self):
        self._do_inplace_tiling()
        sch = self._schedule
        res = self._out
        shape = util.shape_to_list(res.shape)
        block_axes = []
        ub_axes = []
        inner_axes = []
        b_idx = self._block_split_axis
        u_idx = self._ub_split_axis

        if self._need_do_block:
            # 1. split ub first
            u_o, u_i = sch[res].split(res.op.axis[u_idx], factor=self._ub_factor)
            ub_axes.append([u_o, u_idx])
            inner_axes.append([u_i, u_idx])
            self._inner_shape.append([self._ub_factor, u_idx])
            for i in range(u_idx):
                block_axes.append([res.op.axis[i], i])
            block_axes.append([u_o, u_idx])

            # 2. fuse block axis
            block_fuse_axis = sch[res].fuse(*[x[0] for x in block_axes])

            # 3. spilt block
            b_o, b_i = sch[res].split(block_fuse_axis, factor=self._block_factor)

            for i in range(u_idx + 1, len(res.op.axis)):
                inner_axes.append([res.op.axis[i], i])
                self._inner_shape.append([shape[i], i])

            self._block_bind_axis = b_o
            self._compute_at_axis.compute_at_axis = b_i
            self._compute_at_axis_idx = u_idx
            self._emit_insn_axis.emit_insn_axis = inner_axes[0][0]
        else:
            self._emit_insn_axis.emit_insn_axis = res.op.axis[0]

    def _do_storage_bound(self):
        def _is_valid_broadcast_node(tensor):
            broadcast_valid_node = self._broadcast_tensors - self._compute_inline_tensors
            if tensor in broadcast_valid_node:
                return True

            for tensor_ori in broadcast_valid_node:
                if tensor == self._gm_tensor_cache_write_buffer_map.get(tensor_ori):
                    return True

            return False

        def is_aba_broadcast_pattern(tensor):
            if not _is_valid_broadcast_node(tensor):
                return False

            if not tensor.op.input_tensors:
                return False

            s_shape = util.shape_to_list(tensor.op.input_tensors[0].shape)
            d_shape = util.shape_to_list(tensor.shape)
            if len(d_shape) != 3 or len(s_shape) != 3:
                return False

            if s_shape[0] != d_shape[0] or s_shape[1] == d_shape[1] or s_shape[-1] != d_shape[-1]:
                return False

            if s_shape[0] == 1 or s_shape[1] != 1 or s_shape[-1] == 1:
                return False

            if d_shape[0] == 1 or d_shape[1] == 1 or d_shape[-1] == 1:
                return False

            return True

        def is_ab_broadcast_pattern(tensor):
            if not _is_valid_broadcast_node(tensor):
                return False

            if not tensor.op.input_tensors:
                return False

            s_shape = util.shape_to_list(tensor.op.input_tensors[0].shape)
            d_shape = util.shape_to_list(tensor.shape)
            if len(d_shape) != 2 or len(s_shape) != 2:
                return False

            if s_shape[0] != d_shape[0]:
                return False

            if s_shape[0] == 1 or s_shape[-1] != 1:
                return False

            if d_shape[0] == 1 or d_shape[-1] == 1:
                return False

            return True

        def calc_real_storage_bound(tensor, dst_shape):
            no_last_axis_soc_white_list = (ASCEND_910, ASCEND_310P)
            if get_soc_spec(SHORT_SOC_VERSION) not in no_last_axis_soc_white_list:
                return reduce(mul, dst_shape[self._ub_split_axis:], 1)

            _is_aba_broadcast = is_aba_broadcast_pattern(tensor)
            _is_ab_broadcast = is_ab_broadcast_pattern(tensor)

            if not _is_aba_broadcast and not _is_ab_broadcast:
                return reduce(mul, dst_shape[self._ub_split_axis:], 1)

            align_factor = DTYPE_BRC_ALIGN_MAPPING.get(tensor.dtype)
            if align_factor is None:
                return reduce(mul, dst_shape[self._ub_split_axis:], 1)

            if _is_aba_broadcast and self._ub_split_axis == 0 and dst_shape[self._ub_split_axis] != 1:
                real_shape = ((dst_shape[0] * dst_shape[1] + align_factor - 1) // \
                    align_factor * align_factor) * dst_shape[2]
            elif _is_ab_broadcast and self._ub_split_axis == 0 and dst_shape[self._ub_split_axis] != 1:
                real_shape = ((dst_shape[0] + align_factor - 1) // \
                    align_factor * align_factor) * dst_shape[1]
            else:
                real_shape = reduce(mul, dst_shape[self._ub_split_axis:], 1)

            return real_shape

        def calc_min_bound():
            min_bound = 0
            if const_set_storage_bound:
                for tensor_i in self._broadcast_tensors - self._compute_inline_tensors:
                    dst_shape = util.shape_to_list(tensor_i.shape)
                    if self._need_do_block:
                        dst_shape[self._ub_split_axis] = 1 if dst_shape[self._ub_split_axis] == 1 else self._ub_factor
                    if dst_shape[-1] != 1:
                        dst_shape[-1] = (dst_shape[-1] + ele_in_block - 1) // ele_in_block * ele_in_block
                    storage_bound = calc_real_storage_bound(tensor_i, dst_shape)
                    brc_size.add(storage_bound)
                if not brc_size:
                    return min_bound

                min_bound = min(ROW_LIMIT * ele_in_block * ele_in_block,
                                int(self._tensor_space // self._max_dtype_bytes))
                for _tensor in self._broadcast_tensors - self._compute_inline_tensors:
                    if not _tensor.op.input_tensors:
                        continue
                    s_shape = util.shape_to_list(_tensor.op.input_tensors[0].shape)
                    d_shape = util.shape_to_list(_tensor.shape)
                    if s_shape[-1] == 1 and d_shape[-1] != 1:
                        if s_shape[self._ub_split_axis] != 1:
                            s_shape[self._ub_split_axis] = self._ub_factor
                        shape_size = reduce(mul, s_shape[self._ub_split_axis:], 1)
                        row_size = ROW_LIMIT * ele_in_block
                        if self._is_vnchwconv_align:
                            row_factor = (shape_size + row_size - 1) // row_size
                        else:
                            row_factor = shape_size // row_size
                        min_bound = max(row_factor * row_size * ele_in_block, min_bound)
            return min_bound

        self._do_inplace_graph_bound()
        sch = self._schedule
        tensors = set(graph_tensor.ub_tensor
                      for graph_tensor in self._ori_graph_unify_tensors
                      if graph_tensor.ub_mode in (CACHE_READ_MODE, CACHE_WRITE_MODE, SET_SCOPE_MODE, CACHE_CLONE_MODE))

        output_shape = util.shape_to_list(self._out.shape)
        ele_in_block = self._ub_block_size // self._max_brc_bytes
        const_set_storage_bound = self._tiling_strategy == TilingStrategy.CONST and \
                                  (output_shape[-1] % ele_in_block == 0 or self._is_store_align)
        brc_size = set()
        min_bound = calc_min_bound()

        cst_before_ub_tensor = self._get_all_ub_tensors(self._cst_compute_root_tensors)
        dyn_before_ub_tensor = \
            self._get_all_ub_tensors(self._dyn_compute_root_tensors) | self._dyn_compute_root_tensors
        for tensor_i in tensors:
            storage_bound = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            if tensor_i in dyn_before_ub_tensor:
                sch[tensor_i].set_buffer_size(self._compute_root_helper.compute_root_size)
                continue
            if tensor_i in cst_before_ub_tensor or tensor_i in self._cst_compute_root_tensors:
                continue
            if const_set_storage_bound:
                reused_brd_src = None
                dst_shape = util.shape_to_list(tensor_i.shape)
                use_tensor = list(self._in_out_map.get(self._get_ori_tensor(tensor_i), []))
                if len(use_tensor) == 1 and self._broadcast_by_no_other_use.get(self._get_ori_tensor(use_tensor[0])):
                    dst_shape = util.shape_to_list(use_tensor[0].shape)
                    reused_brd_src = use_tensor[0]
                ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
                if self._need_do_block:
                    dst_shape[self._ub_split_axis] = 1 if dst_shape[self._ub_split_axis] == 1 else self._ub_factor
                if dst_shape[-1] != 1:
                    dst_shape[-1] = (dst_shape[-1] + ele_in_block - 1) // ele_in_block * ele_in_block

                if reused_brd_src is not None:
                    real_storage_bound = calc_real_storage_bound(reused_brd_src, dst_shape)
                else:
                    real_storage_bound = calc_real_storage_bound(tensor_i, dst_shape)

                if real_storage_bound in brc_size and real_storage_bound < min_bound:
                    real_storage_bound = min_bound
                if real_storage_bound % ele_in_block != 0:
                    real_storage_bound = (real_storage_bound + ele_in_block - 1) // ele_in_block * ele_in_block
                if self._brc_avoid_bank_conflict:
                    extent_size = int((ROW_LIMIT * self._ub_block_size) // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
                    real_storage_bound += extent_size
                if cast_ints2ints_need_alignment(tensor_i):
                    extra_ratio = DTYPE_BYTE_MAPPING.get(tensor_i.op.input_tensors[0].dtype) \
                                // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
                    real_storage_bound = extra_ratio * real_storage_bound

                storage_bound = min(storage_bound, int(real_storage_bound))
            sch[tensor_i].set_buffer_size(storage_bound)

    def _do_compute_inline(self):
        sch = self._schedule
        for tensor_i in self._compute_inline_tensors:
            sch[tensor_i].compute_inline()

    def _do_storage_align(self):
        sch = self._schedule
        before_ub_tensor = self._get_all_ub_tensors(self._before_broadcast_tensors)
        elewise_ub_tensor = self._get_all_ub_tensors(self._elewise_graph_tensors)
        dyn_compute_root_before_ub_tensor = \
            self._get_all_ub_tensors(self._dyn_compute_root_tensors) | self._dyn_compute_root_tensors

        storage_align_set = set()
        storage_align_op_set = set()
        for tensor_i in self._store_align_tensors:
            align_factor = int(self._ub_block_size // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            src_shape = util.shape_to_list(tensor_i.shape)
            if len(src_shape) - self._ub_split_axis == 1:
                continue
            if tensor_i in dyn_compute_root_before_ub_tensor:
                continue
            if tensor_i in self._elewise_graph_tensors or tensor_i in elewise_ub_tensor:
                continue
            if tensor_i in self._before_broadcast_tensors or tensor_i in before_ub_tensor:
                if src_shape[-1] == 1:
                    continue
                align_factor = tvm.select(src_shape[-1] == 1, 1, align_factor)
                if self._get_ori_tensor(tensor_i) not in self._input_tensors:
                    self._before_broadcast_axis_group_tensors.add(tensor_i)
            sch[tensor_i].storage_align(tensor_i.op.axis[-2], align_factor, 0)
            storage_align_set.add(sch[tensor_i])
            storage_align_op_set.add(sch[tensor_i].op)

        def _check_storage_align_valid():
            if self._tiling_strategy != TilingStrategy.CONST:
                return

            for storage_align_tensor in storage_align_set:
                # after cache read input tensors src may change to local.ub
                # so we should compare stage and input tensor using op info.
                producers = storage_align_tensor.op.input_tensors
                if len(producers) <= 1:
                    continue
                for producer in producers:
                    producer_ub = self._get_ub_tensor(producer)
                    if producer_ub.op in storage_align_op_set:
                        continue
                    producer_ub_align_factor = int(self._ub_block_size // DTYPE_BYTE_MAPPING.get(producer_ub.dtype))
                    producer_ub_shape = util.shape_to_list(producer_ub.shape)
                    if producer_ub_shape[-1] != 1 and producer_ub_shape[-1] % producer_ub_align_factor != 0:
                        raise RuntimeError("Storage align failed, not support fusion elewise subgraph to "
                                           "a broadcast graph with an element or one of its consumers intersetion. "
                                           "Please check compute graph.")
        _check_storage_align_valid()

    def _do_axis_group(self):

        def do_axis_group_by_group_id(ub_tensor, group_id):
            begin_axis = self._ub_split_axis
            if self._get_ori_tensor(ub_tensor) in self._cst_compute_root_tensors | self._dyn_compute_root_tensors:
                begin_axis = 0
            for axis_id in range(begin_axis, len(ub_tensor.shape)):
                sch[ub_tensor].pragma(ub_tensor.op.axis[axis_id], "axis_group", group_id)

        sch = self._schedule
        if self._enable_inplace_cache_clone or self._enable_compute_root and ScheduleUtil.is_mov_align_sch():
            return
        before_broadcast_group_id = tvm.call_extern("int32", "axis_group", 0, "append")
        compute_root_not_input_tensors = \
            [tensor for tensor in self._dyn_compute_root_tensors if tensor not in self._input_tensors]
        self._before_broadcast_axis_group_tensors.update(self._get_all_ub_tensors(compute_root_not_input_tensors))
        for tensor_i in self._before_broadcast_axis_group_tensors:
            do_axis_group_by_group_id(tensor_i, before_broadcast_group_id)

        # pure elewise dsl should compute continuously, except for storage_align
        elewise_axis_group_id = tvm.call_extern("int32", "axis_group", 0, "overwrite")
        for tensor_i in self._elewise_axis_group_tensors:
            do_axis_group_by_group_id(tensor_i, elewise_axis_group_id)

        if ScheduleUtil.is_mov_align_sch():
            return

        input_ub_tensors = self._get_all_ub_tensors(self._input_tensors)
        for tensor_i in input_ub_tensors:
            do_axis_group_by_group_id(tensor_i, before_broadcast_group_id)

    def _do_mem_unique(self):
        sch = self._schedule
        if self._enable_compute_root:
            for tensor in self._compute_root_helper.mem_unique_tensors:
                sch[self._get_ub_tensor(tensor)].mem_unique()

    def _do_multi_core(self):
        if self._block_bind_axis is not None:
            self._schedule[self._out].bind(self._block_bind_axis, self._block_var)
        if self._enable_inplace_cache_clone and self._inplace_bind_axis is not None:
            self._schedule[self._cache_clone_helper.inplace_out].bind(self._inplace_bind_axis, self._inplace_block_var)

    def _do_ub_align(self):
        pass

    def _do_remove_pad(self):
        pass

    def _do_compute_at(self):
        sch = self._schedule

        _unfold_broadcast_tensors_ub = self._get_all_ub_tensors(self._unfold_broadcast_tensors)

        for tensor_i, param in self._compute_at_map.items():
            if tensor_i in _unfold_broadcast_tensors_ub:
                sch[tensor_i].compute_at(sch[param[0]], self._unfold_brd_compute_at_axis)
            else:
                sch[tensor_i].compute_at(sch[param[0]], param[1].compute_at_axis)

    def _do_store_predicate(self):

        def calc_predicate_condition(tensor, ub_split_src):
            u_idx = self._ub_split_axis
            b_idx = self._block_split_axis
            ub_factor = None
            block_factor = None
            ub_out_shape = None

            if self._tiling_strategy == TilingStrategy.CONST:
                ub_factor = self._ub_factor
                block_factor = self._block_factor
            else:
                ub_factor = self._ub_tiling_vars.get(u_idx)
                block_factor = self._block_tiling_vars.get(b_idx)
            ub_out_shape = tvm.floordiv(self._out.shape[u_idx] - 1, ub_factor) + 1

            if self._enable_db:
                return tvm.any(
                    (self._block_bind_axis * block_factor + self._compute_at_axis.compute_at_axis) % ub_out_shape < 2,
                    self._compute_at_axis.compute_at_axis < 2,
                    ub_split_src != 1)
            return tvm.any(
                (self._block_bind_axis * block_factor + self._compute_at_axis.compute_at_axis) % ub_out_shape < 1,
                self._compute_at_axis.compute_at_axis < 1,
                ub_split_src != 1)

        sch = self._schedule
        u_idx = self._ub_split_axis
        for tensor_i in self._broadcast_store_predicate:
            if tensor_i in self._cst_compute_root_tensors:
                continue
            if tensor_i.op.tag == "elewise_binary_powi":
                continue
            input_tensors = tensor_i.op.input_tensors
            is_vreduce_tensor = len(input_tensors) > 0 and util.is_broadcast(input_tensors[0]) \
                and tensor_i in self._remove_pad_cache_read_buffer
            if util.is_broadcast(tensor_i):
                ub_split_src = tensor_i.op.input_tensors[0].shape[u_idx]
            elif is_vreduce_tensor:
                ub_split_src = tensor_i.op.input_tensors[0].op.input_tensors[0].shape[u_idx]
            else:
                ub_split_src = tensor_i.shape[u_idx]
            cond = calc_predicate_condition(tensor_i, ub_split_src)
            sch[self._get_ub_tensor(tensor_i)].set_store_predicate(cond)
            sch[self._get_ub_tensor(tensor_i)].mem_unique()
        for tensor_i in self._all_pre_node_broadcast:
            if tensor_i in self._cst_compute_root_tensors:
                continue
            if tensor_i.op.tag == "elewise_binary_powi":
                continue
            if util.is_broadcast(tensor_i) and not util.is_scalar_broadcast(tensor_i):
                ub_split_src = tensor_i.op.input_tensors[0].shape[u_idx]
            else:
                ub_split_src = tensor_i.shape[u_idx]
            cond = calc_predicate_condition(tensor_i, ub_split_src)
            sch[self._get_ub_tensor(tensor_i)].set_store_predicate(cond)
            if tensor_i in self._middle_out_tensors:
                sch[tensor_i].set_store_predicate(cond)
        for tensor_i in self._store_predicate_common_tensors:
            if tensor_i in self._gm_tensor_cache_read_buffer_map:
                sch[self._gm_tensor_cache_read_buffer_map.get(tensor_i)].mem_unique()
            else:
                sch[tensor_i].mem_unique()

        if self._enable_compute_root:
            for tensor_i in self._compute_root_helper.block_one_out_tensors:
                # When schedule use single core, it's not necessary to use block one condition
                if self._block_bind_axis is not None:
                    sch[tensor_i].set_store_predicate(self._block_var < 1)

    def _do_double_buffer(self):
        if self._enable_db:
            sch = self._schedule
            tensors = set(graph_tensor.ub_tensor
                          for graph_tensor in self._ori_graph_unify_tensors
                          if graph_tensor.ub_mode in
                          (CACHE_READ_MODE, CACHE_WRITE_MODE, SET_SCOPE_MODE, CACHE_CLONE_MODE))
            before_ub_tensor = self._get_all_ub_tensors(self._cst_compute_root_tensors)
            for tensor_i in tensors:
                if not (tensor_i in before_ub_tensor or tensor_i in self._cst_compute_root_tensors):
                    sch[tensor_i].double_buffer()

    def _do_mem_reuse(self):
        sch = self._schedule
        for _a, _b in self._mem_reuse_map.items():
            for b_i in _b:
                sch[_a].reused_by(b_i)

    def _do_constraints(self):
        sch = self._schedule
        for cond in self._constraints:
            sch.set_constraint(cond)

    def _get_ori_tensor(self, tensor_i, is_ori_graph=True):
        """
        get gm tensor from ub tensor
        @param tensor_i:
        @param is_ori_graph:
        @return:
        """
        for _graph_tensor in self._ori_graph_unify_tensors:
            if _graph_tensor.ub_tensor == tensor_i:
                return _graph_tensor.gm_tensor
        for _graph_tensor in self._inplace_graph_unify_tensors:
            if _graph_tensor.ub_tensor == tensor_i:
                return _graph_tensor.gm_tensor
        return tensor_i

    def _do_emit_insn(self):

        def dfs_path(tensor, visited_tensors, out_i, outs_path_map):
            for tensor_i in tensor.op.input_tensors:
                if tensor_i in visited_tensors:
                    continue
                outs_path_map.get(out_i, set()).add(tensor_i)
                visited_tensors.add(tensor_i)
                dfs_path(tensor_i, visited_tensors, out_i, outs_path_map)

        def calc_out_path(outs):
            outs_path_map = {}
            for out_i in outs:
                if out_i in self._dyn_compute_root_tensors:
                    continue
                path_i = {out_i}
                visited_tensors = {out_i}
                outs_path_map[out_i] = path_i
                dfs_path(out_i, visited_tensors, out_i, outs_path_map)
            return outs_path_map

        def calc_max_out_intersect_num(tensor, outs_path_map):
            current_path = outs_path_map.get(tensor, set())
            max_out_intersect_num = 0
            for out, path in outs_path_map.items():
                if out == tensor:
                    continue
                common_path = current_path.intersection(path)
                max_out_intersect_num = max(max_out_intersect_num, len(common_path))
            return max_out_intersect_num

        sch = self._schedule
        for tensor_i, param in self._emit_insn_map.items():
            emit_insn_axis = param[0]
            if isinstance(emit_insn_axis, self.EmitInsn):
                emit_insn_axis = emit_insn_axis.emit_insn_axis
            if len(param) > 2:
                sch[tensor_i].emit_insn(emit_insn_axis, param[2])
            attrs = {}
            tensor_bound = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            ele_in_block = self._ub_block_size // DTYPE_BYTE_MAPPING.get(tensor_i.dtype)
            last_dim_threshold = int(tensor_bound // (ele_in_block * ROW_LIMIT) // ele_in_block * ele_in_block)
            if util.is_v220() and param[1] in ["vector_add", "vector_sub"]:
                attrs["parallel_int64"] = 1
            if param[1] in (VECTOR_BROADCAST, UNKNOWN_BROADCAST):
                src_shapes = []
                if param[1] == UNKNOWN_BROADCAST:
                    if self._tiling_strategy == TilingStrategy.NONE_CUT:
                        src_shapes = tensor_i.op.input_tensors[0].shape
                    else:
                        u_idx = self._ub_split_axis
                        src_shapes = tensor_i.op.input_tensors[0].shape[u_idx + 1:]
                        if self._ub_factor_is_one:
                            src_shapes.append(1)
                        else:
                            src_shapes.append(self._ub_factor)
                    is_all_const = all(isinstance(s, int) for s in util.shape_to_list(src_shapes))
                    if is_all_const:
                        param[1] = VECTOR_BROADCAST

                # when brc source tensor shape is all one, need fuse all axis, avoid vnchw_conv brc exceed storage_bound
                if not src_shapes and tensor_i.op.input_tensors:
                    src_shapes = tensor_i.op.input_tensors[0].shape
                is_all_one = src_shapes and all(expr_equal(s, 1) for s in util.shape_to_list(src_shapes))
                last_dim_threshold = 0 if is_all_one else last_dim_threshold

                attrs = {"dynamic_fuse": False,
                         "dynamic_split": False}
                if not ScheduleUtil.is_mov_align_sch() or self._enable_vdup_align:
                    attrs["enable_align_broadcast"] = True

                if self._without_temp_buffer():
                    attrs["without_temp_buffer"] = True

                enough_buffer = self._emit_insn_attr_map.get(ENOUGH_BUFFER)
                if enough_buffer is not None:
                    attrs["enough_buffer"] = enough_buffer
                if self._broadcast_by_no_other_use.get(self._get_ori_tensor(tensor_i)):
                    attrs["reuse_src_tensor"] = True
                if not self._is_store_align or self._enable_vdup_align:
                    attrs["fuse_axis_threshold"] = last_dim_threshold
                if self._brc_avoid_bank_conflict:
                    attrs["avoid_bank_conflict"] = True
                if self._enable_vdup_align:
                    attrs["last_dup_threshold"] = BRC_LAST_DUP_LIMIT_VDUP_ALIGN
                else:
                    attrs["last_dup_threshold"] = BRC_LAST_DUP_LIMIT
                if tensor_i in self._compute_align_map:
                    attrs["last_src_valid_element"] = self._compute_align_map.get(tensor_i)[1]
            elif tensor_i in self._out_tensors:
                if self._is_one_dim:
                    attrs = {"no_overlap": 0}
                elif self._is_pure_brc_common_db:
                    attrs = {"no_overlap": LAST_LOOP_REG_MOV}
                elif self._is_store_align and self._ub_split_axis != len(self._out.shape) - 1:
                    outs_path_map = calc_out_path(self._out_tensors)
                    max_out_intersect_num = calc_max_out_intersect_num(tensor_i, outs_path_map)
                    attrs = {"no_overlap": MISSALIGN_STRIDE_WHITH_MALLOC_BUF, "no_overlap_malloc_buf_for_tail": 1}
                    out_ub_tensor = self._gm_tensor_cache_write_buffer_map.get(tensor_i)
                    none_reuse_tensors = self._broadcast_store_predicate.union(self._store_predicate_common_tensors)
                    none_reuse_ub_tensors = self._get_all_ub_tensors(none_reuse_tensors)
                    predicate_nodes = len(none_reuse_tensors)
                    real_coexisting_quantity = self._coexisting_quantity - predicate_nodes - max_out_intersect_num
                    if real_coexisting_quantity < LAST_LOOP_COEXISTING_QUANTITY_LIMIT:
                        attrs = {"no_overlap": LAST_LOOP_REG_MOV}
                    elif real_coexisting_quantity < MISSALIGN_STRIDE_COEXISTING_QUANTITY_LIMIT and \
                            out_ub_tensor not in none_reuse_ub_tensors:
                        attrs = {"no_overlap": MISSALIGN_STRIDE_WHITH_MALLOC_BUF, "no_overlap_malloc_buf_for_tail": 0}
                else:
                    attrs = {"no_overlap": "default"}
            if param[1] == "vector_select_bool" and util.is_nano() and "_mode" in tensor_i.op.attrs:
                attrs = {"mode": tensor_i.op.attrs.get("_mode")}
            if param[1] == "vector_dup":
                attrs = {"trans_assign_opt": True}
            if tensor_i in self._const_brc_inline_tensor:
                attrs["use_ba_pattern_brc"] = True
            if param[1].startswith("vector_conv") and tensor_i in self._compute_align_map:
                attrs["dst_align_with_validity"] = True
                attrs["align_axes_value"] = tvm.call_intrin("handle", "tir.tvm_tuple", *[tensor_i.shape[-1]])
            if tensor_i.dtype == "int64" and param[1] in ("vector_max", "vector_min"):
                attrs = {"parallel_int64": 1}
            sch[tensor_i].emit_insn(emit_insn_axis, param[1], attrs)

    def _add_compile_info(self):
        before_node_nums = operation.get_context().get("_node_nums") or 0
        current_node_nums = max(len(self._middle_tensors), before_node_nums)
        operation.get_context().add("_node_nums", current_node_nums)
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()
        if self._mode == CONST_MODE:
            # const shape: one compute, one schedule
            cpt_compute.add("_broadcast_const_block_dim", self._block_dims)
        else:
            cpt_schedule.add(CompileInfo.MAX_DTYPE, self._max_dtype_bytes)
            cpt_schedule.add(CompileInfo.COEXISTING_QUANTITY, self._coexisting_quantity)
            cpt_schedule.add(CompileInfo.UB_SIZE, self._ub_size)
            cpt_schedule.add(CompileInfo.CORE_NUM, util.get_core_num())
            cpt_schedule.add("_tiling_key", self._schedule.tiling_key)

        operation.add_compile_info_inner(CompileInfo.UB_FACTOR_ALIGN, self._ub_factor_align)
        operation.add_compile_info_inner(CompileInfo.IS_VNCHWCONV_ALIGN, self._is_vnchwconv_align)
        operation.add_compile_info_inner(CompileInfo.MAX_ELE_IN_BLOCK, self._max_ele_in_block)
        operation.add_compile_info_inner(CompileInfo.UB_BLOCK_SIZE, self._ub_block_size)
        operation.get_compile_info().update(self._calc_inplace_clone_compiler_info())
        operation.get_compile_info().update(self._calc_compute_root_compiler_info())
        cpt_schedule.add(CompileInfo.MAX_BRC_TYPE, self._max_brc_bytes)

    def _check_tiling_case(self):
        def _check_inplace_tiling_case():
            if self._enable_inplace_cache_clone:
                # ub factor must be smaller than ub axis
                inplace_shape = util.shape_to_list(self._cache_clone_helper.inplace_out.shape)
                if inplace_shape[self._inplace_graph_ub_axis] == 1:
                    return False
            return True

        if self._tiling_strategy == TilingStrategy.CONST:
            return True
        if not _check_inplace_tiling_case():
            return False
        lower_bound = 1
        under_ub_len = len(self._inner_shape)
        ele_in_block = int(self._ub_block_size // self._max_dtype_bytes)
        for index, item in enumerate(self._inner_shape[::-1]):
            cur_bound = util.get_bound(item[0])[0]
            if cur_bound is None:
                return False
            if index == 0 and under_ub_len != 1 and cur_bound % ele_in_block != 0:
                cur_bound = (cur_bound + ele_in_block - 1) // ele_in_block * ele_in_block
            if index == 0 and under_ub_len != 1 and not self._is_store_align and not ScheduleUtil.is_mov_align_sch():
                tensor_bound = int(self._tensor_space // self._max_dtype_bytes)
                last_dim_threshold = int(tensor_bound // (ele_in_block * ROW_LIMIT) / ele_in_block * ele_in_block)
                l_bound, u_bound = util.get_bound(item[0])
                l_bound_align = (l_bound + ele_in_block - 1) // ele_in_block * ele_in_block
                if l_bound > last_dim_threshold and l_bound_align > u_bound:
                    return False
            lower_bound *= cur_bound
        if not self._tensor_space // self._max_dtype_bytes >= lower_bound:
            return False
        return True

    def __calc_tensor_space(self):
        one_dim_align = self._one_dim_align
        # minus tmp size
        self._correct_factor = 2 if self._enable_db else 1
        tmp_ub_size = \
            self._compute_root_tmp_ub_size + self._correct_factor * (self._tmp_ub_size - self._compute_root_tmp_ub_size)
        self._ub_size -= tmp_ub_size
        tensor_space = self._ub_size // self._coexisting_quantity
        if self._enable_db:
            tensor_space = self._ub_size // 2 // self._coexisting_quantity
        self._tensor_space = tensor_space // self._ub_block_size * self._ub_block_size

        # adjust storage bound by tiling handle one dim (128 align)
        if self._is_one_dim and self._tensor_space > one_dim_align \
                and get_soc_spec(SHORT_SOC_VERSION) not in (ASCEND_910B, ASCEND_910_93):
            self._tensor_space = self._tensor_space // one_dim_align * one_dim_align

        tensors = set(graph_tensor.ub_tensor
                      for graph_tensor in self._ori_graph_unify_tensors
                      if graph_tensor.ub_mode in (CACHE_READ_MODE, CACHE_WRITE_MODE, SET_SCOPE_MODE, CACHE_CLONE_MODE))

        min_storage_bound = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(self._out.dtype))
        for tensor_i in tensors:
            storage_bound = int(self._tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            if storage_bound < min_storage_bound:
                min_storage_bound = storage_bound
        self._min_storage_bound = min_storage_bound

    def __dfs_sub_graph(self, out, visited_tensors: set):
        for tensor_i in out.op.input_tensors:
            util.merge_value(self._in_out_map, tensor_i, out)
            self._dtypes.add(tensor_i.dtype)

            if util.is_placeholder(tensor_i):
                self._input_tensors.add(tensor_i)
            else:
                self._middle_tensors.add(tensor_i)
                if util.is_broadcast(tensor_i):
                    self._broadcast_tensors.add(tensor_i)
                if util.is_one_shape_broadcast(tensor_i):
                    self._one_shape_broadcast_tensors.add(tensor_i)

            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self.__dfs_sub_graph(tensor_i, visited_tensors)

    def _calc_set_value(self):
        if operation.get_context().get_current_compute().get("_is_fractal_format"):
            self._5hd_actions = padding.calc_padding(self._outs)

            if self._5hd_actions is not None and len(self._5hd_actions) > 0:
                operation.add_compile_info_inner(CompileInfo.CONTAINS_NEED_PAD_COMPUTE, True)

    def _do_set_value(self):
        if self._5hd_actions is not None and len(self._5hd_actions) > 0:
            for action in self._5hd_actions:
                action_type = action.get_action_type()
                tensor = action.get_tensor()
                conditon = action.get_condition()
                value = action.get_value()
                target_tensors = action.get_target_tensors()
                value_type = action.get_value_type()

                if value_type == ActionValueType.TENSOR:
                    value = value(self._get_ub_tensor(tensor))

                if action_type == ActionType.SET_VALUE:
                    self._schedule[self._get_ub_tensor(tensor)].set_value(conditon, value)
                elif action_type == ActionType.CACHE_READ_AND_SET_VALUE:
                    set_value_cache_read_buffer = self._schedule.cache_read(tensor, self._scope, target_tensors)
                    # add dma copy emit insn
                    self._emit_insn_map[set_value_cache_read_buffer] = \
                        [set_value_cache_read_buffer.op.axis[0], DMA_COPY]
                    self._schedule[set_value_cache_read_buffer].set_value(conditon, value)

    def _calc_axis_group(self):
        def _calc_no_need_axis_group_tensor(elewise_tensors):
            # broadcast ba pattern inline tensor, cannot use overwrite axis_grop
            no_need_axis_group_tensor = self._const_brc_inline_tensor

            # compute align and consume broadcast inline tensors, cannot use overwrite axis_grop
            if not ScheduleUtil.is_mov_align_sch():
                return no_need_axis_group_tensor
            for _tensor in elewise_tensors:
                producers = _tensor.op.input_tensors
                is_producer_inline = any(producer in self._compute_inline_tensors for producer in producers)
                if is_producer_inline:
                    no_need_axis_group_tensor.add(_tensor)
            return no_need_axis_group_tensor

        if self._is_store_align or self._tiling_strategy == TilingStrategy.CONST or self._enable_inplace_cache_clone:
            return

        all_elewise_tensors = GraphUtil.calc_elewise_tensors(self._outs)
        no_need_axis_group_tensors = _calc_no_need_axis_group_tensor(all_elewise_tensors)
        all_elewise_continuous_tensors = \
            [tensor for tensor in all_elewise_tensors if tensor not in no_need_axis_group_tensors]
        all_elewise_ub_tensors = self._get_all_ub_tensors(all_elewise_continuous_tensors)
        self._elewise_axis_group_tensors.update(all_elewise_ub_tensors)

    def _create_schedule(self):
        """
        cache clone schedule need more than one output op
        @return:
        """
        schedule = tvm.create_schedule(self._out.op)
        if not self._enable_inplace_cache_clone:
            return schedule
        return tvm.create_schedule([self._cache_clone_helper.inplace_out.op, self._out.op])

    def _do_cache_clone(self):
        def calc_clone_consumer(tensor):
            gm_consumer = self._cache_clone_helper.cache_clone_producer_consumer_map.get(_tensor, set())
            # cache clone start tensor consumer maybe contains clone_buffer and gm_tensor
            cur_clone_consumer = set()
            for tensor_i in gm_consumer:
                if tensor_i in self._gm_tensor_and_clone_buffer_map:
                    cur_clone_consumer.add(self._gm_tensor_and_clone_buffer_map.get(tensor_i))
                    continue
                cur_clone_consumer.add(tensor_i)
            return cur_clone_consumer

        if self._enable_inplace_cache_clone:
            # middle tensors need cache clone
            for _tensor in self._cache_clone_helper.cache_clone_middle_tensors:
                clone_consumer = calc_clone_consumer(_tensor)
                clone_buffer = self._schedule.cache_clone(_tensor, self._scope, clone_consumer)
                self._cache_clone_buffers.add(clone_buffer)
                self._gm_tensor_and_clone_buffer_map[_tensor] = clone_buffer

            # input tensors need cache_read, cache_clone not support placeholder op
            for _tensor in self._cache_clone_helper.cache_clone_input_tensors:
                clone_consumer = calc_clone_consumer(_tensor)
                clone_buffer = self._schedule.cache_read(_tensor, self._scope, clone_consumer)
                self._cache_clone_buffers.add(clone_buffer)
                self._gm_tensor_and_clone_buffer_map[_tensor] = clone_buffer

    def _calc_inplace_clone_compiler_info(self):
        """
        calc inplace storage bound, inplace out indexes
        @return:
        """
        inplace_compile_info = {}
        if self._enable_inplace_cache_clone:
            db_tmp_ub_size = self._compute_root_tmp_ub_size + 2 * (self._tmp_ub_size - self._compute_root_tmp_ub_size)

            inplace_storage_bound = (self._ub_size - db_tmp_ub_size) // \
                                    self._cache_clone_helper.inplace_coexisting_quantity // \
                                    self._cache_clone_helper.inplace_max_dtype_bytes
            inplace_out_indexes = list(self._cache_clone_helper.inplace_out_indexes)
            inplace_out_volume = self._cache_clone_helper.inplace_out_volume
            inplace_graph_out_indexes = list(self._cache_clone_helper.inplace_graph_out_indexes)
            inplace_compile_info[CompileInfo.HAS_INPLACE_CACHE_CLONE] = True

            inplace_storage_bound = min(operation.get_compile_info()
                .get(CompileInfo.INPLACE_STORAGE_BOUND, inplace_storage_bound), inplace_storage_bound)
            inplace_compile_info[CompileInfo.INPLACE_STORAGE_BOUND] = inplace_storage_bound
            inplace_compile_info[CompileInfo.INPLACE_OUT_INDEXES] = inplace_out_indexes
            inplace_compile_info[CompileInfo.INPLACE_OUT_VOLUME] = inplace_out_volume
            inplace_compile_info[CompileInfo.INPLACE_GRAPH_ALL_OUT_INDEXES] = inplace_graph_out_indexes

        return inplace_compile_info

    def _calc_compute_root_compiler_info(self):
        """
        calc compute root out indexes, is_contains_compute_tensors
        @return:
        """
        compute_root_compile_info = {}
        cur_compile_info = operation.get_compile_info()

        if self._enable_compute_root and not cur_compile_info.get(CompileInfo.COMPUTE_ROOT_OUT_IDEX):
            compute_root_compile_info[CompileInfo.COMPUTE_ROOT_OUT_IDEX] = list(
                self._compute_root_helper.compute_root_out_index)

        is_contains_compute_root_tensors = len(self._cst_compute_root_tensors) > 0 or self._enable_compute_root
        if is_contains_compute_root_tensors and not cur_compile_info.get(CompileInfo.COMPUTE_ROOT_OUT_IDEX):
            compute_root_compile_info[CompileInfo.IS_CONTAINS_COMPUTE_ROOT_TENSORS] = is_contains_compute_root_tensors

        if self._enable_compute_root and not cur_compile_info.get(CompileInfo.COMPUTE_ROOT_VOLUME) and \
                self._only_has_tensor_volume:
            compute_root_compile_info[CompileInfo.COMPUTE_ROOT_VOLUME] = self._compute_root_helper.compute_root_size

        if len(self._cst_compute_root_tensors) > 0 and ScheduleUtil.is_fusion_op():
            compute_root_compile_info[CompileInfo.IS_FUSION_OP] = True

        if len(self._cst_compute_root_tensors) > 0 and self._cst_compute_root_last_brc_cut_last:
            compute_root_compile_info[CompileInfo.CST_COMPUTE_ROOT_LAST_BRC_CUT_LAST] = True

        return compute_root_compile_info

    def _do_inplace_tiling(self):
        """
        inplace graph tiling: split ub firstly, then split block secondly
        """
        if not self._enable_inplace_cache_clone:
            return

        inplace_out = self._cache_clone_helper.inplace_out
        inplace_out_axes = inplace_out.op.axis
        if (self._tiling_strategy == TilingStrategy.CONST and not self._need_do_block) or \
                self._tiling_strategy == TilingStrategy.NONE_CUT:
            self._inplace_emit_insn.emit_insn_axis = inplace_out_axes[-1]
            return

        sch = self._schedule
        # split ub
        inplace_u_o, inplace_u_i = sch[inplace_out].split(inplace_out_axes[self._inplace_graph_ub_axis],
                                                          factor=self._inplace_graph_ub_factor)
        # fuse block axis
        block_axes = []
        for i in range(self._inplace_graph_ub_axis):
            block_axes.append(inplace_out.op.axis[i])
        block_axes.append(inplace_u_o)
        block_fuse_axis = sch[inplace_out].fuse(*[x for x in block_axes])
        # split block
        inplace_b_o, inplace_b_i = sch[inplace_out].split(block_fuse_axis, factor=self._inplace_graph_block_factor)

        self._inplace_compute_at.compute_at_axis = inplace_b_i
        self._inplace_emit_insn.emit_insn_axis = inplace_u_i
        self._inplace_bind_axis = inplace_b_o

        for tensor_i in self._cache_clone_helper._inplace_outs:
            buffer_tile_list = []
            for j in range(len(tensor_i.shape)):
                if j == self._inplace_graph_ub_axis:
                    extent_ub_tail = tensor_i.shape[j] - self._inplace_graph_ub_factor * (
                        self._inplace_graph_block_factor * inplace_b_o + inplace_b_i)
                    buffer_tile_list.append((None, tvm.min(self._inplace_graph_ub_factor, extent_ub_tail)))
                else:
                    buffer_tile_list.append((None, None))
            sch[tensor_i].buffer_tile(*buffer_tile_list)

    def _do_inplace_graph_bound(self):
        if self._enable_inplace_cache_clone:
            inplace_ub_tensors = set(graph_tensor.ub_tensor
                                     for graph_tensor in self._inplace_graph_unify_tensors
                                     if graph_tensor.ub_mode in (CACHE_READ_MODE, CACHE_WRITE_MODE, SET_SCOPE_MODE))
            inplace_buffer_size = self._ub_size // self._cache_clone_helper.inplace_coexisting_quantity
            for _tensor in inplace_ub_tensors:
                dtype_bytes = DTYPE_BYTE_MAPPING.get(_tensor.dtype)
                inplace_storage_bound = int(inplace_buffer_size // dtype_bytes)
                self._schedule[_tensor].set_buffer_size(inplace_storage_bound)

    def _calc_unify_graph_tensors(self):
        def _calc_inplace_graph_unify_tensors():
            if self._enable_inplace_cache_clone:
                for _tensor in self._cache_clone_helper.inplace_graph_tensors:
                    if _tensor in self._cache_clone_helper.inplace_input_tensors:
                        unify_tensor = UnifyGraphTensor(self._gm_tensor_cache_read_buffer_map.get(_tensor))
                        unify_tensor.build_tensor(_tensor, INPLACE_GRAPH_TENSOR, INPUT_TYPE, CACHE_READ_MODE)
                        self._inplace_graph_unify_tensors.add(unify_tensor)

                    if _tensor in self._cache_clone_helper.inplace_middle_tensors - \
                            self._cache_clone_helper.inplace_graph_out_tensors:
                        unify_tensor = UnifyGraphTensor(_tensor)
                        unify_tensor.build_tensor(_tensor, INPLACE_GRAPH_TENSOR, PURE_MIDDLE_TYPE, SET_SCOPE_MODE)
                        self._inplace_graph_unify_tensors.add(unify_tensor)

                    if _tensor in self._cache_clone_helper.inplace_graph_out_tensors:
                        tensor_type = PURE_OUT_TYPE
                        if _tensor in self._cache_clone_helper.inplace_middle_tensors:
                            tensor_type = MIDDLE_OUT_TYPE
                        unify_tensor_write = UnifyGraphTensor(self._gm_tensor_cache_write_buffer_map.get(_tensor))
                        unify_tensor_write.build_tensor(_tensor, INPLACE_GRAPH_TENSOR, tensor_type, CACHE_WRITE_MODE)
                        self._inplace_graph_unify_tensors.add(unify_tensor_write)

                        unify_tensor = UnifyGraphTensor(_tensor)
                        unify_tensor.build_tensor(_tensor, INPLACE_GRAPH_TENSOR, tensor_type, GM_MODE)
                        self._inplace_graph_unify_tensors.add(unify_tensor)

                        if _tensor in self._cache_clone_helper.inplace_middle_tensors:
                            unify_tensor_read = UnifyGraphTensor(self._gm_tensor_cache_read_buffer_map.get(_tensor))
                            unify_tensor_read.build_tensor(_tensor, INPLACE_GRAPH_TENSOR, tensor_type, CACHE_READ_MODE)
                            self._inplace_graph_unify_tensors.add(unify_tensor_read)

                if len(self._cache_clone_helper.inplace_graph_out_tensors -
                       self._cache_clone_helper.inplace_middle_tensors) > 1:
                    unify_tensor = UnifyGraphTensor(self._cache_clone_helper.inplace_out)
                    unify_tensor.build_tensor(self._cache_clone_helper.inplace_out, INPLACE_GRAPH_TENSOR,
                                              FAKE_NODE_TYPE,
                                              GM_MODE)
                    self._inplace_graph_unify_tensors.add(unify_tensor)

        def _calc_ori_graph_unify_tensors():
            # calc original graph input tensors
            for _tensor in self._input_tensors:
                if self._enable_inplace_cache_clone and _tensor in self._cache_clone_helper.inplace_graph_tensors:
                    continue
                graph_type = ORI_GRAPH_TENSOR
                if _tensor in self._dyn_compute_root_tensors | self._cst_compute_root_tensors:
                    graph_type = COMPUTE_ROOT_GRAPH_TENSOR
                unify_tensor = UnifyGraphTensor(self._gm_tensor_cache_read_buffer_map.get(_tensor))
                unify_tensor.build_tensor(_tensor, graph_type, INPUT_TYPE, CACHE_READ_MODE)
                self._ori_graph_unify_tensors.add(unify_tensor)

            # calc original graph pure middle tensors
            for _tensor in self._pure_middle_tensors:
                if self._enable_inplace_cache_clone and _tensor in self._cache_clone_helper.inplace_graph_tensors:
                    continue
                graph_type = ORI_GRAPH_TENSOR
                if _tensor in self._dyn_compute_root_tensors | self._cst_compute_root_tensors:
                    graph_type = COMPUTE_ROOT_GRAPH_TENSOR
                unify_tensor = UnifyGraphTensor(_tensor)
                unify_tensor.build_tensor(_tensor, graph_type, PURE_MIDDLE_TYPE, SET_SCOPE_MODE)
                self._ori_graph_unify_tensors.add(unify_tensor)

            # calc original graph pure middle tensors
            for _tensor in self._out_tensors:
                if self._enable_inplace_cache_clone and _tensor in self._cache_clone_helper.inplace_graph_tensors:
                    continue
                graph_type = ORI_GRAPH_TENSOR
                tensor_type = PURE_OUT_TYPE
                if _tensor in self._dyn_compute_root_tensors | self._cst_compute_root_tensors:
                    graph_type = COMPUTE_ROOT_GRAPH_TENSOR
                if _tensor in self._middle_out_tensors:
                    tensor_type = MIDDLE_OUT_TYPE

                unify_tensor_write = UnifyGraphTensor(self._gm_tensor_cache_write_buffer_map.get(_tensor))
                unify_tensor_write.build_tensor(_tensor, graph_type, tensor_type, CACHE_WRITE_MODE)
                self._ori_graph_unify_tensors.add(unify_tensor_write)

                unify_tensor = UnifyGraphTensor(_tensor)
                unify_tensor.build_tensor(_tensor, graph_type, tensor_type, GM_MODE)
                self._ori_graph_unify_tensors.add(unify_tensor)

                if _tensor in self._middle_out_tensors:
                    unify_tensor_read = UnifyGraphTensor(self._gm_tensor_cache_read_buffer_map.get(_tensor))
                    unify_tensor_read.build_tensor(_tensor, graph_type, tensor_type, CACHE_READ_MODE)
                    self._ori_graph_unify_tensors.add(unify_tensor_read)

            # calc cache_clone tensor
            if self._enable_inplace_cache_clone:
                for gm_tensor, clone_buffer in self._gm_tensor_and_clone_buffer_map.items():
                    if gm_tensor in self._input_tensors:
                        tensor_type = INPUT_TYPE
                    else:
                        tensor_type = PURE_MIDDLE_TYPE
                    unify_tensor_read = UnifyGraphTensor(clone_buffer)
                    unify_tensor_read.build_tensor(gm_tensor, ORI_GRAPH_TENSOR, tensor_type, CACHE_CLONE_MODE)
                    self._ori_graph_unify_tensors.add(unify_tensor_read)
            # calc fake node tensor
            if len(self._pure_out_tensors) > 1:
                unify_out = UnifyGraphTensor(self._out)
                unify_out.build_tensor(self._out, ORI_GRAPH_TENSOR, FAKE_NODE_TYPE, GM_MODE)
                self._ori_graph_unify_tensors.add(unify_out)
                if self._enable_compute_root and self._compute_root_helper.compute_root_fake_node is not None:
                    unify_compute_root_out = UnifyGraphTensor(self._compute_root_helper.compute_root_fake_node)
                    unify_compute_root_out.build_tensor(self._compute_root_helper.compute_root_fake_node,
                                                        COMPUTE_ROOT_GRAPH_TENSOR, FAKE_NODE_TYPE, GM_MODE)
                    self._ori_graph_unify_tensors.add(unify_compute_root_out)

        _calc_inplace_graph_unify_tensors()
        _calc_ori_graph_unify_tensors()

