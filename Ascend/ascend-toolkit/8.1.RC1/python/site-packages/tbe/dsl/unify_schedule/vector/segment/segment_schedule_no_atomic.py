#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
scatter schedule atomic
"""
from typing import Any

from tbe import tvm
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_compile_info

from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import SegmentPattern
from ...constants import Pattern
from ...schedule import Schedule
from .segment_tilingcase import TilingStrategy
from .segment_schedule_atomic import SegmentAtomicSchedule

DEFAULT = "default"

# block size in D architecture
BLOCK_SIZE_BYTE = 32

# STORE AREA
VAR_STORE_GM = 0
VAR_STORE_UB = 1
# Represents the axis that does not need to be split
DUMMY_DIM = -10


# 'pylint: disable=R0902, R0903
class SegmentNoAtomicSchedule(SegmentAtomicSchedule):
    """
    segment schedule
    """

    def _calc_tiling_dynamic(self):
        output_shape = util.shape_to_list(self._out_tensor.shape)
        id_shape = util.shape_to_list(self._id_gm_tensor.shape)

        b_idx = self._tiling_case["block_tiling_norm_axis"]
        u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
        u_idx_reduce = self._tiling_case["ub_tiling_reduce_axis"]
        is_cache = self._tiling_case["is_cache"]
        is_core_x = self._tiling_case["is_core_x"]

        b_bound = (1, util.get_bound(output_shape[b_idx])[1])

        self._block_tiling_vars[b_idx] = operation.var_inner("_block_factor_" + str(b_idx), b_bound, "int64")
        if is_cache:
            self._cache_num_tiling_vars[0] = operation.var_inner("_segment_cache_num_" + str(0), (1, None), "int64")
            self._cache_start_tiling_vars[0] = operation.var_inner("_segment_cache_start_" + str(0), (1, None), "int64")
        if not is_cache and not is_core_x:
            self._last_dim_tiling_vars[0] = operation.var_inner("_segment_last_dim_" + str(0), (1, None), "int64")
        if u_idx_reduce is not None:
            u_bound = (1, util.get_bound(id_shape[u_idx_reduce])[1])
            self._ub_reduce_tiling_vars[u_idx_reduce] = operation.var_inner("_ub_reduce_factor_" + str(u_idx_reduce),
                                                                            u_bound, "int64")
        if u_idx_norm is not None:
            u_bound = (1, util.get_bound(output_shape[u_idx_norm])[1])
            self._ub_norm_tiling_vars[u_idx_norm] = operation.var_inner("_ub_norm_factor_" + str(u_idx_norm),
                                                                        u_bound, "int64")

    def _do_tiling(self):
        if self._tiling_strategy == TilingStrategy.DYNAMIC:
            b_idx = self._tiling_case["block_tiling_norm_axis"]
            u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
            u_idx_reduce = self._tiling_case["ub_tiling_reduce_axis"]

            self._block_factor = self._block_tiling_vars.get(b_idx)
            ub_norm_factor = self._ub_norm_tiling_vars.get(u_idx_norm)
            ub_reduce_factor = self._ub_reduce_tiling_vars.get(u_idx_reduce)
            self._cache_num = self._cache_num_tiling_vars.get(0)
            self._cache_start = self._cache_start_tiling_vars.get(0)
            self._last_dim = self._last_dim_tiling_vars.get(0)
        else:
            b_idx = 0
            u_idx_norm = self._const_ub_norm_axis if self._const_ub_norm_axis != DUMMY_DIM else None
            u_idx_reduce = self._const_ub_reduce_axis

            self._block_factor = self._const_block_factor
            ub_norm_factor = self._const_ub_norm_factor
            ub_reduce_factor = self._const_ub_reduce_factor
            self._cache_num = self._const_cache_num
            self._cache_start = self._const_cache_start
            self._last_dim = self._const_last_dim

        if not self._is_core_x:
            b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[0],
                                                              factor=self._block_factor)
        else:
            b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[1],
                                                              factor=self._block_factor)

        u_reduce_o, u_reduce_i = self._schedule[self._out_tensor].split(self._out_tensor.op.reduce_axis[0],
                                                                        factor=ub_reduce_factor)
        self._reorder_axis.extend([b_o, u_reduce_o, u_reduce_i, b_i])
        if u_idx_norm is None:
            self._reorder_axis_core_x.extend([u_reduce_o, u_reduce_i, b_o, b_i, self._out_tensor.op.axis[0]])
        if u_idx_norm is not None:
            if not self._is_core_x:
                u_norm_o, u_norm_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[1],
                                                                            factor=ub_norm_factor)
            else:
                u_norm_o, u_norm_i = self._schedule[self._out_tensor].split(b_i,
                                                                            factor=ub_norm_factor)
                self._reorder_axis_core_x.extend(
                    [u_reduce_o, u_reduce_i, b_o, u_norm_o, u_norm_i, self._out_tensor.op.axis[0]])
            self._reorder_axis.extend([u_norm_o, u_norm_i])
        else:
            self._reorder_axis.append(self._out_tensor.op.axis[1])
        self._block_bind_axis = b_o
        self._compute_at_reduce_axis = u_norm_o if u_idx_norm is not None else u_reduce_o
        self._compute_at_norm_axis = u_norm_o if u_idx_norm is not None else u_reduce_o
        self._emit_axis = u_norm_i if u_idx_norm is not None else b_i
        self._emit_segment_axis = u_norm_i if u_idx_norm is not None else u_reduce_i

    def _calc_reorder(self):
        if not self._is_core_x:
            self._reorder_map[self._out_tensor] = self._reorder_axis
        else:
            self._reorder_map[self._out_tensor] = self._reorder_axis_core_x

    def _calc_emit_insn(self):
        self._schedule[self._out_tensor].remove_init()
        self._emit_insn_map[self._var_ub] = [self._var_ub.op.axis[0], "dma_copy"]
        self._emit_insn_map[self._id_ub] = [self._id_ub.op.axis[0], "dma_copy"]
        if self._is_need_align_pad:
            self._emit_insn_map[self._var_align_pad_ub] = [self._var_align_pad_ub.op.axis[0],
                                                           "align_pad", {"enough_buffer": False}]
        if self._is_cache and not self._is_core_x:
            if self._is_set_mask:
                self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                         {"segment_atomic": 1, "segment_cache_num": self._cache_num,
                                                          "segment_cache_start": self._cache_start * self._block,
                                                          "set_vector_mask_hoist": True,
                                                          "atomic_total_len": self._out_atomic_len}]
            else:
                self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                         {"segment_atomic": 1, "segment_cache_num": self._cache_num,
                                                          "segment_cache_start": self._cache_start * self._block,
                                                          "set_vector_mask_hoist": False,
                                                          "atomic_total_len": self._out_atomic_len}]
        elif not self._is_cache and not self._is_core_x:
            self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                     {"segment_atomic": 1, "segment_block_factor": self._block_factor,
                                                      "segment_block_index": self._last_dim * self._block,
                                                      "atomic_total_len": self._out_atomic_len}]
        else:
            self._emit_insn_map[self._out_tensor] = [self._emit_axis, "dma_copy",
                                                     {"segment_atomic": 1, "segment_block_factor": self._block_factor,
                                                      "segment_block_index": self._block,
                                                      "atomic_total_len": self._out_atomic_len}]


