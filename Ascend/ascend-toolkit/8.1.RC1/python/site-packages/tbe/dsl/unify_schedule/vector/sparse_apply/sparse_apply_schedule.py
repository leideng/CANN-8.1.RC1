#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
sparse apply schedule
"""

from tbe import tvm
from tbe.dsl.base import operation
import te.platform as tbe_platform

from ...constants import INSN_MAPPING
from ...constants import Pattern
from ...schedule import Schedule


class SparseSchedule(Schedule):
    """
    sparse schedule
    """
    def __init__(self, outs, tiling_case):
        self._schedule = None
        self._tiling_case = tiling_case
        self._tiling_key = tiling_case.get("key")
        self.outs = outs
        self.ub_tensors = []

        self.s_factor = None
        self.c_block_factor = None
        self.c_ub_factor = None
        self.idx_factor = None

        self.col = outs[0].shape[1]
        self.idx = outs[0].op.sparse_axis[0].dom.extent
        self.core_num = tiling_case.get("core_num")
        self.max_c_in_ub = tiling_case.get("max_col_in_ub")
        self.min_col_threshold = tiling_case.get("min_col_threshold")
        self.min_col_for_block = tiling_case.get("min_col_for_block")
        self.datas = dict()
        self.tiling_out = []

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return ["default"]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.SPARSE_APPLY]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [""]

    def do_schedule(self):
        """
        do_schedule

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        self._schedule = tvm.create_schedule([out.op for out in self.outs])
        self._schedule.tiling_key = self._tiling_key
        self._crawl_graph()
        self._do_cache_read()
        self._do_set_scope()
        self._do_tiling()
        self._do_axis_transform()
        self._do_storage_align()
        self._do_bind_block()
        self._do_compute_at()
        self._do_set_var_range()
        self._do_set_buffer_size()
        self._do_double_buffer()
        self._do_emit_insn()
        return self._schedule

    def _ceil_div(self, a, b):
        """
        a common fuction for ceil div

        Parameters
        ----------
        None

        Returns
        -------
        ceil div int
        """
        return (a + b - 1) // b

    def _dfs_sub_graph(self, out, visited):
        """
        dfs algorithm

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        for tensor_i in out.op.input_tensors:
            if isinstance(tensor_i.op, tvm.PlaceholderOp):
                if tensor_i in self.datas:
                    self.datas.get(tensor_i).append(out)
                else:
                    self.datas[tensor_i] = [out]
            else:
                if tensor_i in visited:
                    continue
                self.ub_tensors.append(tensor_i)
            visited.add(tensor_i)
            self._dfs_sub_graph(tensor_i, visited)

    def _crawl_graph(self):
        """
        crawl computation graph by depth first search algorithm

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        visited = set()
        visited.update(self.outs)
        for _out in self.outs:
            self._dfs_sub_graph(_out, visited)

    def _get_insn(self, tensor):
        """
        get a tensor's  emit_insn string

        Parameters
        ----------
        None

        Returns
        -------
        a string used when emit insn
        """
        tag = tensor.op.tag
        if tag.find("|") != -1:
            insn = tag.split("|")[0]
        else:
            insn = tag
        insn = INSN_MAPPING.get(insn)
        if not insn:
            insn = "dma_copy"
        return insn

    def _need_cache_read(self, tensor):
        """
        check whether the tensor need to do cache read, inputs exclude gather is necessary, else not

        Parameters
        ----------
        None

        Returns
        -------
        Bool result whether the tensor need to do cache read
        """
        if len(tensor.shape) <= 1:
            return True
        for consumer in self.datas.get(tensor):
            if consumer.op.tag == "gather":
                return False
        return True

    def _do_cache_read(self):
        """
        do cache read 

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        s = self._schedule
        for data in self.datas:
            if self._need_cache_read(data):
                ub_tensor = s.cache_read(data, tbe_platform.scope_ubuf, self.datas.get(data))
                self.ub_tensors.append(ub_tensor)

    def _do_tiling(self):
        if operation.get_op_mode() == "dynamic":
            self.s_factor = operation.var_inner("_s_factor", dtype="int64")
            self.c_block_factor = operation.var_inner("_c_block_factor", dtype="int64")
            self.c_ub_factor = operation.var_inner("_c_ub_factor", dtype="int64")
            self.idx_factor = operation.var_inner("_idx_factor", dtype="int64")
        else:
            tiling_data = self._tiling_case.get("tiling_data")
            self.s_factor = tiling_data.get("_s_factor")
            self.c_block_factor = tiling_data.get("_c_block_factor")
            self.c_ub_factor = tiling_data.get("_c_ub_factor")
            self.idx_factor = tiling_data.get("_idx_factor")

    def _do_axis_transform(self):
        """
        do axis transform, include split, reorder, fuse, compute_with

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        s = self._schedule
        block_axis = self._tiling_case.get("block_axis")
        split_col_for_ub_tiling = self._tiling_case.get("split_col_for_ub_tiling")
        vec_multi_idx = self._tiling_case.get("vec_multi_idx")
        for out in self.outs:
            sparse = s[out].op.sparse_axis[0]
            row, col = s[out].op.axis[0], s[out].op.axis[1]
            if block_axis == "sparse":
                s[out].reorder(sparse, col, row)
                ub_split_axis = col
                bind_axis = sparse
                axis_with_level = 2
                compute_at_axis = sparse
                emit_insn_axis = col
            elif block_axis == "sparse_out":
                s_o, s_i = s[out].split(sparse, factor=self.s_factor)
                if vec_multi_idx:
                    s_i_o, s_i_i = s[out].split(s_i, factor=self.idx_factor)
                    s[out].reorder(s_o, s_i_o, s_i_i, col, row)
                    compute_at_axis = s_i_o
                    emit_insn_axis = col
                    axis_with_level = 4
                else:
                    s[out].reorder(s_o, s_i, col, row)
                    compute_at_axis = s_i
                    emit_insn_axis = col
                    axis_with_level = 3
                ub_split_axis = col
                bind_axis = s_o
            elif block_axis == "col":
                col_b, col_i = s[out].split(col, factor=self.c_block_factor)
                s[out].reorder(col_b, sparse, col_i, row)
                ub_split_axis = col_i
                bind_axis = col_b
                compute_at_axis = sparse
                emit_insn_axis = col_i
                axis_with_level = 3
            elif block_axis == "mix":
                # block axis is sparse with col
                col_b, col_i = s[out].split(col, factor=self.c_block_factor)
                s[out].reorder(sparse, col_b, col_i, row)
                fused_axis = s[out].fuse(sparse, col_b)
                ub_split_axis = col_i
                bind_axis = fused_axis
                compute_at_axis = fused_axis
                emit_insn_axis = col_i
                axis_with_level = 2
            else:
                s[out].reorder(sparse, col, row)
                ub_split_axis = col
                axis_with_level = 2
                bind_axis = None
                compute_at_axis = sparse
                emit_insn_axis = col

            if split_col_for_ub_tiling:
                ub_o, ub_i = s[out].split(ub_split_axis, factor=self.c_ub_factor)
                axis_with_level += 1
                compute_at_axis = ub_o
                emit_insn_axis = ub_i

            tiling = {"bind_axis": bind_axis, "compute_at_axis": compute_at_axis, "emit_insn_axis": emit_insn_axis}

            self.tiling_out.append(tiling)

        if len(self.outs) > 1:
            s.compute_with(self.outs, axis_with_level)

    def _do_bind_block(self):
        """
        bind block axis, enable multi-core

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        block_axis = self._tiling_case.get("block_axis")
        s = self._schedule
        if block_axis != "none":
            block = tvm.thread_axis("blockIdx.x")
            bind_block_axis = self.tiling_out[0].get("bind_axis")
            s[self.outs[0]].bind(bind_block_axis, block)

    def _do_set_buffer_size(self):
        """
        set_buffer_size

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        s = self._schedule
        for ub_tensor in self.ub_tensors:
            s[ub_tensor].set_buffer_size(self.max_c_in_ub)

    def _justify_factor(self, num, factor):
        """
        justify

        Parameters
        ----------
        num
        factor

        Returns
        -------
        factor justified
        """
        nparts = self._ceil_div(num, factor)
        last = num - (nparts - 1) * factor
        while last < 8 and factor >= 16:
            factor -= 8
            nparts = self._ceil_div(num, factor)
            last = num - (nparts - 1) * factor
        return int(factor)

    def _do_set_var_range(self):
        """
        set var range

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        if operation.get_op_mode() == "static":
            return
        s = self._schedule
        block_axis = self._tiling_case.get("block_axis")
        split_col_for_ub_tiling = self._tiling_case.get("split_col_for_ub_tiling")
        _col_imm_factor = 0
        c_block_factor_imm = None
        if block_axis == "sparse_out":
            if isinstance(self.idx, tvm.tir.IntImm):
                _factor = self._ceil_div(int(self.idx), self.core_num)
                s.set_var_range(self.s_factor, _factor, _factor)
        elif block_axis == "col":
            if isinstance(self.col, tvm.tir.IntImm):
                factor = self._ceil_div(int(self.col), self.core_num)
                if factor <= self.min_col_for_block:
                    factor = self.min_col_for_block
                else:
                    if factor % self.min_col_for_block:
                        factor += (self.min_col_for_block - (factor % self.min_col_for_block))
                factor = self._justify_factor(int(self.col), factor)
                s.set_var_range(self.c_block_factor, factor, factor)
                c_block_factor_imm = factor
        elif block_axis == "mix":
            # block axis is sparse with col
            if isinstance(self.idx, tvm.tir.IntImm) and isinstance(self.col, tvm.tir.IntImm):
                c_nparts = self.core_num // int(self.idx)
                if c_nparts >= 1:
                    factor = self._ceil_div(int(self.col), c_nparts)
                    if factor < self.min_col_for_block:
                        factor = self.min_col_for_block
                    else:
                        if factor % self.min_col_for_block:
                            factor += (self.min_col_for_block - (factor % self.min_col_for_block))
                    factor = self._justify_factor(int(self.col), factor)
                    s.set_var_range(self.c_block_factor, factor, factor)
                    c_block_factor_imm = factor

        if split_col_for_ub_tiling:
            if block_axis == "col" or block_axis == "mix":
                if c_block_factor_imm:
                    if c_block_factor_imm > self.max_c_in_ub:
                        factor = self._justify_factor(c_block_factor_imm, self.max_c_in_ub)
                        s.set_var_range(self.c_ub_factor, factor, factor)
                    else:
                        s.set_var_range(self.c_ub_factor, c_block_factor_imm, c_block_factor_imm)
            else:
                if isinstance(self.col, tvm.tir.IntImm):
                    if int(self.col) > self.max_c_in_ub:
                        factor = self._justify_factor(int(self.col), self.max_c_in_ub)
                        s.set_var_range(self.c_ub_factor, factor, factor)

    def _do_compute_at(self):
        """
        _do_compute_at

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        s = self._schedule
        for ub_tensor in self.ub_tensors:
            s[ub_tensor].compute_at(s[self.outs[0]], self.tiling_out[0].get("compute_at_axis"))

    def _do_emit_insn(self):
        """
        _do_emit_insn

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        s = self._schedule
        block_axis = self._tiling_case.get("block_axis")
        vec_multi_idx = self._tiling_case.get("vec_multi_idx")
        for ub_tensor in self.ub_tensors:
            insn = self._get_insn(ub_tensor)
            if insn == "dma_copy" and vec_multi_idx and len(ub_tensor.shape) > 1:
                s[ub_tensor].emit_insn(ub_tensor.op.axis[1], insn)
            else:
                s[ub_tensor].emit_insn(ub_tensor.op.axis[0], insn)

        for i, out in enumerate(self.outs):
            no_overlap = 2
            if block_axis == "none":
                no_overlap = "process_data_smaller_than_one_block_by_calcute_index"

            s[out].emit_insn(self.tiling_out[i].get("emit_insn_axis"), "dma_copy", {"no_overlap": no_overlap})

    def _do_set_scope(self):
        """
        set ub tensor

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        for tensor in self.ub_tensors:
            self._schedule[tensor].set_scope(tbe_platform.scope_ubuf)

    def _do_double_buffer(self):
        """
        enable double buffer

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        vec_multi_idx = self._tiling_case.get("vec_multi_idx")
        if vec_multi_idx:
            return
        for tensor in self.ub_tensors:
            self._schedule[tensor].double_buffer()

    def _do_storage_align(self):
        vec_multi_idx = self._tiling_case.get("vec_multi_idx")
        s = self._schedule
        if vec_multi_idx:
            for tensor in self.ub_tensors:
                if len(tensor.shape) > 1:
                    # for fp32, 8 elements aligned
                    s[tensor].storage_align(tensor.op.axis[0], 8, 0)

        if self.col == 1:
            for tensor in self.ub_tensors:
                s[tensor].compute_align(tensor.op.axis[-1], 8)
