#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2023-2024. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

from typing import List
from typing import Dict
from typing import Set
from functools import reduce
from operator import mul

from tbe import tvm
from tbe.dsl.base.expr_compare import expr_equal
from tbe.dsl.base import operation
from ... import util
from ...constants import COMPUTE_TYPE_INSN_MAPPING
from ...constants import ComputeType
from ...constants import ELEWISE_BROADCAST_INSNS
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG


class GraphUtil:
    """
    GraphUtil: graph visit, node calc
    """

    @staticmethod
    def find_outermost_brc(root: tvm.Tensor, need_skip_brc_tensors=None) -> List[tvm.Tensor]:
        need_skip_brc_tensors = need_skip_brc_tensors or []

        def dfs_graph(tensor, in_brc):
            for tensor_i in tensor.op.input_tensors:
                need_skip = len(tensor_i.op.input_tensors) == 0 \
                            or tensor_i in brc_nodes or tensor_i in visited_tensors
                if need_skip:
                    continue
                visited_tensors.add(tensor_i)
                if util.is_broadcast(tensor_i) and tensor_i not in need_skip_brc_tensors:
                    if in_brc > 0 and per_layer_num[in_brc - 1] > 0:
                        brc_nodes.pop()
                        per_layer_num[in_brc - 1] -= 1
                    brc_nodes.append(tensor_i)
                    per_layer_num[in_brc] += 1
                    if len(per_layer_num) <= in_brc + 1:
                        per_layer_num.append(0)
                    dfs_graph(tensor_i, in_brc + 1)
                else:
                    dfs_graph(tensor_i, in_brc)

        brc_nodes = []
        per_layer_num = [0]
        in_brc = 0
        visited_tensors = {root}
        if util.is_broadcast(root):
            brc_nodes.append(root)
            per_layer_num[0] += 1
            if len(per_layer_num) <= in_brc + 1:
                per_layer_num.append(0)
            dfs_graph(root, in_brc + 1)
        else:
            dfs_graph(root, in_brc)
        return brc_nodes

    @staticmethod
    def brc_grouping(brc_nodes: List[tvm.Tensor], max_dtype_bytes: int) -> Dict[int, List[tvm.Tensor]]:
        block_size_bytes = util.get_ub_block_size()
        brc_nodes_size_map = {}
        ele_in_block = block_size_bytes // max_dtype_bytes
        for tensor_i in brc_nodes:
            before_brc_shape = util.shape_to_list(tensor_i.op.input_tensors[0].shape)
            before_brc_shape_size = reduce(mul, before_brc_shape, 1)
            before_brc_shape_size_align = (before_brc_shape_size + ele_in_block - 1) // ele_in_block * ele_in_block
            if before_brc_shape_size_align in brc_nodes_size_map:
                brc_nodes_size_map.get(before_brc_shape_size_align).append(tensor_i)
            else:
                brc_nodes_size_map[before_brc_shape_size_align] = [tensor_i]
        return brc_nodes_size_map

    @staticmethod
    def update_groups_by_out(groups: Dict[int, List[tvm.Tensor]], max_dtype_bytes: int,
                             outs: List[tvm.Tensor]):
        block_size_bytes = util.get_ub_block_size()
        ele_in_block = block_size_bytes // max_dtype_bytes
        for tensor_i in outs:
            shapes = util.shape_to_list(tensor_i.shape)
            shapes_size = reduce(mul, shapes, 1)
            shapes_size_align = (shapes_size + ele_in_block - 1) // ele_in_block * ele_in_block
            has_key = groups.get(shapes_size_align, [])
            if has_key:
                has_key.append(tensor_i)

    @staticmethod
    def get_in_out_map(root: tvm.Tensor) -> Dict[tvm.Tensor, Set[tvm.Tensor]]:
        def dfs_graph(tensor):
            for tensor_i in tensor.op.input_tensors:
                util.merge_value(in_out_map, tensor_i, tensor)
                if tensor_i in visited_tensors:
                    continue
                visited_tensors.add(tensor_i)
                dfs_graph(tensor_i)

        in_out_map = {}
        visited_tensors = set()
        dfs_graph(root)
        return in_out_map

    @staticmethod
    def max_live_node(root: tvm.Tensor,
                      in_out_map: Dict[tvm.Tensor, Set[tvm.Tensor]],
                      need_one_more: bool = False) -> int:
        """
        calc max coexist_quantity for simple graph
        @param root: root tensor for graph
        @param in_out_map: input-output relationship for graph
        @param need_one_more: if need_one_more, means not reused for 2 inputs ins
        @return:
        """

        def refresh_dependent(tensor):
            for tensor_i in tensor.op.input_tensors:
                if tensor_i not in dependent_map:
                    continue
                dependent_map.get(tensor_i).discard(tensor)
                if not dependent_map.get(tensor_i):
                    dependent_map.pop(tensor_i)

        def dfs_graph(tensor):
            if tensor in dependent_map:
                return len(dependent_map)
            need_space = []
            for tensor_i in tensor.op.input_tensors:
                need_space.append(dfs_graph(tensor_i))
            current_space = len(dependent_map) + 1 if need_one_more else len(dependent_map)
            need_space.append(current_space)
            refresh_dependent(tensor)
            if tensor not in dependent_map:
                dependent_map[tensor] = in_out_map[tensor].copy()
            return max(need_space)

        dependent_map = {}
        coexisting_quantities = []
        for tensor_in in root.op.input_tensors:
            coexisting_quantities.append(dfs_graph(tensor_in))
        coexisting_quantities.append(len(dependent_map) + 1 if need_one_more else len(dependent_map))
        return max(coexisting_quantities)

    @staticmethod
    def get_all_nodes(root: tvm.Tensor):
        def dfs_graph(tensor):
            for tensor_i in tensor.op.input_tensors:
                if tensor_i in visited_tensors:
                    continue
                visited_tensors.add(tensor_i)
                all_nodes.add(tensor_i)
                dfs_graph(tensor_i)

        all_nodes = set()
        visited_tensors = set()
        all_nodes.add(root)
        dfs_graph(root)
        return all_nodes

    @staticmethod
    def only_last_brc(input_tensors):
        if not input_tensors:
            return False

        max_dim_length = max(len(_input.shape) for _input in input_tensors)
        input_shapes = []
        for _input in input_tensors:
            input_shape = util.shape_to_list(_input.shape)
            input_shapes.append([1] * (max_dim_length - len(input_shape)) + input_shape)

        input_shapes = list(map(list, zip(*input_shapes)))
        last_dim = max_dim_length - 1
        is_last_brc = any(input_shapes[last_dim][0] != s for s in input_shapes[last_dim])
        if not is_last_brc:
            return False
        for i in range(last_dim - 1, -1, -1):
            if any(input_shapes[i][0] != s for s in input_shapes[i]):
                return False
        return True

    @staticmethod
    def calc_elewise_tensors(outs):
        """
        calc elewise tensors from compute graph outs,
        except one_shape, one_rank, scalar_broaccast
        @param outs:
        @return:
        """

        def is_pure_elewise_tensor(tensor):
            dsl_tag = util.get_dsl_insn(tensor)
            if dsl_tag in (COMPUTE_TYPE_INSN_MAPPING.get(ComputeType.ELEWISE) - set(ELEWISE_BROADCAST_INSNS)):
                return True
            return False

        def dfs_graph_elewise_tensor(tensor, visited_tensors, pure_elewise_tensors):
            for input_tensor in tensor.op.input_tensors:
                if input_tensor in visited_tensors:
                    continue

                if is_pure_elewise_tensor(input_tensor):
                    pure_elewise_tensors.add(input_tensor)

                visited_tensors.add(input_tensor)
                dfs_graph_elewise_tensor(input_tensor, visited_tensors, pure_elewise_tensors)

        pure_elewise_tensors = set()
        visited_tensors = set()
        for out in outs:
            visited_tensors.add(out)
            if is_pure_elewise_tensor(out):
                pure_elewise_tensors.add(out)
            dfs_graph_elewise_tensor(out, visited_tensors, pure_elewise_tensors)
        return pure_elewise_tensors

    @staticmethod
    def calc_all_graph_tensors(outs):
        def dfs_graph(tensor, graph_tensors):
            for tensor_i in tensor.op.input_tensors:
                if tensor_i in graph_tensors:
                    continue
                graph_tensors.add(tensor_i)
                dfs_graph(tensor_i, graph_tensors)

        graph_tensors = set()
        if not outs:
            return graph_tensors

        for out in outs:
            graph_tensors.add(out)
            dfs_graph(out, graph_tensors)

        return graph_tensors

    @staticmethod
    def calc_tensor_dependency(tensor_x, tensor_y):
        """
        if tensor_x depend on tensor_y, return -1
        if tensor_y depend on tensor_x, return 1
        if tensor_x have no dependency with tensor_y, return 0
        @param tensor_x:
        @param tensor_y:
        """
        # tensor_x and tensor_y must have no interdependency
        x_graph_tensors = GraphUtil.calc_all_graph_tensors([tensor_x])
        y_graph_tensors = GraphUtil.calc_all_graph_tensors([tensor_y])
        if tensor_y in x_graph_tensors:
            return -1
        elif tensor_x in y_graph_tensors:
            return 1
        else:
            return 0


class ScheduleUtil:

    @staticmethod
    def fake_node(tensors, node_name="fake_node"):
        dtype = tensors[0].dtype
        dim_length = max(len(t.shape) for t in tensors)
        shape = [1] * dim_length
        for tensor_i in tensors:
            if DTYPE_BYTE_MAPPING.get(tensor_i.dtype) > DTYPE_BYTE_MAPPING.get(dtype):
                dtype = tensor_i.dtype
            shape_i = util.shape_to_list(tensor_i.shape)
            diff = dim_length - len(shape_i)
            shape_i = [1] * diff + shape_i
            for j in range(diff, dim_length):
                if util.equals_one(shape[j]):
                    shape[j] = shape_i[j]
                elif not expr_equal(shape[j], shape_i[j]) and not util.equals_one(shape_i[j]):
                    shape[j] = tvm.max(shape_i[j], shape[j])

        def _fake_compute(*indices):
            res_ = tvm.const(1, dtype)
            for tensor in tensors:
                cur_indices = []
                for idx, dim in enumerate(tensor.shape):
                    if util.equals_one(dim):
                        cur_indices.append(0)
                    else:
                        cur_indices.append(indices[idx])
                res_ *= tvm.expr.Cast(dtype, tensor(*cur_indices))
            return res_

        with tvm.tag_scope(FAKE_NODE_TAG):
            res = tvm.compute(shape, _fake_compute, name=node_name)

        return res

    @staticmethod
    def copy_node(tensor):
        shape = tensor.shape
        with tvm.tag_scope("dma_copy"):
            res = tvm.compute(shape, lambda *i: tensor(*i), name="copy_node")
        return res

    @staticmethod
    def is_mov_align_sch():
        """
        mov out to ub with effective pad, used in v220 currently
        @return:
        """
        return util.is_v220()

    @staticmethod
    def is_elewise_or_brc_inline(tensor):
        if util.is_broadcast(tensor):
            return False
        return True

    @staticmethod
    def is_fusion_op():
        """
        fusion_op: op_info list length > 1
        @return:
        """
        op_info = operation.get_op_context().get_op_info()
        return op_info and len(op_info) > 1

    @staticmethod
    def add_compile_info_if_absent(key, value):
        cur_compile_info = operation.get_compile_info()
        # return if key already in compile info
        if cur_compile_info and key in cur_compile_info:
            return
        if cur_compile_info and key not in cur_compile_info:
            cur_compile_info[key] = value

    @staticmethod
    def is_b64_vmax_vmin(tensor):
        """
        int64 case vector_max do not support last axis brc_inline
        @return:
        """
        dsl_tag = util.get_dsl_insn(tensor)
        if tensor.dtype == "int64" and dsl_tag in ("elewise_binary_max", "elewise_binary_min"):
            return True
        return False
