#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2024-2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
reduce ar high precision schedule
"""

from tbe import tvm
from tbe.dsl.base.operation import var_inner_adaptive
from tbe.dsl.base.operation import get_context

from .reduce_tilingcase import ReduceTilingCase
from .reduce_tilingcase import SingleReduceInfo
from .vector_info import ComputeGraphInfo
from .reduce_base_precision_schedule import ReduceBasePrecisionSchedule
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG
from ...constants import ReduceSchType
from ... import util


BLOCK_IDX = "blockIdx.x"
INT32_MAX = 2 ** 31 - 1
CONST = "const"
LOCAL_UB = "local.UB"
PARENT = "parent"
SCOPE = "scope"
INSTRUCTION = "instruction"
ATTRS = "attrs"


class ReduceARNormalPrecisionSchedule(ReduceBasePrecisionSchedule):
    """
    Schedule for Normal High Precision Reduce
    """
    def __init__(self, graph_info: ComputeGraphInfo, reduce_info: SingleReduceInfo,
                 tiling_case: ReduceTilingCase, outs):
        super().__init__(graph_info, reduce_info, tiling_case, outs)

        self.reduce_rf_tensor = None
        self.reduce_rf_ub_tensor = None
        self.reduce_rf_reread_ub_tensor = None

        self.iter_rf_outer = None
        self.iter_rf_inner = None
        self.iter_ub_outer_2 = None
        self.iter_ub_inner_2 = None

    def do_schedule(self):
        self._create_schedule()

        self._do_cache_read()
        self._do_cache_write()
        self._do_mid_output_tensor_process()
        self._set_scope()
        self._init_branch_info()
        self._do_input_db()

        self._do_tiling()
        self._do_reorder()

        self._calc_storage_align()
        self._do_storage_align()
        self._calc_compute_align()
        self._do_compute_align()

        self._do_storage_bound()
        self._do_set_constraint()
        self._do_multi_core()

        self._calc_compute_at()
        self._do_compute_at()
        self._do_reverse_compute_at()

        self._calc_emit_insn()
        self._do_emit_insn()

        # 该场景下只有一个workspace，且没有sync tensor，需要额外增加一个fake workspace
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            util.add_sch_additional_entry(self.sch, "need_extra_fake_workspace", True)

        return self.sch

    def _do_tiling(self):
        self.reduce_tensor = self.cache_write_tensors_and_buffer_map.get(self.reduce_info.reduce_tensor,
                                                                         self.reduce_info.reduce_tensor)
        self.res_tensor = tuple(self.graph_info.endpoint_output_tensor_set)[0]

        block_factor = self.tiling_case.block_factor
        block_inner = block_factor if block_factor is not None else var_inner_adaptive("_block_factor", (1, None))
        self.block_split_result["factor"] = block_inner
        ub_factor = self.tiling_case.ub_factor
        ub_inner = ub_factor if ub_factor is not None else var_inner_adaptive("_ub_factor", (1, INT32_MAX))
        self.ub_split_result["factor"] = ub_inner

        # blk split
        iter_blk_outer, iter_blk_inner = \
            self.sch[self.res_tensor].split(self.res_tensor.op.axis[self.tiling_case.block_split_axis_index],
                                              factor=block_inner)
        self.block_split_result["outer_itervar"] = iter_blk_outer
        self.block_split_result["inner_itervar"] = iter_blk_inner

        # ub split  -- 先直接固定写 0 轴
        iter_ub_outer, iter_ub_inner = \
            self.sch[self.reduce_tensor].split(self.sch[self.reduce_tensor].op.reduce_axis[0], factor=ub_inner)
        self.ub_split_result["outer_itervar"] = iter_ub_outer
        self.ub_split_result["inner_itervar"] = iter_ub_inner

        # input -> cast -> reduce_rf_tensor -> reduce_tensor -> cast
        # reduce tensor rfactor, (A, R) -> (A, Ar.o, R.i) or (A, 1, R) -> (A, 1, Ar.o, R.i)
        self.reduce_rf_tensor = self.sch.rfactor(self.reduce_tensor, iter_ub_outer, factor_axis=-1)
        self.update_stage(self.reduce_rf_tensor, self.reduce_tensor, True)
        self.sch[self.reduce_rf_tensor].set_scope(LOCAL_UB)
    
        # 需要占用workspace，说明 iter_ub_outer 也超过了 ub factor，ub内放不下了
        # input -> cast -> reduce_rf_ub_tensor -> reduce_rf_tensor -> 
        # -> reduce_rf_reread_ub_tensor -> reduce_tensor -> cast
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            self.reduce_rf_ub_tensor = self.sch.cache_write(self.reduce_rf_tensor, LOCAL_UB)
            self.reduce_rf_reread_ub_tensor = \
                self.sch.cache_read(self.reduce_rf_tensor, LOCAL_UB, [self.reduce_tensor])
            self.sch[self.reduce_rf_tensor].set_scope("")
            self.outs.append(self.reduce_rf_tensor)

            self.cache_write_tensors_and_buffer_map[self.reduce_rf_tensor] = self.reduce_rf_ub_tensor
            self.update_stage(self.reduce_rf_ub_tensor, self.reduce_rf_tensor, True)
            self.cache_read_tensors_and_buffer_map[self.reduce_rf_tensor] = self.reduce_rf_reread_ub_tensor
            self.update_stage(self.reduce_rf_reread_ub_tensor, self.reduce_rf_tensor, False)

            # Ar.o需要第二次切分 (A, Ar.o) -> (A, Ar.o.o, Ar.o.i) or (A, 1, Ar.o) -> (A, 1, Ar.o.o, Ar.o.i)
            self.iter_rf_outer, self.iter_rf_inner = \
                self.sch[self.reduce_rf_tensor].split(self.sch[self.reduce_rf_tensor].op.axis[-1], factor=ub_inner)

            # reduce_tensor的reduce轴大小已经变了
            self.iter_ub_outer_2, self.iter_ub_inner_2 = \
                self.sch[self.reduce_tensor].split(self.sch[self.reduce_tensor].op.reduce_axis[0], factor=ub_inner)

    def _do_reorder(self):
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            self._do_reorder_ar(self.reduce_rf_ub_tensor)
            reordered_axis_list = []
            reordered_axis_list.extend([x for x in list(self.sch[self.reduce_tensor].op.axis)[:]])
            reordered_axis_list.append(self.iter_ub_outer_2)
            reordered_axis_list.append(self.iter_ub_inner_2)
            self.sch[self.reduce_tensor].reorder(*reordered_axis_list)
        else:
            # 1. reduce_rf_tensor reorder: (A, Ar.o, R.i) or (A, 1, Ar.o, R.i)
            self._do_reorder_ar(self.reduce_rf_tensor)

    def _calc_storage_align(self):
        # AR, ub split last axis, no need storage_align
        pass

    def _calc_compute_align(self):
        pass

    def _do_storage_bound(self):
        tensors_before_reduce = self.get_all_producers_stages(self.reduce_tensor)
        tensors_before_reduce = tensors_before_reduce.union(
            self.branch_tensor_info.get_all_branch_stages(self.backward_stage_graph_map))
        for stage_tensor in self.forward_stage_graph_map:
            if stage_tensor in self.graph_info.real_output_tensor_set:
                # don't set bound for real_output_tensors(gm)
                continue
            if stage_tensor in self.graph_info.input_tensor_set:
                # don't set bound for input_tensor_set(gm)
                continue
            if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE and \
                stage_tensor is self.reduce_rf_tensor:
                # don't set bound for reduce_rf_tensor(gm)
                continue

            # reduce_tensor is after reduce
            if stage_tensor in tensors_before_reduce:
                ub_count = self.tiling_case.tensor_ub_size_before_reduce
            else:
                ub_count = self.tiling_case.tensor_ub_size_after_reduce
            self.sch[stage_tensor].set_buffer_size(ub_count)

    def _do_set_constraint(self):
        if get_context().get_current_compute().get("_mode") == CONST:
            return
        # 1. set for ub constraint
        self._do_set_ub_constraint_ar()
        # 2. set for block constraint
        self.sch.set_constraint(self.block_split_result.get("factor") <= self.tiling_case.tensor_ub_size_after_reduce)

    def _do_multi_core(self):
        self.sch[self.res_tensor].bind(self.block_split_result.get("outer_itervar"), tvm.thread_axis(BLOCK_IDX))

    def _calc_compute_at(self):
        iter_blk_outer = self.block_split_result.get("outer_itervar")
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            # input -> cast -> reduce_rf_ub_tensor -> reduce_rf_tensor -> 
            # -> reduce_rf_reread_ub_tensor -> reduce_tensor -> cast
            # 1. before reduce_rf_ub_tensor
            tensors_before_reduce = self.get_all_producers_stages(self.reduce_rf_ub_tensor)
            scop_axis = self.sch[self.reduce_rf_ub_tensor].op.axis[-1]
            for tensor in tensors_before_reduce:
                if tensor not in self.graph_info.input_tensor_set:
                    self.compute_at_map[tensor] = {PARENT: self.sch[self.reduce_rf_ub_tensor], SCOPE: scop_axis}

            # 2. reduce_rf_ub_tensor at reduce_rf_tensor
            self.compute_at_map[self.reduce_rf_ub_tensor] = {PARENT: self.sch[self.reduce_rf_tensor],
                                                             SCOPE: self.iter_rf_outer}

            # 3. reduce_rf_reread_ub_tensor at reduce_tensor
            self.compute_at_map[self.reduce_rf_reread_ub_tensor] = {PARENT: self.sch[self.reduce_tensor],
                                                                    SCOPE: self.iter_ub_outer_2}

            # 4. after reduce_tensor / include reduce_tensor compute at res_tensor
            tensors_after_reduce = self.get_all_producers_stages(self.res_tensor) - \
                                    self.get_all_producers_stages(self.reduce_tensor)
            for tensor in tensors_after_reduce:
                self.compute_at_map[tensor] = {PARENT: self.sch[self.res_tensor], SCOPE: iter_blk_outer}

            # 5. reduce_rf_tensor compute at res_tensor
            self.compute_at_map[self.reduce_rf_tensor] = {PARENT: self.sch[self.res_tensor],
                                                          SCOPE: iter_blk_outer}
        else:
            # input -> cast -> reduce_rf_tensor -> reduce_tensor -> cast
            # 1. before reduce_rf_tensor (A,A.o,R.i) or (A,1,A.o,R.i)
            tensors_before_rf = self.get_all_producers_stages(self.reduce_rf_tensor)
            for tensor in tensors_before_rf:
                self.compute_at_map[tensor] = {PARENT: self.sch[self.reduce_rf_tensor],
                                               SCOPE: self.sch[self.reduce_rf_tensor].op.axis[-1]}

            # 2. reduce_rf_tensor compute at reduce_tensor
            self.compute_at_map[self.reduce_rf_tensor] = {PARENT: self.sch[self.reduce_tensor],
                                                          SCOPE: self.sch[self.reduce_tensor].op.axis[0]}

            # 3. after reduce_tensor / include reduce_tensor compute at res_tensor
            tensors_after_reduce = self.get_all_producers_stages(self.res_tensor) - \
                                    self.get_all_producers_stages(self.reduce_tensor)
            for tensor in tensors_after_reduce:
                self.compute_at_map[tensor] = {PARENT: self.sch[self.res_tensor], SCOPE: iter_blk_outer}

    def _calc_emit_insn(self):
        self._calc_reduce_emit_insn()
        self._calc_other_emit_insn()

    def _calc_reduce_emit_insn(self):
        insn = self._get_insn(self.reduce_info.reduce_tensor)

        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            self.emit_insn_map[self.reduce_rf_tensor] = {SCOPE: self.iter_rf_inner,
                                                         INSTRUCTION: "dma_copy"}
            self.emit_insn_map[self.reduce_rf_reread_ub_tensor] = {
                SCOPE: self.sch[self.reduce_rf_reread_ub_tensor].op.axis[0],
                INSTRUCTION: "dma_copy"
            }
            self.emit_insn_map[self.reduce_rf_ub_tensor] = {
                SCOPE: self.sch[self.reduce_rf_ub_tensor].op.reduce_axis[-1],
                INSTRUCTION: insn,
                ATTRS: self.ar_reduce_attr
            }
            self.emit_insn_map[self.reduce_tensor] = {
                SCOPE: self.iter_ub_inner_2,
                INSTRUCTION: insn,
                ATTRS: self.ar_reduce_attr_scalar}
        elif self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION:
            self.emit_insn_map[self.reduce_rf_tensor] = {SCOPE: self.sch[self.reduce_rf_tensor].op.reduce_axis[-1],
                                                         INSTRUCTION: insn,
                                                         ATTRS: self.ar_reduce_attr}
            self.emit_insn_map[self.reduce_tensor] = {SCOPE: self.sch[self.reduce_tensor].op.reduce_axis[-1],
                                                      INSTRUCTION: insn,
                                                      ATTRS: self.ar_reduce_attr}

    def _calc_other_emit_insn(self):
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            tensor_before_reduce = self.get_all_producers_stages(self.reduce_rf_ub_tensor)
        else:
            tensor_before_reduce = self.get_all_producers_stages(self.reduce_rf_tensor)
        tensor_before_reduce = tensor_before_reduce.\
            union(self.branch_tensor_info.get_all_branch_stages(self.backward_stage_graph_map))
        # not include self.reduce_tensor
        tensors_after_reduce = self.get_all_producers_stages(self.res_tensor) - \
                                    self.get_all_producers_stages(self.reduce_tensor) - {self.reduce_tensor}
        remaining_tensors = tensor_before_reduce | tensors_after_reduce | self.graph_info.endpoint_output_tensor_set

        for tensor in remaining_tensors:
            if tensor in self.graph_info.input_tensor_set:
                continue
            insn = self._get_insn(tensor)
            iter_var = self.sch[tensor].op.axis[0]
            if tensor in self.mid_output_tensor_cache_read_list:
                insn = "phony_insn"
            elif tensor in self.graph_info.real_output_tensor_set:
                insn = "dma_copy"
            if insn == "":
                insn = "dma_copy"

            if tensor in self.graph_info.endpoint_output_tensor_set:
                iter_var = self.block_split_result.get("inner_itervar")

            self.emit_insn_map[tensor] = {
                SCOPE: self.branch_tensor_info.get_hook_tensor_axis(tensor, iter_var),
                INSTRUCTION: insn
            }


class ReduceARGroupPrecisionSchedule(ReduceBasePrecisionSchedule):
    """
    Schedule for Group High Precision AR Reduce
    """
    def __init__(self, graph_info: ComputeGraphInfo, reduce_info: SingleReduceInfo,
                 tiling_case: ReduceTilingCase, outs):
        super().__init__(graph_info, reduce_info, tiling_case, outs)

        self.reduce_blk_rf_tensor = None
        self.reduce_blk_rf_ub_tensor = None
        self.reduce_blk_rf_reread_ub_tensor = None

        self.reduce_rf_rf_tensor = None
        self.reduce_rf_rf_ub_tensor = None
        self.reduce_rf_rf_reread_ub_tensor = None

        self.iter_rf_rf_outer = None
        self.iter_rf_rf_inner = None
        self.iter_rf_ub_outer = None
        self.iter_rf_ub_inner = None

        self.multi_core_bind_axis = None
        self.thread = None

    def do_schedule(self):
        self._create_schedule()

        # do prepare schedule
        self._do_cache_read()
        self._do_cache_write()
        self._do_mid_output_tensor_process()
        self._set_scope()
        self._init_branch_info()
        self._do_input_db()

        # do tiling and reorder
        self._do_tiling()
        self._do_reorder()

        # align
        self._calc_storage_align()
        self._do_storage_align()

        # group reduce special
        self._do_storage_bound()
        self._do_set_constraint()
        self._do_multi_core()
        self._do_set_store_predicate()
        self._do_block_sync()

        self._calc_compute_at()
        self._do_compute_at()
        self._do_reverse_compute_at()
        self._do_compute_root()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._add_build_args()
        self._add_block_sync_flag(self.tiling_case.tiling_key)

        return self.sch

    def _do_tiling(self):
        self.reduce_tensor = self.cache_write_tensors_and_buffer_map.get(self.reduce_info.reduce_tensor,
                                                                         self.reduce_info.reduce_tensor)
        self.res_tensor = tuple(self.graph_info.endpoint_output_tensor_set)[0]

        block_factor = self.tiling_case.block_factor
        block_inner = block_factor if block_factor is not None else var_inner_adaptive("_block_factor", (1, None))
        ub_factor = self.tiling_case.ub_factor
        ub_inner = ub_factor if ub_factor is not None else var_inner_adaptive("_ub_factor", (1, INT32_MAX))
        self.ub_split_result["factor"] = ub_inner

        # 对 reduce_tensor 做blk切分，并做rfactor 借A轴开多核
        # input -> cast -> reduce_rf_rf_tensor-> reduce_blk_rf_ub_tensor -> reduce_blk_rf_tensor ->
        # -> reduce_blk_rf_reread_ub_tensor -> reduce_tensor -> cast
        block_split_axis_index = self.tiling_case.block_split_axis_index
        block_split_reduce_axis_index = self.reduce_info.reduce_axis_indexes.index(block_split_axis_index)
        self._do_blk_tiling(block_split_axis_index, block_split_reduce_axis_index, block_inner)
        # 对 reduce_blk_rf_ub_tensor 做ub切分
        self._do_ub_tiling(block_split_reduce_axis_index, ub_inner)

        # reduce_blk_rf_ub_tensor rfactor, (A, A.blk.o, R.i) -> (A, A.blk.o, A.ub.o, R.i.i)
        # or (A, A.blk.o, 1, R.i) -> (A, A.blk.o, 1, A.ub.o, R.i.i)
        self.reduce_rf_rf_tensor = self.sch.rfactor(self.reduce_blk_rf_ub_tensor,
                                                    self.ub_split_result["outer_itervar"],
                                                    factor_axis=-1)
        self.update_stage(self.reduce_rf_rf_tensor, self.reduce_blk_rf_ub_tensor, True)
        self.sch[self.reduce_rf_rf_tensor].set_scope(LOCAL_UB)

        # workspace场景, ub outer 超过了 ub factor
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            # A.ub.o需要第二次切分 (A, A.blk.o, A.ub.o, R.i.i) -> (A, A.blk.o, A.ub.o.o, A.ub.o.i, R.i.i)
            self.iter_rf_rf_outer, self.iter_rf_rf_inner = \
                self.sch[self.reduce_rf_rf_tensor].split(self.sch[self.reduce_rf_rf_tensor].op.axis[-1],
                                                         factor=ub_inner)

            # reduce_blk_rf_ub_tensor 的reduce轴大小已经变了, 一次ub装不下，需要二次切分
            self.iter_rf_ub_outer, self.iter_rf_ub_inner = \
                self.sch[self.reduce_blk_rf_ub_tensor].split(self.sch[self.reduce_blk_rf_ub_tensor].op.reduce_axis[0],
                                                             factor=ub_inner)

    def _do_blk_tiling(self, block_split_axis_index, block_split_reduce_axis_index, block_inner):
        block_axis_var = self.reduce_tensor.op.reduce_axis[block_split_reduce_axis_index]
        iter_blk_outer, iter_blk_inner = self.sch[self.reduce_tensor].split(block_axis_var,
                                                                            factor=block_inner)

        if not self.reduce_info.keepdims:
            block_outer_inner, block_inner_inner = self.sch[self.reduce_tensor].split(iter_blk_inner, factor=1)
            self.reduce_blk_rf_tensor = self.sch.rfactor(self.reduce_tensor, [iter_blk_outer, block_inner_inner],
                                                          [block_split_axis_index, -1])
        else:
            self.reduce_blk_rf_tensor = self.sch.rfactor(self.reduce_tensor, iter_blk_outer, block_split_axis_index)

        self.reduce_blk_rf_ub_tensor = self.sch.cache_write(self.reduce_blk_rf_tensor, LOCAL_UB)
        self.reduce_blk_rf_reread_ub_tensor = self.sch.cache_read(self.reduce_blk_rf_tensor, LOCAL_UB,
                                                                  [self.reduce_tensor])
        self.update_stage(self.reduce_blk_rf_tensor, self.reduce_tensor, True)
        self.sch[self.reduce_blk_rf_tensor].set_scope("")
        self.outs.append(self.reduce_blk_rf_tensor)

        self.cache_write_tensors_and_buffer_map[self.reduce_blk_rf_tensor] = self.reduce_blk_rf_ub_tensor
        self.update_stage(self.reduce_blk_rf_ub_tensor, self.reduce_blk_rf_tensor, True)
        self.cache_read_tensors_and_buffer_map[self.reduce_blk_rf_tensor] = self.reduce_blk_rf_reread_ub_tensor
        self.update_stage(self.reduce_blk_rf_reread_ub_tensor, self.reduce_blk_rf_tensor, False)

    def _do_ub_tiling(self, block_split_reduce_axis_index, ub_inner):
        ub_split_axis_index = self.tiling_case.ub_split_axis_index
        if ub_split_axis_index in self.reduce_info.reduce_axis_indexes:
            # some reduce axes have been fused to split block
            ub_tiling_reduce_axis_index = \
                self.reduce_info.reduce_axis_indexes.index(ub_split_axis_index) - block_split_reduce_axis_index
            ub_axis_var = self.sch[self.reduce_blk_rf_ub_tensor].op.reduce_axis[ub_tiling_reduce_axis_index]
        else:
            ub_axis_var = self.sch[self.reduce_blk_rf_ub_tensor].op.axis[ub_split_axis_index]
        iter_blk_rf_ub_outer, iter_blk_rf_ub_inner = \
            self.sch[self.reduce_blk_rf_ub_tensor].split(ub_axis_var, factor=ub_inner)

        self.ub_split_result["outer_itervar"] = iter_blk_rf_ub_outer
        self.ub_split_result["inner_itervar"] = iter_blk_rf_ub_inner

    def _do_reorder(self):
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            reordered_axis_list = []
            for i in range(0, len(self.sch[self.reduce_rf_rf_tensor].op.axis) - 1):
                reordered_axis_list.append(self.sch[self.reduce_rf_rf_tensor].op.axis[i])
            reordered_axis_list.append(self.iter_rf_rf_outer)
            reordered_axis_list.append(self.iter_rf_rf_inner)
            reordered_axis_list.append(self.sch[self.reduce_rf_rf_tensor].op.reduce_axis[0])
            self.sch[self.reduce_rf_rf_tensor].reorder(*reordered_axis_list)

            reordered_axis_list = []
            reordered_axis_list.extend([x for x in list(self.sch[self.reduce_blk_rf_ub_tensor].op.axis)[:]])
            reordered_axis_list.append(self.iter_rf_ub_outer)
            reordered_axis_list.append(self.iter_rf_ub_inner)
            self.sch[self.reduce_blk_rf_ub_tensor].reorder(*reordered_axis_list)
        else:
            self._do_reorder_ar(self.reduce_rf_rf_tensor)
            self._do_reorder_ar(self.reduce_blk_rf_ub_tensor)

    def _calc_storage_align(self):
        # AR, reduce_blk_rf_reread_ub_tensor align by R axis
        reduce_align_factor = int(self.block_size_byte // DTYPE_BYTE_MAPPING.get(self.reduce_tensor.dtype))
        self.storage_align_map[self.reduce_blk_rf_reread_ub_tensor] = \
                [self.sch[self.reduce_blk_rf_reread_ub_tensor].op.axis[0], reduce_align_factor, 0]

    def _do_storage_bound(self):
        tensors_before_reduce = self.get_all_producers_stages(self.reduce_tensor)
        tensors_before_reduce = tensors_before_reduce.union(
            self.branch_tensor_info.get_all_branch_stages(self.backward_stage_graph_map))
        for stage_tensor in self.forward_stage_graph_map:
            if stage_tensor in self.graph_info.real_output_tensor_set:
                # don't set bound for real_output_tensors(gm)
                continue
            if stage_tensor in self.graph_info.input_tensor_set:
                # don't set bound for input_tensor_set(gm)
                continue
            if stage_tensor is self.reduce_blk_rf_tensor:
                continue

            # reduce_tensor is after reduce
            if stage_tensor in tensors_before_reduce:
                ub_count_gr = self.tiling_case.tensor_ub_size_before_reduce
            else:
                ub_count_gr = self.tiling_case.tensor_ub_size_after_reduce
            self.sch[stage_tensor].set_buffer_size(ub_count_gr)

    def _do_set_constraint(self):
        if get_context().get_current_compute().get("_mode") == CONST:
            return
        self._do_set_ub_constraint_ar()

    def _do_multi_core(self):
        fuse_axis_list = self.sch[self.reduce_blk_rf_tensor].op.axis[:self.tiling_case.block_split_axis_index + 1]
        self.multi_core_bind_axis = self.sch[self.reduce_blk_rf_tensor].fuse(*fuse_axis_list)
        self.thread = tvm.thread_axis(BLOCK_IDX)
        self.sch[self.reduce_blk_rf_tensor].bind(self.multi_core_bind_axis, self.thread)

    def _do_set_store_predicate(self):
        #after reduce_tensor / include reduce_tensor
        tensors_after_reduce = self.get_all_producers_stages(self.res_tensor) - \
                                    self.get_all_producers_stages(self.reduce_tensor)

        set_store_predicate_tensor_set = \
            tensors_after_reduce | {self.reduce_blk_rf_reread_ub_tensor, self.res_tensor}
        for after_reduce_tensor in set_store_predicate_tensor_set:
            self.sch[after_reduce_tensor].set_store_predicate(self.thread.var.equal(0),
                                                              partition=False, rebase_root=True)

    def _do_block_sync(self):
        sync_tensor = self.sch.create_block_sync()
        # sync axis is the axis after self.multi_core_bind_axis
        sync_axis = self.sch[self.reduce_blk_rf_tensor].leaf_iter_vars[1]
        self.sch[self.reduce_blk_rf_tensor].wait_block_sync(sync_axis, tensor=sync_tensor, bottom=True)
        self.sch[self.reduce_blk_rf_tensor].set_block_sync(sync_axis, tensor=sync_tensor, bottom=True)

    def _calc_compute_at(self):
        # input -> cast -> reduce_rf_rf_tensor-> reduce_blk_rf_ub_tensor -> reduce_blk_rf_tensor ->
        # -> reduce_blk_rf_reread_ub_tensor -> reduce_tensor -> cast
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            # 1. before reduce_rf_rf_tensor
            tensors_before_rf = self.get_all_producers_stages(self.reduce_rf_rf_tensor)
            for tensor in tensors_before_rf:
                self.compute_at_map[tensor] = {PARENT: self.sch[self.reduce_rf_rf_tensor],
                                               SCOPE: self.iter_rf_rf_inner}

            # 2. reduce_rf_rf_tensor compute at reduce_blk_rf_ub_tensor
            self.compute_at_map[self.reduce_rf_rf_tensor] = {PARENT: self.sch[self.reduce_blk_rf_ub_tensor],
                                                             SCOPE: self.iter_rf_ub_outer}

            # 3. reduce_blk_rf_ub_tensor at reduce_blk_rf_tensor -- blk
            self.compute_at_map[self.reduce_blk_rf_ub_tensor] = {PARENT: self.sch[self.reduce_blk_rf_tensor],
                                                                 SCOPE: self.multi_core_bind_axis}
        else:
            # 1. before reduce_rf_rf_tensor
            tensors_before_rf = self.get_all_producers_stages(self.reduce_rf_rf_tensor)
            for tensor in tensors_before_rf:
                self.compute_at_map[tensor] = {PARENT: self.sch[self.reduce_rf_rf_tensor],
                                               SCOPE: self.sch[self.reduce_rf_rf_tensor].op.axis[-1]}

            # 2. reduce_rf_rf_tensor compute at reduce_blk_rf_ub_tensor
            scope_axis = self.sch[self.reduce_blk_rf_ub_tensor].op.axis[-1]
            self.compute_at_map[self.reduce_rf_rf_tensor] = {PARENT: self.sch[self.reduce_blk_rf_ub_tensor],
                                                             SCOPE: scope_axis}

            # 3. reduce_blk_rf_ub_tensor at reduce_blk_rf_tensor -- blk
            self.compute_at_map[self.reduce_blk_rf_ub_tensor] = {PARENT: self.sch[self.reduce_blk_rf_tensor],
                                                                 SCOPE: self.multi_core_bind_axis}

    def _do_compute_root(self):
        tensors_after_reduce = self.get_all_producers_stages(self.res_tensor) - \
                                    self.get_all_producers_stages(self.reduce_tensor)
        compute_root_tensor_set = \
            tensors_after_reduce | {self.reduce_blk_rf_reread_ub_tensor, self.res_tensor}
        for tensor in compute_root_tensor_set:
            self.sch[tensor].compute_root()

    def _calc_emit_insn(self):
        self._calc_reduce_emit_insn()
        self._calc_other_emit_insn()

    def _calc_reduce_emit_insn(self):
        insn = self._get_insn(self.reduce_info.reduce_tensor)

        # input -> cast -> reduce_rf_rf_tensor -> reduce_blk_rf_ub_tensor -> reduce_blk_rf_tensor -> 
        # -> reduce_blk_rf_reread_ub_tensor -> reduce -> cast
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            self.emit_insn_map[self.reduce_rf_rf_tensor] = {
                SCOPE: self.sch[self.reduce_rf_rf_tensor].op.reduce_axis[-1],
                INSTRUCTION: insn,
                ATTRS: self.ar_reduce_attr
            }
            self.emit_insn_map[self.reduce_blk_rf_ub_tensor] = {
                SCOPE: self.iter_rf_ub_inner,
                INSTRUCTION: insn,
                ATTRS: self.ar_reduce_attr_scalar
            }
        elif self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION:
            self.emit_insn_map[self.reduce_rf_rf_tensor] = {
                SCOPE: self.sch[self.reduce_rf_rf_tensor].op.reduce_axis[-1],
                INSTRUCTION: insn,
                ATTRS: self.ar_reduce_attr
            }
            self.emit_insn_map[self.reduce_blk_rf_ub_tensor] = {
                SCOPE: self.sch[self.reduce_blk_rf_ub_tensor].op.reduce_axis[-1],
                INSTRUCTION: insn,
                ATTRS: self.ar_reduce_attr
            }

        self.emit_insn_map[self.reduce_blk_rf_tensor] = {
            SCOPE: self.sch[self.reduce_blk_rf_tensor].leaf_iter_vars[1],
            INSTRUCTION: "dma_copy"
        }
        self.emit_insn_map[self.reduce_blk_rf_reread_ub_tensor] = {
            SCOPE: self.sch[self.reduce_blk_rf_reread_ub_tensor].op.axis[0],
            INSTRUCTION: "dma_copy"
        }
        # reduce_tensor R is less than core_num, no need entire_reduce
        self.emit_insn_map[self.reduce_tensor] = {
            SCOPE: self.sch[self.reduce_tensor].op.axis[0],
            INSTRUCTION: insn,
            ATTRS: {"storage_bound": self.tiling_case.tensor_ub_size_before_reduce}
        }

    def _calc_other_emit_insn(self):
        if self.tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            tensors_before_reduce = self.get_all_producers_stages(self.reduce_rf_rf_tensor)
        else:
            tensors_before_reduce = self.get_all_producers_stages(self.reduce_rf_rf_tensor)
        tensors_before_reduce = tensors_before_reduce.\
            union(self.branch_tensor_info.get_all_branch_stages(self.backward_stage_graph_map))
        # not include self.reduce_tensor
        tensors_after_reduce = self.get_all_producers_stages(self.res_tensor) - \
                                    self.get_all_producers_stages(self.reduce_tensor) - {self.reduce_tensor}
        remaining_tensors = tensors_before_reduce | tensors_after_reduce | self.graph_info.endpoint_output_tensor_set

        for tensor in remaining_tensors:
            if tensor in self.graph_info.input_tensor_set:
                continue
            insn = self._get_insn(tensor)
            iter_var = self.sch[tensor].op.axis[0]
            if tensor in self.mid_output_tensor_cache_read_list:
                insn = "phony_insn"
            elif tensor in self.graph_info.real_output_tensor_set:
                insn = "dma_copy"
            if insn == "":
                insn = "dma_copy"

            self.emit_insn_map[tensor] = {SCOPE: self.branch_tensor_info.get_hook_tensor_axis(tensor, iter_var),
                                          INSTRUCTION: insn}

        if self.res_tensor.op.tag == FAKE_NODE_TAG:
            self.emit_insn_map[self.res_tensor] = {SCOPE: self.res_tensor.op.axis[0],
                                                   INSTRUCTION: "phony_insn"}
