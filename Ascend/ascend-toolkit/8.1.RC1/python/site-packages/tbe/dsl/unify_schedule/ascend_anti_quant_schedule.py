#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2021. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
ascend_anti_quant schedule
"""
from tbe import tvm
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform import scope_ubuf
from ..base.operation import register_schedule
from ..base.operation import get_context
from ..base.operation import var
from .constants import Pattern, DTYPE_BYTE_MAPPING
from .ascend_anti_quant_tilingcase import QuantTilingCase
from .ascend_quant_schedule import round_emit_insn

CAST_F16_NAME = "anti_quant_cast_f16_ub"
INPUT_NAME = "anti_quant_input_ub"
VMULS_REFORM_NAME = "anti_reform_by_vmuls"
SQRT_NAME = "anti_scale_sqrt_ub"
OFFSET_NAME = "anti_offset_ub"


@register_schedule(pattern=Pattern.ASCEND_ANTI_QUANT)
def schedule(outs, tiling_case):
    """
    ascend_anti_quant schedule
    """
    if not isinstance(tiling_case, QuantTilingCase):
        raise RuntimeError("QuantTilingCase required for AscendAntiQuant Schedule")

    return AscendAntiQuantSchedule(outs, tiling_case).do_schedule()


class AscendAntiQuantSchedule:
    def __init__(self, outs, tiling_case):
        self._scope = scope_ubuf
        self._core_dim = get_soc_spec("CORE_NUM")
        if self._scope.lower().find('.ub') != -1:
            self._total_size = get_soc_spec("UB_SIZE")
        else:
            raise RuntimeError("only support UB buffer now")

        self._schedule = None
        self._outs = outs
        self._res = outs[0]
        self._tiling_case = tiling_case
        self._ub_split_result = []
        self._block_tiling_vars = {}
        self._ub_tiling_vars = {}

        self._graph_info = get_context().get_current_compute().get("quant_graph_info")
        self._tensor_map = self._graph_info.mid_tensor_set
        self._max_ub_size = self._graph_info.max_single_tensor_ub_size
        self._quant_input_ub = self._graph_info.input_ub_tensor_set
        self._quant_reorder_ub = self._graph_info.reorder_tensor_set
        self._buffer_align_tensors = self._graph_info.buffer_align_tensor_set
        self._compute_inline_tensors = set()
        self._out_ub_tensors = set()

    def _do_cache_write(self):
        if self._graph_info.elewise_fuse and not self._graph_info.quant_fuse:
            for output_tensor in self._graph_info.output_tensor_set:
                output_tensor_stage = self._schedule.cache_write(output_tensor, self._scope)
                self._out_ub_tensors.add(output_tensor_stage)

    def _set_buffer_align(self):
        for tensor in self._buffer_align_tensors:
            self._schedule[tensor].buffer_align((1, 1),
                                                (1, 1),
                                                (1, 1),
                                                (32, 32))

    def _set_buffer_scope(self):
        """
        set the scope for tensors
        """
        for value in self._tensor_map | self._out_ub_tensors:
            self._schedule[value].set_scope(self._scope)

    def _do_storage_bound(self):
        for stage in self._tensor_map | self._out_ub_tensors:
            storage_bound = self._max_ub_size // DTYPE_BYTE_MAPPING[stage.dtype]
            self._schedule[stage].set_buffer_size(storage_bound)

    def _cal_compute_inline(self):
        for tensor in self._quant_input_ub:
            c_out = tensor.op.attrs['c_out'].value
            c1_transform = tensor.op.attrs['c1_transform'].value
            if c_out % c1_transform == 0:
                self._compute_inline_tensors.add(tensor)

    def _do_compute_inline(self):
        for tensor in self._compute_inline_tensors:
            self._schedule[tensor].compute_inline()

    def _reorder_buffer(self):
        """
        reorder all tensors to the same shape
        """
        factor = 2
        for tensor in self._quant_reorder_ub:
            self._schedule[tensor].split(tensor.op.axis[1], factor)

    def _do_tiling(self):
        case = self._tiling_case
        res = self._res
        sch = self._schedule

        # get tiling axis
        block_tiling_axis = case.block_tiling_axis
        ub_tiling_axis = case.ub_tiling_axis

        # get tiling params
        block_factor = case.block_factor
        ub_factor = case.ub_factor

        block_inner_factor = block_factor if block_factor is not None else var(
            "block_factor_" + str(block_tiling_axis) + "_" + str(case.is_fuse_block), (1, None))
        ub_inner_factor = ub_factor if ub_factor is not None else var(
            "ub_factor_" + str(ub_tiling_axis) + "_" + str(case.is_fuse_block), (1, None))

        # block tiling
        block_outer, block_inner = sch[res].split(res.op.axis[block_tiling_axis],
                                                  nparts=block_inner_factor)
        if case.is_split_ub:
            ub_outer, ub_inner = sch[res].split(res.op.axis[ub_tiling_axis],
                                                factor=ub_inner_factor)
        else:
            ub_outer, ub_inner = sch[res].split(block_inner, factor=ub_inner_factor)
        self._ub_split_result = [ub_outer, ub_inner]

        if case.is_fuse_block:
            fuse_axis_list = [sch[res].op.axis[i] for i in range(block_tiling_axis)]
            fuse_axis_list.append(block_outer)
            if len(fuse_axis_list) > 1:
                multi_core_bind_axis = sch[res].fuse(*fuse_axis_list)
            else:
                multi_core_bind_axis = fuse_axis_list[0]
        else:
            multi_core_bind_axis = block_outer

        if case.multi_core:
            block = tvm.thread_axis("blockIdx.x")
            sch[res].bind(multi_core_bind_axis, block)

    def _set_buffer_compute_at(self):
        """
        set the compute axis for tensors
        """
        ub_outer = self._ub_split_result[0]
        res = self._res
        sch = self._schedule

        for value in self._tensor_map | self._out_ub_tensors - self._compute_inline_tensors:
            sch[value].compute_at(sch[res], ub_outer)

    def _do_double_buffer(self):
        sch = self._schedule
        for value in self._tensor_map | self._out_ub_tensors:
            sch[value].double_buffer()

    def _set_buffer_emit_insn(self):
        """
        instruction mapping
        """
        res = self._res
        sch = self._schedule
        tensor_map = self._tensor_map
        ub_inner = self._ub_split_result[1]

        for tensor in tensor_map | self._out_ub_tensors - self._compute_inline_tensors:
            if tensor.op.tag == INPUT_NAME:
                sch[tensor].emit_insn(tensor.op.axis[0], 'dma_copy')
            elif tensor.op.tag == "input_ub":
                sch[tensor].emit_insn(tensor.op.axis[0], 'dma_padding')
            elif tensor.op.tag == "cast_i8_ub":
                round_mode = 'Round'
                if self._res.op.attrs:
                    round_mode = self._res.op.attrs['round_mode']
                sch[tensor].emit_insn(tensor.op.axis[0], round_emit_insn(round_mode))
            else:
                sch[tensor].emit_insn(tensor.op.axis[0], 'vector_auto')

        sch[res].emit_insn(ub_inner, 'dma_copy')

    def do_schedule(self):
        """
        auto_schedule for cce AI-CORE
        """
        self._schedule = tvm.create_schedule(self._res.op)

        self._schedule.tiling_key = self._tiling_case.tiling_key

        self._do_cache_write()

        self._set_buffer_align()

        self._set_buffer_scope()

        self._do_storage_bound()

        self._cal_compute_inline()

        self._do_compute_inline()

        self._reorder_buffer()

        self._do_tiling()

        self._set_buffer_compute_at()

        self._do_double_buffer()

        self._set_buffer_emit_insn()

        return self._schedule
