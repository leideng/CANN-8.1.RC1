#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
weight quant bmm tiling case
"""
import collections
import copy
from itertools import product

from tbe import tvm
from tbe.common.platform import platform_info as tbe_platform_info
from tbe.common.context import op_context
from tbe.dsl.base.operation import in_dynamic
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import register_tiling_case
from tbe.dsl.unify_schedule.cube_tilingcase import CubeTilingOp
from tbe.dsl.unify_schedule.cube_tilingcase import TilingSelection
from tbe.dsl.unify_schedule.cube_tilingcase import TilingUtils as utils
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.static_schedule.util import get_value
from .cube_tensor import TensorCube
from .gemm_tiling_util import CubeTiling



class TensorWeightQuantBatchMatMul(TensorCube):
    """
    use to get tensor for weight_quant_batch_matmul
    """

    def __init__(self, res_list):
        super().__init__(res_list)
        self._init_tensor_map()

    def set_pre_conv_flag(self):
        if self.tensor_map.get("tensor_weight_fp16").op.attrs.get("pre_conv") == "S322F16":
            return False
        return True

    def get_tensor_and_para_map(self):
        """
        get the tensor and para for weight_quant_batch_matmul
        """
        return [*self.all_tensor, self.tensor_map, self.para_map]

    def _init_tensor_map(self):
        self.tensor_map["tensor_out"] = self.last_tensor
        self.tensor_map["tensor_fixpipe_fp16"] = self.last_tensor.op.input_tensors[0]
        self.tensor_map["tensor_mad_fp32"] = self.tensor_map["tensor_fixpipe_fp16"].op.input_tensors[0]
        self.tensor_map["tensor_a_zz_fp16"] = self.tensor_map["tensor_mad_fp32"].op.input_tensors[0]
        self.tensor_map["tensor_b_zn_fp16"] = self.tensor_map["tensor_mad_fp32"].op.input_tensors[1]
        if len(self.tensor_map["tensor_mad_fp32"].op.input_tensors) > 2:
            self.tensor_map["tensor_bias_fp32"] = self.tensor_map["tensor_mad_fp32"].op.input_tensors[2]
            self.tensor_map["input_bias"] = self.tensor_map["tensor_bias_fp32"].op.input_tensors[0]
        self.tensor_map["input_a"] = self.tensor_map["tensor_a_zz_fp16"].op.input_tensors[0]
        self.tensor_map["tensor_weight_fp16"] = self.tensor_map["tensor_b_zn_fp16"].op.input_tensors[0]
        self.tensor_map["tensor_mad_int32"] = self.tensor_map["tensor_weight_fp16"].op.input_tensors[0]
        self.tensor_map["tensor_a_zz_int8"] = self.tensor_map["tensor_mad_int32"].op.input_tensors[0]
        self.tensor_map["tensor_b_zn_int8"] = self.tensor_map["tensor_mad_int32"].op.input_tensors[1]
        if len(self.tensor_map["tensor_mad_int32"].op.input_tensors) > 2:
            self.tensor_map["tensor_bias_int32"] = self.tensor_map["tensor_mad_int32"].op.input_tensors[2]
            self.tensor_map["input_q_bias"] = self.tensor_map["tensor_bias_int32"].op.input_tensors[0]
        self.tensor_map["input_b"] = self.tensor_map["tensor_b_zn_int8"].op.input_tensors[0]
        self.tensor_map["input_diag"] = self.tensor_map["tensor_a_zz_int8"].op.input_tensors[0]



class WeightQuantBatchMatMulStatisicTiling(CubeTiling):
    """
    the cube tiling op of weight_quant_batch_matmul
    """

    def __init__(self, tensor_list):
        super().__init__(tensor_list)
        self._trans_a = self.para_map.get("trans_a", False)
        self._trans_b = self.para_map.get("trans_b", False)

    def get_tiling_info_dict(self):
        """
        get the tiling input: tiling info dict
        """
        a_shape, b_shape = self._get_input_shape()
        info_dict = {
            "op_type": "weight_quant_batch_matmul",
            "A_shape": a_shape,
            "B_shape": b_shape,
            "C_shape": None,
            "A_dtype": self.dtype_a,
            "B_dtype": self.dtype_b,
            "C_dtype": self.dtype_c,
            "mad_dtype": "float16",
            "padl": 0,
            "padr": 0,
            "padu": 0,
            "padd": 0,
            "strideH": 1,
            "strideW": 1,
            "strideH_expand": 1,
            "strideW_expand": 1,
            "dilationH": 1,
            "dilationW": 1,
            "group": 1,
            "bias_flag": self.tensor_map.get("input_bias") is not None,
            "fused_double_operand_num": 1,
            "kernel_name": self.para_map.get("kernel_name"),
        }
        if in_dynamic():
            info_dict_dynamic = {
                "op_tag": "weight_quant_batch_matmul",
                "dynamic_shape_flag": True,
                "trans_a": bool(self._trans_a),
                "trans_b": bool(self._trans_b),
                "unaligned_flag": bool(self.para_map.get("unaligned_flag"))
            }
            info_dict.update(info_dict_dynamic)
        self.tiling_info_dict = info_dict
        return self.tiling_info_dict

    def _get_input_shape(self):
        a_shape = [get_value(i) for i in self.tensor_map["input_a"].shape]
        b_shape = [get_value(i) for i in self.tensor_map["input_b"].shape]
        return a_shape, b_shape


@register_tiling_case(pattern=Pattern.WEIGHT_QUANT_BATCH_MATMUL)
def calc_weight_quant_bmm(outs, option=None):
    """
    tiling_case func for dynamic shape weight_quant_batch_matmul

    Parameters
    ----------
    outs: tvm tensor or list of tvm tensor, results for tvm compute

    Returns
    -------
    list of dict, each dict for a tiling case
    """
    tensor_weight_quant_bmm = TensorWeightQuantBatchMatMul(outs)
    tensor_list = tensor_weight_quant_bmm.get_tensor_and_para_map()
    pre_conv_flag = tensor_weight_quant_bmm.set_pre_conv_flag()
    weight_quant_bmm_tiling = WeightQuantBatchMatMulStatisicTiling(tensor_list)
    tiling_info_dict = weight_quant_bmm_tiling.get_tiling_info_dict()
    context = op_context.get_context()

    tiling_cases = []
    new_info = copy.deepcopy(tiling_info_dict)
    dynamic_mod = "weight_quant_bmm"
    tiling_op = WeightQuantBatchMatMulTiling(new_info, dynamic_mod, pre_conv_flag)
    selector = TilingSelection(tiling_op)
    tiling_cases += selector.calc_tiling(None)

    return tiling_cases


class WeightQuantBatchMatMulTiling(CubeTilingOp):
    """
    the cube tiling op of matmul
    """

    def __init__(self, tiling_info, dynamic_mod, pre_conv_flag):
        super().__init__(tiling_info, dynamic_mod)
        self.a_info = self.tiling_info["A_shape"]
        self.b_info = self.tiling_info["B_shape"]
        self.c_info = self.tiling_info["C_shape"]
        self.a_type = self.tiling_info["A_dtype"]
        self.b_type = self.tiling_info["B_dtype"]
        self.c_type = self.tiling_info["C_dtype"]
        self.format_a = "nd"
        self.format_b = "nd"
        self.unaligned_flag = self.tiling_info["unaligned_flag"]
        self.bias_flag = self.tiling_info["bias_flag"]
        self.support_l0c2out = tbe_platform_info.intrinsic_check_support("Intrinsic_fix_pipe_l0c2out")
        self._get_calc_info()
        self.key = ("A_shape", "B_shape")
        self.op_type = "weight_quant_batch_matmul"
        self.use_cache_tiling = True
        self.pre_conv_flag = pre_conv_flag
        # block_n, block_m, mal1, nbl1, kal1, kbl1, n_l0c_value, m_l0c_value, k_l0a, is_al1_double, is_bl1_double
        self.tiling_value = [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]

    @staticmethod
    def preprocess_tiling(tiling_in):
        """
        preprocess tiling for get tiling range

        Parameters
        ----------
        tiling_in : dict, result of tiling fetch

        Returns
        -------
        tiling_case, range covered for tiling
        """

        tiling = copy.deepcopy(tiling_in)
        return tiling

    @staticmethod
    def get_compile_time(target_area):
        """
        caculate total all compile time depends on target_area
        """
        compile_time = 1
        for value in target_area:
            compile_time *= (value[1] - value[0] + 1)
        return compile_time

    def get_cache_tiling(self):
        '''
        according to size in l1, generate 9 kind of templates, each subdivided into 132 different
        templates as follows templates according to size in l1 sub template
        --------------------------------------------|-----
        al1 @l0c and bl1 @l0c                       | 48
        al1 @l0c and bl1 @ddr                       | 16
        al1 @l0c and bl1 full load                  | 8
        al1 @ddr and bl1 @l0c                       | 16
        al1 @ddr and bl1 @ddr                       | 16
        al1 @ddr and bl1 full load                  | 8
        al1 full load and bl1 @l0c                  | 8
        al1 full load and bl1 @ddr                  | 8
        al1 full load and bl1 full load             | 4

        Returns
        ----------
        cache_tiling_all: list, include 132 different tiling templates
        '''
        # add compile_info
        info_dict = self.tiling_info
        add_compile_info("binary_mode_flag", True)
        # get cache_tiling
        cache_tiling_all = {}
        add_compile_info("fused_double_operand_num", 0)
        add_compile_info("binary_attrs", {
            "bias_flag": True,
            "nd_flag": True,
            "split_k_flag": False,
            "zero_flag": False,
            "weight_nz": False
        })
        attach_choices = self._get_attach_choices()
        for choice in attach_choices:
            cache_tiling = {
                'block_dim': [-1, -1, -1, 1],
                'AL0_matrix': [-1, -1, utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
                'BL0_matrix': [-1, -1, utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
                'CL0_matrix': [-1, -1, utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
                'CUB_matrix': [-1, -1, utils.CUBE_SIZE, utils.CUBE_SIZE, 1, 1],
                'BUB_shape': [-1, -1, 1, 1], 'AL1_shape': [-1, -1, 1, 1], 'BL1_shape': [-1, -1, 1, 1],
                'AUB_shape': [-1, -1, 1, 1], 'n_bef_batch_flag': 0, 'n_bef_group_flag': 0,
                'batch_bef_group_flag': 0, 'A_overhead_opt_flag': 0, 'B_overhead_opt_flag': 0,
                'AUB_channel_wise_flag': None, 'BUB_channel_wise_flag': None, 'CUB_channel_wise_flag': None,
                'manual_pingpong_buffer':
                    {'AUB_pbuffer': utils.DB_ON, 'BUB_pbuffer': utils.DB_ON,
                     'AL1_pbuffer': utils.DB_ON, 'BL1_pbuffer': utils.DB_ON,
                     'AL0_pbuffer': utils.DB_ON, 'BL0_pbuffer': utils.DB_ON,
                     'CL0_pbuffer': utils.DB_ON, 'CUB_pbuffer': utils.DB_ON,
                     'UBG_pbuffer': utils.DB_OFF},
                'attach_at_flag': {
                    'cl0_attach_flag': utils.ATTACH_LARGE,
                    'al1_attach_flag': -1, 'bl1_attach_flag': -1,
                    'aub_attach_flag': utils.ATTACH_LESS,
                    'bub_attach_flag': utils.ATTACH_LESS,
                    'abkl1_attach_flag': -1, 'aub_multi_flag': -1,
                    'bub_multi_flag': -1, 'l0c_multi_batch': -1},
                "non_factor_bmn_flag": -1, 'non_factor_k_flag': -1,
                "performance_flag": -1, "unaligned_flag": -1
            }
            if self._check_template_valid(choice):
                continue

            cache_tiling.get('manual_pingpong_buffer')['AL1_pbuffer'] = choice[0]
            cache_tiling.get('manual_pingpong_buffer')['BL1_pbuffer'] = choice[1]
            cache_tiling.get('manual_pingpong_buffer')['CL0_pbuffer'] = choice[2]
            cache_tiling.get('attach_at_flag')['abkl1_attach_flag'] = choice[3]
            cache_tiling.get('attach_at_flag')['al1_attach_flag'] = choice[4]
            cache_tiling.get('attach_at_flag')['bl1_attach_flag'] = choice[5]
            cache_tiling_all[self._get_tiling_id(choice)] = [[], cache_tiling, []]
        return cache_tiling_all

    def assembly_case(self, m_k_n_shape, tiling, coverage, cnt):
        """
        get the covered info of a tiling

        Parameters
        ----------
        tiling : dict, result of tiling fetch

        coverage : list, size of dymanic element

        cnt: index of tiling

        Returns
        -------
        tiling_case, range covered for tiling
        """

        var_range = collections.OrderedDict()

        return {"key": cnt, "tiling_strategy": tiling, "var_range": var_range, "m_k_n_shape": m_k_n_shape}

    def _check_template_valid(self, choice):
        """
        Check if the template is valid

        Returns
        -------
        bool: True, the template is valid
        """
        (al1_db, bl1_db, _, abkl1_attach, al1_attach_flag, bl1_attach_flag, _) = choice

        # al1 full load, check invalid abkl1_attach
        invalid_choice = al1_attach_flag == 1 and al1_db == 2

        invalid_choice |= al1_attach_flag == 0 and al1_db == 1

        invalid_choice |= bl1_attach_flag == 0 and bl1_db == 1

        invalid_choice |= al1_attach_flag == 1 and bl1_attach_flag == 1 and abkl1_attach == 0

        invalid_choice |= al1_attach_flag != bl1_attach_flag and abkl1_attach == 1

        invalid_choice |= al1_attach_flag == 0 and al1_db == 1

        return invalid_choice

    def _get_calc_info(self):
        """
        preprocess info, convert tvm var to -1
        """

        self._convert_type(self.a_info, self.b_info)

    def _get_tiling_id(self, choice):
        '''
        ---------------------------------------
        | db_flag | al1_db | bl1_db | cl0_db
        ---------------------------------------
        |    1    |   off  |   off  |  off
        |    2    |   off  |   off  |  on
        |    3    |   off  |   on   |  off
        |    4    |   off  |   on   |  on
        |    5    |   on   |   off  |  off
        |    6    |   on   |   off  |  on
        |    7    |   on   |   on   |  off
        |    8    |   on   |   on   |  on

        Returns
        ----------
        kernel_mode: int, kernel flag in binary
        '''

        (al1_pb, bl1_pb, l0c_pb, abkl1_attach, al1_attach_flag, bl1_attach_flag, pre_conv_flag) = choice
        tiling_id = pre_conv_flag
        tiling_id = (tiling_id << 1) + bl1_attach_flag
        tiling_id = (tiling_id << 1) + al1_attach_flag
        tiling_id = (tiling_id << 1) + abkl1_attach
        tiling_id = (tiling_id << 1) + l0c_pb - 1
        tiling_id = (tiling_id << 1) + bl1_pb - 1
        tiling_id = (tiling_id << 1) + al1_pb - 1

        return tiling_id

    def _get_attach_choices(self):
        """
        generates all selections of l0 flags

        Returns
        -------
        list: all selections of flags
        """
        pre_conv_flag_choice = [0]
        if self.pre_conv_flag:
            pre_conv_flag_choice = [1]
        (al1_pb, bl1_pb, l0c_pb, abkl1_attach, al1_attach_flag, bl1_attach_flag,
         pre_conv_flag) = ([utils.DB_OFF, utils.DB_ON], [utils.DB_OFF, utils.DB_ON],
                           [utils.DB_OFF, utils.DB_ON], [0, 1], [0, 1], [0, 1], pre_conv_flag_choice)
        choice_list = [al1_pb, bl1_pb, l0c_pb, abkl1_attach, al1_attach_flag, bl1_attach_flag, pre_conv_flag]
        attach_choices = list(product(*choice_list))
        return attach_choices
