#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
split schedule
"""
from typing import Optional
from typing import Dict
from typing import Tuple
from typing import List
from operator import mul
from functools import reduce

from tbe import tvm
from tbe.dsl.base import operation
from tbe.common.utils import op_tiling
from tbe.dsl.base.operation import get_compile_info
from tbe.common.platform import get_block_size

from tbe.dsl.unify_schedule import util
from tbe.dsl.unify_schedule.schedule import Schedule
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.constants import CompileInfo
from tbe.dsl.unify_schedule.constants import SplitPattern
from tbe.dsl.unify_schedule.constants import DTYPE_BYTE_MAPPING
from .split_tilingcase import TilingStrategy
from .split_tilingcase import SplitTilingCase
from ...constants import FAKE_NODE_TAG

DEFAULT = "default"
SCOPE_UB = "local.UB"
DMA_COPY = "dma_copy"
COPY_UB_TO_UB = "dma_copy"
VNCHWCONV = "vnchwconv"
PHONY_INSN = "phony_insn"
EMPTY_TENSOR = "empty_tensor"

REPEAT_LIMIT = 255
TRANSPOSE_FACTOR = 16
B8_TRANSPOSE_FACTOR = 2
TYPE_CAST_LIMIT = 2
M_INDEX = 0
N_INDEX = 1
ALIGN_NODE_NUMBERS = 2
BASE_NODE_NUMBERS = 2
GENERAL_NODE_NUMBERS = 3
B16_OUTPUT_NUM_LIMIT = 56
DEFAULT_OUTPUT_NUM_LIMIT = 40


class ComputeAt:
    """
    SplitSchedule ComputeAt
    """

    def __init__(self):
        self._compute_at_axis = None

    @property
    def compute_at_axis(self):
        """
        :return: compute_at_axis
        """
        return self._compute_at_axis

    @compute_at_axis.setter
    def compute_at_axis(self, axis):
        """
        set compute_at_axis
        :param axis:
        :return:
        """
        self._compute_at_axis = axis


class EmitInsn:
    """
    SplitSchedule EmitInsn Bean
    """

    def __init__(self):
        self._emit_insn_axis = None

    @property
    def emit_insn_axis(self):
        """
        :return: emit_insn_axis
        """
        return self._emit_insn_axis

    @emit_insn_axis.setter
    def emit_insn_axis(self, axis):
        """
        :param axis:
        :return: emit_insn_axis
        """
        self._emit_insn_axis = axis


class SplitSchedule(Schedule):
    """
    SplitSchedule
    """

    def __init__(self, outs: List[tvm.Tensor], tiling_case: SplitTilingCase):
        self._out: List[tvm.Tensor] = outs if isinstance(outs, (list, tuple)) else [outs]
        self._dtype = self._out[0].dtype
        self._schedule: Optional[tvm.schedule] = None
        self._tiling_case: SplitTilingCase = tiling_case
        self._is_const = tiling_case.tiling_strategy == TilingStrategy.CONST
        self._avg_split = operation.get_context().get("_avg_split")

        self._in_out_map = {}
        self._input_tensors = set()
        self._out_tensors = []
        self._middle_tensors = set()

        self._cache_read_tensors = []
        self._cache_write_tensors = []
        self._cache_read_buffers = []
        self._cache_write_buffers = []

        self._block_split_axis = self._tiling_case.block_split_axis
        self._block_factor = 1
        self._ub_factors_m = []
        self._ub_factors_n = []
        self._block_dims = 1

        self._in_reshape_map = {}
        self._out_reshape_map = {}
        self._in_reshape_tensors = set()
        self._out_transpose_tensors = []
        self._in_transpose_tensors = set()
        self._ub_split_tensors = []

        self._coexisting_quantity = GENERAL_NODE_NUMBERS
        self.block_size = get_block_size()

        self._storage_align_map = {}
        self._compute_align_map: Dict[List[Tuple[int, int]]] = {}

        self._reorder_axis = []

        self._compute_inline_tensors = set()

        self._compute_at_map = {}
        self._compute_at_axis: List[ComputeAt] = [ComputeAt() for _ in range(len(self._out))]

        self._emit_insn_axis: List[EmitInsn] = [EmitInsn() for _ in range(len(self._out))]
        self._emit_insn_map = {}

        self._constraints = []

    @property
    def is_none_cut(self):
        return self._tiling_case.tiling_strategy in [TilingStrategy.NONE_CUT, TilingStrategy.EMPTY]

    @property
    def is_base_single(self):
        return self._tiling_case.tiling_strategy == TilingStrategy.BASE_SINGLE

    @property
    def is_base_double(self):
        return self._tiling_case.tiling_strategy == TilingStrategy.BASE_DOUBLE

    @property
    def is_base(self):
        return self.is_base_single or self.is_base_double

    @property
    def is_general_transpose(self):
        return self._tiling_case.tiling_strategy == TilingStrategy.GENERAL_TRANSPOSE

    @property
    def is_all_align_mte(self):
        return self._tiling_case.tiling_strategy == TilingStrategy.ALL_ALIGN_MTE

    @property
    def is_all_align_ub_copy(self):
        return self._tiling_case.tiling_strategy == TilingStrategy.ALL_ALIGN_UB_COPY

    @property
    def is_cut_m_transpose(self):
        return self._tiling_case.tiling_strategy == TilingStrategy.CUT_M_TRANSPOSE

    @property
    def is_all_general(self):
        return self._tiling_case.tiling_strategy in [TilingStrategy.GENERAL_TRANSPOSE, TilingStrategy.NONE_CUT]

    @property
    def is_transpose_strategy(self):
        return self._tiling_case.tiling_strategy in \
               [TilingStrategy.GENERAL_TRANSPOSE, TilingStrategy.NONE_CUT, TilingStrategy.CUT_M_TRANSPOSE]

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.SPLIT]

    @classmethod
    def get_supported_sub_pattern(cls):
        return [SplitPattern.S_0]

    def do_schedule(self):
        self._construct_compute_graph()
        self._calc_tiling()

        self._schedule = self._create_schedule()
        self._schedule.tiling_key = self._tiling_case.tiling_key

        self._calc_cache_read()
        self._do_cache_read()

        self._calc_cache_write()
        self._do_cache_write()

        self._set_scope()

        self._calc_reshape()
        self._calc_transpose()
        self._do_reshape()
        self._do_transpose()

        self._reinterpret_cast()

        self._calc_storage_align()
        self._calc_compute_align()
        self._calc_storage_bound()
        self._calc_reorder()
        self._calc_compute_inline()
        self._calc_multi_core()
        self._calc_compute_at()
        self._calc_double_buffer()
        self._calc_axis_group()
        self._calc_constraints()
        self._calc_emit_insn()

        self._do_storage_align()
        self._do_compute_align()
        self._do_tiling()
        self._do_reorder()
        self._do_compute_with()
        self._do_multi_core()
        self._do_storage_bound()
        self._do_compute_at()
        self._do_compute_inline()
        self._do_double_buffer()
        self._do_constraints()
        self._do_emit_insn()
        self._do_axis_group()

        self._add_compile_info()

        return self._schedule if self._check_tiling_case() else None

    def _is_b8_dtype(self):
        return DTYPE_BYTE_MAPPING.get(self._dtype) == 1

    def _ele_in_block(self):
        return get_block_size() // DTYPE_BYTE_MAPPING.get(self._dtype)

    def _use_compute_with(self):
        return not self._avg_split and self._is_const and self.is_base

    def _construct_compute_graph(self):
        def _dfs_sub_graph(out):
            for tensor_i in out.op.input_tensors:
                util.merge_value(self._in_out_map, tensor_i, out)
                if util.is_placeholder(tensor_i):
                    self._input_tensors.add(tensor_i)
                else:
                    self._middle_tensors.add(tensor_i)
                _dfs_sub_graph(tensor_i)

        for _out in self._out:
            _dfs_sub_graph(_out)
        self._out_tensors.extend(self._out)

    def _fake_node(self, dst_shape, outputs, axis):
        def concat_func(*indices):
            func = None
            concat_axis_size = sum(t.shape[axis] for t in outputs)
            for tensor_i in reversed(outputs):
                index = []
                for i, _ in enumerate(dst_shape):
                    if i == axis:
                        index.append(indices[i] - (concat_axis_size - tensor_i.shape[axis]))
                    else:
                        index.append(indices[i])
                if func is None:
                    func = tensor_i(*index)
                else:
                    func = tvm.select(indices[axis] < concat_axis_size, tensor_i(*index), func)
                concat_axis_size -= tensor_i.shape[axis]
            return func

        with tvm.tag_scope(FAKE_NODE_TAG):
            concat = tvm.compute(dst_shape, concat_func, name="fake_node")
        return concat

    def _create_schedule(self):
        if len(self._out) > 1 and not self.is_base:
            dst_shape = util.shape_to_list(list(self._input_tensors)[0].shape)
            self._out = [self._fake_node(dst_shape, self._out, 1)]
        res_op = [_out.op for _out in self._out]
        return tvm.create_schedule(res_op)

    def _calc_cache_read(self):
        self._cache_read_tensors.extend(self._input_tensors)

    def _do_cache_read(self):
        input_tensor = self._cache_read_tensors[0]
        for tensor_i in self._out:
            if len(self._out) == 1:
                use_tensors = self._in_out_map.get(input_tensor)
            else:
                use_tensors = [tensor_i]
            buffer_tensor = self._schedule.cache_read(input_tensor, SCOPE_UB, use_tensors)
            self._cache_read_buffers.append(buffer_tensor)

    def _calc_cache_write(self):
        self._cache_write_tensors.extend(self._out_tensors)

    def _do_cache_write(self):
        for tensor_i in self._cache_write_tensors:
            buffer_tensor = self._schedule.cache_write(tensor_i, SCOPE_UB)
            self._cache_write_buffers.append(buffer_tensor)

    def _set_scope(self):
        sch = self._schedule
        for tensor_i in self._middle_tensors:
            sch[tensor_i].set_scope(SCOPE_UB)

    def _calc_reshape(self):
        def calc_cut_reshape_factor():
            factors = [("factor", self._ub_factors_m[0], 1)]
            if self._is_b8_dtype():
                factors.append(("nparts", TRANSPOSE_FACTOR * B8_TRANSPOSE_FACTOR, 1))
                factors.append(("factor", B8_TRANSPOSE_FACTOR, 0))
            else:
                factors.append(("nparts", TRANSPOSE_FACTOR, 1))
            return factors

        def calc_none_cut_reshape_factor():
            ele_in_block = self._ele_in_block()
            factors = [("factor", ele_in_block, 1)]
            if self._is_b8_dtype():
                factors.append(("factor", TRANSPOSE_FACTOR, 1))
            return factors

        if not self.is_transpose_strategy:
            return

        if self.is_none_cut:
            reshape_factors = calc_none_cut_reshape_factor()
        else:
            reshape_factors = calc_cut_reshape_factor()
        for tensor_i in self._cache_read_buffers:
            read_cache_cache = self._schedule.cache_read(tensor_i, SCOPE_UB, self._cache_write_buffers)
            self._in_reshape_map[read_cache_cache] = reshape_factors

        for tensor_i in self._cache_write_buffers:
            self._out_reshape_map[tensor_i] = reshape_factors

    def _calc_transpose(self):
        pass

    def _calc_tiling(self):
        funcs = {TilingStrategy.NONE_CUT: self._calc_tiling_none_cut,
                 TilingStrategy.CONST: self._calc_tiling_const,
                 TilingStrategy.BASE_SINGLE: self._calc_tiling_base_single_split,
                 TilingStrategy.BASE_DOUBLE: self._calc_tiling_base_double_split,
                 TilingStrategy.GENERAL_TRANSPOSE: self._calc_tiling_general_split,
                 TilingStrategy.ALL_ALIGN_MTE: self._calc_tiling_align_mte_split,
                 TilingStrategy.ALL_ALIGN_UB_COPY: self._calc_tiling_align_ub_copy_split,
                 TilingStrategy.CUT_M_TRANSPOSE: self._calc_tiling_cut_m_transpose_split,
                 TilingStrategy.EMPTY: self._calc_tiling_none_cut}
        funcs.get(self._tiling_case.tiling_strategy)()

    def _calc_tiling_base_single_split(self):
        # nparts split
        self._block_factor = operation.var_inner("_core_num", (1, util.get_core_num()), dtype="int64")
        ub_factor_m = 1
        for index, _out in enumerate(self._out):
            if (index == 0 and self._avg_split) or not self._avg_split:
                shape = util.shape_to_list(_out.shape)
                ub_bound_m = (1, util.get_bound(shape[M_INDEX])[1])
                ub_factor_m = operation.var_inner(f"_ub_factor_{M_INDEX}_{index}", ub_bound_m, dtype="int64")
            self._ub_factors_m.append(ub_factor_m)

    def _calc_tiling_base_double_split(self):
        self._block_factor = operation.var_inner("_core_num", (1, util.get_core_num()), dtype="int64")
        ub_factor_m = 1
        ub_factor_n = 1
        for index, _out in enumerate(self._out):
            if (index == 0 and self._avg_split) or not self._avg_split:
                shape = util.shape_to_list(_out.shape)
                ub_bound_m = (1, util.get_bound(shape[M_INDEX])[1])
                ub_bound_n = (1, util.get_bound(shape[N_INDEX])[1])
                ub_factor_m = operation.var_inner(f"_ub_factor_{M_INDEX}_{index}", ub_bound_m, dtype="int64")
                ub_factor_n = operation.var_inner(f"_ub_factor_{N_INDEX}_{index}", ub_bound_n, dtype="int64")
            self._ub_factors_m.append(ub_factor_m)
            self._ub_factors_n.append(ub_factor_n)

    def _calc_tiling_align_ub_copy_split(self):
        shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        b_i = self._block_split_axis
        b_bound = (1, util.get_bound(shape[b_i])[1])
        ub_bound_m = (1, util.get_bound(shape[M_INDEX])[1])
        self._block_factor = operation.var_inner("_block_factor_" + str(b_i), b_bound, dtype="int64")
        self._ub_factors_m = [operation.var_inner(f"_ub_factor_{M_INDEX}", ub_bound_m, dtype="int64")]

    def _calc_tiling_align_mte_split(self):
        shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        b_i = self._block_split_axis
        block_bound = (1, util.get_bound(shape[M_INDEX] * shape[N_INDEX])[1])
        ub_bound_m = (1, util.get_bound(shape[M_INDEX])[1])
        ub_bound_n = (1, util.get_bound(shape[N_INDEX])[1])
        self._block_factor = operation.var_inner("_block_factor_" + str(b_i), block_bound, dtype="int64")
        self._ub_factors_m = [operation.var_inner("_ub_factor_" + str(M_INDEX), ub_bound_m, dtype="int64")]
        self._ub_factors_n = [operation.var_inner("_ub_factor_" + str(N_INDEX), ub_bound_n, dtype="int64")]

    def _calc_tiling_cut_m_transpose_split(self):
        shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        b_i = self._block_split_axis
        upper_bound = util.get_bound(shape[b_i])[1]
        block_bound = (1, upper_bound)
        ele_in_block = self._ele_in_block()
        self._block_factor = operation.var_inner("_block_factor_" + str(b_i), block_bound, dtype="int64")
        if isinstance(upper_bound, int):
            upper_bound = max(upper_bound, TRANSPOSE_FACTOR * ele_in_block)
        self._ub_factors_m = [operation.var_inner(f"_ub_factor_{M_INDEX}", (1, upper_bound), dtype="int64")]

    def _calc_tiling_general_split(self):
        shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        b_i = self._block_split_axis
        block_bound = (1, util.get_bound(shape[b_i])[1])
        u_h_bound = (1, util.get_bound(shape[N_INDEX])[1])
        self._block_factor = operation.var_inner("_block_factor_" + str(b_i), block_bound, dtype="int64")
        ele_in_block = self._ele_in_block()
        ub_factor_m = TRANSPOSE_FACTOR * ele_in_block
        self._ub_factors_m = [ub_factor_m]
        self._ub_factors_n = [operation.var_inner("_ub_factor_" + str(N_INDEX), u_h_bound, dtype="int64")]

    def _calc_tiling_none_cut(self):
        pass

    def _calc_tiling_const(self):
        def get_tiling():
            inputs = []
            for _input in self._input_tensors:
                input_shape = util.shape_to_list(_input.shape)
                inputs.append({"shape": input_shape, "dtype": _input.dtype})
            outputs = []
            for _output in self._out_tensors:
                output_shape = util.shape_to_list(_output.shape)
                outputs.append({"shape": output_shape, "dtype": _output.dtype})
            ori_compile_info = get_compile_info()
            new_compile_info = {
                CompileInfo.PATTERN: ori_compile_info.get(CompileInfo.PATTERN),
                CompileInfo.CORE_NUM: util.get_core_num(),
                CompileInfo.UB_SIZE: util.get_ub_size(),
                CompileInfo.UB_BLOCK_SIZE: util.get_ub_block_size(),
                "_ori_axis": 1,
                "_only_const_tiling": True,
                "_is_const": False,
                "_avg_split": self._avg_split,
            }

            op_type = "AutoTiling"
            run_info = op_tiling.do_op_tiling(op_type, new_compile_info, inputs, outputs)
            return run_info

        def calc_const_tiling_strategy(tiling_data):
            need_do_block = tiling_data.get("need_multi_core") > 0
            const_is_base = tiling_data.get("is_base") > 0
            const_is_base_double = tiling_data.get("is_base_double") > 0
            const_is_all_align_copy = tiling_data.get("is_all_align_ub_copy") > 0
            const_is_all_align = tiling_data.get("is_all_align_mte") > 0
            const_only_cut_m = tiling_data.get("cut_m_transpose") > 0
            if not need_do_block:
                return TilingStrategy.NONE_CUT
            if const_is_base_double:
                return TilingStrategy.BASE_DOUBLE
            if const_is_base and not const_is_base_double:
                return TilingStrategy.BASE_SINGLE
            if const_is_all_align_copy:
                self._tiling_case.enable_db = True
                return TilingStrategy.ALL_ALIGN_UB_COPY
            if const_is_all_align:
                return TilingStrategy.ALL_ALIGN_MTE
            if const_only_cut_m:
                return TilingStrategy.CUT_M_TRANSPOSE
            input_shape = util.shape_to_list(list(self._input_tensors)[0].shape)
            if reduce(mul, input_shape, 1) == 0:
                return TilingStrategy.EMPTY
            return TilingStrategy.GENERAL_TRANSPOSE

        def mark_empty_tensor():
            for index, _out in enumerate(self._out):
                out_shape = util.shape_to_list(self._out_tensors[index].shape)
                is_empty_tensor = (isinstance(out_shape[0], int) and out_shape[0] == 0 or
                                   isinstance(out_shape[1], int) and out_shape[1] == 0)
                if is_empty_tensor:
                    _out.op.attrs[EMPTY_TENSOR] = True

        def decode_tiling(run_info):
            output_num = 1 if self._avg_split else len(self._out_tensors)
            tiling_format = {
                "need_multi_core": "int64",
                "is_base": "int64",
                "is_base_double": "int64",
                "is_all_align_ub_copy": "int64",
                "is_all_align_mte": "int64",
                "cut_m_transpose": "int64",
                "block_axis": "int64",
                "block_factor": "int64",
                "ub_factors_m": [output_num, "int64"],
                "ub_factors_n": [output_num, "int64"]
            }
            tiling_data = op_tiling.decode(run_info.get("tiling_data"), tiling_format)
            self._block_dims = run_info.get("block_dim")
            const_is_base = tiling_data.get("is_base") > 0
            self._block_split_axis = tiling_data.get("block_axis")
            self._block_factor = tiling_data.get("block_factor")
            self._ub_factors_m = tiling_data.get("ub_factors_m")
            self._ub_factors_n = tiling_data.get("ub_factors_n")
            if self._avg_split and const_is_base:
                self._ub_factors_m = [self._ub_factors_m[0]] * len(self._out_tensors)
                self._ub_factors_n = [self._ub_factors_n[0]] * len(self._out_tensors)
            return tiling_data

        run_info = get_tiling()
        tiling_data = decode_tiling(run_info)
        self._tiling_case.tiling_strategy = calc_const_tiling_strategy(tiling_data)
        mark_empty_tensor()

    def _calc_storage_bound(self):
        self._coexisting_quantity = GENERAL_NODE_NUMBERS
        if self.is_base:
            self._coexisting_quantity = BASE_NODE_NUMBERS
            if len(self._out_tensors) == 1:
                self._coexisting_quantity = 1
        if self.is_all_align_mte:
            self._coexisting_quantity = 1
        if self.is_all_align_ub_copy:
            self._coexisting_quantity = ALIGN_NODE_NUMBERS

    def _calc_reorder(self):
        pass

    def _calc_compute_inline(self):
        if self.is_transpose_strategy:
            self._compute_inline_tensors.update(self._in_reshape_map.keys())
            self._compute_inline_tensors.update(self._in_reshape_tensors)
            self._compute_inline_tensors.update(self._cache_write_buffers)

        if self.is_all_align_mte or self.is_base:
            self._compute_inline_tensors.update(self._cache_write_buffers)

    def _calc_multi_core(self):
        pass

    def _calc_compute_at(self):
        if self.is_none_cut:
            return

        for index, out_tensor in enumerate(self._out):
            self._compute_at_map[self._cache_read_buffers[index]] = [out_tensor, self._compute_at_axis[index]]
            self._compute_at_map[self._cache_write_buffers[index]] = [out_tensor, self._compute_at_axis[index]]

        if self.is_all_align_ub_copy:
            for tensor_i in self._cache_write_buffers:
                self._compute_at_map[tensor_i] = [self._out[0], self._compute_at_axis[0]]

        for tensor_i in self._in_transpose_tensors:
            self._compute_at_map[tensor_i] = [self._out[0], self._compute_at_axis[0]]
        for tensor_i in self._ub_split_tensors:
            self._compute_at_map[tensor_i] = [self._out[0], self._compute_at_axis[0]]
        for tensor_i in self._out_transpose_tensors:
            self._compute_at_map[tensor_i] = [self._out[0], self._compute_at_axis[0]]
        if not self.is_base:
            for tensor_i in self._out_tensors:
                self._compute_at_map[tensor_i] = [self._out[0], self._compute_at_axis[0]]

    def _calc_storage_align(self):
        offset = 0
        factor = self._ele_in_block()
        if self.is_general_transpose:
            for tensor_i in self._cache_read_buffers:
                self._storage_align_map[tensor_i] = [tensor_i.op.axis[-2], factor, offset]
        if self.is_base and len(self._out) != 1:
            for tensor_i in self._cache_read_buffers:
                self._storage_align_map[tensor_i] = [tensor_i.op.axis[-2], factor, offset]
            for tensor_i in self._cache_write_buffers:
                self._storage_align_map[tensor_i] = [tensor_i.op.axis[-2], factor, offset]

    def _calc_compute_align(self):
        def get_align_axis():
            if self._is_b8_dtype():
                return -3
            return -2

        def add_compute_align(tensor, axis, factor):
            if tensor in self._compute_align_map:
                self._compute_align_map.get(tensor).append((axis, factor))
            else:
                self._compute_align_map[tensor] = [(axis, factor)]

        def compute_align_list(tensor_list, factor_list):
            for tensor_i in tensor_list:
                for axis_index, factor in factor_list:
                    add_compute_align(tensor_i, tensor_i.op.axis[axis_index], factor)

        def get_align_factor():
            if self._is_b8_dtype():
                return TRANSPOSE_FACTOR
            return ele_in_block

        def general_out_align_const():
            cur_sum = 0
            for index, (ub_tensor, out_tensor) in \
                    enumerate(zip(self._ub_split_tensors, self._out_transpose_tensors)):
                out_shape = util.shape_to_list(self._out_tensors[index].shape)
                cur_sum += out_shape[1]
                if cur_sum > self._ub_factors_n[0]:
                    add_compute_align(ub_tensor, ub_tensor.op.axis[get_align_axis()], ele_in_block)
                    add_compute_align(out_tensor, out_tensor.op.axis[-1], ele_in_block)
                cur_sum %= self._ub_factors_n[0]

        def general_out_align_dynamic():
            for index, (ub_tensor, out_tensor) in \
                    enumerate(zip(self._ub_split_tensors, self._out_transpose_tensors)):
                factor = operation.var_inner(f"_align_factor_{index}", (1, ele_in_block), dtype="int64")
                add_compute_align(ub_tensor, ub_tensor.op.axis[get_align_axis()], factor)
                add_compute_align(out_tensor, out_tensor.op.axis[-1], factor)

        def general_out_align():
            if not self._avg_split:
                if self._is_const:
                    general_out_align_const()
                else:
                    general_out_align_dynamic()

        ele_in_block = self._ele_in_block()
        if self.is_general_transpose:
            general_out_align()
            # value format: [(axis_index, factor_value)]
            align_list = [(get_align_axis(), ele_in_block), (-1, TRANSPOSE_FACTOR)]
            compute_align_list(self._in_transpose_tensors, align_list)
            compute_align_list(self._ub_split_tensors, [(-1, TRANSPOSE_FACTOR)])
            compute_align_list(self._out_transpose_tensors, [(1, TRANSPOSE_FACTOR)])
        if self.is_base_double:
            compute_align_list(self._cache_write_buffers, [(-1, ele_in_block)])
        if self._is_const and self.is_cut_m_transpose:
            factor_m = self._ub_factors_m[0]
            trans_factor = TRANSPOSE_FACTOR
            if self._is_b8_dtype():
                trans_factor *= B8_TRANSPOSE_FACTOR
            in_align_list = [(-1, TRANSPOSE_FACTOR), (1, factor_m // trans_factor)]
            compute_align_list(self._in_transpose_tensors, in_align_list)
            compute_align_list(self._ub_split_tensors, in_align_list)
            out_align_list = [(1, TRANSPOSE_FACTOR), (-2, factor_m // trans_factor)]
            compute_align_list(self._out_transpose_tensors, out_align_list)
            if self._is_b8_dtype():
                compute_align_list(self._in_transpose_tensors, [(-2, B8_TRANSPOSE_FACTOR)])
                compute_align_list(self._ub_split_tensors, [(-2, B8_TRANSPOSE_FACTOR)])
                compute_align_list(self._out_transpose_tensors, [(2, B8_TRANSPOSE_FACTOR)])
        if self.is_none_cut:
            align_factor = get_align_factor()
            in_align_list = [(0, align_factor), (-1, TRANSPOSE_FACTOR)]
            compute_align_list(self._in_transpose_tensors, in_align_list)
            compute_align_list(self._ub_split_tensors, in_align_list)
            out_align_list = [(0, TRANSPOSE_FACTOR), (1, ele_in_block)]
            compute_align_list(self._out_transpose_tensors, out_align_list)
            if self._is_b8_dtype():
                compute_align_list(self._in_transpose_tensors, [(-2, B8_TRANSPOSE_FACTOR)])
                compute_align_list(self._ub_split_tensors, [(-2, B8_TRANSPOSE_FACTOR)])
                out_align_list = [(1, B8_TRANSPOSE_FACTOR), (2, TRANSPOSE_FACTOR)]
                compute_align_list(self._out_transpose_tensors, out_align_list)

    def _calc_double_buffer(self):
        pass

    def _calc_axis_group(self):
        pass

    def _calc_constraints(self):
        input_shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        if self.is_none_cut:
            ele_in_block = self._ele_in_block()
            self._constraints.append(input_shape[0] <= ele_in_block * TRANSPOSE_FACTOR)
        elif self.is_cut_m_transpose:
            factor = self._ub_factors_m[0]
            if self._is_b8_dtype():
                self._constraints.append(tvm.expr.EQ(factor % TRANSPOSE_FACTOR, 0))
                self._constraints.append(tvm.expr.EQ(factor % (TRANSPOSE_FACTOR * B8_TRANSPOSE_FACTOR), 0))
            else:
                self._constraints.append(tvm.expr.EQ(factor % TRANSPOSE_FACTOR, 0))

    def _calc_emit_insn(self):
        for tensor_i in self._cache_read_buffers:
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], DMA_COPY]

        for tensor_i in self._cache_write_buffers:
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], COPY_UB_TO_UB]

        if self.is_base:
            attrs = {"no_overlap": 3, "no_overlap_malloc_buf_for_tail": 0}
            if len(self._out_tensors) == 1:
                attrs = {"no_overlap": 0}
            for tensor_i, axis in zip(self._out_tensors, self._emit_insn_axis):
                self._emit_insn_map[tensor_i] = [axis, DMA_COPY, attrs]
        else:
            attrs = {}
            is_no_overlap = self.is_all_align_mte or self.is_cut_m_transpose or \
                            (self.is_all_general and self._avg_split) or self.is_all_align_ub_copy
            if is_no_overlap:
                attrs = {"no_overlap": 0}
            if self.is_general_transpose:
                tuple_val = tvm.call_intrin('handle', 'tir.tvm_tuple', 2, 3)
                attrs = {"no_overlap": tuple_val, "no_overlap_malloc_buf_for_tail": 0, "map_policy": "1d"}
            for tensor_i in self._out_tensors:
                self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], DMA_COPY, attrs]
            for index, tensor_i in enumerate(self._out):
                self._emit_insn_map[tensor_i] = [self._emit_insn_axis[index], PHONY_INSN]

        for tensor_i in self._in_transpose_tensors:
            attrs = {"map_policy": "2d_dim2_size_no_overflow"}
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], VNCHWCONV, attrs]
        for tensor_i in self._ub_split_tensors:
            attrs = {"map_policy": "2d"}
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], COPY_UB_TO_UB, attrs]
        for tensor_i in self._out_transpose_tensors:
            attrs = {"map_policy": "2d_dim2_size_no_overflow"}
            self._emit_insn_map[tensor_i] = [tensor_i.op.axis[0], VNCHWCONV, attrs]

    def _do_tiling(self):
        funcs = {TilingStrategy.NONE_CUT: self._do_tiling_none_cut,
                 TilingStrategy.GENERAL_TRANSPOSE: self._do_tiling_general_split,
                 TilingStrategy.BASE_SINGLE: self._do_tiling_base_single_split,
                 TilingStrategy.BASE_DOUBLE: self._do_tiling_base_double_split,
                 TilingStrategy.CUT_M_TRANSPOSE: self._do_tiling_single_split,
                 TilingStrategy.ALL_ALIGN_MTE: self._do_tiling_double_split,
                 TilingStrategy.ALL_ALIGN_UB_COPY: self._do_tiling_single_split,
                 TilingStrategy.EMPTY: self._do_tiling_none_cut}
        funcs.get(self._tiling_case.tiling_strategy)()

    def _do_tiling_single_split(self, block_is_nparts=False):
        sch = self._schedule
        for index, _out in enumerate(self._out):
            m_out, m_inner = sch[_out].split(_out.op.axis[M_INDEX], factor=self._ub_factors_m[index])
            if block_is_nparts:
                b_out, b_inner = sch[_out].split(m_out, nparts=self._block_factor)
            else:
                b_out, b_inner = sch[_out].split(m_out, factor=self._block_factor)
            self._reorder_axis.append([b_out, b_inner, m_inner, _out.op.axis[N_INDEX]])
            self._compute_at_axis[index].compute_at_axis = b_inner
            self._emit_insn_axis[index].emit_insn_axis = m_inner

    def _do_tiling_base_single_split(self):
        self._do_tiling_single_split(True)

    def _do_tiling_double_split(self, block_is_nparts=False):
        sch = self._schedule
        for index, _out in enumerate(self._out):
            if (_out.op.attrs.get(EMPTY_TENSOR, False)):
                continue
            m_out, m_inner = sch[_out].split(_out.op.axis[0], factor=self._ub_factors_m[index])
            n_out, n_inner = sch[_out].split(_out.op.axis[1], factor=self._ub_factors_n[index])
            reorder_axis = [m_out, n_out, m_inner, n_inner]
            sch[_out].reorder(*reorder_axis)
            fused_axis = sch[_out].fuse(m_out, n_out)
            if block_is_nparts:
                b_out, b_inner = sch[_out].split(fused_axis, nparts=self._block_factor)
            else:
                b_out, b_inner = sch[_out].split(fused_axis, factor=self._block_factor)
            self._reorder_axis.append([b_out, b_inner, m_inner, n_inner])
            self._compute_at_axis[index].compute_at_axis = b_inner
            self._emit_insn_axis[index].emit_insn_axis = m_inner

    def _do_tiling_base_double_split(self):
        self._do_tiling_double_split(True)

    def _do_tiling_general_split(self):
        sch = self._schedule
        _low_ub_split_axis = 0
        _high_ub_split_axis = 1
        output_num_limit = DEFAULT_OUTPUT_NUM_LIMIT
        if DTYPE_BYTE_MAPPING.get(self._dtype) == TYPE_CAST_LIMIT:
            output_num_limit = B16_OUTPUT_NUM_LIMIT
        for index, _out in enumerate(self._out):
            m_out, m_inner = sch[_out].split(_out.op.axis[_low_ub_split_axis], factor=self._ub_factors_m[index])
            n_out, n_inner = sch[_out].split(_out.op.axis[_high_ub_split_axis], factor=self._ub_factors_n[index])
            if self._block_split_axis == _low_ub_split_axis:
                b_out, b_inner = sch[_out].split(m_out, factor=self._block_factor)
                if len(self._out_tensors) > output_num_limit:
                    sch[_out].reorder(b_out, b_inner, n_out, m_inner, n_inner)
                    fused_axis = sch[_out].fuse(b_inner, n_out)
                    self._reorder_axis.append([b_out, fused_axis, m_inner, n_inner])
                    self._compute_at_axis[index].compute_at_axis = fused_axis
                else:
                    self._reorder_axis.append([b_out, b_inner, n_out, m_inner, n_inner])
                    self._compute_at_axis[index].compute_at_axis = n_out
            else:
                b_out, b_inner = sch[_out].split(n_out, factor=self._block_factor)
                self._reorder_axis.append([m_out, b_out, b_inner, m_inner, n_inner])
                self._compute_at_axis[index].compute_at_axis = b_inner
            self._emit_insn_axis[index].emit_insn_axis = m_inner

    def _do_tiling_none_cut(self):
        for index, _out in enumerate(self._out):
            self._emit_insn_axis[index].emit_insn_axis = _out.op.axis[0]

    def _reshape(self, tensor, factors):
        sch = self._schedule
        axes = (tensor.op.axis[0], tensor.op.axis[0])
        for index, (action, value, axis_idx) in enumerate(factors):
            tail_strategy = "round_up"
            if index == 0:
                tail_strategy = "guard_with_if"
            split_axis = axes[axis_idx]
            if action == "nparts":
                axes = sch[tensor].split(split_axis, nparts=value, tail_strategy=tail_strategy)
            else:
                axes = sch[tensor].split(split_axis, factor=value, tail_strategy=tail_strategy)
        reshape_tensor = sch.cache_write(tensor, SCOPE_UB, "round_up")
        return reshape_tensor

    def _transpose(self, tensor, permute):
        sch = self._schedule
        reorder_axis = []
        for axis in permute:
            reorder_axis.append(tensor.op.axis[axis])
        sch[tensor].reorder(*reorder_axis)
        transpose_tensor = sch.cache_write(tensor, SCOPE_UB)
        return transpose_tensor

    def _do_reshape(self):
        for tensor_i, factors in self._in_reshape_map.items():
            reshape_tensor = self._reshape(tensor_i, factors)
            self._in_reshape_tensors.add(reshape_tensor)

        for tensor_i, factors in self._out_reshape_map.items():
            reshape_tensor = self._reshape(tensor_i, factors)
            self._out_transpose_tensors.append(reshape_tensor)

    def _do_transpose(self):
        # eg input: (m, n) dtype: float16
        # step 0: (m, n) --> (m_out, 256, n) // split
        # step 1: (m_out, 256, n) --> (m_out, 16, 16, n) // reshape
        # step 2: (m_out, 16, 16, n) --> (m_out, 16, n, 16) // transpose, permute:[0, 2, 3, 1]
        if self.is_none_cut:
            if self._is_b8_dtype():
                permute = [2, 3, 1, 0]
            else:
                permute = [1, 2, 0]
        else:
            if self._is_b8_dtype():
                permute = [0, 3, 4, 2, 1]
            else:
                permute = [0, 2, 3, 1]
        for tensor_i in self._in_reshape_tensors:
            transpose_tensor = self._transpose(tensor_i, permute)
            self._in_transpose_tensors.add(transpose_tensor)

        for tensor_i in self._out_transpose_tensors:
            transpose_tensor = self._transpose(tensor_i, permute)
            self._ub_split_tensors.append(transpose_tensor)

    def _reinterpret_cast(self):
        if DTYPE_BYTE_MAPPING.get(self._dtype) <= TYPE_CAST_LIMIT:
            return
        sch = self._schedule
        if self.is_none_cut:
            cast_index = [0, 1, 2]
        else:
            cast_index = [1, 2, 3]
        for tensor_i in self._in_transpose_tensors:
            sch[tensor_i].reinterpret_cast('float16', sch[tensor_i].op.axis[cast_index[0]])
        for tensor_i in self._out_transpose_tensors:
            sch[tensor_i].reinterpret_cast('float16', sch[tensor_i].op.axis[cast_index[2]])

    def _do_storage_align(self):
        sch = self._schedule
        for tensor_i, params in self._storage_align_map.items():
            sch[tensor_i].storage_align(*params)

    def _do_compute_align(self):
        sch = self._schedule
        for tensor_i, params in self._compute_align_map.items():
            for align_value in params:
                sch[tensor_i].compute_align(*align_value)

    def _do_reorder(self):
        if self.is_none_cut:
            return

        sch = self._schedule
        for _out, reorder_axes in zip(self._out, self._reorder_axis):
            if (_out.op.attrs.get(EMPTY_TENSOR, False)):
                continue
            sch[_out].reorder(*reorder_axes)

    def _calc_tensor_space(self):
        ub_size = util.get_ub_size()
        if self.is_base and len(self._out) != 1:
            ub_size -= (2 * self.block_size)
        tensor_space = ub_size // self._coexisting_quantity
        if self._tiling_case.enable_db:
            tensor_space = ub_size // 2 // self._coexisting_quantity
        tensor_space = tensor_space // self.block_size * self.block_size
        return tensor_space

    def _do_storage_bound(self):
        sch = self._schedule
        tensors = self._middle_tensors.union(
            self._cache_read_buffers).union(
            self._cache_write_buffers).union(
            self._in_transpose_tensors).union(
            self._ub_split_tensors).union(
            self._out_transpose_tensors)

        tensor_space = self._calc_tensor_space()

        for tensor_i in tensors:
            storage_bound = int(tensor_space // DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            sch[tensor_i].set_buffer_size(storage_bound)

    def _do_compute_with(self):
        if self._use_compute_with():
            sch = self._schedule
            sch.compute_with(self._out, 1)

    def _do_multi_core(self):
        if self.is_none_cut:
            return
        sch = self._schedule
        block = tvm.thread_axis("blockIdx.x")
        block_axis_end = self._block_split_axis + 1
        for _out, reorder_axes in zip(self._out, self._reorder_axis):
            block_axis = reorder_axes[:block_axis_end]
            block_bind_axis = sch[_out].fuse(*block_axis)
            sch[_out].bind(block_bind_axis, block)
            if self._use_compute_with():
                break

    def _do_compute_at(self):
        sch = self._schedule
        for tensor_i, param in self._compute_at_map.items():
            if (param[0].op.attrs.get(EMPTY_TENSOR, False)):
                continue
            sch[tensor_i].compute_at(sch[param[0]], param[1].compute_at_axis)

    def _do_compute_inline(self):
        sch = self._schedule
        for tensor_i in self._compute_inline_tensors:
            sch[tensor_i].compute_inline()

    def _do_double_buffer(self):
        if self._tiling_case.enable_db:
            sch = self._schedule

            tensors = self._middle_tensors.union(
                self._cache_read_buffers).union(
                self._cache_write_buffers)

            for tensor_i in tensors:
                sch[tensor_i].double_buffer()

    def _do_constraints(self):
        sch = self._schedule
        for cond in self._constraints:
            if isinstance(cond, tvm.tir.PrimExpr):
                sch.set_constraint(cond)

    def _do_emit_insn(self):
        sch = self._schedule
        for tensor_i, param in self._emit_insn_map.items():
            if (tensor_i.op.attrs.get(EMPTY_TENSOR, False)):
                continue
            emit_insn_axis = param[0]
            if isinstance(emit_insn_axis, EmitInsn):
                emit_insn_axis = emit_insn_axis.emit_insn_axis
                if self.is_base and len(self._out) != 1:
                    sch[tensor_i].pragma(emit_insn_axis, "local.UB_fragments_memory_size", self.block_size)
            if len(param) > 2:
                sch[tensor_i].emit_insn(emit_insn_axis, param[1], param[2])
            else:
                sch[tensor_i].emit_insn(emit_insn_axis, param[1])

    def _do_axis_group(self):
        def do_pragma(tensors, axes_index, value):
            for tensor_i in tensors:
                for idx in axes_index:
                    sch[tensor_i].pragma(tensor_i.op.axis[idx], "axis_group", value)

        sch = self._schedule
        group_id = tvm.call_extern("int32", "axis_group", 0, "overwrite")
        if self.is_transpose_strategy or self.is_cut_m_transpose or self.is_none_cut:
            group_axis = -2
            if self._is_b8_dtype():
                group_axis -= 1
            do_pragma(self._in_transpose_tensors, [group_axis, group_axis - 1], group_id)
            do_pragma(self._out_transpose_tensors, [-1, -2], group_id)

        is_continuous_out = (self.is_all_general and self._avg_split) or \
                            self.is_cut_m_transpose or self.is_none_cut or self.is_all_align_ub_copy
        if is_continuous_out:
            do_pragma(self._out_tensors, [-1, -2], group_id)

        group_id = tvm.call_extern("int32", "axis_group", 0, "append")
        if self.is_general_transpose and not self._avg_split:
            for tensor_i, axes in zip(self._out, self._reorder_axis):
                sch[tensor_i].pragma(axes[-1], "axis_group", group_id)
                sch[tensor_i].pragma(axes[-2], "axis_group", group_id)

        if self.is_general_transpose and not self._avg_split:
            do_pragma(self._out_tensors, [-1, -2], group_id)

    def _add_compile_info(self):
        operation.get_context().get_current_compute().get_current_schedule().add("_split_num", len(self._out_tensors))
        if CompileInfo.CORE_NUM in get_compile_info() and not self._is_const:
            return

        operation.add_compile_info_inner(CompileInfo.CORE_NUM, util.get_core_num())
        operation.add_compile_info_inner(CompileInfo.UB_SIZE, util.get_ub_size())
        operation.add_compile_info_inner(CompileInfo.UB_BLOCK_SIZE, util.get_ub_block_size())
        operation.add_compile_info_inner("_avg_split", self._avg_split)
        operation.add_compile_info_inner("_split_is_const",
                                         operation.get_context().get_current_compute().set_default("_split_is_const",
                                                                                                   False))
        operation.add_compile_info_inner("_only_const_tiling", False)
        if self._is_const:
            operation.add_compile_info_inner("_is_const", True)
            operation.add_compile_info_inner("_const_dims", self._block_dims)
        else:
            operation.add_compile_info_inner("_is_const", False)
        shape_is_var = []
        shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        for value in shape:
            if isinstance(value, int):
                shape_is_var.append(False)
            else:
                shape_is_var.append(True)
        operation.add_compile_info_inner("_split_vars", shape_is_var)

    def _check_tiling_case(self):
        if self._is_const:
            return True
        tensor_space = self._calc_tensor_space()
        storage_bound = int(tensor_space // DTYPE_BYTE_MAPPING.get(self._dtype))
        input_shape = util.shape_to_list(list(self._input_tensors)[0].shape)
        if self.is_none_cut:
            m_lower, _ = util.get_bound(input_shape[0])
            n_lower, _ = util.get_bound(input_shape[1])
            if m_lower == 0:
                m_lower = 1
            if self._avg_split and n_lower is not None and n_lower == 1:
                n_lower *= len(self._out_tensors)
            ele_in_block = self._ele_in_block()
            if m_lower is None or n_lower is None:
                return False
            align_factor = ele_in_block * TRANSPOSE_FACTOR
            m_lower = util.ceil_div(m_lower, align_factor) * align_factor
            if m_lower // align_factor * n_lower >= REPEAT_LIMIT:
                return False
            if m_lower * n_lower > storage_bound:
                return False
        if self.is_cut_m_transpose:
            n_lower, _ = util.get_bound(input_shape[1])
            if n_lower is None:
                return False
            m_lower, _ = util.get_bound(self._ub_factors_m[0])
            if m_lower is None:
                return False
            if m_lower * n_lower > storage_bound:
                return False
        return True
