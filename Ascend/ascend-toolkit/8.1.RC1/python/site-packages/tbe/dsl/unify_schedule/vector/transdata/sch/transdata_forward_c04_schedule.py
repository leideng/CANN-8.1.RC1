#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright(C) 2022. Huawei Technologies Co., Ltd. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
transdata forward schedule
"""
from copy import copy
from tbe.dsl.base.operation import var_inner
from tbe.dsl.unify_schedule.constants import TransdataCategory
from tbe.dsl.classifier.transdata.constants import NO_OVERLAP, INT64
from ..common.transdata_base_sch import TransdataBaseSch


class TransC04ForwardSchedule(TransdataBaseSch):
    """
    TransC04ForwardSchedule
    """

    def __init__(self, outs, tiling_case):
        TransdataBaseSch.__init__(self, outs, tiling_case)
        self.pad_n_tensor = None
        self.split_n_tensor = None
        self.transpose_0_tensor = None
        self.pad_c_tensor = None
        self.transpose_1_tensor = None
        self.fuse_hc4_tensor = None
        self.pad_hc4_tensor = None
        self.split_hc4_tensor = None
        self.transpose_2_tensor = None
        self.transpose_3_tensor = None
        self.ub2ub_0_tensor = None
        self.split_once = False
        self.is_split_full_hw = tiling_case.is_split_full_hw

    @classmethod
    def get_supported_sub_pattern(cls):
        return TransdataCategory.GENERAL_C04_FORWARD

    def do_schedule(self):
        """
        Process of schedule
        """
        self._create_schedule()
        self._do_cache_read()
        self._do_set_scope()
        self._do_cache_write()
        self._init_tensors()

        self._calc_tiling()
        self._do_tiling()

        self._do_set_constraint()
        self._do_storage_align()
        self._do_storage_bound()
        self._do_set_predicate()
        self._do_mem_reused()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_compute_at()
        self._do_compute_at()

        self._calc_emit_insn()
        self._do_emit_insn()
        self._do_pragma()

        self.schedule.tiling_key = self.tiling_case.tiling_key
        return self.schedule

    def _init_tensors(self):
        self.mte2_tensor = self.child(list(self.graph_info.input_tensor_set)[0])
        # N,C,H -> Nx,C,H
        self.pad_n_tensor = self.child(self.mte2_tensor)
        # Nx,C,H -> No,16,C,H
        self.split_n_tensor = self.child(self.pad_n_tensor)
        # No,16,C,H -> No,C,H,16
        self.transpose_0_tensor = self.child(self.split_n_tensor)
        # No,C,H,16 -> No,C4,H,16
        self.pad_c_tensor = self.child(self.transpose_0_tensor)
        # No,C4,H,16 -> No,H,C4,16
        self.transpose_1_tensor = self.child(self.pad_c_tensor)
        # No,H,C4,16 -> No,HC4,16
        self.fuse_hc4_tensor = self.child(self.transpose_1_tensor)
        # No,HC4,16 -> No,T,16
        self.pad_hc4_tensor = self.child(self.fuse_hc4_tensor)
        # No,HC4,16 -> No,T1,T0,16
        self.split_hc4_tensor = self.child(self.pad_hc4_tensor)
        # No,T1,T0,16 -> T1,No,T0,16
        self.transpose_2_tensor = self.child(self.split_hc4_tensor)
        # T1,No,T0,16 -> T1,No,16,T0
        self.transpose_3_tensor = self.child(self.transpose_2_tensor)
        # Tiling
        self.tiling_tensor = list(self.graph_info.output_tensor_set)[0]

        # [UB]Tensors -> Transpose 
        self.transpose_tensors = list(self.graph_info.transpose_tensor_set)
        self.transpose_tensors[self.transpose_tensors.index(self.tiling_tensor)] = self.parent(self.tiling_tensor)

        # [UB]Tensors -> Reshape
        self.reshape_tensors = list(self.graph_info.s_reshape_tensor_set) + \
                               list(self.graph_info.f_reshape_tensor_set)
        # [UB]Tensors -> Pad
        self.pad_tensors = list(self.graph_info.pad_tensor_set)

    def _calc_tiling(self):
        self.tiling_axes = [x for x in self.tiling_tensor.op.axis]
        case = self.tiling_case
        if not case.ub_first_factor:
            case.ub_first_factor = var_inner("_ub_first_factor", (1, None), dtype=INT64)
        if not self.split_once and not case.ub_second_factor:
            case.ub_second_factor = var_inner("_ub_second_factor", (1, None), dtype=INT64)
        if not case.block_factor:
            case.block_factor = var_inner("_block_factor", (1, None), dtype=INT64)

    def _do_tiling(self):
        self._do_ub_tiling()
        self._do_block_tiling()
        self._do_reorder()
        self._do_fragments()

    def _do_reorder(self):
        """
        Regulation-0:
        [A.o, B.o, A.i, B.i, C, D]
        [A.o, B.o]: ub_outer
        [A.i, B.i, C, D]: ub_inner

        Regulation-1:
        Output is [C1, N1, N0, C0], we wanner output is [N1, C1, N0, C0], which make performance better.
        After ub split, output is [c1.o, n1.o, c1.i, n1.i, n0, c0], reorder would change order is
        [n1.o, c1.o, c1.i, n1.i, n0, c0].

        Regulation-2:
        let n1.o and c1.o be block-split
        """
        self.axis_in_ub = copy(self.tiling_axes)
        self.axis_in_ub[0] = self.iter_ub_first_inner
        self.axis_in_ub[1] = self.iter_ub_second_inner

        if self.tiling_case.block_split_idx == 0:
            # split c1.o
            self.axis_not_in_ub = [self.iter_ub_second_outer, self.iter_block_outer, self.iter_block_inner]
        else:
            # split n.o
            self.axis_not_in_ub = [self.iter_block_outer, self.iter_block_inner, self.iter_ub_first_outer]

        self.reorder_list = self.axis_not_in_ub + self.axis_in_ub
        self.ub_outer = self.axis_not_in_ub[-1]
        self.ub_inner = self.axis_in_ub[0]
        self.reorder_list = self.axis_not_in_ub + self.axis_in_ub
        self.schedule[self.tiling_tensor].reorder(*self.reorder_list)

    def _do_set_constraint(self):
        case = self.tiling_case
        blk_idx = case.block_split_idx
        ub_first_idx = case.ub_split_first_idx
        ub_second_idx = case.ub_split_second_idx
        tiling_shape = [x.dom.extent for x in self.tiling_axes]

        constraint = []
        constraint.append(case.block_factor <= tiling_shape[blk_idx])
        constraint.append(case.ub_first_factor <= tiling_shape[ub_first_idx])
        if not self.split_once:
            constraint.append(case.ub_second_factor <= tiling_shape[ub_second_idx])

        for item in constraint:
            if isinstance(item, bool):
                continue
            self.schedule.set_constraint(item)

    def _do_storage_bound(self):
        for stage in self.forward_stage_graph_map:
            self.schedule[stage].set_buffer_size(self.tiling_case.buffer_size)

    def _do_set_predicate(self):
        for tensor in self.pad_tensors:
            self.schedule[tensor].set_store_predicate(tensor.op.body[0].condition)

    def _do_storage_align(self):
        if self.is_split_full_hw:
            self.schedule[self.mte2_tensor].storage_align(self.mte2_tensor.op.axis[-3], 16, 0)
            # pad_n do on mte2
            self.schedule[self.pad_n_tensor].storage_align(self.pad_n_tensor.op.axis[-3], 16, 0)
            self.schedule[self.split_n_tensor].storage_align(self.split_n_tensor.op.axis[-3], 16, 0)

            self.schedule[self.transpose_0_tensor].storage_align(self.transpose_0_tensor.op.axis[-4], 16 * 16, 0)
            self.ub2ub_0_tensor = self.single_cache_read(self.transpose_0_tensor)
            self.schedule[self.ub2ub_0_tensor].compute_align(self.ub2ub_0_tensor.op.axis[-3], 4)
            # pad_c do on ub2ub0
            # pad_hc4 do on transpose1
            self.schedule[self.transpose_1_tensor].storage_align(self.transpose_1_tensor.op.axis[-4], 16 * 16, 0)
            self.schedule[self.fuse_hc4_tensor].storage_align(self.fuse_hc4_tensor.op.axis[-3], 16 * 16, 0)
        else:
            self.schedule[self.mte2_tensor].storage_align(self.mte2_tensor.op.axis[-2], 16, 0)
            self.schedule[self.pad_n_tensor].compute_align(self.pad_n_tensor.op.axis[-1], 16)
            self.schedule[self.split_n_tensor].compute_align(self.split_n_tensor.op.axis[-1], 16)
            self.schedule[self.transpose_0_tensor].compute_align(self.transpose_0_tensor.op.axis[-2], 16)
            self.ub2ub_0_tensor = self.single_cache_read(self.transpose_0_tensor)
            self.schedule[self.ub2ub_0_tensor].compute_align(self.ub2ub_0_tensor.op.axis[-3], 4)
            self.schedule[self.transpose_1_tensor].storage_align(self.transpose_1_tensor.op.axis[-4], 16 * 16, 0)
            self.schedule[self.fuse_hc4_tensor].storage_align(self.fuse_hc4_tensor.op.axis[-3], 16 * 16, 0)

    def _do_mem_reused(self):
        self.schedule[self.mte2_tensor].reused_by(self.pad_n_tensor)
        self.schedule[self.mte2_tensor].reused_by(self.split_n_tensor)
        self.schedule[self.ub2ub_0_tensor].reused_by(self.pad_c_tensor)
        self.schedule[self.transpose_1_tensor].reused_by(self.fuse_hc4_tensor)
        self.schedule[self.transpose_1_tensor].reused_by(self.pad_hc4_tensor)
        self.schedule[self.transpose_1_tensor].reused_by(self.split_hc4_tensor)

    def _transpose_emit_insn(self, tensor, perm):
        if perm and perm[-1] != len(perm) - 1:
            self.vnchwconv_insn_map(tensor, "vector_transpose", tensor.op.axis[0], perm)
        else:
            self.emit_insn_map[tensor] = {"scope": tensor.op.axis[0], "instruction": "dma_copy"}

    def _calc_emit_insn(self):
        self.emit_insn_map[self.mte2_tensor] = {"scope": self.mte2_tensor.op.axis[0], "instruction": "dma_copy",
                                                "attrs": {"gm_to_ub_gap_opt": 1}}

        self.emit_insn_map[self.ub2ub_0_tensor] = {"scope": self.ub2ub_0_tensor.op.axis[0], "instruction": "dma_copy"}

        for tensor in self.pad_tensors:
            self.emit_insn_map.update({tensor: {"scope": tensor.op.axis[0], "instruction": "vector_dup"}})

        for tensor in self.reshape_tensors:
            self.emit_insn_map.update({tensor: {"scope": tensor.op.axis[0], "instruction": "phony_insn"}})

        for tensor in self.transpose_tensors:
            self._transpose_emit_insn(tensor, [int(x) for x in tensor.op.attrs["permute"]])

        self.emit_insn_map[self.tiling_tensor] = {"scope": self.ub_inner, "instruction": "dma_copy",
                                                  "attrs": {NO_OVERLAP: 2, "no_overlap_malloc_buf_for_tail": 0}}

    def _do_pragma(self):
        axes = [self.transpose_0_tensor.op.axis[i] for i in [1, 2]]
        self._do_group(self.transpose_0_tensor, axes, number=0)

        if self.is_split_full_hw:
            axes = [self.mte2_tensor.op.axis[i] for i in [1, 2]]
            self._do_group(self.mte2_tensor, axes, number=0)

        axes = [self.pad_n_tensor.op.axis[i] for i in [1, 2]]
        self._do_group(self.pad_n_tensor, axes, number=0)

        axes = [self.pad_c_tensor.op.axis[i] for i in [2, 3]]
        self._do_group(self.pad_c_tensor, axes, number=0)
