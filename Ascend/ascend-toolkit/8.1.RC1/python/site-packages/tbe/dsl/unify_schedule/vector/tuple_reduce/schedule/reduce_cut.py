#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Reduce Cut Schedule
"""
# Standard Packages
from typing import List
from functools import reduce
# Local Packages
from tbe import tvm
from tbe.tvm import Tensor
from tbe.dsl.base.operation import var_inner
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.constants import DTYPE_BYTE_MAPPING
from tbe.dsl.unify_schedule.constants import TupleReducePattern
# Tuple-Reduce Packages
from ..common.constants import Options
from ..common.schedule_helper import Schedule


class ReduceCut:
    """
    Block cut on reduce axis
    """

    def __init__(self, outs, tiling_case):
        self.outs: List[Tensor] = outs
        self.tiling_case = tiling_case
        self.info = self.tiling_case.info
        self.tiling_key = self.tiling_case.tiling_key

        # SCHEDULE INFORMATION
        self.intermediate_output = []
        sch = Schedule(self.outs)
        tensors = sch.tensors
        for t in self.outs:
            input_tensors = []
            for ten in tensors:
                input_tensors = input_tensors + [tensor for tensor in ten.op.input_tensors]
            if t in input_tensors:
                self.intermediate_output.append(t)
        self.real_outs = [t for t in self.outs if t not in self.intermediate_output]
        self.sch = Schedule(self.real_outs)
        self.scope = "local.UB"

        self.reduce_stage = None
        self.reduce_rf_stage = None
        self.block_split_axis = 0
        self.block_tiling_axis = 0
        self.block_outer = 0
        self.block_inner = 0
        self.ub_split_axis = 0
        self.ub_tiling_axis = 0
        self.ub_outer = 0
        self.ub_inner = 0

        # align pad
        self.align_pad_stages = set()
        self.remove_pad_stages = set()

        # set switches
        self.info.buffer_size.switches.transpose_reduce = self.tiling_case.options == Options.TransposeReduce
        self.info.buffer_size.switches.align_pad = self.tiling_case.options == Options.UBAlignPad
        # buffer size options
        self.info.buffer_size.estimate(self.info.max_dtype_size)

        # reduce emit insn
        self.dichotomy_reduce_attr = {"reduce_opt_mode": "dichotomy_reduce",
                                      "storage_bound": self.info.buffer_size.grande_buffer_size // 2,
                                      "reuse_src_tensor": True,
                                      "nlast_reduce_dichotomy": 16}
        self.transpose_reduce_attr = {"trans": True}

        # intermediate_output
        self.intermediate_output_stage = set()

    def do_schedule(self):
        self._cache_read()
        self._intermediate_output()
        self._align_pad()
        self._reorder_reduce_stage()
        self._tiling_block()
        self._reorder_again()
        self._set_scope()
        self._compute_at()
        self._bind_block()
        self._emit_insn()
        self._storage_align()
        self._buffer_size()
        self._compute_inline()
        self._compute_root()
        self._mem_unique()

        return self.sch.sch

    def _cache_read(self):
        sch = self.sch
        for ph in sch.placeholder:
            consumers = sch.consumer(sch[ph])
            readers = [stage.origin_op for stage in consumers]
            sch.cache_read(ph, self.scope, readers)
    
    def _intermediate_output(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        for tensor in self.intermediate_output:
            # cache write
            cache_write_ub_tensor = sch.sch.cache_write(tensor, "")
            cache_write_ub_stage = sch.get_stage(cache_write_ub_tensor)
            # cache read
            consumers = sch.consumer(sch[tensor])
            readers = [stage.origin_op for stage in consumers]
            cache_read_ub_tensor = sch.sch.cache_read(tensor, "local.UB", readers)
            cache_read_ub_stage = sch.get_stage(cache_read_ub_tensor)
            # mem reuse
            cache_write_ub_stage.reused_by(cache_read_ub_tensor)
            cache_read_ub_stage.reused_by(reuse_data=True)
            cache_read_ub_stage.emit_insn(cache_read_ub_stage.op.axis[0], "phony_insn")
            # add to intermediate_output_stage
            self.intermediate_output_stage.add(sch.get_stage(tensor))

    def _align_pad(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.align_pad:
            cache_read_stages_zero = set(sch.cache_read_stages)
            for stage in sch.broadcast_branch_roots:
                cache_read_stages_zero -= sch.poset(stage)

            for stage in cache_read_stages_zero:
                consumers = sch.consumer(stage)
                readers = [stage.origin_op for stage in consumers]
                t = sch.cache_read(sch.get_tensor(stage), self.scope, readers)
                self.align_pad_stages.add(sch[t])

    def _reorder_reduce_stage(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        # Get reorder stage
        reduce_tensor = info.graph.reduce_tensor[0]
        reduce_stage = sch[reduce_tensor]

        # original order
        reduce_axis_one_hot = [1 if i in info.reduce_axis else 0 for i, _ in enumerate(info.max_shape)]
        origin_order = []
        for i, _ in enumerate(info.max_shape):
            if reduce_axis_one_hot[i]:
                origin_order.append(reduce_stage.op.reduce_axis[info.reduce_axis.index(i)])
            else:
                origin_order.append(reduce_stage.op.axis[i])

        # tiling order [A,...,A,R,...,R,*]
        tiling_order = []
        for i, _ in enumerate(info.max_shape[:-1]):
            if origin_order[i] in reduce_stage.op.axis:
                tiling_order.append(origin_order[i])
        for i, _ in enumerate(info.max_shape[:-1]):
            if origin_order[i] in reduce_stage.op.reduce_axis:
                tiling_order.append(origin_order[i])
        tiling_order.append(origin_order[-1])
        reduce_stage.reorder(*tiling_order)

    def _tiling_block(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        # Get tiling stage
        reduce_tensor = info.graph.reduce_tensor[0]
        reduce_stage = sch[reduce_tensor]

        # Get tiling axes index
        block_split_axis = case.block_axis
        ub_split_axis = case.ub_axis
        # Get tiling factors
        if case.info.is_const:
            block_factor = info.block_factor
            ub_factor = info.ub_factor
        else:
            block_factor = var_inner("_blocka_factor", (1, None), dtype="int64")
            block_factor = var_inner("_blockr_factor", (1, None), dtype="int64")
            ub_factor = var_inner("_ub_factor", (1, None), dtype="int64")

        # Get block tiling axis
        axis_idx = info.reduce_axis.index(block_split_axis)
        block_tiling_axis = reduce_stage.op.reduce_axis[axis_idx]
        if ub_split_axis in info.reduce_axis:
            axis_idx = info.reduce_axis.index(ub_split_axis)
            ub_tiling_axis = reduce_stage.op.reduce_axis[axis_idx]
        else:
            ub_tiling_axis = reduce_stage.op.axis[ub_split_axis]

        # block tiling
        block_outer, block_inner = reduce_stage.split(block_tiling_axis, factor=block_factor)
        # fuse all R before block tiling axis
        to_fuse_block_outer = []
        for axis in reduce_stage.op.reduce_axis:
            if axis == block_tiling_axis:
                break
            to_fuse_block_outer.append(axis)
        to_fuse_block_outer.append(block_outer)
        fused_block_outer = reduce_stage.fuse(*to_fuse_block_outer)

        # rfactor
        reduce_rf = sch.rfactor(reduce_tensor, fused_block_outer, 0)[0]
        reduce_rf_stage = sch[reduce_rf]
        reduce_stage = sch.get_stage(reduce_tensor)

        # ub_split on reduce_rf stage
        # find ub_tiling_axis in reduce_rf_stage
        if ub_split_axis == block_split_axis:
            ub_tiling_axis = block_inner
        for thisaxis in reduce_rf_stage.leaf_iter_vars:
            if thisaxis.var.name == ub_tiling_axis.var.name:
                ub_tiling_axis = thisaxis
                break
        # ub tiling
        ub_outer, ub_inner = reduce_rf_stage.split(ub_tiling_axis, factor=ub_factor)

        # save
        self.reduce_rf_stage, self.reduce_stage = reduce_rf_stage, reduce_stage
        self.block_split_axis, self.block_tiling_axis = block_split_axis, block_tiling_axis
        self.block_outer, self.block_inner = fused_block_outer, block_inner
        self.ub_split_axis, self.ub_tiling_axis = ub_split_axis, ub_tiling_axis
        self.ub_outer, self.ub_inner = ub_outer, ub_inner

    def _reorder_again(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        # Get reorder stages
        reduce_stage = self.reduce_stage
        reduce_rf_stage = self.reduce_rf_stage

        # Pivot block outer in reduce stage
        # by the definition of rfactor
        # there will be only one reduce axis in this stage
        gm_reduce_order = list(reduce_stage.op.reduce_axis) + list(reduce_stage.op.axis)
        reduce_stage.reorder(*gm_reduce_order)

        # Pivot rfactor in a proper order
        data_par_iter = sch.data_parallel_iteration(reduce_rf_stage)
        common_reduce_iter = sch.comm_reduce(reduce_rf_stage)
        common_reduce_iter = sorted(common_reduce_iter, key=lambda thisaxis: thisaxis.var.name.split('.')[0])

        if info.last_reduce:  # last reduce
            tiling_order = data_par_iter + common_reduce_iter
        else:  # nlast reduce
            tiling_order = data_par_iter[:-1] + common_reduce_iter + data_par_iter[-1:]
        reduce_rf_stage.reorder(*tiling_order)

    def _set_scope(self):
        self.sch.stages_not_on_ub.add(self.reduce_stage)
        for stage in self.sch.stages_on_ub - self.intermediate_output_stage:
            stage.set_scope(self.scope)

    def _compute_at(self):
        sch, reduce_stage, reduce_rf_stage = self.sch, self.reduce_stage, self.reduce_rf_stage
        stages_before_reduce_rf = sch.stages_on_ub.intersection(sch.poset(self.reduce_rf_stage))
        for stage in stages_before_reduce_rf:
            stage.compute_at(self.reduce_rf_stage, self.ub_outer)
        reduce_rf_stage.compute_at(reduce_stage, reduce_stage.op.reduce_axis[0])

    def _bind_block(self):
        reduce_stage, sch = self.reduce_stage, self.sch
        block = tvm.thread_axis("blockIdx.x")
        reduce_stage.bind(reduce_stage.op.reduce_axis[0], block)

    def _emit_insn(self):
        sch, info = self.sch, self.info

        # emit insn atomic add stage
        self.reduce_stage.emit_insn(self.reduce_stage.op.axis[0], "dma_copy")

        # get reduce mode
        if info.switches.transpose_reduce:
            reduce_attr = self.transpose_reduce_attr
        elif not info.last_reduce:
            reduce_attr = self.dichotomy_reduce_attr
        else:
            reduce_attr = {}

        # get emit insn axis
        emit_axis = self.ub_inner
        if not info.last_reduce and self.ub_split_axis not in info.reduce_axis:
            emit_axis = sch.reduce_emit_axis(self.reduce_rf_stage, self.ub_inner)
        # emit insn reduce node
        self.reduce_rf_stage.emit_insn(emit_axis, "vector_reduce_sum", attrs=reduce_attr)

        # emit insn align pad stages
        for stage in self.align_pad_stages:
            stage.emit_insn(stage.op.axis[0], "align_pad", {"avoid_bank_conflict": True})

        # emit insn others
        vector_with_broadcast_stages = []
        for stage in sch.broadcast_branch_roots:
            vector_with_broadcast_stages += sch.consumer(stage)

        for stage in self.sch.stages_on_ub - {self.reduce_rf_stage} - \
            self.align_pad_stages - self.intermediate_output_stage:
            if stage in sch.cache_read_stages:
                stage.emit_insn(stage.op.axis[0], "dma_copy")
            elif stage in vector_with_broadcast_stages:
                stage.emit_insn(stage.op.axis[0], info.get_insn(stage), attrs={"use_ba_pattern_brc": 1})
            else:
                stage.emit_insn(stage.op.axis[0], info.get_insn(stage))

    def _storage_align(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.transpose_reduce:
            return
        stages_before_reduce_rf = sch.stages_on_ub.intersection(sch.poset(self.reduce_rf_stage))
        storage_align_stages = stages_before_reduce_rf
        for stage in self.align_pad_stages:
            storage_align_stages = storage_align_stages - sch.poset(stage)
        # stages in broadcast_branch do not need storage_align
        for stage in sch.broadcast_branch_roots:
            storage_align_stages -= sch.poset(stage)

        for stage in storage_align_stages:
            dtype_size = DTYPE_BYTE_MAPPING.get(sch.get_tensor(stage).dtype)
            if stage.op.attrs.get("_type") == "broadcast.tensor":
                # fix milan last broadcast emit insn not support storage align issue
                stage.compute_align(stage.op.axis[-1], info.soc.block_size // dtype_size)
            elif len(stage.op.axis) > 1:
                stage.storage_align(stage.op.axis[-2], info.soc.block_size // dtype_size, 0)

    def _buffer_size(self):
        def product(lst): return reduce(lambda x, y: x * y, lst)
        case, sch, info = self.tiling_case, self.sch, self.info
        for stage in sch.stages_on_ub:
            if stage == self.reduce_rf_stage:
                if info.is_const:
                    reduced_shape = [1 if info.reduce_axis_one_hot[i] else v for i, v in enumerate(info.max_shape)]
                    stage.set_buffer_size(product(reduced_shape))
                else:
                    stage.set_buffer_size(info.buffer_size.short_buffer_size)
            elif sch.cache_read_stages.get(stage) in info.buffer_size.short_tensors or \
                    sch.get_ori_tensor(stage) in info.buffer_size.short_tensors:
                stage.set_buffer_size(info.buffer_size.short_buffer_size)
            else:
                stage.set_buffer_size(info.buffer_size.grande_buffer_size)

    def _compute_inline(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        for stage in sch.stages_on_ub.intersection(sch.broadcast_stages):
            if set(sch.consumer(stage)).intersection(sch.reduce_stages):
                continue
            if not sch.is_last_broadcast(stage) or sch.is_scalar_broadcast(stage):
                stage.compute_inline()

    def _compute_root(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.compute_root:
            for stage in sch.broadcast_branch.intersection(sch.stages_on_ub):
                stage.compute_root()

    def _mem_unique(self):
        case, sch, info = self.tiling_case, self.sch, self.info
        if info.switches.mem_unique:
            for stage, tensor in sch.cache_read_stages.items():
                if tensor in info.buffer_size.unique_tensors:
                    stage.mem_unique()
