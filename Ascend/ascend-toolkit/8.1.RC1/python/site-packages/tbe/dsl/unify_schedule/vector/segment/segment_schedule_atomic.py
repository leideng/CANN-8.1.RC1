#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
scatter schedule atomic
"""
from abc import ABC
from typing import Any
from functools import reduce
from operator import mul

from tbe import tvm
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import add_build_arg
from te.utils import shape_util

from ... import util
from ...constants import CompileInfo
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import SegmentPattern
from ...constants import Pattern
from ...schedule import Schedule
from .segment_tilingcase import TilingStrategy
from .segment_tilingcase import SegmentCompileInfo

DEFAULT = "default"

# block size in D architecture
BLOCK_SIZE_BYTE = 32

# STORE AREA
VAR_STORE_GM = 0
VAR_STORE_UB = 1
# Represents the axis that does not need to be split
DUMMY_DIM = -10
BASE_KET = 770000000


# 'pylint: disable=R0902, R0903
class SegmentAtomicSchedule(ABC):
    """
    segment schedule
    """

    def __init__(self, outs, tiling_case):
        self._out_tensor = outs[0]
        self._schedule = None
        self._tiling_case = tiling_case
        self._tiling_strategy = self._tiling_case.get("tiling_strategy")
        self._tiling_key = self._tiling_case.get("key")
        self._pattern = (self._tiling_key % 10000) // 100
        self._segment_axis = self._tiling_case.get("segment_axis", 0)
        self._is_need_storage_align = self._tiling_case.get("is_need_storage_align", False)
        self._is_need_align_pad = self._tiling_case.get("is_need_align_pad", False)
        self._is_cache = self._tiling_case.get("is_cache", False)
        self._is_atomic = self._tiling_case.get("is_atomic", True)
        self._impl_mode = self._tiling_case.get("impl_mode", 0)
        self.num_segments = self._tiling_case.get("num_segments", 0)
        self._is_core_x = self._tiling_case.get("is_core_x", False)
        self._op_type = self._tiling_case.get("op_type", "segmentensor_sum")
        self._move_pad = self._tiling_case.get("move_pad", 0)
        self._all_cache = self._tiling_case.get("all_cache", False)
        self._is_set_mask = self._tiling_case.get("is_set_mask", False)
        # DB
        self._is_db = self._tiling_case.get("is_db", False)
        self._segment_compute_type = 0
        self._scope = "local.UB"

        self._input_tensors = set()
        self._var_gm_tensor = None
        self._id_gm_tensor = None
        self._num_segments_gm_tensor = None
        self._block = None

        self._var_name = None
        self._id_name = None
        self._num_segments_name = None

        self._max_dtype_bytes = 4
        self._coexisting_quantity = 1
        self._ub_size = util.get_ub_size() - 32

        self._var_dtype_size = 4
        self._id_dtype_size = 4

        self._in_out_map = {}
        self._reorder_map = {}
        self._compute_at_map = {}
        self._emit_insn_map = {}

        # const tiling
        self._const_block_axis = 0
        self._const_ub_norm_axis = 0
        self._const_ub_reduce_axis = 0
        self._const_block_factor = 1
        self._const_ub_norm_factor = 1
        self._const_ub_reduce_factor = 1
        self._const_cache_num = 0
        self._const_cache_start = 0
        self._const_last_dim = 0

        self._id_storage_bound = 0
        self._var_storage_bound = 0
        self._cache_storage_bound = 0
        self._segment_storage_bound = 0

        self._fake_schedule = False

        self._id_ub = None
        self._var_ub = None
        self._var_align_pad_ub = None

        self._align_factor = None

        self._block_tiling_vars = {}
        self._ub_norm_tiling_vars = {}
        self._ub_reduce_tiling_vars = {}
        self._cache_num_tiling_vars = {}
        self._cache_start_tiling_vars = {}
        self._last_dim_tiling_vars = {}
        self._block_bind_axis = None
        self._block_factor = None
        self._cache_num = None
        self._cache_start = None
        self._last_dim = None
        self._compute_at_norm_axis = None
        self._compute_at_reduce_axis = None
        self._emit_axis = None
        self._emit_segment_axis = None
        self._reorder_axis_core_x = []
        self._reorder_axis = []

        self._out_atomic_len = 0

    def do_schedule(self):
        """
        schedule body
        :return:
        """
        if self._tiling_strategy == TilingStrategy.STATIC:
            add_build_arg("enable_loop_partition", True)
        self._construct_compute_graph()

        self._schedule = tvm.create_schedule(self._out_tensor.op)
        self._schedule.tiling_key = self._tiling_key

        self._cal_cache_read()
        self._do_cache_read()

        self._cal_cache_write()
        self._do_cache_write()

        self._cal_storage_bound()
        self._do_storage_bound()

        self._cal_mem_use()
        self._do_mem_use()

        self._calc_tiling()

        if self._fake_schedule:
            return None

        self._do_tiling()

        self._calc_multi_core()
        self._do_multi_core()

        self._calc_reorder()
        self._do_reorder()

        self._calc_storage_align()
        self._do_storage_align()

        self._calc_compute_at()
        self._do_compute_at()

        self._calc_double_buffer()
        self._do_double_buffer()

        self._calc_out_atomic_len()

        self._calc_emit_insn()
        self._do_emit_insn()

        self._do_group_axis()

        self._add_compile_info()

        return self._schedule

    def _construct_compute_graph(self):
        visited_tensors = set()
        self.__dfs_sub_graph(self._out_tensor, visited_tensors)

        for one_input_tensor in self._input_tensors:
            if one_input_tensor.name == self._var_name:
                self._var_gm_tensor = one_input_tensor
            elif one_input_tensor.name == self._id_name:
                self._id_gm_tensor = one_input_tensor
            elif one_input_tensor.name == self._num_segments_name:
                self._num_segments_gm_tensor = one_input_tensor

        self._var_dtype_size = DTYPE_BYTE_MAPPING.get(self._var_gm_tensor.dtype)
        self._id_dtype_size = DTYPE_BYTE_MAPPING.get(self._id_gm_tensor.dtype)

    def _cal_cache_read(self):
        pass

    def _do_cache_read(self):
        self._id_ub = self._schedule.cache_read(self._id_gm_tensor, self._scope,
                                                self._in_out_map.get(self._id_gm_tensor))
        self._var_ub = self._schedule.cache_read(self._var_gm_tensor, self._scope,
                                                 self._in_out_map.get(self._var_gm_tensor))
        if self._is_need_align_pad:
            self._var_align_pad_ub = self._schedule.cache_read(self._var_ub, self._scope,
                                                               self._in_out_map.get(self._var_gm_tensor))

    def _cal_cache_write(self):
        pass

    def _do_cache_write(self):
        pass

    def _cal_storage_bound(self):
        _coexisting_quantity_var = 1
        _coexisting_quantity_id = 1
        _coexisting_quantity_segment = 1 if not self._is_atomic else 0

        _coexisting_quantity_align_pad = 4 if self._is_need_align_pad else 1
        _coexisting_quantity_cache = 1 if self._is_cache else 0
        self._coexisting_quantity = (_coexisting_quantity_var * _coexisting_quantity_align_pad +
                                     _coexisting_quantity_id +
                                     _coexisting_quantity_segment + _coexisting_quantity_cache)

    def _do_storage_bound(self):
        ub_size = util.get_ub_size()
        cache_ub = ((2 * ub_size // 3)) // 32 * 32
        def _do_storange_bound_not_bfp16():
            if self._all_cache:
                if self._is_atomic and not self._is_need_align_pad:
                    tensor_space = ((ub_size - cache_ub) * 4 // 5) // 32 * 32
                elif self._is_atomic and self._is_need_align_pad:
                    tensor_space = ((ub_size - cache_ub) * 4 // 17) // 32 * 32
                elif not self._is_atomic and not self._is_need_align_pad:
                    tensor_space = ((ub_size - cache_ub) * 4 // 9) // 32 * 32
                else:
                    tensor_space = ((ub_size - cache_ub) * 4 // 21) // 32 * 32
                self._var_storage_bound = int(tensor_space // self._var_dtype_size)
                self._schedule[self._var_ub].set_buffer_size(self._var_storage_bound)
                self._id_storage_bound = int((tensor_space // 4) // self._id_dtype_size)
                self._schedule[self._id_ub].set_buffer_size(self._id_storage_bound)
                self._cache_storage_bound = int((cache_ub // 8 // 32 * 32) // self._var_dtype_size)
                if self._is_need_align_pad:
                    self._schedule[self._var_align_pad_ub].set_buffer_size(self._var_storage_bound)
            else:
                tensor_space = self._ub_size // self._coexisting_quantity // BLOCK_SIZE_BYTE * BLOCK_SIZE_BYTE
                self._var_storage_bound = int(tensor_space // self._var_dtype_size)
                self._cache_storage_bound = self._var_storage_bound
                self._schedule[self._var_ub].set_buffer_size(self._var_storage_bound)
                self._id_storage_bound = int(tensor_space // self._id_dtype_size)
                self._schedule[self._id_ub].set_buffer_size(self._id_storage_bound)
                if self._is_need_align_pad:
                    self._schedule[self._var_align_pad_ub].set_buffer_size(self._var_storage_bound)

        def _do_storange_bound_bfp16():
            if self._all_cache:
                self._cache_storage_bound = int((cache_ub // 8 // 32 * 32) // 4)
                var_storage_bound_ub = int((ub_size * 4) // 39) // 32 * 32
                id_storage_bound_ub = int((var_storage_bound_ub // 4) // 32 * 32)
                self._var_storage_bound = var_storage_bound_ub // 2
                if self._is_need_align_pad:
                    var_storage_bound_ub = int((ub_size * 4) // 75) // 32 * 32
                    id_storage_bound_ub = int((var_storage_bound_ub // 4) // 32 * 32)
                    self._var_storage_bound = var_storage_bound_ub // 2
                    self._schedule[self._var_align_pad_ub].set_buffer_size(self._var_storage_bound)
                self._id_storage_bound = id_storage_bound_ub // self._id_dtype_size
                self._schedule[self._var_ub].set_buffer_size(self._var_storage_bound)
                self._schedule[self._id_ub].set_buffer_size(self._id_storage_bound)
            else:
                var_storage_bound_ub = int((ub_size * 4) // 21) // 32 * 32
                id_storage_bound_ub = int((var_storage_bound_ub // 4) // 32 * 32)
                self._var_storage_bound = var_storage_bound_ub // 2
                if self._is_need_align_pad:
                    var_storage_bound_ub = int((ub_size * 4) // 33) // 32 * 32
                    id_storage_bound_ub = int((var_storage_bound_ub // 4) // 32 * 32)
                    self._var_storage_bound = var_storage_bound_ub // 2
                    self._schedule[self._var_align_pad_ub].set_buffer_size(self._var_storage_bound)
                self._id_storage_bound = id_storage_bound_ub // self._id_dtype_size
                self._schedule[self._var_ub].set_buffer_size(self._var_storage_bound)
                self._schedule[self._id_ub].set_buffer_size(self._id_storage_bound)
                self._cache_storage_bound = self._var_storage_bound
        if self._var_gm_tensor.dtype != "bfloat16" or (self._var_gm_tensor.dtype == "bfloat16" and not self._is_cache):
            _do_storange_bound_not_bfp16()
        else:
            _do_storange_bound_bfp16()

    def _cal_mem_use(self):
        pass

    def _do_mem_use(self):
        pass

    def _calc_tiling(self):
        if self._tiling_key != BASE_KET:
            funcs = {TilingStrategy.DYNAMIC: self._calc_tiling_dynamic,
                     TilingStrategy.STATIC: self._calc_tiling_static}
            funcs.get(self._tiling_strategy)()
        else:
            pass

    def _calc_tiling_dynamic(self):
        var_shape = util.shape_to_list(self._var_gm_tensor.shape)
        id_shape = util.shape_to_list(self._id_gm_tensor.shape)

        b_idx_reduce = self._tiling_case["block_tiling_reduce_axis"]
        u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
        u_idx_reduce = self._tiling_case["ub_tiling_reduce_axis"]
        is_cache = self._tiling_case["is_cache"]

        b_bound = (1, util.get_bound(id_shape[b_idx_reduce])[1])
        self._block_tiling_vars[b_idx_reduce] = operation.var_inner("_block_factor_" + str(b_idx_reduce), b_bound,
                                                                    "int64")
        if is_cache:
            self._cache_num_tiling_vars[0] = operation.var_inner("_segment_cache_num_" + str(0), (1, None), "int64")

        if u_idx_norm is not None:
            u_bound = (1, util.get_bound(var_shape[u_idx_norm])[1])
            self._ub_norm_tiling_vars[u_idx_norm] = operation.var_inner("_ub_norm_factor_" + str(u_idx_norm),
                                                                        u_bound, "int64")

        if u_idx_reduce is not None:
            u_bound = (1, util.get_bound(id_shape[u_idx_reduce])[1])
            self._ub_reduce_tiling_vars[u_idx_reduce] = operation.var_inner("_ub_reduce_factor_" + str(u_idx_reduce),
                                                                            u_bound, "int64")

    def _calc_tiling_static(self):
        tmp_output_shape = tuple(i.value for i in self._out_tensor.shape)
        outputs = [{"shape": tmp_output_shape, "dtype": self._out_tensor.dtype}]

        tmp_var_shape = tuple(i.value for i in self._var_gm_tensor.shape)
        tmp_id_shape = tuple(i.value for i in self._id_gm_tensor.shape)

        inputs = [{"shape": tmp_var_shape, "dtype": self._var_gm_tensor.dtype},
                  {"shape": tmp_id_shape, "dtype": self._id_gm_tensor.dtype}]

        base_info = [util.get_core_num(), self._ub_size, self._segment_compute_type,
                     self._var_dtype_size, self._id_dtype_size]
        special_soc = operation.get_context().get("_is_spe_soc")

        tensor_sizes = {self._pattern: [self._var_storage_bound, self._id_storage_bound, self._cache_storage_bound]}
        if self._is_atomic and self._pattern != SegmentCompileInfo.BASE_SCHEDULE_PATTERN_ATOMIC:

            tensor_sizes[SegmentCompileInfo.BASE_SCHEDULE_PATTERN_ATOMIC] = [self._var_storage_bound,
                                                                             self._id_storage_bound,
                                                                             self._cache_storage_bound]
        if not self._is_atomic and self._pattern != SegmentCompileInfo.BASE_SCHEDULE_PATTERN_NO_ATOMIC:

            tensor_sizes[SegmentCompileInfo.BASE_SCHEDULE_PATTERN_NO_ATOMIC] = [self._var_storage_bound,
                                                                                self._id_storage_bound,
                                                                                self._cache_storage_bound]
        const_compile_info = {
            CompileInfo.BASE_INFO: base_info,
            SegmentCompileInfo.CUSTOM_INFO: [int(self._is_atomic), self._impl_mode, self.num_segments, special_soc],
            SegmentCompileInfo.CONST_AXIS: self._segment_axis,
            SegmentCompileInfo.TENSOR_SIZES: tensor_sizes
        }
        const_compile_info.update(get_compile_info())

        op_type = "AutoTiling"
        run_info = op_tiling.do_op_tiling(op_type, const_compile_info, inputs, outputs)
        tiling_format = {
            "tiling_key": "int64",
            "block_axis": "int64",
            "block_factor": "int64",
            "ub_norm_axis": "int64",
            "ub_norm_factor": "int64",
            "ub_reduce_axis": "int64",
            "ub_reduce_factor": "int64",
            "segment_cache_num": "int64",
            "segment_cache_start": "int64",
            "segment_last_dim": "int64"}

        tiling_data = op_tiling.decode(run_info["tiling_data"], tiling_format)
        const_tiling_key = tiling_data.get("tiling_key")
        self._const_block_axis = tiling_data.get("block_axis")
        self._const_block_factor = tiling_data.get("block_factor")
        self._const_ub_norm_axis = tiling_data.get("ub_norm_axis")
        self._const_ub_norm_factor = tiling_data.get("ub_norm_factor")
        self._const_ub_reduce_axis = tiling_data.get("ub_reduce_axis")
        self._const_ub_reduce_factor = tiling_data.get("ub_reduce_factor")
        self._const_cache_num = tiling_data.get("segment_cache_num")
        self._const_cache_start = tiling_data.get("segment_cache_start")
        self._const_last_dim = tiling_data.get("segment_last_dim")

        if operation.get_context().get(SegmentCompileInfo.STATIC_SUCCESS) or const_tiling_key != self._tiling_key:
            operation.get_context().get_current_compute().get_current_schedule().add(SegmentCompileInfo.FAKE_SCHEDULE,
                                                                                     True)
            self._fake_schedule = True
        else:
            operation.get_context().add(SegmentCompileInfo.STATIC_SUCCESS, True)

    def _do_tiling(self):
        if self._tiling_strategy == TilingStrategy.DYNAMIC:
            b_idx_reduce = 0
            u_idx_norm = self._tiling_case["ub_tiling_norm_axis"]
            u_idx_reduce = self._tiling_case["ub_tiling_reduce_axis"]

            self._block_factor = self._block_tiling_vars.get(b_idx_reduce)
            ub_norm_factor = self._ub_norm_tiling_vars.get(u_idx_norm)
            ub_reduce_factor = self._ub_reduce_tiling_vars.get(u_idx_reduce)
            self._cache_num = self._cache_num_tiling_vars.get(0)
        else:
            b_idx_reduce = 0
            u_idx_norm = self._const_ub_norm_axis if self._const_ub_norm_axis != DUMMY_DIM else None
            u_idx_reduce = self._const_ub_reduce_axis
            self._block_factor = self._const_block_factor
            ub_norm_factor = self._const_ub_norm_factor
            ub_reduce_factor = self._const_ub_reduce_factor
            self._cache_num = self._const_cache_num
        b_o, b_i = self._schedule[self._out_tensor].split(self._out_tensor.op.reduce_axis[b_idx_reduce],
                                                          factor=self._block_factor)
        if u_idx_reduce is not None:
            u_reduce_o, u_reduce_i = self._schedule[self._out_tensor].split(b_i, factor=ub_reduce_factor)
            self._reorder_axis.extend([b_o, u_reduce_o, u_reduce_i])

        if u_idx_norm is not None:
            u_norm_o, u_norm_i = self._schedule[self._out_tensor].split(self._out_tensor.op.axis[1],
                                                                        factor=ub_norm_factor)
            self._reorder_axis.extend([u_norm_o, u_norm_i, self._out_tensor.op.axis[0]])
        else:
            for i in range(len(self._out_tensor.op.axis)):
                self._reorder_axis.append(self._out_tensor.op.axis[i])

        self._block_bind_axis = b_o
        self._compute_at_reduce_axis = u_reduce_o
        self._compute_at_norm_axis = u_norm_o if u_idx_norm is not None else u_reduce_o
        self._emit_axis = 1 if u_idx_norm else 0
        self._emit_segment_axis = u_norm_i if u_idx_norm is not None else u_reduce_i

    def _calc_multi_core(self):
        pass

    def _do_multi_core(self):
        self._block = tvm.thread_axis("blockIdx.x")
        self._schedule[self._out_tensor].bind(self._block_bind_axis, self._block)

    def _calc_reorder(self):
        self._reorder_map[self._out_tensor] = self._reorder_axis

    def _do_reorder(self):
        for single_tensor, param in self._reorder_map.items():
            self._schedule[single_tensor].reorder(*param)

    def _calc_storage_align(self):
        self._align_factor = BLOCK_SIZE_BYTE // self._var_dtype_size

    def _do_storage_align(self):
        if self._is_need_storage_align and not self._is_need_align_pad:
            self._schedule[self._var_ub].storage_align(self._var_ub.op.axis[-2],
                                                       self._align_factor, 0)
        if self._is_need_align_pad:
            self._schedule[self._var_align_pad_ub].storage_align(self._var_align_pad_ub.op.axis[-2],
                                                                 self._align_factor, 0)

    def _calc_compute_at(self):
        self._compute_at_map[self._id_ub] = [self._out_tensor, self._compute_at_reduce_axis]
        self._compute_at_map[self._var_ub] = [self._out_tensor, self._compute_at_norm_axis]
        if self._is_need_align_pad:
            self._compute_at_map[self._var_align_pad_ub] = [self._out_tensor, self._compute_at_norm_axis]

    def _do_compute_at(self):
        for tensor_i, param in self._compute_at_map.items():
            self._schedule[tensor_i].compute_at(self._schedule[param[0]], param[1])

    def _calc_double_buffer(self):
        pass

    def _do_double_buffer(self):
        pass

    def _calc_out_atomic_len(self):
        if get_soc_spec(SHORT_SOC_VERSION) in (ASCEND_910B, ASCEND_910_93):
            add_build_arg("enforce_mix_mode", True)
            out_shape = shape_util.shape_to_list(self._out_tensor.shape)
            out_shape_size = reduce(mul, out_shape, 1)
            self._out_atomic_len = out_shape_size * DTYPE_BYTE_MAPPING.get(self._out_tensor.dtype)

    def _calc_emit_insn(self):
        self._emit_insn_map[self._var_ub] = [self._var_ub.op.axis[self._emit_axis], "dma_copy"]
        self._emit_insn_map[self._id_ub] = [self._id_ub.op.axis[0], "dma_copy"]
        if self._is_need_align_pad:
            self._emit_insn_map[self._var_align_pad_ub] = [self._var_align_pad_ub.op.axis[self._emit_axis],
                                                           "align_pad", {"enough_buffer": False}]
        if self._all_cache:
            if self._is_set_mask:
                self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                         {"segment_atomic": 1, "segment_cache_num": self._cache_num,
                                                          "segment_cache_all":1, "set_vector_mask_hoist": True,
                                                          "atomic_total_len": self._out_atomic_len}]
            else:
                self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                         {"segment_atomic": 1, "segment_cache_num": self._cache_num,
                                                          "segment_cache_all":1, "set_vector_mask_hoist": False,
                                                          "atomic_total_len": self._out_atomic_len}]
        elif self._is_cache and not self._all_cache:
            if self._is_set_mask:
                self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                         {"segment_atomic": 1, "segment_cache_num": self._cache_num,
                                                          "set_vector_mask_hoist": True,
                                                          "atomic_total_len": self._out_atomic_len}]
            else:
                self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                         {"segment_atomic": 1, "segment_cache_num": self._cache_num,
                                                          "set_vector_mask_hoist": False,
                                                          "atomic_total_len": self._out_atomic_len}]
        else:
            self._emit_insn_map[self._out_tensor] = [self._emit_segment_axis, "dma_copy",
                                                     {"segment_atomic": 1, "atomic_total_len": self._out_atomic_len}]

    def _do_emit_insn(self):
        for tensor_i, param in self._emit_insn_map.items():
            self._schedule[tensor_i].emit_insn(*param)

    def _do_group_axis(self):
        pass

    def _add_compile_info(self):
        cpt_compute = operation.get_context().get_current_compute()
        cpt_schedule = cpt_compute.get_current_schedule()

        cpt_schedule.add(SegmentCompileInfo.FAKE_SCHEDULE, False)
        # BASE INFO
        cpt_schedule.add(CompileInfo.CORE_NUM, util.get_core_num())
        cpt_schedule.add(CompileInfo.UB_SIZE, self._ub_size)
        cpt_schedule.add(SegmentCompileInfo.SEGMENT_TYPE, self._segment_compute_type)
        cpt_schedule.add(SegmentCompileInfo.VAR_DTYPE_SIZE, self._var_dtype_size)
        cpt_schedule.add(SegmentCompileInfo.ID_DTYPE_SIZE, self._id_dtype_size)

        # CUSTOM INFO
        cpt_schedule.add(SegmentCompileInfo.IS_SUPPORT_ATOMIC, int(self._is_atomic))
        cpt_schedule.add(SegmentCompileInfo.IMPL_MODE, int(self._impl_mode))
        cpt_schedule.add(SegmentCompileInfo.NUM_SEGMENT, self.num_segments)
        cpt_schedule.add(SegmentCompileInfo.PATTERN, self._pattern)
        cpt_schedule.add(SegmentCompileInfo.VAR_NUM, self._var_storage_bound)
        cpt_schedule.add(SegmentCompileInfo.ID_NUM, self._id_storage_bound)
        cpt_schedule.add(SegmentCompileInfo.CACHE_NUM, self._cache_storage_bound)

    def __dfs_sub_graph(self, out, visited_tensors: set):
        if len(out.op.attrs) > 0:
            _segment_mode = operation.get_context().get("_segment_mode")
            self._segment_compute_type = 0 if _segment_mode == "segment" else 1

            if "var_name" in out.op.attrs:
                self._var_name = out.op.attrs["var_name"]
            if "id_name" in out.op.attrs:
                self._id_name = out.op.attrs["id_name"]
            if "num_segments_name" in out.op.attrs:
                self._num_segments_name = out.op.attrs["num_segments_name"]
        for tensor_i in out.op.input_tensors:
            util.merge_value(self._in_out_map, tensor_i, out)
            self._max_dtype_bytes = max(self._max_dtype_bytes, DTYPE_BYTE_MAPPING.get(tensor_i.dtype))
            if util.is_placeholder(tensor_i):
                self._input_tensors.add(tensor_i)
            if tensor_i in visited_tensors:
                continue
            visited_tensors.add(tensor_i)
            self.__dfs_sub_graph(tensor_i, visited_tensors)
