#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Reduce Schedule Remake stage 1 - Tilingcase
"""

# Standard Package
from enum import Enum
from typing import Dict
from typing import Iterable
from typing import List
from typing import NoReturn
from typing import Optional
from typing import Tuple
from typing import Union
from copy import deepcopy

from tbe import tvm
from tbe.common.platform import ASCEND_910
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_310P
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import get_block_size as get_block_size_byte
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.buildcfg import get_current_build_config
from tbe.common.utils import op_tiling
from tbe.dsl.base import operation
from tbe.dsl.base.operation import add_compile_info_inner
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import get_compile_info
from tbe.dsl.base.operation import get_context
from tbe.dsl.base.operation import get_op_context
from tbe.dsl.base.operation import register_build_pointcut
from tbe.tvm.tir import IntImm
from tbe.tvm import Var
from tbe.tvm import Tensor

from ...constants import ReduceSchType
from ...constants import CompileInfo
from ...constants import Pattern
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import ReducePattern
from ...constants import AtomicSupportMapDefault
from ...constants import AtomicSupportMap910BC
from ...computation import Computation
from ...util import get_reduce_all_axes
from ...util import get_reduce_axes
from ...util import get_reduce_axis_indexes
from ...util import is_reduce_tensor
from ...util import shape_to_list
from ...util import get_dsl_insn
from .vector_info import ComputeGraphInfo
from .vector_tilingcase import TilingCaseBase
from .....common.utils.errormgr import get_error_message
from ... import util

CONST = "const"
ZERO = "zero"
DEFAULT = "default"
OP_TYPE_AUTO_TILING = "AutoTiling"
TILINGKEY_NONE_REDUCE_AXIS = 2 ** 31 - 2
REDUCE_CASE_LENGTH_TWO = 2
COMMON_INFO_TRANSPOSE_IDX = 6
COMMON_INFO_ENTIRE_REDUCE_IDX = 10
NO_DB_IDX = 0
DB_IDX = 1
DIM_ARAR = 4

BLKOUTER = "block_outer"
BLKINNER = "block_inner"
UBOUTER = "ub_outer"
UBINNER = "ub_inner"
A = "A"
R = "R"

REDUCE_EMITSN_SUPPORT_TRANSPOSE = {
    "reduce_sum", "reduce_min", "reduce_max"
}

REDUCE_SUPPORT_TRANSPOSE = {
    # AR
    2: {"0_0"},
}

REDUCE_SUPPORT_ENTIRE_REDUCE = {
    # AR
    2: {"0_0"},
}

REDUCE_SUPPORT_HIGH_PRECISION_NORMAL = {
    # AR
    2: {"0_1"},
}

REDUCE_SUPPORT_PAD = {
    # AR
    2: {"0_0": False},

    # ARA
    3: {"0_0": True, "0_1": True, "1_1": False, "1_0": False}
}

REDUCE_EMPTY_TENSOR_TILINGKEY = 2 ** 63 - 1


class CalcReduceTilingCase(Computation):
    """
    Calculate Reduce TilingCase
    """

    def __init__(self, outs, option):
        self.outs = outs
        self.option = option

    def get_sub_pattern(self):
        return ReducePattern.R_0

    @classmethod
    def get_instance(cls, outs, option):
        return cls(outs, option)

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.REDUCE]

    @classmethod
    def get_supported_soc(cls):
        return [DEFAULT]

    def do_tiling_case(self):
        """
        do tiling case
        """
        # get result of tiling case
        return self.calc_tiling_case()

    def calc_tiling_case(self):
        """
        get tiling case of different situation
        """
        outs = list(self.outs) if isinstance(self.outs, (list, tuple)) else [self.outs]
        # to check reduce not support outputs with different shapes
        # need add check elewise op before and after reduce.
        current_compute = get_context().get_current_compute()

        # construct information of graph
        compute_graph_info = ComputeGraphInfo(outs)
        _current_support_check(outs, compute_graph_info)

        single_reduce_info = SingleReduceInfo(compute_graph_info)
        apply_common_compile_info(compute_graph_info, single_reduce_info)
        current_compute.add("_compute_graph_info", compute_graph_info)
        current_compute.add("_single_reduce_info", single_reduce_info)
        current_compute.add("_enable_atomic_deterministic", check_enable_deterministic_mode())

        # Different Cases
        tiling_case_list: List[ReduceTilingCase] = []
        if current_compute.get("_mode") == CONST:
            def _calc_const_case(const_tiling_case_list):
                # Get all possible ub_info
                possible_case_list = []
                possible_case_list += _calculate_tiling_cases(single_reduce_info)
                possible_case_list += _calculate_atomic_tiling_cases(single_reduce_info)
                possible_case_list += _calculate_group_reduce_tiling_cases(single_reduce_info)
                for _case in possible_case_list:
                    _calc_maximum_subgraph(_case, compute_graph_info, single_reduce_info)
                apply_dyn_compile_info(single_reduce_info, possible_case_list, model=CONST)
                # Get Real TilingCase
                cst_case = ReduceTilingCase()
                _calc_const_tiling_case(single_reduce_info, compute_graph_info, cst_case)
                const_tiling_case_list.append(cst_case)

            _calc_const_case(tiling_case_list)
        elif current_compute.get("_mode") == ZERO:
            # SingleCase
            tiling_case_list += [_gen_zero_tiling_case(), ]
            ComputeGraphInfo.get_maximum_subgraph(compute_graph_info, single_reduce_info, tiling_case_list[0])
        else:
            def _calc_dynamic_case(dynamic_tiling_case_list):
                dynamic_tiling_case_list += _calculate_tiling_cases(single_reduce_info)
                dynamic_tiling_case_list += _calculate_atomic_tiling_cases(single_reduce_info)
                dynamic_tiling_case_list += _calculate_group_reduce_tiling_cases(single_reduce_info)
                for tiling_case in dynamic_tiling_case_list:
                    _calc_tiling_key(single_reduce_info, tiling_case)
                # Empty schedule will always be there in the tiling_case_list
                # noinspection PyProtectedMember
                if "_empty_schedule_flag" not in get_context()._addition:
                    dynamic_tiling_case_list.append(ReduceTilingCase())
                    dynamic_tiling_case_list[-1].type = ReduceTilingCase.Type.EMPTY
                    get_context().add("_empty_schedule_flag", True)

                for _case in dynamic_tiling_case_list:
                    if _case.type == ReduceTilingCase.Type.EMPTY:
                        continue
                    _calc_maximum_subgraph(_case, compute_graph_info, single_reduce_info)
                apply_dyn_compile_info(single_reduce_info, dynamic_tiling_case_list)

            _calc_dynamic_case(tiling_case_list)

        return tiling_case_list


def _calc_maximum_subgraph(case, compute_graph_info, single_reduce_info):
    ComputeGraphInfo.get_maximum_subgraph(compute_graph_info, single_reduce_info, case)



class SingleReduceInfo:
    """
    data struct for a single reduce info
    """

    def __init__(self, compute_graph_info: ComputeGraphInfo):
        if len(compute_graph_info.reduce_tensor_set) != 1:
            raise RuntimeError("ComputeGraph is not in Single Reduce Pattern: %s" %
                               compute_graph_info.reduce_tensor_set)
        # Assume only one reduce node
        self.reduce_tensor: Tensor = tuple(compute_graph_info.reduce_tensor_set)[0]
        self.all_axes: List[Var] = get_reduce_all_axes(self.reduce_tensor)
        self.reduce_axes: List[Var] = get_reduce_axes(self.reduce_tensor)
        self.shape_before_reduce: List[Union[Var, IntImm]] = list(self.reduce_tensor.op.input_tensors[0].shape)
        self.shape_after_reduce: List[Union[Var, IntImm]] = list(self.reduce_tensor.shape)
        self.reduce_axis_indexes: List[int] = get_reduce_axis_indexes(self.reduce_tensor)
        self.keepdims: bool = len(self.shape_before_reduce) == len(self.shape_after_reduce)
        self.graph_info: ComputeGraphInfo = compute_graph_info

    def is_reduce_not_last_axis(self) -> bool:
        """
        check if reduce not last axis
        """
        is_not_last_axis = self.all_axes[-1] not in self.reduce_axes
        return is_not_last_axis

    def is_reduce_last_axis(self) -> bool:
        """
        check if reduce last axis
        """
        return self.all_axes[-1] in self.reduce_axes

    def is_reduce_all_axes(self) -> bool:
        """
        check if reduce all axis
        """
        return set(self.all_axes) == set(self.reduce_axes)

    def is_reduce_endpoint(self) -> bool:
        """
        check whether endpoint is reduce or not
        """
        return is_reduce_tensor(list(self.graph_info.endpoint_output_tensor_set)[0])

    def is_arar(self) -> bool:
        """
        check whether pattern is arar or not
        """
        return (len(self.shape_before_reduce) == DIM_ARAR and self.is_reduce_last_axis())

    @staticmethod
    def find_last_reduce_axis(shape, reduce_axis_indexes: Iterable[int]):
        """
        find last reduce axis
        """
        # shape_before_reduce:(ak+1,rk,...,r2,a2,r1,a1) or (ak,rk,...,r2,a1,r1)
        # find r1 position, r1 may contain continues axis
        r1_end_index = None
        for i in range(len(shape) - 1, -1, -1):
            if i in reduce_axis_indexes:
                r1_end_index = i
                break
        r1_start_index = r1_end_index
        if r1_end_index is None:
            return r1_start_index, r1_end_index
        for i in range(r1_end_index, -1, -1):
            if i not in reduce_axis_indexes:
                r1_start_index = i + 1
                break
            if i == 0:
                r1_start_index = i

        return r1_start_index, r1_end_index

    @staticmethod
    def find_last_none_reduce_axis(shape_before_reduce: list,
                                   reduce_axis_index: List[int]) -> Tuple[Optional[int], Optional[int]]:
        """
        :param shape_before_reduce
        :param reduce_axis_index
        :return the last axis or the last serials axises that are not in reduce_axis
        """
        # shape_before_reduce:(ak+1,rk,...,r2,a2,r1,a1) or (ak,rk,...,r2,a1,r1)
        # find a1 position, a1 may contain continues axis
        a1_end_index = None
        for i in range(len(shape_before_reduce) - 1, -1, -1):
            if i not in reduce_axis_index:
                a1_end_index = i
                break
        a1_start_index = a1_end_index
        if a1_end_index is None:
            return a1_start_index, a1_end_index
        for i in range(a1_end_index, -1, -1):
            if i in reduce_axis_index:
                a1_start_index = i + 1
                break
            if i == 0:
                a1_start_index = i

        return a1_start_index, a1_end_index

    @staticmethod
    def find_none_reduce_axis_map(shape_before_reduce: list, reduce_axis_index: List[int]) -> Dict[int, int]:
        """
        find none reduce axis map
        """
        none_reduce_index_map = {}
        count = 0
        for i in range(0, len(shape_before_reduce)):
            if i not in reduce_axis_index:
                none_reduce_index_map[i] = count
                count += 1
        return none_reduce_index_map


class ReduceTilingCase(TilingCaseBase):
    """
    tiling case data struct for reduce
    """

    class Type(Enum):
        """
        Reduce type Enum
        """
        NORMAL_REDUCE = "NORMAL"
        ATOMIC_REDUCE = "ATOMIC"
        GROUP_REDUCE = "GROUP"
        EMPTY = "EMPTY"
        REDUCE_EMPTY = "REDUCE_EMPTY"

    def __init__(self):
        self.type: Optional[ReduceTilingCase.Type] = ReduceTilingCase.Type.NORMAL_REDUCE
        self.block_split_axis_index = None
        self.block_factor = None
        self.block_split_axis_index_2 = None
        self.block_factor_2 = None
        self.is_secondary_block_split = False
        self.ub_split_axis_index = None
        self.ub_factor = None
        self.ub_split_axis_index_2 = None
        self.ub_factor_2 = None
        self.buffer_tile_max = None
        self.multi_core: Optional[bool] = None
        self.db = False
        self.tiling_key = 2 ** 31 - 1
        self.tensor_ub_size_before_reduce: Optional[int] = None
        self.tensor_ub_size_after_reduce: Optional[int] = None
        self.need_remove_pad = False
        self.is_secondary_ub_split = False
        self.reduce_sch_type = ReduceSchType.NORMAL
        # rar high precision schedule. 3表示下标，用于tiling和schedule使用数值传递data, 见 _reduce_post_build
        self.ub_factor_r_3 = None # workspace后面的reduce的切分
        self.ub_factor_a_4 = None # group reduce, ub在A上的切分

    def __repr__(self):
        segment0 = self.type.value
        segment1 = "ENABLED" if self.multi_core else "DISABLED"
        return "%s REDUCE: (%d, %d) with multicore %s" % (segment0,
                                                          self.block_split_axis_index, self.ub_split_axis_index,
                                                          segment1)

    def __hash__(self):
        return hash((self.type.value, self.block_split_axis_index, self.block_factor,
                     self.ub_split_axis_index, self.ub_factor, self.multi_core))

    def __eq__(self, other) -> bool:
        condition0 = other.type == self.type
        condition1 = other.block_split_axis_index == self.block_split_axis_index
        condition2 = other.block_factor == self.block_factor
        condition3 = other.ub_split_axis_index == self.ub_split_axis_index
        condition4 = other.ub_factor == self.ub_factor
        condition5 = other.multi_core == self.multi_core
        return (isinstance(other, ReduceTilingCase)
                and condition0 and condition1 and condition2 and condition3 and condition4 and condition5)

    def __ne__(self, other) -> bool:
        condition0 = other.type != self.type
        condition1 = other.block_split_axis_index != self.block_split_axis_index
        condition2 = other.block_factor != self.block_factor
        condition3 = other.ub_split_axis_index != self.ub_split_axis_index
        condition4 = other.ub_factor != self.ub_factor
        condition5 = other.multi_core != self.multi_core
        return (not isinstance(other, ReduceTilingCase)
                or condition0 or condition1 or condition2 or condition3 or condition4 or condition5)


class Dim:
    """
    do actions for dim
    """

    def __init__(self, axis_type, idx, var_type=None):
        self.axis_type = axis_type
        self.var_type = var_type
        self.idx = idx

    @staticmethod
    def split(in_shape, split_idx, model=None):
        """
        do dim split
        """
        if model == "UBSplit":
            outer, inner = UBOUTER, UBINNER
        else:
            outer, inner = BLKOUTER, BLKINNER

        # update index
        for item in in_shape[split_idx + 1:]:
            item.idx += 1

        # insert split
        in_shape[split_idx].var_type = outer
        in_shape.insert(split_idx + 1, Dim(in_shape[split_idx].axis_type, split_idx + 1, inner))

    @staticmethod
    def rfactor(in_shape, axis, factor_axis=0):
        """
        get rfactor shape
        """
        temp_shape, a_shape, r_shape = [], [], []
        for item in in_shape:
            if item not in axis:
                temp_shape.append(item)

        for item in reversed(axis):
            item.axis_type = A
            temp_shape.insert(factor_axis, item)

        for item in temp_shape:
            if item.axis_type == A:
                a_shape.append(item)
            else:
                r_shape.append(item)

        return a_shape, r_shape

    @staticmethod
    def group(nums):
        """
        group nums
        """
        nums = sorted(set(nums))
        gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s + 1 < e]
        edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])
        return list(zip(edges, edges))


@register_build_pointcut(pattern=Pattern.REDUCE)
def build_pointcut(func, *args, **kwargs):
    """
    build pointcut
    :param func:
    :param args:
    :param kwargs:
    :return:
    """

    def _find_idx_in_tensor_list():
        if len(args) < 2:
            dict_args = {"errCode": "E90003", "detailed_cause": "Size of args should more than 2"}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        tensor_list = args[1].get("tensor_list")
        _before_reduce = operation.get_context().get("_placeholder_before_reduce")
        _after_reduce = operation.get_context().get("_placeholder_after_reduce")

        if not _before_reduce and not _after_reduce:
            return

        for tensors in tensor_list:
            for _idx, _tensor in enumerate(tensors):
                if _tensor in _before_reduce:
                    operation.add_compile_info_inner("_idx_before_reduce", _idx)
                    return
            for _idx, _tensor in enumerate(tensors):
                if _tensor in _after_reduce:
                    operation.add_compile_info_inner("_idx_before_reduce", _idx)
                    return

        dict_args = {"errCode": "E90003", "detailed_cause": "Can not find placeholder_op"}
        raise RuntimeError(dict_args, get_error_message(dict_args))

    def _add_fake_workspace(_args):
        schedule_list, _ = _args
        parameter_length_set = set()
        for same_compute_schs in schedule_list:
            for single_sch in same_compute_schs:
                parameter_length_set.add(len(util.get_sch_additional_entry(single_sch, "real_outs")))

        parameter_max_length, parameter_min_length = max(parameter_length_set), min(parameter_length_set)
        if parameter_max_length == parameter_min_length:
            return

        for same_compute_schs in schedule_list:
            for single_sch in same_compute_schs:
                ori_real_outs = util.get_sch_additional_entry(single_sch, "real_outs")
                ext_workspace = util.get_sch_additional_entry(single_sch, "need_extra_fake_workspace")
                if len(ori_real_outs) == parameter_max_length and (ext_workspace is None or ext_workspace is False):
                    continue

                # not exactly: for index in range(parameter_max_length - parameter_min_length + 1):
                for index in range(parameter_max_length - len(ori_real_outs) + 1):
                    fake_workspace = tvm.placeholder([], dtype="uint8", name="fake_workspace_" + str(index))
                    ori_real_outs.append(fake_workspace)

                util.add_sch_additional_entry(single_sch, "real_outs", ori_real_outs)
        workspace_num = parameter_max_length - parameter_min_length + 1
        add_compile_info_inner("_workspace_size", workspace_num)

        for i in range(workspace_num):
            get_op_context().add_workspace(f"workspace_{i}")

        return

    def _add_build_config():
        operation.add_build_arg("enable_s64_to_s32", True)

    _find_idx_in_tensor_list()
    _add_fake_workspace(args)
    _add_build_config()
    func(*args, **kwargs)
    _reduce_post_build()


def _reduce_post_build():
    """
    encode reduce vars in reduce sch
    """

    def _encode_var_name(_var_names):
        after_encode_name = []
        for name in _var_names:
            names = name[1:].split('_')
            if names[0] == 'ori':
                after_encode_name.append(10000)
            elif names[0] == 'dim':
                after_encode_name.append(20000 + int(names[1]))
            elif names[0] == 'block':
                if len(names) > 2:
                    after_encode_name.append(30000 + int(names[-1])) # 30002
                else:
                    after_encode_name.append(30000)
            elif names[0] == 'ub':
                if len(names) > 2:
                    after_encode_name.append(40000 + int(names[-1])) # 40002 4003 4004
                else:
                    after_encode_name.append(40000)
            elif names[0] == 'buffer':
                after_encode_name.append(50000)
            else:
                raise_error("unknown var name in reduce schedule, please check")

        return after_encode_name

    normal_vars = get_compile_info().get(CompileInfo.NORMAL_VARS)
    reduce_vars = {}
    for tiling_key, var_names in normal_vars.items():
        reduce_vars[tiling_key] = _encode_var_name(var_names)
    add_compile_info_inner("_reduce_vars", reduce_vars)


def check_ar_high_precision_support(reduce_info: SingleReduceInfo):
    """
    check if current tiling case support high_precision
    """
    reduce_tensor: Tensor = reduce_info.reduce_tensor
    if reduce_tensor is None:
        return False

    shape_limit = len(reduce_info.shape_before_reduce) == 2 and reduce_info.is_reduce_last_axis()
    if shape_limit and add_high_precision_support(reduce_info):
        return True

    return False


def check_ara_high_precision_support(reduce_info: SingleReduceInfo):
    """
    check if current ara tiling case support high_precision
    """
    reduce_tensor: Tensor = reduce_info.reduce_tensor
    if reduce_tensor is None:
        return False

    # 当前只支持RA
    shape_before_reduce = shape_to_list(reduce_info.shape_before_reduce)
    # ARA len is 3
    shape_limit = len(reduce_info.reduce_axis_indexes) == 1 and len(shape_before_reduce) == 3 and \
        reduce_info.is_reduce_not_last_axis()
    if shape_limit and add_high_precision_support(reduce_info):
        return True

    return False


def add_high_precision_support(reduce_info: SingleReduceInfo):
    """
    add for compile info, cannot include shape_limit. 动态场景会有多个shape.
    """
    reduce_tensor: Tensor = reduce_info.reduce_tensor
    if reduce_tensor is None:
        return False

    if (util.is_v220() and get_dsl_insn(reduce_tensor) in ("reduce_sum", )
            and reduce_tensor.dtype in ("float32", "float16", "bfloat16")):
        return True

    return False


def apply_common_compile_info(graph_info, reduce_info):
    """
    apply common compile info
    """
    # Common_Info: message from ori computation that only attach once
    pre_compile_info = get_compile_info()
    if pre_compile_info:
        reduce_tensor_dtype_byte = DTYPE_BYTE_MAPPING.get(reduce_info.reduce_tensor.dtype)
        # Only dtype uint1 use float reduce_tensor_dtype_byte. Need to convert to
        # negative reciprocal in order to pass through common_info to tiling stage.
        if isinstance(reduce_tensor_dtype_byte, float):
            reduce_tensor_dtype_byte = -int(1 / reduce_tensor_dtype_byte)
        if CompileInfo.COMMON_INFO not in pre_compile_info.keys():
            atomic = 0
            if check_atomic_add_support(reduce_info):
                atomic = 1

            core_num = get_soc_spec("CORE_NUM")
            keep_dims = 1
            min_block_size = int(32 // DTYPE_BYTE_MAPPING[graph_info.min_type])
            support_group_reduce = True
            support_transpose = 0
            support_entire_reduce = 0
            if reduce_info.reduce_tensor.op.tag in REDUCE_EMITSN_SUPPORT_TRANSPOSE:
                support_transpose = 1
            if util.is_v220():
                support_entire_reduce = 1
            is_v220 = util.is_v220()
            common_info = [core_num, keep_dims, min_block_size, atomic, graph_info.coef,
                           graph_info.pad_max_entire_size, support_transpose, reduce_tensor_dtype_byte,
                           support_group_reduce, is_v220, support_entire_reduce]
            add_compile_info_inner(CompileInfo.COMMON_INFO, common_info)
        else:
            # update reduce_tensor_dtype_byte
            cur_common_info = pre_compile_info.get(CompileInfo.COMMON_INFO)
            reduce_tensor_dtype_byte_index = 7
            cur_common_info[reduce_tensor_dtype_byte_index] = max(cur_common_info[reduce_tensor_dtype_byte_index],
                                                                  reduce_tensor_dtype_byte)
    else:
        raise RuntimeError("pre_compile_info is Null")


def append_initial_ub_info(pre_compile_info, ub_info_map, ub_info, ub_info_rf):
    for sch_type in ReduceSchType:
        if sch_type == ReduceSchType.NORMAL:
            ub_info[NO_DB_IDX].append(0)
            ub_info[DB_IDX].append(0)
            ub_info_rf[NO_DB_IDX].append(0)
            ub_info_rf[DB_IDX].append(0)
        else:
            ub_info_str = "_ub_info_" + sch_type.name.lower()
            ub_info_map[sch_type] = pre_compile_info.get(ub_info_str)
            ub_info_map.get(sch_type)[NO_DB_IDX].append(0)
            ub_info_map.get(sch_type)[DB_IDX].append(0)


def _append_db_tiling_case(info: SingleReduceInfo, tiling_case_list: List[ReduceTilingCase],
                           tiling_case: ReduceTilingCase, reduce_axis_index):
    """
    check db can enable
    """
    if len(reduce_axis_index) != 1:
        return
    # AR_HIGH_PRECISION AR_HIGH_PRECISION_WORKSPACE ARA_HIGH_PRECISION ARA_HIGH_PRECISION_BIG_DIM
    # ARA_HIGH_PRECISION_WORKSPACE ARA_HIGH_PRECISION_BIG_DIM_WORKSPACE
    if tiling_case.reduce_sch_type.value >= ReduceSchType.AR_HIGH_PRECISION.value and \
        tiling_case.reduce_sch_type.value <= ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM.value:
        return

    shape_before_reduce = shape_to_list(info.shape_before_reduce)
    _is_all_reduce_pattern = len(shape_before_reduce) == 2 and shape_before_reduce[0] == 1 \
                             and len(reduce_axis_index) == 1 and reduce_axis_index[0] == 1
    if len(info.graph_info.reduce_parents_list) > 0 and not _is_all_reduce_pattern:
        # has input db scenario, no need enable db
        return

    if get_context().get("_mode") != CONST:
        if len(info.graph_info.elewise_tensor_set) > 10:
            # ub fusion too much tensor
            return
    tiling_case_db = deepcopy(tiling_case)
    tiling_case_db.db = True
    tiling_case_list.append(tiling_case_db)
    return


def _append_tiling_case(info: SingleReduceInfo, tiling_case_list: List[ReduceTilingCase], tiling_case: ReduceTilingCase,
                        reduce_axis_index):
    if tiling_case is None:
        return

    tiling_case_list.append(tiling_case)
    _append_db_tiling_case(info, tiling_case_list, tiling_case, reduce_axis_index)


def check_enable_deterministic_mode():
    """
    check whether enable atomic branch or not
    """
    version = get_soc_spec("SHORT_SOC_VERSION")
    if version not in (ASCEND_910B, ASCEND_910_93, ASCEND_310P):
        return False
    return True


def apply_dyn_compile_info(reduce_info, tiling_case_list, model="dynamic"):
    """
    apply dynamic compile info
    """

    # dynamic message from each case of tiling_case_list
    def _is_rfactor(_item):
        cond_0 = reduce_info.is_reduce_last_axis()
        cond_1 = reduce_info.all_axes[_case.ub_split_axis_index] in reduce_info.reduce_axes
        cond_2 = _item.type == ReduceTilingCase.Type.NORMAL_REDUCE
        if cond_0 and cond_1 and cond_2:
            return True
        return False

    if model == CONST:
        compile_axis = get_context().get_current_compute().get("_ori_axis")
        pattern_info = _gen_const_tiling_key(compile_axis)
    else:
        pattern_info = _get_pattern_key(reduce_info.shape_before_reduce,
                                        reduce_info.reduce_axis_indexes)

    pre_compile_info = get_compile_info()
    if pre_compile_info:
        # different patterns
        # pattern_info must not be repeated
        ub_info_map = {}
        if "_pattern_info" not in pre_compile_info.keys():
            current_pattern = [pattern_info, ]
            ub_info = [[0, ], [0, ]]
            ub_info_rf = [[0, ], [0, ]]
            for sch_type in ReduceSchType:
                ub_info_map[sch_type] = [[0, ], [0, ]]
        else:
            current_pattern = pre_compile_info.get("_pattern_info")
            current_pattern.append(pattern_info)
            ub_info = pre_compile_info.get("_ub_info")
            ub_info_rf = pre_compile_info.get("_ub_info_rf")
            append_initial_ub_info(pre_compile_info, ub_info_map, ub_info, ub_info_rf)

        # different tilings in the pattern
        enabled_db = pre_compile_info.get("_enabled_db", False)
        _idx = current_pattern.index(pattern_info)
        for _case in tiling_case_list:
            if _case.type == ReduceTilingCase.Type.EMPTY:
                continue
            if _case.db:
                enabled_db = _case.db
            index = DB_IDX if _case.db else NO_DB_IDX
            max_ub_count = _case.tensor_ub_size_before_reduce
            if _case.reduce_sch_type == ReduceSchType.NORMAL:
                if _is_rfactor(_case):
                    ub_info_rf[index][_idx] = max_ub_count
                    if not ub_info[index][_idx]:
                        ub_info[index][_idx] = max_ub_count
                else:
                    ub_info[index][_idx] = max_ub_count
                    if not ub_info_rf[index][_idx]:
                        ub_info_rf[index][_idx] = max_ub_count
            else:
                ub_info_map.get(_case.reduce_sch_type)[index][_idx] = max_ub_count

        add_compile_info_inner("_enabled_db", enabled_db)
        add_compile_info_inner("_enable_deterministic_mode", check_enable_deterministic_mode())
        add_compile_info_inner("_enable_high_precision", add_high_precision_support(reduce_info))
        add_compile_info_inner("_is_reduce_endpoint", reduce_info.is_reduce_endpoint())
        add_compile_info_inner("_pattern_info", current_pattern)
        add_compile_info_inner("_ub_info_rf", ub_info_rf)
        add_compile_info_inner("_ub_info", ub_info)
        for sch_type in ReduceSchType:
            if sch_type == ReduceSchType.NORMAL:
                continue
            ub_info_str = "_ub_info_" + sch_type.name.lower()
            add_compile_info_inner(ub_info_str, ub_info_map.get(sch_type))


def _calc_tiling_key(reduce_info, tiling):
    if get_context().get("_mode") == CONST:
        ori_axis = get_context().get_current_compute().get("_ori_axis")
        tiling_key = _gen_const_tiling_key(ori_axis)
    else:
        # tiling: single_case
        tiling_key = _get_tiling_key(tiling, reduce_info)

    tiling.tiling_key = tiling_key


def _check_add_workspace(const_tiling_case):
    if operation.in_dynamic():
        return False

    if const_tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE or \
        const_tiling_case.reduce_sch_type == ReduceSchType.ARA_HIGH_PRECISION_WORKSPACE or \
        const_tiling_case.reduce_sch_type == ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM_WORKSPACE:
        return True
    if const_tiling_case.type == const_tiling_case.Type.GROUP_REDUCE:
        return True
    if (const_tiling_case.type == const_tiling_case.Type.ATOMIC_REDUCE and check_enable_deterministic_mode()):
        return True
    return False


def _calc_const_tiling_case(single_reduce_info, compute_graph_info, const_tiling_case):
    add_compile_info_inner("_reduce_shape_known", True)
    shape_before_reduce = shape_to_list(single_reduce_info.shape_before_reduce)
    shape_after_reduce = shape_to_list(single_reduce_info.shape_after_reduce)
    reduce_axis_index = single_reduce_info.reduce_axis_indexes
    input_dtype = tuple(compute_graph_info.input_tensor_set)[0].dtype
    output_dtype = tuple(compute_graph_info.output_tensor_set)[0].dtype
    axes_dtype = get_context().get_current_compute().get("_axis_dtype")
    axes_dtype = axes_dtype if axes_dtype is not None else "int64"
    known_reduce_axis_flag = get_context().get("_known_reduce_axis_flag")

    # staging axis info in ops
    # _ori_axis: original axes from func_enter of ReduceD, maybe None while op is ReduceSum
    # ori_axis: axes from classify, always existed
    ori_reduce_axis = get_compile_info().get("_ori_axis")
    compile_axis = get_context().get_current_compute().get("_ori_axis")
    if known_reduce_axis_flag:
        # invoking op_tiling interface during compilation need axis info in sch
        add_compile_info_inner("_ori_axis", reduce_axis_index)
        inputs = [{"shape": shape_before_reduce, "dtype": input_dtype}]
    else:
        add_compile_info("axes_idx", 1)
        inputs = [{"shape": shape_before_reduce, "dtype": input_dtype},
                  {"shape": (len(reduce_axis_index),), "dtype": axes_dtype, "name": "axes",
                   "const_value": reduce_axis_index}]
    outputs = [{"shape": shape_after_reduce, "dtype": output_dtype}]
    # the flag of invoking op_tiling interface during compilation
    add_compile_info_inner("_const_shape_post", False)
    add_compile_info_inner("_compile_pattern", _gen_const_tiling_key(compile_axis))
    add_compile_info_inner("_int64_mode", operation.is_int64_mode())

    run_info = op_tiling.do_op_tiling(OP_TYPE_AUTO_TILING, get_compile_info(), inputs, outputs)

    _gen_const_tiling_case(single_reduce_info, const_tiling_case, run_info)
    # the flag of invoking op_tiling interface during running
    add_compile_info_inner("_const_shape_post", True)
    # invoking op_tiling interface during running need axis info in ops
    if ori_reduce_axis is not None:
        add_compile_info_inner("_ori_axis", ori_reduce_axis)

    block_dims = get_compile_info().get(CompileInfo.BLOCK_DIMS)
    if block_dims is None:
        block_dims = {}
        add_compile_info_inner(CompileInfo.BLOCK_DIMS, block_dims)
    block_dims[str(const_tiling_case.tiling_key)] = run_info["block_dim"]
    atomic_flags = get_compile_info().get(CompileInfo.ATOMIC_FLAGS)
    if atomic_flags is None:
        atomic_flags = {}
        add_compile_info_inner(CompileInfo.ATOMIC_FLAGS, atomic_flags)
    atomic_flags[str(const_tiling_case.tiling_key)] = run_info["clear_atomic"]
    workspace_bytes = get_compile_info().get(CompileInfo.WORKSPACES)
    if workspace_bytes is None:
        workspace_bytes = {}
        add_compile_info_inner(CompileInfo.WORKSPACES, workspace_bytes)
    workspace_bytes[str(const_tiling_case.tiling_key)] = run_info["workspaces"]

    if _check_add_workspace(const_tiling_case):
        _add_workspace_info_to_context(run_info["workspaces"])


def _gen_const_tiling_case(single_reduce_info, const_tiling_case, run_info):
    shape_before_reduce = shape_to_list(single_reduce_info.shape_before_reduce)
    tiling_type = "int64" if operation.is_int64_mode() else "int"
    tiling_format = {"block_axis": tiling_type,
                     "block_factor": tiling_type,
                     "ub_axis": tiling_type,
                     "ub_factor": tiling_type,
                     "tensor_ub_size_before_reduce": tiling_type,
                     "tensor_ub_size_after_reduce": tiling_type,
                     "sch_type": tiling_type,
                     "is_group_reduce": tiling_type,
                     "ub_axis_2": tiling_type,
                     "ub_factor_2": tiling_type,
                     "ub_factor_r_3": tiling_type,
                     "ub_factor_a_4": tiling_type,
                     "buffer_tile_max": tiling_type,
                     "enable_db": tiling_type,
                     "block_axis_2": tiling_type,
                     "block_factor_2": tiling_type}

    tiling_data = op_tiling.decode(run_info["tiling_data"], tiling_format)
    const_tiling_case.block_split_axis_index = tiling_data["block_axis"]
    const_tiling_case.block_factor = tiling_data["block_factor"]
    const_tiling_case.block_split_axis_index_2 = tiling_data["block_axis_2"]
    const_tiling_case.block_factor_2 = tiling_data["block_factor_2"]
    const_tiling_case.ub_split_axis_index = tiling_data["ub_axis"]
    const_tiling_case.ub_factor = tiling_data["ub_factor"]
    const_tiling_case.ub_split_axis_index_2 = tiling_data["ub_axis_2"]
    const_tiling_case.ub_factor_2 = tiling_data["ub_factor_2"]
    const_tiling_case.ub_factor_r_3 = tiling_data["ub_factor_r_3"]
    const_tiling_case.ub_factor_a_4 = tiling_data["ub_factor_a_4"]
    const_tiling_case.buffer_tile_max = tiling_data["buffer_tile_max"]
    const_tiling_case.tensor_ub_size_before_reduce = tiling_data["tensor_ub_size_before_reduce"]
    const_tiling_case.tensor_ub_size_after_reduce = tiling_data["tensor_ub_size_after_reduce"]
    if const_tiling_case.ub_split_axis_index_2 >= 0:
        const_tiling_case.is_secondary_ub_split = True
    if const_tiling_case.block_split_axis_index_2 >= 0:
        const_tiling_case.is_secondary_block_split = True
    if run_info.get("clear_atomic"):
        if tiling_data.get("is_group_reduce"):
            const_tiling_case.type = const_tiling_case.Type.GROUP_REDUCE
        else:
            const_tiling_case.type = const_tiling_case.Type.ATOMIC_REDUCE
    else:
        const_tiling_case.type = const_tiling_case.Type.NORMAL_REDUCE
    const_tiling_case.multi_core = True if run_info["block_dim"] > 1 else False
    sch_type = tiling_data.get("sch_type")
    const_tiling_case.reduce_sch_type = ReduceSchType(sch_type)
    const_tiling_case.db = tiling_data["enable_db"]

    def _calc_const_pad_case():
        if const_tiling_case.reduce_sch_type == ReduceSchType.PAD or \
           const_tiling_case.reduce_sch_type == ReduceSchType.PAD_ACCUMULATION:
            split_axis_info = \
                str(const_tiling_case.block_split_axis_index) + "_" + str(const_tiling_case.ub_split_axis_index)
            support_pattern = REDUCE_SUPPORT_PAD.get(len(shape_before_reduce))
            if support_pattern is not None and split_axis_info in support_pattern:
                const_tiling_case.need_remove_pad = support_pattern.get(split_axis_info)
            else:
                if const_tiling_case.reduce_sch_type == ReduceSchType.PAD_ACCUMULATION:
                    const_tiling_case.reduce_sch_type = ReduceSchType.ACCUMULATION
                else:
                    const_tiling_case.reduce_sch_type = ReduceSchType.NORMAL

    _calc_const_pad_case()
    _calc_tiling_key(single_reduce_info, const_tiling_case)


def _add_workspace_info_to_context(workspaces):
    # 32 means fake workspace size
    workspace_size = 32
    if len(workspaces) > 0:
        workspace_size = workspaces[0]

    get_op_context().add_workspace(f"workspace_0", size=workspace_size)


def _gen_zero_tiling_case():
    zero_tiling_case = _create_tiling_case(0, 0, case_type=ReduceTilingCase.Type.REDUCE_EMPTY)
    zero_tiling_case.tiling_key = _gen_zero_tiling_key()

    return zero_tiling_case


def _calculate_tiling_cases(info: SingleReduceInfo) -> List[ReduceTilingCase]:
    tiling_case_list = []
    if info.is_reduce_not_last_axis():
        _gen_tiling_case_not_last_axis(info, tiling_case_list)
    else:
        _gen_tiling_case_last_axis(info, tiling_case_list)
        if not tiling_case_list:
            _gen_tiling_case_reduce_all(info, tiling_case_list)

    return tiling_case_list


def _calculate_atomic_tiling_cases(info: SingleReduceInfo) -> List[ReduceTilingCase]:
    tiling_case_list = []
    if check_atomic_add_support(info):
        shape_before_reduce = info.shape_before_reduce
        reduce_axis_index = info.reduce_axis_indexes
        is_determst = check_enable_deterministic_mode()
        is_b16_arar = (not info.is_reduce_endpoint() and info.is_arar() and is_determst)
        if info.is_reduce_all_axes() and info.is_reduce_endpoint():
            tiling_case_list += _gen_atomic_tiling_case_reduce_all(shape_before_reduce)

        elif info.is_reduce_not_last_axis() and info.is_reduce_endpoint():
            tiling_case_list += _gen_atomic_tiling_case_not_last_axis(info, reduce_axis_index)

        elif info.is_reduce_last_axis() and (info.is_reduce_endpoint() or is_b16_arar):
            tiling_case_list += _gen_atomic_tiling_case_last_axis(info, shape_before_reduce, reduce_axis_index)
    return tiling_case_list


def _calculate_group_reduce_tiling_cases(info: SingleReduceInfo) -> List[ReduceTilingCase]:
    tiling_case_list = []
    if check_ar_high_precision_support(info):
        _calc_ar_high_precision_group_case(info, tiling_case_list)
    if check_ara_high_precision_support(info):
        _calc_ara_high_precision_group_case(tiling_case_list)

    # need to make compilation when reduce is not endpoint
    if check_atomic_add_support(info) and info.is_reduce_endpoint():
        return tiling_case_list

    def group_reduce_pattern():
        if len(info.reduce_axis_indexes) > 1:
            return False
        # ARA
        is_ara = len(info.shape_before_reduce) == 3 and not info.is_reduce_last_axis()
        # AR
        is_ar = len(info.shape_before_reduce) == 2 and info.is_reduce_last_axis() and util.is_v220()
        return is_ara or is_ar

    # not ARA and AR
    if not group_reduce_pattern():
        return tiling_case_list

    is_none_reduce = True
    for reduce_axis in info.reduce_axis_indexes:
        if not util.expr_equal(info.shape_before_reduce[reduce_axis], 1):
            is_none_reduce = False
            break
    if is_none_reduce:
        return tiling_case_list

    for block_split_axis in range(len(info.shape_before_reduce)):
        if block_split_axis not in info.reduce_axis_indexes:
            continue
        for ub_split_axis in range(len(info.shape_before_reduce)):
            if ub_split_axis < block_split_axis:
                continue
            _gen_group_reduce_case(info, tiling_case_list, block_split_axis, ub_split_axis)
            # secondary ub split only for ARA pattern
            if len(info.shape_before_reduce) == 3:
                for ub_split_axis_2 in range(len(info.shape_before_reduce)):
                    # not support split last A
                    if ub_split_axis_2 == len(info.shape_before_reduce) - 1:
                        continue
                    _gen_group_reduce_case(info, tiling_case_list, block_split_axis, ub_split_axis, ub_split_axis_2)

    return tiling_case_list


def _calc_ar_high_precision_group_case(info: SingleReduceInfo, tiling_case_list):
    block_split_axis = 1
    ub_split_axis = 1
    tiling_case = _create_tiling_case(block_split_axis, ub_split_axis,
                                      case_type=ReduceTilingCase.Type.GROUP_REDUCE,
                                      sch_type=ReduceSchType.AR_HIGH_PRECISION)
    tiling_case_list.append(tiling_case)

    tiling_case_workspace = _create_tiling_case(block_split_axis, ub_split_axis,
                                                case_type=ReduceTilingCase.Type.GROUP_REDUCE,
                                                sch_type=ReduceSchType.AR_HIGH_PRECISION_WORKSPACE)
    tiling_case_list.append(tiling_case_workspace)


def _calc_ara_high_precision_group_case(tiling_case_list):
    block_split_axis = 1
    ub_split_axis = 1
    tiling_case = _create_tiling_case(block_split_axis, ub_split_axis,
                                      case_type=ReduceTilingCase.Type.GROUP_REDUCE,
                                      sch_type=ReduceSchType.ARA_HIGH_PRECISION_WORKSPACE)
    tiling_case_list.append(tiling_case)

    tilingcase_ub = deepcopy(tiling_case)
    tilingcase_ub.ub_split_axis_index_2 = 2 # 2: last A
    tilingcase_ub.is_secondary_ub_split = True
    tiling_case_list.append(tilingcase_ub)

    tiling_case_big_dim = _create_tiling_case(block_split_axis, ub_split_axis,
                                                case_type=ReduceTilingCase.Type.GROUP_REDUCE,
                                                sch_type=ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM_WORKSPACE)
    tiling_case_list.append(tiling_case_big_dim)

    tiling_case_big_dim_ub = deepcopy(tiling_case_big_dim)
    tiling_case_big_dim_ub.ub_split_axis_index_2 = 2 # 2: last A
    tiling_case_big_dim_ub.is_secondary_ub_split = True
    tiling_case_list.append(tiling_case_big_dim_ub)


def _gen_group_reduce_case(info, tiling_case_list, block_split_axis, ub_split_axis, ub_split_axis_2=None):
    tiling_case = _create_tiling_case(block_split_axis, ub_split_axis, case_type=ReduceTilingCase.Type.GROUP_REDUCE)
    if ub_split_axis_2 is not None:
        tiling_case.ub_split_axis_index_2 = ub_split_axis_2
        tiling_case.is_secondary_ub_split = True
    tiling_case_list.append(tiling_case)

    tiling_case_pad = _calc_pad_case(info.shape_before_reduce, block_split_axis, ub_split_axis,
                                     ReduceTilingCase.Type.GROUP_REDUCE)
    if tiling_case_pad is not None:
        if ub_split_axis_2 is not None:
            tiling_case_pad.ub_split_axis_index_2 = ub_split_axis_2
            tiling_case_pad.is_secondary_ub_split = True
        tiling_case_list.append(tiling_case_pad)

    tiling_case_accumulation = \
                _calc_accumulation_case(info, block_split_axis,
                                        ub_split_axis, ReduceTilingCase.Type.GROUP_REDUCE)
    if tiling_case_accumulation is not None:
        if ub_split_axis_2 is not None:
            tiling_case_accumulation.ub_split_axis_index_2 = ub_split_axis_2
            tiling_case_accumulation.is_secondary_ub_split = True
        tiling_case_list.append(tiling_case_accumulation)

    if tiling_case_pad is not None and tiling_case_accumulation is not None:
        tiling_case_pad_accumulation = _calc_pad_accumulation_case(tiling_case_accumulation)
        if ub_split_axis_2 is not None:
            tiling_case_pad_accumulation.ub_split_axis_index_2 = ub_split_axis_2
            tiling_case_pad_accumulation.is_secondary_ub_split = True
        tiling_case_list.append(tiling_case_pad_accumulation)


def check_atomic_add_support(reduce_info: SingleReduceInfo):
    """
    check if current tiling case support atomic
    """
    # Common Regulation
    version = get_soc_spec("SHORT_SOC_VERSION")
    if version not in [ASCEND_910B, ASCEND_910_93, ASCEND_910, ASCEND_310P]:
        return False

    reduce_tensor: Tensor = reduce_info.reduce_tensor
    if reduce_tensor is None:
        return False

    if not check_enable_deterministic_mode() and not reduce_info.is_reduce_endpoint():
        return False

    # Special Regulation
    _map = AtomicSupportMap910BC if version in (ASCEND_910B, ASCEND_910_93) else AtomicSupportMapDefault
    if reduce_tensor.dtype not in _map.get("support_dtype"):
        return False
    if reduce_tensor.op.tag not in _map.get("support_insn"):
        return False

    return True


def _create_tiling_case(block_split_axis_index, ub_split_axis_index,
                        case_type=ReduceTilingCase.Type.NORMAL_REDUCE,
                        sch_type=ReduceSchType.NORMAL,
                        multi_core=True):
    tiling_case = ReduceTilingCase()
    tiling_case.block_split_axis_index = block_split_axis_index
    tiling_case.ub_split_axis_index = ub_split_axis_index
    tiling_case.multi_core = multi_core
    tiling_case.type = case_type
    tiling_case.reduce_sch_type = sch_type
    return tiling_case


def _check_block_split_2(tiling_case: ReduceTilingCase, block_split_axis_2):
    if tiling_case is None:
        return
    if block_split_axis_2 is not None:
        tiling_case.block_split_axis_index_2 = block_split_axis_2
        tiling_case.is_secondary_block_split = True


def _gen_tiling_case_normal(tiling_info_params, info: SingleReduceInfo, block_split_axis_2=None):
    tiling_case_list, shape_before_reduce, reduce_axis_index, block_split_axis, ub_split_axis = tiling_info_params

    tiling_case = _create_tiling_case(block_split_axis, ub_split_axis)
    _check_block_split_2(tiling_case, block_split_axis_2)
    _append_tiling_case(info, tiling_case_list, tiling_case, reduce_axis_index)

    tiling_case_pad = _calc_pad_case(shape_before_reduce, block_split_axis, ub_split_axis,
                                     ReduceTilingCase.Type.NORMAL_REDUCE)
    _check_block_split_2(tiling_case_pad, block_split_axis_2)
    _append_tiling_case(info, tiling_case_list, tiling_case_pad, reduce_axis_index)

    tiling_case_accumulation = _calc_accumulation_case(info, block_split_axis,
                                                       ub_split_axis, ReduceTilingCase.Type.NORMAL_REDUCE)
    if block_split_axis_2 is None or block_split_axis != len(shape_before_reduce) - 1:
        _check_block_split_2(tiling_case_accumulation, block_split_axis_2)
        _append_tiling_case(info, tiling_case_list, tiling_case_accumulation, reduce_axis_index)

    if tiling_case_pad is not None and tiling_case_accumulation is not None:
        tiling_case_pad_accumulation = _calc_pad_accumulation_case(tiling_case_accumulation)
        if block_split_axis_2 is None or block_split_axis != len(shape_before_reduce) - 1:
            _check_block_split_2(tiling_case_pad_accumulation, block_split_axis_2)
            _append_tiling_case(info, tiling_case_list, tiling_case_pad_accumulation, reduce_axis_index)


def _gen_tiling_case_not_last_axis(info: SingleReduceInfo, tiling_case_list: List[ReduceTilingCase]) -> NoReturn:
    shape_before_reduce = info.shape_before_reduce
    reduce_axis_index = info.reduce_axis_indexes

    reordered_shape, reorder_to_orignal_axis_map, _ = \
        _reorder_reduce_nlast_shape(shape_before_reduce, reduce_axis_index)
    
    if check_ara_high_precision_support(info):
        _calc_ara_high_precision_normal_case(tiling_case_list)

    for i in range(0, len(reordered_shape)):
        orignal_axis = reorder_to_orignal_axis_map.get(i)
        if orignal_axis not in reduce_axis_index:
            block_split_axis = orignal_axis

            for j in range(0, len(reordered_shape)):
                orignal_axis = reorder_to_orignal_axis_map.get(j)
                if orignal_axis not in reduce_axis_index and j < i:
                    continue
                ub_split_axis = reorder_to_orignal_axis_map.get(j)
                tiling_info_params = (tiling_case_list, shape_before_reduce, reduce_axis_index,
                                      block_split_axis, ub_split_axis)
                _gen_tiling_case_normal(tiling_info_params, info)
                # blk secondary split, AR ARA ARAR ARARA
                if len(info.reduce_axis_indexes) > 1:
                    continue
                for i2 in range(0, i + 1):
                    orignal_axis = reorder_to_orignal_axis_map.get(i2)
                    if orignal_axis not in reduce_axis_index and (i2 == 0 or i2 == i):
                        block_split_axis_2 = orignal_axis
                        tiling_info_params = (tiling_case_list, shape_before_reduce, reduce_axis_index,
                                              block_split_axis, ub_split_axis)
                        _gen_tiling_case_normal(tiling_info_params, info, block_split_axis_2)


def _calc_pad_accumulation_case(tilingcase):
    tilingcase_pad_accumulation = deepcopy(tilingcase)
    tilingcase_pad_accumulation.reduce_sch_type = ReduceSchType.PAD_ACCUMULATION
    return tilingcase_pad_accumulation


def _calc_accumulation_case(info, block_split_axis, ub_split_axis, case_type):
    # not ARA
    if len(info.reduce_axis_indexes) > 1 or info.is_reduce_last_axis():
        return None

    tiling_case_accumulation = _create_tiling_case(block_split_axis, ub_split_axis, case_type,
                                                   ReduceSchType.ACCUMULATION)
    return tiling_case_accumulation


def _calc_pad_case(shape_before_reduce, block_split_axis, ub_split_axis, case_type):
    if util.is_v220():
        return None
    # for pure move case don't need do pad
    first_dim_is_one = util.expr_equal(shape_before_reduce[0], 1)
    if first_dim_is_one and len(shape_before_reduce) == REDUCE_CASE_LENGTH_TWO:
        return None

    split_axis_info = str(block_split_axis) + "_" + str(ub_split_axis)
    support_pattern = REDUCE_SUPPORT_PAD.get(len(shape_before_reduce))
    if support_pattern is not None and split_axis_info in support_pattern:
        tiling_case_pad = _create_tiling_case(block_split_axis, ub_split_axis, case_type, ReduceSchType.PAD)
        tiling_case_pad.need_remove_pad = support_pattern.get(split_axis_info)
        return tiling_case_pad
    return None


def _calc_transpose_case(shape_before_reduce, block_split_axis, ub_split_axis, case_type):
    if get_compile_info().get(CompileInfo.COMMON_INFO)[COMMON_INFO_TRANSPOSE_IDX] == 0:
        return None

    first_dim_is_one = util.expr_equal(shape_before_reduce[0], 1)
    if first_dim_is_one and len(shape_before_reduce) == REDUCE_CASE_LENGTH_TWO:
        return None

    split_axis_info = str(block_split_axis) + "_" + str(ub_split_axis)
    support_pattern = REDUCE_SUPPORT_TRANSPOSE.get(len(shape_before_reduce))
    if support_pattern is not None and split_axis_info in support_pattern:
        tiling_case_transpose = _create_tiling_case(block_split_axis, ub_split_axis, case_type, ReduceSchType.TRANSPOSE)
        return tiling_case_transpose
    return None


def _calc_entire_reduce_case(shape_before_reduce, block_split_axis, ub_split_axis, case_type):
    if get_compile_info().get(CompileInfo.COMMON_INFO)[COMMON_INFO_ENTIRE_REDUCE_IDX] == 0:
        return None

    split_axis_info = str(block_split_axis) + "_" + str(ub_split_axis)
    support_pattern = REDUCE_SUPPORT_ENTIRE_REDUCE.get(len(shape_before_reduce))
    if support_pattern is not None and split_axis_info in support_pattern:
        tiling_case_entire_reduce = _create_tiling_case(block_split_axis, ub_split_axis, case_type,
                                                        ReduceSchType.ENTIRE_REDUCE)
        return tiling_case_entire_reduce
    return None


def _calc_ar_high_precision_normal_case(tiling_info_params, info: SingleReduceInfo, block_split_axis_2=None):
    tiling_case_list, shape_before_reduce, reduce_axis_index, block_split_axis, ub_split_axis = tiling_info_params
    split_axis_info = str(block_split_axis) + "_" + str(ub_split_axis)
    support_pattern = REDUCE_SUPPORT_HIGH_PRECISION_NORMAL.get(len(shape_before_reduce))
    if support_pattern is not None and split_axis_info in support_pattern:
        tiling_case_high_precision = _create_tiling_case(block_split_axis, ub_split_axis,
                                    ReduceTilingCase.Type.NORMAL_REDUCE, ReduceSchType.AR_HIGH_PRECISION)
        _check_block_split_2(tiling_case_high_precision, block_split_axis_2)
        _append_tiling_case(info, tiling_case_list, tiling_case_high_precision, reduce_axis_index)

        tiling_case_high_precision = _create_tiling_case(block_split_axis, ub_split_axis,
                                    ReduceTilingCase.Type.NORMAL_REDUCE, ReduceSchType.AR_HIGH_PRECISION_WORKSPACE)
        _check_block_split_2(tiling_case_high_precision, block_split_axis_2)
        _append_tiling_case(info, tiling_case_list, tiling_case_high_precision, reduce_axis_index)


def _gen_ara_high_precision_normal_case(tiling_case_list, ara_sch_type):
    # ARA
    block_split_axis = 2 # last A
    ub_split_axis = 1 # R
    block_split_axis_2 = 2 # blk2必切在last A上

    # ARA_HIGH_PRECISION
    ara_high_precision = _create_tiling_case(block_split_axis, ub_split_axis,
                        ReduceTilingCase.Type.NORMAL_REDUCE, ara_sch_type)
    tiling_case_list.append(ara_high_precision)

    ara_high_precision_blk = deepcopy(ara_high_precision)
    _check_block_split_2(ara_high_precision_blk, block_split_axis_2)
    tiling_case_list.append(ara_high_precision_blk)


def _calc_ara_high_precision_normal_case(tiling_case_list):
    _gen_ara_high_precision_normal_case(tiling_case_list, ReduceSchType.ARA_HIGH_PRECISION)
    _gen_ara_high_precision_normal_case(tiling_case_list, ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM)
    _gen_ara_high_precision_normal_case(tiling_case_list, ReduceSchType.ARA_HIGH_PRECISION_WORKSPACE)
    _gen_ara_high_precision_normal_case(tiling_case_list, ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM_WORKSPACE)


def _gen_tiling_case_normal_last(tiling_info_params, info: SingleReduceInfo, block_split_axis_2=None):
    tiling_case_list, shape_before_reduce, reduce_axis_index, block_split_axis, ub_split_axis = tiling_info_params
    tiling_case = _create_tiling_case(block_split_axis, ub_split_axis)
    _check_block_split_2(tiling_case, block_split_axis_2)
    _append_tiling_case(info, tiling_case_list, tiling_case, reduce_axis_index)

    tiling_case_pad = _calc_pad_case(shape_before_reduce, block_split_axis, ub_split_axis,
                                        ReduceTilingCase.Type.NORMAL_REDUCE)
    _check_block_split_2(tiling_case_pad, block_split_axis_2)
    _append_tiling_case(info, tiling_case_list, tiling_case_pad, reduce_axis_index)

    tiling_case_transpose = _calc_transpose_case(shape_before_reduce, block_split_axis,
                                                    ub_split_axis, ReduceTilingCase.Type.NORMAL_REDUCE)
    _check_block_split_2(tiling_case_transpose, block_split_axis_2)
    _append_tiling_case(info, tiling_case_list, tiling_case_transpose, reduce_axis_index)

    tiling_case_entire_reduce = _calc_entire_reduce_case(shape_before_reduce, block_split_axis,
                                                    ub_split_axis, ReduceTilingCase.Type.NORMAL_REDUCE)
    _check_block_split_2(tiling_case_entire_reduce, block_split_axis_2)
    _append_tiling_case(info, tiling_case_list, tiling_case_entire_reduce, reduce_axis_index)

    # R轴较大的场景才会走进来，理论上不会存在block的二次切分
    if check_ar_high_precision_support(info) and block_split_axis_2 is None:
        _calc_ar_high_precision_normal_case(tiling_info_params, info, block_split_axis_2)


def _gen_tiling_case_last_axis(info: SingleReduceInfo, tiling_case_list: List[ReduceTilingCase]):
    shape_before_reduce = info.shape_before_reduce
    reduce_axis_index = info.reduce_axis_indexes

    reordered_shape, reorder_to_orignal_axis_map, _ = \
        _reorder_reduce_last_shape(shape_before_reduce, reduce_axis_index)

    for i in range(0, len(reordered_shape)):
        orignal_axis = reorder_to_orignal_axis_map.get(i)
        if orignal_axis not in reduce_axis_index:
            block_split_axis = orignal_axis

            for j in range(i, len(reordered_shape)):
                ub_split_axis = reorder_to_orignal_axis_map.get(j)
                tiling_info_params = (tiling_case_list, shape_before_reduce, reduce_axis_index,
                                      block_split_axis, ub_split_axis)
                _gen_tiling_case_normal_last(tiling_info_params, info)
                # blk secondary split, AR ARA ARAR ARARA
                if len(info.reduce_axis_indexes) > 1:
                    continue
                for i2 in range(0, i + 1):
                    orignal_axis = reorder_to_orignal_axis_map.get(i2)
                    if orignal_axis not in reduce_axis_index and (i2 == 0 or i2 == i):
                        block_split_axis_2 = orignal_axis
                        tiling_info_params = (tiling_case_list, shape_before_reduce, reduce_axis_index,
                                              block_split_axis, ub_split_axis)
                        _gen_tiling_case_normal_last(tiling_info_params, info, block_split_axis_2)


def _gen_tiling_case_reduce_all(info: SingleReduceInfo, tiling_case_list: List[ReduceTilingCase]):
    shape_before_reduce = info.shape_before_reduce
    reduce_axis_index = info.reduce_axis_indexes
    reordered_shape, reorder_to_orignal_axis_map, _ = \
        _reorder_reduce_last_shape(shape_before_reduce, reduce_axis_index)
    block_split_axis = 0
    for i in range(0, len(reordered_shape)):
        ub_split_axis = reorder_to_orignal_axis_map[i]
        tiling_case = _create_tiling_case(block_split_axis, ub_split_axis, multi_core=False)
        _append_tiling_case(info, tiling_case_list, tiling_case, reduce_axis_index)


def _gen_atomic_tiling_case_not_last_axis(info, reduce_axis_index) -> List[ReduceTilingCase]:
    shape_before_reduce = info.shape_before_reduce
    reordered_shape, reorder_to_orignal_axis_map, _ = \
        _reorder_reduce_nlast_shape(shape_before_reduce,
                                    reduce_axis_index)
    tiling_case_list = []
    for i in range(0, len(reordered_shape)):
        orignal_axis = reorder_to_orignal_axis_map.get(i)
        if orignal_axis in reduce_axis_index:
            block_split_axis = orignal_axis
            for j in range(0, len(reordered_shape)):
                orignal_axis = reorder_to_orignal_axis_map.get(j)
                if orignal_axis in reduce_axis_index and j < i:
                    continue
                ub_split_axis = reorder_to_orignal_axis_map.get(j)
                tiling_case = _create_tiling_case(block_split_axis, ub_split_axis,
                                                  case_type=ReduceTilingCase.Type.ATOMIC_REDUCE)
                _append_tiling_case(info, tiling_case_list, tiling_case, reduce_axis_index)

                tiling_case_pad = _calc_pad_case(shape_before_reduce, block_split_axis, ub_split_axis,
                                                 ReduceTilingCase.Type.ATOMIC_REDUCE)
                _append_tiling_case(info, tiling_case_list, tiling_case_pad, reduce_axis_index)

                tiling_case_accumulation = \
                    _calc_accumulation_case(info, block_split_axis,
                                            ub_split_axis, ReduceTilingCase.Type.ATOMIC_REDUCE)
                _append_tiling_case(info, tiling_case_list, tiling_case_accumulation, reduce_axis_index)

                if tiling_case_pad is not None and tiling_case_accumulation is not None:
                    tiling_case_pad_accumulation = _calc_pad_accumulation_case(tiling_case_accumulation)
                    _append_tiling_case(info, tiling_case_list, tiling_case_pad_accumulation, reduce_axis_index)

    return tiling_case_list


def _gen_atomic_tiling_case_last_axis(info: SingleReduceInfo, shape_before_reduce,
                                      reduce_axis_index) -> List[ReduceTilingCase]:
    """
    :param shape_before_reduce:
    :param reduce_axis_index:
    :return:
    """
    reordered_shape, reorder_to_orignal_axis_map, _ = _reorder_reduce_last_shape(shape_before_reduce,
                                                                                 reduce_axis_index)
    tiling_case_list = []
    for i in range(0, len(reordered_shape)):
        orignal_axis = reorder_to_orignal_axis_map.get(i)
        if orignal_axis in reduce_axis_index:
            block_split_axis = orignal_axis
            for j in range(0, len(reordered_shape)):
                orignal_axis = reorder_to_orignal_axis_map.get(j)
                if orignal_axis in reduce_axis_index and j < i:
                    continue
                ub_split_axis = reorder_to_orignal_axis_map.get(j)
                tiling_case = _create_tiling_case(block_split_axis, ub_split_axis,
                                                  case_type=ReduceTilingCase.Type.ATOMIC_REDUCE)
                _append_tiling_case(info, tiling_case_list, tiling_case, reduce_axis_index)
    return tiling_case_list


def _gen_atomic_tiling_case_reduce_all(shape_before_reduce) -> List[ReduceTilingCase]:
    tiling_case_list = []
    for i in range(0, len(shape_before_reduce)):
        block_split_axis = i
        for j in range(i, len(shape_before_reduce)):
            ub_split_axis = j
            tiling_case = _create_tiling_case(block_split_axis, ub_split_axis,
                                              case_type=ReduceTilingCase.Type.ATOMIC_REDUCE)
            tiling_case_list.append(tiling_case)
    return tiling_case_list


def _reorder_reduce_nlast_shape(shape_before_reduce: list,
                                reduce_axis_index: List[int]):
    """
    reorder shape (r4,a4,r3,a3,r2,a2,r1,a1) to (a4,a3,a2, r4,r3,r2,r1,a1)
    :param shape_before_reduce: like (r4,a4,r3,a3,r2,a2,r1,a1)
    :param reduce_axis_index
    :return:
    """
    # shape_before_reduce: (ak+1,rk,..,r2,a2,r1,a1)
    # find the last none-reduce axis a1

    a1_start_index, _ = SingleReduceInfo.find_last_none_reduce_axis(shape_before_reduce, reduce_axis_index)
    last_none_reduce_axis = a1_start_index

    orignal_to_reorder_axis_map = {}
    reorder_to_orignal_axis_map = {}
    # (ak+1,ak,...,a2, rk,..,r2,r1,a1)
    reordered_shape = list(shape_before_reduce)
    temp_axis = last_none_reduce_axis - 1
    for i in range(len(reduce_axis_index) - 1, -1, -1):
        reordered_shape[temp_axis] = shape_before_reduce[
            reduce_axis_index[i]]
        reorder_to_orignal_axis_map[temp_axis] = reduce_axis_index[i]
        orignal_to_reorder_axis_map[reduce_axis_index[i]] = temp_axis
        temp_axis = temp_axis - 1
    for i in range(last_none_reduce_axis - 1, -1, -1):
        if i not in reduce_axis_index:
            reordered_shape[temp_axis] = shape_before_reduce[i]
            reorder_to_orignal_axis_map[temp_axis] = i
            orignal_to_reorder_axis_map[i] = temp_axis
            temp_axis = temp_axis - 1

    for i in range(last_none_reduce_axis, len(shape_before_reduce)):
        reorder_to_orignal_axis_map[i] = i
        orignal_to_reorder_axis_map[i] = i

    return reordered_shape, reorder_to_orignal_axis_map, orignal_to_reorder_axis_map


def _reorder_reduce_last_shape(shape_before_reduce: list,
                               reduce_axis_index: List[int]):
    """
    reorder shape (a4,r4,a3,r3,a2,r2,a1,r1) to (a4,a3,a2,a1,r4,r3,r2,r1)
    :param shape_before_reduce: like(a4,r4,a3,r3,a2,r2,a1,r1)
    :param reduce_axis_index
    :return:
    """
    # shape_before_reduce: (a4,r4,a3,r3,a2,r2,a1,r1)

    orignal_to_reorder_axis_map = {}
    reorder_to_orignal_axis_map = {}

    reordered_shape = []
    temp_axis = 0
    for i, ele in enumerate(shape_before_reduce):
        if i not in reduce_axis_index:
            reordered_shape.append(ele)
            reorder_to_orignal_axis_map[temp_axis] = i
            orignal_to_reorder_axis_map[i] = temp_axis
            temp_axis = temp_axis + 1

    for i, ele in enumerate(shape_before_reduce):
        if i in reduce_axis_index:
            reordered_shape.append(ele)
            reorder_to_orignal_axis_map[temp_axis] = i
            orignal_to_reorder_axis_map[i] = temp_axis
            temp_axis = temp_axis + 1

    return reordered_shape, reorder_to_orignal_axis_map, orignal_to_reorder_axis_map


def _get_tiling_key(tiling, reduce_info):
    """
    :param tiling: tiling case.
    :param reduce_info: reduce info about shape and axis etc.

    :return: key(int64)
    """

    shape = reduce_info.shape_before_reduce
    reduce_axis_idx = reduce_info.reduce_axis_indexes
    block_split_axis = tiling.block_split_axis_index
    ub_split_axis = tiling.ub_split_axis_index
    atomic = tiling.type == tiling.Type.ATOMIC_REDUCE
    enable_db = tiling.db
    shape_type = tiling.reduce_sch_type.value
    ub_secondary_split_axis = 0
    block_secondary_split_axis = 0
    if tiling.is_secondary_ub_split:
        ub_secondary_split_axis = tiling.ub_split_axis_index_2 + 1
    if tiling.is_secondary_block_split:
        block_secondary_split_axis = tiling.block_split_axis_index_2 + 1

    pattern = _get_pattern_key(shape, reduce_axis_idx)
    pos = (block_secondary_split_axis, atomic, enable_db, ub_secondary_split_axis, shape_type,
           block_split_axis, ub_split_axis, pattern)
    val = (10 ** 12, 10 ** 11, 10 ** 10, 10 ** 9, 10 ** 7, 10 ** 6, 10 ** 5, 10 ** 2)
    key = 0
    for item, value in enumerate(pos):
        # no need check the dim size _check(item, value)
        key += value * val[item]
    return key


def _get_pattern_key(_shape, _reduce_idx_list):
    pattern_key = 0
    length = len(_shape)
    for i in range(length):
        if i in _reduce_idx_list:
            pattern_key += 2 * 2 ** (length - i - 1)
        else:
            pattern_key += 2 ** (length - i - 1)

    return pattern_key


def _gen_const_tiling_key(reduce_axis):
    """
    generate dict key from reduce_axis
    :param reduce_axis:
    :return:
    """
    if not reduce_axis:
        return TILINGKEY_NONE_REDUCE_AXIS
    reduce_axis_local = list(reduce_axis)[:]
    reduce_axis_local = sorted(reduce_axis_local)
    dict_key = 0
    for i in reduce_axis_local:
        dict_key = 10 * dict_key + i + 1

    return dict_key


def _gen_zero_tiling_key():
    compute = get_context().get_current_compute()
    shape = compute.get("_shape")
    if shape[-1] == 0:
        return REDUCE_EMPTY_TENSOR_TILINGKEY
    else:
        _raise_error("Generate zero tiling key not support shape: {0}.".format(shape))


def _raise_error(message):
    dict_args = {"errCode": "E90003", "detailed_cause": message}
    raise RuntimeError(dict_args, get_error_message(dict_args))


def _current_support_check(outs, _graph_info):
    if not _graph_info.reduce_tensor_set:
        raise RuntimeError("Couldn't find reduce node for ReduceSchedule")

    for out in outs:
        if out in _graph_info.reduce_tensor_set and out in _graph_info.trunk_mid_output_tensor_set | \
                _graph_info.branch_mid_output_tensor_set:
            raise RuntimeError("Dynamic reduce schedule not support mid output on reduce tensor.")


def get_block_size(dtype):
    ub_block_size = get_block_size_byte()
    dtype_and_block_size_map = {
    "uint1": ub_block_size * 8,
    "bool": ub_block_size,
    "int8": ub_block_size,
    "uint8": ub_block_size,
    "bfloat16": ub_block_size // 2,
    "float16": ub_block_size // 2,
    "float32": ub_block_size // 4,
    "int32": ub_block_size // 4,
    "int64": ub_block_size // 8
    }
    if dtype not in dtype_and_block_size_map:
        _raise_error("[%s] is not support type in norm" % dtype)
    return dtype_and_block_size_map.get(dtype)


def raise_error(message):
    """
    raise error
    """
    dict_args = {"errCode": "E90003", "detailed_cause": message}
    raise RuntimeError(dict_args, get_error_message(dict_args))
