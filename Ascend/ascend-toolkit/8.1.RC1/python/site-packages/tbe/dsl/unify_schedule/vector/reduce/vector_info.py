#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Information containers for compute graph of vector operator schedule
"""

# Standard Packages
from typing import Any
from typing import Callable
from typing import Dict
from typing import Iterable
from typing import List
from typing import Optional
from typing import Set
from typing import Tuple
from typing import Union

from tbe import tvm
from tbe.common.platform import ASCEND_310
from tbe.common.platform import ASCEND_310P
from tbe.common.platform import ASCEND_610
from tbe.common.platform import BS9SX1A
from tbe.common.platform import ASCEND_910
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_910_93
from tbe.common.platform import HI3796CV300CS
from tbe.common.platform import HI3796CV300ES
from tbe.common.platform import SD3403
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform import get_block_size
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform.platform_info import intrinsic_check_support
from tbe.dsl.base import operation
from tbe.dsl.padding.padding import Action
from tbe.dsl.padding.padding import ActionType
from tbe.dsl.padding.padding import calc_padding
from tbe.tvm import PlaceholderOp
from tbe.tvm import Tensor

from .....common.utils.errormgr import get_error_message
from ...constants import DTYPE_BYTE_MAPPING
from ...constants import FAKE_NODE_TAG
from ...constants import ReduceSchType
from ...util import equals_one
from ...util import expr_equal
from ...util import get_reduce_all_axes
from ...util import get_reduce_axes
from ...util import is_placeholder
from ...util import shape_to_list
from ...util import get_dsl_insn
from ...util import need_extent_node
from ...util import is_vcmpsel_insn
from ...util import is_vcmp_insn
from ...util import is_vsel_insn

ASCEND_SHISI = "smallhisi"
BLOCK = 16
ALIGN_AND_REMOVE_PAD_EXTRA_NODES = 3

REDUCE_MAX_MIN_SUPPORT_VCROSSFUNC = {
    "Default": ("float16", "float32", "int32"),
    ASCEND_310: ("float16",),
    ASCEND_910: ("float16",),
    ASCEND_910B: ("float16", "float32", "int32"),
    ASCEND_910_93: ("float16", "float32", "int32"),
    ASCEND_310P: ("float16",),
    ASCEND_610: ("float16",),
    BS9SX1A: ("float16",),
    ASCEND_SHISI: ("float16",),
}

OP_TAG_VC_INSTRINSIC_MAP = {
    "reduce_sum": "Intrinsic_vcadd",
    "reduce_min": "Intrinsic_vcmin",
    "reduce_max": "Intrinsic_vcmax",
    "reduce_prod": None
}

REDUCE_COMPUTE = {
    "reduce_min", "reduce_max", "reduce_sum",
    "reduce_all", "reduce_any",
    "reduce_prod", "tuple_reduce_sum",
}


class ComputeGraphInfo:
    """
    Operator Compute Graph Info collector and container
    """

    def __init__(self, output_tensors: Iterable[Tensor]):
        """
        Initialize containers and try to collect info
        """
        self.soc_ub_size = get_soc_spec("UB_SIZE")
        # For ReduceSch(only)
        self.coexisting_quantities_map = {}
        self.soc_reserved_ub_size_map = {}
        for sch_type in ReduceSchType:
            self.coexisting_quantities_map[sch_type] = []
            self.soc_reserved_ub_size_map[sch_type] = 0

        self.pad_available_ub_size_before_reduce = 0
        self.pad_available_ub_size_after_reduce = 0
        self.pad_max_entire_size = 0
        self.output_tensor_set: Optional[Set[Tensor]] = set(output_tensors)
        # real_output_tensor_set: set doesn't contain fake_node
        self.real_output_tensor_set: Optional[Set[Tensor]] = set()
        self.mid_tensor_set: Set[Tensor] = set()

        self.tensor_consumers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_producers_map: Optional[Dict[Tensor, Set[Tensor]]] = None
        self.tensor_list: Optional[List[Tensor]] = None

        # Extra info initialized by hooks
        self.reduce_tensor_set: Set[Tensor] = set()
        self.broadcast_tensor_set: Set[Tensor] = set()
        self.elewise_tensor_set: Set[Tensor] = set()
        self.input_tensor_set: Set[Tensor] = set()

        self.trunk_tensor_list: Optional[List[Tensor]] = None
        # Extra info initialized after pre-initialization
        self.trunk_mid_output_tensor_set: Set[Tensor] = set()
        self.trunk_output_tensors: Set[Tensor] = set()
        self.endpoint_output_tensor_set: Set[Tensor] = set()

        self.max_type: Optional[str] = None
        self.min_type: Optional[str] = None
        self.coef: Optional[int] = 1
        self.max_type_of_input_tensors: Optional[int] = None
        self.tensors_before_reduce: Optional[List[Tensor]] = []
        self.tensors_after_reduce: Optional[List[Tensor]] = []
        self.hook_tensor: Optional[Tensor] = None
        self.need_set_value_action_set: List[Action] = None

        self.branch_output_tensors: Set[Tensor] = set()
        self.branch_mid_output_tensor_set: Set[Tensor] = set()
        self.branch_tensor_hook_map: Optional[Dict[Tensor, Set[Tensor]]] = {}
        self.branch_tensor_set: Set[Tensor] = set()
        self.schedule_tvm_out_set: Set[Tensor] = set()
        self.reduce_parents_list: Optional[List[Tensor]] = None

        self._collect_info(output_tensors)
        # Initialize non-hookable info
        self._get_mid_tensor_sets(output_tensors)
        self._get_set_value_tensor(output_tensors)
        self._fake_node()
        self._init_max_ub_count()
        self._init_r_parents()

    @staticmethod
    def get_maximum_subgraph(graph_info, reduce_info, tiling_case):
        """
        get maximum subgraph
        """
        reduce_tensors = list(graph_info.reduce_tensor_set)
        coefficients = graph_info.coef
        reduce_type = reduce_tensors[0].dtype
        sch_type = tiling_case.reduce_sch_type
        soc_resverd_ub_size_for_sch_type = graph_info.soc_reserved_ub_size_map[sch_type]

        def _calc_value(_graph):
            value = 0
            for _item in _graph.get("_sNodeNum"):
                value += DTYPE_BYTE_MAPPING.get(_item)
            for _item in _graph.get("_bNodeNum"):
                value += coefficients * DTYPE_BYTE_MAPPING.get(_item)

            if _graph.get("SubGraphBeforeReduce"):
                # compute_at reduce cause value++
                if not _graph.get("_sNodeNum"):
                    value += coefficients * DTYPE_BYTE_MAPPING.get(reduce_type)
                # rfactor cause value++

                cond0 = reduce_info.is_reduce_last_axis()
                cond1 = reduce_info.all_axes[tiling_case.ub_split_axis_index] in reduce_info.reduce_axes
                if cond0 and cond1:
                    # ac_tensor is rf_tensor
                    value += coefficients * DTYPE_BYTE_MAPPING.get(reduce_type)
            return value

        def _calc_small_ubsize(_node_list):
            soc_ub_size = (graph_info.soc_ub_size - soc_resverd_ub_size_for_sch_type if not tiling_case.db else
                           (graph_info.soc_ub_size - soc_resverd_ub_size_for_sch_type) // 2)
            max_value = _calc_value(_node_list[0])
            for item in _node_list:
                item_value = _calc_value(item)
                if max_value < item_value:
                    max_value = item_value

            for parent_i in graph_info.reduce_parents_list:
                # Reserve space for input's "db"
                max_value += DTYPE_BYTE_MAPPING.get(parent_i.dtype) * coefficients

            # [Common Reduce] avoid bank conflict in malloc ub
            soc_ub_size -= 1024
            small_ub_size = soc_ub_size // max_value // 128 * 128
            return small_ub_size

        def _get_align_pow_2(value: int):
            # align to 2^k
            # less 4096; eg: 8191->4096; 8193->8192
            if value <= 4096:
                return value
            n = value
            n = n | n >> 1  # 1 bit
            n = n | n >> 2  # 2 bit
            n = n | n >> 4  # 4 bit
            n = n | n >> 8  # 8 bit
            n = n | n >> 16 # 16 bit
            return (n + 1) >> 1
 
        small_ub_size_res = _calc_small_ubsize(graph_info.coexisting_quantities_map[sch_type])
        if tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION or \
            tiling_case.reduce_sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
            small_ub_size_res = _get_align_pow_2(int(small_ub_size_res))
 
        tiling_case.tensor_ub_size_before_reduce = int(small_ub_size_res * coefficients)
        tiling_case.tensor_ub_size_after_reduce = int(small_ub_size_res)

    @staticmethod
    def set_map_deepcopy(_map: Dict[Tensor, Set[Tensor]]) -> Dict[Tensor, Set[Tensor]]:
        """
        deep copy tensor map
        """
        return {key: _map[key].copy() for key in _map}

    @staticmethod
    def _eq_tvm_shape(shape_a: List, shape_b: List):
        length_a = len(shape_a)
        length_b = len(shape_b)
        if length_a != length_b:
            return False
        for idx, _ in enumerate(range(length_a)):
            ret_value = hasattr(shape_a[idx], "value") and hasattr(shape_b[idx], "value")
            ret_name = hasattr(shape_a[idx], "name") and hasattr(shape_b[idx], "name")
            if ret_value:
                if shape_a[idx].value != shape_b[idx].value:
                    return False
            elif ret_name:
                if shape_a[idx].name != shape_b[idx].name:
                    return False
            else:
                if shape_a[idx] != shape_b[idx]:
                    return False
        return True

    @staticmethod
    def dfs_compute_graph(root_tensor: Union[Iterable[Tensor], Tensor],
                          hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                             Callable[[Tensor], Any],
                                             Callable[[Tensor], Any]], ...]):
        """
        compute graph using dfs algorithm
        """

        def recursive_func(_root_tensor: Tensor,
                           _visited_list: Set[Tensor],
                           _tensor_consumers_map: Dict[Tensor, Union[Set[Tensor]]],
                           _tensor_producers_map: Dict[Tensor, Union[Set[Tensor]]],
                           _hooks: Tuple[Tuple[Callable[[Tensor], bool],
                                               Callable[[Tensor], Any],
                                               Callable[[Tensor], Any]], ...]):
            _visited_list.add(_root_tensor)
            _tensor_producers_map.setdefault(_root_tensor, set())
            _tensor_consumers_map.setdefault(_root_tensor, set())
            for hook in hooks:
                if hook[0](_root_tensor):
                    hook[1](_root_tensor)
                else:
                    hook[2](_root_tensor)
            for in_tensor in _root_tensor.op.input_tensors:
                _tensor_consumers_map.setdefault(in_tensor, set())
                _tensor_consumers_map[in_tensor].add(_root_tensor)
                _tensor_producers_map[_root_tensor].add(in_tensor)
                recursive_func(in_tensor,
                               _visited_list,
                               _tensor_consumers_map,
                               _tensor_producers_map,
                               _hooks)

        visited_list = set()
        tensor_consumers_map = {}
        tensor_producers_map = {}
        if isinstance(root_tensor, (list, tuple, set)):
            for tensor in root_tensor:
                recursive_func(tensor, visited_list,
                               tensor_consumers_map,
                               tensor_producers_map,
                               hooks)
        elif isinstance(root_tensor, Tensor):
            recursive_func(root_tensor, visited_list,
                           tensor_consumers_map, tensor_producers_map,
                           hooks)
        else:
            raise RuntimeError("dfs_compute_graph() supports list, tuple, Tensor only. Received %s"
                               % str(type(root_tensor)))
        return list(visited_list), tensor_consumers_map, tensor_producers_map

    def _init(self):
        self.tensor_consumers_map.clear()
        self.tensor_producers_map.clear()
        self.tensor_list.clear()
        self.reduce_tensor_set.clear()
        self.broadcast_tensor_set.clear()
        self.elewise_tensor_set.clear()
        self.input_tensor_set.clear()
        self.trunk_tensor_list.clear()
        self.trunk_mid_output_tensor_set.clear()
        self.trunk_output_tensors.clear()
        self.endpoint_output_tensor_set.clear()
        self.tensors_before_reduce.clear()
        self.tensors_after_reduce.clear()
        self.hook_tensor = None
        self.branch_output_tensors.clear()
        self.branch_mid_output_tensor_set.clear()
        self.branch_tensor_hook_map.clear()
        self.branch_tensor_set.clear()
        self.schedule_tvm_out_set.clear()

    def _collect_branch_consumer_map_info(self):
        def _dfs_compute(tensor: tvm.Tensor, branch_tensor_set):
            if tensor in self.trunk_tensor_list:
                self.hook_tensor = tensor
                return
            branch_tensor_set.add(tensor)
            for tensor_i in tensor.op.input_tensors:
                _dfs_compute(tensor_i, branch_tensor_set)

        for out in self.branch_output_tensors:
            # use set to eliminate duplicate node
            branch_tensors = set()
            # init hook tensor
            self.hook_tensor = None
            _dfs_compute(out, branch_tensors)
            if self.hook_tensor is None:
                _raise_error("Output before reduce should has a hook tensor on trunk.")
            self.branch_tensor_set = self.branch_tensor_set.union(branch_tensors)
            # to unify all tensors on branch with the same hook tensor
            tmp_branch_tensor_set = self.branch_tensor_hook_map.get(self.hook_tensor)
            if tmp_branch_tensor_set is not None:
                branch_tensors = branch_tensors.union(tmp_branch_tensor_set)
            self.branch_tensor_hook_map[self.hook_tensor] = branch_tensors

    def _collect_trunk_info(self, output_tensors):
        def _dfs_compute(tensor: tvm.Tensor, tensor_map):
            insn = get_dsl_insn(tensor)
            if insn in REDUCE_COMPUTE:
                return True
            for tensor_i in tensor_map[tensor]:
                if _dfs_compute(tensor_i, tensor_map):
                    return True
            return False

        for out in output_tensors:
            ret = _dfs_compute(out, self.tensor_producers_map) or _dfs_compute(out, self.tensor_consumers_map)
            if ret:
                self.trunk_output_tensors.add(out)
            else:
                self.branch_output_tensors.add(out)
        self.trunk_tensor_list, _, _ = self.dfs_compute_graph(self.trunk_output_tensors, [])
        self._get_endpoint_output_tensor_set()
        self._get_trunk_mid_output_tensor_set()

    def _get_trunk_mid_output_tensor_set(self):
        for tensor in self.trunk_output_tensors:
            if self.tensor_consumers_map.get(tensor):
                self.trunk_mid_output_tensor_set.add(tensor)

    def _get_set_value_tensor(self, output_tensors):
        current_compute = operation.get_context().get_current_compute()
        if current_compute.get("_is_fractal_format"):
            self.need_set_value_action_set = calc_padding(output_tensors)

    def _collect_info(self, output_tensors: Iterable[Tensor]):
        self.tensor_list, self.tensor_consumers_map, self.tensor_producers_map = \
            self.dfs_compute_graph(output_tensors,
                                   (  # self.input_tensor_set hook
                                       (lambda _tensor: isinstance(_tensor.op, PlaceholderOp),
                                        lambda _tensor: self.input_tensor_set.add(_tensor),
                                        lambda _tensor: None),
                                       # self.reduce_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find("reduce") != -1,
                                        lambda _tensor: self.reduce_tensor_set.add(_tensor),
                                        lambda _tensor: None),
                                       # self.broadcast_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find("broadcast") != -1,
                                        lambda _tensor: self.broadcast_tensor_set.add(_tensor),
                                        lambda _tensor: None),
                                       # self.elewise_tensor_set hook
                                       (lambda _tensor: _tensor.op.tag.find("elewise") != -1,
                                        lambda _tensor: self.elewise_tensor_set.add(_tensor),
                                        lambda _tensor: None)
                                   ))
        # endpoint_output_tensor_set
        self._collect_trunk_info(output_tensors)
        self._collect_branch_info()
        self._get_all_tensors_before_reduce()
        self._get_max_type_of_input_tensors()
        self.schedule_tvm_out_set = self.endpoint_output_tensor_set | \
                                    self.branch_output_tensors - self.branch_mid_output_tensor_set

    def _get_max_type_of_input_tensors(self):
        for input_tensor in self.input_tensor_set:
            if input_tensor not in self.tensors_before_reduce:
                continue
            input_tensor_type = int(DTYPE_BYTE_MAPPING.get(input_tensor.dtype))
            if self.max_type_of_input_tensors is None:
                self.max_type_of_input_tensors = input_tensor_type
                continue
            if input_tensor_type > self.max_type_of_input_tensors:
                self.max_type_of_input_tensors = input_tensor_type

    def _collect_branch_info(self):
        self._collect_branch_consumer_map_info()
        self._collect_branch_mid_output_set()

    def _collect_branch_mid_output_set(self):
        for tensor in self.branch_output_tensors:
            if self.tensor_consumers_map.get(tensor):
                self.branch_mid_output_tensor_set.add(tensor)

    def _fake_node(self):
        # after _collect_info(outer_most), middle_output_tensors has been assured.
        # middle_output_tensors: connect to next node in graph,
        # fake_node doesn't need them as producers for it

        def _fake_node_compute(tensors):
            # fake_node must be the biggest node (type and shape)
            dtype = tensors[0].dtype
            dim_length = max(len(t.shape) for t in tensors)
            shape = [1] * dim_length

            # update fake_node's shape and dtype
            for tensor_i in tensors:
                if DTYPE_BYTE_MAPPING.get(tensor_i.dtype) > DTYPE_BYTE_MAPPING.get(dtype):
                    dtype = tensor_i.type
                shape_i = shape_to_list(tensor_i.shape)
                diff = dim_length - len(shape_i)
                shape_i = [1] * diff + shape_i
                for j in range(diff, dim_length):
                    if equals_one(shape[j]):
                        shape[j] = shape_i[j]
                    elif not expr_equal(shape[j], shape_i[j]) and not equals_one(shape_i[j]):
                        # ReduceSch couldn't in the branch, while ReduceSch not support broadcast
                        shape[j] = tvm.max(shape_i[j], shape[j])

            def _compute(*indexes):
                res_ = tvm.const(1, dtype)
                for tensor in tensors:
                    cur_indexes = []
                    for idx, dim in enumerate(tensor.shape):
                        if equals_one(dim):
                            cur_indexes.append(0)
                        else:
                            cur_indexes.append(indexes[idx])
                    res_ *= tvm.expr.Cast(dtype, tensor(*cur_indexes))
                return res_

            with tvm.tag_scope(FAKE_NODE_TAG):
                res = tvm.compute(shape, _compute, name="fake_node")

            return res

        self.real_output_tensor_set = self.output_tensor_set
        trunk_pure_out_tensors = list(self.real_output_tensor_set -
                                      self.trunk_mid_output_tensor_set -
                                      self.branch_output_tensors)
        if len(trunk_pure_out_tensors) > 1:
            _out = _fake_node_compute(trunk_pure_out_tensors)
            _out_set = set()
            _out_set.add(_out)
            _out_set.update(self.branch_output_tensors)
            # update info while last_node is fake_node
            self._init()
            self._collect_info(list(_out_set))

    def _get_endpoint_output_tensor_set(self):
        """
        get pure endpoint output tensors without middle output tensors
        """
        for trunk_output_tensor in self.trunk_output_tensors:
            if not self.tensor_consumers_map.get(trunk_output_tensor):
                self.endpoint_output_tensor_set.add(trunk_output_tensor)

    def _get_mid_tensor_sets(self, output_tensor_set):
        """
        get  middle tensors(with middle output tensors) and middle output tensors
        """
        # mid_output_tensor_set
        # mid_tensor_set
        for tensor in self.tensor_list:
            if tensor in output_tensor_set and self.tensor_consumers_map.get(tensor) \
                    or tensor not in self.output_tensor_set | self.input_tensor_set:
                self.mid_tensor_set.add(tensor)

    def _get_all_producer_tensors(self, tensor):
        """
        get all produces stages for current tensor
        """
        producers = set()
        for producer in self.tensor_producers_map.get(tensor):
            producers.add(producer)
            producers.update(self._get_all_producer_tensors(producer))
        return producers

    def _get_all_tensors_before_reduce(self):
        """
        Strategy assumes:
        1. the graph not exist "broadcast".
        2. only support single reduce node.
        """
        reduce_tensor = list(self.reduce_tensor_set)[0]
        self.max_type = self.tensor_list[0].dtype
        self.min_type = self.tensor_list[0].dtype

        for item in self.tensor_list:
            if DTYPE_BYTE_MAPPING.get(item.dtype) > DTYPE_BYTE_MAPPING.get(self.max_type):
                self.max_type = item.dtype
            elif DTYPE_BYTE_MAPPING.get(item.dtype) < DTYPE_BYTE_MAPPING.get(self.min_type):
                self.min_type = item.dtype

        trunk_tensor_before_reduce = self._get_all_producer_tensors(reduce_tensor)
        self.tensors_before_reduce = trunk_tensor_before_reduce.union(self.branch_tensor_set)
        all_trunk_tensors = self._get_all_producer_tensors(list(self.endpoint_output_tensor_set)[0])
        self.tensors_after_reduce = all_trunk_tensors - trunk_tensor_before_reduce

        if not self.tensors_after_reduce:
            # pure data_move pattern
            self.tensors_after_reduce = self.tensors_before_reduce

    def _init_r_parents(self):
        def _r_parent(_child, _parent):
            if not self.tensor_producers_map.get(_child):
                # reduce_i isn't connect to input directly
                if reduce_i not in self.tensor_consumers_map.get(_child):
                    _parent.append(_child)
                return
            for _tensor in self.tensor_producers_map.get(_child):
                _r_parent(_tensor, _parent)

        reduce_tensors = list(self.reduce_tensor_set)
        tmp_reduce_parents = []
        for reduce_i in reduce_tensors:
            _r_parent(reduce_i, tmp_reduce_parents)
        self.reduce_parents_list = list(set(tmp_reduce_parents))

    def _init_max_ub_count(self):
        """
        Func: Get the biggest value in node_exist_list that from different sub_graph.
              tensors_before_reduce belong to bNode, tensors_after_reduce belong to sNode.
              sizeof(bNode) = coef * sizeof(sNode)
        """

        def _analysis_dependent(dependent):
            _dict = {"SubGraphBeforeReduce": False, "_bNodeNum": [], "_sNodeNum": []}
            for item in dependent.keys():
                if item in self.tensors_before_reduce:
                    _dict.get("_bNodeNum").append(item.dtype)
                    _dict["SubGraphBeforeReduce"] = True
                elif item in self.tensors_after_reduce:
                    _dict.get("_sNodeNum").append(item.dtype)
                else:
                    raise RuntimeError("tensor_i is not in discrimination.")
            return _dict

        def _current_compute_need_space(_tensor, _dict, sch_type=ReduceSchType.NORMAL):
            tag = _tensor.op.tag
            dtype = _tensor.dtype

            def get_dtypes_support_vcross_func():
                """
                get_dtypes_support_vcross_func

                the func get all dtypes that reduce_max and reduce_min supports vcross func (vcmax or vcmin)
                by soc type
                """
                soc_ver = get_soc_spec(SHORT_SOC_VERSION)
                if soc_ver in (HI3796CV300CS, HI3796CV300ES, SD3403):
                    soc_ver = ASCEND_SHISI
                soc_support_dtype = REDUCE_MAX_MIN_SUPPORT_VCROSSFUNC.get(soc_ver)
                if soc_support_dtype is None:
                    soc_support_dtype = REDUCE_MAX_MIN_SUPPORT_VCROSSFUNC.get("Default")
                    if soc_support_dtype is None:
                        return []

                return list(soc_support_dtype)

            def _last_reduce():
                """
                get last reduce axis
                """
                return all_axes[-1] in reduce_axes

            def _reduce_sum_space(_reduce_tensor):
                _dict.get("_sNodeNum").append(dtype)
                if dtype == "int64":
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    return
                if not _last_reduce():
                    self.soc_reserved_ub_size_map[sch_type] = 64
                else:
                    if len(reduce_axes) == 1:
                        self.soc_reserved_ub_size_map[sch_type] = 4096
                    else:
                        self.soc_reserved_ub_size_map[sch_type] = 2048

            def _reduce_max_min_space():
                _dict.get("_sNodeNum").append(dtype)
                if dtype == "int64":
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    _dict.get("_bNodeNum").append(dtype)
                    return
                if not _last_reduce():
                    self.soc_reserved_ub_size_map[sch_type] = 64
                else:
                    dtypes_support_vcross_func = get_dtypes_support_vcross_func()
                    if dtype in dtypes_support_vcross_func:
                        _dict["_bNodeNum"].append(dtype)
                        self.soc_reserved_ub_size_map[sch_type] = 4096
                    else:
                        self.soc_reserved_ub_size_map[sch_type] = 512

            def _reduce_prod_space(_reduce_tensor):
                if all_axes[-1] not in reduce_axes:
                    _dict["_sNodeNum"].append(dtype)
                    self.soc_reserved_ub_size_map[sch_type] = 64
                else:
                    _dict["_sNodeNum"].append(dtype)
                    self.soc_reserved_ub_size_map[sch_type] = 512

            def _dst_no_reuse_src(_tensor_n):
                if sch_type.value >= ReduceSchType.AR_HIGH_PRECISION.value and \
                    sch_type.value <= ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM_WORKSPACE.value:
                    return is_vcmp_insn(_tensor_n) or is_vsel_insn(_tensor_n) or is_vcmpsel_insn(_tensor_n)
                return False

            if tag.find("reduce") != -1:
                reduce_axes = get_reduce_axes(_tensor)
                all_axes = get_reduce_all_axes(_tensor)
                if tag in ["reduce_sum", ]:
                    _reduce_sum_space(_tensor)
                elif tag in ["reduce_max", "reduce_min"]:
                    _reduce_max_min_space()
                elif tag in ["reduce_prod", "reduce_all", "reduce_any"]:
                    _reduce_prod_space(_tensor)
                else:
                    raise RuntimeError("Unknown reduce_insn is %s" % tag)

                # add two live tensor for transpose
                if sch_type == ReduceSchType.TRANSPOSE:
                    _dict["_bNodeNum"].append(dtype)
                    _dict["_bNodeNum"].append(dtype)

                if sch_type == ReduceSchType.ENTIRE_REDUCE:
                    # one live tensor for dst_buf
                    _dict["_bNodeNum"].append(dtype)
                    # one live tensor for scalar_ub
                    _dict["_bNodeNum"].append(dtype)
                    # only support _last_reduce use vc instrinsic
                    op_tag_intrinsic = OP_TAG_VC_INSTRINSIC_MAP.get(tag)
                    # need one live tensor for scalar_buf if not support vc instrinsic
                    if op_tag_intrinsic is None:
                        _dict["_bNodeNum"].append(dtype)
                    else:
                        if not intrinsic_check_support(op_tag_intrinsic, dtype):
                            _dict["_bNodeNum"].append(dtype)

                if sch_type == ReduceSchType.AR_HIGH_PRECISION or \
                    sch_type == ReduceSchType.AR_HIGH_PRECISION_WORKSPACE:
                    # one live tensor for dst_buf
                    _dict["_bNodeNum"].append(dtype)
                    # one live tensor for R.o->A.o rfactor (get_maximum_subgraph reserves one tensor)
 
                if sch_type == ReduceSchType.ACCUMULATION or sch_type == ReduceSchType.PAD_ACCUMULATION:
                    _dict["_bNodeNum"].append(dtype)

                if sch_type.value >= ReduceSchType.ARA_HIGH_PRECISION.value and \
                    sch_type.value <= ReduceSchType.ARA_HIGH_PRECISION_BIG_DIM_WORKSPACE.value:
                    # 二分累加需要预留一个临时空间
                    _dict["_bNodeNum"].append(dtype)
                    # "ub间二分" 临时空间
                    _dict["_bNodeNum"].append(dtype)

            else:
                if _tensor in self.tensors_before_reduce:
                    _dict["_bNodeNum"].append(dtype)
                    _dict["SubGraphBeforeReduce"] = True
                    if need_extent_node(_tensor) or _dst_no_reuse_src(_tensor):
                        _dict["_bNodeNum"].append(dtype)
                else:
                    _dict["_sNodeNum"].append(dtype)
                    if need_extent_node(_tensor) or _dst_no_reuse_src(_tensor):
                        _dict["_sNodeNum"].append(dtype)

            if sch_type == ReduceSchType.PAD or sch_type == ReduceSchType.PAD_ACCUMULATION:
                if _tensor in self.input_tensor_set:
                    _dict["_bNodeNum"].append(dtype)
                    _dict["_bNodeNum"].append(dtype)
                    _dict["_bNodeNum"].append(dtype)
                if _tensor in self.output_tensor_set:
                    # the space that remove pad buffer need
                    size = ALIGN_AND_REMOVE_PAD_EXTRA_NODES - len(self.tensor_producers_map.get(_tensor))
                    if size > 0:
                        for i in range(0, size):
                            _dict["_sNodeNum"].append(dtype)

            def _calc_cache_read_set_value(_dict):
                current_compute = operation.get_context().get_current_compute()
                if current_compute.get("_is_fractal_format"):
                    for action in self.need_set_value_action_set:
                        if _tensor == action.get_tensor() and action.get_action_type == \
                                ActionType.CACHE_READ_AND_SET_VALUE:
                            if _tensor in self.tensors_before_reduce:
                                _dict["_bNodeNum"].append(dtype)
                            else:
                                _dict["_sNodeNum"].append(dtype)

            _calc_cache_read_set_value(_dict)

        def _refresh_dependent(_tensor):
            for _tensor_i in _tensor.op.input_tensors:
                if _tensor_i not in dependent_map:
                    continue
                dependent_map.get(_tensor_i).remove(_tensor)
                if not dependent_map[_tensor_i]:
                    dependent_map.pop(_tensor_i)

        def _r_coexisting(_tensor, _need_space, sch_type):
            if _tensor in dependent_map:
                _need_space.append(_analysis_dependent(dependent_map))
                return

            for _tensor_i in _tensor.op.input_tensors:
                _r_coexisting(_tensor_i, _need_space, sch_type)

            _curr_dict = _analysis_dependent(dependent_map)
            _current_compute_need_space(_tensor, _curr_dict, sch_type)
            _need_space.append(_curr_dict)

            _refresh_dependent(_tensor)
            if _tensor not in dependent_map:
                dependent_map[_tensor] = self.tensor_consumers_map.get(_tensor).copy()

        # [Common Reduce] Find maximum sub_graph
        if not operation.get_context().get("_placeholder_before_reduce") and \
                not operation.get_context().get("_placeholder_after_reduce"):
            placeholder_before_reduce = [x for x in self.tensors_before_reduce if is_placeholder(x)]
            operation.get_context().add("_placeholder_before_reduce", placeholder_before_reduce)
            placeholder_after_reduce = [x for x in self.tensors_after_reduce if is_placeholder(x)]
            operation.get_context().add("_placeholder_after_reduce", placeholder_after_reduce)

        _out_set = self.endpoint_output_tensor_set.union(self.branch_output_tensors - self.branch_mid_output_tensor_set)

        for _out in _out_set:
            for sch_type, sch_variable in self.coexisting_quantities_map.items():
                dependent_map = {}
                for tensor_i in _out.op.input_tensors:
                    _r_coexisting(tensor_i, sch_variable, sch_type)

                # [Common Reduce] Multi outputs
                if not _out.op.tag == FAKE_NODE_TAG:
                    curr_dict = _analysis_dependent(dependent_map)
                    _current_compute_need_space(_out, curr_dict, sch_type)
                    sch_variable.append(curr_dict)

        min_input_type = "int64"
        for item in self.input_tensor_set:
            if DTYPE_BYTE_MAPPING.get(item.dtype) < DTYPE_BYTE_MAPPING.get(min_input_type):
                min_input_type = item.dtype
        self.pad_max_entire_size = get_pad_entire_size(min_input_type)


def get_pad_entire_size(dtype):
    c0_size = get_block_size() // 2
    dtype_and_pad_entire_size_map = {
    "bool": BLOCK * c0_size * 4,
    "int8": BLOCK * c0_size * 4,
    "uint8": BLOCK * c0_size * 4,
    "float16": BLOCK * c0_size,
    "bfloat16": BLOCK * c0_size,
    "float32": BLOCK * c0_size // 2,
    "int32": BLOCK * c0_size // 2,
    "int64": BLOCK * c0_size // 4,
    }
    if dtype not in dtype_and_pad_entire_size_map:
        _raise_error("[%s] is not support type in reduce." % dtype)

    return dtype_and_pad_entire_size_map.get(dtype)


def _raise_error(message):
    dict_args = {"errCode": "E90003", "detailed_cause": message}
    raise RuntimeError(dict_args, get_error_message(dict_args))
