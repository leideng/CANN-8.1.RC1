#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2023-2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
schedule base
"""
import dataclasses
from typing import Iterable
from typing import List
from typing import Union

from tbe import tvm
from tbe.tvm import Tensor
from tbe.tvm import Schedule
from tbe.tvm.tir import IterVar

from . import constants
from . import helper
from .info import buffer_size_info
from .info import graph_info
from .info import schedule_node_info
from .info import tensor_info


class ScheduleBase:
    def __init__(self, graph_info_obj, outs):
        self._outs: Union[Iterable[Tensor], Tensor] = outs
        self._graph_info: graph_info.GraphInfo = graph_info_obj
        self._sch: Schedule = tvm.create_schedule([self._graph_info.res_tensor_obj.tvm_tensor.op])
        self._sch_node_obj_list: List[schedule_node_info.ScheduleNode] = []
        self._collect_info()
        self._res_sch_node_obj: schedule_node_info.ScheduleNode = \
            self._get_sch_node_obj(tensor_obj=graph_info_obj.res_tensor_obj)
        self._buffer_size_info_obj: buffer_size_info.BufferSizeInfo = None

    def unify_cache_read(self, sch_node_obj, scope, reader_sch_node_objs):
        cache_read_tensor = self._sch.cache_read(sch_node_obj.get_tvm_tensor(), scope,
                                                 [n.get_tvm_tensor() for n in reader_sch_node_objs])
        cache_read_tensor_obj = tensor_info.gen_tensor_obj(cache_read_tensor, constants.NodeType.DATA_MOVE)
        cache_read_sch_node_obj = schedule_node_info.ScheduleNode(self._sch, cache_read_tensor_obj)
        self._append_sch_node_obj(sch_node_obj, cache_read_sch_node_obj, True)
        cache_read_sch_node_obj.stage_obj.flags.cache_read_flag = True

        return cache_read_sch_node_obj

    def unify_cache_write(self, sch_node_obj, scope):
        cache_write_tensor = self._sch.cache_write(sch_node_obj.get_tvm_tensor(), scope)
        cache_write_tensor_obj = \
            tensor_info.gen_tensor_obj(cache_write_tensor, tensor_info.get_tensor_type(sch_node_obj.get_tvm_tensor()))
        cache_write_sch_node_obj = schedule_node_info.ScheduleNode(self._sch, cache_write_tensor_obj)
        self._append_sch_node_obj(sch_node_obj, cache_write_sch_node_obj, False)
        cache_write_sch_node_obj.stage_obj.flags.cache_write_flag = True
        # update sch_node_obj
        self._copy_and_replace_sch_node_obj(sch_node_obj, constants.NodeType.DATA_MOVE)

        return cache_write_sch_node_obj

    def unify_rfactor(self, sch_node_obj, reduce_axes_list, axes_indices, is_overlap_mode=True):
        helper.check_true(isinstance(axes_indices, int),
                          "unify_rfactor only support one reduce axis rfactor currently.")
        mode = self._sch.RfactorModeOverlap if is_overlap_mode else self._sch.RfactorModeNormal
        reduce_rf_tensor = self._sch.rfactor(sch_node_obj.get_tvm_tensor(), reduce_axes_list, axes_indices, mode)
        reduce_rf_tensor_obj = tensor_info.gen_tensor_obj(reduce_rf_tensor, constants.NodeType.REDUCE)
        reduce_rf_sch_node_obj = schedule_node_info.ScheduleNode(self._sch, reduce_rf_tensor_obj)
        self._append_sch_node_obj(sch_node_obj, reduce_rf_sch_node_obj, False)
        reduce_rf_sch_node_obj.stage_obj.flags.rfactor_flag = True
        reduce_rf_sch_node_obj.stage_obj.unify_set_scope(sch_node_obj.get_tvm_stage().scope)
        # update sch_node_obj
        new_sch_node_obj = self._copy_and_replace_sch_node_obj(sch_node_obj, constants.NodeType.REDUCE)
        # keep_dims attr of rfactor stage obj and new stage obj must be same as ori stage obj's
        ori_keep_dims = sch_node_obj.stage_obj.keep_dims
        reduce_rf_sch_node_obj.stage_obj.keep_dims = ori_keep_dims
        new_sch_node_obj.stage_obj.keep_dims = ori_keep_dims
        # insn name of rfactor obj must be same as ori obj's
        ori_insn_name = sch_node_obj.insn_obj.insn_name
        reduce_rf_sch_node_obj.insn_obj.insn_name = ori_insn_name
        new_sch_node_obj.insn_obj.insn_name = ori_insn_name
        # refine actual shape of rfactor tensor
        # A 1 A 1 Ar -> A 1 A Ar
        if ori_keep_dims:
            if axes_indices < 0:
                axes_indices += len(reduce_rf_sch_node_obj.tensor_obj.actual_shape)
            reduce_rf_sch_node_obj.tensor_obj.actual_shape.pop(axes_indices - 1)

        return reduce_rf_sch_node_obj

    def post_unify_compute_inline(self, compute_inlined_sch_node_obj):
        for consumer_sch_node_obj in compute_inlined_sch_node_obj.consumers:
            consumer_sch_node_obj.stage_obj.compute_inlined_stage_obj = compute_inlined_sch_node_obj.stage_obj
        self._delete_sch_node_obj(compute_inlined_sch_node_obj)

    def _get_sch_node_obj(self, tensor_obj=None, stage_obj=None):
        for sch_node in self._sch_node_obj_list:
            if tensor_obj is not None and sch_node.tensor_obj == tensor_obj:
                return sch_node
            if stage_obj is not None and sch_node.stage_obj == stage_obj:
                return sch_node

        return None

    def _collect_info(self):
        self._sch_node_obj_list = \
            [schedule_node_info.ScheduleNode(self._sch, t) for t in self._graph_info.tensor_obj_list]

        for sch_node_obj in self._sch_node_obj_list:
            tensor_obj = sch_node_obj.tensor_obj
            consumer_tensor_objs = self._graph_info.consumer_tensor_objs_map.get(tensor_obj)
            sch_node_obj.consumers = [self._get_sch_node_obj(tensor_obj=t) for t in consumer_tensor_objs]
            producer_tensor_obj = self._graph_info.producer_tensor_objs_map.get(tensor_obj)
            sch_node_obj.producers = [self._get_sch_node_obj(tensor_obj=t) for t in producer_tensor_obj]

    def _append_sch_node_obj(self, ori_sch_node_obj, new_sch_node_obj, is_forward):
        self._sch_node_obj_list.append(new_sch_node_obj)
        if not is_forward:
            for producer_sch_node_obj in ori_sch_node_obj.producers[:]:
                ori_sch_node_obj_index = producer_sch_node_obj.consumers.index(ori_sch_node_obj)
                producer_sch_node_obj.consumers[ori_sch_node_obj_index] = new_sch_node_obj
                ori_sch_node_obj.producers.remove(producer_sch_node_obj)
                helper.ListHelper.add(new_sch_node_obj.producers, producer_sch_node_obj)
            helper.ListHelper.add(new_sch_node_obj.consumers, ori_sch_node_obj)
            helper.ListHelper.add(ori_sch_node_obj.producers, new_sch_node_obj)
        else:
            for consumer_sch_node_obj in ori_sch_node_obj.consumers[:]:
                ori_sch_node_obj_index = consumer_sch_node_obj.producers.index(ori_sch_node_obj)
                consumer_sch_node_obj.producers[ori_sch_node_obj_index] = new_sch_node_obj
                ori_sch_node_obj.consumers.remove(consumer_sch_node_obj)
                helper.ListHelper.add(new_sch_node_obj.consumers, consumer_sch_node_obj)
            helper.ListHelper.add(ori_sch_node_obj.consumers, new_sch_node_obj)
            helper.ListHelper.add(new_sch_node_obj.producers, ori_sch_node_obj)

    def _delete_sch_node_obj(self, ori_sch_node_obj):
        if ori_sch_node_obj in self._sch_node_obj_list:
            self._sch_node_obj_list.remove(ori_sch_node_obj)

        producer_sch_node_objs = ori_sch_node_obj.producers
        ori_indices_in_producer_consumer = []
        consumer_sch_node_objs = ori_sch_node_obj.consumers
        ori_indices_in_consumer_producer = []

        for producer_sch_node_obj in producer_sch_node_objs:
            ori_sch_node_obj_index = producer_sch_node_obj.consumers.index(ori_sch_node_obj)
            ori_indices_in_producer_consumer.append(ori_sch_node_obj_index)
            producer_sch_node_obj.consumers.remove(ori_sch_node_obj)

        for consumer_sch_node_obj in consumer_sch_node_objs:
            ori_sch_node_obj_index = consumer_sch_node_obj.producers.index(ori_sch_node_obj)
            ori_indices_in_consumer_producer.append(ori_sch_node_obj_index)
            consumer_sch_node_obj.producers.remove(ori_sch_node_obj)

        for producer_index, producer_sch_node_obj in enumerate(producer_sch_node_objs):
            for consumer_index, consumer_sch_node_obj in enumerate(consumer_sch_node_objs):
                helper.ListHelper.add(consumer_sch_node_obj.producers, producer_sch_node_obj,
                                      insert_index=ori_indices_in_consumer_producer[consumer_index])
                helper.ListHelper.add(producer_sch_node_obj.consumers, consumer_sch_node_obj,
                                      insert_index=ori_indices_in_producer_consumer[producer_index])

        ori_sch_node_obj.producers = []
        ori_sch_node_obj.consumers = []

    def _copy_and_replace_sch_node_obj(self, ori_sch_node_obj, node_type):
        producer_sch_node_obj = ori_sch_node_obj.producers[0]
        self._delete_sch_node_obj(ori_sch_node_obj)
        new_tensor_obj = tensor_info.gen_tensor_obj(ori_sch_node_obj.get_tvm_tensor(), node_type)
        new_tensor_obj.copy(ori_sch_node_obj.tensor_obj)
        new_sch_node_obj = schedule_node_info.ScheduleNode(self._sch, new_tensor_obj)
        new_sch_node_obj.stage_obj.copy(ori_sch_node_obj.stage_obj)
        self._append_sch_node_obj(producer_sch_node_obj, new_sch_node_obj, True)
        # update res_sch_node_obj
        if self._res_sch_node_obj == ori_sch_node_obj:
            self._res_sch_node_obj = new_sch_node_obj

        return new_sch_node_obj


@dataclasses.dataclass
class ScheduleSplitAxes:
    block_outer: IterVar = None
    block_inner: IterVar = None
    ub_outer: IterVar = None
    ub_inner: IterVar = None
