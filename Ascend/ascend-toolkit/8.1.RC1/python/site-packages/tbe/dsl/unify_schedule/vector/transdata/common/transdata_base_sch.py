#!/usr/bin/env python.
# -*- coding:utf-8 -*-
# Copyright(C) 2022. Huawei Technologies Co., Ltd. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
transdata base schedule
"""

from tbe import tvm
from tbe.common.platform import scope_ubuf
from tbe.common.platform import get_soc_spec
from tbe.common.utils.shape_util import shape_to_list
from tbe.common.utils.errormgr import get_error_message
from tbe.common.platform.platform_info import get_soc_spec
from tbe.dsl.base import operation
from tbe.dsl.base.operation import get_context
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl.unify_schedule.schedule import Schedule

from tbe.dsl.classifier.transdata.constants import DATA_TYPE_SIZE, STRIDE_16, NO_OVERLAP
from tbe.dsl.classifier.transdata.constants import DEFAULT, B8, B16, B32, TransposeMode
from tbe.dsl.classifier.transdata.constants import BLK_STRIDE_LIMIT, SUPPORT_VOR_FORMATS
from tbe.dsl.classifier.transdata.constants import VNC_MAP, C08
from tbe.dsl.classifier.transdata.constants import get_reshape
from tbe.dsl.classifier.transdata.constants import intrinsic_check_support

from .graph.transdata_graph_info import ComputeGraphInfo
from .graph.transdata_graph_info import set_align
from .graph.transdata_graph_info import ceil_div


class Stream:
    """
    Interface for TVM Tensor To Describe AxesMapping
    """

    def __init__(self, tensor, attrs=None, tag=None):
        self.tensor = tensor
        self.shape = tensor.shape if hasattr(tensor, "shape") else None
        self.dtype = tensor.dtype if hasattr(tensor, "dtype") else None
        self.tag = tag if tag else self.tensor.op.tag
        self.attrs = attrs if attrs else self.init_attrs()

    def __repr__(self):
        return f'Stream(tensor={self.tensor})'

    def __hash__(self):
        return hash(id(self.tensor))

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return hash(id(other.tensor)) == hash(id(self.tensor))
        return False

    def init_attrs(self):
        # regulation of axes mapping for tensor and parent.
        if self.tensor.op.attrs:
            cond0 = self.tag.find("transdata|transpose") != -1
            cond1 = self.tag.find("transdata|f_reshape") != -1
            cond2 = self.tag.find("transdata|s_reshape") != -1
            if cond0:
                return [int(x) for x in self.tensor.op.attrs["permute"]]
            if cond1 or cond2:
                return get_reshape(self.tensor)

        return list(range(len(self.shape)))

    def set_tag(self, tag):
        self.tag = tag

    def set_attrs(self, attrs):
        self.attrs = attrs


class TransdataBaseSch(Schedule):
    """
    Class for transdata base schedule
    """

    def __init__(self, outs, tiling_case):
        self.outs = outs
        self.tiling_case = tiling_case
        self.graph_info = get_context().get_current_compute().get("_compute_graph_info")
        self.streams = {tensor: Stream(tensor) for tensor in self.graph_info.tensor_list}

        self.schedule = None
        self.forward_compute_graph_map = self.graph_info.tensor_consumers_map
        self.forward_stage_graph_map = ComputeGraphInfo.set_map_deepcopy(self.forward_compute_graph_map)
        self.backward_compute_graph_map = self.graph_info.tensor_producers_map
        self.backward_stage_graph_map = ComputeGraphInfo.set_map_deepcopy(self.backward_compute_graph_map)
        self.cache_read_tensors_and_buffer_map = {}
        self.cache_write_tensors_and_buffer_map = {}
        self.double_buffer_tensors = []
        self.need_multi_core = True
        self.multi_core_bind_tensor = None
        self.multi_core_fused_axis = None
        self.compute_at_map = {}
        self.emit_insn_map = {}
        self.compute_inline_list = []

        # Tensor
        self.depad_tensors = []
        self.pad_tensors = []
        self.transpose_tensors = []
        self.reshape_tensors = []
        self.tiling_tensor = None
        self.mte2_tensor = None
        self.remove_pad_tensor = None
        self.align_pad_tensors = []

        # Attr
        self.depad_axis_list = []
        self.pad_axis_list = []
        self.pad_var_list = []
        self.dtype = None
        self.split_once = None
        self.align_factor = None
        self.axis_in_ub = []
        self.axis_not_in_ub = []
        self.tiling_axes = []
        self.permutes = []

        # AxisVar
        self.iter_block_outer = None
        self.iter_block_inner = None
        self.iter_ub_first_outer = None
        self.iter_ub_first_inner = None
        self.iter_ub_second_outer = None
        self.iter_ub_second_inner = None
        self.ub_outer = None
        self.ub_inner = None
        self.reorder_list = []
        self.block = int(get_soc_spec("ubblock_size"))
        self.b8_block = self.block // B8
        self.b16_block = self.block // B16
        self.b32_block = self.block // B32
        self.b32_align_size = STRIDE_16
        if hasattr(self.graph_info, "is_last_transpose") and self.graph_info.is_last_transpose:
            key = (self.graph_info.c0, B32, self.graph_info.is_forward, intrinsic_check_support())
            if VNC_MAP.get(key, None):
                self.b32_align_size, _ = VNC_MAP.get(key)

    @classmethod
    def get_instance(cls, outs, tiling_case):
        return cls(outs, tiling_case)

    @classmethod
    def get_supported_soc(cls):
        return [DEFAULT]

    @classmethod
    def get_supported_pattern(cls):
        return [Pattern.TRANSDATA]

    @staticmethod
    def optimization_vnc(a, b, c0, tensor):
        """
        Func: avoid transpose (a,b)->(b,a) by storage-align, c0 is align_value
        Return: storage-align factor
        """
        bit = DATA_TYPE_SIZE.get(tensor.dtype, 1)
        if bit not in [B8, B16]:
            dict_args = {"errCode": "E90003", "detailed_cause": "optimization_vnc is error."}
            raise RuntimeError(dict_args, get_error_message(dict_args))

        if isinstance(a, int) and isinstance(b, int):
            src_cond = b >= a and ceil_div(b, c0) % 2 == 0
            dst_cond = a > b and ceil_div(a, c0) % 2 == 0
        else:
            src_cond = tvm.all(*[b >= a, ceil_div(b, c0) % 2 == 0])
            dst_cond = tvm.all(*[a > b, ceil_div(a, c0) % 2 == 0])

        src_tensor_var = tvm.select(src_cond, (ceil_div(b, c0) + 1), 1) * c0
        dst_tensor_var = tvm.select(dst_cond, (ceil_div(a, c0) + 1), 1) * c0
        return src_tensor_var, dst_tensor_var

    def optimization_vor(self, a, b, c0, tensor):
        """
        Func: avoid transpose(a,b,c0)->(b,a,c0) by storage-align, return align factor.
        Regulation:
        1. choose bigger as repeat
        2. hardware decide work-mode
        3. C0 is 16 and fp32 can be seem as (a,b,2,c0) -> (b,a,2,c0) fp16
        """
        bit = DATA_TYPE_SIZE.get(tensor.dtype, 1)
        row_c0, col_c0 = c0, self.block // bit
        hardware_limit = BLK_STRIDE_LIMIT * col_c0
        src_cond, dst_cond = True, True

        if isinstance(a, int):
            if b * c0 > hardware_limit:
                src_cond = False
            if a * c0 > hardware_limit:
                dst_cond = False
            src_cond = src_cond and b >= a and b % 2 == 0
            dst_cond = dst_cond and a > b and a % 2 == 0
        else:
            src_cond = tvm.all(*[b >= a, b % 2 == 0])
            dst_cond = tvm.all(*[a > b, a % 2 == 0])

        src_tensor_var = tvm.select(src_cond, (b + 1) * col_c0, row_c0)
        dst_tensor_var = tvm.select(dst_cond, a + 1, 1) * col_c0
        return src_tensor_var, dst_tensor_var

    def get_all_producers_stages(self, tensor):
        """
        get all produce stages for current tensor
        """
        producers = set()
        for producer in self.backward_stage_graph_map[tensor]:
            producers.add(producer)
            producers.update(self.get_all_producers_stages(producer))
        return producers

    def single_cache_read(self, source_tensor):
        """
        Create tensor after source-tensor
        """
        readers = self.forward_stage_graph_map[source_tensor].copy()
        dst_tensor = self.schedule.cache_read(source_tensor, scope_ubuf, readers)
        self.cache_read_tensors_and_buffer_map[source_tensor] = dst_tensor
        for item in readers:
            self.update_stage(dst_tensor, item, True)
        self.update_stream(source_tensor, dst_tensor, mode="cache_read")
        return dst_tensor

    def single_cache_transpose(self, src_tensor, s_perm, d_perm):
        """
        Create tensor for transpose
        """
        _list = [src_tensor.op.axis[i] for _, i in sorted(zip(s_perm, range(len(s_perm))))]
        self.schedule[src_tensor].reorder(*_list)
        dst_tensor = self.single_cache_write(src_tensor)
        # update streams
        self.streams.get(dst_tensor).set_attrs(d_perm)
        self.streams.get(src_tensor).set_attrs(s_perm)
        return dst_tensor

    def single_cache_write(self, source_tensor):
        """
        Create tensor before source-tensor
        """
        writers = self.backward_stage_graph_map[source_tensor].copy()
        dst_tensor = self.schedule.cache_write(source_tensor, scope_ubuf)
        self.cache_write_tensors_and_buffer_map[source_tensor] = dst_tensor
        for item in writers:
            self.update_stage(dst_tensor, item, False)
        self.update_stream(dst_tensor, source_tensor, mode="cache_write")
        return dst_tensor

    def update_stage(self, source_tensor, dst_tensor, before):
        """
        update graph stage map by new tensor
        """
        if before:
            self.forward_stage_graph_map.setdefault(source_tensor, set())
            self.backward_stage_graph_map.setdefault(source_tensor, set())
            for producer in tuple(self.backward_stage_graph_map[dst_tensor]):
                self.forward_stage_graph_map[producer].remove(dst_tensor)
                self.forward_stage_graph_map[producer].add(source_tensor)
                self.backward_stage_graph_map[dst_tensor].remove(producer)
                self.backward_stage_graph_map[source_tensor].add(producer)
            self.forward_stage_graph_map[source_tensor].add(dst_tensor)
            self.backward_stage_graph_map[dst_tensor].add(source_tensor)
        else:
            self.forward_stage_graph_map.setdefault(source_tensor, set())
            self.backward_stage_graph_map.setdefault(source_tensor, set())
            for consumer in tuple(self.forward_stage_graph_map[dst_tensor]):
                self.forward_stage_graph_map[dst_tensor].discard(consumer)
                self.backward_stage_graph_map[consumer].discard(dst_tensor)
                self.backward_stage_graph_map[consumer].add(source_tensor)
                self.forward_stage_graph_map[source_tensor].add(consumer)
            self.forward_stage_graph_map[dst_tensor].add(source_tensor)
            self.backward_stage_graph_map[source_tensor].add(dst_tensor)

    def update_stream(self, src, dst, mode="cache_read"):
        """
        Update relation while stream had been changed,
        src is parent, dst is child in stream no matter mode.
        """

        def get_tag(tensor):
            stream = self.streams.get(tensor, None)
            return stream.tag if stream else None

        def get_attrs(tensor):
            stream = self.streams.get(tensor, None)
            return stream.attrs if stream else None

        def set_attrs(tensor, attrs):
            stream = self.streams.get(tensor, None)
            if stream:
                stream.set_attrs(attrs)

        if mode == "cache_read":
            tag = get_tag(src)
            attrs = list(range(len(src.shape)))
            self.streams.update({dst: Stream(dst, attrs=attrs, tag=tag)})
            return

        if mode == "cache_write":
            tag = get_tag(dst)
            src_attrs = get_attrs(dst)
            dst_attrs = list(range(len(dst.shape)))
            set_attrs(dst, dst_attrs)
            self.streams.update({src: Stream(src, attrs=src_attrs, tag=tag)})
            return

    def child(self, src_tensor):
        """
        :param src_tensor:
        :return: direct consumer of src_tensor
        """
        return list(self.forward_stage_graph_map.get(src_tensor, None))[0]

    def parent(self, src_tensor):
        """
        :param src_tensor:
        :return: direct producer of src_tensor
        """
        return list(self.backward_stage_graph_map.get(src_tensor, None))[0]

    def parses_f_reshape(self, src_tensor):
        """
        :param src_tensor: f_reshape_tensor
        :return:
            1. Index of mapping between producer of src_tensor and src_tensor.
            2. Find indexes of x1|x0 in producer of src_tensor.
        """
        mapping_indexes, x1, x0 = [], None, None
        axes = self.get_reshape(src_tensor)
        for index, value in enumerate(axes):
            if isinstance(value, (list, tuple)) and len(value) == 2:
                mapping_indexes.extend([index, ] * 2)
                x1 = value[0]
                x0 = value[1]
            else:
                mapping_indexes.append(index)
        return mapping_indexes, x1, x0

    def parses_axis_type(self, permute, ub_internal_c=False):
        """
        Func: Classified transpose_tensor's axis into UB Internal and UB External
        """
        split_i = self.tiling_case.ub_split_first_idx
        split_o = self.tiling_case.ub_split_second_idx
        length = len(permute)

        ub_internal_input = {permute.index(x) for x in range(permute[split_i], length, 1)}
        ub_internal_output = set(range(split_o, length, 1))
        self.axis_in_ub = ub_internal_output.union(ub_internal_input)
        self.axis_not_in_ub = set(range(length)).difference(self.axis_in_ub)
        self.axis_not_in_ub = self.axis_not_in_ub.union({split_i, split_o})

        # Regulation of split maybe make c1 in ub-external, we need c1 c0 in ub-internal
        if ub_internal_c:
            for c in self.graph_info.c1c0:
                if c not in self.axis_in_ub:
                    self.axis_in_ub.add(c)
                if c in self.axis_not_in_ub:
                    self.axis_not_in_ub.remove(c)

        # Regulation: make x0 in ub-internal <Template>
        if self.graph_info.x1x0:
            if self.graph_info.x1x0[-1] not in self.axis_in_ub:
                self.axis_in_ub.add(self.graph_info.x1x0[-1])
            if self.graph_info.x1x0[-1] in self.axis_not_in_ub:
                self.axis_not_in_ub.remove(self.graph_info.x1x0[-1])

        # for reorder
        self.axis_in_ub = list(self.axis_in_ub)
        self.axis_not_in_ub = list(self.axis_not_in_ub)
        self.axis_in_ub.sort()
        self.axis_not_in_ub.sort()

    def parses_tiling_info(self, mapping_indexes, x1_index, x0_index, factor):
        """Update real tiling info based on tiling_tensor
        Func:
        1. Do extra split for tiling_tensor(split x as x1 and x0).
        2. Collect tiling_axes.
        3. Update axis_in_ub and axis_not_in_ub base on tiling_tensor.
        """
        split_b = self.tiling_case.block_split_idx
        split_i = self.tiling_case.ub_split_first_idx
        split_o = self.tiling_case.ub_split_second_idx
        is_x1_out_ub = x1_index in self.axis_not_in_ub
        is_x0_in_ub = x0_index in self.axis_in_ub
        is_split_x1 = x1_index in [split_i, split_o]
        is_do_extra_split_x = is_x1_out_ub and is_x0_in_ub and not is_split_x1

        # update factor while split x1:
        # 1. split x1 mean that do not extra split x as x1 and x0
        # 2. ub split firstly, if ub split x1, factor is factor * 16(float16)
        # 3. block split secondly, if block split x1 and ub not split x1, factor is factor * 16(float16)
        if split_i == x1_index:
            self.tiling_case.ub_first_factor *= factor
        if not self.split_once and split_o == x1_index:
            self.tiling_case.ub_second_factor *= factor
        if split_b == x1_index and split_i != x1_index and split_o != x1_index and not is_do_extra_split_x:
            self.tiling_case.block_factor *= factor

        if is_do_extra_split_x:
            # do extra split
            root_var = self.tiling_tensor.op.axis[mapping_indexes[x1_index]]
            x1_axis, x0_axis = self.schedule[self.tiling_tensor].split(root_var, factor=factor)
            for idx, value in enumerate(self.tiling_tensor.op.axis):
                if idx == mapping_indexes[x1_index]:
                    self.tiling_axes.extend([x1_axis, x0_axis])
                else:
                    self.tiling_axes.append(value)
        else:
            for value in self.tiling_tensor.op.axis:
                self.tiling_axes.append(value)

            # update axis_in_ub and axis_not_in_ub that make them based on tiling_tensor
            self.tiling_case.block_split_idx = mapping_indexes[split_b]
            self.tiling_case.ub_split_first_idx = mapping_indexes[split_i]
            self.tiling_case.ub_split_second_idx = mapping_indexes[split_o]
            if x1_index in self.axis_in_ub and x0_index in self.axis_in_ub:
                self.axis_in_ub.remove(x1_index)
            if x1_index in self.axis_not_in_ub and x0_index in self.axis_not_in_ub:
                self.axis_not_in_ub.remove(x0_index)
            self.axis_in_ub = [mapping_indexes[x] for x in self.axis_in_ub]
            self.axis_not_in_ub = [mapping_indexes[x] for x in self.axis_not_in_ub]

    def choose_transpose_insn(self, dtype, perm):
        """
        Return transpose emit insn
        """

        def transpose_pass_limit(_perm):
            # Pass not support perm is [0,2,1], but [2,1] while B32 and B64
            if self.get_block_size(dtype) in [self.b8_block, self.b16_block]:
                return [0, insn, _perm]

            enable_b32 = self.get_block_size(dtype) == self.b32_block
            enable_b32 = enable_b32 and intrinsic_check_support()
            if enable_b32:
                return [0, insn, _perm]

            invariant, emit_idx = 0, 0
            for k, v in enumerate(_perm):
                if k != v:
                    break
                invariant += 1
            if invariant == 0:
                emit_idx = 0
            else:
                emit_idx = invariant - len(_perm)
                ori_perm = _perm[emit_idx:].copy()
                back = sorted(ori_perm)
                _perm = [back.index(i) for i in ori_perm]
            return [emit_idx, insn, _perm]

        def vor_pass_limit():
            # Vor not support b8 and some formats
            if self.get_block_size(dtype) in [self.b8_block, ]:
                return "dma_copy"
            formats = get_context().get("_transdata_format")
            return "vector_or" if formats in SUPPORT_VOR_FORMATS else "dma_copy"

        if not perm:
            return [0, "phony_insn", perm]

        insn = "vector_transpose" if perm[-1] != len(perm) - 1 else vor_pass_limit()

        return transpose_pass_limit(perm) if insn == "vector_transpose" else [0, insn, perm]

    def vnchwconv_insn_map(self, tensor, _insn, _iter, perm, stride_first=False):
        # scatter-vnc-instruction
        if _insn == "phony_insn":
            self.schedule[self.parent(tensor)].reused_by(tensor)
            self.emit_insn_map[tensor] = {"scope": _iter, "instruction": _insn}
            return

        # perm's index is dst, value is src. attr need index is src, value is dst
        if _insn == "vector_transpose":
            enable_mode = 0
            bit_size = DATA_TYPE_SIZE.get(tensor.dtype, None)
            cond0 = bit_size == B32 and intrinsic_check_support(dtype="float32") # 71 51 80
            c0_is_c08 = hasattr(self.graph_info, "c0") and self.graph_info.c0 == C08
            cond1 = bit_size == B16 and intrinsic_check_support(dtype="float16") and c0_is_c08 # Nano
            if cond0 or cond1:
                key = (self.graph_info.c0, bit_size, self.graph_info.is_forward, True)
                m_align, _ = VNC_MAP.get(key, (None, None))
                enable_mode = TransposeMode.T816 if m_align == C08 else TransposeMode.T168

            if stride_first:
                attrs = dict(attrs={"src_in_dst_order": tvm.call_intrin("handle", 'tir.tvm_tuple', *perm),
                                    "trans_first_mode": tvm.call_intrin("handle", 'tir.tvm_tuple', 0, 1),
                                    "is_trans_align": 1, "enable_vnchwconv_mode": enable_mode})
            else:
                attrs = dict(attrs={"src_in_dst_order": tvm.call_intrin("handle", 'tir.tvm_tuple', *perm),
                                    "is_trans_align": 1, "enable_vnchwconv_mode": enable_mode})
            self.emit_insn_map[tensor] = {"scope": _iter, "instruction": "vector_transpose"}
            self.emit_insn_map[tensor].update(attrs)
            return

    def common_align_insn_map(self, tensor, _insn, _iter, pad_value=None):
        # RemovePad or AlignPad
        attrs = dict(attrs={"enough_buffer": False, "last_axis": 1,
                            "pad_value": pad_value, "avoid_bank_conflict": True})
        self.emit_insn_map[tensor] = {"scope": _iter, "instruction": _insn}
        self.emit_insn_map[tensor].update(attrs)

    def do_vnchwconv_align(self, tensor, factor):
        """
        :param tensor: last-transpose-tensor
        :param factor: last dim align value
        Func: (m,n)->(n,m), only consider dst-tensor(n,m).
        While dtype belong to [B8,B16], factor should be [32,16].
        While dtype belong to [B32,B64],factor should be [128,64].
        """
        c0 = self.graph_info.c0
        ele_byte = DATA_TYPE_SIZE.get(tensor.dtype, None)
        is_forward = self.graph_info.is_forward
        instr_support = intrinsic_check_support() if ele_byte == B32 else True
        m_align, n_align = VNC_MAP.get((c0, ele_byte, is_forward, instr_support))
        factor = tvm.select(factor >= m_align, factor, m_align)

        # align m
        if len(tensor.shape) >= 2:
            self.schedule[tensor].storage_align(tensor.op.axis[-2], factor, 0)

        # align n
        shape = shape_to_list(tensor.shape)
        if len(shape) >= 3:
            factor = set_align(shape[-1], factor) * n_align
            self.schedule[tensor].storage_align(tensor.op.axis[-3], factor, 0)

    def axes_exclude_ub(self, tensor):
        # which axis out of ub based on tensor
        exclude = []
        for k in self.axis_not_in_ub:
            if k not in self.axis_in_ub:
                exclude.append(k)

        exclude = self.infer_axes(self.tiling_tensor, tensor, exclude)
        if not self.graph_info.x1x0 and not self.graph_info.c1c0:
            return exclude

        # Forward + BH\BN: c1c0 is tuple-2, infer tensors that c1c0 maybe tuple-1 (fuse as c)
        # Backward + BH\BN: c1c0 is tuple-1, infer tensors that c1c0 maybe tuple-2 (split as c1,c0)
        # Forward + Backward + General: c1c0 is tuple-2...
        # c0 always in ub-inner
        # c1 must in ub-inner while BH + BN + N_Last_Transpose
        # c1 maybe in ub-inner while General + Last_Transpose
        if self.graph_info.c1c0:
            # c1c0 must be [c1,c0] or [c,]
            c1c0 = self.infer_axes(self.graph_info.tiling_tensor, tensor, self.graph_info.c1c0)
            c1c0_infer_c1c0 = len(c1c0) == 2 and len(self.graph_info.c1c0) == 2 and c1c0[0] != c1c0[1]
            c1c0_infer_c = len(c1c0) == 2 and c1c0[0] == c1c0[1] and len(self.graph_info.c1c0) == 2
            c_infer_c1c0 = len(c1c0) == 2 and c1c0[0] != c1c0[1] and len(self.graph_info.c1c0) == 1
            c_infer_c = len(c1c0) == 1 and len(self.graph_info.c1c0) == 1
            if c1c0_infer_c1c0 or c1c0_infer_c or c_infer_c1c0:
                if c1c0[1] in exclude:
                    exclude.pop(exclude.index(c1c0[1]))
            elif c_infer_c:
                if c1c0[0] in exclude:
                    exclude.pop(exclude.index(c1c0[0]))
            else:
                dict_args = {"errCode": "E90003", "detailed_cause": "ERROR_C1C0_EXCLUDE"}
                raise RuntimeError(dict_args, get_error_message(dict_args))

        if self.graph_info.x1x0:
            # x1x0 must be [x1,x0]
            x1x0 = self.infer_axes(self.graph_info.tiling_tensor, tensor, self.graph_info.x1x0)
            x1x0_infer_x1x0 = len(x1x0) == 2 and len(self.graph_info.x1x0) == 2 and x1x0[0] != x1x0[1]
            x1x0_infer_x = len(x1x0) == 2 and len(self.graph_info.x1x0) == 2 and x1x0[0] == x1x0[1]
            if x1x0_infer_x1x0 or x1x0_infer_x:
                if x1x0[1] in exclude:
                    exclude.pop(exclude.index(x1x0[1]))
            else:
                dict_args = {"errCode": "E90003", "detailed_cause": "ERROR_X1X0_EXCLUDE"}
                raise RuntimeError(dict_args, get_error_message(dict_args))
        return exclude

    def update_ub_perm(self, tensor, transpose_work=True):
        """
        Func: judge transpose work in ub.
        :param tensor: transpose_tensor.
        :param transpose_work: transpose maybe not work.
        :return: real permute.
        """
        exclude = self.axes_exclude_ub(tensor)
        perm = self.streams.get(tensor, None).attrs
        ori_perm = []
        for k, v in enumerate(perm):
            if k not in exclude:
                ori_perm.append(v)

        back = sorted(ori_perm)
        return [back.index(i) for i in ori_perm] if transpose_work and back != ori_perm else []

    def unroll_transpose(self, tensor):
        """
        Unroll transpose from complex to simply.
        tensor: complex-transpose tensor.
        Regulation:
            Forward: first last transpose{c0 to end}.
            HC1C0N -> C1HNC0
            NC1C0H -> C1HNC0/NC1HC0
            Backward: first n-last transpose.
            C1HNC0 -> HC1C0N
            C1HNC0 -> NC1C0H
        Forward_Eg:
        perm = [1, 3, 0, 2] : [0, 1, 3, 2] -> [1, 2, 0, 3]
        perm = [1, 0, 3, 2] : [0, 1, 3, 2] -> [1, 0, 2, 3]
        Backward_Eg:
        perm = [2, 0, 3, 1] : [2, 0, 1, 3] -> [0, 1, 3, 2]
        """
        perm = self.streams.get(tensor).attrs
        length = len(perm)
        if self.graph_info.is_forward:
            m = perm[-1]
            n, ex = list(range(m + 1, length)), list(range(m))
            last_perm = ex + n + [m, ]
            n_last_perm = [last_perm.index(x) for x in perm]
            transpose_tensor = self.single_cache_transpose(tensor, n_last_perm, last_perm)
        else:
            index = get_last_dim_index(perm)  # backward return c0'idx in dst
            n_last_perm = perm[:index[0]] + perm[index[-1] + 1:] + [perm[x] for x in index]
            last_perm = [n_last_perm.index(x) for x in perm]
            transpose_tensor = self.single_cache_transpose(tensor, last_perm, n_last_perm)

        perms = [self.update_ub_perm(x) for x in [transpose_tensor, tensor]]
        return [transpose_tensor, tensor], perms

    def complex2simple_by_transpose(self):
        """
        Make Complex transpose like (1,0,3,2) to simply transpose [(1,0,2,3), (0,1,3,2)]
        Regulation:
        1. ComplexMode contains last-transpose and n-last-transpose
        2. ComplexMode would split last and n-last
        3. last transpose would be parent of n-last transpose
        """

        def is_complex_transpose(_perm):
            """
            Transpose contains last-transpose and n-last-transpose belong to complex.
            Eg:
            perm is [3,0,1,2], only last-transpose not belong to complex.
            perm is [1,0,3,2], last transpose + n-last transpose belong to complex.
            perm is [2,0,1], work in NC1C0H -> C1HNC0 while C1 is outer of ub
            """
            if _perm == [2, 0, 1]:
                return True
            if not _perm:
                return False
            if _perm[-1] == len(_perm) - 1:
                return False

            new_perm = []
            index = get_last_dim_index(_perm)
            last_perm = [_perm[x] for x in index]
            for i in _perm:
                if i not in last_perm:
                    new_perm.append(i)
            new_perm += last_perm
            return new_perm != list(range(len(_perm)))

        perms, tensors = [], []
        for perm, tensor in zip(self.permutes, self.transpose_tensors):
            if not is_complex_transpose(perm):
                perms.append(perm)
                tensors.append(tensor)
                continue
            _tensors, _perms = self.unroll_transpose(tensor)
            tensors += _tensors
            perms += _perms

        self.transpose_tensors = tensors
        self.permutes = perms

    def infer_axes(self, src, dst, s_index):
        """Common Function
        :param src: tiling_tensor
        :param dst: target_tensor
        :param s_index: idx based on tiling_tensor
        :return: d_index
        """
        if src == dst:
            return s_index

        src_stream = self.streams.get(src, None)
        if not src_stream:
            raise RuntimeError("Tensor not in Stream")

        if src_stream.tag == "transdata|f_reshape":
            s_index = _mapping_f_reshape(src_stream.attrs, s_index)
        elif src_stream.tag == "transdata|s_reshape":
            s_index = _mapping_s_reshape(src_stream.attrs, s_index)
        else:
            s_index = [src_stream.attrs[i] for i in s_index]

        src = self.parent(src)
        return self.infer_axes(src, dst, s_index)

    def get_reshape(self, tensor):
        """
        Get reshape-info from reshape-tensor
        """
        stream = self.streams.get(tensor, None)
        return [x for x in stream.attrs] if stream and stream.attrs else []

    def get_block_size(self, dtype):
        return self.block // DATA_TYPE_SIZE.get(dtype, 1)

    def _create_schedule(self):
        self.schedule = tvm.create_schedule([tensor.op for tensor in self.graph_info.output_tensor_set])

    def _do_cache_read(self):
        for tensor in self.graph_info.input_tensor_set:
            readers = self.forward_stage_graph_map[tensor].copy()
            read_buffer = self.schedule.cache_read(tensor, scope_ubuf, readers)
            self.cache_read_tensors_and_buffer_map[tensor] = read_buffer
            for item in readers:
                self.update_stage(read_buffer, item, True)
            self.update_stream(tensor, read_buffer, mode="cache_read")

    def _do_cache_write(self):
        for tensor in self.graph_info.output_tensor_set:
            writers = self.backward_stage_graph_map[tensor].copy()
            write_buffer = self.schedule.cache_write(tensor, scope_ubuf)
            self.cache_write_tensors_and_buffer_map[tensor] = write_buffer
            for item in writers:
                self.update_stage(write_buffer, item, False)
            self.update_stream(write_buffer, tensor, mode="cache_write")

    def _do_set_scope(self):
        for tensor in self.graph_info.mid_tensor_set:
            if tensor not in self.graph_info.output_tensor_set:
                self.schedule[tensor].set_scope(scope_ubuf)

    def _do_tiling(self):
        if self.tiling_case.fuse_mode == 0:
            self._do_ub_tiling()
            self._do_block_tiling()
            self._do_reorder()
            self._do_fragments()
        else:
            self._do_ub_tiling()
            self._do_fuse_reorder()
            self._do_fragments()

    def _do_ub_tiling(self):
        first_factor = self.tiling_case.ub_first_factor
        second_factor = self.tiling_case.ub_second_factor

        # first ub tiling
        first_axis_var = self.tiling_axes[self.tiling_case.ub_split_first_idx]
        self.iter_ub_first_outer, self.iter_ub_first_inner = \
            self.schedule[self.tiling_tensor].split(first_axis_var, factor=first_factor)

        # second ub tiling
        if not self.split_once:
            sec_axis_var = self.tiling_axes[self.tiling_case.ub_split_second_idx]
            self.iter_ub_second_outer, self.iter_ub_second_inner = \
                self.schedule[self.tiling_tensor].split(sec_axis_var, factor=second_factor)

    def _do_block_tiling(self):
        """
        block tiling only split axis which belong to outsiders of ub
        """
        case = self.tiling_case
        if case.block_split_idx == case.ub_split_first_idx:
            tiling_axis_var = self.iter_ub_first_outer
        elif not self.split_once and case.block_split_idx == case.ub_split_second_idx:
            tiling_axis_var = self.iter_ub_second_outer
        else:
            tiling_axis_var = self.tiling_axes[case.block_split_idx]

        self.iter_block_outer, self.iter_block_inner = \
            self.schedule[self.tiling_tensor].split(tiling_axis_var, factor=case.block_factor)

    def _calc_multi_core(self):
        if self.need_multi_core:
            idx = self.reorder_list.index(self.iter_block_outer)
            fused_list = self.reorder_list[:idx + 1]
            self.multi_core_fused_axis = self.schedule[self.tiling_tensor].fuse(*fused_list)
            self.multi_core_bind_tensor = self.tiling_tensor

    def _do_multi_core(self):
        if self.need_multi_core:
            res = self.multi_core_bind_tensor
            blk = tvm.thread_axis("blockIdx.x")
            self.schedule[res].bind(self.multi_core_fused_axis, blk)

    def _calc_compute_at(self):
        self.compute_at_map.clear()
        for tensor in self.get_all_producers_stages(self.tiling_tensor):
            if tensor not in self.graph_info.input_tensor_set:
                self.compute_at_map[tensor] = {"parent": self.schedule[self.tiling_tensor], "scope": self.ub_outer}

    def _do_compute_at(self):
        for stage in self.compute_at_map:
            parent_stage = self.compute_at_map[stage].get("parent", None)
            scope_iter_var = self.compute_at_map[stage].get("scope", None)
            self.schedule[stage].compute_at(parent_stage, scope_iter_var)
    
    def _do_compute_inline(self):
        if self.compute_inline_list:
            for stage in self.compute_inline_list:
                self.schedule[stage].compute_inline()

    def _do_emit_insn(self):
        for stage in self.emit_insn_map:
            if stage in self.compute_inline_list:
                continue
            scope_iter_var = self.emit_insn_map[stage].get("scope", None)
            instruction = self.emit_insn_map[stage].get("instruction", None)
            cond_0 = get_soc_spec("SHORT_SOC_VERSION") in ["Ascend310B", "AS31XM1"]
            cond_1 = instruction in ["dma_copy"]
            cond_2 = stage not in [self.mte2_tensor, self.tiling_tensor]
            if cond_0 and cond_1 and cond_2:
                self.emit_insn_map[stage].update(dict(attrs={"align_inner_for_extent": 1}))
            attrs = self.emit_insn_map[stage].get("attrs", None)
            if isinstance(attrs, dict) and NO_OVERLAP in attrs.keys() and attrs.get(NO_OVERLAP) == 3:
                attrs[NO_OVERLAP] = tvm.call_intrin('handle', 'tir.tvm_tuple', 2, 3)
            self.schedule[stage].emit_insn(scope_iter_var, instruction, attrs=attrs)

    def _do_double_buffer(self):
        for _tensor in self.double_buffer_tensors:
            self.schedule[_tensor].double_buffer()
        operation.add_build_arg("double_buffer_non_reuse", True)

    def _do_reorder(self):
        """
        Regulation: [D,E,C,B,A] ~ [D,E,C.outer,C.inner,B,A.outer,A.inner]
        if [D,E,B] belong to ub_outer, reorder is [D,E,C.outer,B,A.outer,C.inner,A.inner]
        """
        split_i = self.tiling_case.ub_split_first_idx
        split_o = self.tiling_case.ub_split_second_idx
        split_b = self.tiling_case.block_split_idx

        for idx in self.axis_in_ub:
            if idx == split_i:
                self.reorder_list.append(self.iter_ub_first_inner)
            elif idx == split_o:
                self.reorder_list.append(self.iter_ub_second_inner)
            else:
                self.reorder_list.append(self.tiling_axes[idx])

        outside = []
        for idx in self.axis_not_in_ub:
            if idx == split_b:
                outside.extend([self.iter_block_outer, self.iter_block_inner])
            elif idx == split_i:
                outside.append(self.iter_ub_first_outer)
            elif idx == split_o:
                outside.append(self.iter_ub_second_outer)
            else:
                outside.append(self.tiling_axes[idx])

        self.ub_outer = outside[-1]
        self.ub_inner = self.reorder_list[0]
        self.reorder_list = outside + self.reorder_list
        self.schedule[self.tiling_tensor].reorder(*self.reorder_list)

    def _do_fuse_reorder(self):
        split_i = self.tiling_case.ub_split_first_idx
        split_o = self.tiling_case.ub_split_second_idx
        for idx in self.axis_in_ub:
            if idx == split_i:
                self.reorder_list.append(self.iter_ub_first_inner)
            elif idx == split_o:
                self.reorder_list.append(self.iter_ub_second_inner)
            else:
                self.reorder_list.append(self.tiling_axes[idx])

        outside = []
        for idx in self.axis_not_in_ub:
            if idx == split_i:
                outside.append(self.iter_ub_first_outer)
            elif idx == split_o:
                outside.append(self.iter_ub_second_outer)
            else:
                outside.append(self.tiling_axes[idx])

        # do reorder before fused
        _reorder = outside + self.reorder_list
        self.schedule[self.tiling_tensor].reorder(*_reorder)

        # make sure fused axis
        fused_axis = self.schedule[self.tiling_tensor].fuse(*outside)
        self.iter_block_outer, self.iter_block_inner = \
            self.schedule[self.tiling_tensor].split(fused_axis, factor=self.tiling_case.block_factor)
        outside = [self.iter_block_outer, self.iter_block_inner]

        # update params
        self.ub_outer = outside[-1]
        self.ub_inner = self.reorder_list[0]
        self.reorder_list = outside + self.reorder_list

    def _do_storage_bound(self):
        for stage in self.forward_stage_graph_map:
            ub_count = self.tiling_case.tensor_ub_size_list[self.tiling_case.shape_type]
            self.schedule[stage].set_buffer_size(ub_count)

    def _do_fragments(self):
        self.schedule[self.tiling_tensor].pragma(self.iter_block_inner, "local.UB_fragments_memory_size", 256)

    def _do_group(self, tensor, axes, number=0):
        group_id = tvm.call_extern("int32", "axis_group", number, "overwrite")
        for i in axes:
            self.schedule[tensor].pragma(i, "axis_group", group_id)

    def _do_transpose_pragma(self, tensor, permute):
        """
        :param tensor: transpose-tensor don't do reorder
        :param permute: tensor's permute that belong to ub-inner
        :return: pragma for ub-serial-transpose
        """

        def _serial(perm):
            if len(perm) == 1:
                return [0, ]

            result, j = [], 0
            while j < len(perm) - 1:
                left, right = perm[j], perm[j + 1]
                if right != left + 1:
                    result.append(j + 1)
                j += 1
            result.append(j + 1)
            return result

        exclude = set(self.axes_exclude_ub(tensor))
        axis_in_ub = list(set(range(len(tensor.shape))).difference(exclude))
        axis_in_ub.sort()
        gap = _serial(permute if permute else list(range(len(axis_in_ub))))
        begin = 0
        for k, v in enumerate(gap):
            axes = [tensor.op.axis[j] for j in axis_in_ub[begin:v]]
            self._do_group(tensor, axes, number=k)
            begin = v


def _mapping_s_reshape(axes, idx_list):
    # Eg: src=[(0,1),2,3,4] -> dst=[0,1,2,3,4], dst[0] mapping (0,1).
    result = []
    for i in idx_list:
        if i in axes:
            result.append(axes.index(i))
        else:
            for k, v in enumerate(axes):
                if isinstance(v, (list, tuple)) and i in v:
                    result.append(k)
    return result


def _mapping_f_reshape(axes, idx_list):
    result = []
    for i in idx_list:
        if isinstance(axes[i], int):
            result.append(axes[i])
        else:
            result.extend(axes[i])
    return result


def get_last_dim_index(_perm):
    """
    Get last dim in transpose
    Eg:
    perm is [1,0,3,2], lastDim is 3.
    perm is [0,2,3,1], lastDim is [2,3].
    """
    value = len(_perm) - 1
    idx = _perm.index(value)
    last = [idx, ]
    for i in range(idx - 1, -1, -1):
        if _perm[i] == value - 1:
            last.insert(0, i)
            value = value - 1
            continue
        break
    return last
