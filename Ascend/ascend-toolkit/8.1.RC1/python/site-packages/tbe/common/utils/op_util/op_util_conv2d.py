#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2022-2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.

Description:
"""
from enum import IntEnum, auto
import copy
import math
import tbe
from tbe.tvm import Tensor
from tbe.common import platform
from tbe.dsl.base import operation
from tbe.common.context import get_context
from tbe.common.utils import log
from tbe.dsl.base.operation import add_compile_info
from tbe.dsl.base.operation import get_compile_info
from tbe.common.utils.errormgr import error_manager_cube as err_man

m_bit_ratio = {"float16": 2, "int8": 1, "float32": 4, "bfloat16": 2, "int4": 0.5}

AUB_COEFFICIENT_INDEX = 0
CUB_COEFFICIENT_INDEX = 2

# conv2d_compress support alg
WEIGHT_UNZIP = "weight_unzip"
WEIGHT_SPARSE_4_2 = "weight_sparse_4_2"
COMPRESS_ALG_SUPPORT = [WEIGHT_UNZIP, WEIGHT_SPARSE_4_2]

# binary var range max
TILING_DIM_MAX = 32
CONV2D_DILATION_MAX = 2147483647
CONV2D_STRIDE_MAX = 2147483647
CONV2D_PAD_MAX = 2147483647
CONV2D_KERNEL_MAX = 2147483647
BINARY_CONFIG_KEY = "multiple_templates"

# mkn index
CUBE_MKN_IDX_M = 0
CUBE_MKN_IDX_K = 1
CUBE_MKN_IDX_N = 2
CHANNEL_SPLIT_FACTOR = 2

# conv inpute index
CONV_FM_IDX = 0
CONV_FILTER_IDX = 1
CONV_BIAS_IDX = 2

# conv attr index
CONV_ATTR_GROUP_IDX = 3

CUBE_UNIT = 16
ELTWISE_FUSION_OTHER_INPUT_SHAPE_DIMS = 4  # [N, C1, HW, C0]
C0_BYTE_LEN = 32
INT32_BIT_SIZE = 4

# load3dv2 postk can not bigger than 65535
MAX_POSTK_VALUE = 65535
UINT32_MAX = 2**32 - 1
UINT64_MAX = 2**64 - 1

# winograd support attr
WINO_SUPPORT_KH = 3
WINO_SUPPORT_KW = 3
WINO_SUPPORT_MIN_CIN = 32
WINO_SUPPORT_MIN_COUT = 32
WINO_SUPPORT_MIN_H = 4
WINO_SUPPORT_MIN_W = 4
WINO_SUPPORT_MIN_HW = 16 * 16
WINO_SUPPORT_WO = 32
WINO_WEIGHT_DIM = 6
WINO_IN_TILE_HW = 4
WINO_OUT_TILE_HW = 2

UB_COEFF_CONVERT = {
    "int4": 0.25,
    "int8": 0.5,
    "uint8": 0.5,
    "bfloat16": 1,
    "float16": 1,
    "float32": 2,
    "int32": 2,
} # ub space is calculated in fp16 uniformly.

FUSED_UB_CL0_CONVERT = {
    "float16": 1,
    "float32": 2
} # fused_cl0 convert map

BIT_RATIO_MAP = {
    "float16": 2,
    "int8": 1,
    "int32": 4,
    "float32": 4,
    "bfloat16": 2,
    "int4": 0.5,
    "int16": 2,
    "uint64": 8,
    "int64": 8
}

CONV_RES_DTYPE_MAP = {
    "float16": "float16",
    "bfloat16": "bfloat16",
    "float32": "float32",
    "int8": "int32",
    "int4": "int32",
}

BINARY_SRC_SHAPE_ATTR = "src_shape_attr"
L0A_DMA_SCENE = "l0a_dma_scene"
C0_OPTIM_FLG = "c0_optim_flg"


class ConstValue(object):
    DOUBLE_VALUE = 2
    HALF_CUBE_UNIT = 8
    ONE_HALF = 0.5

    C04_CONST = 4

    # based on cub
    PRE_TRANSDATA_TENSORS_SPACE_OCCUPY_NUM = 2
    POST_TRANSDATA_TENSORS_SPACE_OCCUPY_NUM = 1

    FILTER_HW_MAX = 511   # only in load3dv2 after v220
    STRIDE_MAX = 63
    PAD_MAX = 255
    DILATE_MAX = 255


class AttachMode(object):
    """
    binary attach mode
    """
    ATTACH_FULL_LOAD = 0
    ATTACH_PASS = 3
    ATTACH_RES = 1
    ATTACH_CL0 = 2
    ATTACH_CL0_FOR_BL0 = 1


class Conv2dTensorName(object):
    """
    conv2d tensor map key.
    """
    # weight
    FILTER = "filter"
    BL0 = "bl0"
    WEIGHT_INDEX = "weight_index"

    # fm
    FMAP = "fmap"
    FMAP_IMG2COL = "fmap_im2col"
    FMAP_L1 = "fmap_l1"
    FMAP_ROW_MAJOR = "fmap_row_major"
    FMAP_RAW_MAJOR_RESHAPE = "fmap_row_major_reshape"
    FMAP_L1_DMA_ZERO = "fmap_l1_dma_zero"
    FMAP_L1_DMA_IM2COL = "fmap_l1_dma_im2col"

    # bias
    BIAS_L1 = "bias_l1"
    BIAS_BT = "bias_bt"
    BIAS_UB = "bias_ub"
    BIAS_L0C = "bias_l0c"

    # mad
    CL0 = "mad1"
    CL0_WINO = "cl0_wino"

    # l0c
    L0C_BIAS_ADD = "l0c_bias_add"

    # ub
    CUB = "cub"
    CUB_BIAS_ADD = "cub_bias_add"
    REMOVE_PADDED_COLUMN = "remove_padded_column"

    #RES
    FIXPIPE_RES = "fixpipe_res"
    RES_CONV2D = "res_conv2d"
    RES_CONV2D_WINO = "res_conv2d_wino"
    RES_FP32_CONV2D = "res_fp32_conv2d"
    REMOVE_PAD_CC = "remove_pad_cc"
    INVALID_REMOVE_PAD = "invalid_conv2d_rmpad"


class Conv2dParaDictKey(object):
    BIAS_TENSOR = "bias_tensor"
    OFFSET_W_TENSOR = "offset_w_tensor"


class Conv2dCompileInfoKey(object):
    FMAP_C1 = "fmap_c1"
    CONV_OP_TYPE_LIST = "conv_op_type_list"
    OP_TYPE_LIST = "op_type_list"
    FUSION_UTILIZE = "fusion_utilize"
    UB_FUSION_PATTERN = "ub_fusion_pattern"
    HARDWARE_INFO = "hardware_info"
    CONV2D_INPUT = "conv2d_input_bit"
    IS_SUPPORT_FIXPIPE = "is_support_fixpipe"
    CURRENT_FEATURE_FLAG = "current_feature_flag"
    COMPILE_GET_TILING_FLAG = "compile_get_tiling_flag"
    TILING_TYPE = "tiling_type"
    BINARY_STATIC_FLAG = "binary_static_flag"


class Conv2dTilingInfoDictKey(object):
    CACHE_TILING_FLAG = "cache_tiling_flag"
    FUSED_COEFFICIENT = "fused_coefficient"
    FUSED_CHANNEL_WISE = "fused_channel_wise"


class TilingDataIdx(IntEnum):
    """
    tiling data define index
    """
    TILINGDATA_IDX_START = 0
    IDX_BATCH_N = TILINGDATA_IDX_START
    IDX_C_IN = auto()
    IDX_FMAP_H = auto()
    IDX_FMAP_W = auto()
    IDX_C_OUT = auto()
    IDX_K_H = auto()
    IDX_K_W = auto()
    IDX_GROUP_OPT = auto()
    IDX_CIN1_OPT = auto()
    IDX_COUT1_OPT = auto()
    IDX_HO = auto()
    IDX_WO = auto()
    IDX_DILATION_H = auto()
    IDX_DILATION_W = auto()
    IDX_STRIDE_H = auto()
    IDX_STRIDE_W = auto()
    IDX_PAD_TOP = auto()
    IDX_PAD_BOTTOM = auto()
    IDX_PAD_LEFT = auto()
    IDX_PAD_RIGHT = auto()
    IDX_HF32_MODE = auto()
    IDX_BATCH_SINGLE_CORE = auto()
    IDX_N_SINGLE_CORE = auto()
    IDX_BATCH_DIM = auto()
    IDX_N_DIM = auto()
    IDX_M_DIM = auto()
    IDX_GROUP_DIM = auto()
    IDX_K_AUB = auto()
    IDX_M_AUB = auto()
    IDX_CUB_N1 = auto()
    IDX_N_UB_L0C_FACTOR = auto()
    IDX_M_L0 = auto()
    IDX_K_L0 = auto()
    IDX_M_AL1_FACTOR = auto()
    IDX_N_BL1_FACTOR = auto()
    IDX_KAL1_16 = auto()
    IDX_KBL1_16 = auto()
    IDX_KAL1_FACTOR = auto()
    IDX_KBL1_FACTOR = auto()
    IDX_GROUP = auto()
    IDX_ENLARGE = auto()
    IDX_AL1_PBUFFER = auto()
    IDX_BL1_PBUFFER = auto()
    IDX_AL0_PBUFFER = auto()
    IDX_BL0_PBUFFER = auto()
    IDX_CL0_PBUFFER = auto()
    IDX_AUB_PBUFFER = auto()
    IDX_BUB_PBUFFER = auto()
    IDX_CUB_PBUFFER = auto()
    IDX_UBG_PBUFFER = auto()
    TILINGDATA_IDX_END = auto()


class TilingDataKey(object):
    """
    tiling data key string.
    """
    BATCH_SINGLE_CORE = "batch_single_core"
    N_SINGLE_CORE = "n_single_core"
    BATCH_DIM = "batch_dim"
    N_DIM = "n_dim"
    M_DIM = "m_dim"
    GROUP_DIM = "group_dim"
    K_AUB = "k_aub"
    M_AUB = "m_aub"
    CUB_N1 = "cub_n1"
    N_UB_L0C_FACTOR = "n_ub_l0c_factor"
    M_L0 = "m_l0"
    K_L0 = "k_l0"
    M_AL1_FACTOR = "m_al1_factor"
    N_BL1_FACTOR = "n_bl1_factor"
    KAL1_16 = "kal1_16"
    KBL1_16 = "kbl1_16"
    KAL1_FACTOR = "kal1_factor"
    KBL1_FACTOR = "kbl1_factor"
    DILATION_H = "dilation_h"
    DILATION_W = "dilation_w"
    STRIDE_H = "stride_h"
    STRIDE_W = "stride_w"
    HF32_MODE = "hf32_mode"
    C_IN = "c_in"
    C_OUT = "c_out"
    K_H = "k_h"
    K_W = "k_w"
    PAD_TOP = "pad_top"
    PAD_BOTTOM = "pad_bottom"
    PAD_LEFT = "pad_left"
    PAD_RIGHT = "pad_right"
    BATCH_N = "batch_n"
    FMAP_H = "fmap_h"
    FMAP_W = "fmap_w"
    HO = "ho"
    WO = "wo"
    GROUP = "group"
    GROUP_OPT = "group_opt"
    CIN1_OPT = "cin1_opt"
    COUT1_OPT = "cout1_opt"
    ENLARGE = "enlarge"
    OTHER_INPUT = "input"
    AL1_PBUFFER = "al1_pbuffer"
    BL1_PBUFFER = "bl1_pbuffer"
    AL0_PBUFFER = "al0_pbuffer"
    BL0_PBUFFER = "bl0_pbuffer"
    CL0_PBUFFER = "cl0_pbuffer"
    AUB_PBUFFER = "aub_pbuffer"
    BUB_PBUFFER = "bub_pbuffer"
    CUB_PBUFFER = "cub_pbuffer"
    UBG_PBUFFER = "ubg_pbuffer"


class Conv2DL0aDmaScene(object):
    BASIC = 0
    AUB_ONLY_LOAD_K0 = 1
    DMA_CONV1D_WITHOUT_PAD = 2


TILINGDATA_KEY_MAP = {
    TilingDataIdx.IDX_DILATION_H: TilingDataKey.DILATION_H,
    TilingDataIdx.IDX_DILATION_W: TilingDataKey.DILATION_W,
    TilingDataIdx.IDX_STRIDE_H: TilingDataKey.STRIDE_H,
    TilingDataIdx.IDX_STRIDE_W: TilingDataKey.STRIDE_W,
    TilingDataIdx.IDX_C_IN: TilingDataKey.C_IN,
    TilingDataIdx.IDX_C_OUT: TilingDataKey.C_OUT,
    TilingDataIdx.IDX_K_H: TilingDataKey.K_H,
    TilingDataIdx.IDX_K_W: TilingDataKey.K_W,
    TilingDataIdx.IDX_PAD_TOP: TilingDataKey.PAD_TOP,
    TilingDataIdx.IDX_PAD_BOTTOM: TilingDataKey.PAD_BOTTOM,
    TilingDataIdx.IDX_PAD_LEFT: TilingDataKey.PAD_LEFT,
    TilingDataIdx.IDX_PAD_RIGHT: TilingDataKey.PAD_RIGHT,
    TilingDataIdx.IDX_HF32_MODE: TilingDataKey.HF32_MODE,
    TilingDataIdx.IDX_BATCH_N: TilingDataKey.BATCH_N,
    TilingDataIdx.IDX_FMAP_H: TilingDataKey.FMAP_H,
    TilingDataIdx.IDX_FMAP_W: TilingDataKey.FMAP_W,
    TilingDataIdx.IDX_HO: TilingDataKey.HO,
    TilingDataIdx.IDX_WO: TilingDataKey.WO,
    TilingDataIdx.IDX_GROUP: TilingDataKey.GROUP,
    TilingDataIdx.IDX_GROUP_OPT: TilingDataKey.GROUP_OPT,
    TilingDataIdx.IDX_CIN1_OPT: TilingDataKey.CIN1_OPT,
    TilingDataIdx.IDX_COUT1_OPT: TilingDataKey.COUT1_OPT,
    TilingDataIdx.IDX_ENLARGE: TilingDataKey.ENLARGE,
    TilingDataIdx.IDX_BATCH_SINGLE_CORE: TilingDataKey.BATCH_SINGLE_CORE,
    TilingDataIdx.IDX_N_SINGLE_CORE: TilingDataKey.N_SINGLE_CORE,
    TilingDataIdx.IDX_BATCH_DIM: TilingDataKey.BATCH_DIM,
    TilingDataIdx.IDX_N_DIM: TilingDataKey.N_DIM,
    TilingDataIdx.IDX_M_DIM: TilingDataKey.M_DIM,
    TilingDataIdx.IDX_GROUP_DIM: TilingDataKey.GROUP_DIM,
    TilingDataIdx.IDX_K_AUB: TilingDataKey.K_AUB,
    TilingDataIdx.IDX_M_AUB: TilingDataKey.M_AUB,
    TilingDataIdx.IDX_CUB_N1: TilingDataKey.CUB_N1,
    TilingDataIdx.IDX_N_UB_L0C_FACTOR: TilingDataKey.N_UB_L0C_FACTOR,
    TilingDataIdx.IDX_M_L0: TilingDataKey.M_L0,
    TilingDataIdx.IDX_K_L0: TilingDataKey.K_L0,
    TilingDataIdx.IDX_M_AL1_FACTOR: TilingDataKey.M_AL1_FACTOR,
    TilingDataIdx.IDX_N_BL1_FACTOR: TilingDataKey.N_BL1_FACTOR,
    TilingDataIdx.IDX_KAL1_16: TilingDataKey.KAL1_16,
    TilingDataIdx.IDX_KBL1_16: TilingDataKey.KBL1_16,
    TilingDataIdx.IDX_KAL1_FACTOR: TilingDataKey.KAL1_FACTOR,
    TilingDataIdx.IDX_KBL1_FACTOR: TilingDataKey.KBL1_FACTOR,
    TilingDataIdx.IDX_AL1_PBUFFER: TilingDataKey.AL1_PBUFFER,
    TilingDataIdx.IDX_BL1_PBUFFER: TilingDataKey.BL1_PBUFFER,
    TilingDataIdx.IDX_AL0_PBUFFER: TilingDataKey.AL0_PBUFFER,
    TilingDataIdx.IDX_BL0_PBUFFER: TilingDataKey.BL0_PBUFFER,
    TilingDataIdx.IDX_CL0_PBUFFER: TilingDataKey.CL0_PBUFFER,
    TilingDataIdx.IDX_AUB_PBUFFER: TilingDataKey.AUB_PBUFFER,
    TilingDataIdx.IDX_BUB_PBUFFER: TilingDataKey.BUB_PBUFFER,
    TilingDataIdx.IDX_CUB_PBUFFER: TilingDataKey.CUB_PBUFFER,
    TilingDataIdx.IDX_UBG_PBUFFER: TilingDataKey.UBG_PBUFFER,
}

TILINGDATA_KEY_DEFAULT_RANGE = [1, None]
TILINGDATA_KEY_RANGE_MAP = {
    TilingDataKey.BATCH_SINGLE_CORE: [1, 1024],
    TilingDataKey.N_SINGLE_CORE: [1, 1024],
    TilingDataKey.BATCH_DIM: [1, TILING_DIM_MAX],
    TilingDataKey.N_DIM: [1, TILING_DIM_MAX],
    TilingDataKey.M_DIM: [1, TILING_DIM_MAX],
    TilingDataKey.GROUP_DIM: [1, TILING_DIM_MAX],
    TilingDataKey.K_AUB: [1, 1024],
    TilingDataKey.M_AUB: [1, 1024],
    TilingDataKey.CUB_N1: [1, 128],
    TilingDataKey.N_UB_L0C_FACTOR: [1, 64],
    TilingDataKey.M_L0: [1, 128],
    TilingDataKey.K_L0: [1, 128],
    TilingDataKey.M_AL1_FACTOR: [1, 1024],
    TilingDataKey.N_BL1_FACTOR: [1, 1024],
    TilingDataKey.KAL1_16: [1, 64],
    TilingDataKey.KBL1_16: [1, 64],
    TilingDataKey.KAL1_FACTOR: [1, 64],
    TilingDataKey.KBL1_FACTOR: [1, 64],
    TilingDataKey.DILATION_H: [1, CONV2D_DILATION_MAX],
    TilingDataKey.DILATION_W: [1, CONV2D_DILATION_MAX],
    TilingDataKey.STRIDE_H: [1, CONV2D_STRIDE_MAX],
    TilingDataKey.STRIDE_W: [1, CONV2D_STRIDE_MAX],
    TilingDataKey.C_IN: [1, None],
    TilingDataKey.C_OUT: [1, None],
    TilingDataKey.K_H: [1, CONV2D_KERNEL_MAX],
    TilingDataKey.K_W: [1, CONV2D_KERNEL_MAX],
    TilingDataKey.PAD_TOP: [1, CONV2D_PAD_MAX],
    TilingDataKey.PAD_BOTTOM: [1, CONV2D_PAD_MAX],
    TilingDataKey.PAD_LEFT: [1, CONV2D_PAD_MAX],
    TilingDataKey.PAD_RIGHT: [1, CONV2D_PAD_MAX],
    TilingDataKey.FMAP_H: [1, None],
    TilingDataKey.FMAP_W: [1, None],
    TilingDataKey.HO: [1, None],
    TilingDataKey.WO: [1, None],
    TilingDataKey.BATCH_N: [1, None],
    TilingDataKey.GROUP: [1, None],
    TilingDataKey.GROUP_OPT: [1, None],
    TilingDataKey.CIN1_OPT: [1, None],
    TilingDataKey.COUT1_OPT: [1, None],
    TilingDataKey.ENLARGE: [1, None],
    TilingDataKey.AL1_PBUFFER: [0, 1],
    TilingDataKey.BL1_PBUFFER: [0, 1],
    TilingDataKey.AL0_PBUFFER: [0, 1],
    TilingDataKey.BL0_PBUFFER: [0, 1],
    TilingDataKey.CL0_PBUFFER: [0, 1],
    TilingDataKey.AUB_PBUFFER: [0, 1],
    TilingDataKey.BUB_PBUFFER: [0, 1],
    TilingDataKey.CUB_PBUFFER: [0, 1],
    TilingDataKey.UBG_PBUFFER: [0, 1],
}


class BinaryTilingKey(object):
    """
    binary attach key
    """
    BL0_ATTACH_FLAG = "bl0_attach_flag"
    AL1_ATTACH_FLAG = "al1_attach_flag"
    BL1_ATTACH_FLAG = "bl1_attach_flag"
    BIAS_CHANNEL_WISE_FLAG = "BIAS_channel_wise_flag"


class KernelIdKeyOffset(object):
    """
    kernel id key bit offset
    """
    OFFSET_AL1 = 0
    OFFSET_BL1 = 3
    OFFSET_BL0 = 6
    OFFSET_BATCH_SPLIT = 9
    OFFSET_GROUP_SPLIT = 11
    OFFSET_CUB_CHANNEL_WISE = 13
    OFFSET_LOADMODE = 15  # 15-16
    OFFSET_CONV1D = 17  # occupy No.17 - No.18 bits, represent conv1d scenario.
    OFFSET_Nx1 = 19  # occupy No.19 - No.20 bits, represent Nx1 scenario.
    OFFSET_GROUPOPT = 21  # offset of group from 21 to 22.
    OFFSET_AL0BOUNDCHECK = 23
    # offset from 23 to 26 is reserved bit.
    OFFSET_L0A_DMA_SCENE = 27  # offset of different dma template scenes from 27 to 29.
    OFFSET_BROADCAST = 32


class LoadModeValue(object):
    """
    loadmode
    """
    MODE_3D = 0
    MODE_2D = 1
    MODE_DMA = 2


# tiling template support list
TILING_ATTACH_SUPPORT_LIST = [
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_FULL_LOAD,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_PASS,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_FULL_LOAD,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_CL0,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_PASS,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_PASS,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_CL0,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_CL0,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_FULL_LOAD,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_FULL_LOAD,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_CL0_FOR_BL0},
    {BinaryTilingKey.AL1_ATTACH_FLAG: AttachMode.ATTACH_RES,
     BinaryTilingKey.BL1_ATTACH_FLAG: AttachMode.ATTACH_PASS,
     BinaryTilingKey.BL0_ATTACH_FLAG: AttachMode.ATTACH_FULL_LOAD},
]


class BinaryInfoKey(object):
    """
    binary compile feature flag.
    """
    LOAD2D_FLAG = "load2d_flag"
    LOAD3D_FLAG = "load3d_flag"
    DMA_FLAG = "dma_flag"
    CONV1D_FLAG = "conv1d_flag"
    GROUPOPT_FLAG = "groupopt_flag"
    Nx1_FLAG = "nx1_flag"
    BROADCAST_FLAG = "broadcast_flag"
    AL0BoundCheck_Flag = "al0boundcheck_flag"


BINARY_LOAD_MODES = [BinaryInfoKey.LOAD2D_FLAG,
                     BinaryInfoKey.LOAD3D_FLAG,
                     BinaryInfoKey.DMA_FLAG]

BINARY_FEATURE_FLAG_OFFSET_MAP = {
    BinaryInfoKey.CONV1D_FLAG: KernelIdKeyOffset.OFFSET_CONV1D,
    BinaryInfoKey.Nx1_FLAG: KernelIdKeyOffset.OFFSET_Nx1,
    BinaryInfoKey.GROUPOPT_FLAG: KernelIdKeyOffset.OFFSET_GROUPOPT,
    BinaryInfoKey.BROADCAST_FLAG: KernelIdKeyOffset.OFFSET_BROADCAST,
    BinaryInfoKey.AL0BoundCheck_Flag: KernelIdKeyOffset.OFFSET_AL0BOUNDCHECK
}

BINARY_LOADMODE_MAP = {
    BinaryInfoKey.LOAD3D_FLAG: LoadModeValue.MODE_3D,
    BinaryInfoKey.LOAD2D_FLAG: LoadModeValue.MODE_2D,
    BinaryInfoKey.DMA_FLAG: LoadModeValue.MODE_DMA
}

EXTRA_BUFFERS_V220 = ["fb0_size", "fb1_size", "bt_size"]
EXTRA_BUFFERS_V300 = ["fb0_size", "fb1_size", "fb2_size", "fb3_size", "bt_size"]
INTRINSIC_FIXPIPE_UNIT_LIST = "Intrinsic_fix_pipe_unit_list"
INTRINSIC_CONV_UB_TO_UB = "Intrinsic_conv_ub_to_ub"
UNIT_POST_ELTWISE = "post_eltwise"
L0A_LAYOUT_IS_ZN = "L0A_LAYOUT_IS_zN"
INTRINSIC_DATA_MOVE_OUT2L1_ND2NZ = "Intrinsic_data_move_out2l1_nd2nz"
INTRINSIC_WINOGRAD_CONV = "Intrinsic_winograd_conv"


def ceil_div(x_1, x_2):
    """
    ceil divide for inputs
    """

    if x_1 is None:
        return x_1
    if x_2 == 0:
        raise RuntimeError("division by zero")
    return (x_1 + x_2 - 1) // x_2


def get_binary_infos():
    """
    get binary compile feature infos.
    """
    conv2d_binary_infos_tmp = {}
    op_infos = get_context().get_op_info(None)
    if op_infos:
        extra_params = op_infos[0].extra_params
        if extra_params:
            conv2d_binary_infos_tmp = extra_params.get(BINARY_CONFIG_KEY, {})
    conv2d_binary_infos = {}
    conv2d_binary_infos[BinaryInfoKey.LOAD2D_FLAG] = conv2d_binary_infos_tmp.get(
        BinaryInfoKey.LOAD2D_FLAG, False)
    return conv2d_binary_infos


def show_class_var(class_obj):
    """
    get all tensor_name defined
    """
    def get_tensor_name_vars():
        list_tensor_name_vars = []
        for attr in dir(class_obj):
            if not callable(getattr(class_obj, attr)) and not attr.startswith("__"):
                list_tensor_name_vars.append(attr)
        return list_tensor_name_vars

    tensor_name_vars = get_tensor_name_vars()
    log.debug("show class variable of class [{}]".format(class_obj.__name__))
    for var in tensor_name_vars:
        log.debug("[{}.{}]:{}".format(class_obj.__name__, var, class_obj.__dict__.get(var)))


def is_support_v300():
    """
    Check v300 intrinsic support.
    """
    if tbe.common.platform.platform_info.intrinsic_check_support(INTRINSIC_FIXPIPE_UNIT_LIST):
        return tbe.common.platform.platform_info.intrinsic_check_support(
            INTRINSIC_FIXPIPE_UNIT_LIST, UNIT_POST_ELTWISE)

    return False


def support_conv_instruction():
    return tbe.common.platform.platform_info.intrinsic_check_support(INTRINSIC_CONV_UB_TO_UB)


def get_cur_soc():
    """
    get soc version
    """
    return tbe.common.platform.platform_info.get_soc_spec("SHORT_SOC_VERSION")


def check_nosupport_binary_op():
    """
    check if op type supports binary op
    """
    if is_conv2d_binary() or is_pooling_binary():
        return False

    return True


def check_nosupport_binary_dtype(dtype):
    """
    check if op dtype support binary op
    """
    if dtype not in ["float16"]:
        return True

    return False


def is_v300_soc():
    """
    check v300 soc.
    """
    soc_version = tbe.common.platform.platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in ["Ascend310B", "AS31XM1"]:
        return True

    return False


def is_support_v220():
    """
    Check v220 intrinsic support.
    """
    if tbe.common.platform.platform_info.intrinsic_check_support(INTRINSIC_FIXPIPE_UNIT_LIST):
        return not tbe.common.platform.platform_info.intrinsic_check_support(
            INTRINSIC_FIXPIPE_UNIT_LIST, UNIT_POST_ELTWISE)

    return False


def is_support_fixpipe():
    """
    Check fixpipe support.
    """
    return tbe.common.platform.platform_info.intrinsic_check_support(INTRINSIC_FIXPIPE_UNIT_LIST)


def is_support_nd2nz():
    """
    Check move_out2l1_nd2nz support.
    """
    return tbe.common.platform.platform_info.intrinsic_check_support(INTRINSIC_DATA_MOVE_OUT2L1_ND2NZ)


def is_v310_soc():
    """
    check v310 soc.
    """
    soc_version = tbe.common.platform.platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in ["Ascend610Lite", "BS9SX2A", "MC61AM21A"]:
        return True

    return False


def is_l0a_layout_zn():
    """
    Check if l0a_layout zn (k1, m1, m0, k0)
    """
    return tbe.common.platform.platform_info.get_soc_spec(L0A_LAYOUT_IS_ZN)


def winograd_conv_flag_judge(wino_params):
    """
    check winograd support
    """
    h_in, w_in, c_in, c_out, h_k, w_k, pad_top, pad_bottom, pad_left, pad_right, \
        stride_h, stride_w, dilate_h, dilate_w, groups, input_dtype, dynamic_flag = wino_params

    pad_support_list = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (2, 0)]
    h_in_padded = h_in + pad_top + pad_bottom
    w_in_padded = w_in + pad_left + pad_right
    w_k_dilated = (w_k - 1) * dilate_w + 1
    w_out = (w_in_padded - w_k_dilated) // stride_w + 1
    if dynamic_flag:
        return False
    l0a_size = int(platform.platform_info.get_soc_spec("l0_a_size"))
    l0c_size = int(platform.platform_info.get_soc_spec("l0_c_size"))
    config = platform.CUBE_MKN["int8"]
    _, block_size_k, block_size_n = config['mac']
    wo_max_l0a = l0a_size // (WINO_IN_TILE_HW * WINO_IN_TILE_HW * block_size_k * BIT_RATIO_MAP.get("int8"))
    wo_max_l0c = l0c_size // (WINO_OUT_TILE_HW * WINO_OUT_TILE_HW * block_size_n * BIT_RATIO_MAP.get("int32"))
    unsupport_condition_list = [
        input_dtype != "int8",
        not tbe.common.platform.platform_info.intrinsic_check_support(INTRINSIC_WINOGRAD_CONV),
        h_k != WINO_SUPPORT_KH,
        w_k != WINO_SUPPORT_KW,
        groups != 1,
        stride_h != 1,
        stride_w != 1,
        dilate_h != 1,
        dilate_w != 1,
        (pad_top, pad_bottom) not in pad_support_list,
        (pad_left, pad_right) not in pad_support_list,
        c_in < WINO_SUPPORT_MIN_CIN,
        c_out < WINO_SUPPORT_MIN_COUT,
        h_in_padded < WINO_SUPPORT_MIN_H,
        w_in_padded < WINO_SUPPORT_MIN_W,
        h_in_padded * w_in_padded < WINO_SUPPORT_MIN_HW,
        w_out % WINO_SUPPORT_WO != 0,
        w_out > min(wo_max_l0a, wo_max_l0c) * WINO_OUT_TILE_HW
    ]
    return True not in unsupport_condition_list


def get_winograd_conv_flag(params):
    """
    input: params contains inputs, weights, bias, offset_w, outputs, strides, pads,
                           dilations, groups, data_format, offset_x, kernel_name
    output: winograd_conv_flag(bool)
    ------------------
    judge if support winograd conv
    """
    inputs, weights, bias, _, _, strides, pads, dilations, groups, data_format, _, _ = params

    bias_shape = [] if bias is None else bias.get("ori_shape")
    dynamic_flag = check_dynamic(inputs.get("ori_shape"), weights.get("ori_shape"), bias_shape)

    pos_h = data_format.find('H')
    pos_w = data_format.find('W')
    stride_h = strides[pos_h]
    stride_w = strides[pos_w]
    dilate_h = dilations[pos_h]
    dilate_w = dilations[pos_w]
    pad_top, pad_bottom, pad_left, pad_right = pads

    fm_ori_format = inputs.get("ori_format")
    filter_ori_format = weights.get("ori_format")
    fm_pos_c = fm_ori_format.find('C')
    fm_pos_h = fm_ori_format.find('H')
    fm_pos_w = fm_ori_format.find('W')
    c_in = inputs.get("ori_shape")[fm_pos_c]
    h_in = inputs.get("ori_shape")[fm_pos_h]
    w_in = inputs.get("ori_shape")[fm_pos_w]
    filter_pos_n = filter_ori_format.find('N')
    filter_pos_h = filter_ori_format.find('H')
    filter_pos_w = filter_ori_format.find('W')
    c_out = weights.get("ori_shape")[filter_pos_n]
    h_k = weights.get("ori_shape")[filter_pos_h]
    w_k = weights.get("ori_shape")[filter_pos_w]

    wino_params = [h_in, w_in, c_in, c_out, h_k, w_k, pad_top, pad_bottom,
                   pad_left, pad_right, stride_h, stride_w, dilate_h, dilate_w,
                   groups, inputs.get("dtype"), dynamic_flag]

    return winograd_conv_flag_judge(wino_params)
    

def check_load3d_w_out_1_support():
    """
    check if current soc version load3d instruction support w_out==1 or not
    only Ascend310 and Hi3796CS support w_out==1
    when fmap_w(with padding) == filters_w(after dilation)
    -------

    Returns
    -------
    True: support
    False: not support
    """
    if is_support_fixpipe():
        return True

    soc_version = tbe.common.platform.platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version in ["Ascend310", "Hi3796CV300CS"]:
        return True
    return False


def get_op_input_dtype(op_dict):
    if op_dict.get("dtype"):
        return op_dict.get("dtype")

    if op_dict.get("data_type"):
        return op_dict.get("data_type")

    raise RuntimeError("key [dtype] or [data_type] not in op dict, get op dtype error")


def get_min_al1_ci1(sparse_4to2_flag, in_c1, kernel_h, kernel_w):
    """
    get min al1 ci1
    """
    if sparse_4to2_flag and in_c1 > 1 and (kernel_h * kernel_w) & 1 != 0:
        # means the min value that needs to be reachable, should be even number.
        return 2
    # common scene, the min value is 1, that is enough.
    return 1


def is_conv2d_binary():
    """
    true: create binary variable shape
    false: dynamic variable shape
    """
    soc_versions_which_supports_binary_mode = ["Ascend310",
                                               "Ascend310B",
                                               "Ascend310P",
                                               "Ascend910",
                                               "Ascend910B",
                                               "Ascend910_93"]
    soc_version = tbe.common.platform.platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version not in soc_versions_which_supports_binary_mode:
        return False

    for op_info in get_context().get_op_info():
        if op_info.op_type != "Conv2D":
            continue

        if not op_info.inputs:
            log.debug("Conv2D: Input not found in op_info")
            return False

        conv_dtype = get_op_input_dtype(op_info.inputs[0])
        if conv_dtype == "float16":
            return True

        if is_support_fixpipe() and conv_dtype in ("float32", "bfloat16"):
            return True

        log.debug("Conv2D input data type is %s, Conv2D does not support binary mode for this dtype.", conv_dtype)
        return False

    return False


def is_pooling_binary():
    soc_versions_which_supports_binary_mode = ["Ascend310B",
                                               "Ascend910B",
                                               "Ascend910_93",
                                               "Ascend310P",
                                               "Ascend910"]
    soc_version = tbe.common.platform.platform_info.get_soc_spec("SHORT_SOC_VERSION")
    if soc_version not in soc_versions_which_supports_binary_mode:
        return False
    for op_info in get_context().get_op_info():
        if op_info.op_type != "Pooling":
            return False

        if not op_info.inputs:
            log.debug("Pooling: Input not found in op_info")
            return False

        pooling_dtype = get_op_input_dtype(op_info.inputs[0])
        if pooling_dtype in ("float16", "float32"):
            return True
        else:
            log.debug("Pooling input dtype is {pooling_dtype}, Pooling does not support binary mode for this dtype.")
            return False
    return False


def create_conv2d_input_shape(inputs):
    shape_out = []
    k0 = CUBE_UNIT
    # variable get order can not be changed
    batch_n = get_variable_by_name(TilingDataKey.BATCH_N, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.BATCH_N))
    c_in = get_variable_by_name(TilingDataKey.C_IN, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.C_IN))
    fmap_h = get_variable_by_name(TilingDataKey.FMAP_H, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.FMAP_H))
    fmap_w = get_variable_by_name(TilingDataKey.FMAP_W, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.FMAP_W))
    c_out = get_variable_by_name(TilingDataKey.C_OUT, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.C_OUT))
    k_h = get_variable_by_name(TilingDataKey.K_H, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.K_H))
    k_w = get_variable_by_name(TilingDataKey.K_W, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.K_W))
    group_opt = get_variable_by_name(TilingDataKey.GROUP_OPT, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.GROUP_OPT))
    cin1_opt = get_variable_by_name(TilingDataKey.CIN1_OPT, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.CIN1_OPT))
    cout1_opt = get_variable_by_name(TilingDataKey.COUT1_OPT, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.COUT1_OPT))
    # create ho wo variable
    _ = get_variable_by_name(TilingDataKey.HO, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.HO))
    _ = get_variable_by_name(TilingDataKey.WO, TILINGDATA_KEY_RANGE_MAP.get(TilingDataKey.WO))

    for i, input_item in enumerate(inputs):
        if i == CONV_FM_IDX:
            if input_item.get("format") != "NC1HWC0":
                raise RuntimeError("conv2d fm input format shuild be NC1HWC0, current is {}".format(
                    input_item.get("format")))
            k0 = platform.CUBE_MKN.get(get_op_input_dtype(input_item)).get("mac")[CUBE_MKN_IDX_K]
            cin_1 = ceil_div(c_in, k0)
            fm_shape = [batch_n, cin_1, fmap_h, fmap_w, k0]
            shape_out.append(fm_shape)
            continue

        if i == CONV_FILTER_IDX:
            if input_item.get("format") != "FRACTAL_Z":
                raise RuntimeError("conv2d filter input format shuild be FRACTAL_Z, current is {}".format(
                    input_item.get("format")))
            weight_fz_shape = [group_opt * cin1_opt * k_h * k_w, cout1_opt, CUBE_UNIT, k0]
            shape_out.append(weight_fz_shape)
            continue

        if i == CONV_BIAS_IDX and input_item.get("shape") != 'NULL':
            bias_shape = [c_out]
            shape_out.append(bias_shape)
            continue

        shape_out.append(input_item.get("shape"))

    return shape_out


def get_variable_by_name(name, bound=None):
    if operation.get_te_var(name):
        return operation.get_te_var(name).get_tvm_var()

    return operation.var(name, bound)


def get_conv_out_shape():
    # get conv out shape as eltwise other inputshape
    n_shape = get_variable_by_name(TilingDataKey.BATCH_N)
    cout = get_variable_by_name(TilingDataKey.C_OUT)
    co1 = (cout + CUBE_UNIT - 1) // CUBE_UNIT
    ho = get_variable_by_name(TilingDataKey.HO)
    wo = get_variable_by_name(TilingDataKey.WO)
    c0 = CUBE_UNIT
    # outputshape shuild be same as shape single conv2d compute defined
    return [n_shape, co1, ho * wo, c0]


def create_broadcast_other_input(idx):
    # creat broadcast other inputshape
    dim0 = get_variable_by_name(TilingDataKey.OTHER_INPUT + "{}_dim0".format(idx), bound=TILINGDATA_KEY_DEFAULT_RANGE)
    dim1 = get_variable_by_name(TilingDataKey.OTHER_INPUT + "{}_dim1".format(idx), bound=TILINGDATA_KEY_DEFAULT_RANGE)
    dim2 = get_variable_by_name(TilingDataKey.OTHER_INPUT + "{}_dim2".format(idx), bound=TILINGDATA_KEY_DEFAULT_RANGE)
    c0 = CUBE_UNIT

    return [dim0, dim1, dim2, c0]


def replace_conv2d_vector_tvm_shapes(vector_inputs, ins_attrs_options):
    def get_broadcast_fusion_flag():
        ins, _, options = ins_attrs_options
        if options is None or not options:
            raise RuntimeError("get conv2d options is None or empty")
        if len(options) != 1:
            raise RuntimeError("conv2d options len should be 1, current is {}".format(len(options)))

        return options[0].get('options').get(BinaryInfoKey.BROADCAST_FLAG, False)

    res_shapes = []
    eltwise_dims = ELTWISE_FUSION_OTHER_INPUT_SHAPE_DIMS
    broadcast_fusion_flag = get_broadcast_fusion_flag()
    log.debug("current compile broadcast_fusion_flag:{}".format(broadcast_fusion_flag))
    for idx, vec_input in enumerate(vector_inputs):
        shape = vec_input.get("shape")
        shape_dims = len(shape)
        if shape_dims != 1 and shape_dims != eltwise_dims:
            raise RuntimeError("only 1 or 4 dim shape of eltwise other input is supported, "
                                "current is {}".format(shape_dims))
        if shape_dims == 1:
            if shape[0] == -1:  # channel wise input
                out_shape = [get_variable_by_name(TilingDataKey.C_OUT)]
            elif shape[0] == -2:
                # ub fusion pass only support eltwise fusion
                out_shape = get_conv_out_shape()
            else:
                out_shape = shape  # scalar input

        if shape_dims == eltwise_dims:
            if broadcast_fusion_flag:
                out_shape = create_broadcast_other_input(idx)
            else:
                out_shape = get_conv_out_shape()

        res_shapes.append(out_shape)
    return res_shapes


def update_l1_size(l1_size):
    if is_support_v300():
        extra_buffers = EXTRA_BUFFERS_V300
    else:
        extra_buffers = EXTRA_BUFFERS_V220
    for buffer in extra_buffers:
        try:
            buffer_size = int(tbe.common.platform.platform_info.get_soc_spec(buffer))
        except Exception as e:
            log.warn("Failed to get_soc_spec(%s). err is %s" % (buffer, e))
            buffer_size = 0
        l1_size -= buffer_size
    return l1_size


def check_splitw_l1_size_invalid(conv_params):
    h_in, w_in, w_out, strideh, stridew, hk_dilation, wk_dilation, w_dtype, c0_optim_flag = conv_params
    config = platform.CUBE_MKN[w_dtype]
    ci0 = config['mac'][1]
    ho_upper = ceil_div(config['mac'][0], w_out)
    hi_infer_by_ho = min(h_in, ((ho_upper - 1) * strideh + hk_dilation))
    wi_infer_by_wo = min(w_in, ((config['mac'][0] - 1) * stridew + wk_dilation))
    max_feature_map_l1 = ci0 * hi_infer_by_ho * wi_infer_by_wo * m_bit_ratio.get(w_dtype)
    l1_buffer_size = tbe.common.platform.platform_info.get_soc_spec("L1_SIZE")
    l1_buffer_size = update_l1_size(l1_buffer_size)
    if max_feature_map_l1 > l1_buffer_size:
        return True
    return False


def check_l1_size_invalid(conv_params):
    h_in, w_in, w_out, strideh, stridew, hk_dilation, wk_dilation, w_dtype, c0_optim_flag = conv_params
    if w_out == 0:
        err_man.raise_err_message_cube("The value zero of w_out is not supported !")
    config = platform.CUBE_MKN[w_dtype]
    ci0 = config['mac'][1]
    if c0_optim_flag:
        ci0 = 4
    ho_upper = math.floor(config['mac'][0] / w_out) + 2  # 2: extra lines loads in l1
    tmp = min(h_in, ((ho_upper - 1) * strideh + hk_dilation)) * w_in
    max_feature_map_l1 = ci0 * tmp * BIT_RATIO_MAP.get(w_dtype)
    l1_buffer_size = tbe.common.platform.platform_info.get_soc_spec("L1_SIZE")
    l1_buffer_size = update_l1_size(l1_buffer_size)
    if max_feature_map_l1 > l1_buffer_size:
        return True
    return False


def check_dynamic(shape_fm: list, shape_weight: list, shape_bias: list) -> bool:
    '''
    get dynamic flag
    '''
    if shape_fm is not None and (-1 in shape_fm or -2 in shape_fm):
        return True
    if shape_weight is not None and (-1 in shape_weight or -2 in shape_weight):
        return True
    if shape_bias is not None and (-1 in shape_bias or -2 in shape_bias):
        return True
    return False


def check_load3dv2_postk_params_invalid(h_k, w_k, dtype):
    config = platform.CUBE_MKN[dtype]
    block_size_k = config['mac'][1]
    last_post_k_value = h_k * w_k * block_size_k
    if last_post_k_value > MAX_POSTK_VALUE:
        return True
    return False


def check_support_db_fold():
    db_fold_soc = ("Ascend910", "Ascend310P", "Ascend910B", "Ascend310B", "Ascend910_93")
    if tbe.common.platform.platform_info.get_soc_spec("Short_SoC_version") not in db_fold_soc:
        log.debug("The current SoC not support db fold.")
        return False

    if not is_conv2d_binary() and not is_pooling_binary():
        log.debug("The current op is not in binary mode, not support db fold.")
        return False

    log.debug("Support db fold.")
    return True


def parse_binary_tilingdata(binary_tiling_data):
    tiling_data_len = int(len(binary_tiling_data) / INT32_BIT_SIZE)
    num_tiling_data_key = len(TILINGDATA_KEY_MAP)

    if tiling_data_len != num_tiling_data_key:
        raise RuntimeError("Get unexpected size of tiling_data, {} excepted, but got {}!"
                            .format(num_tiling_data_key, tiling_data_len))
    tiling_list = []
    for i in range(tiling_data_len):
        decimal_tiling_data = int.from_bytes(
            binary_tiling_data[i * INT32_BIT_SIZE : (i + 1) * INT32_BIT_SIZE], "little")
        tiling_list.append(decimal_tiling_data)
    return tiling_list


def parse_tilingdata_map(binary_tiling_data):
    tiling_map = copy.deepcopy(TILINGDATA_KEY_RANGE_MAP)
    for i in range(len(binary_tiling_data)):
        tiling_attr_key = TILINGDATA_KEY_MAP.get(i, None)
        if tiling_attr_key in tiling_map:
            tiling_map[tiling_attr_key] = [binary_tiling_data[i], binary_tiling_data[i]]
    return tiling_map


def get_hardware_info():
    """
    get hardware info of platform and save in a dict

    Notice
    ----------
    NA

    Parameters
    ----------
    NA

    Returns
    -------
    hardware_info_dict: dict of hardware information
    """
    hardware_info_dict = {}
    # keys used by get_soc_spec(key) whose value should be convert to int
    platform_info_name_int = ["ai_core_cnt", "l2_size",
                              "l1_size", "l0_a_size",
                              "l0_b_size", "l0_c_size",
                              "ub_size", "bt_size",
                              "ddr_read_rate", "ddr_write_rate",
                              "l2_rate", "l2_read_rate",
                              "l2_write_rate", "l1_to_l0_a_rate",
                              "l1_to_l0_b_rate", "l1_to_ub_rate",
                              "l0_c_to_ub_rate", "ub_to_l2_rate",
                              "ub_to_ddr_rate", "ub_to_l1_rate"]
    for platform_key in platform_info_name_int:
        if platform.platform_info.get_soc_spec(platform_key) is None:
            raise RuntimeError("get_soc_spec {} failed!".format(platform_key))

        if platform.platform_info.get_soc_spec(platform_key) == "unknown":
            hardware_info_dict[platform_key] = 0
            continue

        if platform_key == "ai_core_cnt":
            hardware_info_dict["aicore_num"] = int(platform.platform_info.get_soc_spec(platform_key))
            continue

        hardware_info_dict[platform_key] = int(platform.platform_info.get_soc_spec(platform_key))

    hardware_info_dict["cube_vector_split_bool"] = \
        platform.platform_info.get_soc_spec("cube_vector_combine").startswith("split")
    hardware_info_dict["soc_version"] = platform.platform_info.get_soc_spec("SoC_version")
    hardware_info_dict["short_soc_version"] = platform.platform_info.get_soc_spec("Short_SoC_version")
    hardware_info_dict["cube_bandwidth"] = 0
    hardware_info_dict["vector_bandwidth"] = 0
    return hardware_info_dict


def get_ub_fusion_utilize(fused_element_wise_list, fused_channel_wise_list, conv_param):
    """
    Get the number of pre/post fusion ub utilize and save in a dict.

    Notice
    ----------
    NA

    Parameters
    ----------
    fused_element_wise_list: list
        fused element-wise coefficients from tiling dict
    fused_channel_wise_list: list
        fused channel-wise coefficients from tiling dict
    conv_param: ConvParam
        conv information from compute

    Returns
    -------
    ub_utilize_dict: dict
    """
    pre_fusion_str_prefix = "pre_fusion_ub"
    post_fusion_str_prefix = "post_fusion_ub"
    if conv_param.broadcast_fusion_flag:
        pre_fusion_str_prefix += "_broadcast"
        post_fusion_str_prefix += "_broadcast"
    else:
        pre_fusion_str_prefix += "_elementwise"
        post_fusion_str_prefix += "_elementwise"
    if conv_param.v200_width_out_1_flag:
        pre_fusion_str_prefix += "_nx1"
        post_fusion_str_prefix += "_nx1"

    # tuple content: (key in return dict, index of fused_coefficient_list)
    fusion_ub_list = [("pre_fusion_ub_utilize", AUB_COEFFICIENT_INDEX),
                      ("post_fusion_ub_utilize", CUB_COEFFICIENT_INDEX)]
    if conv_param.binary_static_flag and conv_param.broadcast_fusion_flag:
        fusion_ub_list = [("pre_fusion_ub_utilize", AUB_COEFFICIENT_INDEX),
                          ("post_fusion_ub_utilize", CUB_COEFFICIENT_INDEX),
                          (pre_fusion_str_prefix, AUB_COEFFICIENT_INDEX),
                          (post_fusion_str_prefix, CUB_COEFFICIENT_INDEX)]
    elif conv_param.broadcast_fusion_flag or conv_param.v200_width_out_1_flag:
        fusion_ub_list = [(pre_fusion_str_prefix, AUB_COEFFICIENT_INDEX),
                          (post_fusion_str_prefix, CUB_COEFFICIENT_INDEX)]

    ub_utilize_dict = {}
    # add element-wise/broadcast info
    for dict_key, index in fusion_ub_list:
        if index >= len(fused_element_wise_list):
            raise RuntimeError("Get utilize info failed. The index should not be larger than fused_element_wise_list.")
        ub_utilize_dict[dict_key] = fused_element_wise_list[index]
    # add channel-wise info
    if CUB_COEFFICIENT_INDEX >= len(fused_channel_wise_list):
        raise RuntimeError("Get utilize info failed. The index should not be larger than fused_channel_wise_list.")
    ub_utilize_dict["post_fusion_ub_channelwise"] = fused_channel_wise_list[CUB_COEFFICIENT_INDEX]

    return ub_utilize_dict


def get_vec_fusion_utilize(tiling_dict):
    """
    get the number of pre/post fusion vector utilize and save in a dict

    Notice
    ----------
    NA

    Parameters
    ----------
    tiling_dict: dict

    Returns
    -------
    vec_utilize_dict: dict of pre/post fusion utilize
    """
    pre_vec_util, post_vec_util = tiling_dict.get("fusion_vector_utilize", [0, 0])
    vec_utilize_dict = {"pre_fusion_vector_utilize": pre_vec_util,
                        "post_fusion_vector_utilize": post_vec_util}
    return vec_utilize_dict


def add_tiling_compile_info(tiling_dict, conv_param, static_op_flag=False):
    """
    Get and add info that optiling required into optiling compile info.

    Notice
    ----------
    NA

    Parameters
    ----------
    tiling_dict: dict
        info_dict from schedule
    conv_param: ConvParam
        conv information from compute
    static_op_flag: bool
        True indicate supports static operators

    Returns
    -------
    NA
    """
    if static_op_flag:
        add_compile_info(Conv2dCompileInfoKey.FMAP_C1, tiling_dict.get("fm_shape")[1])
    if not tiling_dict.get(Conv2dTilingInfoDictKey.CACHE_TILING_FLAG, False):
        add_compile_info(Conv2dCompileInfoKey.FMAP_C1, conv_param.dim_map.get("fmap_5hd_shape")[1])
    # ---------- Add op type info ----------
    op_info_list = get_context().get_op_info()
    op_type_list = []
    conv_op_type_list = []
    for info in op_info_list:
        op_type_list.append(info.op_type)
        if info.pattern != "Convolution":
            continue
        conv_op_type_list.append(info.op_type)
    # For distinguishing the compile info of ops whose pattern is "Convolution"
    if len(conv_op_type_list) > 0:
        add_compile_info(Conv2dCompileInfoKey.CONV_OP_TYPE_LIST, conv_op_type_list)
    else:
        add_compile_info(Conv2dCompileInfoKey.OP_TYPE_LIST, op_type_list)
    # add compile info to check is support fixpipe or not
    add_compile_info(Conv2dCompileInfoKey.IS_SUPPORT_FIXPIPE, is_support_fixpipe())

    # ---------- Add feature flag into compile_info ---------
    if conv_param.binary_static_flag:
        current_feature_flag = {
            "load3d_flag": True,
            "load2d_flag": conv_param.l0a_load2d_flag,
            "dma_flag": conv_param.l0a_dma_flag,
            "conv1d_flag": conv_param.conv1d_split_w_flag,
            "groupopt_flag": conv_param.groupopt_flag
        }
        add_compile_info('tiling_type', 'binary')
    else:
        current_feature_flag = {
            "load2d_flag": conv_param.l0a_load2d_flag,
            "dma_flag": conv_param.l0a_dma_flag,
            "conv1d_flag": conv_param.conv1d_split_w_flag,
            "groupopt_flag": conv_param.groupopt_flag
        }
    add_compile_info(Conv2dCompileInfoKey.CURRENT_FEATURE_FLAG, current_feature_flag)

    if static_op_flag:
        compile_get_tiling_flag = False
        # ------ static operator only support single operator -------
        utilize_dict = dict()
        utilize_dict["pre_fusion_ub_utilize"] = 0
        utilize_dict["post_fusion_ub_utilize"] = 0
        utilize_dict["pre_fusion_ub_elementwise_nx1"] = 0
        utilize_dict["post_fusion_ub_elementwise_nx1"] = 0
        utilize_dict["pre_fusion_ub_broadcast"] = 0
        utilize_dict["post_fusion_ub_broadcast"] = 0
        utilize_dict["pre_fusion_ub_broadcast_nx1"] = 0
        utilize_dict["post_fusion_ub_broadcast_nx1"] = 0
        utilize_dict["post_fusion_ub_channelwise"] = 0
        utilize_dict["pre_fusion_vector_utilize"] = 0
        utilize_dict["post_fusion_vector_utilize"] = 0
        add_compile_info(Conv2dCompileInfoKey.FUSION_UTILIZE, utilize_dict)
        add_compile_info(Conv2dCompileInfoKey.UB_FUSION_PATTERN, "")
        # To identify the procedure called by the static shape, set the tiling type
        add_compile_info(Conv2dCompileInfoKey.TILING_TYPE, "binary")
    else:
        compile_get_tiling_flag = False
        # ---------- Add fusion info into compile_info ----------
        utilize_dict = get_compile_info().get(Conv2dCompileInfoKey.FUSION_UTILIZE, {})
        utilize_dict.update(get_ub_fusion_utilize(tiling_dict.get(Conv2dTilingInfoDictKey.FUSED_COEFFICIENT),
            tiling_dict.get(Conv2dTilingInfoDictKey.FUSED_CHANNEL_WISE), conv_param))
        utilize_dict.update(get_vec_fusion_utilize(tiling_dict))
        add_compile_info(Conv2dCompileInfoKey.FUSION_UTILIZE, utilize_dict)
        fusion_ub_pattern = get_compile_info().get(Conv2dCompileInfoKey.UB_FUSION_PATTERN, "")
        if fusion_ub_pattern != "broadcast":
            add_compile_info(Conv2dCompileInfoKey.UB_FUSION_PATTERN,
                            "broadcast" if conv_param.broadcast_fusion_flag else "elementwise")
    add_compile_info(Conv2dCompileInfoKey.COMPILE_GET_TILING_FLAG, compile_get_tiling_flag)

    # ---------- Add hardware info into compile_info
    hardware_info_dict = get_hardware_info()
    add_compile_info(Conv2dCompileInfoKey.HARDWARE_INFO, hardware_info_dict)

    # ---------- Add conv2d input num into compile_info ----------
    conv2d_input_bit = 12  # feature map and filter is necessary (0b1100)
    bias_tensor = conv_param.para_dict.get(Conv2dParaDictKey.BIAS_TENSOR, None)
    offset_w_tensor = conv_param.para_dict.get(Conv2dParaDictKey.OFFSET_W_TENSOR, None)
    if isinstance(bias_tensor, Tensor):
        conv2d_input_bit |= 1 << 1  # bias use the 2nd bit
    if isinstance(offset_w_tensor, Tensor):
        conv2d_input_bit |= 1  # offset_w use the 1st bit
    add_compile_info(Conv2dCompileInfoKey.CONV2D_INPUT, conv2d_input_bit)


def check_range_illegal(value, lower_limit, upper_limit, dim_str):
    # check if the value is illegal(out of range)
    if isinstance(value, list):
        for i in value:
            if i < lower_limit or i > upper_limit:
                err_man.raise_err_specific_user(
                    "conv2d", "{} only support range [{}, {}], but given = {}"
                    .format(dim_str, str(lower_limit), str(upper_limit), str(value)))
        return
    if value < lower_limit or value > upper_limit:
        err_man.raise_err_specific_user(
            "conv2d", "{} only support range [{}, {}], but given = {}"
            .format(dim_str, str(lower_limit), str(upper_limit), str(value)))