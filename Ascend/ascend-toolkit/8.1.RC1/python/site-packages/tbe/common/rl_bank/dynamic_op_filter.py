#!/usr/bin/env python3
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
op filter for static and dynamic auto_search
"""
import functools

from tbe.common.utils import log

DYNC_OUTPUT_DTYPE_WHITE_LIST = {"float16", "float32", "int8", "uint8", "bfloat16"}
DYNC_OP_WHITE_LIST = {
    "Abs",
    "Add",
    "AddN",
    "Atan2",
    "Axpy",
    "BiasAdd",
    "BiasAddGrad",
    "BinaryCrossEntropy",
    "BinaryCrossEntropyGrad",
    "BNInfer",
    "BNInferGrad",
    "BNTrainingReduce",
    "BNTrainingReduceGrad",
    "BNTrainingUpdate",
    "BNTrainingUpdateGrad",
    "BroadcastTo",
    "ClipByNormNoDivSum",
    "ClipByValue",
    "Cos",
    "Div",
    "DivNoNan",
    "Elu",
    "EluGrad",
    "Equal",
    "Exp",
    "Floor",
    "FloorDiv",
    "FloorMod",
    "Gelu",
    "GeluGrad",
    "Greater",
    "GreaterEqual",
    "L2Loss",
    "LayerNormXBackpropV2",
    "LeakyReluGrad",
    "LeakyRelu",
    "Less",
    "LessEqual",
    "Log",
    "Log1p",
    "LogicalOr",
    "LogicalNot",
    "LogSoftmaxGrad",
    "LogSoftmaxV2",
    "MaxPool",
    "MaxPool3D",
    "MaxPoolV3",
    "Maximum",
    "MaximumGrad",
    "Minimum",
    "MinimumGrad",
    "Mul",
    "Muls",
    "Neg",
    "NotEqual",
    "Pow",
    "PRelu",
    "RealDiv",
    "Reciprocal",
    "ReduceAll",
    "ReduceAny",
    "ReduceMean",
    "ReduceMin",
    "ReduceMaxD",
    "ReduceMeanD",
    "ReduceSumD",
    "Relu",
    "ReluGradV2",
    "Rsqrt",
    "RsqrtGrad",
    "Sigmoid",
    "Sign",
    "SoftmaxV2",
    "Softplus",
    "SqrtGrad",
    "Square",
    "SquaredDifference",
    "SquareSumV1",
    "Sub",
    "Tanh",
    "TanhGrad",
    "TileD",
    "AccumulateNV2",
    "Adds",
    "DiagPartD",
    "LogicalAnd",
    "ReduceAllD",
    "ReduceMinD",
    "Round",
    "SoftmaxCrossEntropyWithLogits",
}
DYNC_OP_BLACK_LIST = {
    "antiquant",
    "ascenddequant",
    "ascendquant",
    "batchmatmul",
    "batchmatmulv2",
    "batchnorm",
    "batchnormext2",
    "batchnormgrad",
    "bninferconv2dbackpropinputd",
    "bninference",
    "bninferenced",
    "bninfergrad",
    "concat",
    "concatd",
    "conv2d",
    "conv2dbackpropfilterd",
    "convolution",
    "crop",
    "deconvolution",
    "depthwiseconv2d",
    "depthwiseconv2dbackpropfilterd",
    "depthwiseconv2dbackpropinputd",
    "dequant",
    "dropout",
    "dynamicgru",
    "dynamicgruv2",
    "dynamicgruv2hidden",
    "dynamiclstmv2",
    "flatten",
    "fsrdetectionoutput",
    "fullyconnection",
    "gather",
    "gathernd",
    "gatherv2",
    "gatherv2d",
    "gaussian",
    "innerproduct",
    "lrn",
    "matmul",
    "matmulv2",
    "maxpool",
    "msra",
    "pooling",
    "priorboxdv2",
    "proposal",
    "proposald",
    "psroipooling",
    "quant",
    "reshape",
    "roipooling",
    "select",
    "shufflechannel",
    "slice",
    "ssddecodebbox",
    "ssddetectionoutput",
    "transdata",
    "transposed",
    "upsample",
    "xavier",
    "yolo",
    "yolov2detectionoutputd",
    "yolov3detectionoutputv2d",
    "Fills",
    "OnesLike",
    "ZerosLike"
}
DYNC_FUSION_OP_BLACK_LIST = []
DYNC_OP_CONFIG = {
    "all_op_tuning_switch": "off"
}


def singleton(cls):
    _instance = {}

    @functools.wraps(cls)
    def wrapper(*args, **kwargs):
        if cls not in _instance:
            _instance[cls] = cls(*args, **kwargs)
        return _instance[cls]
    return wrapper


@singleton
class DynamicOpFilter:
    """
    class for filtering dynamic op by black_list and white_list
    """
    def __init__(self):
        self.op_config_info = DYNC_OP_CONFIG
        self.tune_all_op = self._need_to_tune_all_op()

        self.output_dtype_white_list = {output_dtype.lower() for output_dtype in DYNC_OUTPUT_DTYPE_WHITE_LIST}

        self.single_op_black_list = {op_type.lower() for op_type in DYNC_OP_BLACK_LIST}
        self.single_op_white_list = {op_type.lower() for op_type in DYNC_OP_WHITE_LIST}

        self.fusion_op_black_list = ["_".join(
            sorted((op_type.lower() for op_type in fusion_op_rule))) for fusion_op_rule in DYNC_FUSION_OP_BLACK_LIST]

    def optype_check_support(self, op_type_list: list, kernel_name: str) -> bool:
        """
        check whether op is supported by auto_search tune, called in the search_bank.py
        :param op_type_list: op's type from context
        :param kernel_name: op's kernel_name from context
        :return: is_supported
        """
        if not op_type_list:
            log.warn("Auto search dynamic filter info: op_type_list is empty, optype_check_support return not support.")
            return False

        if not isinstance(op_type_list[0], str):
            log.warn("Auto search dynamic filter info: op_type format is not str, "
                     "optype_check_support return not support.")
            return False

        if len(op_type_list) == 1:
            is_supported = self._check_single_optype_support(op_type_list[0])
        else:
            is_supported = self._check_fusion_optype_support(op_type_list, kernel_name)
        return is_supported

    def check_op_support(self, json_dict: dict) -> bool:
        """
        check whether op is supported by auto_search tune, called in the tuning_utils.py
        :param json_dict:
        :return: is_supported
        """
        scope_id = int(json_dict.get("scope_id"))
        if scope_id < 0:
            is_supported = self._check_single_op_support(json_dict)
        else:
            is_supported = self._check_fusion_op_support(json_dict)
        log.info("Auto search dynamic filter info: op %s is supported or not by auto_search tune: %s",
                  json_dict.get("fusion_op_name"), str(is_supported))
        return is_supported

    def _check_single_optype_support(self, op_type: str) -> bool:
        """
        check single op_type by white_list and black_list
        priority:
        1.single_op_black_list -> False
        2.tune_all_op: all_op_tuning_switch is on -> True
        3.single_op_white_list -> True
        :param op_typeï¼š op's type loaded from op json(fusion_op_te_xxx.json) or get from context
        :return: is_supported
        """
        op_type_lower = op_type.lower()
        if op_type_lower in self.single_op_black_list:
            log.info("Auto search dynamic filter info: op type [%s] in op blacklist, auto_search tune not support!",
                     op_type)
            return False

        if self.tune_all_op:
            # if all_op_tuning_switch is on, then tune all op
            log.info("Auto search dynamic filter info: "
                     "op type [%s] is enabled by all_op_tuning_switch=on in op_config, tuning all op.", op_type)
            return True

        # if all_op_tuning_switch is off, then filter single op by white list
        if op_type_lower in self.single_op_white_list:
            log.debug("Auto search dynamic filter info: op type [%s] in op white_list, "
                      "auto_search tune support!", op_type)
        else:
            log.info("Auto search dynamic filter info: op type [%s] not in op white_list, "
                     "auto_search tune not support!", op_type)
            return False
        return True

    def _check_fusion_optype_support(self, op_type_list: list, kernel_name: str) -> bool:
        """
        check fusion op_type by black_list
        priority:
        1.single_op_black_list -> False
        2.fusion_op_black_list -> False
        :param op_type_list: op's type loaded from op json(fusion_op_te_xxx.json) or get from context
        :param kernel_name: op's kernel_name loaded from op json(fusion_op_te_xxx.json) or get from context
        :return: is_supported
        """
        op_type_lower = []
        for op_type in op_type_list:
            # if one of fusion ops in black_list, will not supported by auto_search tune
            op_type_lower.append(op_type.lower())
            if op_type.lower() in self.single_op_black_list:
                log.info("Auto search dynamic filter info: fusion op %s is not supported.", kernel_name)
                return False

        # if fusion ops in fusion_op_black_list, will not supported by auto_search tune
        op_type_join = "_".join(sorted(op_type_lower))
        for fused_op_black_rule in self.fusion_op_black_list:
            if fused_op_black_rule in op_type_join:
                log.info("Auto search dynamic filter info: fusion op %s is not supported.", kernel_name)
                return False
        return True

    def _check_single_op_support(self, json_dict: dict) -> bool:
        """
        check single op by whitelist and blacklist
        1.single optype
        2.output_dtype
        :param json_dict: op's json dict info loaded from op json(fusion_op_te_xxx.json)
        :return: is_supported
        """
        kernel_name = json_dict.get("fusion_op_name")
        node_list = json_dict.get("op_list")
        input_name_list = []
        all_output_info = []
        op_type = ""
        for node_info in node_list:
            if node_info.get("type") == "Data":
                continue
            all_output_info.extend(node_info.get("output_desc"))
            input_name_list.extend((x.get("name") for x in node_info.get("input_desc")))
            op_type = node_info.get("type")

        optype_is_supported = self._check_single_optype_support(op_type)
        dtype_is_supported = self._check_output_dtype_support(all_output_info, input_name_list, kernel_name)
        return optype_is_supported and dtype_is_supported

    def _check_fusion_op_support(self, json_dict: dict) -> bool:
        """
        check fusion op by blacklist
        1.single optype
        2.output_dtype
        :param json_dict: op's json dict info loaded from op json(fusion_op_te_xxx.json)
        :return: is_supported
        """
        kernel_name = json_dict.get("fusion_op_name")
        node_list = json_dict.get("op_list")
        input_name_list = []
        all_output_info = []
        op_type_list = []
        for node_info in node_list:
            if node_info.get("type") == "Data":
                continue
            op_type_list.append(node_info.get("type"))
            all_output_info.extend(node_info.get("output_desc"))
            input_name_list.extend((x.get("name") for x in node_info.get("input_desc")))

        optype_is_supported = self._check_fusion_optype_support(op_type_list, kernel_name)
        dtype_is_supported = self._check_output_dtype_support(all_output_info, input_name_list, kernel_name)
        return optype_is_supported and dtype_is_supported

    def _need_to_tune_all_op(self):
        """
        parse all_op_tuning_switch in config,
        all_op_tuning_switch only support "on" and "off"
        """
        tune_all_op = False
        if self.op_config_info and self.op_config_info.get("all_op_tuning_switch"):
            all_op_tuning_switch = str(self.op_config_info.get("all_op_tuning_switch")).lower()
            if all_op_tuning_switch not in {"on", "off"}:
                log.warn("Auto search dynamic filter info: all_op_tuning_switch: %s is not supported, "
                         "only support \"on\" and \"off\"", str(all_op_tuning_switch))
            tune_all_op = (all_op_tuning_switch == "on")
        log.info("Auto search dynamic filter info: tune_all_op is %s.", str(tune_all_op))
        return tune_all_op

    def _check_output_dtype_support(self, all_output_info: list, input_name_list: list, kernel_name: str) -> bool:
        """
        check op's output dtype
        """
        for output_info in all_output_info:
            # inter output, not real op output
            if output_info.get("name") in input_name_list:
                continue
            if output_info.get("data_type") not in self.output_dtype_white_list:
                log.info("Auto search dynamic filter info: op %s output_dtype is %s, not in %s, "
                         "auto_search tune not support!",
                         kernel_name, output_info.get("data_type"), str(self.output_dtype_white_list))
                return False
        return True


dynamic_op_filter = DynamicOpFilter()
