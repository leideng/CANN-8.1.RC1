#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
TBE operator param encoder
"""
import json
import math
from tvm import deepcopy

from tbe.common.tiling.tiling_api_internal_use import is_support_fixpipe_flatform
from tbe.common.utils.errormgr.error_manager_cube import raise_err_message_cube
from tbe.common.tiling.op_param_encode.operator_params_encoder import (
    BaseClassParamsEncoder
)

# define const value
CONST_VALUE0 = 0
CONST_VALUE1 = 1

# define length of shape
SHAPE_LENGHT2 = 2
SHAPE_LENGHT4 = 4
SHAPE_LENGHT5 = 5

OP_TYPE = "matmul"
MAX_UINT32 = 4294967295
MAX_UINT16 = 65535


class MatmulParamsEncoder(BaseClassParamsEncoder):
    """
    Child class for matmul Params Encoder
    """

    def __init__(self):
        super().__init__()
        self.input_args = {}
        self.flag_new_tiling = False

    @staticmethod
    def get_kb_query_attr(info_dict):
        """
        get typical key for repository query

        Parameters
        ----------
        info_dict: input params

        Returns
        ----------
        attr_dict: input params string
        """
        attr_dict = {"op_type": "matmul", "A_shape": [], "B_shape": [], "C_shape": [],
                     "A_dtype": "float16", "B_dtype": "float16", "C_dtype": "float16", "mad_dtype": "float32",
                     "padl": 0, "padr": 0, "padu": 0, "padd": 0, "strideH": 1, "strideW": 1,
                     "strideH_expand": 1, "strideW_expand": 1, "dilationH": 1, "dilationW": 1,
                     "group": 1, "bias_flag": False, "fused_double_operand_num": 0}

        attr_dict_static = {"shape_a_align": 1, "shape_b_align": 1, "scalar_size": 0,
                            "batch_type": 0, "reduce_fusion": 0}
        attr_dict.update(attr_dict_static)

        for key, value in info_dict.items():
            if key in attr_dict:
                attr_dict[key] = value

        if "bias_dtype" in info_dict:
            attr_dict.update({"bias_dtype": info_dict.get("bias_dtype")})
        if "fixpipe_flag" in info_dict:
            attr_dict.update({"fixpipe_flag": info_dict.get("fixpipe_flag")})
        return json.dumps(attr_dict)

    def encode_array(self, input_args):
        """
        encode the input params to NDArray

        Parameters
        ----------
        input_args: input params

        Returns
        ----------
        NDArray: tvm.nd.array
        """
        self.flag_new_tiling = is_support_fixpipe_flatform(input_args.get("op_type"))
        params_in = deepcopy(input_args)
        self.input_args = params_in
        # check params
        self.check_info_dict(params_in)
        # preprocess params
        self.preprocess_info_dict(params_in)

        return self.encode(params_in)

    def decode(self, tiling_encode):
        """
        decode the tiling from _get_tiling()

        Parameters
        ----------
        tiling_encode: str, tiling from _get_tiling()

        Returns
        ----------
        tiling: dict or list, decoded tiling
        """
        if not tiling_encode:
            if self.input_args.get("dynamic_shape_flag", False):
                # continue to cost-model tiling
                return []
            raise_err_message_cube(
                "only support legal tiling, "
                "but the return value of tiling is [%s]." % tiling_encode
            )

        tiling = json.loads(tiling_encode)

        if isinstance(tiling, list):
            self.dynamic_decode(tiling)

        return tiling

    def dynamic_decode(self, tilings):
        """
        decode for dynamic shape tiling

        Parameters
        ----------
        tiling: list, tiling results

        Returns
        """
        for idx, _ in enumerate(tilings):
            # shape
            tilings[idx]["C_shape"] = self.input_args["C_shape"]
            tiling_type = self.input_args.get("tiling_type")
            if tiling_type == "cost_model_tiling":
                tilings[idx]["pad"] = [
                    self.input_args["padl"],
                    self.input_args["padr"],
                    self.input_args["padu"],
                    self.input_args["padd"]
                ]

    def check_info_dict(self, params_in):
        """
        check the type, length and support-range of input params

        Parameters
        ----------
        params_in: input params

        Returns
        """
        # check param types
        self.check_param_type(params_in, [dict])
        self.check_param_type(params_in.get("op_type", OP_TYPE), [str])
        self.check_param_type(params_in.get("A_shape"), [list])
        self.check_param_type(params_in.get("B_shape"), [list])
        c_shape = params_in.get("C_shape")
        if c_shape is not None:
            self.check_param_type(c_shape, [list])

        self.check_param_type(params_in.get("A_dtype"), [str])
        self.check_param_type(params_in.get("B_dtype"), [str])
        self.check_param_type(params_in.get("C_dtype"), [str])
        self.check_param_type(params_in.get("mad_dtype"), [str])
        self.check_param_type(params_in.get("bias_dtype", "float16"), [str])

        self.check_param_type(params_in.get("padl"), [int])
        self.check_param_type(params_in.get("padr"), [int])
        self.check_param_type(params_in.get("padu"), [int])
        self.check_param_type(params_in.get("padd"), [int])
        self.check_param_type(params_in.get("strideH"), [int])
        self.check_param_type(params_in.get("strideW"), [int])
        self.check_param_type(params_in.get("strideH_expand"), [int])
        self.check_param_type(params_in.get("strideW_expand"), [int])
        self.check_param_type(params_in.get("dilationH"), [int])
        self.check_param_type(params_in.get("dilationW"), [int])
        self.check_param_type(params_in.get("group", CONST_VALUE1), [int])
        self.check_param_type(params_in.get("reduce_fusion", CONST_VALUE0), [int])
        self.check_param_type(params_in.get("bias_flag", False), [bool, int])
        self.check_param_type(params_in.get("batch_type", CONST_VALUE0), [int])
        self.check_param_type(params_in.get("fixpipe_flag", CONST_VALUE0), [int])

        self.check_param_type(params_in.get(
            "fused_double_operand_num"), [int, float])

        self.check_param_type(params_in.get(
            "kernel_name", OP_TYPE + "_kernel"), [str])

        # check length of params
        self.check_param_length(params_in.get("A_shape"), [SHAPE_LENGHT5])
        self.check_param_length(params_in.get("B_shape"), [SHAPE_LENGHT5])
        if c_shape is not None:
            self.check_param_length(c_shape, [SHAPE_LENGHT4, SHAPE_LENGHT5])

        # check the support range of params
        self.check_support_range(params_in.get(
            "op_type", OP_TYPE), self.op_type_dict)
        self.check_support_range(params_in.get("A_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("B_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("C_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("mad_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("bias_dtype", "float16"), self.dtype_dict)

        # dynamic shape
        dynamic_shape_flag = params_in.get("dynamic_shape_flag", False)
        self.check_param_type(dynamic_shape_flag, [bool])

        if dynamic_shape_flag is True:
            # M/K/N range
            # tensor A: [batch_a, ca1, ha, wa, ca0] : [1, K/16, M/16, 16, 16]
            # tensor B: [batch_b, cb1, hb, wb, cb0] : [K, N/16, 1, 1, 16]
            self.check_param_type(params_in.get("ca1_var_range"), [list])
            self.check_param_type(params_in.get("ha_var_range"), [list])
            self.check_param_type(params_in.get("cb1_var_range"), [list])

            self.check_param_length(params_in.get(
                "ca1_var_range", [CONST_VALUE0, CONST_VALUE0]), [SHAPE_LENGHT2])
            self.check_param_length(params_in.get(
                "ha_var_range", [CONST_VALUE0, CONST_VALUE0]), [SHAPE_LENGHT2])
            self.check_param_length(params_in.get(
                "cb1_var_range", [CONST_VALUE0, CONST_VALUE0]), [SHAPE_LENGHT2])
            self.check_param_length(params_in.get(
                "batch_var_range", [CONST_VALUE0, CONST_VALUE0]), [SHAPE_LENGHT2])

    def preprocess_info_dict(self, params_in):
        """
        encode input params and set default value of input params

        Parameters
        ----------
        params_in: input params

        Returns
        """
        # set the defalut value of params
        if self.flag_new_tiling:
            params_in["op_type"] = params_in.get("op_type", OP_TYPE)
        else:
            params_in["op_type"] = self.op_type_dict.get(params_in.get("op_type", OP_TYPE))
        params_in["A_shape"] = params_in.get("A_shape")
        params_in["B_shape"] = params_in.get("B_shape")
        params_in["C_shape"] = params_in.get("C_shape")

        params_in["A_dtype"] = self.dtype_dict.get(
            params_in.get("A_dtype", "float16"))
        params_in["B_dtype"] = self.dtype_dict.get(
            params_in.get("B_dtype", "float16"))
        params_in["C_dtype"] = self.dtype_dict.get(
            params_in.get("C_dtype", "float16"))
        params_in["mad_dtype"] = self.dtype_dict.get(
            params_in.get("mad_dtype", "float16"))
        if params_in.get("bias_dtype"):
            params_in["bias_dtype"] = self.dtype_dict.get(params_in.get("bias_dtype"))

        params_in["padl"] = params_in.get("padl", CONST_VALUE0)
        params_in["padr"] = params_in.get("padr", CONST_VALUE0)
        params_in["padu"] = params_in.get("padu", CONST_VALUE0)
        params_in["padd"] = params_in.get("padd", CONST_VALUE0)
        params_in["strideH"] = params_in.get("strideH", CONST_VALUE1)
        params_in["strideW"] = params_in.get("strideW", CONST_VALUE1)
        params_in["strideH_expand"] = params_in.get(
            "strideH_expand", CONST_VALUE1)
        params_in["strideW_expand"] = params_in.get(
            "strideW_expand", CONST_VALUE1)
        params_in["dilationH"] = params_in.get(
            "dilationH", CONST_VALUE1)
        params_in["dilationW"] = params_in.get(
            "dilationW", CONST_VALUE1)
        params_in["group"] = params_in.get("group", CONST_VALUE1)
        bias_flag = params_in.get("bias_flag", False)
        params_in["bias_flag"] = 1 if bias_flag else 0

        # process fixed-point number (%2.f)
        fused_double_operand_num = params_in.get("fused_double_operand_num")
        params_in["fused_double_operand_num"] = math.ceil(
            100 * fused_double_operand_num)

        params_in["kernel_name"] = params_in.get(
            "kernel_name", OP_TYPE + "_kernel")

        params_in["reduce_fusion"] = params_in.get("reduce_fusion", CONST_VALUE0)
        params_in["scalar_size"] = params_in.get("scalar_size", CONST_VALUE0)

        params_in["batch_type"] = params_in.get("batch_type", CONST_VALUE0)

        # dynamic shape params
        dynamic_shape_flag = params_in.get("dynamic_shape_flag", False)
        params_in["dynamic_shape_flag"] = dynamic_shape_flag

        if dynamic_shape_flag is True:
            params_in["ca1_var_range"] = params_in.get(
                "ca1_var_range", [CONST_VALUE0, CONST_VALUE0])
            params_in["ha_var_range"] = params_in.get(
                "ha_var_range", [CONST_VALUE0, CONST_VALUE0])
            params_in["cb1_var_range"] = params_in.get(
                "cb1_var_range", [CONST_VALUE0, CONST_VALUE0])
            params_in["batch_var_range"] = params_in.get(
                "batch_var_range", [CONST_VALUE1, CONST_VALUE1])
