#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
TBE operator param encoder
"""
import json
import math
from copy import deepcopy

from tbe.common.tiling.tiling_api_internal_use import is_support_fixpipe_flatform
from tbe.common.utils.errormgr.error_manager_cube import raise_err_message_cube
from tbe.common.tiling.op_param_encode.operator_params_encoder import (
    BaseClassParamsEncoder
)

# define memory type
DDR_MEMORY = 0
L1_MEMORY = 1
L2_MEMORY = 2
EMPTY_MEMORY = 3

# define L1 fusion type
DEFAULT_VALUE = -1
L1_NO_FUSION = 2

# define const value
CONST_VALUE0 = 0
CONST_VALUE1 = 1

# define length of shape
SHAPE_LENGHT4 = 4
SHAPE_LENGHT5 = 5

OP_TYPE = "conv2d_backprop_input"
MAX_UINT32 = 4294967295
MAX_UINT16 = 65535


def decode_for_dynamic(tiling, input_args):
    """
    encode the input params to NDArray

    Parameters
    ----------
    input_args: input params

    Returns
    ----------
    NDArray: tvm.nd.array
    """

    # dynamic shape tiling
    tiling_result = []
    for tiling_res in tiling:
        c_shape = deepcopy(input_args["C_shape"])
        pad = [
            deepcopy(input_args["padl"]),
            deepcopy(input_args["padr"]),
            deepcopy(input_args["padu"]),
            deepcopy(input_args["padd"])
        ]
        if input_args.get("tiling_type") is None:
            c_shape[2] = tiling_res["C_shape"][0]
            c_shape[3] = tiling_res["C_shape"][1]
            pad = tiling_res["pad"]
        tiling_res["C_shape"] = deepcopy(c_shape)
        tiling_res["pad"] = deepcopy(pad)
        tiling_result.append(tiling_res)
    return tiling_result


class Conv2dBpInputParamsEncoder(BaseClassParamsEncoder):
    """
    Child class for conv2d backprop input Params Encoder
    """

    def __init__(self):
        super().__init__()
        self.input_args = {}
        self.flag_new_tiling = False

    @staticmethod
    def get_kb_query_attr(info_dict):
        """
        get typical key for repository query

        Parameters
        ----------
        info_dict: input params

        Returns
        ----------
        attr_dict: input params string
        """
        attr_dict = {'op_type': 'conv2d_backprop_input', 'A_shape': [], 'B_shape': [], 'C_shape': None,
                     'A_dtype': 'float16', 'B_dtype': 'float16', 'C_dtype': 'float16', 'mad_dtype': 'float32',
                     'padl': 0, 'padr': 0, 'padu': 0, 'padd': 0, 'strideH': 1, 'strideW': 1,
                     'strideH_expand': 1, 'strideW_expand': 1, 'dilationH': 1, 'dilationW': 1,
                     'group': 1, 'bias_flag': 0, 'fused_double_operand_num': 0,
                     'in_fm_memory_type': [0], 'out_fm_memory_type': [0], 'l1_fusion_type': -1, 'fusion_type': 1}
        for key, value in info_dict.items():
            if key in attr_dict:
                attr_dict[key] = value
        if "bias_dtype" in info_dict:
            attr_dict.update({"bias_dtype": info_dict.get("bias_dtype")})
        if "fixpipe_flag" in info_dict:
            attr_dict.update({"fixpipe_flag": info_dict.get("fixpipe_flag")})
        if "4To2_structured_sparsity" in info_dict:
            attr_dict.update({"4To2_structured_sparsity": info_dict.get("4To2_structured_sparsity")})
        return json.dumps(attr_dict)

    def encode_array(self, input_args):
        """
        encode the input params to NDArray

        Parameters
        ----------
        input_args: input params

        Returns
        ----------
        NDArray: tvm.nd.array
        """
        self.flag_new_tiling = is_support_fixpipe_flatform(input_args.get("op_type"))
        params_in = deepcopy(input_args)
        self.input_args = params_in
        # check params
        # 请接入910_93 & 310B的tiling新通路时，新通路中的op_type做以下处理，旧通路保持原样不变：
        # 在python层处理，两个conv2d_dx算子模板的区别, 根据输入参数不同, 将op_type进行区分，
        # 不用再次传入到C++侧后, 再区分两个模板
        # 请在python侧进行区分，以不同的op_type传入C++侧，调用tiling space生成模块
        # 建议两个算子模板 1、op_type 命名为 "conv2d_backprop_input"
        #                  2、op_type 命名为 "conv2d_backprop_input_optimize"
        # 这样的话，跟C++侧注册时，tiling_transfomer模块采用的两个op_type保持一致。
        self.check_info_dict(params_in)
        # preprocess params
        self.preprocess_info_dict(params_in)

        return self.encode(params_in)

    def decode(self, tiling_encode):
        """
        encode the input params to tvm.nd.array
        Parameters
        ----------
        input_args: the input params
        Returns
        -------
        tvm.nd.array: the NDArray
        """
        if not self.input_args["dynamic_shape_flag"] and not tiling_encode:
            raise_err_message_cube(
                "only support legal tiling, "
                "but the return value of tiling is [%s]." % tiling_encode
            )
        if self.input_args["dynamic_shape_flag"] and not tiling_encode:
            return []
        tiling = json.loads(tiling_encode)

        if isinstance(tiling, dict):
            # When group is bigger than 1 and axis k is FULL_LOAD,
            # convert k value to real number.
            channel_a_1, channel_a_0 = self.input_args["A_shape"][1], self.input_args["A_shape"][4]
            kernel_h, kernel_w = self.input_args["B_shape"][2], self.input_args["B_shape"][3]
            l1_ka = channel_a_1 * channel_a_0 * kernel_h * kernel_w
            if tiling["AL1_shape"][-1] > 1 and tiling["AL1_shape"][0] == MAX_UINT32:
                tiling["AL1_shape"][0] = l1_ka
            if tiling["BL1_shape"][-1] > 1 and tiling["BL1_shape"][0] == MAX_UINT32:
                tiling["BL1_shape"][0] = l1_ka
            if tiling["BL0_matrix"][-1] > 1 and tiling["BL0_matrix"][0] == MAX_UINT16:
                tiling["BL0_matrix"][0] = tiling["BL1_shape"][0] // tiling["BL0_matrix"][3]
        elif isinstance(tiling, list):
            tiling = decode_for_dynamic(tiling, self.input_args)
        return tiling

    def check_info_dict(self, params_in):
        """
        check the type, length and support-range of input params

        Parameters
        ----------
        params_in: input params

        Returns
        """

        # check param types
        self.check_param_type(params_in, [dict])
        self.check_param_type(params_in.get("op_type", OP_TYPE), [str])
        self.check_param_type(params_in.get("A_shape"), [list])
        self.check_param_type(params_in.get("B_shape"), [list])
        c_shape = params_in.get("C_shape")
        if c_shape is not None:
            self.check_param_type(c_shape, [list])

        self.check_param_type(params_in.get("A_dtype"), [str])
        self.check_param_type(params_in.get("B_dtype"), [str])
        self.check_param_type(params_in.get("C_dtype"), [str])
        self.check_param_type(params_in.get("mad_dtype"), [str])
        self.check_param_type(params_in.get("bias_dtype", "float16"), [str])

        self.check_param_type(params_in.get("padl"), [int])
        self.check_param_type(params_in.get("padr"), [int])
        self.check_param_type(params_in.get("padu"), [int])
        self.check_param_type(params_in.get("padd"), [int])
        self.check_param_type(params_in.get("strideH"), [int])
        self.check_param_type(params_in.get("strideW"), [int])
        self.check_param_type(params_in.get("strideH_expand"), [int])
        self.check_param_type(params_in.get("strideW_expand"), [int])
        self.check_param_type(params_in.get("dilationH"), [int])
        self.check_param_type(params_in.get("dilationW"), [int])
        self.check_param_type(params_in.get("group", CONST_VALUE1), [int])
        self.check_param_type(params_in.get("bias_flag", False), [bool, int])

        self.check_param_type(params_in.get("fused_double_operand_num"), [int, float])

        self.check_param_type(params_in.get("in_fm_memory_type"), [list])
        self.check_param_type(params_in.get("out_fm_memory_type"), [list])
        self.check_param_type(params_in.get("l1_fusion_type"), [int])
        self.check_param_type(params_in.get("fusion_type", CONST_VALUE0), [int])
        self.check_param_type(params_in.get("kernel_name", OP_TYPE + "_kernel"), [str])
        self.check_param_type(params_in.get("split_axis_mode", CONST_VALUE0), [int])

        # check length of params
        self.check_param_length(params_in.get("A_shape"), [SHAPE_LENGHT5])
        self.check_param_length(params_in.get("B_shape"), [SHAPE_LENGHT5])
        if c_shape is not None:
            self.check_param_length(c_shape, [SHAPE_LENGHT4, SHAPE_LENGHT5])

        # check the support range of params
        self.check_support_range(params_in.get("op_type", OP_TYPE), self.op_type_dict)
        self.check_support_range(params_in.get("A_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("B_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("C_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("mad_dtype"), self.dtype_dict)
        self.check_support_range(params_in.get("bias_dtype", "float16"), self.dtype_dict)

    def preprocess_info_dict(self, params_in):
        """
        encode input params and set default value of input params

        Parameters
        ----------
        params_in: input params

        Returns
        """

        def encode_memory_type(type_flag, memory_type_list):
            """
            encode input params to decimal value
            e.g. [a, b], a and b belong to [0, 1, 2] respectively
                 there will be 9 statuses (ternary to decimal)

            Parameters
            ----------
            type_flag: now support "in" or "out"
            memory_type_list: input memory list

            Returns
            -------
            encode_value: encoded value
            """
            # define encode rule
            # L2_MEMORY is not available for now
            encode_table = {EMPTY_MEMORY: 0, L1_MEMORY: 1, L2_MEMORY: 2, DDR_MEMORY: 3}
            k = len(encode_table)
            # encode
            encode_value = 0
            for m_idx, m_type in enumerate(memory_type_list[::-1]):
                if type_flag == "in":  # no EMPTY_MEMORY
                    encode_value += encode_table[m_type] * ((k - 1) ** m_idx)
                elif type_flag == "out":
                    encode_value += encode_table[m_type] * (k ** m_idx)
                else:
                    raise_err_message_cube("input type_list is not support")

            return encode_value

        # get the memory type of op
        in_fm_memory_type = params_in.get("in_fm_memory_type", [DDR_MEMORY])
        out_fm_memory_type = params_in.get("out_fm_memory_type", [DDR_MEMORY])
        l1_fusion_type = params_in.get("l1_fusion_type", DEFAULT_VALUE)

        if l1_fusion_type == DEFAULT_VALUE:
            l1_fusion_type = L1_NO_FUSION

        # encode memory type
        in_fm_memory_type_encode = encode_memory_type("in", in_fm_memory_type)
        out_fm_memory_type_encode = encode_memory_type("out", out_fm_memory_type)

        # set the defalut value of params
        if self.flag_new_tiling:
            params_in["op_type"] = params_in.get("op_type", OP_TYPE)
        else:
            params_in["op_type"] = self.op_type_dict.get(params_in.get("op_type", OP_TYPE))
        params_in["A_shape"] = params_in.get("A_shape")
        params_in["B_shape"] = params_in.get("B_shape")
        c_shape = params_in.get("C_shape")
        params_in["C_shape"] = [0, 0, 0, 0] if c_shape is None else c_shape

        params_in["A_dtype"] = self.dtype_dict.get(params_in.get("A_dtype", "float16"))
        params_in["B_dtype"] = self.dtype_dict.get(params_in.get("B_dtype", "float16"))
        params_in["C_dtype"] = self.dtype_dict.get(params_in.get("C_dtype", "float16"))
        params_in["mad_dtype"] = self.dtype_dict.get(
            params_in.get("mad_dtype", "float16")
        )
        if params_in.get("bias_dtype"):
            params_in["bias_dtype"] = self.dtype_dict.get(params_in.get("bias_dtype", "float16"))

        params_in["padl"] = params_in.get("padl", CONST_VALUE0)
        params_in["padr"] = params_in.get("padr", CONST_VALUE0)
        params_in["padu"] = params_in.get("padu", CONST_VALUE0)
        params_in["padd"] = params_in.get("padd", CONST_VALUE0)
        params_in["strideH"] = params_in.get("strideH", CONST_VALUE1)
        params_in["strideW"] = params_in.get("strideW", CONST_VALUE1)
        params_in["strideH_expand"] = params_in.get("strideH_expand", CONST_VALUE1)
        params_in["strideW_expand"] = params_in.get("strideW_expand", CONST_VALUE1)
        params_in["dilationH"] = params_in.get("dilationH", CONST_VALUE1)
        params_in["dilationW"] = params_in.get("dilationW", CONST_VALUE1)
        params_in["group"] = params_in.get("group", CONST_VALUE1)
        bias_flag = params_in.get("bias_flag", False)
        params_in["bias_flag"] = 1 if bias_flag else 0

        # process fixed-point number (%2.f)
        fused_double_operand_num = params_in.get("fused_double_operand_num")
        params_in["fused_double_operand_num"] = math.ceil(
            100 * fused_double_operand_num
        )

        params_in["fused_coefficient"] = params_in.get("fused_coefficient", [0.0, 0.0, 0.0])
        params_in["in_fm_memory_type"] = in_fm_memory_type_encode
        params_in["out_fm_memory_type"] = out_fm_memory_type_encode
        params_in["l1_fusion_type"] = l1_fusion_type
        params_in["fusion_type"] = params_in.get("fusion_type", CONST_VALUE0)
        params_in["kernel_name"] = params_in.get("kernel_name", OP_TYPE + "_kernel")
        # Determine whether it is dynamic shape or fixed shape
        params_in["dynamic_shape_flag"] = params_in.get("dynamic_shape_flag", False)
        # whether tiling chose general branch
        params_in["general_flag"] = params_in.get("general_flag", False)
        params_in["split_axis_mode"] = params_in.get("split_axis_mode", CONST_VALUE0)
