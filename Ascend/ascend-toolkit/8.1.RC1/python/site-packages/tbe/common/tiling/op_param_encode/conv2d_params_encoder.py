#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
TBE operator param encoder
"""
import math
import json
import hashlib
from typing import Any
from tbe.tvm import deepcopy
from tbe.dsl.base.operation import get_compile_info
from tbe.common.platform import CUBE_MKN
from tbe.common.context import get_context
from tbe.common.repository_manager.interface import cann_kb_search
from tbe.common.utils.errormgr.error_manager_cube import raise_err_message_cube
from tbe.common.utils.op_util.op_util_conv2d import BinaryTilingKey, TilingDataIdx, AttachMode, KernelIdKeyOffset
from tbe.common.tiling.op_param_encode.operator_params_encoder import BaseClassParamsEncoder
from tbe.common.tiling.op_param_encode.operator_params_encoder import transfer_special_value
from tbe.common.tiling.tiling_api_internal_use import is_support_fixpipe_flatform
from tbe.common.tiling.tiling_api_internal_use import dict_value_none_deepcopy
from tbe.common.tiling.tiling_global_var_internal_use import AUTO_TILING_TYPE
from tbe.common.tiling.tiling_global_var_internal_use import REPOSITORY_TILING_TYPE
from tbe.common.tiling.tiling_global_var_internal_use import CUSTOM_TILING_TYPE
from tbe.common.tiling.tiling_global_var_internal_use import MODEL_TILING_TYPE
from tbe.common.tiling.auto_tiling_log import AUTOTILINGLOG, tiling_show_long_log
from tbe.common.tiling.auto_tiling_log import INFO, WARN, DEBUG, ERROR


# used to distinguish between op_tiling and tiling for dict
DB_ON = 2
DB_OFF = 1
DOUBLEBUFFER_MAP = {
    0: DB_OFF,
    1: DB_ON
}

C04 = 4
# define the type of memory
DDR_MEMORY = 0
L1_MEMORY = 1
L2_MEMORY = 2
EMPTY_MEMORY = 3

# define the type of L1 fusion
DEFAULT_VALUE = -1
L1_DEPTH_FUSION = 0
L1_BREADTH_FUSION = 1
L1_NO_FUSION = 2
L2_FUSION = 3
L2_NO_FUSION = 2

# define the const value
CONST_VALUE0 = 0
CONST_VALUE1 = 1

# length of shape
SHAPE_LENGTH2 = 2
SHAPE_LENGTH3 = 3
SHAPE_LENGTH4 = 4
SHAPE_LENGTH5 = 5

MAX_UINT32 = 4294967295
MAX_UINT16 = 65535

# Define the c04 mode in ascend610, ascend310P3 and hi3796CV300cs
# 0 represent for default mode
# 1 represent for v100 c04 mode
# 2 represent for v200 c04 mode
c04_mode = [0, 1, 2]

# 0: default mode
# 1: high_performance mode
high_performance_mode = [0, 1]

# 0: default mode
# 1: high_precision mode: HF32
high_precision_mode = [0, 1]

# tmp fusion_change_map
fusion_type_dict = {
  # conv
  0 : "1",
  # conv + fixpipe
  200 : "80_fixpipe_1",
  # conv + fixpipe with bias
  201 : "80_fixpipe_1_9_2_2",
  # v300 conv2d with bias
  202 : "1_9_2_2",
  # conv with bias, strideh_opmization
  203 : "1_9_NHWC_trans_5HD_convolution_A_2_2",
  # al1 load2d + fixpipe
  204 : "80_fixpipe_1_9_NHWC_trans_5HD_convolution_A_2_2",
  # conv2d_sigmoid_mul
  205 : "4_1_9_2_2_4_3_3_4_3_3_3_4_3_3_4_3",
  # conv2d_sigmoid_mul_ascend_quant
  206 : "7_4_1_9_2_2_4_3_3_4_3_3_3_4_3_3_4_3",
  # conv2d_fixpipe_sigmoid_mul
  207 : "4_4_3_3_4_3_3_3_80_fixpipe_1_9_2_2_4_3_3_4_3",
  # conv2d_fixpipe_sigmoid_mul_ascend_quant
  208 : "7_4_80_fixpipe_1_9_2_2_4_3_3_4_3_3_3_4_3_3_4_3",
  # dynamic conv2d_fixpipe
  209 : "80_fixpipe_1_5HD_trans_NHWC",
  # conv2d_fixpipe_maxpool
  210 : "12_80_fixpipe_1_9_2_2",
  # aipp_conv2d_fixpipe_maxpool
  211 : "12_80_fixpipe_1_9_81_2_2",
  # conv2d_fixpipe_maxpool_ascend_quant
  212 : "7_12_80_fixpipe_1_9_2_2",
  # aipp_conv2d_fixpipe_maxpool_ascend_quant
  213 : "7_12_80_fixpipe_1_9_81_2_2"
}

AIPP_FUSION_TYPE_BIT = 8
MAX_VALUE_4_BITS = 0xF
AIPP_FUSION_TYPE_FLAG = 3

SINGLE_DATA_BYTES_LEN = 4


def decode_for_dynamic_v2(tiling: list, input_args: dict) -> list:
    tiling_result = []
    c_shape = deepcopy(input_args["result_shape"])
    for tiling_res in tiling:
        if input_args.get("tiling_type") is None:
            c_shape[2] = tiling_res["result_shape"][0]
            c_shape[3] = tiling_res["result_shape"][1]
        elif input_args.get("tiling_type") == "cost_model_tiling":
            tiling_res["A_shape"] = deepcopy(input_args["fm_shape"])
            tiling_res["B_shape"] = deepcopy(input_args["filter_shape"])
        tiling_res["C_shape"] = deepcopy(c_shape)
        tiling_result.append(tiling_res)
    return tiling_result


def decode_for_dynamic_v1(tiling: list, input_args: dict) -> list:
    tiling_result = []
    c_shape = deepcopy(input_args["c_shape"])
    for tiling_res in tiling:
        if input_args.get("tiling_type") is None:
            c_shape[2] = tiling_res["C_shape"][0]
            c_shape[3] = tiling_res["C_shape"][1]
        elif input_args.get("tiling_type") == "cost_model_tiling":
            tiling_res["A_shape"] = deepcopy(input_args["a_shape"])
            tiling_res["B_shape"] = deepcopy(input_args["b_shape"])
        tiling_res["C_shape"] = deepcopy(c_shape)
        tiling_result.append(tiling_res)
    return tiling_result


def decode_for_dynamic(tiling: list, input_args: dict) -> list:
    """
    encode the input params to NDArray

    Parameters
    ----------
    input_args: input params

    Returns
    ----------
    NDArray: tvm.nd.array
    """
    # dynamic shape tiling
    new_tiling_flag = is_support_fixpipe_flatform(input_args.get("op_type"))
    if new_tiling_flag:
        return decode_for_dynamic_v2(tiling, input_args)
    else:
        return decode_for_dynamic_v1(tiling, input_args)


def process_default_value(input_value: Any, default_value: Any) -> Any:
    """
    process the default of input param

    Parameters
    ----------
    input_value: the input param
    default_value: the default value of input param

    Returns
    -------
    ret: the real value of input param
    """
    if input_value is None:
        ret = default_value
    else:
        ret = input_value
    return ret


def encode_memory_type(type_flag: str, memory_type_list: list) -> int:
    """
    encode the input params to encode_value

    Parameters
    ----------
    memory_type_list: the input memory list

    Returns
    -------
    encode_value: encode value
    """
    # get the length of memory_type_list
    value_length = len(memory_type_list)
    # define the encode table
    encode_table = {EMPTY_MEMORY: 0,
                    L1_MEMORY: 1,
                    L2_MEMORY: 2,
                    DDR_MEMORY: 3}
    # encode the encode_value
    encode_value = 0
    encode_index = 0
    while value_length:
        # if the type_flag is output, using ternary code
        if type_flag == "out":
            encode_value += encode_table[memory_type_list[encode_index]] \
                * (4**encode_index)
        # if if the type_flag is input, using binary code
        elif type_flag == "in":
            encode_value += memory_type_list[encode_index] * (3**encode_index)
        else:
            raise_err_message_cube(
                "the input param is illegal, only support {}, but the param is {}.".format(
                    "out or in", type_flag))
        value_length -= 1
        encode_index += 1

    return encode_value


def get_l0a_dma_flag(params_in_: dict) -> int:
    l0a_dma_flag = params_in_['special_mode'].get('l0a_dma_flag', False)
    l0a_dma_flag = (1 if l0a_dma_flag else 0)
    return l0a_dma_flag


def parse_fast_tiling(tiling_data_code: list) -> list:
    """
    Parse tiling data from data flow
    """
    tilingdata_list = []
    for i in range(0, len(tiling_data_code), SINGLE_DATA_BYTES_LEN):
        data = int.from_bytes(tiling_data_code[i:(i+SINGLE_DATA_BYTES_LEN)], "little")
        tilingdata_list.append(data)

    return tilingdata_list


class Conv2dParamsEncoder(BaseClassParamsEncoder):
    """
    Child class for conv2d Params Encoder
    """

    def __init__(self):
        '''
        init the super class
        '''
        super(Conv2dParamsEncoder, self).__init__()
        self.input_args = {}
        self.kb_type = "conv2d"

    @staticmethod
    def get_kb_query_attr(info_dict_json: dict) -> str:
        '''
        get attr form infodict for searching in repo
        '''
        def get_special_mode_attr(info_special_mode: dict) -> dict:
            '''
            get special mode form infodict for searching in repo
            '''
            attr_special_mode = {}
            attr_special_mode['use_c04_mode'] = info_special_mode.get('use_c04_mode')
            attr_special_mode['input_nd_flag'] = info_special_mode.get('input_nd_flag', False)
            attr_special_mode['scalar_num'] = info_special_mode.get('scalar_num', CONST_VALUE0)
            attr_special_mode['high_performance_mode'] = info_special_mode.get(
                'high_performance_mode', False)
            attr_special_mode['high_precision_mode'] = info_special_mode.get(
                'high_precision_mode', False)
            attr_special_mode['l0a_dma_flag'] = info_special_mode.get('l0a_dma_flag', False)
            attr_special_mode['4To2_structured_sparsity'] = info_special_mode.get(
                '4To2_structured_sparsity', False)
            if info_special_mode.get("split_w_flag"):
                attr_special_mode['split_w_flag'] = True
            if info_special_mode.get("winograd_conv_flag"):
                attr_special_mode['winograd_conv_flag'] = True
            if info_special_mode.get("winograd_mc_multi_wo_flag"):
                attr_special_mode['winograd_mc_multi_wo_flag'] = True
            if info_special_mode.get("ub_deep_fusion_flag"):
                attr_special_mode['ub_deep_fusion_flag'] = True
            return attr_special_mode
        # ====== get repo attrs =======
        attr_dict = {}
        # get input attr
        attr_dict['fm_shape'] = info_dict_json['fm_shape']
        attr_dict['filter_shape'] = info_dict_json['filter_shape']
        attr_dict['fm_dtype'] = info_dict_json['fm_dtype']
        attr_dict['filter_dtype'] = info_dict_json['filter_dtype']
        # get output attr
        attr_dict['mad_dtype'] = info_dict_json['mad_dtype']
        attr_dict['result_shape'] = info_dict_json['result_shape']
        attr_dict['result_dtype'] = info_dict_json['result_dtype']
        attr_dict['output_num'] = info_dict_json['output_num']
        # get conv attr
        attr_dict['pad'] = info_dict_json['pad']
        attr_dict['stride'] = info_dict_json['stride']
        attr_dict['dilation'] = info_dict_json['dilation']
        attr_dict['group'] = info_dict_json['group']
        attr_dict['bias_flag'] = info_dict_json['bias_flag']
        attr_dict['bias_dtype'] = info_dict_json['bias_dtype']
        attr_dict['pooling_shape'] = info_dict_json['pooling_shape']
        attr_dict['pooling_stride'] = info_dict_json['pooling_stride']
        # get extra attr
        attr_dict['l1_fusion_type'] = info_dict_json['l1_fusion_type']
        attr_dict['fusion_type'] = info_dict_json['fusion_type']
        attr_dict['reserved_ub'] = info_dict_json.get('reserved_ub', CONST_VALUE0)
        attr_dict['fused_channel_wise'] = info_dict_json['fused_channel_wise']
        attr_dict['in_fm_memory_type'] = info_dict_json['in_fm_memory_type']
        attr_dict['out_fm_memory_type'] = info_dict_json['out_fm_memory_type']
        attr_dict['out_c_memory_type'] = info_dict_json['out_c_memory_type']
        # sort fixpipe_fused_type
        fixpipe_type_list = []
        for fixpipe_type_dict in info_dict_json['fixpipe_fused_type']:
            fixpipe_type_dict_sorted = {}
            for unit in sorted(fixpipe_type_dict):
                fixpipe_type_dict_sorted[unit] = fixpipe_type_dict[unit]
            fixpipe_type_list.append(fixpipe_type_dict_sorted)
        attr_dict['fixpipe_fused_type'] = fixpipe_type_list
        attr_dict['fm_l1_valid_size'] = info_dict_json['fm_l1_valid_size']
        attr_dict['fused_coefficient'] = info_dict_json['fused_coefficient']
        attr_dict['fixpipe_fused_coefficient'] = info_dict_json['fixpipe_fused_coefficient']
        # get special mode
        info_special_mode = info_dict_json['special_mode']
        attr_special_mode = get_special_mode_attr(info_special_mode)
        attr_dict['special_mode'] = attr_special_mode
        return json.dumps(attr_dict)

    def encode_array(self, input_args: dict) -> str:
        """
        encode the input params to string

        Parameters
        ----------
        input_args: the input params

        Returns
        -------
        string: the json string
        """
        new_tiling_flag = is_support_fixpipe_flatform(input_args.get("op_type"))
        # change fusion type temporary adaptation
        if new_tiling_flag and isinstance(input_args.get("fusion_type"), int):
            input_args["fixpipe_fused_type"] = [input_args["fixpipe_fused_type"]]
            fusion_type_pre = input_args.get("fusion_type")
            if ((fusion_type_pre >> AIPP_FUSION_TYPE_BIT) & MAX_VALUE_4_BITS) == AIPP_FUSION_TYPE_FLAG:
                input_args["fusion_type"] = "1_s_3"
            elif fusion_type_pre == 20:
                if input_args.get("bias_flag"):
                    input_args["fusion_type"] = "11_3_3_3_1_5_9_4"
                else:
                    input_args["fusion_type"] = "11_3_3_3_1_5_4"
            elif input_args["fusion_type"] in fusion_type_dict.keys():
                input_args["fusion_type"] = fusion_type_dict[input_args["fusion_type"]]
        params_in = dict_value_none_deepcopy(input_args)
        self.input_args = params_in
        if new_tiling_flag:
            # the new tiling access enable, it will check input parameters in new platform
            # check the params from the interface
            self.check_info_dict_v2(params_in)
            # preprocess the params from the interface
            self.preprocess_info_dict_v2(params_in)
        else:
            # otherwise, it will check input paramters in old platform
            # check the params from the interface
            self.check_info_dict(params_in)
            # preprocess the params from the interface
            self.preprocess_info_dict(params_in)

        # third: encode the params to json string
        return self.encode(params_in)

    def decode(self, tiling_encode: str) -> list:
        """
        encode the input params to tvm.nd.array
        Parameters
        ----------
        input_args: the input params
        Returns
        -------
        tvm.nd.array: the NDArray
        """
        if not self.input_args["dynamic_shape_flag"] and not tiling_encode:
            raise_err_message_cube(
                "only support legal tiling, but the return value of tiling is {}.".format(
                    tiling_encode))
        if self.input_args["dynamic_shape_flag"] and not tiling_encode:
            return []
        tiling = json.loads(tiling_encode)
        if isinstance(tiling, list):
            tiling = decode_for_dynamic(tiling, self.input_args)
        return tiling

    def parse_optiling(self, run_info: dict, info_dict: dict) -> str:
        """
        parse the tiling info from run_info
        Parameters
        ----------
        run_info: the input params
        info_dict: original info_dict

        Returns
        -------
        tiling: string
        """
        self.check_param_type(run_info, [dict])
        kernel_id = run_info.get("tiling_key")
        fast_tiling_data_code = run_info.get("tiling_data")
        fast_tiling_data = parse_fast_tiling(fast_tiling_data_code)

        # init all content of the tiling
        tiling = {'AL0_matrix': [0, 0, 16, 16, 1, 0, 0], 'AL1_shape': [0, 0, 0, 0], 'AUB_shape': [0, 0, 0, 0],
        'BL0_matrix': [0, 0, 16, 16, 1, 0, 0], 'BL1_shape': [0, 0, 0, 0], 'BUB_shape': [0, 0, 0, 0],
        'CL0_matrix': [0, 0, 16, 16, 1, 0, 0], 'INPUT_L1_BT_param': 0, 'INPUT_L1_FB_param': 0,
        'INPUT_L1_eltwise_param': 0, 'INPUT_L1_sparse_index': 0,
        'L0C_OUTPUT_matrix': [0, 0, 16, 16, 1, 0], 'UB_channel_wise_input': [0, 0, 0],
        'block_dim': [0, 0, 0, 0, 0], 'control_reorder_flag': 0,
        'manual_pingpong_buffer': {'AL0_pbuffer': DB_OFF, 'AL1_pbuffer': DB_OFF, 'AUB_pbuffer': DB_OFF,
        'BL0_pbuffer': DB_OFF, 'BL1_pbuffer': DB_OFF, 'BUB_pbuffer': DB_OFF, 'CL0_pbuffer': DB_OFF,
        'INPUT_L1_BT_pbuffer': DB_OFF, 'INPUT_L1_FB_pbuffer': DB_OFF, 'INPUT_L1_eltwise_pbuffer': DB_OFF,
        'L0C_OUTPUT_pbuffer': DB_OFF, 'UBG_pbuffer': DB_OFF},
        'special_optimize_flag': 0, 'tbe_compile_para': 0, 'vector_block_num': 0}

        attach_at_flag = dict()
        attach_at_flag[BinaryTilingKey.AL1_ATTACH_FLAG] = kernel_id & 0b111
        attach_at_flag[BinaryTilingKey.BL1_ATTACH_FLAG] = (kernel_id >> KernelIdKeyOffset.OFFSET_BL1) & 0b111
        attach_at_flag[BinaryTilingKey.BL0_ATTACH_FLAG] = (kernel_id >> KernelIdKeyOffset.OFFSET_BL0) & 0b111
        bias_channel_wise_flag = (kernel_id >> KernelIdKeyOffset.OFFSET_CUB_CHANNEL_WISE) & 0b11

        tiling["block_dim"][0] = fast_tiling_data[TilingDataIdx.IDX_BATCH_DIM]
        tiling["block_dim"][1] = fast_tiling_data[TilingDataIdx.IDX_N_DIM]
        tiling["block_dim"][SHAPE_LENGTH2] = fast_tiling_data[TilingDataIdx.IDX_M_DIM]
        tiling["block_dim"][SHAPE_LENGTH3] = fast_tiling_data[TilingDataIdx.IDX_GROUP_DIM]
        tiling["AL0_matrix"][0] = fast_tiling_data[TilingDataIdx.IDX_M_L0]
        tiling["AL0_matrix"][1] = fast_tiling_data[TilingDataIdx.IDX_K_L0]
        tiling["CL0_matrix"][0] = fast_tiling_data[TilingDataIdx.IDX_N_UB_L0C_FACTOR] * \
                                  fast_tiling_data[TilingDataIdx.IDX_CUB_N1]
        tiling["CL0_matrix"][1] = fast_tiling_data[TilingDataIdx.IDX_M_L0]
        tiling["L0C_OUTPUT_matrix"][0] = fast_tiling_data[TilingDataIdx.IDX_CUB_N1]
        tiling["L0C_OUTPUT_matrix"][1] = fast_tiling_data[TilingDataIdx.IDX_M_L0]
        tiling["AUB_shape"][0] = fast_tiling_data[TilingDataIdx.IDX_K_AUB]
        tiling["AUB_shape"][1] = fast_tiling_data[TilingDataIdx.IDX_M_AUB]

        if attach_at_flag.get(BinaryTilingKey.BL0_ATTACH_FLAG) == AttachMode.ATTACH_FULL_LOAD:
            tiling["BL0_matrix"][0] = MAX_UINT16
        else:
            tiling["BL0_matrix"][0] = fast_tiling_data[TilingDataIdx.IDX_K_L0]
            tiling["BL0_matrix"][1] = fast_tiling_data[TilingDataIdx.IDX_N_UB_L0C_FACTOR] * \
                                      fast_tiling_data[TilingDataIdx.IDX_CUB_N1]

        # L1 buffer tiling
        reduce_al1 = ((fast_tiling_data[TilingDataIdx.IDX_K_H] - 1) *
                        fast_tiling_data[TilingDataIdx.IDX_DILATION_H] + 1) * \
                        ((fast_tiling_data[TilingDataIdx.IDX_K_W] - 1) *
                        fast_tiling_data[TilingDataIdx.IDX_DILATION_W] + 1)
        reduce_bl1 = fast_tiling_data[TilingDataIdx.IDX_K_H] * fast_tiling_data[TilingDataIdx.IDX_K_W]

        use_c04_mode = info_dict.get('special_mode', {}).get('use_c04_mode', CONST_VALUE0)
        ci0 = info_dict.get("fm_shape")[-1]
        if ci0 != C04:
            config = CUBE_MKN[info_dict.get("filter_dtype")]
            ci0 = config['mac'][1]

        kh = info_dict.get("filter_shape")[SHAPE_LENGTH2]
        kw = info_dict.get("filter_shape")[SHAPE_LENGTH3]
        kh_dilated = (kh - 1) * info_dict.get("dilation")[0] + 1
        kw_dilated = (kw - 1) * info_dict.get("dilation")[1] + 1
        reduce_kaxis_al1_khdilate_kwdilate_ci0 = ci0 * kh_dilated * kw_dilated
        reduce_kaxis_bl1_khkw_ci0 = ci0 * kh * kw

        if attach_at_flag.get(BinaryTilingKey.AL1_ATTACH_FLAG) == AttachMode.ATTACH_FULL_LOAD:
            tiling["AL1_shape"][0] = MAX_UINT32
        else:
            tiling["AL1_shape"] = [1, 1, 1, 1, 1]
            # common scene, KAL1_16 is ci1's cut * khDilation * kwDilation
            tiling["AL1_shape"][0] = fast_tiling_data[TilingDataIdx.IDX_KAL1_16] // reduce_al1 * \
                reduce_kaxis_al1_khdilate_kwdilate_ci0
            tiling["AL1_shape"][1] = fast_tiling_data[TilingDataIdx.IDX_M_AL1_FACTOR]

        if attach_at_flag.get(BinaryTilingKey.BL1_ATTACH_FLAG) == AttachMode.ATTACH_PASS:
            tiling["BL1_shape"][0] = 0
        elif attach_at_flag.get(BinaryTilingKey.BL1_ATTACH_FLAG) == AttachMode.ATTACH_FULL_LOAD:
            tiling["BL1_shape"][0] = MAX_UINT32
        else:
            tiling["BL1_shape"] = [1, 1, 1, 1, 1]
            tiling["BL1_shape"][0] = fast_tiling_data[TilingDataIdx.IDX_KBL1_16] // reduce_bl1 * \
                reduce_kaxis_bl1_khkw_ci0
            tiling["BL1_shape"][1] = fast_tiling_data[TilingDataIdx.IDX_N_BL1_FACTOR]

        tiling['manual_pingpong_buffer']['AL1_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_AL1_PBUFFER])
        tiling['manual_pingpong_buffer']['BL1_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_BL1_PBUFFER])
        tiling['manual_pingpong_buffer']['AL0_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_AL0_PBUFFER])
        tiling['manual_pingpong_buffer']['BL0_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_BL0_PBUFFER])
        tiling['manual_pingpong_buffer']['CL0_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_CL0_PBUFFER])
        tiling['manual_pingpong_buffer']['AUB_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_AUB_PBUFFER])
        tiling['manual_pingpong_buffer']['BUB_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_BUB_PBUFFER])
        tiling['manual_pingpong_buffer']['L0C_OUTPUT_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_CUB_PBUFFER])
        tiling['manual_pingpong_buffer']['UBG_pbuffer'] = DOUBLEBUFFER_MAP.get(
            fast_tiling_data[TilingDataIdx.IDX_UBG_PBUFFER])
        # BT, FB, Elt DB not enable yet, set 1 to define close db
        tiling['manual_pingpong_buffer']['INPUT_L1_BT_pbuffer'] = 1
        tiling['manual_pingpong_buffer']['INPUT_L1_FB_pbuffer'] = 1
        tiling['manual_pingpong_buffer']['INPUT_L1_eltwise_pbuffer'] = 1

        tiling["INPUT_L1_BT_param"] = 1 if bias_channel_wise_flag else 2
        tiling["INPUT_L1_BT_param"] = tiling["INPUT_L1_BT_param"] if info_dict.get('bias_flag', False) else 0
        tiling['tbe_compile_para'] = 0
        tiling['vector_block_num'] = 0
        optiling = self.decode(json.dumps(tiling))
        optiling = transfer_special_value(optiling)
        optiling["tiling_type"] = "dsl_cache_tiling"

        return optiling

    def check_info_dict(self, params_in: dict) -> None:
        """
        check the input params

        Parameters
        ----------
        params_in: the input params

        Returns
        -------
        """
        # preprocess the param
        fused_coefficient = params_in.get('fused_coefficient', None)
        params_in['fused_coefficient'] = process_default_value( \
            fused_coefficient, [0, 0, 0])
        fused_channel_wise = params_in.get('fused_channel_wise', None)
        params_in['fused_channel_wise'] = process_default_value( \
            fused_channel_wise, [0, 0, 0])
        pooling_shape = params_in.get('pooling_shape', None)
        params_in['pooling_shape'] = process_default_value( \
            pooling_shape, [0, 0])
        pooling_stride = params_in.get('pooling_stride', None)
        params_in['pooling_stride'] = process_default_value( \
            pooling_stride, [0, 0])
        params_in['fm_l1_valid_size'] = params_in.get('fm_l1_valid_size', DEFAULT_VALUE)
        params_in['special_mode'] = params_in.get('special_mode', {})

        # check the type of param
        self.check_param_type(params_in, [dict])
        self.check_param_type(params_in.get('a_shape'), [list])
        self.check_param_type(params_in.get('placeholder_fmap_5hd_shape'), [list])
        self.check_param_type(params_in.get('b_shape'), [list])
        self.check_param_type(params_in.get('c_shape'), [list])
        self.check_param_type(params_in.get('a_dtype'), [str])
        self.check_param_type(params_in.get('b_dtype'), [str])
        self.check_param_type(params_in.get('c_dtype'), [str])
        self.check_param_type(params_in.get('mad_dtype'), [str])
        self.check_param_type(params_in.get('pad'), [list, tuple])
        self.check_param_type(params_in.get('stride'), [list, tuple])
        self.check_param_type(params_in.get('dilation'), [list, tuple])
        self.check_param_type(params_in.get('fused_coefficient'), [list])
        self.check_param_type(params_in.get('fused_ub_cl0', CONST_VALUE0), [int, float])
        self.check_param_type(params_in.get('fused_channel_wise'), [list])
        self.check_param_type(params_in.get('group', CONST_VALUE1), [int])
        self.check_param_type(params_in.get('bias_flag', False), [bool, int])
        self.check_param_type(params_in.get('op_type', 'conv2d'), [str])
        self.check_param_type(params_in.get('in_fm_memory_type'), [list])
        self.check_param_type(params_in.get('out_fm_memory_type'), [list])
        self.check_param_type(params_in.get('l1_fusion_type'), [int])
        self.check_param_type(params_in.get('fusion_type', CONST_VALUE0), [int])
        self.check_param_type(params_in.get('kernel_name', "conv2d_kernel"), [str])
        self.check_param_type(params_in.get('reserved_ub', CONST_VALUE0), [int])
        self.check_param_type(params_in.get('fm_l1_valid_size'), [int])
        self.check_param_type(params_in.get('pooling_shape'), [list])
        self.check_param_type(params_in.get('pooling_stride'), [list])
        self.check_param_type(params_in.get('special_mode'), [dict])
        self.check_param_type(params_in['special_mode'].get("use_c04_mode", CONST_VALUE0), [int])
        self.check_param_type(
            params_in['special_mode'].get("convfp16_double_out", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get("l0a_dma_flag", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get("high_performance_mode", CONST_VALUE0), [int])
        self.check_param_type(params_in['special_mode'].get("input_nd_flag", False), [bool])
        self.check_param_type(params_in['special_mode'].get("old_coeff", CONST_VALUE0), [int, float])

        self.check_param_type(params_in.get('fb_dict', {}), [dict])

        # check the length of param
        self.check_param_length(params_in.get('a_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('placeholder_fmap_5hd_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('b_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('c_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('pad'), [SHAPE_LENGTH4])
        self.check_param_length(params_in.get('stride'), [SHAPE_LENGTH2])
        self.check_param_length(params_in.get('dilation'), [SHAPE_LENGTH2])
        self.check_param_length(params_in.get('fused_coefficient'), [SHAPE_LENGTH3])
        self.check_param_length(params_in.get('fused_channel_wise'), [SHAPE_LENGTH3])
        self.check_param_length(params_in.get('pooling_shape'), [SHAPE_LENGTH2])
        self.check_param_length(params_in.get('pooling_stride'), [SHAPE_LENGTH2])

        # check the support range of param
        self.check_support_range(params_in.get('a_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('b_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('c_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('mad_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('op_type', 'conv2d'), self.op_type_dict)
        self.check_support_range(params_in['special_mode'].get('use_c04_mode', CONST_VALUE0), c04_mode)
        self.check_support_range(params_in['special_mode'].get('high_performance_mode', CONST_VALUE0),
                                 high_performance_mode)
        self.check_param_type(params_in.get('fixpipe_fusion_flag_dict', {}).get('quant_pre_flag', False), [bool, int])
        self.check_param_type(params_in.get('fixpipe_fusion_flag_dict', {}).get('relu_pre_flag', False), [bool, int])
        self.check_param_type(params_in.get('eltwise_dict', {}).get('eltwise_flag', False), [bool, int])
        self.check_support_range(params_in.get('eltwise_dict', {}).get('eltwise_dtype', "float16"),
                                 self.eltwise_support_dtype)

        # check the pooling params
        pooling_shape = params_in.get('pooling_shape')
        pooling_stride = params_in.get('pooling_stride')
        # the channel align to unit of cube
        _ca0 = params_in["b_shape"][4]
        if _ca0 != C04:
            self.check_support_value(pooling_shape, [CONST_VALUE0, CONST_VALUE0])
            self.check_support_value(pooling_stride, [CONST_VALUE0, CONST_VALUE0])

        # check the fm_l1_valid_size whether is legal
        if params_in.get('fm_l1_valid_size') != DEFAULT_VALUE:
            # according to the use_c04_mode, align the channel
            use_c04_mode = params_in['special_mode'].get('use_c04_mode', \
                CONST_VALUE0)
            a_shape = deepcopy(params_in.get('a_shape'))
            if use_c04_mode == 1 and a_shape[4] == 4:
                a_shape[4] = 16
            # with group conv scene, the fm_shape is shape of placehold tensor
            if params_in.get('group') != CONST_VALUE1:
                a_shape = params_in.get('placeholder_fmap_5hd_shape')
            fm_shape_size = self.input_data_byte_width[params_in.get('a_dtype')]
            for elt in a_shape:
                fm_shape_size *= elt
            input_data_level = params_in.get('fm_l1_valid_size') / fm_shape_size
            if input_data_level not in self.input_data_level.keys():
                raise_err_message_cube(
                    "the fm_l1_valid_size must be 1/2, 3/4, 1 times fm_shape_size "
                    "but fm_l1_valid_size is {}, fm_shape_size is {}.".format(
                        params_in.get('fm_l1_valid_size'), fm_shape_size))
            params_in['fm_l1_valid_size_level'] = input_data_level
        else:
            # if no fm_l1_valid_size param, set the default value 0
            params_in['fm_l1_valid_size_level'] = CONST_VALUE0

    def check_op_tiling_supported(self, optiling_info, info_dict) -> bool:
        """
        check the input op_tiling_param_dict

        Parameters
        ----------
        op_tiling_param_dict: the input params

        Returns
        -------
        """
        if (not is_support_fixpipe_flatform("conv2d")):
            AUTOTILINGLOG.debug("conv2d only support milan get dsl cache tiling")
            return False

        if not info_dict.get("support_op_tiling_flag", False):
            AUTOTILINGLOG.debug("op_tiling check not supported in schedule.")
            return False

        op_tiling_param_dict = optiling_info
        # graph_op_info's op_type record original op type, include DepthWise, SpaceToDepth eg
        self.check_param_type(op_tiling_param_dict, [dict])
        self.check_param_type(op_tiling_param_dict.get("inputs"), [tuple, list])
        self.check_param_type(op_tiling_param_dict.get("outputs"), [tuple, list])
        self.check_param_type(op_tiling_param_dict.get("attrs"), [list])
        self.check_param_type(op_tiling_param_dict.get("op_type"), [str])
        self.check_param_type(op_tiling_param_dict.get("compile_info"), [dict])

        self.check_illegal_value(op_tiling_param_dict.get("inputs"), tuple())
        self.check_illegal_value(op_tiling_param_dict.get("outputs"), tuple())

        # When there is a strided read/write feature in the cin direction, op_tiling is not supported
        fm_inputs = op_tiling_param_dict.get("inputs")[0]
        ori_shape = fm_inputs.get("ori_shape", None)
        ori_format = fm_inputs.get("ori_format", None)
        fm_format = fm_inputs.get("format", None)
        shape = fm_inputs.get("shape", None)
        self.check_param_type(ori_shape, [tuple, list])
        self.check_param_type(ori_format, [str])
        self.check_param_type(shape, [tuple, list])
        self.check_param_length(ori_shape, [SHAPE_LENGTH4])
        self.check_param_length(shape, [SHAPE_LENGTH5])
        ori_c = ori_shape[ori_format.index("C")]
        stride_read_scene_flag = (shape[1] * shape[-1] < ori_c)
        # Conv2D's op_tiling only support format that is 'NC1HWC0'
        self.check_support_value(fm_format, "NC1HWC0")
        # op_tiling only support Conv2D
        if (op_tiling_param_dict.get('op_type') in ["Conv2D"]) and (not stride_read_scene_flag):
            tiling_show_long_log(INFO, "[auto_tiling]op_tiling_param_dict", op_tiling_param_dict)
            return True

        return False

    def preprocess_info_dict(self, params_in: dict) -> None:
        """
        encode the information of shape to the list of uint32 digit

        Parameters
        ----------
        params_in: dict of params
            include all information of shape

        Returns
        -------
        """
        def get_fb_dict(params_in_: dict) -> dict:
            fb_dict_ = params_in_.get('fixpipe_fusion_flag_dict', {})
            quant_pre_flag_ = fb_dict_.get('quant_pre_flag', False)
            relu_pre_flag_ = fb_dict_.get('relu_pre_flag', False)
            quant_post_flag_ = fb_dict_.get('quant_post_flag', False)
            relu_post_flag_ = fb_dict_.get('relu_post_flag', False)
            anti_quant_flag_ = fb_dict_.get('anti_quant_flag', False)

            fb_dict_["quant_pre_flag"] = (1 if quant_pre_flag_ else 0)
            fb_dict_["relu_pre_flag"] = (1 if relu_pre_flag_ else 0)
            fb_dict_["quant_post_flag"] = (1 if quant_post_flag_ else 0)
            fb_dict_["relu_post_flag"] = (1 if relu_post_flag_ else 0)
            fb_dict_["anti_quant_flag"] = (1 if anti_quant_flag_ else 0)
            return fb_dict_

        def get_eltwise_dict(params_in_: dict) -> dict:
            eltwise_dict_ = params_in_.get('eltwise_dict', {})
            eltwise_flag_ = eltwise_dict_.get('eltwise_flag', False)

            eltwise_dict_["eltwise_flag"] = eltwise_flag_
            eltwise_dtype_ = eltwise_dict_.get('eltwise_dtype', "float16")
            eltwise_dict_["eltwise_dtype"] = self.dtype_dict[eltwise_dtype_]
            return eltwise_dict_


        # get the missing information on interface
        l1_fusion_type = params_in.get('l1_fusion_type', DEFAULT_VALUE)
        # source buffer of input and destination buffer of output
        in_fm_memory_type = params_in.get('in_fm_memory_type', [DDR_MEMORY])
        out_fm_memory_type = params_in.get('out_fm_memory_type', [DDR_MEMORY])
        # process the fusion type
        # if fuison type is depth fusion, then source and destination is DDR,
        # the l1_fusion_type is L1_no_fusion
        # 0 represent L1_depth_fusion; 1 represent L1_breadth_fusion,
        # 2 represent L1_no_fusion; 3 represent L2_fusion
        if l1_fusion_type == DEFAULT_VALUE:
            l1_fusion_type = L1_NO_FUSION
        # 2 represent L2_no_fusion; 3 represent L2_fusion
        if (L2_MEMORY in in_fm_memory_type) or (L2_MEMORY in out_fm_memory_type):
            l2_fusion_type = L2_FUSION
        else:
            l2_fusion_type = L2_NO_FUSION

        # encode the memory type
        in_fm_memory_type_encode = encode_memory_type("in", in_fm_memory_type)
        out_fm_memory_type_encode = encode_memory_type("out", out_fm_memory_type)
        # set the default value of these params
        op_type = params_in.get('op_type', 'conv2d')
        fusion_type = params_in.get('fusion_type', CONST_VALUE0)
        kernel_name = params_in.get('kernel_name', "conv2d_kernel")
        a_dtype = params_in.get('a_dtype', 'float16')
        b_dtype = params_in.get('b_dtype', 'float16')
        c_dtype = params_in.get('c_dtype', 'float16')
        mad_dtype = params_in.get('mad_dtype', 'float16')
        bias_flag = params_in.get('bias_flag', False)
        bias_flag = (1 if bias_flag else 0)
        convfp16_double_out = params_in['special_mode'].get('convfp16_double_out', False)
        convfp16_double_out = (1 if convfp16_double_out else 0)
        l0a_dma_flag = get_l0a_dma_flag(params_in)
        fixpipe_buffer_dict = get_fb_dict(params_in)
        eltwise_dict = get_eltwise_dict(params_in)

        # the channel align to unit of cube
        _ca0 = params_in["b_shape"][4]
        if _ca0 != C04:
            config = CUBE_MKN[params_in["b_dtype"]]
            _ca0 = config['mac'][1]

        a_shape = params_in.get('a_shape')
        # check the divisor should not be 0
        self.check_illegal_value(_ca0, 0)
        a_shape[1] = (a_shape[1]*a_shape[4] + _ca0 - 1) // _ca0
        a_shape[4] = _ca0
        b_shape = params_in.get('b_shape')
        b_shape[1] = (b_shape[1]*b_shape[4] + _ca0 - 1) // _ca0
        b_shape[4] = _ca0
        c_shape = params_in.get('c_shape')
        c_shape = ([0, 0, 0, 0, 0] if c_shape is None else c_shape)

        # processing fixed-point number
        fused_coefficient = params_in.get('fused_coefficient')
        fused_coefficient = [math.ceil(100*elt) for elt in fused_coefficient]
        fused_ub_cl0 = params_in.get('fused_ub_cl0', CONST_VALUE0)
        fused_ub_cl0 = math.ceil(100*fused_ub_cl0)
        fused_channel_wise = params_in.get('fused_channel_wise')
        fused_channel_wise = [math.ceil(100*elt) for elt in fused_channel_wise]
        reserved_ub = params_in.get('reserved_ub', CONST_VALUE0)

        # transform the value of -1 to 0 for fm_l1_valid_size
        if params_in.get('fm_l1_valid_size') == DEFAULT_VALUE:
            params_in['fm_l1_valid_size'] = CONST_VALUE0
        # endocde the value of fm_l1_valid_size_level
        if params_in.get('fm_l1_valid_size_level') != CONST_VALUE0:
            raw_fm_l1_valid_size_level = params_in.get('fm_l1_valid_size_level')
            params_in['fm_l1_valid_size_level'] = self.input_data_level[raw_fm_l1_valid_size_level]

        # processed params
        params_in['a_shape'] = a_shape
        params_in['b_shape'] = b_shape
        params_in['c_shape'] = c_shape
        params_in['a_dtype'] = self.dtype_dict.get(a_dtype)
        params_in['b_dtype'] = self.dtype_dict.get(b_dtype)
        params_in['c_dtype'] = self.dtype_dict.get(c_dtype)
        params_in['mad_dtype'] = self.dtype_dict.get(mad_dtype)
        # l1_fusion_type have three states:  0 represent L1_depth_fusion;
        # 1 represent L1_breadth_fusion, 2 represent L1_no_fusion
        params_in['l1_fusion_type'] = l1_fusion_type
        # 2 represent L2_no_fusion; 3 represent L2_fusion
        params_in['l2_fusion_type'] = l2_fusion_type
        # source buffer of input and destination buffer of output
        params_in['in_fm_memory_type'] = in_fm_memory_type_encode
        params_in['out_fm_memory_type'] = out_fm_memory_type_encode
        # set the default value of these params
        params_in['pad'] = params_in.get('pad', [0, 0, 0, 0])
        params_in['stride'] = params_in.get('stride', [CONST_VALUE1, CONST_VALUE1])
        params_in['dilation'] = params_in.get('dilation', [CONST_VALUE1, CONST_VALUE1])
        params_in['group'] = params_in.get('group', CONST_VALUE1)
        params_in['bias_flag'] = bias_flag
        params_in['op_type'] = self.op_type_dict.get(op_type)
        params_in['fusion_type'] = fusion_type
        params_in['kernel_name'] = kernel_name
        # the fused_channel_wise and fused_coefficient are fixed-point number
        # count to two decimal places
        params_in['fused_coefficient'] = fused_coefficient
        params_in['fused_ub_cl0'] = fused_ub_cl0
        params_in['fused_channel_wise'] = fused_channel_wise
        params_in['reserved_ub'] = reserved_ub
        params_in['special_mode'] = params_in.get('special_mode', {})
        params_in['special_mode']['use_c04_mode'] = params_in['special_mode'].get('use_c04_mode', CONST_VALUE0)
        params_in['special_mode']['convfp16_double_out'] = convfp16_double_out
        params_in['special_mode']['l0a_dma_flag'] = l0a_dma_flag
        params_in['special_mode']['high_performance_mode'] = params_in['special_mode'].get(
            'high_performance_mode', CONST_VALUE0)
        params_in['special_mode']['input_nd_flag'] = params_in['special_mode'].get("input_nd_flag", False)
        if params_in['special_mode'].get("old_coeff", CONST_VALUE0):
            old_fused_coefficient = math.ceil(100 * params_in['special_mode']["old_coeff"])
            params_in['special_mode']['old_coeff'] = old_fused_coefficient

        params_in['fixpipe_fusion_flag_dict'] = fixpipe_buffer_dict
        params_in['eltwise_dict'] = eltwise_dict

        # Determine whether it is dynamic shape or fixed shape
        params_in["dynamic_shape_flag"] = params_in.get("dynamic_shape_flag", False)


    def check_info_dict_v2(self, params_in: dict) -> None:
        """
        check the input params

        Parameters
        ----------
        params_in: the input params

        Returns
        -------
        """
        # preprocess the param
        fused_coefficient = params_in.get('fused_coefficient', None)
        params_in['fused_coefficient'] = process_default_value( \
            fused_coefficient, [0, 0, 0])
        fused_channel_wise = params_in.get('fused_channel_wise', None)
        params_in['fused_channel_wise'] = process_default_value( \
            fused_channel_wise, [0, 0, 0])
        fixpipe_fused_coefficient = params_in.get('fixpipe_fused_coefficient', None)
        params_in['fixpipe_fused_coefficient'] = process_default_value( \
            fixpipe_fused_coefficient, [0, 0])
        pooling_shape = params_in.get('pooling_shape', None)
        params_in['pooling_shape'] = process_default_value( \
            pooling_shape, [0, 0])
        pooling_stride = params_in.get('pooling_stride', None)
        params_in['pooling_stride'] = process_default_value( \
            pooling_stride, [0, 0])
        params_in['fm_l1_valid_size'] = params_in.get('fm_l1_valid_size', DEFAULT_VALUE)
        params_in['special_mode'] = params_in.get('special_mode', {})

        # check the type of param
        self.check_param_type(params_in, [dict])
        self.check_param_type(params_in.get('fm_shape'), [list])
        self.check_param_type(params_in.get('filter_shape'), [list])
        self.check_param_type(params_in.get('result_shape'), [list])
        self.check_param_type(params_in.get('fm_dtype'), [str])
        self.check_param_type(params_in.get('filter_dtype'), [str])
        self.check_param_type(params_in.get('result_dtype'), [str])
        self.check_param_type(params_in.get('output_num'), [int])
        self.check_param_type(params_in.get('mad_dtype'), [str])
        self.check_param_type(params_in.get('pad'), [list, tuple])
        self.check_param_type(params_in.get('stride'), [list, tuple])
        self.check_param_type(params_in.get('dilation'), [list, tuple])
        self.check_param_type(params_in.get('fused_coefficient'), [list])
        self.check_param_type(params_in.get('fused_channel_wise'), [list])
        self.check_param_type(params_in.get('group', CONST_VALUE1), [int])
        self.check_param_type(params_in.get('bias_flag', False), [bool, int])
        self.check_param_type(params_in.get('bias_dtype', "float32"), [str])
        self.check_param_type(params_in.get('op_type', 'conv2d'), [str])
        self.check_param_type(params_in.get('in_fm_memory_type'), [list])
        self.check_param_type(params_in.get('out_fm_memory_type'), [list])
        self.check_param_type(params_in.get('out_c_memory_type'), [list])
        self.check_param_type(params_in.get('l1_fusion_type'), [int])
        self.check_param_type(params_in.get('fusion_type', "0"), [str])
        self.check_param_type(params_in.get('kernel_name', "conv2d_kernel"), [str])
        self.check_param_type(params_in.get('reserved_ub', CONST_VALUE0), [int])
        self.check_param_type(params_in.get('fm_l1_valid_size'), [int])
        self.check_param_type(params_in.get('pooling_shape'), [list])
        self.check_param_type(params_in.get('pooling_stride'), [list])
        self.check_param_type(params_in.get('special_mode'), [dict])
        self.check_param_type(params_in['special_mode'].get("use_c04_mode", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get("l0a_dma_flag", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get(
                              "high_performance_mode", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get(
                              "high_precision_flag", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get("input_nd_flag", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get(
                              "4To2_structured_sparsity", False), [bool, int])
        self.check_param_type(params_in['special_mode'].get(
                              "scalar_num", CONST_VALUE0), [int])
        self.check_param_type(params_in['special_mode'].get(
                              "bias_set_zero_in_ub_flag", False), [bool, int])

        self.check_param_type(params_in.get('fixpipe_fused_type', []), [list])

        # check the length of param
        self.check_param_length(params_in.get('fm_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('filter_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('result_shape'), [SHAPE_LENGTH5])
        self.check_param_length(params_in.get('pad'), [SHAPE_LENGTH4])
        self.check_param_length(params_in.get('stride'), [SHAPE_LENGTH2])
        self.check_param_length(params_in.get('dilation'), [SHAPE_LENGTH2])
        self.check_param_length(params_in.get('fused_coefficient'), [SHAPE_LENGTH3])
        self.check_param_length(params_in.get('fused_channel_wise'), [SHAPE_LENGTH3])
        self.check_param_length(params_in.get('pooling_shape'), [SHAPE_LENGTH2])
        self.check_param_length(params_in.get('pooling_stride'), [SHAPE_LENGTH2])

        # check the support range of param
        self.check_support_range(params_in.get('fm_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('filter_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('result_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('mad_dtype'), self.dtype_dict)
        self.check_support_range(params_in.get('bias_dtype', "float32"),
                                 self.v220_bais_support_dtype)
        self.check_support_range(params_in.get('op_type', 'conv2d'), self.op_type_dict)

        # check fixpipe fused type
        tmp_fixpipe_fused_type = params_in.get('fixpipe_fused_type', [])
        for fusion_dict in tmp_fixpipe_fused_type:
            self.check_support_range(fusion_dict.get('pre_conv', ""), self.pre_conv)
            self.check_support_range(fusion_dict.get('pre_act', ""), self.pre_act)
            self.check_support_range(fusion_dict.get('post_anti_quant', ""), self.post_anti_quant)
            self.check_support_range(fusion_dict.get('post_eltwise', ""), self.post_eltwise)
            self.check_support_range(fusion_dict.get('post_act', ""), self.post_act)
            self.check_support_range(fusion_dict.get('post_quant', ""), self.post_quant)
            self.check_support_range(fusion_dict.get('post_transform', ""), self.post_transform)

        # check the pooling params
        pooling_shape = params_in.get('pooling_shape')
        pooling_stride = params_in.get('pooling_stride')
        # the channel align to unit of cube
        _ca0 = params_in["filter_shape"][4]
        if _ca0 != C04:
            self.check_support_value(pooling_shape, [CONST_VALUE0, CONST_VALUE0])
            self.check_support_value(pooling_stride, [CONST_VALUE0, CONST_VALUE0])

        # check the fm_l1_valid_size whether is legal
        if params_in.get('fm_l1_valid_size') != DEFAULT_VALUE:
            # according to the use_c04_mode, align the channel
            use_c04_mode = params_in['special_mode'].get('use_c04_mode', \
                CONST_VALUE0)
            a_shape = deepcopy(params_in.get('fm_shape'))
            if use_c04_mode == 1 and a_shape[4] == 4:
                a_shape[4] = 16
            fm_shape_size = self.input_data_byte_width[params_in.get('fm_dtype')]
            for elt in a_shape:
                fm_shape_size *= elt
            input_data_level = params_in.get('fm_l1_valid_size') / fm_shape_size
            if input_data_level not in self.input_data_level.keys():
                raise_err_message_cube(
                    "the fm_l1_valid_size must be 1/2, 3/4, 1 times fm_shape_size "
                    "but fm_l1_valid_size is {}, fm_shape_size is {}.".format(
                        params_in.get('fm_l1_valid_size'), fm_shape_size))
            params_in['fm_l1_valid_size_level'] = input_data_level
        else:
            # if no fm_l1_valid_size param, set the default value 0
            params_in['fm_l1_valid_size_level'] = CONST_VALUE0

    def preprocess_info_dict_v2(self, params_in: dict) -> None:
        """
        encode the information of shape to the list of uint32 digit

        Parameters
        ----------
        params_in: dict of params include all information of shape

        Returns
        -------
        """
        # get the missing information on interface
        l1_fusion_type = params_in.get('l1_fusion_type', DEFAULT_VALUE)
        # source buffer of input and destination buffer of output
        in_fm_memory_type = params_in.get('in_fm_memory_type', [DDR_MEMORY])
        out_fm_memory_type = params_in.get('out_fm_memory_type', [DDR_MEMORY])
        # process the fusion type
        # if fuison type is depth fusion, then source and destination is DDR,
        # the l1_fusion_type is L1_no_fusion
        # 0 represent L1_depth_fusion; 1 represent L1_breadth_fusion,
        # 2 represent L1_no_fusion; 3 represent L2_fusion
        if l1_fusion_type == DEFAULT_VALUE:
            l1_fusion_type = L1_NO_FUSION
        # 2 represent L2_no_fusion; 3 represent L2_fusion
        if (L2_MEMORY in in_fm_memory_type) or (L2_MEMORY in out_fm_memory_type):
            l2_fusion_type = L2_FUSION
        else:
            l2_fusion_type = L2_NO_FUSION

        # encode the memory type
        in_fm_memory_type_encode = encode_memory_type("in", in_fm_memory_type)
        out_fm_memory_type_encode = encode_memory_type("out", out_fm_memory_type)
        # set the default value of these params
        op_type = params_in.get('op_type', 'conv2d')
        fusion_type = params_in.get('fusion_type', "0")
        kernel_name = params_in.get('kernel_name', "conv2d_kernel")
        a_dtype = params_in.get('fm_dtype', 'float16')
        b_dtype = params_in.get('filter_dtype', 'float16')
        c_dtype = params_in.get('result_dtype', 'float16')
        mad_dtype = params_in.get('mad_dtype', 'float16')
        bias_flag = params_in.get('bias_flag', False)
        bias_flag = (1 if bias_flag else 0)
        if bias_flag:
            bias_dtype = params_in.get('bias_dtype', 'float32')
        else:
            bias_dtype = 'float32'
        output_num = params_in.get('output_num', CONST_VALUE1)
        l0a_dma_flag = get_l0a_dma_flag(params_in)
        fixpipe_buffer_dict = params_in.get('fixpipe_fused_type', [])

        # the channel align to unit of cube
        _ca0 = params_in["filter_shape"][4]
        if _ca0 != C04:
            config = CUBE_MKN[params_in["filter_dtype"]]
            _ca0 = config['mac'][1]

        a_shape = params_in.get('fm_shape')
        # check the divisor should not be 0
        self.check_illegal_value(_ca0, 0)
        a_shape[1] = (a_shape[1]*a_shape[4] + _ca0 - 1) // _ca0
        a_shape[4] = _ca0
        b_shape = params_in.get('filter_shape')
        b_shape[1] = (b_shape[1]*b_shape[4] + _ca0 - 1) // _ca0
        b_shape[4] = _ca0
        c_shape = params_in.get('result_shape')
        c_shape = ([0, 0, 0, 0, 0] if c_shape is None else c_shape)

        # processing fixed-point number
        fused_coefficient = params_in.get('fused_coefficient')
        fused_coefficient = [math.ceil(100*elt) for elt in fused_coefficient]
        fused_channel_wise = params_in.get('fused_channel_wise')
        fused_channel_wise = [math.ceil(100*elt) for elt in fused_channel_wise]
        fixpipe_fused_coefficient = params_in.get('fixpipe_fused_coefficient')
        fixpipe_fused_coefficient = [math.ceil(100*elt) for elt in fixpipe_fused_coefficient]
        reserved_ub = params_in.get('reserved_ub', CONST_VALUE0)

        # transform the value of -1 to 0 for fm_l1_valid_size
        if params_in.get('fm_l1_valid_size') == DEFAULT_VALUE:
            params_in['fm_l1_valid_size'] = CONST_VALUE0
        # endocde the value of fm_l1_valid_size_level
        if params_in.get('fm_l1_valid_size_level') != CONST_VALUE0:
            raw_fm_l1_valid_size_level = params_in.get('fm_l1_valid_size_level')
            params_in['fm_l1_valid_size_level'] = self.input_data_level[raw_fm_l1_valid_size_level]

        # processed params
        params_in['fm_shape'] = a_shape
        params_in['filter_shape'] = b_shape
        params_in['result_shape'] = c_shape
        params_in['output_num'] = output_num
        params_in['fm_dtype'] = self.dtype_dict.get(a_dtype)
        params_in['filter_dtype'] = self.dtype_dict.get(b_dtype)
        params_in['result_dtype'] = self.dtype_dict.get(c_dtype)
        params_in['mad_dtype'] = self.dtype_dict.get(mad_dtype)
        # l1_fusion_type have three states:  0 represent L1_depth_fusion;
        # 1 represent L1_breadth_fusion, 2 represent L1_no_fusion
        params_in['l1_fusion_type'] = l1_fusion_type
        # 2 represent L2_no_fusion; 3 represent L2_fusion
        params_in['l2_fusion_type'] = l2_fusion_type
        # source buffer of input and destination buffer of output
        params_in['in_fm_memory_type'] = in_fm_memory_type_encode
        params_in['out_fm_memory_type'] = out_fm_memory_type_encode
        # set the default value of these params
        params_in['pad'] = params_in.get('pad', [0, 0, 0, 0])
        params_in['stride'] = params_in.get('stride', [CONST_VALUE1, CONST_VALUE1])
        params_in['dilation'] = params_in.get('dilation', [CONST_VALUE1, CONST_VALUE1])
        params_in['group'] = params_in.get('group', CONST_VALUE1)
        params_in['bias_flag'] = bias_flag
        params_in['bias_dtype'] = self.dtype_dict.get(bias_dtype)
        params_in['op_type'] = op_type
        params_in['fusion_type'] = fusion_type
        params_in['kernel_name'] = kernel_name
        # the fused_channel_wise and fused_coefficient are fixed-point number
        # count to two decimal places
        params_in['fused_coefficient'] = fused_coefficient
        params_in['fused_channel_wise'] = fused_channel_wise
        params_in['fixpipe_fused_coefficient'] = fixpipe_fused_coefficient
        params_in['reserved_ub'] = reserved_ub
        params_in['special_mode'] = params_in.get('special_mode', {})
        params_in['special_mode']['use_c04_mode'] = params_in['special_mode'].get(
            'use_c04_mode', False)
        params_in['special_mode']['l0a_dma_flag'] = l0a_dma_flag
        params_in['special_mode']['high_performance_mode'] = params_in['special_mode'].get(
            'high_performance_mode', False)
        params_in['special_mode']['high_precision_mode'] = params_in['special_mode'].get(
            'high_precision_mode', False)
        params_in['special_mode']['input_nd_flag'] = params_in['special_mode'].get(
            "input_nd_flag", False)
        params_in['special_mode']['4To2_structured_sparsity'] = params_in['special_mode'].get(
            "4To2_structured_sparsity", False)
        params_in['special_mode']['split_w_flag'] = params_in['special_mode'].get(
            "split_w_flag", False)
        params_in['special_mode']['scalar_num'] = params_in['special_mode'].get(
            "scalar_num", CONST_VALUE0)
        params_in['special_mode']['bias_set_zero_in_ub_flag'] = params_in['special_mode'].get(
            "bias_set_zero_in_ub_flag", False)

        params_in['fixpipe_fused_type'] = fixpipe_buffer_dict

        # Determine whether it is dynamic shape or fixed shape
        params_in["dynamic_shape_flag"] = params_in.get("dynamic_shape_flag", False)

    # using repository manager module to get tiling in repo in new tiling access
    def query_repository(self, info_dict: dict, tiling_type_num: int) -> (str, bool):
        hit_repo_flag = False
        import tvm
        attr = self.get_kb_query_attr(tvm.type_convert(info_dict))
        search_config = {}
        search_config["op_type"] = "conv2d"
        if tiling_type_num == CUSTOM_TILING_TYPE:
            search_config["serach_mode"] = "user"
        elif tiling_type_num == REPOSITORY_TILING_TYPE or tiling_type_num == AUTO_TILING_TYPE:
            search_config["serach_mode"] = "normal"
        elif tiling_type_num == MODEL_TILING_TYPE:
            # when model tiling, infer tiling by costmodel instead of repo, return empty string
            return "", hit_repo_flag
        else:
            # when tiling type is not supported, raise error
            raise_err_message_cube("the tiling_type is invalid for repository search.")
        search_config["dynamic"] = self.input_args.get("dynamic_shape_flag", False)
        # use repository manage to serach tiling in repo
        repo_tiling_list = cann_kb_search(attr, search_config)
        # dfx for kernel name
        attr_md5 = hashlib.md5(attr.encode(encoding="utf-8")).hexdigest()
        kernel_name = info_dict.get("kernel_name")
        tmp_func = lambda x: "invalid" if x is None else x
        info_str = f"[auto_tiling]MD5:[{tmp_func(attr_md5)}] kernel_name:[{tmp_func(kernel_name)}]"
        AUTOTILINGLOG.info(f"{info_str}")
        # if is dynamic, return list, else return str
        if self.input_args.get("dynamic_shape_flag", False):
            AUTOTILINGLOG.info("Repository search is invalid for dynamic shapes and it will return an empty list.")
            return repo_tiling_list, hit_repo_flag
        else:
            if repo_tiling_list:
                repo_tiling = json.dumps(repo_tiling_list[0].get("knowledge", {}))
                hit_repo_flag = True
            else:
                # when case cannot be found in repo for static, return empty string
                repo_tiling = json.dumps({'AL0_matrix': [0, 0, 0, 0, 0, 0, 0], 'AL1_shape': None, 'AUB_shape': None,
                    'BL0_matrix': [0, 0, 0, 0, 0, 0, 0], 'BL1_shape': None, 'BUB_shape': None,
                    'CL0_matrix': [0, 0, 0, 0, 0, 0, 0], 'INPUT_L1_BT_param': None, 'INPUT_L1_FB_param': None,
                    'INPUT_L1_eltwise_param': None, 'INPUT_L1_sparse_index': None,
                    'L0C_OUTPUT_matrix': [0, 0, 16, 16, 0, 0], 'UB_channel_wise_input': [None, None, False],
                    'block_dim': [0, 0, 0, 0, 0], 'control_reorder_flag': 'NM',
                    'manual_pingpong_buffer': {'AL0_pbuffer': 0, 'AL1_pbuffer': 0, 'AUB_pbuffer': 0,
                    'BL0_pbuffer': 0, 'BL1_pbuffer': 0, 'BUB_pbuffer': 0, 'CL0_pbuffer': 0, 'INPUT_L1_BT_pbuffer': 0,
                    'INPUT_L1_FB_pbuffer': 0, 'INPUT_L1_eltwise_pbuffer': 0, 'L0C_OUTPUT_pbuffer': 0,
                    'UBG_pbuffer': 0}, 'special_optimize_flag': 0, 'tbe_compile_para': 0, 'vector_block_num': 0})
        return repo_tiling, hit_repo_flag
