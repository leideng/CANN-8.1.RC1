#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2022. Huawei Technologies Co., Ltd. All rights reserved.

Define the function for update repository
"""

import json
import fcntl
import os
from pathlib import Path
from typing import List

from tbe.common.repository_manager.utils.file_sys_util import FileSysUtil
from tbe.common.repository_manager.utils.repository_manager_log import LOG_INSTANCE

from auto_tune.util_atc import FILE_FLAG
from auto_tune.util_atc import FILE_MODE_640
from auto_tune.util_atc import create_dir
from auto_tune.util_atc import OPP_PATH
from auto_tune.util_atc import remove_file


UNIFIED_DIR_NAME = "unified_bank"
SOC_VERSION = ["Ascend910A", "Ascend910B", "Ascend910PremiumA", "Ascend910ProA", "Ascend910ProB", "Ascend310",
               "Ascend910B1", "Ascend910B2", "Ascend910B2C", "Ascend910B3", "Ascend910B4", "Ascend910B4-1",
               "Ascend910_9391", "Ascend910_9381", "Ascend910_9392", "Ascend910_9382", "Ascend910_9372",
               "Ascend910_9362",
               "Ascend310B1", "Ascend310B2", "Ascend310B3", "Ascend310B4", "AS31XM1X",
               "Ascend610Lite", "Ascend610", "BS9SX1AA", "BS9SX1AB", "BS9SX1AC",
               "Ascend310P3", "Ascend310P1", "Hi3796CV300ES", "Hi3796CV300CS", "SD3403", "BS9SX2AA", "BS9SX2AB",
               "MC61AM21AA", "MC61AM21AB", "Ascend310P5", "Ascend310P7"]

# file path
THIS_FILE_NAME = __file__
FILE_PATH = os.path.dirname(os.path.realpath(THIS_FILE_NAME))


def get_file_name(file_in: str) -> str:
    """
    get file name without ".json"
    """
    file_name  = file_in.rsplit(os.sep, 1)[1].split('.')[0]
    return file_name


def get_json_list(soc_unified_dir: str) -> List[str]:
    """
    get all json file in soc_unified_dir
    """
    custom_json_file_list = []
    for path in Path(soc_unified_dir).glob("*.json"):
        if not os.access(path, os.R_OK):
            LOG_INSTANCE.warn("No permission to read file %s", path)
            continue
        path = os.fspath(path)
        custom_json_file_list.append(path)
    return custom_json_file_list


def get_same_file_dict(unified_dir: str, same_file_dict: dict) -> None:
    """
    get same file dict
    same_file_dict: json file path dict for same file

    return:
    """
    json_file_list = get_json_list(unified_dir)
    if not json_file_list:
        LOG_INSTANCE.warn("No qualified json file to merge in path %s.", unified_dir)
    for json_file in json_file_list:
        file_name = get_file_name(json_file)
        if file_name not in same_file_dict:
            same_file_dict[file_name] = []
        same_file_dict[file_name].append(json_file)


def check_dst_file_exist(dst_file: str) -> bool:
    """
    check if dst_json file exist

    return: true or false
    """
    if os.path.isfile(dst_file):
        LOG_INSTANCE.warn("Object file %s has already exist, it will not merge.", dst_file)
        return True
    return False


def sort_by_cost_time(all_case_dict: dict) -> None:
    """
    sort all case in dict by cost_time in ascend order
    """
    for case in all_case_dict:
        all_case_dict[case] = sorted(all_case_dict[case], key=lambda repo:repo['cost_time'])


def get_all_custom_cases(same_file_list: List[str]) -> dict:
    """
    get all cases dict in same files
    key : case_id
    value: case list
    return all_case_dict
    """
    all_case_dict = {}
    total_cnt = 0
    for single_file in same_file_list:
        total_cnt = get_custom_cases(single_file, all_case_dict, total_cnt)
    LOG_INSTANCE.debug("%s different cases will be merged to %s tilings.", total_cnt, len(all_case_dict))
    return all_case_dict


def get_final_cases(built_in_file: str, all_case_dict: dict) -> None:
    """
    compare built-in repo with repo in all_case_dict
    remove the cases no better than built-in
    """
    if not os.path.exists(built_in_file):
        LOG_INSTANCE.info("No built-in bank file: %s", built_in_file)
        return
    with open(built_in_file, 'r') as b_handler:
        file_loc = b_handler.tell()
        built_in_case = json.load(b_handler)
        for case_id, case_list in built_in_case.items():
            if case_id in all_case_dict and \
                case_list[0].get("cost_time") <= all_case_dict.get(case_id)[0].get("cost_time"):
                all_case_dict.pop(case_id)
        b_handler.seek(file_loc)


def generate_merge_file(dst_json_file: str, update_cases: dict) -> None:
    """
    generate json file for merge
    """
    if not os.path.isfile(dst_json_file):
        with os.fdopen(os.open(dst_json_file, FILE_FLAG, FILE_MODE_640), 'w'):
            pass

    with open(dst_json_file, 'r+') as json_handle:
        fcntl.flock(json_handle.fileno(), fcntl.LOCK_EX)
        FileSysUtil.write_json_file(json_handle, update_cases)
        fcntl.flock(json_handle.fileno(), fcntl.LOCK_UN)


def get_custom_cases(single_file: str, all_case_dict: dict, total_cnt: int) -> int:
    """
    get custom cases from single file
    """
    with open(single_file, 'r', FILE_MODE_640) as f_handler:
        case_id_dict = json.load(f_handler)
        for case_id, case_tiling in case_id_dict.items():
            if case_id not in all_case_dict :
                all_case_dict[case_id] = []
            all_case_dict[case_id].append(case_tiling[0]) # case_tiling is a list contain a tiling str
            total_cnt = total_cnt + 1
    return total_cnt


def merge_same_files(base_file: str, same_file_list: List[str], built_in_path: str, dst_path: str) -> None:
    """
    get all cases dict in same files
    key : case_id
    value: case list
    return all_case_dict
    """
    base_json = '{}.json'.format(base_file)
    built_in_file = os.path.join(built_in_path, base_json)
    dst_json_file = os.path.join(dst_path, base_json)
    if check_dst_file_exist(dst_json_file):
        return
    all_case_dict = {}
    tmp_path = os.path.join(os.getcwd(), "repo.tmp")
    with os.fdopen(os.open(tmp_path, FILE_FLAG, FILE_MODE_640), 'w') as tmp_file:
        fcntl.flock(tmp_file.fileno(), fcntl.LOCK_EX)
        all_case_dict = get_all_custom_cases(same_file_list)
        if all_case_dict:
            sort_by_cost_time(all_case_dict)
            get_final_cases(built_in_file, all_case_dict)
            if not all_case_dict:
                LOG_INSTANCE.info("All cases in %s are better in built-in ga bank.", base_json)
                fcntl.flock(tmp_file.fileno(), fcntl.LOCK_UN)
                remove_file(tmp_path)
                return
            create_dir(dst_path)
            generate_merge_file(dst_json_file, all_case_dict)
        fcntl.flock(tmp_file.fileno(), fcntl.LOCK_UN)
    remove_file(tmp_path)
    return


def check_src_type(src_path_list: List[str]) -> List[str]:
    """
    check if the src_path is tune_bank_path
    return :
    """
    src_paths = []

    for src_path in src_path_list:
        paths = os.listdir(src_path)
        if not paths:
            LOG_INSTANCE.warn("Please check your input src_path: %s", src_path)
            continue
        src_paths.append(src_path)
    return src_paths


def check_dir(base_path: str) -> bool:
    """
    check if soc_dir exist
    """
    if not os.path.isdir(base_path):
        LOG_INSTANCE.warn("No ga bank path: %s.", base_path)
        return False
    if not os.access(base_path, os.R_OK):
        LOG_INSTANCE.warn("No permission to read dir %s.", base_path)
        return False
    return True


def merge_single_soc(dir_list: List[str], opp_path: str, soc_version: str, dst_path: str) -> bool:
    """
    merge all cases in tune_bank_path/{soc}/unified_bank with built-in
    """
    same_file_dict = {}
    for src_dir in dir_list:
        src_unified_dir = os.path.realpath(os.path.join(src_dir, soc_version, UNIFIED_DIR_NAME))
        if not check_dir(src_unified_dir):
            continue
        get_same_file_dict(src_unified_dir, same_file_dict)

    built_in_path = os.path.realpath(os.path.join(opp_path, "built-in", "data", "op", soc_version, "unified_bank"))
    if not os.path.exists(built_in_path):
        built_in_path = os.path.realpath(os.path.join(opp_path, "data", "built_in", "op", soc_version, "unified_bank"))
    if not os.path.isdir(built_in_path):
        LOG_INSTANCE.warn("Please check your built-in bank path in opp package: %s.", built_in_path)
    for base_file in same_file_dict:
        if '_runtime_kb' in base_file:
            continue
        merge_same_files(base_file,
            same_file_dict.get(base_file), built_in_path, dst_path)
    return True


def repo_merge_offline(input_src_path_list: List[str], base_dst_path: str) -> None:
    """
    merge repository function interface
    base_src_path: json file path custom_repo_dir
    xxx/data/ or tune_bank_path
    tune_bank_path/soc/unified_bank/
    base_dst_path: json file path new_repo_dir

    return: None
    """
    opp_path = os.getenv(OPP_PATH)
    if not opp_path:
        LOG_INSTANCE.error("Please check your ASCEND_OPP_PATH in env.")
        return False
    base_src_path_list = check_src_type(input_src_path_list)
    if not base_src_path_list:
        LOG_INSTANCE.warn("All src paths don't support merge repository.")
        return False
    soc_version_list = []
    for base_src_path in base_src_path_list:
        dir_list = os.listdir(base_src_path)
        for soc_version in dir_list:
            if soc_version not in SOC_VERSION:
                continue
            soc_version_list.append(soc_version)
    if not soc_version_list:
        LOG_INSTANCE.warn("No soc_version found in src.")
        return True
    soc_version_set = set(soc_version_list)
    base_dst_path = os.path.realpath(base_dst_path)
    for soc_version in soc_version_set:
        LOG_INSTANCE.info("Start to merge %s unified bank.", soc_version)
        dst_path = os.path.join(base_dst_path, soc_version, UNIFIED_DIR_NAME)
        merge_single_soc(base_src_path_list, opp_path, soc_version, dst_path)
        LOG_INSTANCE.info("Unified bank for %s has finished merging.", soc_version)
    return True
