#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_vbi_tensor_op.py
DESC:     provide vector instructions
CREATED:  2021-09-8 18:53:42
"""
from tbe.tik.common import TikUtil
from tbe.tik.common import DTYPE_SIZE
from tbe.tik.common.common_util import get_blk_valid_list
from tbe.tik.common.common_util import _MIN_BLK_LEN
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import reduce_mul
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp
from tbe.tik.tik_lib.tik_vector_api.tik_vbi_control import VbiControlOp


class VbiTensorOp(TensorOp):
    """
    vbi tensor op
    """
    @staticmethod
    def get_vbi_src_common_extend_interval(block_len, time, stride_list, src_tensor_op, vbi_control_op):
        """
        for vbi, get src0_offset/src1 tensor each blk interval
        """
        blk_stride_ele, rep_stride_ele = stride_list
        extend_interval = []
        begin = (time * vbi_control_op.horizontal_repeat_times_value + 1) * rep_stride_ele + src_tensor_op.offset_value
        if time == vbi_control_op.vertical_repeat_times_value - 1:
            end = begin + (7 * blk_stride_ele + block_len) * (vbi_control_op.horizontal_repeat_times_value - 1)
        else:
            end = begin + (7 * blk_stride_ele + block_len) * vbi_control_op.horizontal_repeat_times_value
        extend_interval.append(
            [begin * DTYPE_SIZE[src_tensor_op.tensor_obj.dtype], end * DTYPE_SIZE[src_tensor_op.tensor_obj.dtype]])
        return extend_interval

    @staticmethod
    def get_vbi_src_common_need_size(dtype, mask_len, total_repeat_times):
        """
        for vbi instruction, get src0_offset/src1 need size(could be an expr),
        unit is elements. when mask is scalar-list, mask is considered as 128

        Parameters
        ----------
        dtype: dst/src1/src0 dtype, float16
        mask_len: length between lowest digit and top effective digit of mask
        total_repeat_times: horizontal_repeat_times multiplied by vertical_repeat_times

        Returns
        -------
        need_src_block: need elements
        """
        if mask_len is None:
            need_src_block = BLK_NUM_PER_REP * total_repeat_times
        else:
            valid_block_len = ceil_div(mask_len, ONE_BLK_SIZE // DTYPE_SIZE[dtype])
            need_src_block = (total_repeat_times - 1) * BLK_NUM_PER_REP + valid_block_len
        return need_src_block

    def check_vbi_tensor_not_single_point(self):
        """
        for vbi, tensor[imme] or tensor[scalar] is not support
        """
        msg = "vbi %s not support single point, please use tensor[imme:] " \
              "or tensor[scalar:]" % self.tensor_op_name
        TikCheckUtil.check_equality(self.tensor_obj.is_single_point(), False, msg)

    def get_vbi_dst_need_size(self, block_len, vbi_control_op):
        """
        for vbi instruction, get dst operator need size, unit is element
        when mask is scalar/scalar-list, mask is considered as 128

        Parameters
        ----------
        vbi_control_op: vbi control op
        block_len: elements num per block(32B)
        Returns
        -------
        max_offset
        """
        mask_len = vbi_control_op.mask_len
        vertical_repeat_times = vbi_control_op.vertical_repeat_times_value
        dst_repeat_offset = vbi_control_op.vertical_repeat_offset_value
        # for cal dst_extent when mask is scalar/scalar-list
        if mask_len is None or is_basic_expr(TikUtil.to_list(mask_len)):
            max_offset = (vertical_repeat_times - 1) * dst_repeat_offset + \
                         (BLK_NUM_PER_REP - 1) * self.blk_stride_value * block_len + \
                         block_len
            return max_offset
        if mask_len % block_len == 0:
            max_offset = (vertical_repeat_times - 1) * dst_repeat_offset + \
                         (mask_len // block_len - 1) * self.blk_stride_value * block_len + \
                         block_len
        else:
            max_offset = (vertical_repeat_times - 1) * dst_repeat_offset + \
                         (mask_len // block_len) * self.blk_stride_value * block_len + \
                         mask_len % block_len
        return max_offset

    def check_vbi_dst_offset_overflow(self, print_name, src0_offset_op, vbi_control_op, block_list_src0_offset):
        """
        for vbi instruction, check whether dst and src0_offset is overflow

        Returns
        -------
        None
        """
        mask_len = vbi_control_op.mask_len
        if mask_len is None or is_basic_expr(TikUtil.to_list(mask_len)):
            return
        total_repeat_times = Expr(vbi_control_op.horizontal_repeat_times *
                                  vbi_control_op.vertical_repeat_times).eval_value()
        if total_repeat_times is None:
            return
        if total_repeat_times == 0:
            return
        block_len = ONE_BLK_SIZE // DTYPE_SIZE[self.tensor_obj.dtype]
        # check src0_offset tensor overflow
        valid_block_len = ceil_div(mask_len, block_len)
        src0_offset_control_op = VbiControlOp(
            valid_block_len, (vbi_control_op.vertical_repeat_times_value, vbi_control_op.horizontal_repeat_times_value),
            vbi_control_op.mask_value, vbi_control_op.vertical_repeat_offset_value)
        src0_offset_op.check_tensor_op_overflow(print_name, src0_offset_control_op, block_list_src0_offset)
        # check dst tensor overflow
        max_offset = self.get_vbi_dst_need_size(block_len, vbi_control_op)
        offset = self.tensor_obj.offset
        total_size = reduce_mul(self.tensor_obj.original_shape)
        need_offset = Expr(max_offset + offset).eval_value()
        if need_offset is not None:
            TikCheckUtil.check_le(need_offset, total_size,
                                  "dst tensor overflow, instruction need {} but "
                                  "only {}".format(need_offset, total_size))

    def get_vbi_src1_tensor_need_size(self, vbi_control_op):
        """
        for vbi instruction, get src1 need size(could be an expr),
        unit is elements, when mask is scalar-list, mask is considered as 128

        Parameters
        ----------
        vbi_control_op: vbi_control_op

        Returns
        -------
        src1 need size
        """
        mask_len = vbi_control_op.mask_len
        if vbi_control_op.repeat_mode == 0:
            src1_num_per_repeat = 1
            need_size = src1_num_per_repeat * vbi_control_op.total_repeat_times_value
        else:
            need_size = self.get_vbi_src_common_need_size(self.tensor_obj.dtype, mask_len,
                                                          vbi_control_op.total_repeat_times_value)
        return need_size

    def check_vbi_src1_tensor_overflow(self, vbi_control_op):
        """
        for vbi instruction, check whether src1 is overflow

        Parameters
        ----------
        vbi_control_op: vbi_control_op

        Returns
        -------
        None
        """
        if vbi_control_op.mask_len is None:
            return
        # first check src1
        total_size = reduce_mul(self.tensor_obj.original_shape)
        need_size = self.get_vbi_src1_tensor_need_size(vbi_control_op)

        if self.context:
            need_size = Expr(need_size + self.offset_value).eval_value()
        else:
            need_size = Expr(need_size + self.tensor_obj.offset).eval_value()
        if need_size is not None:
            TikCheckUtil.check_le(need_size, total_size,
                                  "vbi src1 tensor overflow, instruction need %s but only %s" % (need_size, total_size))

    def check_vbi_src1_src0offset_overlap(self, src0_offset_op, vbi_control_op):
        """

        Returns
        -------

        """
        if src0_offset_op.tensor_obj.buffer == self.tensor_obj.buffer:
            src0_offset_start = Expr(src0_offset_op.tensor_obj.offset *
                                     DTYPE_SIZE[src0_offset_op.tensor_obj.dtype]).eval_value()
            src0_offset_stop_value = self.get_vbi_src_common_need_size(
                                     self.tensor_obj.dtype,
                                     vbi_control_op.mask_len,
                                     vbi_control_op.total_repeat_times_value) * \
                                     DTYPE_SIZE[src0_offset_op.tensor_obj.dtype]
            src0_offset_stop = Expr(src0_offset_start + src0_offset_stop_value).eval_value()
            src1_start = Expr(self.tensor_obj.offset * DTYPE_SIZE[self.tensor_obj.dtype]).eval_value()
            src1_stop_value = self.get_vbi_src1_tensor_need_size(vbi_control_op) * DTYPE_SIZE[self.tensor_obj.dtype]
            src1_stop = Expr(src1_start + src1_stop_value).eval_value()
            if all((value is not None) for value in (src0_offset_start, src0_offset_stop, src1_start, src1_stop)):
                if max(src0_offset_start, src1_start) < min(src0_offset_stop, src1_stop):
                    TikCheckUtil.raise_error("vbi src0_offset and src1 address overlapping error.")

    def check_vbi_dst_src1_overlap(self, src1_tensor_op, vbi_control_op):
        """
        check vbi dst src1 overlap

        Parameters
        ----------
        src1_tensor_op: src1 tensor_op
        vbi_control_op: vbi control_op

        Returns
        -------
        None
        """
        self.get_mask(vbi_control_op.mask)
        if self.tensor_obj.buffer == src1_tensor_op.tensor_obj.buffer:
            if src1_tensor_op.offset_value is None:
                return
            dst_block_len = ONE_BLK_SIZE // DTYPE_SIZE[self.tensor_obj.dtype]
            blk_valid_list = get_blk_valid_list(self.mask_value, self.tensor_obj.dtype, dst_block_len)
            if isinstance(vbi_control_op.total_repeat_times_value, int):
                if vbi_control_op.total_repeat_times_value == 1:
                    self._vbi_dst_src1_overlap_repeat_once(blk_valid_list, src1_tensor_op, vbi_control_op)
                else:
                    self._vbi_dst_src1_overlap_multi(blk_valid_list, src1_tensor_op, vbi_control_op)

    def check_vbi_dst_src0offset_overlap(self, src0_offset_op, vbi_control_op):
        """
        check vbi dst src0_offset overlap

        Parameters
        ----------
        src0_offset_op: src0_offset op
        vbi_control_op: vbi control_op

        Returns
        -------
        None
        """
        self.get_mask(vbi_control_op.mask)
        if self.tensor_obj.buffer == src0_offset_op.tensor_obj.buffer:
            if src0_offset_op.offset_value is None:
                return
            dst_block_len = ONE_BLK_SIZE // DTYPE_SIZE[self.tensor_obj.dtype]
            blk_valid_list = get_blk_valid_list(self.mask_value, self.tensor_obj.dtype, dst_block_len)
            if vbi_control_op.total_repeat_times == 1:
                self._vbi_dst_src0offset_overlap_repeat_once(blk_valid_list, src0_offset_op, vbi_control_op)
            else:
                self._vbi_dst_src0offset_overlap_repeat_multi(blk_valid_list, src0_offset_op, vbi_control_op)

    def get_vbi_dst_extend_interval(self, blk_valid_list, time, vbi_control_op):
        """
        for vbi, get dst tensor each blk interval
        """
        extend_interval = []
        block_len = ONE_BLK_SIZE // DTYPE_SIZE[self.tensor_obj.dtype]
        for blk_id in blk_valid_list:
            begin = blk_id * self.blk_stride_value * block_len + \
                    time * vbi_control_op.vertical_repeat_offset_value + self.offset_value
            end = begin + block_len
            extend_interval.append(
                [begin * DTYPE_SIZE[self.tensor_obj.dtype], end * DTYPE_SIZE[self.tensor_obj.dtype]])
        return extend_interval

    def _vbi_dst_src1_overlap_repeat_once(self, blk_valid_list, src1_tensor_op, vbi_control_op):
        if self.offset_value == src1_tensor_op.offset_value and \
                ((vbi_control_op.repeat_mode == 0 and vbi_control_op.mask_len == 1) or
                 (vbi_control_op.repeat_mode == 1 and vbi_control_op.mask_len == 8)):
            # dst 100 percent same with src1, correct
            pass
        else:
            dst_interval = self.get_vbi_dst_extend_interval(
                blk_valid_list, 0, vbi_control_op)
            src1_start = src1_tensor_op.offset_vlaue * DTYPE_SIZE[src1_tensor_op.tensor_obj.dtype]
            src1_stop = src1_start + (1 if vbi_control_op.repeat_mode == 0 else 8) * \
                        DTYPE_SIZE[src1_tensor_op.tensor_obj.dtype]
            src1_intervel = [[src1_start, src1_stop]]
            if self.check_overlapping(dst_interval, src1_intervel) is False:
                TikCheckUtil.raise_error(
                    "vbi dst and src1 address overlapping error. when repeat_times=1, "
                    "only support dst and src1 100 percent same.")

    def _vbi_dst_src1_overlap_multi(self, blk_valid_list, src1_tensor_op, vbi_control_op):
        for v_id in range(vbi_control_op.vertical_repeat_times_value):
            dst_interval = self.get_vbi_dst_extend_interval(
                blk_valid_list, v_id, vbi_control_op)
            if vbi_control_op.repeat_mode == 0:
                stride_list = [0, 1]
                src1_intervel = self.get_vbi_src_common_extend_interval(
                    _MIN_BLK_LEN, v_id, stride_list, src1_tensor_op, vbi_control_op)
            else:
                stride_list = [1, 8]
                src1_intervel = self.get_vbi_src_common_extend_interval(
                    _MIN_BLK_LEN, v_id, stride_list, src1_tensor_op, vbi_control_op)
            if self.check_overlapping(dst_interval, src1_intervel) is False:
                TikCheckUtil.raise_error(
                    "vbi dst and src1 address overlapping error. It is not support "
                    "iteration N's destination is the source of next iteration")

    def _vbi_dst_src0offset_overlap_repeat_once(self, blk_valid_list, src0_offset_op, vbi_control_op):
        if self.offset_value == src0_offset_op.offset_value and vbi_control_op.mask_len == 8:
            # dst 100 percent same with src0_offset, correct
            pass
        else:
            dst_interval = self.get_vbi_dst_extend_interval(
                blk_valid_list, 0, vbi_control_op)
            src0_offset_start = src0_offset_op.offset_value * DTYPE_SIZE[src0_offset_op.tensor_obj.dtype]
            src0_offset_stop = src0_offset_start + 8 * DTYPE_SIZE[src0_offset_op.tensor_obj.dtype]
            src0_offset_interval = [[src0_offset_start, src0_offset_stop]]
            if self.check_overlapping(dst_interval, src0_offset_interval) is False:
                TikCheckUtil.raise_error(
                    "vbi dst and src0_offset address overlapping error. when repeat_times=1, "
                    "only support dst and src0_offset 100 percent same.")

    def _vbi_dst_src0offset_overlap_repeat_multi(self, blk_valid_list, src0_offset_op, vbi_control_op):
        src0_offset_blk_len = 1
        for v_id in range(vbi_control_op.vertical_repeat_times_value):
            dst_interval = self.get_vbi_dst_extend_interval(
                blk_valid_list, v_id, vbi_control_op)
            stride_list = [1, 8]
            src0_offset_interval = self.get_vbi_src_common_extend_interval(
                src0_offset_blk_len, v_id, stride_list, src0_offset_op, vbi_control_op)
            if self.check_overlapping(dst_interval, src0_offset_interval) is False:
                TikCheckUtil.raise_error(
                    "vbi dst and src0_offset address overlapping error. It is not support "
                    "iteration N's destination is the source of next iteration")
