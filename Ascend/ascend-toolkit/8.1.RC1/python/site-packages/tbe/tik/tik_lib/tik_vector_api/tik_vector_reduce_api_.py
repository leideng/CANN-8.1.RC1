#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_reduce_api_.py
DESC:     reduce max, min ops
CREATED:  2021-11-04 9:36
MODIFIED: 2021-11-04 9:36
"""
from collections import namedtuple
from tbe import tvm
from tbe.common.platform import scope_ubuf
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import ceil_div
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.common.tik_get_soc_name import get_rep_size
from tbe.tik.common.tik_get_soc_name import is_compatible_mode
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_params import VREDUCE_PER_REP_OUTPUT
from tbe.tik.tik_lib.tik_params import VREDUCE_MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import PIPE_V
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import VREDUCE_DEFAULT_SRC_BLK_STRIDE
from tbe.tik.tik_lib.tik_params import VREDUCE_DEFAULT_DST_REP_STRIDE
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import VREDUCE_DEFAULT_SRC_REP_STRIDE
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import BYTE_SIZE
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_expr_convert import type_convert
from tbe.tik.tik_lib.tik_mask_concat_ import mask_concat
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import vec_all_reduce_decorator
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_common import ReduceCheckParams
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp
from tbe.tik.tik_lib.tik_vector_api.tik_compute_control import ControlOp
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_common import DEFAULT_STRIDE
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_common import DEFAULT_REP_STRIDE

_DEFAULT_NBLOCK = 1
_DEFAULT_SRC_STRIDE = 0


class ReduceOp:
    """
    reduce op
    """

    def __init__(self, tik_instance, reduce_api):
        super(ReduceOp, self).__init__()
        self.tik_instance = tik_instance
        self.name = reduce_api.name
        self.control_op = ControlOp(mask=reduce_api.mask, repeat_times=reduce_api.repeat_times)
        self.work_tensor = reduce_api.work_tensor
        self.cal_index = reduce_api.cal_index
        self.src_tensor_op = TensorOp(reduce_api.src, DEFAULT_STRIDE, reduce_api.src_rep_stride, "src")
        self.work_tensor_op = TensorOp(reduce_api.work_tensor, DEFAULT_STRIDE, DEFAULT_REP_STRIDE, "work_tensor")
        self.dst_tensor_op = TensorOp(reduce_api.dst, DEFAULT_STRIDE, DEFAULT_REP_STRIDE, "dst")
        self.check_params = (reduce_api.dst, reduce_api.src, self.work_tensor, self.control_op, self.tik_instance,
                             reduce_api.src_rep_stride)
        self.reduce_check_obj = ReduceCheckParams(self.name, self.check_params, reduce_api.cal_index)
        self.one_blk_size = get_block_size()
        self.one_rep_size = get_rep_size()

    def compute_iter_index(self, index_params):
        """
        compute iter index
        :param index_params:
        :return: it1_index, it2_index, it3_index, it4_index
        """
        index_type, it2_start_pos, it3_start_pos, it4_start_pos, element_num_per_rep = index_params
        it4_index = self.tik_instance.scalar_(dtype=index_type)
        it4_index.set_as(self.reduce_check_obj.work_tensor, src_offset=it4_start_pos + 1)
        it3_index = self.tik_instance.scalar_(dtype=index_type)
        it3_index.set_as(self.reduce_check_obj.work_tensor,
                         src_offset=it3_start_pos + it4_index + 1)
        it2_index = self.tik_instance.scalar_(dtype=index_type)
        it2_index.set_as(self.reduce_check_obj.work_tensor,
                         src_offset=it2_start_pos + element_num_per_rep *
                                    (it4_index // VREDUCE_PER_REP_OUTPUT) + it3_index + 1)
        it1_index = self.tik_instance.scalar_(dtype=index_type)
        it1_index.set_as(
            self.reduce_check_obj.work_tensor,
            src_offset=element_num_per_rep * (
                    element_num_per_rep * (it4_index // VREDUCE_PER_REP_OUTPUT) +
                    it3_index) // VREDUCE_PER_REP_OUTPUT + it2_index + 1)
        return [it1_index, it2_index, it3_index, it4_index]

    def reduce_cal_idx_scalar_it3(self, cal_idx_params_ins):
        """
        iteration3
        :return:
        """
        offset_num_per_rep = self.one_blk_size // cal_idx_params_ins.dtype_size * self.reduce_check_obj.src_rep_stride

        with self.tik_instance.context.freeze():
            ex_output_count, it4_start_pos = self._vreduce_body_cal_scalar(
                (cal_idx_params_ins.it2_output_count, cal_idx_params_ins.it3_start_pos,
                 cal_idx_params_ins.it2_start_pos, cal_idx_params_ins.element_num_per_rep,
                 cal_idx_params_ins.dtype_size))
            self._vreduce_it4_compute(ex_output_count, it4_start_pos, cal_idx_params_ins.it3_start_pos)
            self.reduce_check_obj.dst[0].set_as(self.reduce_check_obj.work_tensor, src_offset=it4_start_pos)

            it1_index, it2_index, it3_index, it4_index = self.compute_iter_index(
                (cal_idx_params_ins.index_type, cal_idx_params_ins.it2_start_pos,
                 cal_idx_params_ins.it3_start_pos, it4_start_pos, cal_idx_params_ins.element_num_per_rep))
            pre_num_elem = (cal_idx_params_ins.element_num_per_rep * (it4_index // 2) + it3_index)
            pre_num = \
                offset_num_per_rep * \
                (cal_idx_params_ins.element_num_per_rep * pre_num_elem // 2 + it2_index) // 2
            res_index = self.tik_instance.scalar_(init_value=pre_num + it1_index,
                                                  dtype=cal_idx_params_ins.index_type)
            tmp_scalar = self.conv_index_type_to_dst_type(cal_idx_params_ins.index_type,
                                                          res_index, self.reduce_check_obj.dst)
            self.reduce_check_obj.dst.set_as(tmp_scalar, dst_offset=1)

    def run_vreduce_max_min_v300(self, dst_offset=0, src_offset=0):
        """
        Find the extremum value and the corresponding index position in all the input data in v300

        Parameters
        ----------
        dst_offset: dst tensor offset
        src_offset: src tensor offset

        Returns
        -------
        no returns
        """
        mask_o = mask_concat(self.tik_instance, self.control_op.mask,
                             tensor_bit_len=max(get_bit_len(self.reduce_check_obj.dst.dtype),
                                                get_bit_len(self.reduce_check_obj.src.dtype)))
        cal_index = 0
        if self.cal_index:
            cal_index = 1
        # gen
        config = [self.control_op.repeat_times, VREDUCE_DEFAULT_DST_REP_STRIDE,
                  VREDUCE_DEFAULT_SRC_BLK_STRIDE, self.reduce_check_obj.src_rep_stride, cal_index]
        if self.reduce_check_obj.name == "vcmax":
            call_name = "vec_reduce_max"
        else:
            call_name = "vec_reduce_min"

        with self.tik_instance.new_scope():
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
            mem_access_param = type_convert(config)
            # 8 Block/repeat, 32Byte/Block
            src_extent_elem = (self.control_op.repeat_times - 1) * self.reduce_check_obj.src_rep_stride
            src_extent = Expr((src_extent_elem + (BLK_NUM_PER_REP - 1) * 1 + 1) * BYTE_SIZE)
            dst_extent = Expr(BLK_NUM_PER_REP)

            dst_offset_expr = Expr(dst_offset)
            src_offset_expr = Expr(src_offset)
            instr = tvm.call_extern(
                self.reduce_check_obj.dst.dtype, call_name,
                self.reduce_check_obj.dst.access_ptr("w", extent=dst_extent.get(),
                                                     offset=dst_offset_expr.get()),
                self.reduce_check_obj.src.access_ptr("r", extent=src_extent.get(),
                                                     offset=src_offset_expr.get()),
                *mem_access_param)
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    def creat_mask_(self, data_len):
        """
        get mask in the "0101010101" format

        Parameters
        ----------
        data_len: int, src_element // 2, src_element is the element count of vreduce instruction will cover

        Returns
        -------
        the new mask
        """
        if is_immediate_number(data_len):
            return self.reduce_check_obj.vreduce_create_mask(data_len)
        return self._vreduce_create_mask_scalar(data_len)

    def get_reduce_max_min_instr(self, max_min_params_ins, max_min_instr_params, dst_offset=0, src_offset=0):
        """
        get reduce instr
        :param src_offset:
        :param dst_offset:
        :param max_min_params_ins:
        :param max_min_instr_params:
        :return:
        """
        mem_access_param = type_convert(max_min_instr_params.config)
        # 8 Block/repeat, 32Byte/Block
        src_extent_elem = (max_min_params_ins.repeat_times - 1) * max_min_params_ins.src_rep_stride + \
                          (8 - 1) * 1 + 1
        src_extent = Expr(src_extent_elem * 32)

        # here is vcmax and vcmin, src f16 2B aligned, dst 4B aligned
        dst_extent = Expr(max_min_params_ins.repeat_times * max_min_instr_params.src_bit_len * VREDUCE_PER_REP_OUTPUT
                          // ONE_BYTE_BIT_LEN)

        # when repeat time >1 and the count of dst write element is not
        # the multi of one_blk_size
        dst_extent = Expr(ceil_div(dst_extent, self.one_blk_size) * self.one_blk_size)
        dst_offset_expr = Expr(dst_offset)
        src_offset_expr = Expr(src_offset)
        if max_min_params_ins.src.dtype == "int16" and not TikSocManager.is_v300_610l_soc():
            instr = tvm.call_extern(
                max_min_params_ins.dst.dtype, self.name,
                max_min_params_ins.dst.reinterpret_cast_to("uint16").access_ptr(
                    "w", extent=dst_extent.get(),
                    offset=dst_offset_expr.get()),
                max_min_params_ins.src.reinterpret_cast_to("uint16").access_ptr(
                    "r", extent=src_extent.get(),
                    offset=src_offset_expr.get()),
                *mem_access_param)
        else:
            instr = tvm.call_extern(
                max_min_params_ins.dst.dtype, self.name,
                max_min_params_ins.dst.access_ptr("w", extent=dst_extent.get(),
                                                  offset=dst_offset_expr.get()),
                max_min_params_ins.src.access_ptr("r", extent=src_extent.get(),
                                                  offset=src_offset_expr.get()),
                *mem_access_param)
        return instr

    def run_vreduce_max_min(self, reduce_max_min_params, dst_offset=0, src_offset=0):
        """
        Find the extremum value and the corresponding index position in all the input data.

        Parameters
        ----------
        reduce_max_min_params: contains mask,dst,src,repeat_times,src_rep_stride
        mask->Effective operation on element, divided into two model: Continuous and bit by bit.
        dst->vector operator
        src->src operator
        repeat_times->numbers of iterations of this instruction
        src_rep_stride->int/Scalar, the stride between each repeat in src
        dst_offset: dst tensor offset
        src_offset: src tensor offset

        Returns
        -------
        no returns
        """
        max_min_params = namedtuple("MaxMinParams", "mask dst src repeat_times src_rep_stride")
        max_min_params_ins = max_min_params(*reduce_max_min_params)

        mask_o = mask_concat(self.tik_instance, max_min_params_ins.mask,
                             tensor_bit_len=max(get_bit_len(max_min_params_ins.dst.dtype),
                                                get_bit_len(max_min_params_ins.src.dtype)))

        self.reduce_check_obj.check_address_overlap(reduce_max_min_params, dst_offset, src_offset)

        # check tensor overflow
        self._check_overflow(reduce_max_min_params, dst_offset, src_offset)

        # gen
        config = [max_min_params_ins.repeat_times, VREDUCE_DEFAULT_DST_REP_STRIDE,
                  VREDUCE_DEFAULT_SRC_BLK_STRIDE, max_min_params_ins.src_rep_stride]
        if TikSocManager.is_910b_soc() or TikSocManager.is_nano_soc():
            config.append(0)
        elif TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc():
            config.append(0 & 0b01)
            config.append((0 & 0b10) >> 1)
            config.append(0)

        src_bit_len = get_bit_len(max_min_params_ins.src.dtype)
        with self.tik_instance.new_scope():

            max_min_instr_params = namedtuple(
                "InstrParams", "config src_bit_len")
            max_min_instr_params_ins = max_min_instr_params(config, src_bit_len)
            instr = self.get_reduce_max_min_instr(max_min_params_ins, max_min_instr_params_ins, dst_offset, src_offset)

            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    def conv_index_type_to_dst_type(self, index_type, res_index, dst):
        """
        conv index type to dst type
        :return:
        """
        if TikSocManager.is_nano_soc():
            tmp_scalar = res_index.reinterpret_cast_to(dst.dtype)
        else:
            tmp_tensor = self.tik_instance.Tensor(dtype=index_type, shape=(1,),
                                                  name="tmp_tensor",
                                                  scope=scope_ubuf)
            tmp_tensor[0].set_as(res_index)
            tmp_scalar = self.tik_instance.scalar_(init_value=tmp_tensor[0],
                                                   dtype=dst.dtype)
        return tmp_scalar

    def get_reduce_cal_idx_imm_it3_scalar(self, iter3_params_ins, offset_num_per_rep):
        """
        reduce cal idx imm it3 tmp scalar
        :param iter3_params_ins:
        :param offset_num_per_rep:
        :return:
        """
        it3_index = self.tik_instance.scalar_(dtype=iter3_params_ins.index_type)
        it3_index.set_as(self.reduce_check_obj.work_tensor, src_offset=iter3_params_ins.it2_start_pos + 1)
        it2_index = self.tik_instance.scalar_(dtype=iter3_params_ins.index_type)
        it2_index.set_as(self.reduce_check_obj.work_tensor, src_offset=it3_index + 1)
        pre_num = offset_num_per_rep * (it3_index //
                                        VREDUCE_PER_REP_OUTPUT)
        res_index = self.tik_instance.scalar_(init_value=pre_num + it2_index,
                                              dtype=iter3_params_ins.index_type)

        tmp_scalar = self.conv_index_type_to_dst_type(iter3_params_ins.index_type, res_index,
                                                      self.reduce_check_obj.dst)
        return tmp_scalar

    def immediate_num_check_space_overflow(self):
        """
        immediate number check space overflow
        :return:
        """
        dst_offset = Expr(self.reduce_check_obj.dst.offset).eval_value()
        work_tensor_offset = Expr(self.reduce_check_obj.work_tensor.offset). \
            eval_value()

        src_offset = Expr(self.reduce_check_obj.src.offset).eval_value()
        if is_immediate_number(self.reduce_check_obj.src_rep_stride):
            self.reduce_check_obj.check_space_overflow((self.control_op.mask, self.reduce_check_obj.dst,
                                                        self.reduce_check_obj.work_tensor, self.reduce_check_obj.src,
                                                        dst_offset, work_tensor_offset,
                                                        src_offset, self.control_op.repeat_times,
                                                        self.reduce_check_obj.src_rep_stride), self.cal_index)

    def run_reduce_func_(self, dst, src):
        """
        For vreduce, there are total 3 iterations.
        This function is the first one. We compute the data into work_tensor

        Parameters
        ----------
        dst: Tensor, store results in it
        src: Tensor, the source tensor

        Returns
        -------
        the number of the first iteration output
        """
        for_range_times = self.control_op.repeat_times // MAX_REPEAT_TIMES
        dtype_len = DTYPE_SIZE[src.dtype]

        # all dst_rep_stride and src_block_stride set to 1,
        # select max or min data from continuous elements of one repeat
        with self.tik_instance.for_range(0, for_range_times) as index:
            self.run_vreduce_max_min((self.control_op.mask, dst, src,
                                      MAX_REPEAT_TIMES, self.reduce_check_obj.src_rep_stride),
                                     index * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT,
                                     index * MAX_REPEAT_TIMES * self.reduce_check_obj.src_rep_stride *
                                     self.one_blk_size // dtype_len)

        left_repeat_times = self.control_op.repeat_times % MAX_REPEAT_TIMES

        if is_immediate_number(self.control_op.repeat_times):
            if left_repeat_times > 0:
                self.run_vreduce_max_min((self.control_op.mask, dst, src, left_repeat_times,
                                          self.reduce_check_obj.src_rep_stride),
                                         for_range_times * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT,
                                         for_range_times * MAX_REPEAT_TIMES * self.reduce_check_obj.src_rep_stride *
                                         self.one_blk_size // dtype_len)
            return VREDUCE_PER_REP_OUTPUT * self.control_op.repeat_times

        with self.tik_instance.if_scope(left_repeat_times > 0):
            self.run_vreduce_max_min(
                (self.control_op.mask, dst, src, left_repeat_times, self.reduce_check_obj.src_rep_stride),
                for_range_times * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT,
                for_range_times * MAX_REPEAT_TIMES * self.reduce_check_obj.src_rep_stride *
                self.one_blk_size // dtype_len)
        return VREDUCE_PER_REP_OUTPUT * self.control_op.repeat_times

    def set_reduce_check_dst_value(self):
        """
        set reduce check dst value
        :return:
        """
        self.reduce_check_obj.dst[0] = self.reduce_check_obj.work_tensor[0]
        self.reduce_check_obj.dst.set_as(self.reduce_check_obj.work_tensor, dst_offset=1, src_offset=1)

    def reduce_freeze(self):
        """
        reduce freeze
        :return:
        """
        with self.tik_instance.context.freeze():
            if TikSocManager.is_v300_610l_soc() or TikSocManager.is_v210_vec_soc():
                self.run_vreduce_max_min_v300()
                return
            if self.cal_index:
                if is_immediate_number(self.control_op.repeat_times):
                    self.reduce_check_obj.check_dtype_overflow(self.control_op.repeat_times,
                                                               self.reduce_check_obj.src,
                                                               self.reduce_check_obj.src_rep_stride)
                    self.immediate_num_check_space_overflow()
                    self._vreduce_cal_idx_imm()
                    return
                self._vreduce_cal_idx_scalar()
            else:
                if is_immediate_number(self.control_op.repeat_times):
                    self.immediate_num_check_space_overflow()
                    self._vreduce_no_idx_imm()
                    return
                self._vreduce_no_idx_scalar()

        self.tik_instance.set_high_level_api_state()

    @vec_all_reduce_decorator()
    def run_all(self):
        """
        check all and gen code
        :return:
        """
        self.reduce_check_obj.check_all()
        mask_concat(self.tik_instance, self.control_op.mask,
                    tensor_bit_len=max(get_bit_len(self.reduce_check_obj.dst.dtype),
                                       get_bit_len(self.reduce_check_obj.src.dtype)))
        self.reduce_freeze()

    def _vreduce_cal_idx_scalar(self):
        """
        the common function for vreduce instruction, repeat_times is scalar.
        this function will get both value and index.

        Parameters
        ----------

        Returns
        -------
        None
        """
        with self.tik_instance.if_scope(self.control_op.repeat_times == 1):
            self.run_vreduce_max_min((self.control_op.mask, self.reduce_check_obj.work_tensor,
                                      self.reduce_check_obj.src, self.control_op.repeat_times,
                                      self.reduce_check_obj.src_rep_stride))
            # 0 is index of value, 1 is index of value index
            self.set_reduce_check_dst_value()
        with self.tik_instance.else_scope():
            dtype_size = DTYPE_SIZE[self.reduce_check_obj.src.dtype]
            element_num_per_rep = self.one_rep_size // dtype_size

            # iteration1
            it1_output_count = self._vreduce_it1_compute()

            # iteration2
            it2_start_pos = self.reduce_check_obj.align_start_pos(it1_output_count, dtype_size)
            it2_output_count, it3_start_pos = self._vreduce_body_cal_scalar(
                (it1_output_count, it2_start_pos, 0, element_num_per_rep, dtype_size))

            if get_bit_len(self.reduce_check_obj.src.dtype) == 16:
                index_type = "uint16"
            else:
                index_type = "uint32"
            cal_idx_params = namedtuple(
                "CalIdxParams", "dtype_size it2_output_count it3_start_pos"
                                " it2_start_pos element_num_per_rep index_type")
            cal_idx_params_ins = cal_idx_params(
                dtype_size, it2_output_count, it3_start_pos, it2_start_pos, element_num_per_rep, index_type)
            self.reduce_cal_idx_scalar_it3(cal_idx_params_ins)

    def _vreduce_body_cal_scalar(self, scalar_params):
        """
        vreduce body cal scalar
        :param scalar_params: contains variables pre_data_count,
        cur_start_pos, pre_start_pos, element_num_per_rep, dtype_size
        :return:
        """
        ex_body_repeat_times = scalar_params[0] // scalar_params[3]
        ex_tail_num_count = scalar_params[0] % scalar_params[3]
        ex_has_tail = ex_tail_num_count != 0
        ex_tail_output_count = self.tik_instance.scalar_(dtype="int32",
                                                         init_value=0)
        if get_bit_len(self.reduce_check_obj.src.dtype) == 16 and is_compatible_mode():
            body_mask = [int("01" * 32, 2), int("01" * 32, 2)]
        else:
            body_mask = [0, int("01" * 32, 2)]
        with self.tik_instance.if_scope(ex_body_repeat_times != 0):
            self.run_vreduce_max_min((body_mask, self.reduce_check_obj.work_tensor,
                                      self.reduce_check_obj.work_tensor, ex_body_repeat_times,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE), scalar_params[1], scalar_params[2])
            ex_body_output_count = VREDUCE_PER_REP_OUTPUT * ex_body_repeat_times
        with self.tik_instance.if_scope(ex_has_tail):
            tail_mask = self.creat_mask_(ex_tail_num_count //
                                         VREDUCE_PER_REP_OUTPUT)
            self.run_vreduce_max_min((tail_mask, self.reduce_check_obj.work_tensor,
                                      self.reduce_check_obj.work_tensor,
                                      VREDUCE_MIN_REPEAT_TIMES, VREDUCE_DEFAULT_SRC_REP_STRIDE),
                                     scalar_params[1] + ex_body_output_count,
                                     scalar_params[2] + scalar_params[3] * ex_body_repeat_times)
            ex_tail_output_count.set_as(VREDUCE_PER_REP_OUTPUT)
        output_count = ex_body_output_count + ex_tail_output_count
        next_start_pos = self.reduce_check_obj.align_start_pos(scalar_params[1] + output_count, scalar_params[4])
        return output_count, next_start_pos

    def _second_step_no_idx_imm(self, dst, src, cur_data, dtype_len):
        """
        second step not idx imm
        :param dst:
        :param src:
        :param cur_data:
        :param dtype_len:
        :return:
        """
        new_repeat_times = cur_data * dtype_len // self.one_rep_size
        if new_repeat_times >= 1:
            new_mask = self.creat_mask_(self.one_rep_size // dtype_len //
                                        VREDUCE_PER_REP_OUTPUT)
            # dst_rep_stride, src_block_stride set to 1,
            # src_rep_stride set to 8
            self.run_vreduce_max_min((new_mask, dst, src, new_repeat_times,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE))

        left_data = cur_data % (self.one_rep_size // dtype_len)

        if left_data > 0:
            new_mask = self.creat_mask_(left_data // VREDUCE_PER_REP_OUTPUT)
            # repeat_times set to 1, dst_rep_stride set to 1,
            # src_block_stride set to 1, src_rep_stride set to 8,
            self.run_vreduce_max_min((new_mask, dst, src, VREDUCE_MIN_REPEAT_TIMES,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE),
                                     new_repeat_times * VREDUCE_PER_REP_OUTPUT,
                                     new_repeat_times * self.one_rep_size // dtype_len)
            # have tail, new_repeat_times used to calculate output data num
            new_repeat_times += 1
        return new_repeat_times * VREDUCE_PER_REP_OUTPUT

    def _vreduce_no_idx_imm(self):
        """
        vreduce no idx imm

        Parameters
        ----------

        Returns
        -------
        None
        """
        if self.control_op.repeat_times == 1:
            self.run_vreduce_max_min(
                (self.control_op.mask,
                 self.reduce_check_obj.work_tensor,
                 self.reduce_check_obj.src, self.control_op.repeat_times, self.reduce_check_obj.src_rep_stride))
            self.reduce_check_obj.dst[0] = self.reduce_check_obj.work_tensor[0]
            return

        # first step
        dtype_len = DTYPE_SIZE[self.reduce_check_obj.src.dtype]
        cur_data = self.run_reduce_func_(self.reduce_check_obj.work_tensor, self.reduce_check_obj.src)
        # second step
        cur_data = self._second_step_no_idx_imm(self.reduce_check_obj.work_tensor, self.reduce_check_obj.work_tensor,
                                                cur_data, dtype_len)
        one_rep_data = self.one_rep_size // dtype_len
        # third step
        if cur_data <= one_rep_data:
            self._third_step_no_idx_imm(self.reduce_check_obj.dst, self.reduce_check_obj.work_tensor,
                                        self.reduce_check_obj.work_tensor, cur_data)
            return

        # if new_repeat_times > 1, need to run second step again,
        # this only for float32, when first second step output 8190 elements
        cur_data = self._second_step_no_idx_imm(self.reduce_check_obj.work_tensor, self.reduce_check_obj.work_tensor,
                                                cur_data, dtype_len)
        new_repeat_times = cur_data * dtype_len // self.one_rep_size

        if new_repeat_times <= 1:
            self._third_step_no_idx_imm(self.reduce_check_obj.dst, self.reduce_check_obj.work_tensor,
                                        self.reduce_check_obj.work_tensor, cur_data)

    def _third_step_no_idx_imm(self, dst, work_tensor, src, cur_data):
        """
        third step no idx imm
        :param dst:
        :param work_tensor:
        :param src:
        :param cur_data:
        :return:
        """
        new_mask = self.creat_mask_(cur_data // VREDUCE_PER_REP_OUTPUT)
        # repeat_times set to 1, dst_rep_stride set to 1,
        # src_block_stride set to 1, src_rep_stride set to 8,
        self.run_vreduce_max_min((new_mask, work_tensor, src,
                                  VREDUCE_MIN_REPEAT_TIMES,
                                  VREDUCE_DEFAULT_SRC_REP_STRIDE))
        # copy result to dst buffer
        dst[0] = work_tensor[0]

    def _third_step_no_idx_scalar(self, dst, work_tensor, src, cur_data):
        """
        third step no idx scalar
        :param dst:
        :param work_tensor:
        :param src:
        :param cur_data:
        :return:
        """
        new_mask = self.creat_mask_(cur_data // VREDUCE_PER_REP_OUTPUT)
        # repeat_times set to 1, dst_rep_stride set to 1,
        # src_block_stride set to 1, src_rep_stride set to 8,
        self.run_vreduce_max_min((new_mask, work_tensor, src,
                                  VREDUCE_MIN_REPEAT_TIMES,
                                  VREDUCE_DEFAULT_SRC_REP_STRIDE))
        # copy result to dst
        dst[0] = work_tensor[0]

    def _second_step_no_idx_scalar(self, dst, src, cur_data, dtype_len):
        """
        second step no idx scalar
        :param dst:
        :param src:
        :param cur_data:
        :param dtype_len:
        :return:
        """
        new_repeat_times = cur_data * dtype_len // self.one_rep_size
        tail_repeat_times = self.tik_instance.scalar_(dtype="int32",
                                                      init_value=0)
        with self.tik_instance.if_scope(new_repeat_times >= 1):
            new_mask = self.creat_mask_(self.one_rep_size // dtype_len //
                                        VREDUCE_PER_REP_OUTPUT)
            # dst_rep_stride, src_block_stride set to 1,
            # src_rep_stride set to 8
            self.run_vreduce_max_min((new_mask, dst, src, new_repeat_times,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE))

        left_data = cur_data % (self.one_rep_size // dtype_len)
        with self.tik_instance.if_scope(left_data > 0):
            new_mask = self.creat_mask_(left_data // VREDUCE_PER_REP_OUTPUT)
            # repeat_times set to 1, dst_rep_stride set to 1,
            # src_block_stride set to 1, src_rep_stride set to 8,
            self.run_vreduce_max_min((new_mask, dst, src, VREDUCE_MIN_REPEAT_TIMES,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE),
                                     new_repeat_times * VREDUCE_PER_REP_OUTPUT,
                                     new_repeat_times * self.one_rep_size // dtype_len)
            # have tail, new_repeat_times used to calculate output data num
            tail_repeat_times.set_as(1)
        return (new_repeat_times + tail_repeat_times) * VREDUCE_PER_REP_OUTPUT

    def _vreduce_it4_compute(self, pre_output_count, cur_start_pos, pre_start_pos):
        """
        run the third iteration of vreduce

        Parameters
        ----------
        pre_output_count: int/Scalar, the output elements count in it2
        cur_start_pos: int/Scalar, the start position in work_tensor
        pre_start_pos: int/Scalar, the stride between each repeat in src

        Returns
        -------

        """
        it3_mask = self.creat_mask_(pre_output_count // VREDUCE_PER_REP_OUTPUT)
        self.run_vreduce_max_min(
            (it3_mask, self.reduce_check_obj.work_tensor,
             self.reduce_check_obj.work_tensor,
             VREDUCE_MIN_REPEAT_TIMES, VREDUCE_DEFAULT_SRC_REP_STRIDE),
            cur_start_pos, pre_start_pos)

    def _vreduce_it1_compute(self):
        """
        run the first iteration of vreduce

        Parameters
        ----------
        Returns
        -------

        """
        return self.run_reduce_func_(self.reduce_check_obj.work_tensor, self.reduce_check_obj.src)

    def _vreduce_create_mask_scalar(self, data_len):
        """
        vreduce create mask scalar
        :param data_len:
        :return:
        """
        # mask_len record the valid data num of low mask
        # B16: max data num of low mask is 64
        # B32: max data num of low mask is 64, all data can select by low mask
        # out data saved as: Data1Index1Data2Index2....DatanIndexn
        # so valid data is half of max data num
        mask_len = 32

        high_mask_bit = self.tik_instance.scalar_(dtype='int64',
                                                  name='high_mask_bit')
        low_mask_bit = self.tik_instance.scalar_(dtype='int64',
                                                 name='low_mask_bit')
        high_mask = self.tik_instance.scalar_(dtype='uint64',
                                              name='high_mask', init_value=0)
        low_mask = self.tik_instance.scalar_(dtype='uint64',
                                             name='low_mask', init_value=0)

        high_mask_bit.set_as(data_len - mask_len)

        # create mask as 01010101
        with self.tik_instance.for_range(0, high_mask_bit):
            high_mask.set_as(high_mask << 2)
            high_mask.set_as(high_mask | 1)

        with self.tik_instance.if_scope(data_len >= mask_len):
            low_mask_bit.set_as(mask_len)
        with self.tik_instance.else_scope():
            low_mask_bit.set_as(data_len)

        with self.tik_instance.for_range(0, low_mask_bit):
            low_mask.set_as(low_mask << 2)
            low_mask.set_as(low_mask | 1)
        return [high_mask, low_mask]

    def _check_overflow(self, check_overflow_params, dst_offset=0, src_offset=0):
        """
        check tensor whether overflow

        Parameters
        ----------
        check_overflow_params:
        dst_offset: dst tensor offset
        src_offset: src tensor offset

        Returns
        -------
        None
        """

        mask, dst, src, repeat_times, src_rep_stride = check_overflow_params
        if self.name in ("vcmin", "vcmax"):
            block_len = 2
            self.reduce_check_obj.vector_tensor_overflow_check((dst, mask, _DEFAULT_NBLOCK, block_len,
                                                                repeat_times, _DEFAULT_SRC_STRIDE,
                                                                VREDUCE_DEFAULT_DST_REP_STRIDE),
                                                               ori_offset=dst_offset)
        else:
            block_len = 1
            self.reduce_check_obj.vector_tensor_overflow_check((dst, mask, _DEFAULT_NBLOCK, block_len,
                                                                repeat_times, _DEFAULT_SRC_STRIDE,
                                                                VREDUCE_DEFAULT_DST_REP_STRIDE), ori_offset=dst_offset)
        src_bit_len = get_bit_len(src.dtype)
        parallelism = self.one_rep_size * ONE_BYTE_BIT_LEN // src_bit_len
        self.reduce_check_obj.vector_tensor_overflow_check((src, mask,
                                                            parallelism // (self.one_rep_size
                                                                            // src_bit_len),
                                                            self.one_rep_size // src_bit_len,
                                                            repeat_times,
                                                            VREDUCE_DEFAULT_SRC_BLK_STRIDE, src_rep_stride),
                                                           ori_offset=src_offset)

    def _vreduce_body_cal_imm(self, body_cal_imm_params):
        """
        reduce body cal imm
        :param body_cal_imm_params: pre_data_count, pre_start_pos, cur_start_pos, element_num_per_rep, dtype_size
        :return:
        """
        body_cal_params = namedtuple(
            "BodyCalParams", "pre_data_count pre_start_pos cur_start_pos element_num_per_rep dtype_size")
        body_cal_params_ins = body_cal_params(*body_cal_imm_params)

        ex_body_repeat_times = body_cal_params_ins.pre_data_count // body_cal_params_ins.element_num_per_rep
        ex_tail_num_count = body_cal_params_ins.pre_data_count % body_cal_params_ins.element_num_per_rep
        ex_has_tail = ex_tail_num_count != 0
        ex_body_output_count = 0
        if get_bit_len(self.reduce_check_obj.src.dtype) == 16 and is_compatible_mode():
            body_mask = [int("01" * 32, 2), int("01" * 32, 2)]
        else:
            body_mask = [0, int("01" * 32, 2)]
        if ex_body_repeat_times != 0:
            self.run_vreduce_max_min((body_mask, self.reduce_check_obj.work_tensor,
                                      self.reduce_check_obj.work_tensor,
                                      ex_body_repeat_times,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE),
                                     body_cal_params_ins.cur_start_pos, body_cal_params_ins.pre_start_pos)
            ex_body_output_count = VREDUCE_PER_REP_OUTPUT * ex_body_repeat_times
        ex_tail_output_count = 0
        if ex_has_tail:
            tail_mask = self.creat_mask_(ex_tail_num_count //
                                         VREDUCE_PER_REP_OUTPUT)
            self.run_vreduce_max_min((tail_mask, self.reduce_check_obj.work_tensor,
                                      self.reduce_check_obj.work_tensor,
                                      VREDUCE_MIN_REPEAT_TIMES,
                                      VREDUCE_DEFAULT_SRC_REP_STRIDE),
                                     body_cal_params_ins.cur_start_pos + ex_body_output_count,
                                     body_cal_params_ins.pre_start_pos + body_cal_params_ins.element_num_per_rep *
                                     ex_body_repeat_times)
            ex_tail_output_count = VREDUCE_PER_REP_OUTPUT
        output_count = ex_body_output_count + ex_tail_output_count
        next_start_pos = self.reduce_check_obj.align_start_pos(body_cal_params_ins.cur_start_pos +
                                                               output_count, body_cal_params_ins.dtype_size)
        return output_count, next_start_pos

    def _vreduce_cal_idx_imm_iteration3(self, iter3_params):
        """
        vreduce cal idx imm iteration3
        :param iter3_params:
        :return:
        """
        it3_params = namedtuple(
            "It3Params", "it2_output_count it2_start_pos index_type dtype_size element_num_per_rep it3_start_pos")
        iter3_params_ins = it3_params(*iter3_params)
        # iteration3
        offset_num_per_rep = self.one_blk_size // iter3_params_ins.dtype_size * self.reduce_check_obj.src_rep_stride

        if iter3_params_ins.it2_output_count == VREDUCE_PER_REP_OUTPUT:
            self.reduce_check_obj.dst[0].set_as(self.reduce_check_obj.work_tensor,
                                                src_offset=iter3_params_ins.it2_start_pos)
            tmp_scalar = self.get_reduce_cal_idx_imm_it3_scalar(iter3_params_ins, offset_num_per_rep)
        else:
            if iter3_params_ins.it2_output_count > iter3_params_ins.element_num_per_rep:
                ex_output_count, it4_start_pos = \
                    self._vreduce_body_cal_imm((iter3_params_ins.it2_output_count, iter3_params_ins.it2_start_pos,
                                                iter3_params_ins.it3_start_pos,
                                                iter3_params_ins.element_num_per_rep,
                                                iter3_params_ins.dtype_size))
                self._vreduce_it4_compute(ex_output_count, it4_start_pos, iter3_params_ins.it3_start_pos)
                self.reduce_check_obj.dst[0].set_as(self.reduce_check_obj.work_tensor, src_offset=it4_start_pos)
                it1_index, it2_index, it3_index, it4_index = self.compute_iter_index(
                    (iter3_params_ins.index_type, iter3_params_ins.it2_start_pos, iter3_params_ins.it3_start_pos,
                     it4_start_pos, iter3_params_ins.element_num_per_rep))
                pre_num = \
                    offset_num_per_rep * \
                    (iter3_params_ins.element_num_per_rep *
                     (iter3_params_ins.element_num_per_rep *
                      (it4_index // VREDUCE_PER_REP_OUTPUT) +
                      it3_index) // VREDUCE_PER_REP_OUTPUT +
                     it2_index) // VREDUCE_PER_REP_OUTPUT
            else:
                self._vreduce_it4_compute(iter3_params_ins.it2_output_count, iter3_params_ins.it3_start_pos,
                                          iter3_params_ins.it2_start_pos)
                it3_index = self.tik_instance.scalar_(dtype=iter3_params_ins.index_type)
                it3_index.set_as(self.reduce_check_obj.work_tensor, src_offset=iter3_params_ins.it3_start_pos + 1)
                self.reduce_check_obj.dst[0].set_as(self.reduce_check_obj.work_tensor,
                                                    src_offset=iter3_params_ins.it3_start_pos)

                it2_index = self.tik_instance.scalar_(dtype=iter3_params_ins.index_type)
                it2_index.set_as(self.reduce_check_obj.work_tensor,
                                 src_offset=iter3_params_ins.it2_start_pos + it3_index + 1)
                it1_index = self.tik_instance.scalar_(dtype=iter3_params_ins.index_type)
                it1_index.set_as(self.reduce_check_obj.work_tensor, src_offset=iter3_params_ins.element_num_per_rep * (
                        it3_index // VREDUCE_PER_REP_OUTPUT) + it2_index + 1)
                it3_rep_out_put = it3_index // VREDUCE_PER_REP_OUTPUT
                pre_num = offset_num_per_rep * (
                        iter3_params_ins.element_num_per_rep * it3_rep_out_put + it2_index) // VREDUCE_PER_REP_OUTPUT
            res_index = self.tik_instance.scalar_(init_value=pre_num + it1_index,
                                                  dtype=iter3_params_ins.index_type)
            tmp_scalar = self.conv_index_type_to_dst_type(iter3_params_ins.index_type,
                                                          res_index, self.reduce_check_obj.dst)
        return tmp_scalar

    def _vreduce_cal_idx_imm(self):
        """
        the common function for vreduce instruction, repeat_times is int.
        this function will get both value and index.

        Parameters
        ----------

        Returns
        -------
        None
        """
        if self.control_op.repeat_times == 1:
            self.run_vreduce_max_min((self.control_op.mask, self.reduce_check_obj.work_tensor,
                                      self.reduce_check_obj.src,
                                      self.control_op.repeat_times, self.reduce_check_obj.src_rep_stride))
            self.set_reduce_check_dst_value()
        else:
            dtype_size = DTYPE_SIZE[self.reduce_check_obj.src.dtype]
            element_num_per_rep = self.one_rep_size // dtype_size

            # iteration1
            it1_output_count = self._vreduce_it1_compute()

            # iteration2
            it2_start_pos = self.reduce_check_obj.align_start_pos(it1_output_count, dtype_size)
            it2_output_count, it3_start_pos = self._vreduce_body_cal_imm(
                (it1_output_count, 0, it2_start_pos, element_num_per_rep, dtype_size))

            if get_bit_len(self.reduce_check_obj.src.dtype) == 16:
                index_type = "uint16"
            else:
                index_type = "uint32"
            tmp_scalar = self._vreduce_cal_idx_imm_iteration3(
                (it2_output_count, it2_start_pos, index_type, dtype_size, element_num_per_rep, it3_start_pos))

            self.reduce_check_obj.dst.set_as(tmp_scalar, dst_offset=1)

    def _vreduce_no_idx_scalar(self):
        """
        the common function for vreduce instruction, repeat_times is scalar.
        this function will get only value.

        Parameters
        ----------
        """
        # repeat_times is not immediate, some condition need to do as scalar
        dtype_len = DTYPE_SIZE[self.reduce_check_obj.src.dtype]
        # first step
        cur_data = self.run_reduce_func_(self.reduce_check_obj.work_tensor, self.reduce_check_obj.src)
        # second step
        cur_data = self._second_step_no_idx_scalar(self.reduce_check_obj.work_tensor,
                                                   self.reduce_check_obj.work_tensor, cur_data,
                                                   dtype_len)
        one_rep_data = self.one_rep_size // dtype_len

        with self.tik_instance.if_scope(cur_data <= one_rep_data):
            self._third_step_no_idx_scalar(self.reduce_check_obj.dst, self.reduce_check_obj.work_tensor,
                                           self.reduce_check_obj.work_tensor, cur_data)
        with self.tik_instance.else_scope():
            # second second step
            cur_data = self._second_step_no_idx_scalar(self.reduce_check_obj.work_tensor,
                                                       self.reduce_check_obj.work_tensor, cur_data,
                                                       dtype_len)
            new_repeat_times = cur_data * dtype_len // self.one_rep_size

            # second third step
            with self.tik_instance.if_scope(new_repeat_times <= 1):
                self._third_step_no_idx_scalar(self.reduce_check_obj.dst, self.reduce_check_obj.work_tensor,
                                               self.reduce_check_obj.work_tensor, cur_data)


class ReduceOpNano(ReduceOp):
    """
    """
    def __init__(self, tik_instance, reduce_api):
        super(ReduceOpNano, self).__init__(tik_instance, reduce_api)
        self.tik_instance = tik_instance
        self.check_params = (reduce_api.dst, reduce_api.src, self.work_tensor, self.control_op, self.tik_instance,
                             reduce_api.src_rep_stride)
        self.reduce_check_obj = ReduceCheckParams(self.name, self.check_params, reduce_api.cal_index)

    def run_vreduce_max_min(self, reduce_max_min_params, dst_offset=0, src_offset=0):
        """
        Find the extremum value and the corresponding index position in all the input data.

        Parameters
        ----------
        reduce_max_min_params: contains mask,dst,src,repeat_times,src_rep_stride
        mask->Effective operation on element, divided into two model: Continuous and bit by bit.
        dst->vector operator
        src->src operator
        repeat_times->numbers of iterations of this instruction
        src_rep_stride->int/Scalar, the stride between each repeat in src
        dst_offset: dst tensor offset
        src_offset: src tensor offset

        Returns
        -------
        no returns
        """
        max_min_params = namedtuple("MaxMinParams", "mask dst src repeat_times src_rep_stride")
        max_min_params_ins = max_min_params(*reduce_max_min_params)

        mask_o = type_convert(max_min_params_ins.mask, "uint64")

        self.reduce_check_obj.check_address_overlap(reduce_max_min_params, dst_offset, src_offset)

        # check tensor overflow
        self._check_overflow(reduce_max_min_params, dst_offset, src_offset)

        # gen
        config = [max_min_params_ins.repeat_times, VREDUCE_DEFAULT_DST_REP_STRIDE,
                  VREDUCE_DEFAULT_SRC_BLK_STRIDE, max_min_params_ins.src_rep_stride, 0]

        src_bit_len = get_bit_len(max_min_params_ins.src.dtype)
        with self.tik_instance.new_scope():

            max_min_instr_params = namedtuple(
                "InstrParams", "config src_bit_len")
            max_min_instr_params_ins = max_min_instr_params(config, src_bit_len)
            instr = self.get_reduce_max_min_instr(max_min_params_ins, max_min_instr_params_ins, dst_offset, src_offset)

            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    @vec_all_reduce_decorator()
    def run_all(self):
        """
        check all and gen code
        :return:
        """
        self.reduce_check_obj.check_all()
        self.reduce_freeze()
