#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_whole_reduce_api_.py
DESC:     provide reduce vector instructions
CREATED:  2021-11-3 09:55:42
MODIFIED: 2021-11-3 09:55:42
"""
from tbe import tvm

from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import ceil_div
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import vec_reduce_decorator
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import vec_reduce_wo_order_decorator
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import vec_reduce_group_decorator
from tbe.tik.tik_lib.tik_expr_convert import type_convert
from tbe.tik.tik_lib.tik_params import PIPE_V
from tbe.tik.tik_lib.tik_params import VECTOR_PAIR_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import VECTOR_PAIR_SEGMENT_LIST
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import INDEX_SHIFT_POS
from tbe.tik.tik_lib.tik_params import TWO_BYTE_VALUE
from tbe.tik.tik_lib.tik_params import CNT_SHIFT_POS
from tbe.tik.tik_lib.tik_params import FOUR_BYTE_VALUE
from tbe.tik.tik_lib.tik_params import MASK_LEN_64
from tbe.tik.tik_lib.tik_util import concat_params
from tbe.tik.tik_lib.tik_vector_api.tik_compute_control import ControlOp
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_vector_api.tik_reduce_params_check import ReduceCheck
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.common.tik_get_soc_name import is_compatible_mode
from tbe.tik.api.tik_vector_api import TikVectorApiv1
from tbe.tik.tik_lib.tik_vector_api.tik_scalar_multis_api_ import NanoScalarMultisOps
from tbe.tik.tik_lib.tik_vector_api.tik_vector_multi_api_ import NanoMultiOpApi
from tbe.common.platform import scope_ubuf
from tbe.tik.tik_lib.tik_mask_concat_ import mask_concat
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.common.util import reassign_mask
from tbe.tik.common.util import check_mask1_mask2
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.common.tik_get_soc_name import TIK_SOC_INFO
from tbe.tik.tik_lib.tik_vector_api.vector_common_util import gen_b64_mask_mode

_DEFAULT_BLK_STRIDE = 1
_ONE_REP_BLK_NUM = 8


class WholeReduceOps:
    """
    Whole Reduce Ops
    """

    def __init__(self, tik_instance, whole_reduce_api):
        super().__init__()
        self.tik_instance = tik_instance
        self.name = whole_reduce_api.name
        self.print_name = whole_reduce_api.name
        self.control_op = ControlOp(whole_reduce_api.mask, whole_reduce_api.repeat_times, whole_reduce_api.stride_unit)
        self.dst_tensor_op = TensorOp(whole_reduce_api.dst, _DEFAULT_BLK_STRIDE, whole_reduce_api.dst_rep_stride, "dst")
        self.src_tensor_op = TensorOp(whole_reduce_api.src, whole_reduce_api.src_blk_stride,
                                      whole_reduce_api.src_rep_stride, "src")
        self.check_params = (self.dst_tensor_op, self.src_tensor_op, self.control_op)
        self.reduce_check_obj = ReduceCheck(self.print_name, self.check_params, whole_reduce_api.maxmin_cnt_index)
        self.order = whole_reduce_api.order
        self.maxmin_cnt_index = whole_reduce_api.maxmin_cnt_index
        self.mask_o = None
        self.one_blk_size = ONE_BLK_SIZE

    @vec_reduce_decorator
    def code_gen(self):
        """
        code gen

        Returns
        -------
        None
        """
        config = self._gen_args_vector_whole_reduce()
        src_bit_len = get_bit_len(self.src_tensor_op.tensor_obj.dtype)
        with self.tik_instance.new_scope():
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
            mem_access_param = type_convert(config)
            # 8 Block/repeat, 32Byte/Block
            repeat_stride = (self.control_op.repeat_times - 1) * self.src_tensor_op.rep_stride
            block_repeat = 8 - 1
            block_stride = block_repeat * self.src_tensor_op.blk_stride + 1

            src_extent = Expr((repeat_stride + block_stride) * 32)
            if self.name == "vcadd":
                # when repeat equals 1, dst fp16 2B aligned, The unit of dst_extend is byte.
                dst_extent = Expr((self.control_op.repeat_times - 1) * self.dst_tensor_op.rep_stride * src_bit_len
                                  // ONE_BYTE_BIT_LEN + 2)
            else:
                # here is vcmax and vcmin, src f16 2B aligned, dst 4B aligned
                dst_extent = Expr((self.control_op.repeat_times - 1) * self.dst_tensor_op.rep_stride * src_bit_len
                                  * 2 // ONE_BYTE_BIT_LEN + 4)
            # when repeat time >1 and the count of dst write element is not
            # the multi of ONE_BLK_SIZE
            dst_extent = Expr(ceil_div(dst_extent, self.one_blk_size) * self.one_blk_size)
            if self.src_tensor_op.tensor_obj.dtype == "int16":
                instr = tvm.call_extern(self.dst_tensor_op.tensor_obj.dtype, self.name,
                                        self.dst_tensor_op.tensor_obj.reinterpret_cast_to("uint16")
                                        .access_ptr("w", extent=dst_extent.get()),
                                        self.src_tensor_op.tensor_obj.reinterpret_cast_to("uint16")
                                        .access_ptr("r", extent=src_extent.get()),
                                        *mem_access_param)
            else:
                instr = tvm.call_extern(self.dst_tensor_op.tensor_obj.dtype, self.name,
                                        self.dst_tensor_op.tensor_obj.access_ptr("w", extent=dst_extent.get()),
                                        self.src_tensor_op.tensor_obj.access_ptr("r", extent=src_extent.get()),
                                        *mem_access_param)
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *self.mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)
        if self.name in ("vcmax", "vcmin") and self.maxmin_cnt_index is not None:
            self._read_maxmin_cnt_index()

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.reduce_check_obj.all_check(self.tik_instance)
        self.code_gen()

    def _gen_args_vector_whole_reduce(self):
        """
        generate args for vector_whole_reduce func

        Returns
        -------
        config
        """
        config = [self.control_op.repeat_times, self.dst_tensor_op.rep_stride, self.src_tensor_op.blk_stride,
                  self.src_tensor_op.rep_stride]
        if TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc():
            config.append(self.control_op.stride_unit & 0b01)
            config.append((self.control_op.stride_unit & 0b10) >> 1)
            if self.name in ("vcmax", "vcmin"):
                config.append(self.order)
            if self.name == "vcadd" and TikSocManager.is_910b_soc():
                config.append(0)
        elif TikSocManager.is_910b_soc():
            if self.name in ("vcmax", "vcmin"):
                config.append(self.order)
            if self.name == "vcadd" and TikSocManager.is_910b_soc():
                config.append(0)
        if self.dst_tensor_op.tensor_obj.dtype == "int64":
            mask_mode = gen_b64_mask_mode(self.control_op.mask)
            config.extend(mask_mode)
        return config

    def _read_maxmin_cnt_index(self):
        """
        read maxmin_cnt_index

        Returns
        -------
        None
        """
        with self.tik_instance.new_scope():
            with self.tik_instance.context.freeze():
                maxmin_cnt_index_scalar = self.tik_instance.scalar_("int64")
                self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
                self.tik_instance.emit(tvm.call_extern(
                    maxmin_cnt_index_scalar.dtype, "reg_set",
                    maxmin_cnt_index_scalar.get(),
                    tvm.call_extern(maxmin_cnt_index_scalar.dtype, "get_max_min_cnt")))
                if not (TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc()):
                    self.maxmin_cnt_index[2].set_as((maxmin_cnt_index_scalar >> INDEX_SHIFT_POS) & TWO_BYTE_VALUE)
                    self.maxmin_cnt_index[1].set_as((maxmin_cnt_index_scalar >> CNT_SHIFT_POS) & TWO_BYTE_VALUE)
                if get_bit_len(self.src_tensor_op.tensor_obj.dtype) == 32:
                    maxmin_cnt_index_scalar.set_as(maxmin_cnt_index_scalar & FOUR_BYTE_VALUE)
                else:
                    maxmin_cnt_index_scalar.set_as(maxmin_cnt_index_scalar & TWO_BYTE_VALUE)
                self.maxmin_cnt_index[0].set_as(
                    tvm.call_intrin(self.maxmin_cnt_index[0].dtype, "tir.reinterpret", maxmin_cnt_index_scalar.get()))


class WholeReduceOpsNano(WholeReduceOps):

    def _gen_args_vector_whole_reduce(self):
        mode = 0
        config = [self.control_op.repeat_times, self.dst_tensor_op.rep_stride, self.src_tensor_op.blk_stride,
                  self.src_tensor_op.rep_stride, mode]
        return config

    def _vec_muls(self, vadd_mask, dst, src):
        """
        Encapsulates the vec_muls interface, which is used to initialize tmp_ub.
        Parameters
        ----------
        vadd_mask: vec_muls mask
        dst: dst
        src: src

        Returns
        -------

        """
        muls_scalar = 0
        default_blk_stride = 1
        dst_rep_stride = 8
        src_rep_stride = 8
        stride_unit = 0
        round_en = 0
        mask_mode = "counter"
        repeat_times = 1
        scalar_multis_api = TikVectorApiv1.scalar_multis_api(
            "vec_muls", vadd_mask, dst, src, muls_scalar, repeat_times, default_blk_stride,
            default_blk_stride, dst_rep_stride, src_rep_stride, stride_unit, round_en, mask_mode)
        vec_muls_obj = NanoScalarMultisOps(self.tik_instance, scalar_multis_api)
        vec_muls_obj.run_all_incompatible()

    def _vcadd_compatible_mode(self):
        """
        vcadd compatibale mode
        vec_add_rep =ceil_div((repeat_times - 1) * dst_rep_stride + 1, 64)
        vadd_mask = max(dst_rep_stride*(repeat-1) + 1, 64)
        tmp_ub = (vadd_mask,)
        vec_muls(vadd_mask, tmp_ub, tmp_ub, 0, 1, 8, 8，mask_mode=”counter”)
        Vcadd(64, dst_ub, src, repeat_times, dst_rep_stride, 2*src_blk_stride, 2*src_rep_stride)
        Vcadd(64, tmp_ub, src[8], repeat_times, dst_rep_stride, 2*src_blk_stride, 2*src_rep_stride)
        Vec_add(vadd_mask, dst_ub, dst_ub, tmp_ub, vec_add_rep, 8, 8, 8)

        Returns
        -------

        """
        one_block_elements = get_block_size() // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask1, mask2 = reassign_mask(self.tik_instance, self.control_op.mask, one_block_elements)
        enable_mask1, enable_mask2 = check_mask1_mask2(mask1, mask2)
        one_rep_ele_num = 128
        vec_add_rep = ceil_div((self.control_op.repeat_times - 1) *
                               self.dst_tensor_op.rep_stride + 1, one_rep_ele_num)
        vadd_mask = self.dst_tensor_op.rep_stride * (self.control_op.repeat_times - 1) + 1
        if isinstance(vadd_mask, int):
            shape_size = max(vadd_mask, MASK_LEN_64)
        else:
            vadd_mask = self.tik_instance.Scalar(init_value=vadd_mask)
            shape_size = vadd_mask
            with self.tik_instance.if_scope(vadd_mask < MASK_LEN_64):
                shape_size.set_as(MASK_LEN_64)
        tmp_ub = self.tik_instance.Tensor(self.src_tensor_op.tensor_obj.dtype, (shape_size,),
                                          name="tmp_ub", scope=scope_ubuf)
        self._vec_muls(vadd_mask, tmp_ub, tmp_ub)
        dst_ub = self.dst_tensor_op.tensor_obj
        self.src_tensor_op.blk_stride *= 2
        self.src_tensor_op.rep_stride *= 2

        if enable_mask1:
            self.mask_o = mask_concat(self.tik_instance, mask1,
                                      tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                         get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
            self.code_gen()

        if enable_mask2:
            src_offset = self.tik_instance.Scalar("int16", init_value=8)
            self.dst_tensor_op.tensor_obj = tmp_ub
            self.src_tensor_op.tensor_obj = self.src_tensor_op.tensor_obj[src_offset]
            self.mask_o = mask_concat(self.tik_instance, mask2,
                                      tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                         get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
            self.code_gen()

        dst_rep_stride = 8
        src0_rep_stride = 8
        src1_rep_stride = 8
        one_rep_ele_num = 128
        vadd_mask_n = vadd_mask // one_rep_ele_num
        vadd_mask_left = vadd_mask % one_rep_ele_num
        left_mask_offset = vadd_mask_n * one_rep_ele_num
        with self.tik_instance.if_scope(vadd_mask_n > 0):
            self.tik_instance.vec_add(one_rep_ele_num, dst_ub, dst_ub, tmp_ub, vadd_mask_n,
                                      dst_rep_stride, src0_rep_stride, src1_rep_stride)
        with self.tik_instance.if_scope(vadd_mask_left > 0):
            self.tik_instance.vec_add(vadd_mask_left, dst_ub[left_mask_offset:],
                                      dst_ub[left_mask_offset:], tmp_ub[left_mask_offset:],
                                      vec_add_rep-vadd_mask_n,
                                      dst_rep_stride, src0_rep_stride, src1_rep_stride)

    def _set_rep_vcmax_vcmin_index(self, rep_index, tmp_ub, dst_rep_stride):
        """
        update every repeat time vcmax/vcmin max value's index
        Parameters
        ----------
        rep_index: repeat for loop i
        tmp_ub: tmp_ub
        dst_rep_stride: dst repeat stride

        Returns
        -------

        """
        compatible_mode_blk_ele_num = 16
        rep_index_offset = rep_index * dst_rep_stride * 2 + 1
        index = self.tik_instance.Scalar("uint16", name="index")
        index.set_as(self.dst_tensor_op.tensor_obj[rep_index_offset].reinterpret_cast_to("uint16"))
        temp_index = self.tik_instance.Scalar("uint16", name="temp_index")
        temp_index.set_as(tmp_ub[_ONE_REP_BLK_NUM * rep_index + index + 1].reinterpret_cast_to("uint16"))
        abs_index = self.tik_instance.Scalar("uint16", "abs_index")
        # third vcmax/vcmin compare src ele is 'value1 index1 value2 index2'
        # compare result is 'value1 0' or 'value2 2', so index is 0 or 2
        with self.tik_instance.if_scope(index == 2):
            abs_index.set_as(temp_index // _ONE_REP_BLK_NUM * compatible_mode_blk_ele_num +
                             _ONE_REP_BLK_NUM + temp_index % _ONE_REP_BLK_NUM)
        with self.tik_instance.else_scope():
            abs_index.set_as(temp_index // _ONE_REP_BLK_NUM * compatible_mode_blk_ele_num +
                             temp_index % _ONE_REP_BLK_NUM)
        self.dst_tensor_op.tensor_obj[rep_index_offset].set_as(abs_index.reinterpret_cast_to("float16"))

    def _vcmax_vcmin_index_compute(self, dst_rep_stride, tmp_ub):
        """
        compute origin index
        Parameters
        ----------
        dst_rep_stride: dst repeat stride
        tmp_ub: tmp ub

        Returns
        -------

        """
        with self.tik_instance.if_scope(dst_rep_stride == 0):
            self._set_rep_vcmax_vcmin_index(self.control_op.repeat_times - 1, tmp_ub, dst_rep_stride)
        with self.tik_instance.else_scope():
            with self.tik_instance.for_range(0, self.control_op.repeat_times) as i:
                self._set_rep_vcmax_vcmin_index(i, tmp_ub, dst_rep_stride)

    def _init_tmp_ub(self, tmp_shape, tmp_ub):
        dup_scalar = 0
        if self.name == "vcmax":
            dup_scalar = -65504
        elif self.name == "vcmin":
            dup_scalar = 65504
        mask_len = 128
        mask_n = tmp_shape // mask_len
        mask_left = tmp_shape % mask_len
        with self.tik_instance.if_scope(mask_n > 0):
            self.tik_instance.vec_dup(mask_len, tmp_ub, dup_scalar, mask_n, 8)
        with self.tik_instance.if_scope(mask_left > 0):
            self.tik_instance.vec_dup(mask_left, tmp_ub[mask_len*mask_n:], dup_scalar, 1, 8)

    def _vcmax_vcmin_compatible_mode(self):
        """
        vcmax vcmin compatible mode compute code
        compute tmp ub shape => max(8 *(repeat-1) + 4, 64)
        vec_dup(mask, tmp_ub, dup_scalar)
        vcmax(mask1, tmp_ub, src_ub, repeat, 4, 2*src_blk_stride, 2*src_rep_stride)
        vcmax(mask2, tmp_ub[2], src_ub[8], repeat, 4, 2*src_blk_stride, 2*src_rep_stride)
        Returns
        -------

        """
        one_block_elements = get_block_size() // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask1, mask2 = reassign_mask(self.tik_instance, self.control_op.mask, one_block_elements)
        enable_mask1, enable_mask2 = check_mask1_mask2(mask1, mask2)
        one_rep_ele_num = 64
        # two vcmax/vcmin save data ele num is 2*2

        double_instr_rep_ele_num = 4
        ele_len = _ONE_REP_BLK_NUM * (self.control_op.repeat_times - 1) + double_instr_rep_ele_num
        if isinstance(self.control_op.repeat_times, int):
            tmp_shape = max(ele_len, one_rep_ele_num)
        else:
            tmp_shape = self.tik_instance.Scalar(init_value=ele_len)
            with self.tik_instance.if_scope(tmp_shape < one_rep_ele_num):
                tmp_shape.set_as(one_rep_ele_num)
        tmp_ub = self.tik_instance.Tensor(self.src_tensor_op.tensor_obj.dtype, (tmp_shape,),
                                          name="tmp_ub", scope=scope_ubuf)
        # init tmp_ub with instr name
        self._init_tmp_ub(tmp_shape, tmp_ub)
        dst_ub = self.dst_tensor_op.tensor_obj
        dst_rep_stride = self.dst_tensor_op.rep_stride
        self.one_blk_size = 16
        if enable_mask1:
            self.src_tensor_op.blk_stride *= 2
            self.src_tensor_op.rep_stride *= 2
            self.dst_tensor_op.rep_stride = 4
            self.dst_tensor_op.tensor_obj = tmp_ub
            self.mask_o = mask_concat(self.tik_instance, mask1,
                                      tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                         get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
            self.code_gen()

        if enable_mask2:
            dst_offset = self.tik_instance.Scalar("uint16", init_value=2)
            src_offset = self.tik_instance.Scalar("uint16", init_value=8)
            self.dst_tensor_op.tensor_obj = self.dst_tensor_op.tensor_obj[dst_offset]
            self.src_tensor_op.tensor_obj = self.src_tensor_op.tensor_obj[src_offset]
            self.mask_o = mask_concat(self.tik_instance, mask2,
                                      tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                         get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
            self.code_gen()
        # third vcmax/vcmin compare src ele is 'value1 index1 value2 index2'
        # src data need value1 and value2, so the value of mask must be [0,5].
        # Indicates that the first and third elements are selected.
        mask3 = [0, 5]
        self.mask_o = mask_concat(self.tik_instance, mask3,
                                  tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                     get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
        self.src_tensor_op.tensor_obj = tmp_ub
        self.dst_tensor_op.tensor_obj = dst_ub
        self.src_tensor_op.blk_stride = 1
        self.src_tensor_op.rep_stride = 1
        self.dst_tensor_op.rep_stride = dst_rep_stride
        self.code_gen()
        self._vcmax_vcmin_index_compute(dst_rep_stride, tmp_ub)

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.reduce_check_obj.all_check(self.tik_instance)
        if is_compatible_mode():
            if self.name == "vcadd":
                self._vcadd_compatible_mode()
            elif self.name in ("vcmax", "vcmin"):
                self._vcmax_vcmin_compatible_mode()
        else:
            self.code_gen()


class GroupReduceOps:
    """
    Group Reduce Ops
    """

    def __init__(self, tik_instance, group_reduce_api):
        super().__init__()
        self.tik_instance = tik_instance
        self.name = group_reduce_api.name
        self.print_name = group_reduce_api.name
        self.control_op = ControlOp(group_reduce_api.mask, group_reduce_api.repeat_times, group_reduce_api.stride_unit)
        self.dst_tensor_op = TensorOp(group_reduce_api.dst, _DEFAULT_BLK_STRIDE, group_reduce_api.dst_rep_stride, "dst")
        self.src_tensor_op = TensorOp(group_reduce_api.src, group_reduce_api.src_blk_stride,
                                      group_reduce_api.src_rep_stride, "src")
        self.check_params = (self.dst_tensor_op, self.src_tensor_op, self.control_op)
        self.reduce_check_obj = ReduceCheck(self.print_name, self.check_params)
        self.mask_o = None
        self.maxmin_cnt_index = group_reduce_api.maxmin_cnt_index
        self.order = group_reduce_api.order
        self.one_blk_size = ONE_BLK_SIZE

    @vec_reduce_group_decorator
    def code_gen(self):
        """
        code gen

        Returns
        -------
        None
        """
        with self.tik_instance.new_scope():
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
            config = [self.control_op.repeat_times, self.dst_tensor_op.rep_stride, self.src_tensor_op.blk_stride,
                      self.src_tensor_op.rep_stride]
            if self.dst_tensor_op.tensor_obj.dtype == "int64":
                mask_mode = gen_b64_mask_mode(self.control_op.mask)
                config.extend(mask_mode)
            src_bit_len = get_bit_len(self.src_tensor_op.tensor_obj.dtype)
            times = self.control_op.repeat_times - 1
            dst_repeat_stride = times * self.dst_tensor_op.rep_stride + 1
            dst_extent = Expr(dst_repeat_stride * src_bit_len * BLK_NUM_PER_REP // ONE_BYTE_BIT_LEN)
            src_repeat_stride = times * self.src_tensor_op.rep_stride
            src_block_len = BLK_NUM_PER_REP - 1
            src_blk = src_block_len * self.src_tensor_op.blk_stride
            src_extent = Expr((src_repeat_stride + src_blk + 1) * self.one_blk_size)
            # when repeat time >1 and the count of dst write element is not
            # the multi of ONE_BLK_SIZE
            dst_extent = Expr(ceil_div(dst_extent, self.one_blk_size) * self.one_blk_size)
            instr = tvm.call_extern(self.dst_tensor_op.tensor_obj.dtype, self.name,
                                    self.dst_tensor_op.tensor_obj.access_ptr("w", extent=dst_extent.get()),
                                    self.src_tensor_op.tensor_obj.access_ptr(
                                        "r", extent=src_extent.get()), *type_convert(config))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *self.mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.reduce_check_obj.all_check(self.tik_instance)
        self.code_gen()


class GroupReduceOpsNano(GroupReduceOps):
    def _vec_instr(self, dst, src0, src1, dst_rep_stride):
        instruction_map = {
            "vcgadd": "vec_add",
            "vcgmax": "vec_max",
            "vcgmin": "vec_min"
        }
        mask = 8
        default_blk_stride = 1

        src0_rep_stride = dst_rep_stride
        src1_rep_stride = dst_rep_stride
        multi_api_ins = TikVectorApiv1.multi_api(instruction_map.get(self.name), mask, dst, src0, src1,
                                                 self.control_op.repeat_times,
                                                 default_blk_stride,
                                                 default_blk_stride, default_blk_stride, dst_rep_stride,
                                                 src0_rep_stride,
                                                 src1_rep_stride, stride_unit=0)
        vec_add_obj = NanoMultiOpApi(self.tik_instance, multi_api_ins)
        # compatible mode inner call not compatible vec_add instr
        TIK_SOC_INFO.cpt_rep_size = 128
        vec_add_obj.run_all_incompatible_mode()
        # reset TIK_SOC_INFO.cpt_rep_size
        TIK_SOC_INFO.cpt_rep_size = 256

    def _vec_dup(self, mask, dst):
        """
        init tmp ub value to 0/-65504/65504 with instr name
        Parameters
        ----------
        mask: tmp ub mask
        dst: tmp ub

        Returns
        -------

        """
        if self.name == "vcgadd":
            dup_scalar = 0
        elif self.name == "vcgmax":
            dup_scalar = -65504
        else:
            dup_scalar = 65504
        dst_rep_stride = 8
        mask_len = 128
        mask_n = mask // mask_len
        mask_left = mask % mask_len
        with self.tik_instance.if_scope(mask_n > 0):
            self.tik_instance.vec_dup(mask_len, dst, dup_scalar, mask_n, dst_rep_stride)
        with self.tik_instance.if_scope(mask_left > 0):
            self.tik_instance.vec_dup(mask_left, dst[mask_len*mask_n:], dup_scalar, 1, dst_rep_stride)

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen
        compatible mode vcgadd/vcgmax/vcgmin with incompatible mode vcgadd/vcgmax/vcgmin and vec_dup/vec_add
        tmp_ub = max((repeat_times - 1)*8* dst_rep_stride +8, 64)
        vec_dup(tmp_ub, 0/-65504/65504)
        vcgadd(mask1, dst_ub, src, repeat_times, dst_rep_stride, 2 * src_blk_stride, 2 * src_rep_stride)
        Vcgadd(mask2, Tmp_ub, src[8], repeat_times, dst_rep_stride, 2*src_blk_stride, 2*src_rep_stride)
        if dst_rep_stride > 0:
            vec_vadd(8, dst_ub, dst_ub, tmp_ub, repeat, dst_rep_stride, dst_rep_stride, dst_rep_stride)
        else:
            Vec_vadd(8, dst_ub, dst_ub, tmp_ub, 1, dst_rep_stride, dst_rep_stride, dst_rep_stride)
        Returns
        -------
        None
        """
        incompatible_mode_mask = 64
        self.mask_o = self.reduce_check_obj.all_check(self.tik_instance)
        if is_compatible_mode():
            one_block_elements = get_block_size() // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
            mask1, mask2 = reassign_mask(self.tik_instance, self.control_op.mask, one_block_elements)
            enable_mask1, enable_mask2 = check_mask1_mask2(mask1, mask2)
            dst_rep_stride = self.dst_tensor_op.rep_stride

            tmp_shape = 8 * (self.control_op.repeat_times - 1) * self.dst_tensor_op.rep_stride + 8
            if isinstance(tmp_shape, int):
                tmp_shape = max(tmp_shape, incompatible_mode_mask)
            else:
                tmp_shape = self.tik_instance.Scalar(init_value=tmp_shape)
                with self.tik_instance.if_scope(tmp_shape < incompatible_mode_mask):
                    tmp_shape.set_as(incompatible_mode_mask)
            tmp_ub = self.tik_instance.Tensor(self.src_tensor_op.tensor_obj.dtype, (tmp_shape,),
                                              name="tmp_ub", scope=scope_ubuf)
            self._vec_dup(tmp_shape, tmp_ub)
            self.src_tensor_op.rep_stride *= 2
            self.src_tensor_op.blk_stride *= 2
            self.one_blk_size = 16
            if enable_mask1:
                self.mask_o = mask_concat(self.tik_instance, mask1,
                                          tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                             get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
                self.code_gen()
            dst_ub = self.dst_tensor_op.tensor_obj
            self.dst_tensor_op.tensor_obj = tmp_ub
            if enable_mask2:
                src_offset = self.tik_instance.Scalar("uint16", init_value=8)
                self.src_tensor_op.tensor_obj = self.src_tensor_op.tensor_obj[src_offset]
                self.mask_o = mask_concat(self.tik_instance, mask2,
                                          tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                             get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
                self.code_gen()
            with self.tik_instance.if_scope(dst_rep_stride > 0):
                self._vec_instr(dst_ub, tmp_ub, dst_ub, dst_rep_stride)
            with self.tik_instance.else_scope():
                # If dst_rep_stride equals 0, tmp_ub dst_ub accumulates the same position.
                self.control_op.repeat_times = 1
                self._vec_instr(dst_ub, tmp_ub, dst_ub, dst_rep_stride)
        else:
            self.code_gen()


class PairReduceOps:
    """
    Pair Reduce Ops
    """

    def __init__(self, tik_instance, pair_reduce_api):
        super().__init__()
        self.tik_instance = tik_instance
        self.print_name = pair_reduce_api.name
        if pair_reduce_api.name in {"vec_cpadd", "vcpadd"}:
            self.name = "vcpadd"
        else:
            self.name = pair_reduce_api.name
        self.control_op = ControlOp(pair_reduce_api.mask, pair_reduce_api.repeat_times, pair_reduce_api.stride_unit)
        self.dst_tensor_op = TensorOp(pair_reduce_api.dst, _DEFAULT_BLK_STRIDE, pair_reduce_api.dst_rep_stride, "dst")
        self.src_tensor_op = TensorOp(pair_reduce_api.src, pair_reduce_api.src_blk_stride,
                                      pair_reduce_api.src_rep_stride, "src")
        self.check_params = (self.dst_tensor_op, self.src_tensor_op, self.control_op)
        self.reduce_check_obj = ReduceCheck(self.print_name, self.check_params)
        self.mask_o = None
        self.maxmin_cnt_index = pair_reduce_api.maxmin_cnt_index
        self.order = pair_reduce_api.order
        self.one_blk_size = ONE_BLK_SIZE

    @vec_reduce_wo_order_decorator
    def code_gen(self):
        """
        code gen

        Returns
        -------
        None
        """
        config = [self.dst_tensor_op.rep_stride, self.src_tensor_op.blk_stride,
                  self.src_tensor_op.rep_stride, self.control_op.repeat_times]
        offset_list = VECTOR_PAIR_OFFSET_LIST
        segment_list = VECTOR_PAIR_SEGMENT_LIST
        args = concat_params(config, offset_list, segment_list, dtype="uint64")
        with self.tik_instance.new_scope():
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
            times = self.control_op.repeat_times - 1
            src_repeat_stride = times * self.src_tensor_op.rep_stride
            src_blk_stride = (BLK_NUM_PER_REP - 1) * self.src_tensor_op.blk_stride
            src_extent = Expr((src_repeat_stride + src_blk_stride + 1) * self.one_blk_size)
            # dst_extent: The result continuous 128B , see ISA
            dst_repeat_stride = times * self.dst_tensor_op.rep_stride
            dst_extent = Expr(dst_repeat_stride * 128 + 128)
            if TikSocManager.is_v300_610l_soc() or TikSocManager.is_v210_vec_soc():
                config = [self.control_op.repeat_times, self.dst_tensor_op.rep_stride,
                          self.src_tensor_op.blk_stride, self.src_tensor_op.rep_stride]
                instr = tvm.call_extern(self.dst_tensor_op.tensor_obj.dtype, self.name,
                                        self.dst_tensor_op.tensor_obj.access_ptr("w", extent=dst_extent.get()),
                                        self.src_tensor_op.tensor_obj.access_ptr("r", extent=src_extent.get()),
                                        *type_convert(config))
            else:
                instr = tvm.call_extern(self.dst_tensor_op.tensor_obj.dtype, self.name,
                                        self.dst_tensor_op.tensor_obj.access_ptr("w", extent=dst_extent.get()),
                                        self.src_tensor_op.tensor_obj.access_ptr("r", extent=src_extent.get()),
                                        type_convert(args))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *self.mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.reduce_check_obj.all_check(self.tik_instance)
        self.code_gen()


class NanoPairReduceOps(PairReduceOps):
    def _reset_mask(self):
        """
        compatible mode, The 128 elements are split into the first 64 and the last 64 elements.
        128 ele => 64 ele / 64 ele
        100 ele => 64 ele / 36 ele
        50 ele => 50 ele / 0 ele
        Returns
        -------
        mask low bit mask, mask high bit mask
        """
        mask = self.control_op.mask
        mask_low_64 = 64
        if isinstance(mask, int):
            mask_l = mask_low_64 if mask > mask_low_64 else mask
            mask_h = mask - mask_low_64 if mask > mask_low_64 else 0
            return mask_l, mask_h
        elif isinstance(mask, Scalar):
            mask_l = self.tik_instance.Scalar("uint16")
            mask_h = self.tik_instance.Scalar("uint16")
            with self.tik_instance.if_scope(mask > mask_low_64):
                mask_l.set_as(mask_low_64)
                mask_h.set_as(mask - mask_low_64)
            with self.tik_instance.else_scope():
                mask_l.set_as(mask)
                mask_h.set_as(0)
            return mask_l, mask_h
        else:
            mask_l = mask[1]
            mask_h = mask[0]
            mask_zero = 0
            if isinstance(mask_l, Scalar):
                mask_zero = self.tik_instance.Scalar(init_value=0)
            return [mask_zero, mask_l], [mask_zero, mask_h]

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.reduce_check_obj.all_check(self.tik_instance)
        if is_compatible_mode():
            mask1, mask2 = self._reset_mask()
            enable_mask1, enable_mask2 = check_mask1_mask2(mask1, mask2)
            self.one_blk_size = 16
            # vec_cpadd
            if enable_mask1:
                self.dst_tensor_op.rep_stride *= 2
                self.src_tensor_op.rep_stride *= 2
                self.mask_o = mask_concat(self.tik_instance, mask1,
                                          tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                             get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
                self.code_gen()
            # vec_cpadd
            if enable_mask2:
                self.mask_o = mask_concat(self.tik_instance, mask2,
                                          tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                             get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
                dst_offset = self.tik_instance.Scalar("uint16", init_value=32)
                src_offset = self.tik_instance.Scalar("uint16", init_value=64)
                self.dst_tensor_op.tensor_obj = self.dst_tensor_op.tensor_obj[dst_offset]
                self.src_tensor_op.tensor_obj = self.src_tensor_op.tensor_obj[src_offset]
                self.code_gen()
        else:
            self.code_gen()
