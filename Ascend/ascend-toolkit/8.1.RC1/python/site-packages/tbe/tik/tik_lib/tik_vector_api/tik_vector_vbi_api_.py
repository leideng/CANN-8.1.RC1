#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_single_api_.py
DESC:     provide vector instructions
CREATED:  2021-09-8 18:53:42
"""


from tbe import tvm

from tbe.tik.common import DTYPE_SIZE
from tbe.common.platform import scope_ubuf
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import reduce_mul
from tbe.tik.debug.decorators import new_high_level_api_debug_decorator
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_vbi_debug import vbi_decorator
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_vbi_debug import vbi_high_api_decorator
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_params import ONE_REP_BYTE_SIZE
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MIN_STRIDE
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_vector_api.tik_params_check_vbi import VbiCheckParams
from tbe.tik.tik_lib.tik_vector_api.tik_vbi_control import VbiControlOp
from tbe.tik.tik_lib.tik_vector_api.tik_vector_vbi_tensor_op import VbiTensorOp
from tbe.tik.tik_lib.tik_expr_convert import type_convert
from tbe.tik.tik_lib.tik_params import PIPE_V


class VbiOpApi:
    """
    Vbi Vector Ops
    """

    def __init__(self, tik_instance, vbi_api):
        super().__init__()
        self.tik_instance = tik_instance
        self.name = vbi_api.name
        self.print_name = vbi_api.name
        repeat_times_list = [vbi_api.vertical_repeat_times, vbi_api.horizontal_repeat_times]
        self.vbi_control_op = VbiControlOp(vbi_api.mask, repeat_times_list,
                                           vbi_api.repeat_mode, vbi_api.vertical_repeat_offset)
        self.vbi_control_op.mask_len = self.vbi_control_op.get_vbi_mask_len()
        self.dst_tensor_op = VbiTensorOp(vbi_api.dst, vbi_api.dst_blk_stride, MIN_STRIDE, "dst")
        self.src0_tensor_op = VbiTensorOp(vbi_api.src0, MIN_STRIDE, MIN_STRIDE, "src0")
        self.src1_tensor_op = VbiTensorOp(vbi_api.src1, MIN_STRIDE, MIN_STRIDE, "src1")
        self.src0_offset_tensor_op = VbiTensorOp(vbi_api.src0_offset, MIN_STRIDE, MIN_STRIDE, "src0_offset")
        self.check_params = (self.dst_tensor_op, self.src0_tensor_op, self.src1_tensor_op,
                             self.src0_offset_tensor_op, self.vbi_control_op)
        self.vbi_check_params = VbiCheckParams(self.print_name, self.check_params)
        self.enable_vbi_double_buffer = vbi_api.enable_vbi_double_buffer
        self.mask_o = None

    @vbi_high_api_decorator
    @new_high_level_api_debug_decorator
    def vbi_v220_gen_code(self):
        """
        generate v220 vbi code
        """
        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        double_buffer = self.enable_vbi_double_buffer

        # The performance is better when the hyper_parameter value of v300 is 1.
        if TikSocManager.is_v300_610l_soc():
            hyper_parameter = 1
        else:
            hyper_parameter = self.generate_hp(one_blk_size)

        h_rep_len = eight_blk_size * self.vbi_control_op.horizontal_repeat_times * hyper_parameter
        with self.tik_instance.new_stmt_scope(disable_sync=True):
            vgatherb_tensor = self.tik_instance.Tensor(self.dst_tensor_op.tensor_obj.dtype, (h_rep_len,),
                                                       name="vbi_work_tensor_1", scope=scope_ubuf)
            vmul_tensor = self.tik_instance.Tensor(self.dst_tensor_op.tensor_obj.dtype, (h_rep_len,),
                                                   name="vbi_work_tensor_2", scope=scope_ubuf)
            self.init_dst_tensor(one_blk_size)
            if self.vbi_control_op.repeat_mode == 0:
                self.gen_vbi_code_repeat_mode_0(vgatherb_tensor, vmul_tensor, hyper_parameter, double_buffer)
            elif self.vbi_control_op.repeat_mode == 1:
                self.gen_vbi_code_repeat_mode_1(vgatherb_tensor, vmul_tensor, hyper_parameter)

            if self.vbi_control_op.repeat_mode == 0 and double_buffer:
                start_v_rep = self.vbi_control_op.vertical_repeat_times // (hyper_parameter * 2) * (hyper_parameter * 2)
            else:
                start_v_rep = self.vbi_control_op.vertical_repeat_times // hyper_parameter * hyper_parameter
            rest_v_rep = self.vbi_control_op.vertical_repeat_times - start_v_rep
            if isinstance(rest_v_rep, int):
                if rest_v_rep == 0:
                    return
                tensor_list = (vgatherb_tensor, vmul_tensor)
                self.gen_vbi_code_handle_rest(tensor_list, start_v_rep, rest_v_rep, double_buffer)
            else:
                with self.tik_instance.if_scope(rest_v_rep > 0):
                    tensor_list = (vgatherb_tensor, vmul_tensor)
                    self.gen_vbi_code_handle_rest(tensor_list, start_v_rep, rest_v_rep, double_buffer)

    def generate_hp(self, one_blk_size):
        """
        generate hyper-parameters that control the size of work tensor
        """
        if isinstance(self.vbi_control_op.vertical_repeat_times, int) and \
                isinstance(self.vbi_control_op.horizontal_repeat_times, int):
            hyper_parameter = 2
            if (self.vbi_control_op.horizontal_repeat_times * hyper_parameter) % one_blk_size != 0:
                hyper_parameter = 4
            if hyper_parameter > self.vbi_control_op.vertical_repeat_times:
                hyper_parameter = self.vbi_control_op.vertical_repeat_times
            if self.vbi_control_op.horizontal_repeat_times * hyper_parameter > 255:
                hyper_parameter = 1
            return hyper_parameter

        hyper_parameter = self.tik_instance.Scalar("int32", init_value=2)
        with self.tik_instance.if_scope(self.vbi_control_op.horizontal_repeat_times * hyper_parameter %
                                        one_blk_size != 0):
            hyper_parameter.set_as(4)
        with self.tik_instance.if_scope(hyper_parameter > self.vbi_control_op.vertical_repeat_times):
            hyper_parameter.set_as(self.vbi_control_op.vertical_repeat_times)
        with self.tik_instance.if_scope(self.vbi_control_op.horizontal_repeat_times * hyper_parameter > 255):
            hyper_parameter.set_as(1)
        return hyper_parameter

    def gen_vbi_code_repeat_mode_0(self, vgatherb_tensor, vmul_tensor, hp, double_buffer=False):
        """
        generate vbi code with repeat_mode is 0
        """

        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask_all = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        repeat = self.vbi_control_op.horizontal_repeat_times * hp
        if not double_buffer:
            with self.tik_instance.for_range(0, self.vbi_control_op.vertical_repeat_times // hp) as j:
                self.tik_instance.vgatherb(vgatherb_tensor, self.src0_tensor_op.tensor_obj,
                                           self.src0_offset_tensor_op.tensor_obj[j * repeat * 8:], repeat, 1, 8)
                self.tik_instance.vbcb(vmul_tensor, self.src1_tensor_op.tensor_obj[repeat * j:],
                                       ceil_div(repeat, 8), 1, 8)
                self.tik_instance.emit(
                    tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
                self.tik_instance.vmul(mask_all, vgatherb_tensor, vmul_tensor, vgatherb_tensor, repeat_times=repeat,
                                       dst_blk_stride=1, src0_blk_stride=0, src1_blk_stride=1,
                                       dst_rep_stride=8, src0_rep_stride=1, src1_rep_stride=8)
                self.inject_sync(one_blk_size)
                with self.tik_instance.for_range(0, hp) as k:
                    self.init_dst_and_inject_sync(j * hp + k, one_blk_size)
                    dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * (j * hp + k):]
                    self.tik_instance.vadd(self.vbi_control_op.mask, dst,
                                           vgatherb_tensor[eight_blk_size *
                                                           self.vbi_control_op.horizontal_repeat_times * k:],
                                           dst, self.vbi_control_op.horizontal_repeat_times,
                                           dst_blk_stride=self.dst_tensor_op.blk_stride, src0_blk_stride=1,
                                           src1_blk_stride=self.dst_tensor_op.blk_stride,
                                           dst_rep_stride=0, src0_rep_stride=8, src1_rep_stride=0, stride_unit=0)
            return
        self.gen_vbi_code_repeat_mode_0_double_buffer(vgatherb_tensor, vmul_tensor, hp)

    def gen_vbi_code_repeat_mode_0_double_buffer(self, vgatherb_tensor, vmul_tensor, hp):
        """
        generate vbi code with repeat_mode is 0 with double_buffer
        """
        eight_blk_size, one_blk_size, mask_all, repeat, vgatherb_tensor_1, \
        vgatherb_tensor_2, vmul_tensor_2 = self._gen_vbi_code_params_double_buffer(vgatherb_tensor, hp)

        self.tik_instance.vgatherb(vgatherb_tensor_1, self.src0_tensor_op.tensor_obj,
                                   self.src0_offset_tensor_op.tensor_obj[0 * repeat * 8:], repeat, 1, 8)
        self.tik_instance.vbcb(vmul_tensor, self.src1_tensor_op.tensor_obj[repeat * 0:], ceil_div(repeat, 8), 1, 8)
        self.tik_instance.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
        with self.tik_instance.for_range(0, self.vbi_control_op.vertical_repeat_times // (hp * 2)) as j:
            self.tik_instance.vgatherb(vgatherb_tensor_2, self.src0_tensor_op.tensor_obj,
                                       self.src0_offset_tensor_op.tensor_obj[(j * 2 + 1) * repeat * 8:], repeat, 1, 8)
            self.tik_instance.vbcb(vmul_tensor_2, self.src1_tensor_op.tensor_obj[repeat * (j * 2 + 1):],
                                   ceil_div(repeat, 8), 1, 8)
            self.tik_instance.vmul(mask_all, vgatherb_tensor_1, vmul_tensor, vgatherb_tensor_1, repeat_times=repeat,
                                   dst_blk_stride=1, src0_blk_stride=0, src1_blk_stride=1,
                                   dst_rep_stride=8, src0_rep_stride=1, src1_rep_stride=8)
            self.inject_sync(one_blk_size)
            with self.tik_instance.for_range(0, hp) as k:
                self.init_dst_and_inject_sync(j * 2 * hp + k, one_blk_size)
                self.tik_instance.vadd(self.vbi_control_op.mask,
                                       self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset *
                                                                     (j * 2 * hp + k):],
                                       vgatherb_tensor_1[eight_blk_size *
                                                         self.vbi_control_op.horizontal_repeat_times * k:],
                                       self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset *
                                                                     (j * 2 * hp + k):],
                                       self.vbi_control_op.horizontal_repeat_times,
                                       dst_blk_stride=self.dst_tensor_op.blk_stride, src0_blk_stride=1,
                                       src1_blk_stride=self.dst_tensor_op.blk_stride, dst_rep_stride=0,
                                       src0_rep_stride=8, src1_rep_stride=0)

            with self.tik_instance.if_scope((j * 2 + 2) < (self.vbi_control_op.vertical_repeat_times // hp)):
                self.tik_instance.vgatherb(vgatherb_tensor_1, self.src0_tensor_op.tensor_obj,
                                           self.src0_offset_tensor_op.tensor_obj[(j * 2 + 2) * repeat * 8:],
                                           repeat, 1, 8)
                self.tik_instance.vbcb(vmul_tensor, self.src1_tensor_op.tensor_obj[repeat * (j * 2 + 2):],
                                       ceil_div(repeat, 8), 1, 8)
            self.tik_instance.vmul(mask_all, vgatherb_tensor_2, vmul_tensor_2, vgatherb_tensor_2, repeat_times=repeat,
                                   dst_blk_stride=1, src0_blk_stride=0, src1_blk_stride=1,
                                   dst_rep_stride=8, src0_rep_stride=1, src1_rep_stride=8)
            self.inject_sync(one_blk_size)
            with self.tik_instance.for_range(0, hp) as k:
                self.init_dst_and_inject_sync((j * 2 + 1) * hp + k, one_blk_size)
                dst_offset = self.vbi_control_op.vertical_repeat_offset * ((j * 2 + 1) * hp + k)
                self.tik_instance.vadd(self.vbi_control_op.mask, self.dst_tensor_op.tensor_obj[dst_offset:],
                                       vgatherb_tensor_2[eight_blk_size * self.vbi_control_op.horizontal_repeat_times
                                                         * k:], self.dst_tensor_op.tensor_obj[dst_offset:],
                                       self.vbi_control_op.horizontal_repeat_times,
                                       dst_blk_stride=self.dst_tensor_op.blk_stride, src0_blk_stride=1,
                                       src1_blk_stride=self.dst_tensor_op.blk_stride, dst_rep_stride=0,
                                       src0_rep_stride=8, src1_rep_stride=0, stride_unit=0)

    def gen_vbi_code_repeat_mode_1(self, vgatherb_tensor, vmul_tensor, hp):
        """
        generate vbi code with repeat_mode is 1
        """
        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask_all = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        repeat = self.vbi_control_op.horizontal_repeat_times * hp
        with self.tik_instance.for_range(0, self.vbi_control_op.vertical_repeat_times // hp) as j:
            self.tik_instance.vgatherb(vgatherb_tensor, self.src0_tensor_op.tensor_obj,
                                       self.src0_offset_tensor_op.tensor_obj[j * repeat * 8:], repeat, 1, 8)
            self.tik_instance.vbcb(vmul_tensor, self.src1_tensor_op.tensor_obj[repeat * 8 * j:], repeat, 1, 8)
            self.tik_instance.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
            self.tik_instance.vmul(mask_all, vmul_tensor, vgatherb_tensor, vmul_tensor, repeat_times=repeat,
                                   dst_blk_stride=1, src0_blk_stride=1, src1_blk_stride=1,
                                   dst_rep_stride=8, src0_rep_stride=8, src1_rep_stride=8)
            self.inject_sync(one_blk_size)
            with self.tik_instance.for_range(0, hp) as k:
                self.init_dst_and_inject_sync(j * hp + k, one_blk_size)
                dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * (j * hp + k):]
                self.tik_instance.vadd(self.vbi_control_op.mask, dst,
                                       vmul_tensor[eight_blk_size * self.vbi_control_op.horizontal_repeat_times * k:],
                                       dst, self.vbi_control_op.horizontal_repeat_times,
                                       dst_blk_stride=self.dst_tensor_op.blk_stride, src0_blk_stride=1,
                                       src1_blk_stride=self.dst_tensor_op.blk_stride,
                                       dst_rep_stride=0, src0_rep_stride=8, src1_rep_stride=0, stride_unit=0)

    def gen_vbi_code_handle_rest(self, tensor_list, start_v_rep, rest_v_rep, double_buffer):
        """
        handle the rest vertical repeat
        """
        if self.vbi_control_op.repeat_mode == 0 and not double_buffer:
            self._gen_vbi_code_not_double_buffer(tensor_list, start_v_rep, rest_v_rep)
        elif self.vbi_control_op.repeat_mode == 0 and double_buffer:
            self._gen_vbi_code_double_buffer(tensor_list, start_v_rep, rest_v_rep)
        elif self.vbi_control_op.repeat_mode == 1:
            self._gen_vbi_code_repeat_mode_one(tensor_list, start_v_rep, rest_v_rep)

    def inject_sync(self, one_blk_size):
        """
        inject sync
        """
        if isinstance(self.dst_tensor_op.blk_stride, int) and \
                isinstance(self.vbi_control_op.vertical_repeat_offset, int):
            if (self.dst_tensor_op.blk_stride * 7 + 1) * one_blk_size <= self.vbi_control_op.vertical_repeat_offset:
                self.tik_instance.emit(
                    tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
        else:
            with self.tik_instance.if_scope((self.dst_tensor_op.blk_stride * 7 + 1) *
                                            one_blk_size <= self.vbi_control_op.vertical_repeat_offset):
                self.tik_instance.emit(
                    tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))

    def init_dst_and_inject_sync(self, index, one_blk_size):
        """
        init dst with value 0 and inject the sync
        """
        if isinstance(self.dst_tensor_op.blk_stride, int) and \
                isinstance(self.vbi_control_op.vertical_repeat_offset, int):
            if (self.dst_tensor_op.blk_stride * 7 + 1) * one_blk_size > self.vbi_control_op.vertical_repeat_offset:
                dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * index:]
                self.tik_instance.vector_dup(self.vbi_control_op.mask, dst, 0, 1, self.dst_tensor_op.blk_stride, 8)
                self.tik_instance.emit(
                    tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
        else:
            with self.tik_instance.if_scope((self.dst_tensor_op.blk_stride * 7 + 1) *
                                            one_blk_size > self.vbi_control_op.vertical_repeat_offset):
                dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * index:]
                self.tik_instance.vector_dup(self.vbi_control_op.mask, dst, 0, 1, self.dst_tensor_op.blk_stride, 8)
                self.tik_instance.emit(
                    tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))

    def init_dst_tensor(self, one_block_size):
        """
        Init the dst tensor with value 0.
        """
        if isinstance(self.dst_tensor_op.blk_stride, int) and \
                isinstance(self.vbi_control_op.vertical_repeat_offset, int):
            self.fill_dst_tensor_imme(one_block_size)
        else:
            self.fill_dst_tensor_scalar(one_block_size)

    def fill_dst_tensor_scalar(self, one_block_size):
        """
        fill the dst tensor with value 0.
        """

        with self.tik_instance.if_scope((self.dst_tensor_op.blk_stride * 7 + 1) *
                                        one_block_size <= self.vbi_control_op.vertical_repeat_offset):
            if isinstance(self.vbi_control_op.vertical_repeat_offset, int):
                if self.vbi_control_op.vertical_repeat_offset * \
                        DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype] // 32 <= 255:
                    self.tik_instance.vector_dup(self.vbi_control_op.mask, self.dst_tensor_op.tensor_obj, 0,
                                                 self.vbi_control_op.vertical_repeat_times,
                                                 self.dst_tensor_op.blk_stride,
                                                 self.vbi_control_op.vertical_repeat_offset *
                                                 DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype] // 32)
                else:
                    with self.tik_instance.for_range(0, self.vbi_control_op.vertical_repeat_times) as j:
                        dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * j:]
                        self.tik_instance.vector_dup(self.vbi_control_op.mask,
                                                     dst, 0, 1, self.dst_tensor_op.blk_stride, 8)
            else:
                with self.tik_instance.if_scope(self.vbi_control_op.vertical_repeat_offset *
                                                DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype] // 32 <= 255):
                    self.tik_instance.vector_dup(self.vbi_control_op.mask, self.dst_tensor_op.tensor_obj, 0,
                                                 self.vbi_control_op.vertical_repeat_times,
                                                 self.dst_tensor_op.blk_stride,
                                                 self.vbi_control_op.vertical_repeat_offset *
                                                 DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype] // 32)
                with self.tik_instance.else_scope():
                    with self.tik_instance.for_range(0, self.vbi_control_op.vertical_repeat_times) as j:
                        dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * j:]
                        self.tik_instance.vector_dup(self.vbi_control_op.mask,
                                                     dst, 0, 1, self.dst_tensor_op.blk_stride, 8)

    def fill_dst_tensor_imme(self, one_block_size):
        """
        fill the dst tensor with value 0.
        """
        if (self.dst_tensor_op.blk_stride * 7 + 1) * one_block_size <= self.vbi_control_op.vertical_repeat_offset:
            repeat = self.vbi_control_op.vertical_repeat_offset * DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype] // 32
            if repeat <= 255:
                self.tik_instance.vector_dup(self.vbi_control_op.mask, self.dst_tensor_op.tensor_obj, 0,
                                             self.vbi_control_op.vertical_repeat_times, self.dst_tensor_op.blk_stride,
                                             self.vbi_control_op.vertical_repeat_offset *
                                             DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype] // 32)
            else:
                with self.tik_instance.for_range(0, self.vbi_control_op.vertical_repeat_times) as j:
                    dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * j:]
                    self.tik_instance.vector_dup(self.vbi_control_op.mask, dst, 0, 1, self.dst_tensor_op.blk_stride, 8)

    @vbi_decorator
    def vbi_v200_gen_code(self):
        """
        generate vbi code
        """
        if not TikSocManager.is_v300_610l_soc():
            self._gen_vbi_code_vadds_part()
        self._gen_vbi_code_vbi_part(
            *self.cal_vbi_extent(),
            [self.vbi_control_op.horizontal_repeat_times, self.vbi_control_op.repeat_mode,
             self.dst_tensor_op.blk_stride, self.vbi_control_op.vertical_repeat_offset,
             self.vbi_control_op.vertical_repeat_times])

    def cal_vbi_extent(self):
        """
        calculate dst/src0_offset/src1 extent for vbi instrction
        """
        # cal extent
        dst_dtype_value = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        dst_extent_value = self.dst_tensor_op.get_vbi_dst_need_size(dst_dtype_value, self.vbi_control_op)
        dst_dtype_size = DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        dst_extent = Expr(dst_extent_value * dst_dtype_size).get()
        src0_offset_extent_value = self.dst_tensor_op.get_vbi_src_common_need_size(
            self.src1_tensor_op.tensor_obj.dtype, self.vbi_control_op.mask_len,
            self.vbi_control_op.total_repeat_times)
        src0_offset_dtype_size = DTYPE_SIZE[self.src0_offset_tensor_op.tensor_obj.dtype]
        src0_offset_extent = Expr(src0_offset_extent_value * src0_offset_dtype_size).get()
        src1_extent_value = self.src1_tensor_op.get_vbi_src1_tensor_need_size(self.vbi_control_op)
        src1_dtype_size = DTYPE_SIZE[self.src1_tensor_op.tensor_obj.dtype]
        src1_extent = Expr(src1_extent_value * src1_dtype_size).get()

        return dst_extent, src0_offset_extent, src1_extent

    @vbi_high_api_decorator
    @new_high_level_api_debug_decorator
    def vbi_regbase_gen_code(self):
        self._gen_vbi_code_vbi_part(
            *self.cal_vbi_extent(),
            [self.dst_tensor_op.blk_stride, self.vbi_control_op.vertical_repeat_times,
             self.vbi_control_op.horizontal_repeat_times, self.vbi_control_op.repeat_mode,
             self.vbi_control_op.vertical_repeat_offset
             ])

    @source_info_decorator(depth=2)
    def vbi_v220_run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.vbi_check_params.check_all(self.tik_instance)
        self.vbi_v220_gen_code()

    @source_info_decorator(depth=2)
    def vbi_run_regbase(self):
        """
        run all_check and regbase code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.vbi_check_params.check_all(self.tik_instance)
        self.vbi_regbase_gen_code()

    @source_info_decorator(depth=2)
    def vbi_v200_run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.mask_o = self.vbi_check_params.check_all(self.tik_instance)
        self.vbi_v200_gen_code()

    def _gen_vbi_code_params_double_buffer(self, vgatherb_tensor, hp):
        """
        gen params with double_buffer
        """
        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask_all = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        repeat = self.vbi_control_op.horizontal_repeat_times * hp
        vgatherb_tensor_1 = vgatherb_tensor
        h_rep_len = eight_blk_size * self.vbi_control_op.horizontal_repeat_times * hp
        vgatherb_tensor_2 = self.tik_instance.Tensor(self.dst_tensor_op.tensor_obj.dtype, (h_rep_len,),
                                                     name="vbi_work_tensor_3", scope=scope_ubuf)
        vmul_tensor_2 = self.tik_instance.Tensor(self.dst_tensor_op.tensor_obj.dtype, (h_rep_len,),
                                                 name="vbi_work_tensor_4", scope=scope_ubuf)
        params_list = (eight_blk_size, one_blk_size, mask_all, repeat, vgatherb_tensor_1,
                       vgatherb_tensor_2, vmul_tensor_2)
        return params_list

    def _gen_vbi_code_not_double_buffer(self, tensor_list, start_v_rep, rest_v_rep):
        vgatherb_tensor, vmul_tensor = tensor_list
        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask_all = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        src0_offset = self.src0_offset_tensor_op.tensor_obj[start_v_rep *
                                                            self.vbi_control_op.horizontal_repeat_times * 8:]
        self.tik_instance.vgatherb(vgatherb_tensor, self.src0_tensor_op.tensor_obj,
                                   src0_offset, self.vbi_control_op.horizontal_repeat_times * rest_v_rep, 1, 8)
        self.tik_instance.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
        with self.tik_instance.for_range(0, rest_v_rep * self.vbi_control_op.horizontal_repeat_times) as i:
            scalar = self.tik_instance.Scalar(self.dst_tensor_op.tensor_obj.dtype)
            scalar.set_as(self.src1_tensor_op.tensor_obj[start_v_rep *
                                                         self.vbi_control_op.horizontal_repeat_times + i])
            self.tik_instance.vmuls(mask_all, vmul_tensor[eight_blk_size * i:],
                                    vgatherb_tensor[eight_blk_size * i:], scalar, 1, 1, 1, 8, 8)
        self.inject_sync(one_blk_size)
        with self.tik_instance.for_range(0, rest_v_rep) as j:
            self.init_dst_and_inject_sync(start_v_rep + j, one_blk_size)
            dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * (start_v_rep + j):]
            self.tik_instance.vadd(self.vbi_control_op.mask, dst,
                                   vmul_tensor[eight_blk_size * self.vbi_control_op.horizontal_repeat_times * j:],
                                   dst, self.vbi_control_op.horizontal_repeat_times,
                                   dst_blk_stride=self.dst_tensor_op.blk_stride, src0_blk_stride=1,
                                   src1_blk_stride=self.dst_tensor_op.blk_stride,
                                   dst_rep_stride=0, src0_rep_stride=8, src1_rep_stride=0, stride_unit=0)

    def _gen_vbi_code_double_buffer(self, tensor_list, start_v_rep, rest_v_rep):
        vgatherb_tensor, vmul_tensor = tensor_list
        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask_all = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        with self.tik_instance.for_range(0, rest_v_rep) as i:
            src0_offset = self.src0_offset_tensor_op.tensor_obj[(start_v_rep + i) *
                                                                self.vbi_control_op.horizontal_repeat_times * 8:]
            self.tik_instance.vgatherb(vgatherb_tensor, self.src0_tensor_op.tensor_obj, src0_offset,
                                       self.vbi_control_op.horizontal_repeat_times, 1, 8)
            self.tik_instance.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
            with self.tik_instance.for_range(0, self.vbi_control_op.horizontal_repeat_times) as j:
                scalar = self.tik_instance.Scalar(self.dst_tensor_op.tensor_obj.dtype)
                scalar.set_as(self.src1_tensor_op.tensor_obj[(start_v_rep + i) *
                                                             self.vbi_control_op.horizontal_repeat_times + j])
                self.tik_instance.vmuls(mask_all, vmul_tensor[eight_blk_size * j:],
                                        vgatherb_tensor[eight_blk_size * j:], scalar, 1, 1, 1, 8, 8)
            self.inject_sync(one_blk_size)
            self.init_dst_and_inject_sync(start_v_rep + i, one_blk_size)
            dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * (start_v_rep + i):]
            self.tik_instance.vadd(self.vbi_control_op.mask, dst, vmul_tensor, dst,
                                   self.vbi_control_op.horizontal_repeat_times,
                                   dst_blk_stride=self.dst_tensor_op.blk_stride,
                                   src0_blk_stride=1, src1_blk_stride=self.dst_tensor_op.blk_stride,
                                   dst_rep_stride=0, src0_rep_stride=8, src1_rep_stride=0, stride_unit=0)

    def _gen_vbi_code_repeat_mode_one(self, tensor_list, start_v_rep, rest_v_rep):
        vgatherb_tensor, vmul_tensor = tensor_list
        eight_blk_size = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        one_blk_size = ONE_BLK_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        mask_all = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        src0_offset = self.src0_offset_tensor_op.tensor_obj[start_v_rep *
                                                            self.vbi_control_op.horizontal_repeat_times * 8:]
        self.tik_instance.vgatherb(vgatherb_tensor, self.src0_tensor_op.tensor_obj,
                                   src0_offset, self.vbi_control_op.horizontal_repeat_times * rest_v_rep, 1, 8)
        self.tik_instance.vbcb(vmul_tensor,
                               self.src1_tensor_op.tensor_obj[start_v_rep *
                                                              self.vbi_control_op.horizontal_repeat_times * 8:],
                               self.vbi_control_op.horizontal_repeat_times * rest_v_rep, 1, 8)
        self.tik_instance.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
        self.tik_instance.vmul(mask_all, vmul_tensor, vgatherb_tensor, vmul_tensor,
                               repeat_times=self.vbi_control_op.horizontal_repeat_times * rest_v_rep,
                               dst_blk_stride=1, src0_blk_stride=1, src1_blk_stride=1,
                               dst_rep_stride=8, src0_rep_stride=8, src1_rep_stride=8)
        self.inject_sync(one_blk_size)
        with self.tik_instance.for_range(0, rest_v_rep) as j:
            self.init_dst_and_inject_sync(start_v_rep + j, one_blk_size)
            dst = self.dst_tensor_op.tensor_obj[self.vbi_control_op.vertical_repeat_offset * (start_v_rep + j):]
            self.tik_instance.vadd(self.vbi_control_op.mask, dst,
                                   vmul_tensor[eight_blk_size * self.vbi_control_op.horizontal_repeat_times * j:],
                                   dst, self.vbi_control_op.horizontal_repeat_times,
                                   dst_blk_stride=self.dst_tensor_op.blk_stride, src0_blk_stride=1,
                                   src1_blk_stride=self.dst_tensor_op.blk_stride,
                                   dst_rep_stride=0, src0_rep_stride=8, src1_rep_stride=0)

    def _need_src0_block_int(self, base_addr, need_src0_block, elements_per_rep, max_repeat_times):
        total_repeats = need_src0_block // elements_per_rep
        vadds_repeat_batch = total_repeats // max_repeat_times
        vadds_repeat_tail = total_repeats % max_repeat_times
        vadds_tail_ele = need_src0_block % elements_per_rep
        if vadds_repeat_batch > 0:
            with self.tik_instance.for_range(0, vadds_repeat_batch) as vadds_sub_i:
                offset = vadds_sub_i * max_repeat_times * elements_per_rep
                self.tik_instance.vadds(elements_per_rep, self.src0_offset_tensor_op.tensor_obj[offset:],
                                        self.src0_offset_tensor_op.tensor_obj[offset:],
                                        base_addr, max_repeat_times, MIN_STRIDE, MIN_STRIDE,
                                        BLK_NUM_PER_REP, BLK_NUM_PER_REP)
        if vadds_repeat_tail > 0:
            offset = vadds_repeat_batch * max_repeat_times * elements_per_rep
            self.tik_instance.vadds(elements_per_rep,
                                    self.src0_offset_tensor_op.tensor_obj[offset:],
                                    self.src0_offset_tensor_op.tensor_obj[offset:],
                                    base_addr, vadds_repeat_tail, MIN_STRIDE, MIN_STRIDE,
                                    BLK_NUM_PER_REP, BLK_NUM_PER_REP)
        if vadds_tail_ele > 0:
            offset = total_repeats * elements_per_rep
            self.tik_instance.vadds(vadds_tail_ele, self.src0_offset_tensor_op.tensor_obj[offset:],
                                    self.src0_offset_tensor_op.tensor_obj[offset:],
                                    base_addr, MIN_REPEAT_TIMES, MIN_STRIDE, MIN_STRIDE,
                                    BLK_NUM_PER_REP, BLK_NUM_PER_REP)

    def _need_src0_block_not_int(self, base_addr, need_src0_block):
        elements_per_rep = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.src0_offset_tensor_op.tensor_obj.dtype]
        total_repeats = need_src0_block // elements_per_rep
        vadds_repeat_batch = total_repeats // MAX_REPEAT_TIMES
        vadds_repeat_tail = total_repeats % MAX_REPEAT_TIMES
        vadds_tail_ele = need_src0_block % elements_per_rep
        with self.tik_instance.for_range(0, vadds_repeat_batch) as vadds_sub_i:
            offset = vadds_sub_i * MAX_REPEAT_TIMES * elements_per_rep
            self.tik_instance.vadds(elements_per_rep, self.src0_offset_tensor_op.tensor_obj[offset:],
                                    self.src0_offset_tensor_op.tensor_obj[offset:],
                                    base_addr, MAX_REPEAT_TIMES, MIN_STRIDE, MIN_STRIDE,
                                    BLK_NUM_PER_REP, BLK_NUM_PER_REP)
        with self.tik_instance.if_scope(vadds_repeat_tail > 0):
            offset = vadds_repeat_batch * MAX_REPEAT_TIMES * elements_per_rep
            self.tik_instance.vadds(elements_per_rep, self.src0_offset_tensor_op.tensor_obj[offset:],
                                    self.src0_offset_tensor_op.tensor_obj[offset:],
                                    base_addr, vadds_repeat_tail, MIN_STRIDE, MIN_STRIDE,
                                    BLK_NUM_PER_REP, BLK_NUM_PER_REP)
        with self.tik_instance.if_scope(vadds_tail_ele > 0):
            offset = total_repeats * elements_per_rep
            self.tik_instance.vadds(vadds_tail_ele, self.src0_offset_tensor_op.tensor_obj[offset:],
                                    self.src0_offset_tensor_op.tensor_obj[offset:],
                                    base_addr, MIN_REPEAT_TIMES, MIN_STRIDE, MIN_STRIDE,
                                    BLK_NUM_PER_REP, BLK_NUM_PER_REP)

    def _gen_vbi_code_vadds_part(self):
        """
        for vbi instruction, generate vadds part code
        """
        with self.tik_instance.context.freeze():
            base_addr = self.tik_instance.scalar_("int32")
            base_addr.set_as(tvm.tir.Cast("int64", tvm.call_extern(
                "handle", "", self.src0_tensor_op.tensor_obj.access_ptr("r"))).astype("int32"))
            need_src0_block = self.dst_tensor_op.get_vbi_src_common_need_size(
                self.dst_tensor_op.tensor_obj.dtype, self.vbi_control_op.mask_len,
                self.vbi_control_op.total_repeat_times)
            # gen vadds with mask counter mode after fix mask counter mode
            elements_per_rep = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.src0_offset_tensor_op.tensor_obj.dtype]
            if isinstance(need_src0_block, int):
                self._need_src0_block_int(base_addr, need_src0_block, elements_per_rep, MAX_REPEAT_TIMES)
            else:
                self._need_src0_block_not_int(base_addr, need_src0_block)

    def _gen_vbi_code_vbi_part(self, dst_extent, src0_offset_extent, src1_extent, config):
        """
        for vbi instruction, generate vbi part code
        """
        extent_value = reduce_mul(self.src0_tensor_op.tensor_obj.original_shape) - self.src0_tensor_op.tensor_obj.offset
        dtype_size = DTYPE_SIZE[self.src0_tensor_op.tensor_obj.dtype]
        src0_extent = Expr(extent_value * dtype_size).get()
        with self.tik_instance.new_scope():
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
                src0_offset_ptr = self.src0_offset_tensor_op.tensor_obj.access_ptr("r", extent=src0_offset_extent)
                src0_ptr = self.src0_tensor_op.tensor_obj.access_ptr("r", extent=src0_extent)
                mask = self.vbi_control_op.mask
                if not isinstance(mask, list):
                    mask = [mask]
                mask = [Expr(m, "uint64").get() for m in mask]
                instr = tvm.call_extern(
                    self.dst_tensor_op.tensor_obj.dtype, "vbi",
                    self.dst_tensor_op.tensor_obj.access_ptr("w", extent=dst_extent),
                    src0_ptr,
                    self.src1_tensor_op.tensor_obj.access_ptr("r", extent=src1_extent),
                    src0_offset_ptr,
                    *type_convert(config), *mask
                )
            else:
                self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *self.mask_o))
                src0_offset_ptr = self.src0_offset_tensor_op.tensor_obj.reinterpret_cast_to("uint16").access_ptr(
                    "r", extent=src0_offset_extent)
                src0_ptr = self.src0_tensor_op.tensor_obj.access_ptr_vbi_src0("r", extent=src0_extent)
                instr = tvm.call_extern(
                    self.dst_tensor_op.tensor_obj.dtype, "vbi",
                    self.dst_tensor_op.tensor_obj.access_ptr("w", extent=dst_extent),
                    src0_offset_ptr,
                    self.src1_tensor_op.tensor_obj.access_ptr("r", extent=src1_extent), *type_convert(config),
                    src0_ptr)
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)
