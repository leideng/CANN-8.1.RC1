#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_params_check_compare.py
DESC:     provide params
CREATED:  2021-11-5 18:53:42
MODIFIED: 2021-11-8 19:17:00
"""
from collections import namedtuple
from tbe.common.platform import intrinsic_check_support
from tbe.tik.common.util import get_bit_len
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import MAX_SRC1_PATTERN_V2
from tbe.tik.tik_lib.tik_params import MIN_MODE_NUMBER
from tbe.tik.tik_lib.tik_params import MAX_VSEL_MODE
from tbe.tik.tik_lib.tik_params import ONE_REP_BYTE_SIZE
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_api_util import check_repeat_times
from tbe.tik.tik_lib.tik_api_util import check_stride_unit
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_vector_api.tik_vector_name_map import VCMP_NAME_DICT
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.common.common_util import vec_template_align
from tbe.tik.common.common_util import int64_support_check
from tbe.common.platform import scope_ubuf
from tbe.common.platform.platform_info import api_check_support
from tbe.tik.common.common_util import check_address_align
from tbe.tik.common.common_check_func import check_sel_overflow
from tbe.tik.common.common_util import get_blk_valid_list
from tbe.tik.common.common_check_func import check_overlap_param
from tbe.tik.common.common_check_func import concate_deqscale_vconv
from tbe.tik.tik_lib.tik_params import CMPMASK_VAR
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.common.util import TikUtil
from tbe.tik.common.util import is_basic_expr
from tbe.tik.common.util import get_mask_len
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.tik_lib.tik_vector_api.vector_common_util import gen_block_list
from tbe.tik.tik_lib.tik_reduce_api_v2_ import vreduce_tensor_check
from tbe.tik.tik_lib.tik_reduce_api_v2_ import vreduce_param_check
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.common.tik_vec_util import get_extents
from tbe.tik.common.tik_vec_util import check_vreduce_overflow
from tbe.tik.common.tik_get_soc_name import get_rep_size
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_api_constants import ROUND_MODE_MAP
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_vector_api.tik_vector_name_map import SINGLE_NAME_DICT
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_basic_data import BasicData
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp
from tbe.tik.tik_lib.tik_check_util import print_error_msg
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.common.tik_get_soc_name import get_soc_core_type
from tbe.tik.common.tik_api_map import int64_support_map
from tbe.tik.common.tik_api_map import ASCEND_910_93


class VselCheckParams:
    """
    Single Params Check
    """

    def __init__(self, print_name, params_list1, params_list2, name):
        self.print_name = print_name
        self.name = name
        self.dst_tensor_op, self.src0_tensor_op, self.src1_tensor_op, self.control_op = params_list1
        self.mode, self.mask_o, self.sel = params_list2

    @staticmethod
    def gen_block_list(tensor_bit_len_max, dtype):
        """
        gen block list for vsel api

        Parameters
        ----------
        tensor_bit_len_max: tensor bit_len max
        dtype: dtype

        Returns
        -------
        block_list: [nblock, block_len], nblock:blocks in repeat, block_len:elements in block, mask=nblock*block_len
        """
        one_rep_size = get_rep_size()
        parallelism = one_rep_size * ONE_BYTE_BIT_LEN // tensor_bit_len_max
        block_len = one_rep_size // get_bit_len(dtype)
        block_list = [parallelism // block_len, block_len]
        return block_list

    def set_context_value(self, context):
        """
        set if debug value

        Parameters
        ----------
        context : the context

        Returns
        -------
        None
        """
        self.dst_tensor_op.set_context(context)
        self.src0_tensor_op.set_context(context)
        self.dst_tensor_op.set_stride_unit(0)
        self.dst_tensor_op.get_mask(self.control_op.mask)
        self.dst_tensor_op.set_repeat_times(self.control_op.repeat_times)

    def check_vsel_mode(self, isdebug):
        """
        check vsel mode

        Parameters
        ----------
        isdebug : if in the debug

        Returns
        -------
        None
        """
        if not isdebug:
            TikCheckUtil.check_type_match(self.mode, int, "mode should be int.")
            TikCheckUtil.check_in_range_by_dtype(
                self.mode, msg="mode should be in the range of [%d, %d]" % (MIN_MODE_NUMBER, MAX_VSEL_MODE),
                var_range=[MIN_MODE_NUMBER, MAX_VSEL_MODE])
            if TikSocManager.is_v100_soc():
                TikCheckUtil.check_equality(self.mode, 0, "v100 doesn't support mode %s." % self.mode)

    def check_vsel_repeat_times(self, isdebug):
        """
        check vsel repeat times

        Parameters
        ----------
        isdebug : if in the debug

        Returns
        -------
        None
        """
        if not isdebug:
            check_repeat_times(self.control_op.repeat_times)

    def check_src1_type(self, align, isdebug):
        """
        check src1 type

        Parameters
        ----------
        align : align
        isdebug : if in the debug

        Returns
        -------
        None
        """
        if not isdebug:
            if self.mode == 1:
                TikCheckUtil.check_type_match(
                    self.src1_tensor_op.tensor_obj, (int, float, Scalar),
                    "when mode is 1, src1 should be int, float or Scalar")
            else:
                TikCheckUtil.check_type_match(self.src1_tensor_op.tensor_obj, Tensor,
                                              "when mode isn't 1, src1 should be tensor")
                TikCheckUtil.check_equality(self.src1_tensor_op.tensor_obj.scope, scope_ubuf, "src1's scope must be UB")
                # check UB address 32B align
                check_address_align((self.src1_tensor_op.tensor_obj,), ("src1",), align)

    def check_vsel_sel(self):
        """
        check vsel sel

        Returns
        -------
        None
        """
        if self.mode in (1, 2):
            TikCheckUtil.check_type_match(
                self.sel, Tensor, "when mode is 1 or 2, sel should be tensor")
            TikCheckUtil.check_equality(self.sel.scope, scope_ubuf,
                                        "sel's scope must be UB")
            TikCheckUtil.check_var_in_list(
                self.sel.dtype, ["uint8", "uint16", "uint32", "uint64"],
                "when mode is 1 or 2, sel should be uint8, uint16, uint32 or "
                "when mode is 1 or 2, sel should be uint8, uint16, uint32 or "
                "uint64")
            # check sel overflow
            check_sel_overflow(self.dst_tensor_op.tensor_obj, self.src0_tensor_op.tensor_obj, self.sel,
                               self.control_op.mask, self.control_op.repeat_times)
        else:
            TikCheckUtil.check_is(self.sel, CMPMASK_VAR, "Please assure sel is cmpmask.")

    def check_vsel_dtype_support(self):
        """
        check vsel dtype support

        Returns
        -------
        None
        """
        dtype = self.dst_tensor_op.tensor_obj.dtype
        if dtype == "int64":
            TikCheckUtil.check_equality(int64_support_check("tik." + self.name, dtype), True,
                                        gen_api_check_statement(dtype, self.print_name))
        else:
            TikCheckUtil.check_equality(self.dst_tensor_op.tensor_obj.dtype, self.src0_tensor_op.tensor_obj.dtype,
                                        "Instruction %s's src0's dtype should be equal to dst's dtype" % self.name)
            if isinstance(self.src1_tensor_op.tensor_obj, (Scalar, Tensor)):
                TikCheckUtil.check_equality(self.dst_tensor_op.tensor_obj.dtype, self.src1_tensor_op.tensor_obj.dtype,
                                            "Instruction %s's src1's dtype should be equal to dst's dtype" % self.name)
            TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_vsel",
                                                                self.dst_tensor_op.tensor_obj.dtype), True,
                                        gen_api_check_statement(self.dst_tensor_op.tensor_obj.dtype, self.print_name))

    def check_vesl_addr_overlap(self, tensor_bit_len_max2, isdebug):
        """
        check vesl overlap

        Parameters
        ----------
        tensor_bit_len_max2 : max tensor bit len
        isdebug : if in the debug

        Returns
        -------
        None
        """
        if self.src0_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:

            check_tuple = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride, self.dst_tensor_op.rep_stride,
                           self.src0_tensor_op.blk_stride, self.src0_tensor_op.rep_stride)

            if all(isinstance(value, int) for value in check_tuple):
                block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len_max2]
                self.dst_tensor_op.check_address_overlapping(self.print_name, self.control_op,
                                                             self.src0_tensor_op, block_list)

        if self.mode in (0, 2) and self.src1_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
            check_tuple = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride, self.dst_tensor_op.rep_stride,
                           self.src0_tensor_op.blk_stride, self.src0_tensor_op.rep_stride)
            if all(isinstance(value, int) for value in check_tuple):
                tensor_bit_len_max3 = max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                          get_bit_len(self.src1_tensor_op.tensor_obj.dtype))
                block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len_max3]
                self.dst_tensor_op.check_address_overlapping(self.print_name, self.control_op,
                                                             self.src1_tensor_op, block_list)
        if self.mode in (1, 2) and self.dst_tensor_op.tensor_obj.buffer == self.sel.buffer:
            value_range = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride, self.dst_tensor_op.rep_stride)
            if all(isinstance(value, int) for value in value_range) or isdebug:
                self.check_sel_dst_overlap(tensor_bit_len_max2)

    def check_sel_dst_overlap(self, tensor_bit_len_max2):
        """
        check vesl sel overlap

        Parameters
        ----------
        tensor_bit_len_max2 : max tensor bit len

        Returns
        -------
        None
        """
        mask = self.dst_tensor_op.mask_value
        repeat = self.dst_tensor_op.repeat_times_value
        sel_offset = Expr(self.sel.offset).eval_value()

        if repeat <= 0:
            return
        if self.dst_tensor_op.offset_value is None or sel_offset is None:
            return
        parallelism = ONE_REP_BYTE_SIZE * ONE_BYTE_BIT_LEN // tensor_bit_len_max2
        sel_num_each_repeat = parallelism // get_bit_len(self.sel.dtype)
        mask_list = TikUtil.to_list(mask)
        if not is_basic_expr(mask_list):
            if repeat == 1:
                self._check_single_it_sel_dst(mask_list, sel_offset)
            else:
                for time in range(repeat - 1):
                    self._check_addr_overlap(time, mask_list, sel_num_each_repeat, sel_offset)

    def check_vsel_overflow(self, tensor_bit_len_max2, isdebug):
        """
        check vesl overflow

        Parameters
        ----------
        tensor_bit_len_max2 : max tensor bit len
        isdebug : in in the debug

        Returns
        -------
        None
        """
        if not isdebug:

            if self.mode in (0, 2):
                tensor_bit_len_max1 = max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                          get_bit_len(self.src0_tensor_op.tensor_obj.dtype),
                                          get_bit_len(self.src1_tensor_op.tensor_obj.dtype))
                block_list_src1 = self.gen_block_list(tensor_bit_len_max1, self.src1_tensor_op.tensor_obj.dtype)
                block_list_dst = self.gen_block_list(tensor_bit_len_max1, self.dst_tensor_op.tensor_obj.dtype)
                block_list_src0 = self.gen_block_list(tensor_bit_len_max1, self.src0_tensor_op.tensor_obj.dtype)
                self.dst_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_dst)
                self.src0_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_src0)
                self.src1_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_src1)
            else:
                block_list_dst = self.gen_block_list(tensor_bit_len_max2, self.dst_tensor_op.tensor_obj.dtype)
                block_list_src0 = self.gen_block_list(tensor_bit_len_max2, self.src0_tensor_op.tensor_obj.dtype)
                self.dst_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_dst)
                self.src0_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_src0)

    def check_all(self, tik_instance=None, context=None, isdebug=False):
        """
        check every param

        Parameters
        ----------
        tik_instance : tik
        context : the context
        isdebug : if in in the debug

        Returns
        -------
        mask_o
        """
        # check mode
        self.check_vsel_mode(isdebug)
        # check repeat_times
        self.check_vsel_repeat_times(isdebug)
        # check tensor's tensor, scope, address, rep stride and blk stride
        align = vec_template_align(self.src0_tensor_op.tensor_obj.dtype)
        self.dst_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.set_rep_stride_value()
        self.src0_tensor_op.set_blk_stride_value()
        self.src0_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_SINGLE_BYTE,
                                                 MAX_REP_STRIDE_SINGLE_BYTE, align)
        self.src0_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_SINGLE_BYTE,
                                                  MAX_REP_STRIDE_SINGLE_BYTE, align)
        # check src1 blk_stride rep_stride
        self.src1_tensor_op.set_rep_stride_value()
        self.src1_tensor_op.set_blk_stride_value()
        self.src1_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_SINGLE_BYTE)
        self.src1_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)
        # check src1 type
        self.check_src1_type(align, isdebug)
        # check vsel dtype
        if not isdebug:
            self.check_vsel_dtype_support()
        # set if in debug value
        self.set_context_value(context)
        # check overflow
        tensor_bit_len_max2 = max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                  get_bit_len(self.src0_tensor_op.tensor_obj.dtype))

        self.check_vsel_overflow(tensor_bit_len_max2, isdebug)
        # check sel and sel overflow
        if not isdebug:
            self.check_vsel_sel()
        # gen mask_o
        self.mask_o = self.control_op.check_and_gen_mask_o(tik_instance, tensor_bit_len_max2, self.mask_o)
        # check overlap
        self.check_vesl_addr_overlap(tensor_bit_len_max2, isdebug)
        return self.mask_o

    def _check_single_it_sel_dst(self, mask_list, sel_offset):
        """
        check single sel dst

        Parameters
        ----------
        mask_list : list of mask
        sel_offset : sel's offset

        Returns
        -------
        None
        """
        if len(mask_list) == 1:
            mask_len = mask_list[0]
        else:
            mask_len = get_mask_len(mask_list)
        sel_extend = ceil_div(mask_len, get_bit_len(self.sel.dtype))
        dst_extend = self.dst_tensor_op.vector_max_offset_cal(self.dst_tensor_op.mask_value,
                                                              self.control_op, self.dst_tensor_op.repeat_times_value)
        sel_need_offset = Expr(sel_offset + sel_extend).eval_value()
        dst_need_offset = Expr(self.dst_tensor_op.offset_value + dst_extend).eval_value()
        if sel_offset and dst_need_offset is not None:
            expression1 = self.dst_tensor_op.offset_value == sel_offset and self.dst_tensor_op.blk_stride_value == 1
            expression2 = sel_offset <= sel_need_offset <= self.dst_tensor_op.offset_value
            expression3 = self.dst_tensor_op.offset_value <= dst_need_offset <= sel_offset
            if not (expression1 or expression2 or expression3):
                TikCheckUtil.raise_error(
                    "when repeat_times=1, vsel dst and sel not support partially address overlapping.")

    def _check_addr_overlap(self, time, mask_list, sel_num_each_repeat, sel_offset):
        """
        check vesl overlap

        Parameters
        ----------
        time : the count number
        mask_list : list of mask
        sel_num_each_repeat : sel's num of each repeat
        sel_offset : sel's offset

        Returns
        -------
        None
        """
        dst_block_len = ONE_REP_BYTE_SIZE // get_bit_len(self.dst_tensor_op.tensor_obj.dtype)

        dst_blk_valid_list = get_blk_valid_list(mask_list, self.dst_tensor_op.tensor_obj.dtype, dst_block_len)
        dst_extend_interval = self.dst_tensor_op.get_extend_interval(dst_blk_valid_list, time, self.dst_tensor_op)
        sel_begin = (time + 1) * sel_num_each_repeat + sel_offset
        interval_sel = [sel_begin * DTYPE_SIZE[self.sel.dtype],
                        (sel_begin + sel_num_each_repeat) * DTYPE_SIZE[self.sel.dtype]]
        # check
        for interval_dst in dst_extend_interval:
            if max(interval_sel[0], interval_dst[0]) < min(interval_sel[1], interval_dst[1]):
                TikCheckUtil.raise_error("When repeat_times>1, vsel dst and sel address overlapping error. It"
                                         " is not support iteration N's destination is the source of next iteration")


class VreduceCheckParams:
    """
    Single Params Check
    """
    get_extents_list = namedtuple('GetExtentsList', ["mask", "mask_mode", "dst", "src0", "src1_pattern",
                                                     "repeat_times", "src0_blk_stride", "src0_rep_stride",
                                                     "src1_rep_stride", "stride_unit", "src1_original_shape",
                                                     "src1_offset"])

    def __init__(self, print_name, params_list, rsvd_scalar):
        self.print_name = print_name
        self.name = VCMP_NAME_DICT.get(self.print_name)
        self.dst_tensor_op, self.src0_tensor_op, self.src1_tensor_op, self.control_op = params_list
        self.rsvd_scalar = rsvd_scalar

    def set_context_value(self, context):
        """
        set if debug value

        Parameters
        ----------
        context : the context

        Returns
        -------
        None
        """
        self.dst_tensor_op.set_vreduce_context(context)
        self.src0_tensor_op.set_vreduce_context(context)
        if isinstance(self.src1_tensor_op.tensor_obj, Tensor):
            self.src1_tensor_op.set_vreduce_context(context)
        else:
            self.src1_tensor_op.context = context
            self.src1_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.set_stride_unit(self.control_op.stride_unit)
        self.control_op.eval_mask(context)
        self.dst_tensor_op.set_repeat_times(self.control_op.repeat_times)
        if context and not isinstance(self.src1_tensor_op.tensor_obj, Tensor):
            self.src1_tensor_op.src1_pattern = context.evaluate_expr(self.src1_tensor_op.tensor_obj)
        else:
            self.src1_tensor_op.src1_pattern = self.src1_tensor_op.tensor_obj

    def cal_mask_and_rep_bytes(self, mask, nblock, repeat):
        """
        check and gen mask, repeat, one_rep_bytes

        Parameters
        ----------
        mask : mask
        nblock : block times
        repeat : repeat number

        Returns
        -------
        mask, repeat, one_rep_bytes
        """
        block_len = ONE_REP_BYTE_SIZE // get_bit_len(self.dst_tensor_op.tensor_obj.dtype)
        if self.control_op.mask_mode == "counter":
            if not self.print_name == "vreducev2":
                repeat = ceil_div(mask, nblock * block_len)
                mask = mask % (nblock * block_len)
                if mask == 0:
                    mask = nblock * block_len
                one_rep_bytes = ONE_REP_BYTE_SIZE
            else:
                one_rep_bytes = ceil_div(mask * DTYPE_SIZE[
                    self.src0_tensor_op.tensor_obj.dtype], ONE_BLK_SIZE) * ONE_BLK_SIZE
        else:
            # normal mode, mask is full
            mask = ONE_REP_BYTE_SIZE // DTYPE_SIZE[self.src0_tensor_op.tensor_obj.dtype]
            one_rep_bytes = ONE_REP_BYTE_SIZE
        return mask, repeat, one_rep_bytes

    def check_address_overlap_vreduce(self, dst_extent, src0_extent, isdebug):
        """
        check address voverlap

        Parameters
        ----------
        dst_extent : dst's extend
        src0_extent : src0's extend

        Returns
        -------
        None
        """
        if not check_overlap_param(self.control_op.eval_mask_value, self.dst_tensor_op.repeat_times_value,
                                   self.dst_tensor_op.tensor_offset_value,
                                   self.src0_tensor_op.tensor_offset_value):
            return
        if src0_extent is None or dst_extent is None:
            return
        src_need_offset = Expr(self.src0_tensor_op.tensor_offset_value + src0_extent).eval_value()
        dst_need_offset = Expr(self.dst_tensor_op.tensor_offset_value + dst_extent).eval_value()
        mask, repeat, one_rep_bytes = self.cal_mask_and_rep_bytes(self.control_op.eval_mask_value, BLK_NUM_PER_REP,
                                                                  self.dst_tensor_op.repeat_times_value)
        if repeat == 1:
            expression1 = self.src0_tensor_op.tensor_offset_value == self.dst_tensor_op.tensor_offset_value and \
                          self.src0_tensor_op.blk_stride_value >= 1
            expression2 = self.src0_tensor_op.tensor_offset_value <= src_need_offset <= \
                          self.dst_tensor_op.tensor_offset_value
            expression3 = self.dst_tensor_op.tensor_offset_value <= dst_need_offset <= \
                          self.src0_tensor_op.tensor_offset_value
            if not (expression1 or expression2 or expression3):
                TikCheckUtil.raise_error(
                    "When repeat_times=1, %s's dst and src0 address overlapping error." % self.print_name)
        else:
            self._check_many_it_vreduce(mask, repeat, one_rep_bytes, isdebug)

    def check_tensor_list(self, isdebug):
        """
        check tensor list
        Parameters
        ----------
        isdebug: is debug

        Returns
        -------

        """
        tensor_check_lit = [self.control_op.mask, self.dst_tensor_op.tensor_obj, self.src0_tensor_op.tensor_obj,
                            self.src1_tensor_op.tensor_obj, self.src1_tensor_op.rep_stride, self.rsvd_scalar,
                            self.control_op.mask_mode]
        if not isdebug and self.print_name == "vreduce":
            vreduce_tensor_check(self.print_name, tensor_check_lit)
        elif not isdebug and self.print_name == "vreducev2":
            vreduce_tensor_check(self.print_name, tensor_check_lit, True)

    def check_all(self, isdebug=False, context=None):
        """
        check all input params

        Parameters
        ----------
        isdebug : if in debug mode
        context : if is debug's context

        Returns
        -------
        None
        """
        self.check_tensor_list(isdebug)
        # set context value
        self.set_context_value(context)
        param_check_list = [self.control_op.eval_mask_value, self.src1_tensor_op.tensor_obj,
                            self.dst_tensor_op.repeat_times_value, self.src0_tensor_op.blk_stride_value,
                            self.src0_tensor_op.rep_stride_value, self.src1_tensor_op.rep_stride_value,
                            self.control_op.mask_mode, self.print_name]
        if self.print_name == "vreduce":
            is_910b = False
            vreduce_param_check(param_check_list, isdebug=isdebug)
        elif self.print_name == "vreducev2":
            is_910b = True
            vreduce_param_check(param_check_list, MAX_REPEAT_TIMES_DOUBLE_BYTE,
                                MAX_REP_STRIDE_DOUBLE_BYTE, MAX_SRC1_PATTERN_V2, isdebug)
        if isinstance(self.src1_tensor_op.tensor_obj, Tensor):
            pattern = 0
            self.src1_tensor_op.set_original_shape()
            src1_original_shape = self.src1_tensor_op.original_shape_value
            src1_offset = self.src1_tensor_op.tensor_offset_value
        else:
            pattern = self.src1_tensor_op.src1_pattern
            src1_original_shape = 0
            src1_offset = 0
        check_stride_unit(self.control_op.stride_unit)
        # gen dst_extent, src0_extent, src1_extent
        get_extents_list = VreduceCheckParams.get_extents_list(
            self.control_op.eval_mask_value, self.control_op.mask_mode,
            self.dst_tensor_op.tensor_obj, self.src0_tensor_op.tensor_obj,
            self.src1_tensor_op.src1_pattern, self.dst_tensor_op.repeat_times_value,
            self.src0_tensor_op.blk_stride_value, self.src0_tensor_op.rep_stride_value,
            self.src1_tensor_op.rep_stride_value, self.dst_tensor_op.stride_unit_value,
            src1_original_shape, src1_offset)
        dst_extent, src0_extent, src1_extent = get_extents(get_extents_list, is_910b)

        # check vreduce overlap
        if self.src0_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
            value_range = (self.control_op.repeat_times, self.src1_tensor_op.tensor_obj,
                           self.src0_tensor_op.blk_stride, self.src0_tensor_op.rep_stride)
            if isdebug or all(isinstance(value, int) for value in value_range):
                self.check_address_overlap_vreduce(dst_extent, src0_extent, isdebug)
        # check vreduce overflow
        input_params = [self.control_op.eval_mask_value, self.dst_tensor_op.tensor_obj,
                        self.src0_tensor_op.tensor_obj, self.src1_tensor_op.src1_pattern,
                        self.dst_tensor_op.repeat_times_value, self.src0_tensor_op.blk_stride_value,
                        self.src0_tensor_op.rep_stride_value, self.src1_tensor_op.rep_stride_value,
                        self.dst_tensor_op.stride_unit_value, self.control_op.mask_mode,
                        self.dst_tensor_op.original_shape_value, self.dst_tensor_op.tensor_offset_value,
                        self.src0_tensor_op.original_shape_value, self.src0_tensor_op.tensor_offset_value,
                        src1_original_shape, src1_offset, is_910b]
        check_vreduce_overflow(input_params, dst_extent)

        return [dst_extent, src0_extent, src1_extent, pattern]

    def _check_vreduce_overlap(self, src_extend_interval, time, one_rep_bytes, isdebug):
        """
        check many iteration for vreduce
        """
        # get dst_extend_interval
        # src1_pattern is 1~2, extract odd/even element of src0
        src1_pattern = self.src1_tensor_op.src1_pattern
        dst = self.dst_tensor_op.tensor_obj
        if not isdebug:
            dst_offset = Expr(dst.offset).eval_value()
        else:
            dst_offset = self.dst_tensor_op.tensor_offset_value
        if isinstance(src1_pattern, Tensor):
            begin = time * one_rep_bytes // \
                    DTYPE_SIZE[dst.dtype] + dst_offset
            end = begin + one_rep_bytes // DTYPE_SIZE[dst.dtype]
        else:
            if src1_pattern < 3:
                begin = time * ONE_REP_BYTE_SIZE // \
                        DTYPE_SIZE[dst.dtype] // 2 + dst_offset
                end = begin + ONE_REP_BYTE_SIZE // DTYPE_SIZE[dst.dtype] // 2
            # src1_pattern is 3~6, extract 1/4 elements of src0
            elif src1_pattern < 7:
                begin = time * ONE_REP_BYTE_SIZE // \
                        DTYPE_SIZE[dst.dtype] // 4 + dst_offset
                end = begin + ONE_REP_BYTE_SIZE // DTYPE_SIZE[dst.dtype] // 4
            elif src1_pattern == 7:  # src1_pattern is 7, all src0 to dst
                begin = time * ONE_REP_BYTE_SIZE // \
                        DTYPE_SIZE[dst.dtype] + dst_offset
                end = begin + ONE_REP_BYTE_SIZE // DTYPE_SIZE[dst.dtype]
            else:
                begin = time * one_rep_bytes // \
                        DTYPE_SIZE[dst.dtype] + dst_offset
                end = begin + one_rep_bytes // DTYPE_SIZE[dst.dtype]

        interval_dst = [begin * DTYPE_SIZE[dst.dtype], end * DTYPE_SIZE[dst.dtype]]
        # check
        for interval_src in src_extend_interval:
            if max(interval_src[0], interval_dst[0]) < \
                    min(interval_src[1], interval_dst[1]):
                TikCheckUtil.raise_error(
                    "When repeat_times>1, %s dst and src0 address"
                    " overlapping error. It is not support "
                    "iteration N's destination is the source"
                    " of next iteration" % self.print_name)

    def _check_many_it_vreduce(self, mask, repeat, one_rep_bytes, isdebug):
        """
        check voverlap every repeat

        Parameters
        ----------
        mask : mask
        nblock : block times
        one_rep_bytes : one rep bytes size

        Returns
        -------
        None
        """
        if not isinstance(mask, (list, tuple)):
            mask = [mask]
        dst_block_len = ONE_REP_BYTE_SIZE // get_bit_len(self.dst_tensor_op.tensor_obj.dtype)
        src_blk_valid_list = get_blk_valid_list(mask, self.src0_tensor_op.tensor_obj.dtype, dst_block_len)
        src_block_list = [dst_block_len, BLK_NUM_PER_REP]
        for time in range(repeat - 1):
            src_extend_interval = self.src0_tensor_op.get_extend_interval(src_blk_valid_list, time + 1, src_block_list,
                                                                          self.src0_tensor_op.tensor_offset_value)
            # check
            self._check_vreduce_overlap(src_extend_interval, time, one_rep_bytes, isdebug)


class VconvCheckParams:
    """
    Single Params Check
    """

    def __init__(self, print_name, params_list1, params_list2):
        self.print_name = print_name
        self.name = SINGLE_NAME_DICT.get(self.print_name)
        self.dst_tensor_op, self.src_tensor_op, self.deqscale_op, self.control_op = params_list1
        self.round_mode, self.ldst_high_half, self.mask_o = params_list2
        self.dtype_str = ""
        self.dst_vconv_op = None

    def check_half_and_mode(self, isdebug):
        """
        check ldst_high_half and round_mode

        Parameters
        ----------
        isdebug : if in debug mode

        Returns
        -------
        None
        """
        if not isdebug:
            TikCheckUtil.check_type_match(self.ldst_high_half, bool, "ldst_high_half should be bool type.")
            TikCheckUtil.check_var_in_list(self.round_mode, ROUND_MODE_MAP,
                                           "round_mode: %s is not supported" % self.round_mode)

    def check_dtype(self):
        """
        check vconv and vec_conv dtype

        Returns
        -------
        None
        """
        self.dtype_str = "%s2%s%s" % (DTYPE_MAP[self.src_tensor_op.tensor_obj.dtype],
                                      DTYPE_MAP[self.dst_tensor_op.tensor_obj.dtype], ROUND_MODE_MAP[self.round_mode])
        # dtype_str is used as instr, in the nano soc, instr name is vconv_deqs322f16.
        if TikSocManager.is_nano_soc() and self.dtype_str == "s322f16":
            self.dtype_str = "deq" + self.dtype_str
        dtype = self.src_tensor_op.tensor_obj.dtype
        soc_name = get_soc_name()
        core_type = get_soc_core_type()
        soc_total_version = soc_name + core_type
        if soc_name not in ("Ascend910B", ASCEND_910_93) and\
                dtype == "int64" and soc_total_version in int64_support_map:
            TikCheckUtil.check_equality(int64_support_check("tik." + self.name, self.dtype_str), True,
                                        gen_api_check_statement(dtype, self.print_name))
        else:
            dtype_str_err = "src %s dst %s round_mode '%s'" % (self.src_tensor_op.tensor_obj.dtype,
                                                               self.dst_tensor_op.tensor_obj.dtype, self.round_mode)
            TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_vconv", self.dtype_str), True,
                                        gen_api_check_statement(dtype_str_err, self.print_name))

    def check_deqscale(self):
        """
        check deqscale and gen dtyoe_str

        Returns
        -------
        None
        """
        if self.dtype_str.endswith("s322f16"):
            # dtype_str is used as instr, except the nano soc, instr name is vconv_deq
            if not TikSocManager.is_nano_soc():
                self.dtype_str = "deq"
            TikCheckUtil.check_type_match(self.deqscale_op.tensor_obj, (float, Scalar),
                                          "Please specify your deqscale: Imm(float)/Scalar(float16) for deq-mode.")
            if isinstance(self.deqscale_op.tensor_obj, Scalar):
                TikCheckUtil.check_equality(self.deqscale_op.tensor_obj.dtype, "float16", "deqscale should be float16.")
        elif self.dtype_str in ["s162u8", "s162s8"] and not TikSocManager().is_v300_610l_soc():
            if isinstance(self.deqscale_op.tensor_obj, (list, tuple)):
                bit_46 = int(self.dtype_str == "s162s8")
                self.deqscale_op.tensor_obj = concate_deqscale_vconv(self.deqscale_op.tensor_obj, bit_46)
            TikCheckUtil.check_type_match(self.deqscale_op.tensor_obj, (int, Tensor, Scalar, Expr),
                                          "Please specify your deqscale: Tensor(uint64) for "
                                          "vdeq8-mode, Imm(int)/Scalar(uint64)/Expr(uint64) for deq8-mode.")
            if isinstance(self.deqscale_op.tensor_obj, Tensor):
                self.dtype_str = "vdeqs162b8"
                TikCheckUtil.check_equality(self.deqscale_op.tensor_obj.dtype, "uint64", "deqscale should be uint64.")
            else:
                self.dtype_str = "deqs162b8"
                if isinstance(self.deqscale_op.tensor_obj, (Scalar, Expr)):
                    TikCheckUtil.check_equality(self.deqscale_op.tensor_obj.dtype,
                                                "uint64", "deqscale should be uint64.")
        else:
            TikCheckUtil.check_is(self.deqscale_op.tensor_obj, None, "deqscale should be None for current conv")

    def check_vconv_deqs162b8_overlap(self, overlap_expression, src_dst_len_ratio, context):
        """
        check deqscale overlap and gen dst's offset

        Parameters
        ----------
        overlap_expression : overlap expression list
        src_dst_len_ratio : divided by the dtype len of dst and src.
        context : the debug's context

        Returns
        -------
        None
        """
        self.dst_vconv_op.set_context(context)
        if all(isinstance(value, int) for value in overlap_expression):
            dst_original_offset = self.dst_tensor_op.tensor_obj.offset
            dst_offset_work = Expr(self.dst_tensor_op.tensor_obj.offset).eval_value()
            if self.ldst_high_half and dst_offset_work is not None:
                dst_offset_work += ONE_REP_BYTE_SIZE // get_bit_len(self.src_tensor_op.tensor_obj.dtype)
            # the tensor offset may be changed
            self._check_dst_deqscale_src_overlap(dst_offset_work, src_dst_len_ratio)
            # Change offset back to the original value.
            self.dst_vconv_op.tensor_obj.data = dst_original_offset

    def check_vconv_deqs162b8_overflow(self, context):
        """
        check deqscale overflow and check extend

        Parameters
        ----------
        context : the debug's context

        Returns
        -------
        None
        """
        src_block_len = ONE_REP_BYTE_SIZE // get_bit_len(self.src_tensor_op.tensor_obj.dtype)
        block_list = [BLK_NUM_PER_REP, src_block_len]
        self.dst_vconv_op.set_context(context)
        if isinstance(self.deqscale_op.tensor_obj, BasicData) and self.deqscale_op.tensor_obj.is_tensor() is True:
            self.deqscale_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list, context)
        self.src_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list, context)
        if not self.ldst_high_half:
            self.dst_vconv_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list, context)
        else:
            self.dst_vconv_op.block_len = src_block_len
            if is_basic_expr(TikUtil.to_list(self.control_op.eval_mask_value)) or \
                    is_basic_expr([self.control_op.repeat_times]):
                return
            if self.control_op.repeat_times == 0:
                return
            offset = self.dst_tensor_op.tensor_obj.offset + src_block_len
            total_size = reduce_mul(self.dst_tensor_op.tensor_obj.original_shape)
            extend_offset = self.dst_vconv_op.vector_max_offset_cal(self.control_op.eval_mask_value,
                                                                    self.control_op, self.control_op.repeat_times)
            if Expr(extend_offset + offset).eval_value() is not None:
                TikCheckUtil.check_le(Expr(extend_offset + offset).eval_value(), total_size,
                                      "dst tensor overflow, instruction need %s but only %s"
                                      % (Expr(extend_offset + offset).eval_value(), total_size))

    def set_context_value(self, context):
        """
        set dst, src, control's value

        Parameters
        ----------
        context : the debug's context

        Returns
        -------
        None
        """
        self.dst_tensor_op.set_context(context)
        self.src_tensor_op.set_context(context)
        self.dst_tensor_op.set_repeat_times(self.control_op.repeat_times)
        self.dst_tensor_op.set_stride_unit(self.control_op.stride_unit)
        self.control_op.eval_mask(context)

    def check_vconv_overflow(self, tensor_bit_len_max, context):
        """
        check vconv overflow

        Parameters
        ----------
        tensor_bit_len_max : tensor's bit len
        context : the debug's context

        Returns
        -------
        None
        """
        block_list_dst = gen_block_list(tensor_bit_len_max, self.dst_tensor_op.tensor_obj.dtype)
        block_list_src = gen_block_list(tensor_bit_len_max, self.src_tensor_op.tensor_obj.dtype)

        # stride_unit is required to be 0 when overflow, but control_op's strdie_unit is global variables
        stride_originals_unit = self.control_op.stride_unit
        self.control_op.stride_unit = 0
        self.dst_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_dst, context)
        self.src_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_src, context)
        self.control_op.stride_unit = stride_originals_unit

    def check_all(self, tik_instance, isdebug=False, context=None):
        """
        check all params

        Parameters
        ----------
        tik_instance : tik
        isdebug : if in debug mode
        context : the debug's context

        Returns
        -------
        self.dtype_str, self.deqscale_op.tensor_obj, self.mask_o
        """
        self.check_half_and_mode(isdebug)
        self.dst_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.set_blk_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)
        self.src_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)
        self.set_context_value(context)
        if not isdebug:
            check_repeat_times(self.control_op.repeat_times)
            self.check_dtype()
            self.check_deqscale()

        tensor_bit_len_max = max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                 get_bit_len(self.src_tensor_op.tensor_obj.dtype))
        overlap_expression = (self.dst_tensor_op.repeat_times_value, self.dst_tensor_op.blk_stride_value,
                              self.dst_tensor_op.rep_stride_value, self.src_tensor_op.blk_stride_value,
                              self.src_tensor_op.rep_stride_value, self.dst_tensor_op.stride_unit_value)
        # check and gen mask_o
        self.mask_o = self.control_op.check_and_gen_mask_o(tik_instance, tensor_bit_len_max, self.mask_o)
        debug_result = self._vconv_debug_condition()
        if "deqs162b8" in self.dtype_str or (debug_result and isdebug):
            # create a temporary Tensor
            self.dst_vconv_op = TensorOp(self.dst_tensor_op.tensor_obj, self.dst_tensor_op.blk_stride,
                                         self.dst_tensor_op.rep_stride, "dst")
            # change blk stride,rep stride
            src_dst_len_ratio = get_bit_len(self.src_tensor_op.tensor_obj.dtype) // \
                                get_bit_len(self.dst_tensor_op.tensor_obj.dtype)
            blk_original_stride = self.dst_vconv_op.blk_stride
            rep_original_stride = self.dst_vconv_op.rep_stride
            self.dst_vconv_op.blk_stride = self.dst_tensor_op.blk_stride_value * src_dst_len_ratio
            self.dst_vconv_op.rep_stride = self.dst_tensor_op.rep_stride_value * src_dst_len_ratio
            # check overlap and overflow
            self.check_vconv_deqs162b8_overlap(overlap_expression, src_dst_len_ratio, context)
            self.check_vconv_deqs162b8_overflow(context)
            self.dst_vconv_op.blk_stride = blk_original_stride
            self.dst_vconv_op.rep_stride = rep_original_stride
        else:
            if self.src_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
                if isdebug or all(isinstance(value, int) for value in overlap_expression):
                    block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len_max]
                    self.dst_tensor_op.check_address_overlapping(self.print_name, self.control_op,
                                                                 self.src_tensor_op, block_list)
            # check overflow
            self.check_vconv_overflow(tensor_bit_len_max, context)
        return self.dtype_str, self.deqscale_op.tensor_obj, self.mask_o

    def _vconv_debug_condition(self):

        debug_result = self.src_tensor_op.tensor_obj.dtype == 'int16' and \
                       self.dst_tensor_op.tensor_obj.dtype in ('int8', 'uint8')
        return debug_result

    def _check_dst_deqscale_src_overlap(self, dst_offset_work, src_dst_len_ratio):
        """
        check deqscale overlap

        Parameters
        ----------
        dst_offset_work : actual executed offset
        src_dst_len_ratio : divided by the dtype len of dst and src.

        Returns
        -------
        None
        """
        self.dst_vconv_op.tensor_obj.data = dst_offset_work
        block_list = [BLK_NUM_PER_REP, src_dst_len_ratio]
        if isinstance(self.deqscale_op.tensor_obj, BasicData) and self.deqscale_op.tensor_obj.is_tensor() is True:
            if self.deqscale_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
                self.dst_vconv_op.check_address_overlapping(self.print_name, self.control_op,
                                                            self.deqscale_op, block_list)
        if self.src_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
            self.dst_vconv_op.check_address_overlapping(self.print_name, self.control_op,
                                                        self.src_tensor_op, block_list)
        self.dst_vconv_op.tensor_obj.data = self.dst_tensor_op.tensor_obj.offset
