#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_build.py
DESC:     tik build
CREATED:  2020-9-08 14:32:13
MODIFIED: 2020-12-7 19:17:00
"""
import numpy as np

from tbe import tvm
from tbe.tvm.tir import StringImm
from tbe.tvm.tir import IntImm
from tbe.tvm.tir.stmt import AttrStmt
from tbe.tvm.runtime.cce_runtime import TIK_ATOMIC_ADD_LIST
from tbe.tvm.runtime.cce_runtime import TIK_WORKSPACE_SIZE_LIST
from tbe.tvm.driver.cce_build_module import build_fatbin
from tbe.common.buildcfg import build_config
from tbe.common.context import get_context
from tbe.common.platform import scope_gm
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_wreg
from tbe.common.utils import op_tiling
from tbe.common.utils.create_kb_query_key import get_op_compile_unique_key
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.api.tik_tensor_addr_list import TensorAddrList
from tbe.tik.api.tik_scalar import InputScalar
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_scalar_array import ScalarArray
from tbe.tik.api.tik_vector import Vector
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import check_is_atomic_add_attr
from tbe.tik.common.util import DTYPE_INT_VALUE
from tbe.tik.common.util import DTYPE_FLOAT_VALUE
from tbe.tik.common.util import flat_list
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.debug.decorators import build_cce_decorator
from tbe.tik.tik_lib.tik_backend import has_dynamic_tensor
from tbe.tik.tik_lib.tik_params import AI_CORE_INDICATE
from tbe.tik.tik_lib.tik_params import VA_REG
from tbe.tik.tik_lib.tik_params import GM_NAME_MAP_CLASS
from tbe.tik.tik_lib.tik_params import FAKE_TENSOR_NAME
from tbe.tik.tik_lib.tik_params import MAX_FORMAT_NUM
from tbe.tik.tik_lib.tik_params import DTYPE_REL_TOL
from tbe.tik.tik_lib.tik_params import VECTOR_PRINTF_DTYPE_MAP
from tbe.tik.tik_lib import Expr
from tbe.tik.tik_lib.tik_source_info import TikSourceInfo
from tbe.tik.tik_lib.tik_check_util import float_in_range
from tbe.tik.tik_lib.tik_check_util import get_error_dict_args
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_util import non_stmt_judge
from tbe.tik.tik_lib.tik_transform import remove_no_op
from tbe.tik.tik_lib.tik_transform import variable_scope_check_pass
from tbe.tik.tik_lib.tik_transform import buildcce_set_scalar_pass
from tbe.tik.tik_lib.tik_transform import dynamic_combine_static_pass
from tbe.tik.tik_lib.tik_transform import dynamic_shape_allocate_memory_preprocess
from tbe.tik.tik_lib.tik_transform import tik_return_instruction_elimination


_MIN_OUTPUTS_LENGTH = 1
_EXTEND_PARAMS_KEY = ("build_multi_kernels", )
_ATOMIC_CLOSE = None
# atomic clean value is 0
_ATOMIC_OPEN = 0.0


def _get_lower_check_params(build_obj):
    """
    get is_only_lower, and check inputs, outputs, flowtable_tmp
    Returns
    -------
    is_only_lower
    """
    is_only_lower = False
    if ("lower_debug_option" in build_obj.config.keys()) and build_obj.config["lower_debug_option"] == "only_tik_lower":
        is_only_lower = True

    if "lower_debug_option" in build_obj.config.keys():
        del build_obj.config["lower_debug_option"]

    for i in build_obj.inputs:
        TikCheckUtil.check_equality(i.scope, scope_gm, "inputs' scope should be scope_gm")
    for i in build_obj.outputs:
        TikCheckUtil.check_equality(i.scope, scope_gm, "outputs' scope should be scope_gm")

    for i in build_obj.flowtable_tmp:
        TikCheckUtil.check_equality(i.scope, scope_gm, "flowtable_tmp' scope should be scope_gm")

    return is_only_lower


def _build_with_fatbin(build_obj, body):
    TikCheckUtil.check_type_match(build_obj.extend_params["build_multi_kernels"],
                                  dict, "build_multi_kernels's value should be dict ")
    tiling_info = _check_build_multi_kernels_value(build_obj.extend_params["build_multi_kernels"])
    _check_element_of_tiling_key(tiling_info[0])
    schedules = []
    args = []
    rules = []
    scalar_value_map = build_obj.scalar_value_map
    if scalar_value_map is None:
        scalar_value_map = {}
    for i, tiling_value in enumerate(tiling_info[1]):
        _check_element_of_tiling_key_value(tiling_value, len(tiling_info[0]), i)

        for scalar, value in zip(tiling_info[0], tiling_value):
            _check_scalar_type_and_value_type(scalar, value)
            scalar_value_map[scalar] = value

        # new_body: body, flowtable_tmp
        new_body = make_body(build_obj, body, scalar_value_map)
        # input_output_buffer: input_buffer, input_placeholder, input_vars, output_buffer, output_flowtable_vars, binds
        input_output_buffer = get_input_output_buffer(build_obj, new_body[1])
        # result: res, binds, schedule
        result = compute_res(get_extern_ops(input_output_buffer[0], build_obj.outputs,
                                            input_output_buffer[3], input_output_buffer[1], new_body[0]),
                             input_output_buffer[3], input_output_buffer[-1])
        schedules.append(result[2])
        args.append(input_output_buffer[1] + input_output_buffer[2] + result[0] + input_output_buffer[-2])
        rules.append(i)
    build_obj.config["build_fatbin"] = True
    cfg = build_config(**build_obj.config)
    build_fatbin(cfg, schedules, args, rules, build_obj.kernel_name)


@build_cce_decorator
def build_cce(build_obj):
    """
    to generate cce file
    """
    is_only_lower = _get_lower_check_params(build_obj)
    # add tik debug context id to config
    build_obj.config["tik_debug_context_id"] = build_obj.tik_instance.tikdb.context.ctx_id
    if build_obj.tik_instance.debug_disabled_ is True:
        build_obj.config["tik_debug_context_id"] = -1  # disable debug

    # get body
    body = build_obj.tik_instance.get()
    TikSourceInfo.set_node_span("tik_build_cce")
    if build_obj.extend_params is not None and "build_multi_kernels" in build_obj.extend_params:
        _build_with_fatbin(build_obj, body)
    else:
        new_body, flowtable_tmp = make_body(build_obj, body, build_obj.scalar_value_map)
        input_buffer, input_placeholder, input_vars, output_buffer, output_flowtable_vars, binds = \
            get_input_output_buffer(build_obj, flowtable_tmp)
        extern_ops = get_extern_ops(input_buffer, build_obj.outputs, output_buffer, input_placeholder, new_body)
        res, binds, schedule = compute_res(extern_ops, output_buffer, binds)
        # sync_num is the number of worksapce that needed by multicore sync
        # call create_block_sync() to store the number of workspace inside the Class::Schedule
        # so that pass can get sync_num from Schedule
        # and automatically apply workspace for multicore sync
        for _ in range(build_obj.tik_instance.sync_num):
            schedule.create_block_sync()
        build_schedules(build_obj.config, schedule, input_placeholder + input_vars + res + output_flowtable_vars,
                        build_obj.kernel_name, [None, binds, is_only_lower])
    # clear source info before end of tik
    TikSourceInfo.end_and_clear()


def make_body_ir_platform(tik_instance, body, config):
    """
    Call tvm.tir api to make a body. make IR_platform
    :param tik_instance: an instance of Tik
    :param body: body
    :param config: build-config key value
    :return: body
    """
    func = tvm.get_global_func("cce.tik_version_init")
    value = func("TIK" + tik_instance.tik_version)
    if value != "success":
        raise RuntimeError("tik_version_init() return error.")
    body = AttrStmt(tvm.thread_axis("cce"), "TIK_version", StringImm(tik_instance.tik_version), body)
    body = AttrStmt(tvm.thread_axis("cce"), "IR_platform", StringImm("TIK"), body)
    config["tik"] = True
    return body


def make_body_buffer_storage(tik_instance, body):
    """
    Call tvm.tir api to make a body. make buffer_storage
    :param tik_instance: an instance of Tik
    :param body: body
    :return: body
    """
    for id_list in tik_instance.buffer_no_reuse_list:
        body = AttrStmt(None, "pragma_buffer_non_reuse",
                        tvm.call_extern("int64", "buffer_non_reuse", *id_list), body)

    for id_list in tik_instance.buffer_reuse_list:
        body = AttrStmt(None, "pragma_buffer_reuse",
                        tvm.call_extern("int64", "buffer_reuse", *id_list), body)

    for data in tik_instance.start_addr_dict:
        tensor_id, addr = tik_instance.start_addr_dict.get(data)
        body = AttrStmt(data, "pragma_buffer_specify_storage",
                        tvm.call_extern("int64", "buffer_specify_storage", tensor_id, addr), body)

    return body


def add_disable_scope_attr(build_obj, body):
    """
    add disable_scope attr for dynamic tik kernel,
    and set "dynamic_tik" and "let_stmt_not_inline" to True for dynamic tik kernel
    :param build_obj: obj of the Tik class
    :param body: stmt body
    :return: body with disable_scope attr
    """
    disable_scope_set = set()
    for tensor in build_obj.tik_instance.context.tensor_list:
        if not tensor.is_static_shape and tensor.max_mem_size is None:
            disable_scope_set.add(str(tensor.tensor_scope))
    disable_scope_list = sorted(disable_scope_set, reverse=True)
    has_dyn_tensor = has_dynamic_tensor(body)
    if len(disable_scope_list) > 0 and has_dyn_tensor:
        for scope in disable_scope_list:
            body = AttrStmt(tvm.thread_axis("cce"), "pragma_sequential_malloc", StringImm(scope), body)
        build_obj.config["dynamic_tik"] = True
        build_obj.config["let_stmt_not_inline"] = True

    return body


def make_body(build_obj, body, scalar_value_map):
    """
    Call tvm.tir api to make a body.
    param build_obj: build_cce params obj
    param scalar_value_map: Scalar and value dict
    body stmt

    Parameters
    ----------
    :return: body, flowtable
    """
    span = TikSourceInfo.get_node_span("tik_build_cce")
    # if no set_address_value called, needn't to add tensor index
    if len(build_obj.tik_instance.buffer_buffer_id_dict) > 0:
        body = _add_tensor_index(build_obj.tik_instance, body)

    body_judge = remove_no_op(body)
    if non_stmt_judge(body_judge):
        tmp_node = tvm.call_extern("uint64", "return")
        body = tvm.tir.Evaluate(tmp_node)
    for i in build_obj.global_scalar_list:
        body = i.merge_scalar(body)
        # each scalar list will generate 2 ir, one for scope, one for allocate

    for i in build_obj.flowtable_tmp:
        if isinstance(i, Tensor):
            # this is the size of tiling size.
            body = AttrStmt(tvm.thread_axis("cce"), "op_params_size", _make_tiling_ir(i), body, span)
            break

    body = make_body_ir_platform(build_obj.tik_instance, body, build_obj.config)

    body = AttrStmt(tvm.thread_axis("cce"), "comment", StringImm(AI_CORE_INDICATE + get_soc_name()), body, span)
    # comment will generate 1 ir

    body = make_body_buffer_storage(build_obj.tik_instance, body)

    for i in VA_REG:
        # each VA_REG will generate 1 ir
        body = AttrStmt(tvm.thread_axis("cce"), "var_pre_def", i, body)

    # add tik pass
    body, flowtable_tmp = add_tik_pass(body, build_obj.inputs, build_obj.outputs,
        build_obj.flowtable_tmp, scalar_value_map)

    body = add_disable_scope_attr(build_obj, body)
    return body, flowtable_tmp


def is_dynamic_combine_static(flowtable_tmp):
    """
    Judge if is the dynamic combine static case.
    :param flowtable_tmp: flowtable
    :return: bool value
    """
    if (get_context() is None) or (get_context().get_op_mode() == "dynamic"):
        return False
    if (flowtable_tmp is None) or (len(flowtable_tmp) == 0) or (not isinstance(flowtable_tmp[0], Tensor)):
        return False
    return True


def _tik_op_compile_tiling(op_info):
    # invoked only here and imported internally to reduce dependency.
    from tbe.common.repository_manager.interface import cann_kb_search
    from tbe.common.register import get_tune_space

    tune_param = get_context().get_addition("tune_param")
    # if TUNE_PARAM parameter is not empty, is optimization scenario.
    # if TUNE_PARAM is empty, is compilation scenario.
    if tune_param:
        get_context().add_compile_info("_tune_param", tune_param)
    else:
        # Check whether the operator registers the optimization solution space function.
        # The optimization result is available only when the operator registers the function.
        if get_tune_space(op_info[-1].op_type):
            # get knowledge query keys
            op_compile_unique_keys = get_op_compile_unique_key(
                op_info[-1].op_type, op_info[-1].inputs, op_info[-1].outputs,
                op_info[-1].attrs, op_info[-1].extra_params, is_sha=False)

            # query the knowledge base to obtain the optimization result.
            op_compile = cann_kb_search(op_compile_unique_keys[0], {"op_type": "tik_vector"})
            # if optimization result is not empty, add it to compileinfo.
            try:
                _tune_param = op_compile[0].get('knowledge')
            except (TypeError, IndexError, AttributeError):
                _tune_param = None

            if _tune_param:
                get_context().add_compile_info("_tune_param", _tune_param)


def get_tiling_params(inputs, flowtable):
    """
    Call do op tiling to get tiling params.
    :param inputs: input data
    :param flowtable: flowtable
    :return: tiling params
    """
    op_info = get_context().get_op_info()
    tiling_inputs = op_info[-1].inputs
    tiling_outputs = op_info[-1].outputs

    inputs_idx = 0
    for tiling_idx, tiling_input in enumerate(tiling_inputs):
        if tiling_input is None:
            continue
        list(flat_list(tiling_inputs))[tiling_idx]["name"] = inputs[inputs_idx].buffer.name
        inputs_idx += 1

    # dynamic shape op_tiling implements static shape automatic optimization of the TIK operator.
    _tik_op_compile_tiling(op_info)

    run_info = op_tiling.do_op_tiling(op_info[-1].op_type, get_context().get_compile_info(),
                                      tiling_inputs, tiling_outputs, attrs=op_info[-1].attrs)
    tiling_data = run_info['tiling_data']
    # The workspace stores a list, indicating the size of the workspace tensor.
    if 'workspaces' in run_info and len(run_info['workspaces']) > 0:
        TIK_WORKSPACE_SIZE_LIST.local_list = run_info['workspaces']
    value_dtype = flowtable[0].dtype
    data = np.frombuffer(tiling_data, dtype=value_dtype)

    return data


def extent_list(ori_list):
    new_list = []
    for i in ori_list:
        if isinstance(i, (list, tuple)):
            new_list.extend(extent_list(i))
        else:
            new_list.append(i)
    return new_list


def get_size_info(inputs, outputs):
    """
    get inputs/outputs/workspace size for oom.
    :return: size info
    """
    op_info = get_context().get_op_info()
    tiling_inputs = extent_list(op_info[-1].inputs)
    tiling_outputs = extent_list(op_info[-1].outputs)

    size_info = []

    for i, item in enumerate(tiling_inputs):
        if isinstance(item, dict):
            size_info.append(reduce_mul(item.get("shape")) * DTYPE_SIZE[inputs[i].dtype])
    for i, item in enumerate(tiling_outputs):
        if isinstance(item, dict):
            size_info.append(reduce_mul(item.get("shape")) * DTYPE_SIZE[outputs[i].dtype])

    size_info += TIK_WORKSPACE_SIZE_LIST.local_list
    return size_info


def add_tik_pass(body, inputs, outputs, flowtable_tmp, scalar_value_map):
    """
    Add tik pass.
    :param body: body from make_body
    :param inputs: input data
    :param flowtable_tmp: flowtable
    :param scalar_value_map: scalar value map
    :return: body
    """
    # visit the stmt to check whether all vars used in the right scope
    variable_scope_check_pass(body)
    if scalar_value_map is not None and len(scalar_value_map) != 0:
        scalar_name_map = {}
        for i, j in scalar_value_map.items():
            scalar_name_map[i.name] = tvm.const(j, i.dtype)
        body = buildcce_set_scalar_pass(body, scalar_name_map)
    if is_dynamic_combine_static(flowtable_tmp):
        tiling_data = get_tiling_params(inputs, flowtable_tmp)
        tiling_tensor = flowtable_tmp[0]
        tiling_value = [tvm.const(i, tiling_tensor.dtype) for i in tiling_data]
        body = dynamic_combine_static_pass(body, tiling_tensor.buffer.name, tiling_value)
        tmp_call = tvm.call_extern(tiling_tensor.dtype, "params", *tiling_value)
        tmp_const = tvm.const(1, dtype="bool")
        body = tvm.tir.Allocate(
            tiling_tensor.data_var, tiling_tensor.dtype, [len(tiling_value)], tmp_const, body, new_expr=tmp_call)
        body = AttrStmt(tiling_tensor.data_var, "storage_scope", StringImm(tiling_tensor.scope), body)
        size_info = get_size_info(inputs, outputs)
        body = AttrStmt(None, "tik_attr_data", tvm.call_intrin("handle", 'tir.tvm_tuple', *size_info), body)
        # tiling data is const, donot need this tensor
        flowtable_tmp = flowtable_tmp[1:]
    elif flowtable_tmp and get_context() is not None and get_context().get_op_mode() == "dynamic":
        if isinstance(flowtable_tmp[0], Tensor):
            body = AttrStmt(None, "tik_tiling_data", flowtable_tmp[0].buffer.data, body)
        elif isinstance(flowtable_tmp[0], InputScalar):
            body = AttrStmt(None, "tik_tiling_data", flowtable_tmp[0].get(), body)
    body = dynamic_shape_allocate_memory_preprocess(body)
    body = tik_return_instruction_elimination(body)

    return body, flowtable_tmp


def build_schedules_impl(schedules, args, kernel_name, build_params):
    """
    :param schedules:
    :param args:
    :param kernel_name: operator kernel name
    :param build_params: current only support [rules, binds, is_only_lower]
    :return:
    """
    rules, binds, is_only_lower = build_params
    if is_only_lower:
        return tvm.lower(schedules, args, simple_mode=True)
    return tvm.build(schedules, args, "cce", name=kernel_name, rules=rules, binds=binds)


def build_schedules(config_map, schedules, args, kernel_name, build_parmas):
    """
    :param config_map: build-config
    :param schedules:
    :param args:
    :param kernel_name: operator kernel name
    :param build_parmas: current only support [rules, binds]
    :return: build schedules
    """
    if config_map is not None:
        with build_config(**config_map):
            return build_schedules_impl(schedules, args, kernel_name, build_parmas)
    else:
        with build_config():
            return build_schedules_impl(schedules, args, kernel_name, build_parmas)


def _compute_res_input(inputs, inputs_placeholder, all_inputs, binds):
    input_buffer = []
    for i in inputs:
        placeholder = tvm.placeholder(i.buffer.shape, i.buffer.dtype, i.buffer.name)
        inputs_placeholder.append(placeholder)
        input_buffer.append(i.buffer)
        binds[placeholder] = i.buffer
        _add_atomic_list_from_tensor_info(i)

    for i in all_inputs:
        if isinstance(i, InputScalar):
            TIK_ATOMIC_ADD_LIST.local_list.append(_ATOMIC_CLOSE)
    return input_buffer


def _compute_res_output(outputs, all_outputs, workspace_tensor_list):
    output_buffer = []
    for i in outputs:
        output_buffer.append(i.buffer)
        _add_atomic_list_from_tensor_info(i)

    for i in all_outputs:
        if isinstance(i, InputScalar):
            TIK_ATOMIC_ADD_LIST.local_list.append(_ATOMIC_CLOSE)

    for i in workspace_tensor_list:
        output_buffer.append(i.buffer)
        _add_atomic_list_from_tensor_info(i)
    return output_buffer


def get_input_buffer(all_inputs):
    """
    Compute the result of inputs and outputs
    :param all_inputs: save all inputs tensor and inputscalar
    :return: input_buffer, inputs_placeholder, input_vars, binds
    """
    binds = []

    inputs_placeholder = []
    input_vars = []
    input_tensors = []
    for i in all_inputs:
        if isinstance(i, InputScalar):
            input_vars.append(i.get())
        else:
            input_tensors.append(i)

    input_buffer = _compute_res_input(input_tensors, inputs_placeholder, all_inputs, binds)
    return [input_buffer, inputs_placeholder, input_vars, binds]


def get_input_output_buffer(build_obj, flowtable):
    """
    Compute the result of inputs and outputs
    :param build_obj: build_cce params obj.
    :param flowtable: flowtable
    :return: res
    """
    TIK_ATOMIC_ADD_LIST.local_list = []

    binds = {}
    inputs_placeholder = []
    input_vars = []
    input_tensors = []
    output_vars = []
    output_tensors = []
    flowtable_vars = []

    for i in build_obj.inputs:
        if isinstance(i, InputScalar):
            input_vars.append(i.get())
        else:
            input_tensors.append(i)

    for i in build_obj.outputs:
        if isinstance(i, InputScalar):
            output_vars.append(i.get())
        else:
            output_tensors.append(i)
    for i in flowtable:
        if isinstance(i, InputScalar):
            flowtable_vars.append(i.get())

    input_buffer = _compute_res_input(input_tensors, inputs_placeholder, build_obj.inputs, binds)
    output_buffer = _compute_res_output(output_tensors, build_obj.outputs, build_obj.workspace_tensor_list)
    output_buffer = add_global_tensor_to_out_buffer(build_obj.global_tensor_list, output_buffer)

    for i in flowtable:
        if isinstance(i, InputScalar):
            TIK_ATOMIC_ADD_LIST.local_list.append(_ATOMIC_CLOSE)
        else:
            output_buffer.append(i.buffer)
            _add_atomic_list_from_tensor_info(i)
    return [input_buffer, inputs_placeholder, input_vars, output_buffer, output_vars + flowtable_vars, binds]


def get_extern_ops(input_buffer, outputs, output_buffer, inputs_placeholder, body):
    """
    Compute the result of inputs and outputs
    :param input_buffer:input buffer
    :param outputs: output
    :param output_buffer: output buffer
    :param inputs_placeholder: placeholder
    :param body: body from make_body
    :return: extern_ops
    """
    if not output_buffer:
        output_name = FAKE_TENSOR_NAME
    else:
        output_name = output_buffer[0].name
    if len(outputs) > _MIN_OUTPUTS_LENGTH:
        for i, output in enumerate(outputs):
            out_put_name = "%s_v%s" % (output_name, i)
            GM_NAME_MAP_CLASS[out_put_name] = output.name
    span = TikSourceInfo.get_node_span("tik_build_cce")
    extern_ops = tvm.te.ExternOp(output_name, "", None, inputs_placeholder, input_buffer, output_buffer, body, span)
    return extern_ops


def compute_res(extern_ops, output_buffer, binds):
    """
    Compute the result of inputs and outputs
    :param extern_ops:
    :param output_buffer: output buffer
    :param binds:
    :return: res, binds, schedule
    """
    res = []
    for index, value in enumerate(output_buffer):
        output = extern_ops.output(index)
        res.append(output)
        binds[output] = value

    schedule = tvm.create_schedule([r.op for r in res])
    return res, binds, schedule


def add_global_tensor_to_out_buffer(global_tensor_list, out_buf):
    """
    add global_tensor to out buffer
    :param global_tensor_list: global tensor list
    :param out_buf: output buffer list
    :return: output buffer
    """
    for i in global_tensor_list:
        out_buf.append(i.buffer)
        _add_atomic_list_from_tensor_info(i)
    return out_buf


def tik_dsl_gen():
    """
    generate DSL
    :return IRBuilder
    """
    return tvm.tir.ir_builder.create()


def _add_tensor_index(tik_instance, body):
    """
    add buffer index to tensor allocate for set_address_value
    :param tik_instance: tik docker
    :param body: ir body
    :return: new ir body
    """
    def _post_order_add(stmt_in):
        if stmt_in.buffer_var in tik_instance.buffer_buffer_id_dict:
            # create a new stmt with stmt_in.body
            attr_stmt = AttrStmt(
                stmt_in.buffer_var, "pragma_buffer_index",
                tvm.call_extern("int64", "buffer_index", tik_instance.buffer_buffer_id_dict[stmt_in.buffer_var]),
                stmt_in.body)

            # make a new Allocate
            stmt_in = tvm.tir.Allocate(
                stmt_in.buffer_var, stmt_in.dtype, stmt_in.extents, stmt_in.condition, attr_stmt)

            # delete the buffer info from buffer dict
            del tik_instance.buffer_buffer_id_dict[stmt_in.buffer_var]
            return stmt_in

        return stmt_in

    # update the tensor index by scan the IR body
    stmt_out = tvm.tir.stmt_functor.ir_transform(body, None, _post_order_add, ["tir.Allocate"])
    return stmt_out


def _make_tiling_ir(tiling_tensor):
    """
    use tiling tensor to gen correct ir
    :param tiling_tensor:
    :return:
    """
    return IntImm("int32", tiling_tensor.size*DTYPE_SIZE[tiling_tensor.dtype])

def _add_atomic_list_from_tensor_info(tensor):
    """use to add 1 for is atomic add tensor, else add 0"""
    if tensor.is_atomic_add:
        if check_is_atomic_add_attr(tensor.original_shape) and tensor.dtype in ("float16", "float32"):
            TIK_ATOMIC_ADD_LIST.local_list.append(_ATOMIC_CLOSE)
        else:
            if tensor.init_value is None:
                # Compatible with operators
                zero_default_type = "float32"
                TIK_ATOMIC_ADD_LIST.local_list.append({"init_value": _ATOMIC_OPEN, "dtype": zero_default_type})
            else:
                TIK_ATOMIC_ADD_LIST.local_list.append({"init_value": tensor.init_value, "dtype": tensor.dtype})
    else:
        TIK_ATOMIC_ADD_LIST.local_list.append(_ATOMIC_CLOSE)

def _check_format_control(format_control, format_args, args_length):
    if format_control:
        TikCheckUtil.raise_error("format_string contains start % but not end with [dfoxsc]")
    if format_args != args_length:
        TikCheckUtil.raise_error("arg length:%s is is not equal to format args:%s!" % (args_length, format_args))


def _check_format_str(format_str):
    if format_str != "":
        TikCheckUtil.raise_error("# should be put at head!")


def format_arg_strong_match(format_seq, arg):
    """
    format arg strong match
    """
    arg_index = 0
    format_args = 0
    args_length = len(arg)
    format_str = ""
    format_control = False
    last_word = ('d', 'f', 'o', 'x', 's', 'c')

    for c in format_seq:
        if c != "%" and format_control is False:
            continue
        if format_control is True:
            if c == '#':
                _check_format_str(format_str)
                format_str = format_str + c
            elif c == '.':
                _check_dot_for_print_arg(format_str, last_word, c)
                format_str = format_str + c
            elif c in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']:
                _check_num_for_print_arg(format_str, last_word)
                format_str = format_str + c
            elif c in last_word:
                _check_print_arg_length(arg_index, args_length)
                _check_float_print_with_format(arg[arg_index], c)
                _analysis_format_nums(format_str)
                _check_print_char_range(arg[arg_index], c)
                format_args += 1
                arg_index += 1
                format_str = ""
                format_control = False
            elif c == '%':
                _check_percent_for_print_arg(format_str)
                format_control = False
                continue
            else:
                TikCheckUtil.raise_error("format arg contains invalid character %s" % c)
        elif c == "%":
            format_control = True

    _check_format_control(format_control, format_args, args_length)


def gen_print_data_length(args, single_print_length):
    """
    we will automatically align the tvl with 32 bytes
    will make all tvl together and put value at last, thus can add together and each 32 align
    """

    # expr add int
    total_data_length = 0
    # total valid length is can change to int
    total_valid_length = 0
    # original data length
    real_data_length = 0
    for _, v in enumerate(args):
        align_length, actual_length = gen_each_print_data_length(v)
        total_data_length += align_length
        real_data_length += actual_length
        if isinstance(align_length, int):
            total_valid_length += align_length
        if total_valid_length >= single_print_length:
            get_error_dict_args(
                "total_data_length:%s is larger than bytes for each block:%s!" % (total_data_length,
                                                                                  single_print_length))
    return total_data_length, total_valid_length, real_data_length


def gen_align_num_for_string(string_length):
    """
    Calculates the string length.
    """
    return ceil_div(string_length, 32) * 32 - string_length


def _check_dot_for_print_arg(format_str, last_word, character):
    if any(s in format_str for s in last_word):
        TikCheckUtil.raise_error("find [dfoxsc] before .!")
    if character in format_str:
        TikCheckUtil.raise_error("find duplicate . in format args!")


def _check_num_for_print_arg(format_str, last_word):
    if any(s in format_str for s in last_word):
        TikCheckUtil.raise_error("find [dfoxsc] before [0-9]!")


def _check_percent_for_print_arg(format_str):
    if format_str != "":
        TikCheckUtil.raise_error("%% should be put together!")


def _check_print_arg_length(arg_index, args_length):
    if arg_index >= args_length:
        TikCheckUtil.raise_error("arg length is less than format args!")


def _check_float_print_with_format(arg, character):
    if "float" in arg.dtype:
        if character in ('o', 'x', 'c'):
            TikCheckUtil.raise_error("arg can't be float when arg type is [oxc]")


def _analysis_format_nums(format_args):
    final_format = ""
    for c in format_args:
        if c in ['.', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']:
            final_format += c
    if final_format.startswith('0'):
        final_format = final_format[1:]
    if final_format.startswith('0'):
        TikCheckUtil.raise_error("format arg not allowed to contains 00 after %!")
    nums_array = final_format.split(".")
    if len(nums_array[0]) > 0:
        TikCheckUtil.check_in_range_by_dtype(
            int(nums_array[0]), msg="format arg's width should be in range of [%d, %d], but get %s"
                                    % (0, MAX_FORMAT_NUM, int(nums_array[0])), var_range=[0, MAX_FORMAT_NUM])
    if len(nums_array) > 1:
        if nums_array[1].startswith('0'):
            TikCheckUtil.raise_error("format arg's precision can't startswith 0")
        TikCheckUtil.check_in_range_by_dtype(
            int(nums_array[1]), msg="format arg's precision should be in range [%d, %d], but get %s"
            % (0, MAX_FORMAT_NUM, int(nums_array[1])), var_range=[0, MAX_FORMAT_NUM])


def _check_print_char_range(arg, character):
    if isinstance(arg, (Scalar, Expr)):
        arg_real_value = Expr(arg, dtype=arg.dtype).eval_value()
        if arg_real_value is not None:
            if isinstance(arg_real_value, int) and arg_real_value not in range(256) and character == 'c':
                TikCheckUtil.raise_error("%c arg not in [0, 255]")


def gen_each_print_data_length(args):
    """
    gen three return value
    Parameters
    ----------
    args: param

    Returns
    -------
    total_data_length, real_data_length
    """
    # first data_length with expr
    # third data length with 8 align.
    if isinstance(args, (Scalar, Expr)):
        # each Scalar means 8 bytes align
        return 32, DTYPE_SIZE[args.dtype]
    if isinstance(args, ScalarArray):
        total_data_length = ceil_div(args.length * DTYPE_SIZE[args.dtype], 32) * 32
        real_data_length = args.length * DTYPE_SIZE[args.dtype]
        return total_data_length, real_data_length
    if isinstance(args, Vector):
        if args.scope == scope_wreg and args.dtype in VECTOR_PRINTF_DTYPE_MAP:
            total_data_length = args.data_len * DTYPE_SIZE[VECTOR_PRINTF_DTYPE_MAP.get(args.dtype)]
        elif args.scope == scope_wreg and args.dtype == "int64":
            # int64 wreg data: [low32, high32, 0, low32, high32, 0, ...]
            total_data_length = args.data_len * DTYPE_SIZE[args.dtype] * 2 // 3
        else:
            total_data_length = args.data_len * DTYPE_SIZE[args.dtype]
        return total_data_length, total_data_length
    shape_of_tensor = Expr(args.size).eval_value()
    tensor_start_offset = Expr(args.offset).eval_value()
    tensor_size = Expr(reduce_mul(args.original_shape)).eval_value()
    if tensor_start_offset is not None:
        align = _gen_align_value_for_tensor(args)
        if tensor_start_offset % align != 0:
            TikCheckUtil.raise_error(
                'Address align error! %s is not %s align' % (tensor_start_offset, align))
    if tensor_size is not None and tensor_start_offset is not None:
        # use to check tensor_start_offset + uint_for_data_move <= tensor_original_size
        TikCheckUtil.check_ge((tensor_size - tensor_start_offset) * DTYPE_SIZE[args.dtype], _uint_for_data_move(args),
                              "tensor's data should larger than min data_move unit %s" % _uint_for_data_move(args))
    if shape_of_tensor is not None:
        total_data_length = ceil_div(shape_of_tensor * DTYPE_SIZE[args.dtype], 32) * 32
        real_data_length = shape_of_tensor * DTYPE_SIZE[args.dtype]
        return total_data_length, real_data_length
    total_data_length = ceil_div(args.size * DTYPE_SIZE[args.dtype], 32) * 32
    real_data_length = args.size * DTYPE_SIZE[args.dtype]
    return total_data_length, real_data_length


def _gen_align_value_for_tensor(args):
    if args.scope == scope_cc:
        return 256
    if args.scope in (scope_ubuf, scope_gm):
        return 1
    return 32 // DTYPE_SIZE[args.dtype]


def _uint_for_data_move(args):
    if args.scope == scope_cc:
        if args.dtype == "float16":
            return 512
        return 1024
    if args.scope == scope_ubuf:
        return DTYPE_SIZE[args.dtype]
    return 32


def gen_args_type_num(args):
    """
    gen data_type num
    """
    data_size = {
        "uint8": 0, "int8": 1, "uint16": 2, "int16": 3, "float16": 4,
        "uint32": 5, "int32": 6, "float32": 7, "uint64": 8, "int64": 9
    }
    if isinstance(args, Vector):
        if args.dtype in VECTOR_PRINTF_DTYPE_MAP:
            return data_size.get(VECTOR_PRINTF_DTYPE_MAP.get(args.dtype))
    return data_size.get(args.dtype)


def check_extend_params_feed_dict(extend_params):
    """
    check extend params feed dict
    :param extend_params: extend params
    :return: None
    """
    for key in extend_params.keys():
        if key not in _EXTEND_PARAMS_KEY:
            TikCheckUtil.raise_error("extend_params doesn't support \"%s\"." % key)


def _check_build_multi_kernels_value(build_multi_kernels_value):
    TikCheckUtil.check_type_match(build_multi_kernels_value,
                                  dict, "build_multi_kernels's value should be dict.")
    keys = build_multi_kernels_value.keys()
    if len(keys) != 2 or "tiling_key" not in keys or "tiling_key_value" not in keys:
        TikCheckUtil.raise_error("build_multi_kernels's value should have two "
                                 "string key --- \"tiling_key\" and \"tiling_key_value\".")
    # check the tiling_key
    tiling_keys = build_multi_kernels_value["tiling_key"]
    if not isinstance(tiling_keys, (tuple, list)):
        TikCheckUtil.raise_error("tiling_key's type should be list or tuple.")
    if len(tiling_keys) < 1:
        TikCheckUtil.raise_error("tiling_key has at least one data.")

    # check the tiling_key_value
    tiling_key_values = build_multi_kernels_value["tiling_key_value"]
    if not isinstance(tiling_key_values, (tuple, list)):
        TikCheckUtil.raise_error("tiling_key_value's type should be list or tuple.")
    if len(tiling_key_values) < 1:
        TikCheckUtil.raise_error("tiling_key_value has at least one data.")

    return tiling_keys, tiling_key_values


def _check_element_of_tiling_key(tiling_keys):
    for tiling_key in tiling_keys:
        TikCheckUtil.check_type_match(tiling_key, Scalar, "tiling_key's value should be Scalar.")


def _check_element_of_tiling_key_value(tiling_value, length, index):
    if not isinstance(tiling_value, (tuple, list)):
        TikCheckUtil.raise_error("The element (at position %d in tiling_key_val"
                                 "ue) should be list or tuple." % index)
    if len(tiling_value) != length:
        TikCheckUtil.raise_error("The length of element (at position %d in tiling_key_value)"
                                 " should be equal to the length of tiling_key." % index)


def _check_scalar_type_and_value_type(scalar, value):
    if scalar.dtype in DTYPE_INT_VALUE:
        if not isinstance(value, int):
            TikCheckUtil.raise_error("%s's type is %s in tiling_key, whose "
                                     "value should be int in tiling_key_value." % (scalar.name, scalar.dtype))
        TikCheckUtil.check_in_range_by_dtype(
            value, msg="%s's type is %s in tiling_key, whose value should in [%d, %d], but get %s in tiling_key_value."
            % (scalar.name, scalar.dtype, DTYPE_INT_VALUE[scalar.dtype][0], DTYPE_INT_VALUE[scalar.dtype][1], value),
            var_range=[DTYPE_INT_VALUE[scalar.dtype][0], DTYPE_INT_VALUE[scalar.dtype][1]])
    elif scalar.dtype in DTYPE_FLOAT_VALUE:
        if not isinstance(value, float):
            TikCheckUtil.raise_error("%s's type is %s in tiling_key, whose "
                                     "value should be float in tiling_key_value." % (scalar.name, scalar.dtype))
        if not float_in_range(value, scalar.dtype, DTYPE_REL_TOL[scalar.dtype]):
            TikCheckUtil.raise_error(
                "%s's type is %s in tiling_key, whose value should in "
                "[%s, %s], but get %s in tiling_key_value." % (
                    scalar.name, scalar.dtype, DTYPE_FLOAT_VALUE[scalar.dtype][0],
                    DTYPE_FLOAT_VALUE[scalar.dtype][1], value))
    else:
        TikCheckUtil.raise_error("%s's type is %s in tiling_key, just support int type." % (scalar.name, scalar.dtype))
