#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_inner_api_.py
DESC:     tik_inner_api_
CREATED:  2021-12-13 3:35 PM
MODIFIED: 2021-12-13 3:35 PM
"""
import numpy as np
from tbe.tvm import Var
from tbe.common.context import get_context
from tbe.common.platform import scope_gm
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_ca
from tbe.common.platform import scope_cb
from tbe.common.platform import scope_vreg
from tbe.common.platform import scope_preg
from tbe.common.platform import scope_wreg
from tbe.common.platform import ASCEND_310
from tbe.common.platform.platform_info import get_soc_spec
from tbe.tik import debug
from tbe.tik.api.tik_ir_builder import TikIRBuilder
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.api.tik_tensor_addr_list import TensorAddrList
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_scalar_array import ScalarArray
from tbe.tik.api.tik_vector import Vector
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import check_scope
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.tik_get_soc_name import get_soc_core_type
from tbe.tik.common.tik_get_soc_name import TIK_SOC_INFO
from tbe.tik.debug.decorators import high_level_api_debug_decorator
from tbe.tik.debug.decorators import printf_decorator
from tbe.tik.tik_lib.tik_params import FAKE_TENSOR_NAME
from tbe.tik.tik_lib.tik_params import scope_cbuf_out
from tbe.tik.tik_lib.tik_params import PRINT_WORKSPACE_DTYPE
from tbe.tik.tik_lib.tik_params import EACH_PRINT_HEAD_BYTES
from tbe.tik.tik_lib.tik_params import DEFAULT_PRINT_UB_BYTES
from tbe.tik.tik_lib.tik_params import PRINT_TL_BYTES
from tbe.tik.tik_lib.tik_params import PRINT_CORE_HEAD_BYTES
from tbe.tik.tik_lib.tik_params import MAGIC_TL_TAG
from tbe.tik.tik_lib.tik_params import TENSOR_SUPPORT_DYTPE
from tbe.tik.tik_lib.tik_params import SCALAR_SUPPORT_DYTPE
from tbe.tik.tik_lib.tik_params import typeMap
from tbe.tik.tik_lib.tik_params import VECTOR_PRINTF_DTYPE_MAP
from tbe.tik.tik_lib import Expr
from tbe.tik.tik_lib.tik_source_info import TikSourceInfo
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_check_util import get_error_dict_args
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_build_ import gen_each_print_data_length
from tbe.tik.tik_lib.tik_build_ import format_arg_strong_match
from tbe.tik.tik_lib.tik_build_ import gen_print_data_length
from tbe.tik.tik_lib.tik_build_ import gen_align_num_for_string
from tbe.tik.tik_lib.tik_build_ import gen_args_type_num
from tbe.tik.tik_lib.tik_build_ import get_tiling_params
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager


class TikInner(TikIRBuilder):
    """
    tik instance class
    """

    def __init__(self):
        """
        Creates a TIK DSL container by passing a tik.Dprofile instance.
        This is interal class
        Using stmt_scope_stack store the sync state and the state of including
        high level api in new_stmt_scope.
        Format:
        [[sync_state, the_state_of_including_high_level_api], ]
        sync_state: the sync state in current scope. bool type
        True: disable the sync in current new_stmt_scope,
        False: enable the sync in current new_stmt_scope.
        the_state_of_including_high_level_api: the state of including high level api.
        True: there is high level api in current new_stmt_scope.
        False: there is not high level api in current new_stmt_scope.
        """
        super(TikInner, self).__init__()
        TikSourceInfo.is_inited = False
        # init soc info, before create Tik obj
        TIK_SOC_INFO.soc_name = None
        TIK_SOC_INFO.soc_core_type = None
        TIK_SOC_INFO.block_size = None
        TIK_SOC_INFO.rep_size = None
        TIK_SOC_INFO.cpt_rep_size = None
        TIK_SOC_INFO.cpt_blk_size = None
        TIK_SOC_INFO.is_compatible = None
        self.stmt_scope_stack = []
        self.single_print_length = 512
        self.has_set_printf_params = False
        self._print_block_ub = None
        self._has_done_printf = False
        self._print_bytes_consumed = 0
        self.current_ub_offset = 0
        self._print_workspace = None
        self._has_printf_out_multi_for = False
        self.printf_file_path = None
        self.is_printf_to_screen = True
        self.current_atomic_value = 0
        # use this to ensure that only one gm Tensor set is_tiling_tensor, and one ub Tensor set is_tiling_tensor
        self._has_tiling_gm = False
        self._has_tiling_ub = False

    @property
    def debug_disabled(self):
        """
        close debug function

        Parameters
        ----------

        Returns
        -------
        None
        """
        return self.debug_disabled_

    @property
    def is_in_debug(self):
        """
        check status of debug
        when _is_in_debug is true, the debug mode is running.
        when _is_in_debug is false, the debug mode is closed.

        Parameters
        ----------

        Returns bool
        -------
        return is_in_debug mode

        """
        return self._is_in_debug

    @property
    def is_building_cce(self):
        """
        check status of debug
        when _is_build_cce is true, the program is running cceBuild function.
        when _is_build_cce is false, the program don't run cceBuild function.

        Parameters
        ----------

        Returns bool
        -------
        return ccebuild model

        """
        return self._is_building_cce

    @is_in_debug.setter
    def is_in_debug(self, debug_mode):
        """
        run debug mode

        Parameters
        ----------
        debug_mode: bool
            if True: debug is running
            if False: debug is not running

        Returns
        -------
        None
        """
        self._is_in_debug = debug_mode

    @is_building_cce.setter
    def is_building_cce(self, buildcce_mode):
        """
        run cceBuild

        Parameters
        ----------
        buildcce_mode: bool
            if True: running cceBuild function
            if False: not running cceBuild function

        Returns
        -------
        None
        """
        self._is_building_cce = buildcce_mode

    @staticmethod
    def expr(expr_, dtype=None):
        """
        create expr

        Parameters
        ----------
        expr_ : str, the input str
        dtype : type

        Returns
        -------
        expr
        """
        return Expr(expr_, dtype)

    @staticmethod
    def _check_atomic_tensor_args(tensor_obj):
        """
        check Whether the init_value is within the range of the data type
        Parameters
        ----------
        tensor_obj: tensor

        Returns tensor init value
        -------

        """
        init_value = tensor_obj.init_value
        if tensor_obj.is_atomic_add is True and tensor_obj.init_value is not None:
            TikCheckUtil.check_type_match(tensor_obj.init_value, (int, float),
                                          "atomic tensor:%s init value can only be int/float." % tensor_obj.name)
            if "float" in tensor_obj.dtype:
                init_value = float(tensor_obj.init_value)
                TikCheckUtil.check_equality(typeMap[tensor_obj.dtype](init_value), init_value,
                                            "atomic tensor:%s init value's should be in [%s, %s]." %
                                            (tensor_obj.name, np.finfo(typeMap[tensor_obj.dtype]).min,
                                             np.finfo(typeMap[tensor_obj.dtype]).max))
            elif "int" in tensor_obj.dtype:
                init_value = int(tensor_obj.init_value)
                TikCheckUtil.check_equality(typeMap[tensor_obj.dtype](init_value), init_value,
                                            "atomic tensor:%s init value's should be in [%s, %s]." %
                                            (tensor_obj.name, np.iinfo(typeMap[tensor_obj.dtype]).min,
                                             np.iinfo(typeMap[tensor_obj.dtype]).max))

        return init_value

    @source_info_decorator()
    def check_barrier(self: any, sync_workspace: any, block_num: any, block_id: any) -> None:
        """
        define a Barrier instance for multi-core synchronization
        :param sync_workspace:
        :param block_num:
        :param block_id:
        :return:
        """
        # check params
        if not self.is_barrier_in_scope():
            TikCheckUtil.raise_error("block_barrier must be used inside for_range and enable multi-core please.")
        TikCheckUtil.check_type_match(
            sync_workspace, Tensor,
            "sync_workspace's type should be Tensor, input type is %s" % type(sync_workspace))
        TikCheckUtil.check_equality(
            sync_workspace.dtype, "int64",
            "sync_workspace's dtype should be int64, input dtype is %s" % sync_workspace.dtype)
        TikCheckUtil.check_equality(
            sync_workspace.scope, scope_gm,
            "sync_workspace's scope should be scope_gm, input scope is %s" % sync_workspace.scope)
        TikCheckUtil.check_equality(
            sync_workspace.is_workspace, True, "sync_workspace's is_workspace param should be True")
        TikCheckUtil.check_equality(
            sync_workspace.is_atomic_add, True, "sync_workspace's is_atomic_add param should be True")
        # check block_num
        if isinstance(block_num, int):
            TikCheckUtil.check_ge(
                block_num, 2, "block_num(%d) should be greater equal 2 when block_arrier is used" % block_num)
            core_num = get_soc_spec("CORE_NUM")
            TikCheckUtil.check_le(block_num, core_num, "param block_num(%d) should be less equal than current chip "
                                                       "core_num(%d) when block_arrier is used" % (block_num, core_num))
            TikCheckUtil.check_equality(block_num, self.for_blk_num,
                                        "param block_num(%s) should be equal to for_range's block_num(%s)"
                                        " param with multi-core" % (block_num, self.for_blk_num))
            # check workspace size
            workspace_ele = reduce_mul(sync_workspace.original_shape) - sync_workspace.offset
            workspace_size = Expr(workspace_ele * DTYPE_SIZE[sync_workspace.dtype]).eval_value()

            # address align is 32B
            require_size = Expr(block_num * 32).eval_value()
            if isinstance(workspace_size, int) and isinstance(require_size, int):
                TikCheckUtil.check_equality(workspace_size, require_size,
                                            "sync_workspace's size application should be equal to %d "
                                            "Byte, input size: %d Byte" % (require_size, workspace_size))
        TikCheckUtil.check_equality(
            str(block_id), str(Var("blockIdx.x", block_id.dtype)),
            "block_id should be var of for_range with multi-core, input is: %s" % str(block_id))

    @source_info_decorator()
    def get_available_buffer_size(self, buffer_scope=None):
        """
        get the available buffer size.
        :param buffer_scope:
        :return:  the buffer size of scope
        """
        check_scope(buffer_scope)
        return self.code_buffer_manager.buffer_aviable()[buffer_scope]

    @source_info_decorator()
    def global_scalar(self, dtype="int64", name="reg_buf", init_value=None):
        """
        create global scalar

        Parameters
        ----------
        dtype : the scalar's type
        init_value : the scalar's init value
        name : scalar's name

        Returns
        -------
        scalar
        """
        # name already has been checked in Scalar func
        new_scalar = Scalar(self, dtype, name, init_value, if_global_scope=True)
        self.global_scalar_list.append(new_scalar)
        return new_scalar

    @source_info_decorator()
    def run_ops(self, feed_dict, outputs_shape=None):
        """
        Start Bbit
        :param feed_dict: input for profiling
        :param outputs_shape:when the output tensor is dynamic shape,
        a static shape should be put into outputs_shape in order
        :return:
        """
        return self._start_model_run((feed_dict, False, False, False), outputs_shape=outputs_shape)

    @source_info_decorator()
    def StartProfiling(self, feed_dict, simulatorlog_path=None, generate_html=False, outputs_shape=None,
                       desc_tensor=None):
        """
        :param feed_dict: input for profiling
        :param simulatorlog_path: where to put log
        :param generate_html: where to put log
        :param outputs_shape:whether generate html or not
        :param desc_tensor: A list is used to specify a special tensor's name for save the gm tensor info.
        a static shape should be put into outputs_shape in order
        :return:

        Parameters
        ----------

        """
        return self._start_model_run((feed_dict, generate_html, True, False),
                                     simulatorlog_path=simulatorlog_path, outputs_shape=outputs_shape,
                                     desc_tensor=desc_tensor)

    @source_info_decorator()
    def gen_cfg(self, feed_dict, outputs_shape=None, desc_tensor=None):
        """
        :param feed_dict: input for profiling
        :param outputs_shape:whether generate html or not
        :param desc_tensor: A list is used to specify a special tensor's name for save the gm tensor info.
        a static shape should be put into outputs_shape in order
        :return:

        Parameters
        ----------

        """

        return self._start_model_run((feed_dict, False, True, True),
                                     outputs_shape=outputs_shape,
                                     desc_tensor=desc_tensor)

    @source_info_decorator()
    @high_level_api_debug_decorator
    def set_printf_params(self, print_workspace_size=128,
                          single_print_length=512,
                          is_printf_to_screen=True,
                          printf_file_path=None):
        """
        optional printf args, can only be set as once and start before printf
        :param print_workspace_size: MB unit [1, 32768]
        :param single_print_length: min 32 + 8 + 16 + 8 , 64 bytes, 32
        :param is_printf_to_screen: whether printf to screen
        :param printf_file_path: where to put printf text
        bytes align. [64, 32G]
        :return:
        """
        # check set_printf_args is defined many times
        if self.has_set_printf_params:
            TikCheckUtil.raise_error("set_printf_params can't be called twice.")
        # check whether has done printf
        if self._has_done_printf:
            TikCheckUtil.raise_error("set_printf_params can't be used after printf")

        self._check_printf_size_args(print_workspace_size, single_print_length)
        # we will use block_num as total_print_workspace_size, per bytes
        shape_of_print_workspace = self._print_workspace_size_in_bytes // DTYPE_SIZE[PRINT_WORKSPACE_DTYPE]
        self._print_workspace = self.Tensor(dtype=PRINT_WORKSPACE_DTYPE, shape=(shape_of_print_workspace,),
                                            scope=scope_gm, name="_print_workspace", is_workspace=True)
        # _print_block_ub add 32 bytes at end to data move used set_as .
        self._print_block_ub = self.Tensor(dtype=PRINT_WORKSPACE_DTYPE, shape=(DEFAULT_PRINT_UB_BYTES,),
                                           scope=scope_ubuf, name="_print_block_ub")
        # use to init print head for each block, set the block_id and set 0.
        self.single_print_length = single_print_length
        self.has_set_printf_params = True

        TikCheckUtil.check_type_match(is_printf_to_screen, bool, "is_printf_to_screen should be bool")
        self.is_printf_to_screen = is_printf_to_screen

        if printf_file_path is not None:
            TikCheckUtil.check_type_match(printf_file_path, str, "printf_file_path should be None or string")
        self.printf_file_path = printf_file_path

    @source_info_decorator()
    def set_tiling_params(self, inputs, tiling_gm):
        """
        set frontend tiling params

        Parameters
        ----------
        inputs : List/Tuple of input Tensor
        tiling_gm : gm Tensor as tiling Tensor
        """
        TikCheckUtil.check_equality(self._has_tiling_gm, False, "set_tiling_params can't be called twice.")

        # check inputs
        TikCheckUtil.check_type_match(inputs, (list, tuple), "inputs can only be list/tuple")
        for tensor in inputs:
            TikCheckUtil.check_type_match(tensor, (Tensor, TensorAddrList),
                "item in inputs can only be Tensor/TensorAddrList")
            TikCheckUtil.check_equality(tensor.scope, scope_gm, "Tensor/TensorAddrList in inputs can only be scope gm")

        # check tiling_gm
        TikCheckUtil.check_type_match(tiling_gm, Tensor, "tiling_gm can only be Tensor")
        TikCheckUtil.check_equality(tiling_gm.scope, scope_gm, "tiling_gm can only be scope gm")
        if tiling_gm.is_workspace or tiling_gm.is_global_tensor:
            TikCheckUtil.raise_error("tiling_gm can not be workspace or global_tensor")

        # check context
        if get_context() is None:
            TikCheckUtil().raise_warning("set_tiling_params does not take effect because context is None")
            return
        elif get_context().get_op_mode() == "dynamic":
            TikCheckUtil().raise_warning("set_tiling_params does not take effect because get_op_mode is dynamic")
            return

        self._has_tiling_gm = True
        tiling_gm._get_last_tensor().is_tiling_tensor = True
        self.tiling_numpy = get_tiling_params(inputs, [tiling_gm, ])

    @source_info_decorator()
    @printf_decorator
    def printf(self, format_string, *arg):
        """
        use to printf for bbit or startProfiling
        :param format_string: used to change display word.
        :param arg: saves the print value. contains at least one agr.
        :return:
        """
        # The atomic add needs to be disabled, data_move is used in printf to move data to workspace(gm).
        self._set_atomic_none()
        # this is to judge whether printf in outside of multi for-range
        if not self._has_printf_out_multi_for and self.for_blk_num == 1:
            self._has_printf_out_multi_for = True
        self._print_bytes_consumed = EACH_PRINT_HEAD_BYTES
        self._init_workspace_in_printf()
        # we will get block idx to block_id
        if self.enable_multi_core is True:
            block_id = self._block_id
        else:
            block_id = 0
        # check format_string, args, block_id is valid
        self._check_printf_args(format_string, arg)
        format_arg_strong_match(format_string, arg)
        shape_of_each_block = self._print_workspace.size // self._print_core_num
        total_bytes_for_each = shape_of_each_block * DTYPE_SIZE[self._print_workspace.dtype]
        # where to start to write, offset
        start_offset_of_each_block = shape_of_each_block * block_id
        # we will use 256 BYTES to move value to gm
        # change block ub to the correct dtype
        # start to while loop and add total size to discuss the situation
        _, print_data_valid_length, real_data_length = gen_print_data_length(arg, self.single_print_length)

        # calculate how many bytes should be used to align
        align_string_bytes = gen_align_num_for_string(len(format_string))

        # valid num bytes
        final_length_without_scalar = EACH_PRINT_HEAD_BYTES + len(format_string) + align_string_bytes + \
                                      len(arg) * PRINT_TL_BYTES + print_data_valid_length
        # check whether total length is larger than MAX_PRINT_LENGTH bytes
        if final_length_without_scalar > self.single_print_length:
            get_error_dict_args("Total print length is larger "
                                "than single_print_length: %s!" % self.single_print_length)

        self._modify_last_print_offset(total_bytes_for_each)

        # this is the first shape of each single print length
        print_start_offset = start_offset_of_each_block + \
                             (PRINT_CORE_HEAD_BYTES + self.last_print_offset - self.single_print_length) // \
                             DTYPE_SIZE[self._print_workspace.dtype]

        print_end_offset = start_offset_of_each_block + (PRINT_CORE_HEAD_BYTES + self.last_print_offset) // \
                           DTYPE_SIZE[self._print_workspace.dtype]
        # step 1: move print head
        self._move_print_head([block_id, len(format_string), real_data_length, len(arg), print_start_offset])

        # step 2: move format-string
        self._move_format_string(format_string, align_string_bytes, print_start_offset)
        # step3: move tlv to print block_ub
        self._move_arg_for_printf(arg, print_start_offset, print_end_offset)
        # that means we will flush the remain ub to workspace
        self._flush_remain_ub_to_workspace(print_start_offset, print_end_offset)
        # restore the settings before printf.
        self._set_atomic_none(is_restore=True)
        # means has done printf
        self._has_done_printf = True
        self.set_high_level_api_state()

    @source_info_decorator()
    def get_left_buffer_size(self, scope):
        """
        get left buffer size
        Parameters
        ----------
        scope : Buffer scope of the Tensor object, that is, buffer space
        where the Tensor object is located:
        -     scope_cbuf: L1 Buffer
        -     scope_ca: L0A Buffer
        -     scope_cb: L0B Buffer
        -     scope_cc: L0C Buffer
        -     scope_ubuf: Unified Buffer (UB)
        Returns
        ----------
        return : None or left_buffer_size
        """
        avaiable_tensor = self.code_buffer_manager.buffer_aviable()
        if scope not in [scope_cbuf, scope_ubuf, scope_ca, scope_cb, scope_cc, scope_cbuf_out]:
            TikCheckUtil.raise_error("The scope must be in [scope_cbuf, scope_ubuf, scope_ca, scope_cb, scope_cc, "
                                     "scope_cbuf_out]")
        left_buffer_size = avaiable_tensor[scope]
        if not isinstance(left_buffer_size, int):
            left_buffer_size = None

        return left_buffer_size

    def init_scope(self, sync_state):
        """
        init new_stmt_scope, if disable sync in new_stmt_scope,
        the new_stmt_scope doesn't support nested call the new_stmt_scope.
        :param sync_state: bool type, False: enable sync, True: disable sync.
        :return: bool
        """
        if self.stmt_scope_stack and self.stmt_scope_stack[-1][0]:
            return False

        self.stmt_scope_stack.append([sync_state, False])
        return True

    def pop_scope(self):
        """
        Pop the top scope.
        """
        if self.stmt_scope_stack:
            self.stmt_scope_stack.pop()

    def set_high_level_api_state(self):
        """
        Update state of including high level api in top scope
        """
        if self.stmt_scope_stack:
            self.stmt_scope_stack[-1][1] = True

    def get_state(self):
        """
        Get sync state and state of including high api level in top scope.
        :return sync state, state of including high api level
        """
        if self.stmt_scope_stack:
            return self.stmt_scope_stack[-1][0], self.stmt_scope_stack[-1][1]

        return False, False

    def check_new_stmt_scope(self):
        """
        check new stmt scope is valid
        :return:None
        """
        sync_state, high_level_api_state = self.get_state()
        if sync_state and high_level_api_state:
            self.pop_scope()
            TikCheckUtil.raise_error("Don't support high level api in new_stmt_scope with disable sync")

    def tensor_(self, tensor_obj):
        """
        create Tensor
        note: use this function to call tik.Tensor inside!!
        """
        TikCheckUtil.check_name_str_valid(tensor_obj.name)
        TikCheckUtil.check_var_in_list(tensor_obj.dtype, TENSOR_SUPPORT_DYTPE,
                                       "Tensor do not support %s" % tensor_obj.dtype)
        # if name is __fake_tensor, scope is gm, it would not appear in cce_args
        if tensor_obj.name == FAKE_TENSOR_NAME and tensor_obj.scope == scope_gm:
            TikCheckUtil.raise_error("gm tensor's name should not be '%s'" % FAKE_TENSOR_NAME)
        TikCheckUtil.check_type_match(tensor_obj.enable_buffer_reuse, bool,
                                      "enable_buffer_reuse should be bool type for Tensor:%s" % tensor_obj.name)
        TikCheckUtil.check_type_match(tensor_obj.is_atomic_add, bool,
                                      "is_atomic_add should be bool type for Tensor:%s" % tensor_obj.name)

        init_value = self._check_atomic_tensor_args(tensor_obj)
        if tensor_obj.no_reuse_list is not None:
            TikCheckUtil.check_equality(tensor_obj.enable_buffer_reuse, True, "please check enable_buffer_reuse param")
            TikCheckUtil.check_type_match(tensor_obj.no_reuse_list, (list, tuple),
                                          "no_reuse_list for Tensor %s should be list or tuple,"
                                          " input type is %s" % (tensor_obj.name, type(tensor_obj.no_reuse_list)))
        if tensor_obj.reuse_list is not None:
            TikCheckUtil.check_equality(tensor_obj.enable_buffer_reuse, True, "please check enable_buffer_reuse param")
            TikCheckUtil.check_type_match(
                tensor_obj.reuse_list, (list, tuple), "reuse_list for Tensor %s should be list or tuple, input type "
                                                      "is %s" % (tensor_obj.name, type(tensor_obj.reuse_list)))

        tmp_tensor = Tensor(self, tensor_obj.dtype, tensor_obj.shape, tensor_obj.scope, tensor_obj.name,
                            enable_buffer_reuse=tensor_obj.enable_buffer_reuse, reuse_list=tensor_obj.reuse_list,
                            is_workspace=tensor_obj.is_workspace, is_atomic_add=tensor_obj.is_atomic_add,
                            max_mem_size=tensor_obj.max_mem_size, init_value=init_value,
                            is_global_tensor=tensor_obj.is_global_tensor, start_addr=tensor_obj.start_addr)
        TikCheckUtil.check_type_match(tensor_obj.is_workspace, bool,
                                      "is_workspace should be bool type for Tensor:%s" % tensor_obj.name)
        TikCheckUtil.check_type_match(tensor_obj.is_global_tensor, bool,
                                      "is_global_tensor should be bool type for Tensor:%s" % tensor_obj.name)

        if tensor_obj.is_workspace and tensor_obj.is_global_tensor:
            TikCheckUtil.raise_error("is_workspace and is_global_tensor cannot be set to True at the same time.")

        if tensor_obj.is_atomic_add:
            # atomic should be gm
            TikCheckUtil.check_equality(tensor_obj.scope, scope_gm,
                                        "Atomic' scope should be scope_gm for Tensor:" +
                                        tensor_obj.name + ", but get:" + tensor_obj.scope)
        self._check_is_workspace(tensor_obj.is_workspace, tensor_obj.scope, tensor_obj.name, tmp_tensor)
        self._check_global_tensor(tensor_obj.is_global_tensor, tensor_obj.scope, tensor_obj.name, tmp_tensor)
        if tmp_tensor.init_value and not tmp_tensor.is_atomic_add:
            self._init_gm_tensor_set.add(tmp_tensor.name)

        if tensor_obj.enable_buffer_reuse:
            if tensor_obj.no_reuse_list:
                self._update_buffer_use_list(tmp_tensor, tensor_obj.no_reuse_list, "no_reuse")
            if tensor_obj.reuse_list:
                self._update_buffer_use_list(tmp_tensor, tensor_obj.reuse_list, "reuse")
            self._buffer_reuse_dict.update({tmp_tensor.name: tmp_tensor.buffer_storage_id})
        return tmp_tensor

    def scalar_(self, dtype="int64", name="reg_buf", init_value=None):
        """
        create scalar
        note: use this function to call tik.Scalar inside!!
        """
        TikCheckUtil.check_name_str_valid(name)
        TikCheckUtil.check_var_in_list(dtype, SCALAR_SUPPORT_DYTPE, "Scalar do not support %s" % dtype)
        return Scalar(self, dtype, name, init_value)

    def scalar_array_(self, dtype="int64", length=1, name="reg_buf", init_value=None):
        """
        create scalar array
        note: use this function to call tik.ScalarArray inside!!
        """
        # check dtype is str
        TikCheckUtil.check_type_match(dtype, str, "dtype should be str")
        TikCheckUtil.check_var_in_list(dtype, SCALAR_SUPPORT_DYTPE, "Scalar do not support %s" % dtype)
        # check name is valid
        TikCheckUtil.check_name_str_valid(name)
        return ScalarArray(self, dtype, length, name, init_value)

    def _move_tl_value(self, print_start_offset, arg):
        """
        use to move tl to print_ub
        :param print_start_offset:
        :param arg:
        :return:
        """
        with self.if_scope(self.current_ub_offset + 32 > 256):
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed.set_as(
                self._print_bytes_consumed + 256)
            self.current_ub_offset.set_as(0)
        self._print_block_ub.reinterpret_cast_to("int64")[
            self.current_ub_offset // 8].set_as(Expr(gen_args_type_num(arg)))
        self.current_ub_offset.set_as(self.current_ub_offset + 8)

        _, actual_length = gen_each_print_data_length(arg)
        self._print_block_ub.reinterpret_cast_to("int64")[
            self.current_ub_offset // 8].set_as(Expr(actual_length))
        self.current_ub_offset.set_as(self.current_ub_offset + 8)

        self._print_block_ub.reinterpret_cast_to("int64")[
            self.current_ub_offset // 8].set_as(Expr(MAGIC_TL_TAG))
        self.current_ub_offset.set_as(self.current_ub_offset + 16)

    def _move_tl_value_in_imm(self, print_start_offset, arg):
        """
        use to move tl to print_ub
        :param print_start_offset:
        :param arg:
        :return:
        """
        if self.current_ub_offset + 32 > 256:
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed += self.current_ub_offset
            self.current_ub_offset = 0
        self._print_block_ub.reinterpret_cast_to(
            "int64")[self.current_ub_offset // 8].set_as(Expr(gen_args_type_num(arg)))
        self.current_ub_offset += 8

        _, actual_length = gen_each_print_data_length(arg)
        self._print_block_ub.reinterpret_cast_to(
            "int64")[self.current_ub_offset // 8].set_as(Expr(actual_length))
        self.current_ub_offset += 8

        self._print_block_ub.reinterpret_cast_to(
            "int64")[self.current_ub_offset // 8].set_as(Expr(MAGIC_TL_TAG))
        self.current_ub_offset += 8
        self.current_ub_offset += 8

    def _move_scalar(self, print_start_offset, arg):
        """
        use to move scalar_value to print_ub
        :param arg:
        :return:
        """
        with self.if_scope(self.current_ub_offset + 32 > 256):
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed.set_as(self._print_bytes_consumed + self.current_ub_offset)
            self.current_ub_offset.set_as(0)
        self._print_block_ub.reinterpret_cast_to(arg.dtype)[self.current_ub_offset // DTYPE_SIZE[arg.dtype]].set_as(arg)
        self.current_ub_offset.set_as(self.current_ub_offset + 32)

    def _move_scalar_array(self, print_start_offset, arg, dtype):
        """
        use to move scalar_array_value to print_ub
        :param arg:
        :param dtype:
        :return:
        """
        dtype_size = DTYPE_SIZE[dtype]
        with self.if_scope(self.current_ub_offset + 32 > 256):
            self.data_move(
                self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed.set_as(self._print_bytes_consumed + self.current_ub_offset)
            self.current_ub_offset.set_as(0)
        self._print_block_ub.reinterpret_cast_to(arg.dtype)[self.current_ub_offset // DTYPE_SIZE[arg.dtype]].set_as(arg)
        self.current_ub_offset.set_as(self.current_ub_offset + dtype_size)

    def _set_print_value(self, print_data_length, arg, offset, print_start_offset):

        data_bytes_in_32_align, actual_data_bytes = print_data_length
        dtype_size = DTYPE_SIZE[arg.dtype]
        # we will use a temp ub to save data move.
        print_ub_for_l0c = self._gen_ub_for_printf(arg.scope, arg.dtype)
        remain_data_element = self.Scalar(init_value=ceil_div(data_bytes_in_32_align, 32))
        with self.for_range(0, ceil_div(actual_data_bytes, 256 * dtype_size)) as i_index:
            self.data_move(print_ub_for_l0c.reinterpret_cast_to(arg.dtype),
                           arg[i_index * 256 + offset], 0, 1, 1, 0, 0)
            with self.for_range(0, 256 * dtype_size // 32) as src_index:
                with self.if_scope(remain_data_element > 0):
                    self._move_print_value(
                        print_start_offset,
                        print_ub_for_l0c.reinterpret_cast_to(arg.dtype)[32 // dtype_size * src_index])
                    remain_data_element.set_as(remain_data_element - 1)

    def _move_tensor(self, print_start_offset, arg, offset, print_data_length):
        """
        use to move tensor_value to print_ub
        Parameters
        ----------
        print_start_offset: start offset
        arg: arg
        offset: offset
        print_data_length: data length

        Returns
        -------

        """
        dtype_size = DTYPE_SIZE[arg.dtype]
        data_bytes_in_32_align, actual_data_bytes = print_data_length
        if arg.scope == scope_ubuf:
            # if ub , just use set_as, to set
            with self.for_range(0, arg.size) as i_index:
                self._move_print_value_to_workspace(print_start_offset, arg[i_index + offset], arg.dtype)
            with self.for_range(0, data_bytes_in_32_align - actual_data_bytes):
                self._move_print_value_to_workspace(print_start_offset, Expr(0), "uint8")
        elif arg.scope == scope_cc:
            self._set_print_value(print_data_length, arg, offset, print_start_offset)
        else:
            # we will make data_nburst as index.
            with self.for_range(0, ceil_div(data_bytes_in_32_align, 32)) as k_index:
                self._move_print_value(print_start_offset, arg[k_index * 32 // dtype_size], offset)

    def _move_tlv_for_not_imm(self, print_start_offset, i, single_print_end_offset, offset):
        print_data_length = gen_each_print_data_length(i)

        with self.if_scope(self.current_ub_offset + print_data_length[0] + self._print_bytes_consumed +
                           PRINT_TL_BYTES + print_start_offset <= single_print_end_offset):
            # first write TL value, 16 bytes per TL
            self._move_tl_value(print_start_offset, i)
            if isinstance(i, (Scalar, Expr)):
                # use reinterpret_cast_to print ub to set
                self._move_scalar(print_start_offset, i)
            elif isinstance(i, ScalarArray):
                with self.for_range(0, i.length) as i_index:
                    self._move_scalar_array(print_start_offset, i[i_index], i.dtype)
                with self.for_range(0, print_data_length[0] - print_data_length[1]):
                    self._move_scalar_array(print_start_offset, Expr(0), "uint8")
            elif isinstance(i, Tensor):
                self._move_tensor(print_start_offset, i, offset, print_data_length)
        with self.else_scope():
            # here we will flush the ub and set
            # current_workspace_offset to end
            self._flush_remain_ub_to_workspace(print_start_offset, single_print_end_offset)

    def _move_scalar_in_imm(self, print_start_offset, arg):
        """
        use to move scalar_value to print_ub
        Parameters
        ----------
        print_start_offset start offset
        arg arg

        Returns None
        -------

        """
        if self.current_ub_offset + 32 > 256:
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed += self.current_ub_offset
            self.current_ub_offset = 0
        self._print_block_ub.reinterpret_cast_to(arg.dtype)[self.current_ub_offset // DTYPE_SIZE[arg.dtype]].set_as(arg)
        self.current_ub_offset += 32

    def _move_scalar_array_in_imm(self, print_start_offset, arg, index):
        """
        use to move scalar_array_value to print_ub
        :param current_offset:
        :param arg:
        :param index:
        :return:
        """
        if self.current_ub_offset + 32 > 256:
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed += self.current_ub_offset
            self.current_ub_offset = 0
        _print_block_ub_tmp = self._print_block_ub.reinterpret_cast_to(
            arg.dtype)[self.current_ub_offset // DTYPE_SIZE[arg.dtype]:]
        for i in range(32 // DTYPE_SIZE[arg.dtype]):
            if index + i < arg.length:
                _print_block_ub_tmp[i].set_as(arg[index + i])
        self.current_ub_offset += 32

    def _move_print_with_scope_cc(self, valid_data_bytes_in_32_align, arg, offset, print_start_offset):
        dtype_size = DTYPE_SIZE[arg.dtype]
        data_nburst = valid_data_bytes_in_32_align // 32
        # we will use a temp ub to save data move.
        print_ub_for_l0c = self._gen_ub_for_printf(arg.scope, arg.dtype)
        for i_index in range(0, ceil_div(valid_data_bytes_in_32_align, 256 * dtype_size)):
            self.data_move(print_ub_for_l0c.reinterpret_cast_to(arg.dtype),
                           arg[i_index * 256 + offset], 0, 1, 1, 0, 0)
            if data_nburst <= 0:
                continue
            for src_index in range(0, 256, 32 // dtype_size):
                self._move_print_value_to_workspace_in_imm(
                    print_start_offset, print_ub_for_l0c.reinterpret_cast_to(arg.dtype)[src_index])
                data_nburst -= 1

    def _move_tensor_in_imm(self, print_start_offset, arg, offset, print_data_length):
        """
        use to move tensor_value to print_ub
        :param current_offset:
        :param arg:
        :param data_nburst:
        :param dtype_size:
        :param offset:
        :param valid_data_bytes_in_32_align:
        :param actual_data_bytes:
        :return:
        """
        valid_data_bytes_in_32_align, actual_data_bytes = print_data_length
        data_nburst = valid_data_bytes_in_32_align // 32
        dtype_size = DTYPE_SIZE[arg.dtype]
        if arg.scope == scope_ubuf:
            # if ub , just use set_as, to set
            # only aicore support ub to ub
            ub_offset = Expr(arg.offset).eval_value()
            if ub_offset is not None and ub_offset % 32 == 0 and \
                get_soc_core_type() != "VectorCore" and not TikSocManager.is_v300_610l_soc():
                for k_index in range(0, data_nburst):
                    self._move_print_value_to_workspace_in_imm(print_start_offset,
                                                               arg[k_index * 32 // dtype_size:], offset)
            else:
                for i_index in range(arg.size):
                    self._move_print_ub_without_scalar(print_start_offset, arg[i_index + offset], arg.dtype)
                for i_index in range(valid_data_bytes_in_32_align - actual_data_bytes):
                    self._move_print_ub_without_scalar(print_start_offset, Expr(0), "uint8")
        elif arg.scope == scope_cc:
            self._move_print_with_scope_cc(valid_data_bytes_in_32_align, arg, offset, print_start_offset)
        else:
            # we will make data_nburst as index.
            for k_index in range(0, data_nburst):
                self._move_print_value_to_workspace_in_imm(print_start_offset, arg[k_index * 32 // dtype_size:], offset)

    def _move_vector_in_imm(self, print_start_offset, arg, offset, print_data_length):
        print_dtype = arg.dtype
        if arg.dtype in VECTOR_PRINTF_DTYPE_MAP:
            print_dtype = VECTOR_PRINTF_DTYPE_MAP.get(arg.dtype)
        #get store_cache ub
        print_ub_for_vector = self._gen_ub_for_printf(arg.scope, arg.dtype)
        reinterpret_ub = print_ub_for_vector.reinterpret_cast_to(print_dtype)

        if arg.scope == scope_vreg or arg.scope == scope_preg:
            # move value to store_cache ub
            self.vector_store(reinterpret_ub, arg, "NORM")
            # move store_cache ub to workspace
            for src_index in range(0, print_data_length[0] // DTYPE_SIZE[print_dtype], 32 // DTYPE_SIZE[print_dtype]):
                self._move_print_value_to_workspace_in_imm(print_start_offset, reinterpret_ub[src_index], offset,
                    is_vector=True)
        elif arg.scope == scope_wreg:
            if arg.dtype == "int24" or arg.dtype == "int48":
                # for int24/int48 wreg, move value to store_cache ub
                result = self._move_unalign_wreg(arg, reinterpret_ub)
            else:
                # for int64 wreg, move value to store_cache ub
                result = self._move_align_wreg(arg)
            data_nburst = print_data_length[0] // len(result) // 32
            # move store_cache ub to workspace
            for vec_dst in result:
                for src_index in range(0, data_nburst):
                    self.vector_store(reinterpret_ub.reinterpret_cast_to(vec_dst.dtype), vec_dst, st_mode="NORM")
                    self._move_print_value_to_workspace_in_imm(print_start_offset,
                        reinterpret_ub[src_index * 32 // DTYPE_SIZE[print_dtype]],
                        offset, is_vector=True)

    def _move_unalign_wreg(self, arg, tmp_ub):
        # for int24/int48 wreg printf
        if arg.dtype == "int24":
            # int24 -> u8/s8/u16/s16
            low_dtype = "uint16"
            high_half_dtype = "int8"
            high_dtype = "int16"
            half_dtype = "uint16"
        else:
            # int48 -> u16/s16/s32
            low_dtype = "int32"
            high_half_dtype = "int16"
            high_dtype = "int32"
            half_dtype = "uint32"

        # get low bits
        vec_even_low = self.Vector(dtype=half_dtype)
        vec_odd_low = self.Vector(dtype=half_dtype)
        if low_dtype == half_dtype:
            # low even
            self.vector_vshr(None, vec_even_low, arg, 0, False, "EVEN")
            # low odd
            self.vector_vshr(None, vec_odd_low, arg, 0, False, "ODD")
        else:
            # st/ld: reinterpret_cast int vreg to uint vreg
            vec_low_cast = self.Vector(dtype=low_dtype)
            # low even
            self.vector_vshr(None, vec_low_cast, arg, 0, False, "EVEN")
            self.vector_store(tmp_ub.reinterpret_cast_to(low_dtype), vec_low_cast, st_mode="NORM")
            self.vector_load(vec_even_low, tmp_ub.reinterpret_cast_to(half_dtype), ld_mode="NORM")
            # low odd
            self.vector_vshr(None, vec_low_cast, arg, 0, False, "ODD")
            self.vector_store(tmp_ub.reinterpret_cast_to(low_dtype), vec_low_cast, st_mode="NORM")
            self.vector_load(vec_odd_low, tmp_ub.reinterpret_cast_to(half_dtype), ld_mode="NORM")

        # get half high bits, and pad 0/1 with cast
        vec_even_high = self.Vector(dtype=half_dtype)
        vec_odd_high = self.Vector(dtype=half_dtype)
        vec_high_half = self.Vector(dtype=high_half_dtype)
        # st/ld: reinterpret_cast int vreg to uint vreg
        vec_high_cast = self.Vector(dtype=high_dtype)
        self.vector_vshr(None, vec_high_half, arg, DTYPE_SIZE[half_dtype] * 8, False, None)
        # high even
        self.vector_cast(None, vec_high_cast, vec_high_half, False, "PART_EVEN")
        self.vector_store(tmp_ub.reinterpret_cast_to(high_dtype), vec_high_cast, st_mode="NORM")
        self.vector_load(vec_even_high, tmp_ub.reinterpret_cast_to(half_dtype), ld_mode="NORM")
        # high odd
        self.vector_cast(None, vec_high_cast, vec_high_half, False, "PART_ODD")
        self.vector_store(tmp_ub.reinterpret_cast_to(high_dtype), vec_high_cast, st_mode="NORM")
        self.vector_load(vec_odd_high, tmp_ub.reinterpret_cast_to(half_dtype), ld_mode="NORM")

        # merge even/odd
        vec_high_front = self.Vector(dtype=half_dtype)
        vec_high_back = self.Vector(dtype=half_dtype)
        vec_low_front = self.Vector(dtype=half_dtype)
        vec_low_back = self.Vector(dtype=half_dtype)
        self.vector_vintlv(vec_high_front, vec_high_back, vec_even_high, vec_odd_high)
        self.vector_vintlv(vec_low_front, vec_low_back, vec_even_low, vec_odd_low)

        # merge high/low
        vec_dst_0 = self.Vector(dtype=half_dtype)
        vec_dst_1 = self.Vector(dtype=half_dtype)
        vec_dst_2 = self.Vector(dtype=half_dtype)
        vec_dst_3 = self.Vector(dtype=half_dtype)
        self.vector_vintlv(vec_dst_0, vec_dst_1, vec_low_front, vec_high_front)
        self.vector_vintlv(vec_dst_2, vec_dst_3, vec_low_back, vec_high_back)

        return [vec_dst_0, vec_dst_1, vec_dst_2, vec_dst_3]

    def _move_align_wreg(self, arg):
        # for int64 wreg printf
        low_dtype = "int32"

        # get low/high bits
        vec_low = self.Vector(dtype=low_dtype)
        vec_high = self.Vector(dtype=low_dtype)
        self.vector_vshr(None, vec_low, arg, 0, False)
        self.vector_vshr(None, vec_high, arg, 32, False)

        # merge high/low
        vec_dst_0 = self.Vector(dtype=low_dtype)
        vec_dst_1 = self.Vector(dtype=low_dtype)
        self.vector_vintlv(vec_dst_0, vec_dst_1, vec_low, vec_high)

        return [vec_dst_0, vec_dst_1]

    def _move_tlv_for_imm(self, print_start_offset, i, offset, print_data_length):
        # here means we could put all the words.
        # put tl value.
        dtype_size = DTYPE_SIZE[i.dtype]
        self._move_tl_value_in_imm(print_start_offset, i)
        data_nburst = print_data_length[0] // 32

        if isinstance(i, (Scalar, Expr)):
            self._move_scalar_in_imm(print_start_offset, i)
        elif isinstance(i, ScalarArray):
            for k_index in range(0, data_nburst):
                self._move_scalar_array_in_imm(print_start_offset, i, k_index * 32 // dtype_size)
        elif isinstance(i, Tensor):
            self._move_tensor_in_imm(print_start_offset, i, offset, print_data_length)
        elif isinstance(i, Vector):
            self._move_vector_in_imm(print_start_offset, i, offset, print_data_length)

    def _check_is_workspace(self, is_workspace, scope, name, tmp_tensor):
        if is_workspace:
            # workspace should be gm
            TikCheckUtil.check_equality(
                scope, scope_gm, "Workspace' scope should be scope_gm for Tensor:" + name + ", but get:" + scope)
            self._workspace_tensor_list.append(tmp_tensor)
            if tmp_tensor.name in self._workspace_tensor_name_set:
                TikCheckUtil.raise_error("Workspace's name: " + tmp_tensor.name + " is already used before")
            self._workspace_tensor_name_set.add(tmp_tensor.name)

    def _check_global_tensor(self, is_global_tensor, scope, name, tmp_tensor):
        if is_global_tensor:
            # is_global_tensor should be gm
            TikCheckUtil.check_equality(
                scope, scope_gm, "global_tensor's scope should be scope_gm for Tensor:" + name + ", but get:" + scope)
            if tmp_tensor.name in self._global_tensor_name_set:
                TikCheckUtil.raise_error("global_tensor's name: " + tmp_tensor.name + " is already used before")
            self._global_tensor_name_set.add(tmp_tensor.name)
            self._global_tensor_list.append(tmp_tensor)

    def _set_atomic_none(self, is_restore=False):
        if TIK_SOC_INFO.soc_name != ASCEND_310:
            if not is_restore and (isinstance(self.atomic_add_value, Scalar) or self.atomic_add_value != 0):
                self.current_atomic_value = self.atomic_add_value
                self.set_atomic_add(0)
            if is_restore and (isinstance(self.current_atomic_value, Scalar) or self.current_atomic_value != 0):
                self.set_atomic_add(self.current_atomic_value)

    def _move_arg_for_printf(self, arg, print_start_offset, single_print_end_offset):
        for i in arg:
            data_bytes_in_32_align, actual_data_bytes = gen_each_print_data_length(i)
            valid_data_bytes_in_32_align = Expr(data_bytes_in_32_align).eval_value()
            if isinstance(i, Tensor) and len(i.dimensions) > 1:
                offset = i.offset
            else:
                offset = 0
            if valid_data_bytes_in_32_align is not None and isinstance(self.current_ub_offset, int) and \
                    isinstance(self._print_bytes_consumed, int):
                # here means we could put all the words.
                self._move_tlv_for_imm(print_start_offset, i, offset, (valid_data_bytes_in_32_align, actual_data_bytes))

            else:
                if isinstance(self.current_ub_offset, int):
                    self.current_ub_offset = self.Scalar(init_value=self.current_ub_offset)
                if isinstance(self._print_bytes_consumed, int):
                    self._print_bytes_consumed = self.Scalar(init_value=self._print_bytes_consumed)
                self._move_tlv_for_not_imm(print_start_offset, i, single_print_end_offset, offset)

    @debug.block_barrier_decorator
    def _block_barrier(self, block_num):
        """
        used to reigster block_barrier_decorator

        Parameters
        ----------
        block_num: need to pass block_num to block_barrier_decorator

        """
        pass
