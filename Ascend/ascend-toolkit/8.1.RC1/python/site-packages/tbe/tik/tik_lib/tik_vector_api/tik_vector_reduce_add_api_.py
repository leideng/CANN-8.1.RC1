#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_reduce_add_api_.py
DESC:     reduce add ops
CREATED:  2021-11-09 9:05
MODIFIED: 2021-11-09 9:05
"""

from collections import namedtuple
from tbe import tvm
from tbe.tik.tik_lib.tik_mask_concat_ import mask_concat
from tbe.common.platform import scope_ubuf
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import compatible_blk_continuous_mask
from tbe.tik.common.tik_ir_builder_util import ForRangeTuple
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.common.tik_get_soc_name import get_rep_size
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_params import PIPE_V
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MASK_VALUE_ZERO
from tbe.tik.tik_lib.tik_params import MAX_VREDUCE_REPEAT_TIMES
from tbe.tik.tik_lib.tik_api_util import check_repeat_times
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_api_util import set_ctrl_counter_mask
from tbe.tik.tik_lib.tik_params import MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MIN_INDEX
from tbe.tik.tik_lib.tik_api_util import reset_ctrl_value
from tbe.tik.tik_lib.tik_expr_convert import type_convert
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import vec_reduce_add_decorator
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_api_ import ReduceOpNano
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_common import ReduceAddCheckParams
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp
from tbe.tik.tik_lib.tik_vector_api.tik_compute_control import ControlOp
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_common import DEFAULT_STRIDE
from tbe.tik.tik_lib.tik_vector_api.tik_vector_reduce_common import DEFAULT_REP_STRIDE


class ReduceAddOp:
    """
    reduce add op
    """

    def __init__(self, tik_instance, reduce_api):
        super(ReduceAddOp, self).__init__()
        self.tik_instance = tik_instance
        self.control_op = ControlOp(mask=reduce_api.mask, repeat_times=reduce_api.repeat_times)
        self.work_tensor = reduce_api.work_tensor
        self.src_tensor_op = TensorOp(reduce_api.src, DEFAULT_STRIDE, reduce_api.src_rep_stride, "src")
        self.work_tensor_op = TensorOp(reduce_api.work_tensor, DEFAULT_STRIDE, DEFAULT_REP_STRIDE, "work_tensor")
        self.check_params = (reduce_api.dst, reduce_api.src, self.work_tensor, self.control_op, self.tik_instance,
                             reduce_api.src_rep_stride)
        self.reduce_check_obj = ReduceAddCheckParams(self.check_params)
        self.mask_o = None
        self.one_blk_size = get_block_size()
        self.one_rep_size = get_rep_size()

    def first_add(self, mask_o, dst, src):
        """
        first_add
        :param mask_o:
        :param dst:
        :param src:
        :return:
        """
        if TikSocManager.is_v100_soc():
            elements_per_block = self.one_blk_size // DTYPE_SIZE[src.dtype]
            # in order to align the first address of dst, 32B align
            max_repeat_times = MAX_REPEAT_TIMES - elements_per_block + 1
        else:
            max_repeat_times = MAX_REPEAT_TIMES
        for_times = self.control_op.repeat_times // max_repeat_times
        left_repeat_times = self.control_op.repeat_times % max_repeat_times

        def first_add_full_repeat(cur_index):
            src_start_index = cur_index * max_repeat_times * \
                              ((self.src_tensor_op.rep_stride * self.one_blk_size) //
                               DTYPE_SIZE[src.dtype])
            self.gen_vec_reduce_add_vcadd_part_code(
                (mask_o, "normal", dst, src, max_repeat_times, self.src_tensor_op.rep_stride),
                dst_offset=cur_index * max_repeat_times,
                src_offset=src_start_index, is_mask_o=True)

        def first_add_left_repeat():
            src_index = for_times * max_repeat_times * \
                        ((self.src_tensor_op.rep_stride * self.one_blk_size) //
                         DTYPE_SIZE[src.dtype])
            self.gen_vec_reduce_add_vcadd_part_code(
                (mask_o, "normal", dst, src, left_repeat_times, self.src_tensor_op.rep_stride),
                dst_offset=for_times * max_repeat_times, src_offset=src_index,
                is_mask_o=True)

        if is_immediate_number(self.control_op.repeat_times):
            for index in range(for_times):
                first_add_full_repeat(index)
            if left_repeat_times > 0:
                first_add_left_repeat()
        else:
            for_range_tuple_ = ForRangeTuple(begint=0, endt=for_times, name="i", thread_num=1,
                                             thread_type="whole", block_num=1, dtype="int32", for_type="serial")
            with self.tik_instance.for_range_(for_range_tuple_) as index:
                first_add_full_repeat(index)
            with self.tik_instance.if_scope_(left_repeat_times > 0):
                first_add_left_repeat()

    def second_add(self, pre_elements, dst, src):
        """
        second add
        :param pre_elements:
        :param dst:
        :param src:
        :return:
        """
        operator_byte_size = get_bit_len(src.dtype) // ONE_BYTE_BIT_LEN
        elements_per_repeat = self.one_rep_size // operator_byte_size
        vcadd_mask = pre_elements % elements_per_repeat
        tmp_repeat_times = pre_elements // elements_per_repeat

        def second_add_with_left_repeat():
            if not is_immediate_number(tmp_repeat_times) or tmp_repeat_times != 0:
                self.gen_vec_reduce_add_vcadd_part_code(
                    (elements_per_repeat, "normal", dst, src,
                     tmp_repeat_times, BLK_NUM_PER_REP))
            # in order to align the first address of dst, 32B align
            tmp_dst = self.tik_instance.Tensor(dtype=dst.dtype, shape=(1,), scope=scope_ubuf, name="tmp_dst")
            self.gen_vec_reduce_add_vcadd_part_code(
                (vcadd_mask, "normal", tmp_dst, src,
                 MIN_REPEAT_TIMES, BLK_NUM_PER_REP),
                src_offset=elements_per_repeat * tmp_repeat_times)
            dst.set_as(tmp_dst[MIN_INDEX], dst_offset=tmp_repeat_times)

        if TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc() or TikSocManager.is_910b_soc():
            default_repeat = 1
            self.gen_vec_reduce_add_vcadd_part_code(
                (pre_elements, "counter", dst, src,
                 default_repeat, BLK_NUM_PER_REP))
        else:
            if is_immediate_number(vcadd_mask):
                if vcadd_mask == 0:
                    self.gen_vec_reduce_add_vcadd_part_code(
                        (elements_per_repeat, "normal", dst, src,
                         tmp_repeat_times, BLK_NUM_PER_REP))
                else:
                    second_add_with_left_repeat()
            else:
                with self.tik_instance.if_scope_(vcadd_mask == MASK_VALUE_ZERO):
                    self.gen_vec_reduce_add_vcadd_part_code(
                        (elements_per_repeat, "normal", dst, src,
                         tmp_repeat_times, BLK_NUM_PER_REP))
                with self.tik_instance.else_scope_():
                    second_add_with_left_repeat()
        return ceil_div(pre_elements, elements_per_repeat)

    def final_add(self, pre_elements, dst, work_tensor):
        """
        final add
        :param pre_elements:
        :param dst:
        :param work_tensor:
        :return: None
        """
        if is_immediate_number(pre_elements):
            if pre_elements == 1:
                # move result from work_tensor to dst
                dst[MIN_INDEX].set_as(work_tensor[MIN_INDEX])
            elif pre_elements > 1:
                self.gen_vec_reduce_add_vcadd_part_code(
                    (pre_elements, "normal", dst, work_tensor,
                     MIN_REPEAT_TIMES, BLK_NUM_PER_REP))
        else:
            with self.tik_instance.if_scope_(pre_elements == 1):
                # move result from work_tensor to dst
                dst[MIN_INDEX].set_as(work_tensor[MIN_INDEX])
            with self.tik_instance.else_scope_():
                self.gen_vec_reduce_add_vcadd_part_code(
                    (pre_elements, "normal", dst, work_tensor,
                     MIN_REPEAT_TIMES, BLK_NUM_PER_REP))

    def gen_vec_reduce_add_vcadd_part_code(self, reduce_add_params, dst_offset=0,
                                           src_offset=0, is_mask_o=False):
        """
        generate vcadd code for instruction vec_reduce_add
        """
        vcadd_part_code_params = namedtuple("ReduceAddParams",
                                            "mask, mask_mode, dst, src, repeat_times, src_rep_stride")
        params_ins = vcadd_part_code_params(*reduce_add_params)

        # check repeat times
        check_repeat_times(params_ins.repeat_times)
        # check mask and get mask_o
        if is_mask_o:
            mask_o = params_ins.mask
        else:
            mask_o = mask_concat(self.tik_instance, params_ins.mask,
                                 mask_mode=params_ins.mask_mode,
                                 tensor_bit_len=get_bit_len(params_ins.src.dtype))
        # gen
        config = [params_ins.repeat_times, DEFAULT_STRIDE, DEFAULT_STRIDE,
                  params_ins.src_rep_stride]

        if TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc():
            config.append(0)
            config.append(0)
        elif TikSocManager.is_nano_soc():
            config.append(0)

        if params_ins.mask_mode == "counter":
            orig_ctrl = set_ctrl_counter_mask(self.tik_instance)
        with self.tik_instance.new_scope():
            # 8 Block/repeat, 32Byte/Block
            src_extent_param = (params_ins.repeat_times - 1) * params_ins.src_rep_stride + \
                               (8 - 1) * DEFAULT_STRIDE + 1
            src_extent = Expr(src_extent_param * 32)
            dst_extent = Expr(params_ins.repeat_times * DEFAULT_STRIDE *
                              get_bit_len(params_ins.src.dtype)
                              // ONE_BYTE_BIT_LEN)
            # when repeat time >1 and the count of dst write element is not
            # the multi of self.one_blk_size
            dst_extent = Expr(ceil_div(dst_extent, self.one_blk_size) * self.one_blk_size)
            instr = tvm.call_extern(params_ins.dst.dtype, "vcadd",
                                    params_ins.dst.access_ptr("w", extent=dst_extent.get(),
                                                              offset=Expr(dst_offset).get()
                                                              ),
                                    params_ins.src.access_ptr("r",
                                                              extent=src_extent.get(),
                                                              offset=Expr(src_offset).get()
                                                              ),
                                    *type_convert(config))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)
        # reset CTRL SPR as orig_ctrl
        if params_ins.mask_mode == "counter":
            reset_ctrl_value(self.tik_instance, orig_ctrl)

    def multiple_additions(self, mask_o):
        """
        multiple additions
        :param mask_o:
        :return:
        """
        # first add
        self.first_add(mask_o, self.reduce_check_obj.work_tensor,
                       self.reduce_check_obj.src)
        # second add
        second_result_num = self.second_add(self.control_op.repeat_times, self.reduce_check_obj.work_tensor,
                                            self.reduce_check_obj.work_tensor)
        # the final add
        self.final_add(second_result_num, self.reduce_check_obj.dst, self.reduce_check_obj.work_tensor)

    def vcadd_910b_code_emit(self, instr_emit_params):
        """
        vcadd code emit
        Parameters
        ----------
        instr_emit_params: contain repeat times, emit config, dst offset, src offset, mask

        Returns
        -------

        """
        instr_emit_obj = namedtuple("InstrEmitParams", "vcadd_repeat_time src config dst_offset src_offset mask_o")
        instr_emit_params_ins = instr_emit_obj(*instr_emit_params)
        with self.tik_instance.new_scope():
            # 8 Block/repeat, 32Byte/Block
            src_extent_param = (instr_emit_params_ins.vcadd_repeat_time - 1) * self.src_tensor_op.rep_stride + \
                               (ONE_BYTE_BIT_LEN - 1) * DEFAULT_STRIDE + 1
            src_extent = Expr(src_extent_param * self.one_blk_size)
            # 910b env vec_reduce_add only one element to save result
            dst_extent = Expr(DTYPE_SIZE[self.reduce_check_obj.dst.dtype])
            instr = tvm.call_extern(self.reduce_check_obj.dst.dtype, "vcadd",
                                    self.reduce_check_obj.dst.access_ptr(
                                        "w", extent=dst_extent.get(),
                                        offset=Expr(instr_emit_params_ins.dst_offset).get()
                                    ),
                                    instr_emit_params_ins.src.access_ptr(
                                        "r", extent=src_extent.get(),
                                        offset=Expr(instr_emit_params_ins.src_offset).get()
                                    ),
                                    *type_convert(instr_emit_params_ins.config))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *instr_emit_params_ins.mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    def sum_work_tensor_elements_to_dst(self):
        """
        910b env, add work tensor val to dst
        Parameters
        ----------

        Returns
        -------

        """
        vcadd_repeat_times = 1
        reduce_mode = 0
        dst_offset = 0
        src_offset = 0
        mask_o = mask_concat(self.tik_instance, ceil_div(self.control_op.repeat_times, MAX_REPEAT_TIMES),
                             mask_mode="normal",
                             tensor_bit_len=get_bit_len(self.reduce_check_obj.work_tensor.dtype))
        # gen
        config = [vcadd_repeat_times, DEFAULT_STRIDE, DEFAULT_STRIDE,
                  self.src_tensor_op.rep_stride, reduce_mode]

        self.vcadd_910b_code_emit((vcadd_repeat_times, self.reduce_check_obj.work_tensor,
                                  config, dst_offset, src_offset, mask_o))

    def gen_reduce_add_910b(self, vcadd_repeat_time, dst, index=0, src_offset=0):
        """
        910b env, mode = 1, one acc_val add
        Parameters
        ----------
        dst: dst tensor or work tensor
        vcadd_repeat_time: vcadd repeat time
        index: dst offset index
        src_offset: src offset

        Returns
        -------

        """

        reduce_mode = 1
        # gen
        config = [vcadd_repeat_time, DEFAULT_STRIDE, DEFAULT_STRIDE,
                  self.src_tensor_op.rep_stride, reduce_mode]
        dst_offset = index * MAX_REPEAT_TIMES
        self.vcadd_910b_code_emit(
            (vcadd_repeat_time, self.reduce_check_obj.src, config, dst_offset, src_offset, self.mask_o))
        self.get_acc_val_register_val(dst, index)

    def get_acc_val_register_val(self, dst, dst_offset=0):
        """
        get acc val val save to dst
        Parameters
        ----------
        dst_offset: dst offset
        dst: dst tensor

        Returns
        -------

        """
        with self.tik_instance.new_stmt_scope(disable_sync=True):
            self.tik_instance.set_flag("PIPE_V", "PIPE_S", 0)
            self.tik_instance.wait_flag("PIPE_V", "PIPE_S", 0)
            acc_val = self.tik_instance.scalar_("int64", name="acc_val")
            self.tik_instance.emit(tvm.call_extern(acc_val.dtype, "reg_set", acc_val.get(),
                                                   tvm.call_extern(acc_val.dtype, "get_acc_val")))

            dst.set_as(acc_val.reinterpret_cast_to(self.reduce_check_obj.dst.dtype), dst_offset=dst_offset)

    def gen_vec_reduce_add_910b(self):
        """
        when soc is 910b, The Accumulated value is obtained from the ACC_VAL register.
        Parameters
        ----------

        Returns
        -------

        """
        max_repeat_times = MAX_REPEAT_TIMES
        for_loop_time = self.control_op.repeat_times // max_repeat_times
        left_repeat_time = self.control_op.repeat_times % max_repeat_times
        max_repeat_offset = max_repeat_times * ((self.src_tensor_op.rep_stride * self.one_blk_size) //
                                                DTYPE_SIZE[self.reduce_check_obj.src.dtype])
        with self.tik_instance.if_scope_(self.control_op.repeat_times <= max_repeat_times):
            self.gen_reduce_add_910b(self.control_op.repeat_times, self.reduce_check_obj.dst)
        with self.tik_instance.else_scope_():
            with self.tik_instance.for_range(begint=0, endt=for_loop_time) as cur_index:
                src_start_index = cur_index * max_repeat_offset
                self.gen_reduce_add_910b(
                    max_repeat_times, self.reduce_check_obj.work_tensor,
                    index=cur_index, src_offset=src_start_index)
            with self.tik_instance.if_scope_(left_repeat_time > 0):
                src_index = for_loop_time * max_repeat_offset
                self.gen_reduce_add_910b(
                    left_repeat_time, self.reduce_check_obj.work_tensor,
                    index=for_loop_time, src_offset=src_index)
            self.sum_work_tensor_elements_to_dst()

    def gen_vec_reduce_add_code_when_repeat_imme(self, mask_o):
        """
        when repeat_times is imme, generate vec_reduce_add code
        """
        if self.control_op.repeat_times == 1:
            # first add
            self.gen_vec_reduce_add_vcadd_part_code(
                (mask_o, "normal", self.reduce_check_obj.dst,
                 self.reduce_check_obj.src, self.control_op.repeat_times, self.src_tensor_op.rep_stride),
                is_mask_o=True)
        else:
            self.multiple_additions(mask_o)

    def gen_vec_reduce_add_code_v300(self, reduce_add_params,
                                     dst_offset=0, src_offset=0):
        """
        generate vcadd code for instruction vec_reduce_add in v300
        """
        vcadd_part_code_params = namedtuple("ReduceAddParams",
                                            "mask, mask_mode, dst, src, repeat_times, src_rep_stride")
        params_ins_v300 = vcadd_part_code_params(*reduce_add_params)

        # check repeat times
        check_repeat_times(params_ins_v300.repeat_times, max_repeat_times=MAX_VREDUCE_REPEAT_TIMES)
        # check mask and get mask_o
        mask = params_ins_v300.mask
        # gen
        config_v300 = [params_ins_v300.repeat_times, DEFAULT_STRIDE, DEFAULT_STRIDE,
                       params_ins_v300.src_rep_stride]

        if params_ins_v300.mask_mode == "counter":
            orig_ctrl = set_ctrl_counter_mask(self.tik_instance)
        with self.tik_instance.new_scope():
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
            src_extent_param = (params_ins_v300.repeat_times - 1) * params_ins_v300.src_rep_stride + \
                               (8 - 1) * DEFAULT_STRIDE + 1
            src_extent = Expr(src_extent_param * 32)
            dst_extent = Expr(get_bit_len(params_ins_v300.src.dtype) // ONE_BYTE_BIT_LEN * 16)
            instr = tvm.call_extern(params_ins_v300.dst.dtype, "vec_reduce_add",
                                    params_ins_v300.dst.access_ptr("w", extent=dst_extent.get(),
                                                                   offset=Expr(dst_offset).get()),
                                    params_ins_v300.src.access_ptr("r",
                                                                   extent=src_extent.get(),
                                                                   offset=Expr(src_offset).get()),
                                    *type_convert(config_v300))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *mask))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)
        if params_ins_v300.mask_mode == "counter":
            reset_ctrl_value(self.tik_instance, orig_ctrl)

    def gen_code(self):
        """
        gen code
        :return: None
        """

        with self.tik_instance.context.freeze():
            if TikSocManager.is_v300_610l_soc() or TikSocManager.is_v210_vec_soc():
                self.gen_vec_reduce_add_code_v300(
                    (self.mask_o, "normal", self.reduce_check_obj.dst, self.reduce_check_obj.src,
                     self.control_op.repeat_times, self.src_tensor_op.rep_stride))
            elif TikSocManager.is_910b_soc():
                self.gen_vec_reduce_add_910b()
            else:
                if is_immediate_number(self.control_op.repeat_times):
                    self.gen_vec_reduce_add_code_when_repeat_imme(self.mask_o)
                else:
                    self._gen_vec_reduce_add_code_when_repeat_scalar(self.mask_o)

    @vec_reduce_add_decorator
    def run_add_all(self):
        """
        check all and gen code
        :return:
        """
        self.reduce_check_obj.check_reduce_add_params()
        # check mask and get mask_o
        self.mask_o = mask_concat(self.tik_instance, self.control_op.mask,
                                  tensor_bit_len=get_bit_len(self.reduce_check_obj.src.dtype))
        self.reduce_check_obj.all_add_reduce_check()
        self.gen_code()
        self.tik_instance.set_high_level_api_state()

    def _gen_vec_reduce_add_code_when_repeat_scalar(self, mask_o):
        """
        when repeat_times is scalar, generate vec_reduce_add code
        """
        with self.tik_instance.if_scope_(self.control_op.repeat_times == 1):
            # first add
            self.gen_vec_reduce_add_vcadd_part_code(
                (mask_o, "normal", self.reduce_check_obj.dst, self.reduce_check_obj.src,
                 self.control_op.repeat_times, self.src_tensor_op.rep_stride),
                is_mask_o=True)
        with self.tik_instance.if_scope_(self.control_op.repeat_times > 1):
            self.multiple_additions(mask_o)


class ReduceAddOpNano(ReduceAddOp):
    """
    vec_reduce_add nano compatible
    """
    def __init__(self, tik_instance, reduce_api):
        super(ReduceAddOpNano, self).__init__(tik_instance, reduce_api)
        self.check_params = (reduce_api.dst, reduce_api.src, self.work_tensor, self.control_op, self.tik_instance,
                             reduce_api.src_rep_stride//2)
        self.reduce_check_obj = ReduceAddCheckParams(self.check_params)
        self.mask_o = reduce_api.mask

    @vec_reduce_add_decorator
    def run_add_all(self):
        """
        check all and gen code
        """
        self.reduce_check_obj.check_reduce_add_params()
        self.reduce_check_obj.all_add_reduce_check()
        self.gen_code()
        self.tik_instance.set_high_level_api_state()


class ReduceOpNanoCompatible:
    """
    vec_reduce_xxx nano compatible
    """
    def __init__(self, tik_instance, name, reduce_op, reduce_api):
        self.tik_instance = tik_instance
        self.mask = reduce_api.mask
        self.dst = reduce_api.dst
        self.src = reduce_api.src
        self.work_tensor = reduce_api.work_tensor
        self.repeat_times = reduce_api.repeat_times
        self.src_rep_stride = reduce_api.src_rep_stride * 2
        self.reduce_op = reduce_op
        self.name = name
        self.cal_index = ""
        if self.name != "vcadd":
            self.cal_index = reduce_api.cal_index
        self.data_size = DTYPE_SIZE[self.dst.dtype]
        self.one_blk_ele = tik_instance.Scalar("int32", init_value=get_block_size() // self.data_size)
        self.one_rep_ele = tik_instance.Scalar("int32", init_value=get_rep_size() // self.data_size)
        self.def_blk_stride = 1
        self.def_rep_stride = 0

    def run_add_all(self):
        """
        vec_reduce_add
        """
        vec_reduce_temp_ub = self.tik_instance.Tensor(self.dst.dtype, (self.one_blk_ele * 2, ),
                                                      self.dst.scope, "vec_reduce_temp_ub")
        self.tik_instance.vec_dup(self.one_blk_ele * 2, vec_reduce_temp_ub, 0, 1, 1)
        mask_o = mask_concat(self.tik_instance, self.mask, tensor_bit_len=get_bit_len(self.src.dtype))
        mask_o1, mask_o2 = compatible_blk_continuous_mask(mask_o, self.data_size)
        # step 1: use compatible API to calculate the data on the left of each repeat
        # and write the result to elements 0 of temp_ub.
        with self.tik_instance.if_scope(mask_o1[1] != 0):
            reduce_add_api1 = self.reduce_op(mask_o1, vec_reduce_temp_ub, self.src, self.work_tensor,
                                             self.repeat_times, self.src_rep_stride)
            reduce_add_obj1 = ReduceAddOpNano(self.tik_instance, reduce_add_api1)
            reduce_add_obj1.run_add_all()

        # step 2: use compatible API to calculate the data on the right of each repeat,
        # and write the result to elements 8 of temp_ub.
        with self.tik_instance.if_scope(mask_o2[1] != 0):
            reduce_add_api2 = self.reduce_op(mask_o2,
                                             vec_reduce_temp_ub[self.one_blk_ele:], self.src[self.one_rep_ele:],
                                             self.work_tensor, self.repeat_times, self.src_rep_stride)
            reduce_add_obj2 = ReduceAddOpNano(self.tik_instance, reduce_add_api2)
            reduce_add_obj2.run_add_all()

        # step 3: compare elements 0 and 8 in temp_ub and write the result to dst.
        # Therefore, set mask to 1, that is, elements 0 and 8.
        self.tik_instance.vec_add(1,
                                  self.dst, vec_reduce_temp_ub, vec_reduce_temp_ub[self.one_blk_ele:],
                                  1, self.def_rep_stride, self.def_rep_stride, self.def_rep_stride)

    def run_all(self):
        """
        vec_reduce_max & vec_reduce_min
        """
        # The calculation is performed in three steps, each step offsets a block.
        vec_reduce_temp_ub = self.tik_instance.Tensor(self.dst.dtype, (self.one_blk_ele * 3, ),
                                                      self.dst.scope, "vec_reduce_temp_ub")
        self.tik_instance.vec_dup(self.one_blk_ele * 3, vec_reduce_temp_ub, 0, 1, self.def_rep_stride)
        mask_o = mask_concat(self.tik_instance, self.mask, tensor_bit_len=get_bit_len(self.src.dtype))
        mask_o1, mask_o2 = compatible_blk_continuous_mask(mask_o, self.data_size)

        # step 1: use compatible API to calculate the data on the left of each repeat
        # and write the result to elements 0 and 1 of temp_ub.
        with self.tik_instance.if_scope(mask_o1[1] != 0):
            reduce_add_api1 = self.reduce_op(self.name, mask_o1, vec_reduce_temp_ub, self.src, self.work_tensor,
                                             self.repeat_times, self.src_rep_stride, self.cal_index)
            reduce_add_obj1 = ReduceOpNano(self.tik_instance, reduce_add_api1)
            reduce_add_obj1.run_all()

        # step 2: use compatible API to calculate the data on the right of each repeat,
        # and write the result to elements 8 and 9 of temp_ub.
        with self.tik_instance.if_scope(mask_o2[1] != 0):
            reduce_add_api2 = self.reduce_op(self.name, mask_o2,
                                             vec_reduce_temp_ub[self.one_blk_ele:], self.src[self.one_rep_ele:],
                                             self.work_tensor, self.repeat_times, self.src_rep_stride, self.cal_index)
            reduce_add_obj2 = ReduceOpNano(self.tik_instance, reduce_add_api2)
            reduce_add_obj2.run_all()

        # step 3: compare elements 0 and 8 in temp_ub and write the result to elements 16 and 17 in temp_ub.
        # Therefore, set mask to 0x101, that is, elements 0 and 8.
        if self.name == "vcmax":
            self.tik_instance.vcmax([0, 0x101], vec_reduce_temp_ub[self.one_blk_ele * 2:], vec_reduce_temp_ub,
                                    self.def_blk_stride, self.def_rep_stride, self.def_blk_stride, self.def_rep_stride)
        else:
            self.tik_instance.vcmin([0, 0x101], vec_reduce_temp_ub[self.one_blk_ele * 2:], vec_reduce_temp_ub,
                                    self.def_blk_stride, self.def_rep_stride, self.def_blk_stride, self.def_rep_stride)

        self.dst[0].set_as(vec_reduce_temp_ub[self.one_blk_ele * 2])

        if self.cal_index:
            with self.tik_instance.new_stmt_scope():
                # self.one_blk_ele * 2 -> the best value, self.one_blk_ele * 2 + 1 -> the best value idx
                # num1 -> idx of the best value on temp_ub: 0 or 8
                idx = self.one_blk_ele * 2 + 1
                num1 = self.tik_instance.Scalar("uint16",
                                                init_value=vec_reduce_temp_ub[idx].reinterpret_cast_to("uint16"))
                # num1 + 1 -> idx of the best value idx on temp_ub: 1 or 9; num2 -> idx of the best value on src
                num2 = self.tik_instance.Scalar("uint16",
                                                init_value=vec_reduce_temp_ub[num1 + 1].reinterpret_cast_to("uint16"))
                # num3 = idx of the best value on src add offset
                num3 = self.tik_instance.Scalar("uint16",
                                                init_value=(num2 + num1 / self.one_blk_ele * self.one_rep_ele))
                mast_value_idx = self.tik_instance.Scalar('int8', init_value=1)
                self.dst[mast_value_idx].set_as(num3.reinterpret_cast_to(self.dst.dtype))
