#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     fixpipe.py
DESC:     provide tiling method
CREATED:  2020-4-23 21:12:13
MODIFIED: 2021-11-18 18:14:00
"""

import math
import itertools
from collections import namedtuple
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_gm
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_cbuf
from tbe.common.platform.platform_info import api_check_support

from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.api.cube.reindex import ReIndexProxy
from tbe.tik.api.cube.fixpipe_info import FixpipeInfo
from tbe.tik.api.cube.fixpipe_info import FixpipeTileInfo
from tbe.tik.common.common_util import get_l0c_align
from tbe.tik.common.common_util import check_address_align
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import ceil_div
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_check_util import print_error_msg
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import MIN_NBURST
from tbe.tik.tik_lib.tik_params import MAX_NBURST_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_BURST_LEN_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import DEFAULT_STRIDE
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager


def _get_ub_used(fixpipe_config, tile_element_count, out_dtype, l1_out_dtype, actual_db_thread_num):
    l1out_tile_size = tile_element_count * DTYPE_SIZE.get(l1_out_dtype) * actual_db_thread_num
    vconv_merge_channel = False
    if l1_out_dtype == "int32" and out_dtype in ("uint8", "int8"):
        vconv_merge_channel = True
    ub_used = l1out_tile_size
    if fixpipe_config.has_ele_wise_bias():
        # element-wise-add apply two tensor(shape is tile_element_count)
        ub_used += tile_element_count*DTYPE_SIZE.get(l1_out_dtype)*2 * \
                   actual_db_thread_num
    if l1_out_dtype != out_dtype:
        deq_dtype_size = DTYPE_SIZE.get("float16")
        ub_used += tile_element_count * deq_dtype_size * actual_db_thread_num
        if isinstance(fixpipe_config.deq_value, Tensor):
            # apply tensor to move deqscale(float16) from l1 to ub
            ub_used += 16*deq_dtype_size
    if vconv_merge_channel:
        ub_used += tile_element_count * DTYPE_SIZE.get(out_dtype) * actual_db_thread_num

    return ub_used


def get_fixpipe_info(l1out_column, l1out_dtype, out_dtype, fixpipe_config, nums):
    """
    get fixpipe info
    """
    _, bias_tail_duplicate_time, bias_block = \
        get_bias_size(fixpipe_config, l1out_column, l1out_dtype)
    l1out_thread_num, howo_thread_num, l1out_tile_blocks, howo_tile_blocks = nums
    vconv_merge_channel = False
    if l1out_dtype == "int32" and out_dtype in ("uint8", "int8"):
        vconv_merge_channel = True
    fixpipe_tile_cls = namedtuple("FixpipeTileInfo_v2", ["l1out_blocks", "l1out_tile_blocks", "l1out_thread_num",
                                                         "howo_blocks", "howo_tile_blocks", "howo_thread_num",
                                                         "bias_blocks", "bias_repeat", "bias_tail_repeat",
                                                         "has_bias", "has_deq", "vconv_merge_channel"])
    fixpipe_tile = fixpipe_tile_cls(
        l1out_column, l1out_tile_blocks, l1out_thread_num,
        fixpipe_config.howo_blocks, howo_tile_blocks, howo_thread_num,
        bias_block, bias_block // 8, bias_tail_duplicate_time,
        fixpipe_config.has_bias(), l1out_dtype != out_dtype, vconv_merge_channel
    )
    return FixpipeTileInfo(fixpipe_tile)


def get_bias_size(fixpipe_config, l1out_column, l1out_dtype):
    """
    get bias size
    """
    if not fixpipe_config.has_bias():
        bias_size = 0
    else:
        bias_size = l1out_column * fixpipe_config.frac_len * DTYPE_SIZE.get(l1out_dtype)
    bias_vec_block_size = 16 if l1out_dtype == "float16" else 8
    bias_block = bias_size // bias_vec_block_size
    real_bias_size = bias_block // 8 * 256
    bias_tail_duplicate_time = 0
    if bias_block % 8 > 0:
        # we should broadcast bias to make use of vector
        bias_tail_duplicate_time = 8 // bias_block
        real_bias_size += 256
    return real_bias_size, bias_tail_duplicate_time, bias_block


def gen_fixpipe_tiling(l1out_column, l1out_dtype, out_dtype, tik_instance, fixpipe_config):
    """
    get fixpipe tiling
    """

    tilings = []
    l1out_min_column = 1
    if l1out_dtype == "int32" and out_dtype in ("uint8", "int8"):
        # at least 2 column in s32 s8 case
        l1out_min_column = 2

    real_bias_size, _, _ = get_bias_size(fixpipe_config, l1out_column, l1out_dtype)

    for nums in itertools.product(range(1, 2), range(1, 2),
                                 range(l1out_min_column, l1out_column + 1), range(1, fixpipe_config.howo_blocks + 1)):
        if nums[0] > math.ceil(l1out_column / nums[2]):
            continue
        if nums[1] > math.ceil(fixpipe_config.howo_blocks / nums[3]):
            continue
        actual_db_thread_num = max(
            nums[0], nums[1], tik_instance.buffer_num if tik_instance.is_double_buffer_for_loop else 1)
        tile_element_count = nums[2] * (16 + nums[3] * 16 * 16)
        # round to 512 byte, thus it's safe to vector simd instr
        tile_element_count = math.ceil(tile_element_count / (512 // DTYPE_SIZE.get(l1out_dtype))) * \
                             (512 // DTYPE_SIZE.get(l1out_dtype))
        ub_used = _get_ub_used(fixpipe_config, tile_element_count, out_dtype, l1out_dtype,
                               actual_db_thread_num)
        if fixpipe_config.has_bias():
            # don't reuse space to avoid rw bankconflit
            ub_used += real_bias_size + tile_element_count * DTYPE_SIZE.get(l1out_dtype) * \
                       actual_db_thread_num * actual_db_thread_num
        if ub_used > tik_instance.d_profiling.get_unified_buffer_size():
            continue
        # if no bias, deq does not need vector
        tile_info = get_fixpipe_info(l1out_column, l1out_dtype, out_dtype, fixpipe_config, nums)
        tilings.append(tile_info)
    best_tiling = sorted(tilings)[0]
    return best_tiling


# @cond
class FixpipeImpl:
    """
    implement class of fixpipe
    """
    def __init__(self, tik_instance):
        """
        init of fixpipe.
        """
        self.tik_instance = tik_instance

    @staticmethod
    def _check_deqtype(quantize_params, fixpipe_config):
        TikCheckUtil.check_type_match(
            quantize_params.get("mode_param"), (float, Scalar, Tensor),
            "Please specify your quantize_params 'mode_param': "
            "immediate/Scalar/Tensor for deq-mode.")
        if isinstance(quantize_params.get("mode_param"), Tensor):
            if TikSocManager.is_hisi_sd_cs():
                TikCheckUtil.check_equality(
                    quantize_params.get("mode_param").dtype,
                    "float32",
                    "quantize_params 'mode_param' should be tensor "
                    "of float32.")
                deq_tensor_shape = quantize_params.get("mode_param").shape
                cout_value = fixpipe_config.cburst_num * fixpipe_config.c_0
                if len(deq_tensor_shape) <= 0 or deq_tensor_shape[0] != cout_value * 2:
                    TikCheckUtil.raise_error(
                        "tensor quantize_params shape is not correct.")
            else:
                TikCheckUtil.check_equality(
                    quantize_params.get("mode_param").dtype, "float16",
                    "quantize_params 'mode_param' should be tensor of "
                    "float16.")
            TikCheckUtil.check_equality(
                quantize_params.get("mode_param").scope, scope_cbuf,
                "quantize_params 'mode_param' tensor's scope should "
                "be L1.")
            actual_ele = Expr(reduce_mul(quantize_params.get("mode_param").original_shape)).eval_value()
            expected_ele = Expr(quantize_params.get(
                "mode_param").offset + 16).eval_value()
            if actual_ele is not None and expected_ele is not None:
                TikCheckUtil.check_ge(actual_ele, expected_ele,
                                      "deqscale tensor overflow, expected elements: %d, "
                                      "actual elements: %d." % (expected_ele, actual_ele))

    @staticmethod
    def _check_fixpipe_tensor_overflow(dst, src, cburst_num, burst_len):
        frac_len = 16
        howo = burst_len * ONE_BLK_SIZE // DTYPE_SIZE.get(src.dtype) // frac_len
        round_howo = ceil_div(howo, frac_len) * frac_len
        # check src overflow
        extent_ele = cburst_num * round_howo * frac_len
        total_ele = Expr(reduce_mul(src.original_shape)).eval_value()
        need_ele = Expr(src.offset + extent_ele).eval_value()
        if total_ele is not None and need_ele is not None:
            TikCheckUtil.check_ge(
                total_ele, need_ele,
                "src tensor overflow, expected elements: %s, "
                "actual elements: %s" % (need_ele, total_ele))
        # check dst overflow
        extent_ele = cburst_num * howo * frac_len
        total_ele = Expr(reduce_mul(dst.original_shape)).eval_value()
        need_ele = Expr(dst.offset + extent_ele).eval_value()
        if total_ele is not None and need_ele is not None:
            TikCheckUtil.check_ge(
                total_ele, need_ele,
                "dst tensor overflow, expected elements: %s, "
                "actual elements: %s" % (need_ele, total_ele))

    @staticmethod
    def _check_operator(fixpipe_api):
        """
        check operator
        """

        TikCheckUtil.check_type_match(fixpipe_api.dst, Tensor,
                                      "dst should be Tensor, input"
                                      " type is %s" % type(fixpipe_api.dst))
        TikCheckUtil.check_type_match(fixpipe_api.src, Tensor,
                                      "src should be Tensor, input"
                                      " type is %s" % type(fixpipe_api.src))
        # check operator scope, waiting for confirmation of naming !!!!
        if TikSocManager.is_hisi_sd_cs() or TikSocManager.is_310b_610l_soc():
            if fixpipe_api.dst.scope not in [scope_gm, scope_ubuf]:
                print_error_msg("dst's scope must be scope_gm or scope_ubuf, "
                                "input scope is: %s" % fixpipe_api.dst.scope)
        else:
            TikCheckUtil.check_equality(fixpipe_api.dst.scope, scope_gm,
                                        "dst's scope must be scope_gm, "
                                        "input scope is: %s" % fixpipe_api.dst.scope)
        TikCheckUtil.check_equality(fixpipe_api.src.scope, scope_cc,
                                    "src's scope must be L1_out, "
                                    "input scope is: %s" % fixpipe_api.src.scope)
        src_align = get_l0c_align(fixpipe_api.src)
        check_address_align((fixpipe_api.src,), ("src",), src_align)

    @staticmethod
    def _check_dtype_burst(fixpipe_api):
        """
        check dtype or nburst
        """
        dtype_str = DTYPE_MAP.get(fixpipe_api.src.dtype) + DTYPE_MAP.get(fixpipe_api.dst.dtype)
        TikCheckUtil.check_equality(api_check_support("tik.fixpipe", dtype_str), True,
                                    gen_api_check_statement(dtype_str, "fixpipe"))
        # check nburst
        TikCheckUtil.check_type_match(
            fixpipe_api.cburst_num, int, "cburst_num should be python int, input type "
                                         "is %s" % type(fixpipe_api.cburst_num))
        TikCheckUtil.check_in_range_by_dtype(
            fixpipe_api.cburst_num, msg="cburst_num should be in range of [%d, %d], input value is %s"
                                        % (MIN_NBURST, MAX_NBURST_DOUBLE_BYTE, fixpipe_api.cburst_num),
            var_range=[MIN_NBURST, MAX_NBURST_DOUBLE_BYTE])
        # check burst_len
        TikCheckUtil.check_type_match(
            fixpipe_api.burst_len, int, "burst_len should be python int, input type "
                                        "is %s" % type(fixpipe_api.burst_len))
        _fixpipe_burst_len = 1
        TikCheckUtil.check_in_range_by_dtype(
            fixpipe_api.burst_len, msg="burst_len should be in range of [%d, %d], input value is %s"
                                       % (_fixpipe_burst_len, MAX_BURST_LEN_DOUBLE_BYTE, fixpipe_api.burst_len),
            var_range=[_fixpipe_burst_len, MAX_BURST_LEN_DOUBLE_BYTE])

    @staticmethod
    def _check_stride(fixpipe_api):
        """
        dst or src stride
        """
        TikCheckUtil.check_type_match(
            fixpipe_api.dst_stride, int, "dst_stride should be python int, input type "
                                         "is %s" % type(fixpipe_api.dst_stride))
        TikCheckUtil.check_in_range_by_dtype(
            fixpipe_api.dst_stride, msg="dst_stride should be in range of [%d, %d], input value is %s"
                                        % (DEFAULT_STRIDE, MAX_REP_STRIDE_DOUBLE_BYTE, fixpipe_api.dst_stride),
            var_range=[DEFAULT_STRIDE, MAX_REP_STRIDE_DOUBLE_BYTE])
        # check src_stride
        TikCheckUtil.check_type_match(
            fixpipe_api.src_stride, int, "src_stride should be python int, input type "
                                         "is %s" % type(fixpipe_api.src_stride))
        TikCheckUtil.check_in_range_by_dtype(
            fixpipe_api.src_stride, msg="src_stride should be in range of [%d, %d], input value is %s"
                                        % (DEFAULT_STRIDE, MAX_REP_STRIDE_DOUBLE_BYTE, fixpipe_api.src_stride),
            var_range=[DEFAULT_STRIDE, MAX_REP_STRIDE_DOUBLE_BYTE])

    @staticmethod
    def _check_fixpipe_bias_params(extend_params, src_dtype, cout):
        bias = extend_params.get("bias")
        if bias is None:
            return
        TikCheckUtil.check_type_match(
            bias, Tensor, "extend_params 'bias' should be Tensor, "
                          "input type is %s" % type(bias))
        TikCheckUtil.check_equality(bias.scope, scope_cbuf,
                                    "extend_params bias's scope should be L1")
        # check L1 address 32B align
        check_address_align((bias,), ("extend_params bias",))
        TikCheckUtil.check_equality(
            src_dtype, bias.dtype,
            "extend_params 'bias' should have the same dtype with src")
        total_ele = Expr(reduce_mul(bias.original_shape)).eval_value()
        need_ele = Expr(bias.offset + cout).eval_value()
        if total_ele is not None and need_ele is not None:
            TikCheckUtil.check_ge(
                total_ele, need_ele,
                "extend_params 'bias' tensor overflow, expected elements: %s, "
                "actual elements: %s" % (need_ele, total_ele))

    @staticmethod
    def _check_fixpipe_ele_bias_params(extend_params, src, cburst_num,
                                       burst_len):
        ele_bias = extend_params.get("element-wise-add")
        if ele_bias is None:
            return
        TikCheckUtil.check_type_match(
            ele_bias, Tensor, "extend_params 'element-wise-add' should be "
                              "Tensor, input type is %s" % type(ele_bias))
        TikCheckUtil.check_equality(ele_bias.scope, scope_cbuf,
                                    "extend_params 'element-wise-add' scope "
                                    "should be L1")
        # check L1 address 32B align
        check_address_align((ele_bias,),
                            ("extend_params element-wise-add tensor",))
        TikCheckUtil.check_equality(
            ele_bias.dtype, src.dtype, "extend_params 'element-wise-add' should "
                                       "have the same dtype with src")
        # check overflow
        frac_len = 16
        howo = burst_len * ONE_BLK_SIZE // DTYPE_SIZE.get(src.dtype) // frac_len
        round_howo = ceil_div(howo, frac_len) * frac_len
        extent_ele = cburst_num * round_howo * frac_len
        total_ele = Expr(reduce_mul(ele_bias.original_shape)).eval_value()
        need_ele = Expr(ele_bias.offset + extent_ele).eval_value()
        if total_ele is not None and need_ele is not None:
            TikCheckUtil.check_ge(
                total_ele, need_ele,
                "extend_params 'element-wise-add' tensor overflow, expected "
                "elements: %s, actual elements: %s" % (need_ele, total_ele))

    @staticmethod
    def _check_fixpipe_relu_params(extend_params, fixpipe_config):
        relu = extend_params.get("relu")
        if "relu" not in extend_params:
            return
        TikCheckUtil.check_type_match(
            relu, bool,
            "extend_params 'relu' should be bool type.")
        if fixpipe_config.has_ele_wise_bias() or fixpipe_config.has_bias():
            TikCheckUtil.check_is(
                relu, False,
                "Intrinsic fixpipe doesn't support relu "
                "when enable element-wise-add or bias, "
                "extend_params 'relu' should be False")
        if fixpipe_config.has_deq() and isinstance(
                extend_params["quantize_params"].get("mode_param"),
                (float, Scalar)):
            TikCheckUtil.check_is(
                relu, False,
                "Intrinsic fixpipe doesn't support relu when quantize "
                "int322fp16 and mode_param is scalar or float, "
                "extend_params 'relu' should be False ")
        if TikSocManager.is_hisi_sd_cs() and "relu" in extend_params:
            TikCheckUtil.raise_error("current soc not support relu.")

    @staticmethod
    def _process_deq(tik_instance, fixpipe_config):
        if fixpipe_config.has_deq():
            deqscale = fixpipe_config.extend_params["quantize_params"].get(
                "mode_param")
            fixpipe_config.deq_value = deqscale
            if isinstance(deqscale, Tensor):
                deqscale_l1 = ReIndexProxy(deqscale, [fixpipe_config.frac_len, ])
                if TikSocManager.is_hisi_sd_cs():
                    cout_val = fixpipe_config.cburst_num * fixpipe_config.c_0
                    deqscale_value = tik_instance.Tensor("float32", [cout_val * 2, ], name="deq_tmp", scope=scope_ubuf)
                    tik_instance.tensor_mov(deqscale_value, deqscale_l1.flat_access(0), '', 1,
                                            cout_val * 2 * 4 // 32, 0, 0)
                    fixpipe_config.deq_value = deqscale_value
                else:
                    deqscale_value = tik_instance.Tensor("float16", [fixpipe_config.frac_len, ],
                                                         name="deq_tmp", scope=scope_ubuf)
                    tik_instance.tensor_mov(deqscale_value, deqscale_l1.flat_access(0), '', 1, 1, 0, 0)
                    fixpipe_config.deq_value = deqscale_value

    @staticmethod
    def _get_l1out_offset(fixpipe_config, data_process):
        l1out_blocks_actual = data_process.tiling.l1out_tile_blocks
        howo_blocks_actual = data_process.tiling.howo_tile_blocks
        if data_process.is_n_tail:
            l1out_blocks_actual = data_process.tiling.l1out_tail_blk
        if data_process.is_m_tail:
            howo_blocks_actual = data_process.tiling.howo_tail_blk

        howo_offset = data_process.l1out_i * data_process.tiling.l1out_tile_blocks * \
                      (fixpipe_config.howo_blocks + fixpipe_config.src_stride) * \
                      fixpipe_config.frac_len * fixpipe_config.frac_len
        l1out_offset = howo_offset + data_process.howo_i * data_process.tiling.howo_tile_blocks * \
                       fixpipe_config.frac_len * fixpipe_config.frac_len
        return l1out_offset, l1out_blocks_actual, howo_blocks_actual

    @staticmethod
    def _get_fixpipe_param(data_process, fixpipe_config, extra_len, avoid_bankconflict):
        """
        get fixpipe param
        """
        frac_len = fixpipe_config.frac_len
        l1out_blocks_actual = data_process.tiling.l1out_tile_blocks
        howo_blocks_actual = data_process.tiling.howo_tile_blocks
        if data_process.is_n_tail:
            l1out_blocks_actual = data_process.tiling.l1out_tail_blk
        if data_process.is_m_tail:
            howo_blocks_actual = data_process.tiling.howo_tail_blk
        output_tile_burst = l1out_blocks_actual
        if data_process.tiling.vconv_merge_channel:
            output_tile_burst = output_tile_burst // 2

        dst_offset = data_process.l1out_i * data_process.tiling.l1out_tile_blocks * \
                     (fixpipe_config.howo + extra_len) * frac_len + data_process.howo_i * \
                     data_process.tiling.howo_tile_blocks * frac_len * fixpipe_config.c_0
        real_dst_stride = fixpipe_config.dst_stride + (fixpipe_config.howo - howo_blocks_actual *
                                                       frac_len) * \
                          fixpipe_config.c_0 * DTYPE_SIZE.get(fixpipe_config.dst_dtype) // ONE_BLK_SIZE
        real_dst_stride = max(real_dst_stride, 0)  # bypass compile error
        tail_dst_stride = fixpipe_config.dst_stride + (fixpipe_config.round_howo - howo_blocks_actual *
                                                       frac_len) * fixpipe_config.c_0 * \
                          DTYPE_SIZE.get(fixpipe_config.dst_dtype) // ONE_BLK_SIZE
        bc_pad_dst_gap = int(avoid_bankconflict) * fixpipe_config.c_0 * DTYPE_SIZE.get(fixpipe_config.dst_dtype) \
                         // ONE_BLK_SIZE
        tail_src_stride = bc_pad_dst_gap + (fixpipe_config.round_howo - fixpipe_config.howo) \
                          * fixpipe_config.c_0 * DTYPE_SIZE.get(fixpipe_config.dst_dtype) // ONE_BLK_SIZE
        real_output_burst_len = howo_blocks_actual * frac_len * fixpipe_config.c_0 * \
                                DTYPE_SIZE.get(fixpipe_config.dst_dtype) // ONE_BLK_SIZE
        tail_output_burst_len = (howo_blocks_actual * frac_len - (fixpipe_config.round_howo - fixpipe_config.howo)) * \
                                fixpipe_config.c_0 * DTYPE_SIZE.get(fixpipe_config.dst_dtype) // ONE_BLK_SIZE

        return [output_tile_burst, dst_offset, real_dst_stride, tail_dst_stride, tail_src_stride,
                real_output_burst_len, tail_output_burst_len, bc_pad_dst_gap]

    def check_all(self, fixpipe_api):
        """
        check fixpipe to param
        """
        self._check_operator(fixpipe_api)
        self._check_dtype_burst(fixpipe_api)
        self._check_stride(fixpipe_api)
        fixpipe_config = self._check_extend_params(fixpipe_api)
        self._check_fixpipe_tensor_overflow(fixpipe_api.dst, fixpipe_api.src,
                                            fixpipe_api.cburst_num, fixpipe_api.burst_len)
        return fixpipe_config

    def execute(self, fixpipe_api, fixpipe_config):
        """
        execute of fixpipe to get best tiling and gen code
        """
        self._process_deq(self.tik_instance, fixpipe_config)
        if fixpipe_config.has_bias():
            bias_l1 = ReIndexProxy(fixpipe_api.extend_params['bias'], [1, fixpipe_api.cburst_num *
                                                                       fixpipe_config.frac_len])
            bias_ub = self.tik_instance.Tensor(fixpipe_api.src.dtype, [1, fixpipe_api.cburst_num *
                                                                       fixpipe_config.frac_len],
                                               name="bias_tmp", scope=scope_ubuf)
            self.tik_instance.tensor_mov(
                bias_ub, bias_l1.flat_access(0), '', fixpipe_api.cburst_num,
                fixpipe_config.frac_len * DTYPE_SIZE.get(bias_l1.dtype) // ONE_BLK_SIZE, 0, 0)
            fixpipe_config.bias_value = ReIndexProxy(bias_ub, [1, fixpipe_api.cburst_num * fixpipe_config.frac_len])
        tiling = gen_fixpipe_tiling(fixpipe_api.cburst_num, fixpipe_api.src.dtype, fixpipe_api.dst.dtype,
                                    self.tik_instance, fixpipe_config)
        l1out_thread_num = tiling.l1out_thread_num
        l1out_iter_num = tiling.l1out_iter_num
        if tiling.l1out_has_tail:
            l1out_iter_num -= 1
            if l1out_iter_num < l1out_thread_num:
                l1out_thread_num = 1
        with self.tik_instance.for_range(0, l1out_iter_num, thread_num=l1out_thread_num) as l1out_i:
            self._make_fixpipe_code(fixpipe_config, tiling, l1out_i, is_n_tail=False)
        if tiling.l1out_has_tail:
            self._make_fixpipe_code(fixpipe_config, tiling, l1out_iter_num, is_n_tail=True)
        self.tik_instance.set_high_level_api_state()

    def _check_deqscale(self, quantize_params, fixpipe_config, dtype_str):
        if dtype_str not in ("s32u8", "s32s8", "s32f16"):
            TikCheckUtil.check_is(
                quantize_params.get("mode_param"), None,
                "quantize_params 'mode_param' should be None when "
                "src and dst dtype is %s" % dtype_str)
        else:
            if isinstance(quantize_params.get("mode_param"), Scalar):
                TikCheckUtil.check_equality(
                    quantize_params.get("mode_param").dtype, "float16",
                    "quantize_params 'mode_param' should be "
                    "a scalar of float16.")
            if fixpipe_config.has_bias() or fixpipe_config.has_ele_wise_bias():
                TikCheckUtil.check_type_match(
                    quantize_params.get("mode_param"), (float, Scalar),
                    "Please specify your quantize_params 'mode_param': "
                    "immediate/Scalar(float16) for deq-mode.")
            else:
                self._check_deqtype(quantize_params, fixpipe_config)

    def _check_fixpipe_deq_params(self, dtype_str, extend_params, cburst_num, fixpipe_config):
        quantize_params = extend_params.get("quantize_params")
        if quantize_params is None:
            return
        mode_dtype_map = {"int322fp16": "s32f16", "fp322fp16": "f32f16"}
        if dtype_str in ("s32s32", "f32f32"):
            TikCheckUtil.check_is(
                quantize_params, None,
                "extend_params['quantize_params'] should be None when "
                "src and dst dtype is %s" % dtype_str)
            return
        TikCheckUtil.check_type_match(
            quantize_params, dict,
            "extend_params['quantize_params'] should be dict, input type is %s" %
            type(quantize_params))
        if not ("mode" in quantize_params and "mode_param" in quantize_params):
            TikCheckUtil.raise_error("extend_params['quantize_params'] dict must "
                                     "contains 'mode' and 'mode_param'")
        # check mode
        TikCheckUtil.check_var_in_list(
            quantize_params.get("mode"), mode_dtype_map,
            "Instruction fixpipe doesn't support with quantize_params 'mode' "
            "%s." % quantize_params.get("mode"))
        TikCheckUtil.check_equality(
            dtype_str, mode_dtype_map.get(quantize_params.get("mode")),
            "src.dtype and dst.dtype mismatch with quantize_params 'mode' "
            "%s." % quantize_params.get("mode"))
        # check cout_blk
        if dtype_str in ("s32u8", "s32s8"):
            TikCheckUtil.check_equality(
                cburst_num % 2, 0, "cburst_num should be multiple of 2 "
                                   "when quantize from int32 to int8/uint8")

        # check deqscale

        self._check_deqscale(quantize_params, fixpipe_config, dtype_str)

    def _check_extend_params(self, fixpipe_api):
        """
        check extend_params
        """
        dtype_str = DTYPE_MAP.get(fixpipe_api.src.dtype) + DTYPE_MAP.get(fixpipe_api.dst.dtype)
        if fixpipe_api.extend_params is not None:
            TikCheckUtil.check_type_match(
                fixpipe_api.extend_params, dict, "extend_params should be dict, input "
                                     "type is %s" % type(fixpipe_api.extend_params))
            if not set(fixpipe_api.extend_params.keys()).issubset(
                    {"bias", "quantize_params", "relu", "element-wise-add"}):
                TikCheckUtil.raise_error("input extend_params dict contains "
                                         "invalid key, please check!")

        fixpipe_config = FixpipeInfo(fixpipe_api)
        # check extend_params
        if dtype_str in ("s32f16", "f32f16"):
            # sd3403 and cs not support int322float16 with bias at same time
            lhisi_soc_limit = TikSocManager.is_hisi_sd_cs() and \
                              (fixpipe_config.has_bias() or fixpipe_config.has_ele_wise_bias())
            if lhisi_soc_limit:
                TikCheckUtil.raise_error(
                    "current soc not support bias when s32f16/f32f16")

            TikCheckUtil.check_not_is(
                fixpipe_api.extend_params, None, "extend_params should not be None when "
                                     "src and dst dtype is %s" % dtype_str)
            TikCheckUtil.check_equality(fixpipe_config.has_deq(), True,
                "extend_params 'quantize_params' should not be None when "
                "src and dst dtype is %s" % dtype_str)
        # bias cannot be used at the same time
        bias_and_ele_wise_bias = fixpipe_config.has_ele_wise_bias() and fixpipe_config.has_bias()
        if bias_and_ele_wise_bias:
            TikCheckUtil.raise_error(
                "fixpipe doesn't support enable extend_params 'bias' and "
                "extend_params 'element-wise-add' at the same time")
        if fixpipe_api.extend_params is not None:
            self._check_fixpipe_deq_params(dtype_str, fixpipe_api.extend_params, fixpipe_api.cburst_num, fixpipe_config)
            self._check_fixpipe_bias_params(fixpipe_api.extend_params, fixpipe_api.src.dtype,
                                            fixpipe_api.cburst_num * fixpipe_config.frac_len)
            self._check_fixpipe_ele_bias_params(fixpipe_api.extend_params, fixpipe_api.src, fixpipe_api.cburst_num,
                                                fixpipe_api.burst_len)
            self._check_fixpipe_relu_params(fixpipe_api.extend_params, fixpipe_config)
        return fixpipe_config

    def _fixpipe_bias(self, input_tensor, fixpipe_config, data_process,
                      avoid_bankconflict=False):
        """
        Parameters
        ----------
        input_tensor: l1out-> ub tensor
        fixpipe_config: fixpipe params
        tiling: fixpipe tiling info
        l1out_i: idx of cout tiling for loop

        Returns
        -------
        ub_bias_result
        """
        l1out_tile_blocks = data_process.tiling.l1out_tile_blocks
        if data_process.is_n_tail:
            l1out_tile_blocks = data_process.tiling.l1out_tail_blk
        howo_tile_blocks = data_process.tiling.howo_tile_blocks
        if data_process.is_m_tail:
            howo_tile_blocks = data_process.tiling.howo_tail_blk
        howo_data_index = howo_tile_blocks * fixpipe_config.frac_len + int(avoid_bankconflict)
        ub_bias_result = self.tik_instance.Tensor(input_tensor.dtype, input_tensor.shape,
                                                  name="fixpipe_bias_result", scope=scope_ubuf)
        if not l1out_tile_blocks:
            return ub_bias_result

        def _fixpipe_vadd_bias(mask, repeat_times, data_index, bias_index):
            if fixpipe_config.src_dtype == "float16":
                self.tik_instance.vadd(mask, ub_bias_result[data_index],
                                       fixpipe_config.bias_value.flat_access(bias_index),
                                       input_tensor[data_index], repeat_times, 1, 0, 1, 8, 0, 8)
            else:
                self.tik_instance.vadd(mask // 2, ub_bias_result[data_index],
                                       fixpipe_config.bias_value.flat_access(bias_index),
                                       input_tensor[data_index], repeat_times, 2, 0, 2, 16, 0, 16)
                self.tik_instance.vadd(mask // 2, ub_bias_result[data_index + fixpipe_config.frac_len // 2],
                                       fixpipe_config.bias_value.flat_access(bias_index + fixpipe_config.frac_len // 2),
                                       input_tensor[data_index + fixpipe_config.frac_len // 2],
                                       repeat_times, 2, 0, 2, 16, 0, 16)
        with self.tik_instance.for_range(0, l1out_tile_blocks) as cout_idx:
            bias_index = data_process.l1out_i * data_process.tiling.l1out_tile_blocks * \
                         fixpipe_config.frac_len + cout_idx * fixpipe_config.frac_len
            if howo_tile_blocks * 16 // 8 // 255:
                if fixpipe_config.src_dtype == "float16":
                    repeat_nums = howo_tile_blocks * 16 // 8
                else:
                    repeat_nums = howo_tile_blocks * 16 // 8 // 255
                with self.tik_instance.for_range(0, repeat_nums) as i:
                    data_index = cout_idx * howo_data_index * fixpipe_config.frac_len + i * 255 * 128
                    _fixpipe_vadd_bias(128, 255, data_index, bias_index)
            if howo_tile_blocks * 16 // 8 % 255:
                data_index = cout_idx * howo_data_index * fixpipe_config.frac_len + howo_tile_blocks * 16 // 8 // \
                             255 * 255 * 128
                _fixpipe_vadd_bias(128, howo_tile_blocks * 16 // 8 % 255, data_index, bias_index)
            if howo_tile_blocks * 16 % 8:
                data_index = cout_idx * howo_data_index * fixpipe_config.frac_len * howo_tile_blocks
                data_index = data_index * 16 // 8 // 255 * 255 * 128 + howo_tile_blocks * 16 // 8 % 255 * 128
                _fixpipe_vadd_bias(howo_tile_blocks * 16 % 8 * 16, 1, data_index, bias_index)

        return ub_bias_result

    def _get_res_tensor(self, input_tensor, ub_tmp):
        """
        get res tensor
        """
        res_tensor = self.tik_instance.Tensor(input_tensor.dtype, input_tensor.shape,
                                              name="fixpipe_ele_bias_result", scope=scope_ubuf)
        vadd_repeat_batch = reduce_mul(input_tensor.shape) // 64 // MAX_REPEAT_TIMES
        vadd_repeat_tail = reduce_mul(input_tensor.shape) // 64 % MAX_REPEAT_TIMES
        with self.tik_instance.for_range(0, vadd_repeat_batch) as vadd_sub_i:
            offset = vadd_sub_i * MAX_REPEAT_TIMES * 64
            self.tik_instance.vadd(64, res_tensor[offset:], input_tensor[offset:],
                      ub_tmp[offset:], MAX_REPEAT_TIMES, 1, 1, 1, 8, 8, 8)
        if vadd_repeat_tail > 0:
            offset = vadd_repeat_batch * MAX_REPEAT_TIMES * 64
            self.tik_instance.vadd(64, res_tensor[offset:], input_tensor[offset:],
                                   ub_tmp[offset:], vadd_repeat_tail, 1, 1, 1, 8, 8, 8)
        return res_tensor

    def _fixpipe_ele_bias(self, input_tensor, fixpipe_config, data_process,
                          avoid_bankconflict=False):
        bias_data_l1 = ReIndexProxy(
            fixpipe_config.extend_params['element-wise-add'],
            (fixpipe_config.cburst_num, fixpipe_config.round_howo,
             fixpipe_config.frac_len))
        dtype = input_tensor.dtype
        l1out_offset, l1out_blocks_actual, howo_blocks_actual = \
            self._get_l1out_offset(fixpipe_config, data_process)
        l1out_ub_src_stride = (fixpipe_config.howo_blocks - howo_blocks_actual) * fixpipe_config.frac_len * \
                              fixpipe_config.frac_len * DTYPE_SIZE.get(dtype) // ONE_BLK_SIZE
        l1out_ub_dst_stride = int(avoid_bankconflict) * fixpipe_config.frac_len * \
            DTYPE_SIZE.get(dtype) // ONE_BLK_SIZE

        # shape: cout_blks, round_howo + avoid_bc_pad_1, 16
        l0c_dim = l1out_blocks_actual * (howo_blocks_actual * fixpipe_config.frac_len +
                                         int(avoid_bankconflict)) * fixpipe_config.frac_len

        # round to 256B for following elementwise vector instr
        if input_tensor.dtype in ("float32", "int32"):
            l0c_dim = ceil_div(l0c_dim, 64) * 64

        ub_tmp = self.tik_instance.Tensor(dtype, (l0c_dim,), name="fixpipe_ub_tmp", scope=scope_ubuf)
        self.tik_instance.tensor_mov(ub_tmp, bias_data_l1.flat_access(l1out_offset), '', l1out_blocks_actual,
                        howo_blocks_actual * fixpipe_config.frac_len * 16 * DTYPE_SIZE.get(dtype) // ONE_BLK_SIZE,
                        l1out_ub_dst_stride, l1out_ub_src_stride)
        res_tensor = self._get_res_tensor(input_tensor, ub_tmp)
        return res_tensor

    def _fixpipe_l1out_to_ub(self, data_process, fixpipe_config, avoid_bankconflict=False, deq=False):
        """
        Parameters
        ----------
        data_process: input_tensor fixpipe_config tiling l1out_i howo_i
        avoid_bankconflict: flag indicating whether add 16 elements for each cout blk
        deq: flag indicating whether deq on the fly

        Returns
        -------
        ub_tmp tensor
        """
        if deq:
            ub_tmp_dtype = "float16"
        else:
            ub_tmp_dtype = data_process.input_tensor.dtype

        en_relu = fixpipe_config.has_relu() and \
                  not fixpipe_config.has_bias() and \
                  not fixpipe_config.has_ele_wise_bias()
        l1out_offset, l1out_blocks_actual, howo_blocks_actual = \
            self._get_l1out_offset(fixpipe_config, data_process)
        l1out_ub_src_stride = \
            fixpipe_config.howo_blocks - howo_blocks_actual + \
            fixpipe_config.src_stride
        l1out_ub_dst_stride = \
            int(avoid_bankconflict) * fixpipe_config.frac_len * \
            DTYPE_SIZE.get(ub_tmp_dtype) // ONE_BLK_SIZE

        # shape: cout_blks, round_howo + avoid_bc_pad_1, 16
        l0c_dim = l1out_blocks_actual * \
                  (howo_blocks_actual * fixpipe_config.frac_len +
                   int(avoid_bankconflict)) * \
                  fixpipe_config.frac_len

        # round to 256B for following elementwise vector instr
        if data_process.input_tensor.dtype in ("float32", "int32"):
            l0c_dim = ceil_div(l0c_dim, 64) * 64

        fixpipe_config.l0c_shape = [l0c_dim, ]
        ub_tmp = self.tik_instance.Tensor(ub_tmp_dtype, (l0c_dim,), name="fixpipe_ub_tmp", scope=scope_ubuf)
        self.tik_instance.tensor_mov(
            ub_tmp, data_process.input_tensor.flat_access(l1out_offset), 'm',
            l1out_blocks_actual, howo_blocks_actual, l1out_ub_dst_stride,
            l1out_ub_src_stride, deqscale=fixpipe_config.deq_value,
            relu=en_relu)

        return ub_tmp

    def _fixpipe_ub_to_out(self, input_tensor, fixpipe_config, data_process, avoid_bankconflict=False):
        if fixpipe_config.dst.dtype in ("uint8", "int8"):
            extra_len = fixpipe_config.dst_stride
            dst = ReIndexProxy(
                fixpipe_config.dst, [fixpipe_config.cburst_num // 2,
                                     fixpipe_config.howo + extra_len, 32])
        else:
            extra_len = fixpipe_config.dst_stride * ONE_BLK_SIZE // DTYPE_SIZE.get(fixpipe_config.dst.dtype) // \
                        fixpipe_config.frac_len
            dst = ReIndexProxy(fixpipe_config.dst, [fixpipe_config.cburst_num, fixpipe_config.howo + extra_len, 16])

        output_tile_burst, dst_offset, real_dst_stride, tail_dst_stride, tail_src_stride, \
        real_output_burst_len, tail_output_burst_len, bc_pad_dst_gap = \
            self._get_fixpipe_param(data_process, fixpipe_config, extra_len, avoid_bankconflict)
        if data_process.is_m_tail:
            self.tik_instance.tensor_mov(dst.flat_access(dst_offset), input_tensor, "", output_tile_burst,
                                         tail_output_burst_len, tail_dst_stride, tail_src_stride)
        else:
            self.tik_instance.tensor_mov(dst.flat_access(dst_offset), input_tensor, "", output_tile_burst,
                                         real_output_burst_len, real_dst_stride, bc_pad_dst_gap)

    def _fixpipe_deq_b32_to_f16(self, ub_bias_result, fixpipe_config, tiling):
        """
        realize fixpipe conv from s32/f32 to f16

        Parameters
        ----------
        ub_bias_result : tensor, scope is ub, if have bias, should after bias, tensor size should be multiple of 256B
        fixpipe_config : FixpipeInfo
        tiling : fixpipe tiling

        Returns
        -------
        ub_deq_fp16_result : Tensor, have same shape as ub_bias_result
        """
        ub_deq_fp16_result = self.tik_instance.Tensor(
            "float16", fixpipe_config.l0c_shape,
            name="fixpipe_deq_fp16_result", scope=scope_ubuf)
        total_elements = reduce_mul(fixpipe_config.l0c_shape)
        elements_per_rep = 64
        total_repeats = total_elements // elements_per_rep
        deq_repeat_batch = total_repeats // tiling.max_repeat
        deq_repeat_tail = total_repeats % tiling.max_repeat
        blk_stride = 1
        with self.tik_instance.for_range(0, deq_repeat_batch) as deq_sub_i:
            offset = deq_sub_i * tiling.max_repeat * elements_per_rep
            self.tik_instance.vconv(
                elements_per_rep, "", ub_deq_fp16_result[offset],
                ub_bias_result[offset], tiling.max_repeat, blk_stride,
                blk_stride, 4, 8, deqscale=fixpipe_config.extend_params[
                    'quantize_params'].get("mode_param"))
        if deq_repeat_tail > 0:
            offset = deq_repeat_batch * tiling.max_repeat * elements_per_rep
            self.tik_instance.vconv(
                elements_per_rep, "", ub_deq_fp16_result[offset],
                ub_bias_result[offset], deq_repeat_tail, blk_stride,
                blk_stride,
                4, 8, deqscale=fixpipe_config.extend_params[
                    'quantize_params'].get("mode_param"))
        return ub_deq_fp16_result

    def _fixpipe_deq_fp16_to_u8s8(self, ub_deq_fp16_result, fixpipe_config, data_process):
        """
        realize fixpipe conv from f16 to u8/s8

        Parameters
        ----------
        ub_deq_fp16_result: tensor, shape is [cout_blk, howo, 16]
        fixpipe_config: fixpipe config info, FixpipeInfo
        data_process: tiling is_n_tail is_m_tail

        Returns
        -------
        ub_deq_result: Tensor, shape is [cout_blk//2, howo, 32]
        """
        l1out_blocks_actual = data_process.tiling.l1out_tile_blocks
        howo_blocks_actual = data_process.tiling.howo_tile_blocks
        if data_process.is_n_tail:
            l1out_blocks_actual = data_process.tiling.l1out_tail_blk
        if data_process.is_m_tail:
            howo_blocks_actual = data_process.tiling.howo_tail_blk
        ub_deq_result = self.tik_instance.Tensor(
            fixpipe_config.dst_dtype, fixpipe_config.l0c_shape,
            name="fixpipe_deq_result", scope=scope_ubuf)
        batch_loop = (howo_blocks_actual * fixpipe_config.frac_len + 1) // \
                     data_process.tiling.max_repeat
        batch_loop_tail = (howo_blocks_actual * fixpipe_config.frac_len + 1) % \
                          data_process.tiling.max_repeat
        blk_stride = howo_blocks_actual * fixpipe_config.frac_len + 1
        with self.tik_instance.for_range(0, l1out_blocks_actual // 8) as batch_i:
            offset = batch_i * 8 * (howo_blocks_actual * fixpipe_config.frac_len +
                                    1) * fixpipe_config.frac_len
            with self.tik_instance.for_range(0, batch_loop) as batch_sub_i:
                sub_block_offset = data_process.tiling.max_repeat * batch_sub_i
                self.tik_instance.vconv(128, "", ub_deq_result[offset + sub_block_offset * 32],
                                        ub_deq_fp16_result[offset + sub_block_offset * 16],
                                        data_process.tiling.max_repeat, blk_stride, blk_stride, 1, 1)
            if batch_loop_tail > 0:
                sub_block_offset = data_process.tiling.max_repeat * batch_loop
                self.tik_instance.vconv(
                    128, "", ub_deq_result[offset + sub_block_offset * 32],
                    ub_deq_fp16_result[offset + sub_block_offset * 16],
                    batch_loop_tail, blk_stride, blk_stride, 1, 1)

        if l1out_blocks_actual % 8 > 0:
            partial_mask = l1out_blocks_actual % 8 * 16
            offset = l1out_blocks_actual // 8 * 8 * (howo_blocks_actual * fixpipe_config.frac_len + 1) * \
                     fixpipe_config.frac_len
            with self.tik_instance.for_range(0, batch_loop) as batch_sub_i:
                sub_block_offset = data_process.tiling.max_repeat * batch_sub_i
                self.tik_instance.vconv(partial_mask, "", ub_deq_result[offset + sub_block_offset * 32],
                                        ub_deq_fp16_result[offset + sub_block_offset * 16],
                                        data_process.tiling.max_repeat, blk_stride, blk_stride, 1, 1)
            if batch_loop_tail > 0:
                sub_block_offset = data_process.tiling.max_repeat * batch_loop
                self.tik_instance.vconv(partial_mask, "", ub_deq_result[offset + sub_block_offset * 32],
                                        ub_deq_fp16_result[offset + sub_block_offset * 16], batch_loop_tail,
                                        blk_stride, blk_stride, 1, 1)
        return ub_deq_result

    def _s32_to_u8s8(self, data_process, fixpipe_config):
        if fixpipe_config.has_bias() or fixpipe_config.has_ele_wise_bias():
            ub_data = self._fixpipe_l1out_to_ub(data_process, fixpipe_config, avoid_bankconflict=True)
            if fixpipe_config.has_bias():
                ub_data = self._fixpipe_bias(ub_data, fixpipe_config, data_process, avoid_bankconflict=True)
            elif fixpipe_config.has_ele_wise_bias():
                ub_data = self._fixpipe_ele_bias(
                    ub_data, fixpipe_config, data_process, avoid_bankconflict=True)
            ub_data = self._fixpipe_deq_b32_to_f16(ub_data, fixpipe_config, data_process.tiling)
            ub_data = self._fixpipe_deq_fp16_to_u8s8(ub_data, fixpipe_config, data_process)
            self._fixpipe_ub_to_out(ub_data, fixpipe_config, data_process, avoid_bankconflict=True)
        else:
            # deq on the fly
            ub_data = self._fixpipe_l1out_to_ub(data_process, fixpipe_config, avoid_bankconflict=True, deq=True)
            ub_data = self._fixpipe_deq_fp16_to_u8s8(ub_data, fixpipe_config, data_process)
            self._fixpipe_ub_to_out(ub_data, fixpipe_config, data_process, avoid_bankconflict=True)

    def _s32_to_s32(self, data_process, fixpipe_config):
        ub_data = self._fixpipe_l1out_to_ub(data_process, fixpipe_config)
        if fixpipe_config.has_bias():
            ub_data = self._fixpipe_bias(ub_data, fixpipe_config, data_process)
        elif fixpipe_config.has_ele_wise_bias():
            ub_data = self._fixpipe_ele_bias(ub_data, fixpipe_config, data_process)
        self._fixpipe_ub_to_out(ub_data, fixpipe_config, data_process)

    def _32_to_float16(self, data_process, fixpipe_config):
        if fixpipe_config.has_bias() or fixpipe_config.has_ele_wise_bias():
            ub_data = self._fixpipe_l1out_to_ub(data_process, fixpipe_config)
            if fixpipe_config.has_bias():
                ub_data = self._fixpipe_bias(ub_data, fixpipe_config, data_process)
            elif fixpipe_config.has_ele_wise_bias():
                ub_data = self._fixpipe_ele_bias(ub_data, fixpipe_config, data_process)
            ub_data = self._fixpipe_deq_b32_to_f16(ub_data, fixpipe_config, data_process.tiling)
            self._fixpipe_ub_to_out(ub_data, fixpipe_config, data_process)
        else:
            # deq on the fly
            ub_data = self._fixpipe_l1out_to_ub(data_process, fixpipe_config, deq=True)
            self._fixpipe_ub_to_out(ub_data, fixpipe_config, data_process)

    def _float16_to_float16(self, data_process, fixpipe_config):
        ub_data = self._fixpipe_l1out_to_ub(data_process, fixpipe_config)
        if fixpipe_config.has_bias():
            ub_data = self._fixpipe_bias(ub_data, fixpipe_config, data_process)
        elif fixpipe_config.has_ele_wise_bias():
            ub_data = self._fixpipe_ele_bias(
                ub_data, fixpipe_config, data_process)
        self._fixpipe_ub_to_out(ub_data, fixpipe_config, data_process)

    def _make_fixpipe_code(self, fixpipe_config, tiling, l1out_i, is_n_tail):
        def _make_fixpipe_m_code(l1out_i, howo_i, is_n_tail, is_m_tail):
            src_reindex = ReIndexProxy(fixpipe_config.src, (fixpipe_config.cburst_num, fixpipe_config.round_howo,
                                                            fixpipe_config.frac_len))
            data_process_cls = namedtuple("DataProcess", ["input_tensor", "tiling", "l1out_i",
                                                          "howo_i", "is_n_tail", "is_m_tail"])
            data_process = data_process_cls(src_reindex, tiling, l1out_i, howo_i, is_n_tail, is_m_tail)
            if fixpipe_config.dst_dtype in ("int8", "uint8"):
                self._s32_to_u8s8(data_process, fixpipe_config)
            elif fixpipe_config.dst_dtype in ("int32", "float32"):
                self._s32_to_s32(data_process, fixpipe_config)
            elif (fixpipe_config.src_dtype, fixpipe_config.dst_dtype) == ("float32", "float16"):
                self._32_to_float16(data_process, fixpipe_config)
            elif (fixpipe_config.src_dtype, fixpipe_config.dst_dtype) == ("int32", "float16"):
                self._32_to_float16(data_process, fixpipe_config)
            # f16->f16
            else:
                self._float16_to_float16(data_process, fixpipe_config)

        def _make_fixpipe_n_code(l1out_i, is_n_tail=False):
            howo_iter_num = tiling.howo_iter_num
            howo_thread_num = tiling.howo_thread_num
            if fixpipe_config.howo_has_round or tiling.howo_has_tail:
                howo_iter_num -= 1
                if howo_iter_num < howo_thread_num:
                    howo_thread_num = 1
            with self.tik_instance.for_range(0, howo_iter_num, thread_num=howo_thread_num) as howo_i:
                _make_fixpipe_m_code(l1out_i, howo_i, is_n_tail, is_m_tail=False)

            if fixpipe_config.howo_has_round or tiling.howo_has_tail:
                _make_fixpipe_m_code(l1out_i, howo_iter_num, is_n_tail, is_m_tail=True)

        _make_fixpipe_n_code(l1out_i, is_n_tail)
# @endcond
