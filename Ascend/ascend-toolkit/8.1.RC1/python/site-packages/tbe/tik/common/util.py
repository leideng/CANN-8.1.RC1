#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     util.py
DESC:     provide common function
CREATED:  2019-08-13 11:33:42
MODIFIED: 2019-08-13 11:33:42
"""
from functools import lru_cache

from tbe import tvm
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_ca
from tbe.common.platform import scope_cb
from tbe.common.platform import scope_cc
from tbe.tik.tik_lib.tik_params import MASK_VALUE_ZERO
from tbe.tik.tik_lib.tik_params import MASK_HIGH_IDX
from tbe.tik.tik_lib.tik_params import MASK_LEN_FULL_MODE
from tbe.tik.tik_lib.tik_params import MASK_LEN_B8_MODE
from tbe.tik.tik_lib.tik_params import MIN_INDEX
from tbe.tik.tik_lib.tik_params import MAX_INDEX
from tbe.tik.tik_lib.tik_params import MASK_LOW_IDX
from tbe.tik.tik_lib.tik_params import CONST_MASK_VALUE
from tbe.tik.tik_lib.tik_params import MAX_LOW_MASK_LEN
from tbe.tik.tik_lib.tik_params import MAX_MASK_HALF
from tbe.tik.tik_lib.tik_params import MIN_MASK_HALF
from tbe.tik.tik_lib.tik_params import MAX_MASK_HALF_INT64
from tbe.tik.tik_lib.tik_params import BIT_LEN_32
from tbe.tik.tik_lib.tik_params import BIT_LEN_64
from tbe.tik.tik_lib.tik_params import MIN_LRU_CACHE_SIZE
from tbe.tik.tik_lib.tik_params import MAX_INT32_VALUE
from tbe.tik.tik_lib.tik_params import MAX_INT64_VALUE
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.tik_lib.tik_expr import has_tik_var
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.common.tik_get_soc_name import is_compatible_mode
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tvm.tir import Div


DTYPE_SIZE = {
    'bool': 1,
    'uint8': 1,
    'int8': 1,
    'uint16': 2,
    'int16': 2,
    'int24': 3,
    'uint32': 4,
    'int32': 4,
    'float16': 2,
    'bfloat16': 2,
    'float32': 4,
    'int48': 6,
    'int64': 8,
    'uint64': 8,
    'float64': 8
}

DTYPE_SHORT_NAME_MAP = {
    'uint8': 'u8',
    'int8': 's8',
    'uint16': 'u16',
    'int16': 's16',
    'uint32': 'u32',
    'int32': 's32',
    'float16': 'f16',
    'bfloat16': 'bf16',
    'float32': 'f32',
    'int64': 's64',
    'uint64': 'u64',
    'float64': 'f64'
}

SCOPE_SIZE_MAP = {
    scope_ubuf: "UB_SIZE",
    scope_ca: "L0A_SIZE",
    scope_cb: "L0B_SIZE",
    scope_cc: "L0C_SIZE",
    scope_cbuf: "L1_SIZE"
}

WDTYPE_TO_DTYPE = {
    "int24": "uint8",
    "int48": "uint16",
    "int64": "uint32"
}

# for wfifr2 instr, input coefficient offset, c4 << 20 | c3 << 15 | c2 << 10 | c1 << 5 | c0
WFIFR2_OFFSET_LIST = [0, 5, 10, 15, 20]

# for wfifr2 instr, input coefficient bit-len, all coefficient is 5bits
WFIFR2_BITWIDE_LIST = [5, 5, 5, 5, 5]

# str is src0_dtype + src1_dtype, for wmul wmula wmuls wmulas, those kind dtype no need Pg
WMUL_NO_PG_DTYPE_LIST = ["s8s16", "u8s16", "u16s32", "u16u32", "s16u32", "s16s32"]

DTYPE_FOR_INPUT_SCALAR = {"uint8": 0, "int8": 1, "uint16": 2, "int16": 3, "uint32": 4,
                          "int32": 5, "uint64": 6,
                          "int64": 7, "float16": 8, "float32": 9}

DTYPE_INT_VALUE = {"uint8": (0, 255), "int8": (-128, 127),
                   "uint16": (0, 65535), "int16": (-32768, 32767),
                   "uint32": (0, 4294967295), "int32": (-2147483648, 2147483647),
                   "uint64": (0, 18446744073709551615),
                   "int64": (-9223372036854775808, 9223372036854775807)
                   }

DTYPE_FLOAT_VALUE = {"float16": (-65504.0, 65504.0),
                     "float32": (-3.4e+38, 3.4e+38),
                     "float64": (-1.8e+308, 1.8e+308)}


def check_scope(buffer_scope):
    """
    check the scope.
    """
    scope_list = [scope_cbuf, scope_ubuf, scope_ca, scope_cb, scope_cc]
    TikCheckUtil.check_var_in_list(buffer_scope, scope_list, "scope out of Tensor Scope")


def flat_list(list_in):
    """
    flatten list
    :param list_in: list
    :return: flattened list
    """
    for i in list_in:
        if not isinstance(i, (list, tuple)):
            yield i
        else:
            yield from flat_list(i)


def reduce_mul(shape):
    """
    calculate shape list value
    Apply a function of two arguments cumulatively to the items of a sequence,
    from left to right, so as to reduce the sequence to a single value.
    For example, reduce(lambda x, y: x*y, [1, 2, 3, 4, 5]) calculates
    ((((1*2)*3)*4)*5).  If initial is present, it is placed before the items
    of the sequence in the calculation, and serves as a default when the
    sequence is empty.

    Parameters
    ----------
    shape: TVM shape

    Returns
    -------
    int, reduce multiplication
    """
    if has_tik_var(shape):
        res = tvm.const(1, "int64")
    else:
        res = 1
    for i in shape:
        if is_basic_expr([i]):
            res = i * res
        else:
            res = res * i
    return res


def ceil_div(a_value, b_value):
    """
    ceil division
    Parameters
    ----------
    a_value :operator
    b_value :division value

    Returns
    -------
    computational results
    """
    return (a_value + b_value - 1) // b_value


def tvm_ceil_align(num, factor):
    """
    :param num:
    :param factor:
    :return:
    """
    return Div(num + factor - 1, factor) * factor


@lru_cache(maxsize=MIN_LRU_CACHE_SIZE)
def get_bit_len(dtype):
    """
    calculate bits of dtype of TVM
    Parameters
    ----------
    dtype: string, dtype of TVM

    Returns
    -------
    int, bit length of dtype.
    """
    index = 0
    for i in dtype:
        if i.isdigit():
            break
        index += 1
    if dtype[index:].isdigit():
        return int(dtype[index:])
    return TikCheckUtil.raise_error("get bits of dtype failed")


def check_imme_mask_full_mode(mask, tensor_bit_len):
    """
    check immediate_number mask value with full mode

    Parameters
    ----------
    mask : Effective operation on element, divided into two model: Continuous and bit by bit.
    tensor_bit_len : bit length of operation tensor's dtype

    Returns
    -------
    None
    """
    # for immediate list mask
    for data in mask:
        TikCheckUtil.check_type_match(data, int, "mask list value should be int, input type: %s" % type(data))
        TikCheckUtil.check_ge(
            data, MIN_MASK_HALF, "mask value should be in the range of [0, 2**64-1], input mask: %s" % data)
        TikCheckUtil.check_le(
            data, MAX_MASK_HALF, "mask value should be in the range of [0, 2**64-1], input mask: %s" % data)
        if tensor_bit_len == BIT_LEN_64:
            TikCheckUtil.check_le(
                data, MAX_MASK_HALF_INT64, "mask value should be in the range of [0, 2**32-1], input mask: %s" % data)
    if get_block_size() == 16 and not is_compatible_mode():
        TikCheckUtil.check_equality(
            mask[0], 0, "mask_h value should be 0 when size of block is 16, input mask: %d" % mask[0])
    # mask can not be all zero
    TikCheckUtil.check_not_equality(sum(mask), 0, "mask list value can not be [0, 0]")
    # b32, mask_h should be 0
    if tensor_bit_len in (BIT_LEN_32, BIT_LEN_64):
        TikCheckUtil.check_equality(
            mask[0], MASK_VALUE_ZERO, "mask_h should be 0 for b%s tensor, input mask_h: %s" % (tensor_bit_len, mask[0]))


def check_scatter_dict_for_overlap(src_dict, dst_dict, name, msg):
    """
    check src_dict and dst_dict
    """
    for buffer in src_dict.keys():
        if buffer in dst_dict.keys():
            if _check_src_dst_dict(src_dict, dst_dict, buffer) is False:
                TikCheckUtil.raise_error(
                    "%s %s not support partially address overlap, only support repeat_time=1"
                    " fully overlapping." % (name, msg))


def _check_src_dst_dict(src_dict, dst_dict, buffer):
    """
    check overlapping
    """
    for interval_src in src_dict[buffer]:
        for interval_dst in dst_dict[buffer]:
            if max(interval_src[0], interval_dst[0]) < min(interval_src[1], interval_dst[1]):
                return False
    return True


def get_mask_len(mask):
    """
    get mask len when in others situation
    """
    # mask length must greater than 1
    TikCheckUtil.check_var_in_list(
        len(mask), (MASK_LEN_FULL_MODE, MASK_LEN_B8_MODE), "length of mask should be 2 or 4")
    mask_len = 0
    if mask[MASK_HIGH_IDX] == 0:
        for index in range(MIN_INDEX, MAX_INDEX):
            if not mask[MASK_LOW_IDX] & (CONST_MASK_VALUE >> index) == 0:
                mask_len = MAX_LOW_MASK_LEN - index
                break
    else:
        for index in range(MIN_INDEX, MAX_INDEX):
            if not mask[MASK_HIGH_IDX] & (CONST_MASK_VALUE >> index) == 0:
                mask_len = MAX_LOW_MASK_LEN - index + MAX_INDEX
                break
    return mask_len


def check_scalar_dtype(var, msg="Scalar dtype should be int"):
    """
    check the Scalar dtype if int
    """
    if is_basic_expr(var):
        TikCheckUtil.check_var_in_list("int", var.dtype, msg)


def check_scalar_dtype_float(var, msg="Scalar dtype should be float"):
    """
    check the Scalar dtype if int
    """
    if is_basic_expr(var):
        TikCheckUtil.check_var_in_list("float", var.dtype, msg)


def check_scalar_int32(var, msg="Scalar dtype must be int32"):
    """
    check the Scalar dtype if int32
    """
    if is_basic_expr(var):
        TikCheckUtil.check_equality(var.dtype, "int32", msg)


def check_scalar_uint16(var, msg="Scalar dtype must be uint16"):
    """
    check the Scalar dtype if uint16
    """
    if is_basic_expr(var):
        TikCheckUtil.check_equality(var.dtype, "uint16", msg)


def instance_judge(data_list, type_list):
    """
    judging data list type

    Parameters
    ----------
    data_list : data list
    type_list : type list

    Returns
    -------
    bool, instance judge result
    """
    if not isinstance(data_list, (tuple, list)):
        data_list = [data_list]
    for data in data_list:
        if not isinstance(data, tuple(type_list)):
            return False
    return True


def tvm_immediate_number(data_list):
    """
    judging whether tvm immediate number is in tvm data type
    Parameters
    ----------
    data_list : data list

    Returns
    -------
    bool, tvm immediate number judge result
    """
    type_list = [tvm.tir.IntImm, tvm.tir.FloatImm]
    return instance_judge(data_list, type_list)


def is_immediate_number(data_list):
    """
    judging python immediate number

    Parameters
    ----------
    data_list : data list

    Returns
    -------
    bool, python immediate number judge result
    """
    return instance_judge(data_list, (float, int))


def tvm_var_type(data_list):
    """
    judging value data type

    Parameters
    ----------
    data_list : data list

    Returns
    -------
    bool, tvm var type judge result
    """
    type_list = [tvm.tir.Load, tvm.tir.expr.BinaryOpExpr, tvm.tir.Var, tvm.tir.Cast]
    return instance_judge(data_list, type_list)


def get_check_feed_dict(feed_dict, input_list_tensor, input_list_var, build_list_tensor, build_cce_names):
    """
    get and check input feed_dict
    """
    build_cce_input_names, build_cce_input_tensor_names, build_cce_input_var_names = build_cce_names
    input_feed_dict_names = " ".join(list(feed_dict.keys()))
    feed_dict_tensor = {}
    feed_dict_var = {}
    for key, value in feed_dict.items():
        if isinstance(value, (int, float)):
            feed_dict_var[key] = value
        else:
            feed_dict_tensor[key] = value

    feed_dict_tensor_names = " ".join(list(feed_dict_tensor.keys()))
    feed_dict_var_names = " ".join(list(feed_dict_var.keys()))
    if build_cce_input_names in (" ", ""):
        build_cce_input_names = "None"
    if input_feed_dict_names == "":
        input_feed_dict_names = "None"
    if build_cce_input_tensor_names == "":
        build_cce_input_tensor_names = "None"
    if build_cce_input_var_names == "":
        build_cce_input_var_names = "None"
    if feed_dict_tensor_names == "":
        feed_dict_tensor_names = "None"
    if feed_dict_var_names == "":
        feed_dict_var_names = "None"
    TikCheckUtil.check_equality(
        build_list_tensor, set(feed_dict.keys()),
        "BuildCCE input list is " + build_cce_input_names +
        ", but feed_dict list is " + input_feed_dict_names)
    TikCheckUtil.check_equality(
        set(input_list_var), set(feed_dict_var.keys()),
        "BuildCCE InputScalar list is " + build_cce_input_var_names +
        ", but feed_dict InputScalar list is " + feed_dict_var_names)
    TikCheckUtil.check_equality(
        set(input_list_tensor), set(feed_dict_tensor.keys()),
        "BuildCCE Tensor list is " + build_cce_input_tensor_names +
        ", but feed_dict Tensor list is " + feed_dict_tensor_names)
    return feed_dict_tensor, feed_dict_var


def get_instr_name_by_acc_mode(acc_mode, name):
    """
    update the intruction name according the acc_mode
    """
    if acc_mode == "ADDA":
        return name + "a"
    if acc_mode == "ADDSUB":
        return name + "sub"
    if acc_mode == "SUBA":
        return name + "a"
    if acc_mode == "MULA":
        return name + "a"
    if acc_mode == "MULAS":
        return "vectorized_wmulas"
    return name


class TikUtil():
    """
    Provide some common util function
    """

    @staticmethod
    def to_list(var):
        """
        if var is not list, convert to list
        """
        var1 = var
        if not isinstance(var, (list, tuple)):
            var1 = [var1]
        return var1

    @staticmethod
    def to_int(var):
        """
        convert var to int
        Parameters
        ----------
        var

        Returns
        -------
        1 or 0
        """
        if var:
            return 1
        return 0

    @staticmethod
    @lru_cache(maxsize=MIN_LRU_CACHE_SIZE)
    def get_storage_scope(name):
        """
        get the storage scope name

        Parameters
        ----------
        name: input

        Returns
        -------
        the result name
        """
        tmp = name.split(".")
        if tmp[0].lower() == "global":
            return "OUT"
        if tmp[1].count('UB'):
            return "UB"
        return tmp[1]

    @staticmethod
    def bit_type(dtype, msg="dtype error"):
        """
        bit type string
        Parameters
        ----------
        dtype: input data type
        msg: error msg

        Returns
        -------
        bit len string
        """

        if dtype in ("float16", "int16", "uint16"):
            bit_type_string = "_b16"
        elif dtype in ("float32", "int32", "uint32"):
            bit_type_string = "_b32"
        elif dtype in ("int8", "uint8"):
            bit_type_string = "_b8"
        else:
            raise RuntimeError(msg)

        return bit_type_string

    @staticmethod
    def dtype_is_b32(dtype):
        """
        check whether data type is int32/uint32/float32
        Parameters
        ----------
        dtype: input data type

        Returns
        -------
        True: int32/uint32/float32 data type
        False: not int32/uint32/float32 data type
        """
        if dtype in ("float32", "int32", "uint32"):
            return True
        return False

    @staticmethod
    def dtype_is_b16(dtype):
        """
        check whether data type is int16/uint16/float16
        Parameters
        ----------
        dtype: input data type

        Returns
        -------
        True: int16/uint16/float16 data type
        False: not int16/uint16/float16 data type
        """
        if dtype in ("float16", "int16", "uint16"):
            return True
        return False

    @staticmethod
    def dtype_is_b8(dtype):
        """
        check whether data type is int8/uint8
        Parameters
        ----------
        dtype: input data type

        Returns
        -------
        True: int8/uint8 data type
        False: not int8/uint8 data type
        """
        if dtype in ("int8", "uint8"):
            return True
        return False

    @staticmethod
    def store_mode(dtype, msg="dtype error"):
        """
        bit type string
        Parameters
        ----------
        dtype: input data type
        msg: error msg

        Returns
        -------
        store mode
        """

        if dtype in ("float16", "int16", "uint16"):
            store_mode_string = "NORM_B16"
        elif dtype in ("float32", "int32", "uint32"):
            store_mode_string = "NORM_B32"
        elif dtype in ("int8", "uint8"):
            store_mode_string = "NORM_B8"
        else:
            raise RuntimeError(msg)

        return store_mode_string


def reassign_mask(tik_instance, mask, one_block_elements):
    """
    Reassign the mask in nano
    Parameters
    ----------
    tik_instance: tik.TIK() class obj
    mask: control effective elements
    one_block_elements: number of elements in a blcok

    Returns
    -------
    mask1 mask2
    """
    """
    The mask is split into mask1 and mask2 because block_size is 16
    mask: 100-
    ----------------   ----------------   ----------------   ----------------
    ----------------   ----------------   ----
    mask1: 48-, mask2: 48=, tails: 4*
    --------========   --------========   --------========   --------========
    --------========   --------========   ****
    mask1 = 48 + 4, mask2 = 48
    
    
    mask: 108-
    ----------------   ----------------   ----------------   ----------------
    ----------------   ----------------   ------------
    mask1: 48-, mask2: 48=, tails: 12*
    --------========   --------========   --------========   --------========
    --------========   --------========   ************
    mask1 = 48 + 8, mask2 = 48 + (12 - 8)
    """
    mask_bit_0_8 = 0xFF
    mask_bit_8_16 = 0xFF00
    mask_bit_16_24 = 0xFF0000
    mask_bit_24_32 = 0xFF000000
    mask_bit_32_40 = 0xFF00000000
    mask_bit_40_48 = 0xFF0000000000
    mask_bit_48_56 = 0xFF000000000000
    mask_bit_56_64 = 0xFF00000000000000
    if type(mask) in (list, tuple):
        # int or scalar
        mask_h, mask_l = mask[0], mask[1]
        if not isinstance(mask_h, int) and mask_h.dtype == "int64":
            mask_h = tik_instance.Scalar(name="new_mask_h", dtype="uint64")
            mask_l = tik_instance.Scalar(name="new_mask_l", dtype="uint64")
            mask_h.set_as(mask[0])
            mask_l.set_as(mask[1])
        mask_h1 = (mask_l & mask_bit_0_8) + ((mask_l & mask_bit_16_24) >> 8) + \
                  ((mask_l & mask_bit_32_40) >> 16) + ((mask_l & mask_bit_48_56) >> 24) + \
                  ((mask_h & mask_bit_0_8) << 32) + ((mask_h & mask_bit_16_24) << 24) + \
                  ((mask_h & mask_bit_32_40) << 16) + ((mask_h & mask_bit_48_56) << 8)
        mask_h2 = ((mask_l & mask_bit_8_16) >> 8) + ((mask_l & mask_bit_24_32) >> 16) + \
                  ((mask_l & mask_bit_40_48) >> 24) + ((mask_l & mask_bit_56_64) >> 32) + \
                  ((mask_h & mask_bit_8_16) << 24) + ((mask_h & mask_bit_24_32) << 16) + \
                  ((mask_h & mask_bit_40_48) << 8) + (mask_h & mask_bit_56_64)
        if isinstance(mask_h1, int):
            mask1, mask2 = [0, mask_h1], [0, mask_h2]
        else:
            scalar_0 = tik_instance.Scalar(name="scalar_0", init_value=0)
            mask1, mask2 = [scalar_0, mask_h1], [scalar_0, mask_h2]
    elif isinstance(mask, int):
        # int
        mask1 = mask // (one_block_elements * 2) * one_block_elements
        mask2 = mask1
        tails = mask - mask1 - mask2
        if tails <= one_block_elements:
            mask1 += tails
        if tails > one_block_elements:
            mask1 += one_block_elements
            mask2 += tails - one_block_elements
    else:
        # scalar
        mask1 = mask // (one_block_elements * 2) * one_block_elements
        mask2 = mask1
        tails = mask - mask1 - mask2
        mask1 += tvm.tir.Min(tails.get(), one_block_elements)
        mask2 += tvm.tir.Max(0, (tails - one_block_elements).get())
    # The types of mask[0] and mask[1] are the same
    return mask1, mask2


def check_mask1_mask2(mask1, mask2):
    """
    Check whether mask is reasonable
    Parameters
    ----------
    mask1: first mask
    mask2: second mask

    Returns
    -------
    mask1 mask2
    """
    enable_mask1 = False
    enable_mask2 = False
    check_mask1, check_mask2 = mask1, mask2
    if isinstance(check_mask1, list):
        check_mask1, check_mask2 = mask1[1], mask2[1]
    if not (isinstance(check_mask1, int) and check_mask1 == 0):
        enable_mask1 = True
    if not (isinstance(check_mask2, int) and check_mask2 == 0):
        enable_mask2 = True
    return enable_mask1, enable_mask2


def compatible_blk_continuous_mask(mask_o, dtype_size):
    """
    In compatibility mode, continuous data in a single repeat, mask split.
    """
    if dtype_size == 4:
        # dtype is b32
        mask1 = mask_o[1] & 0xffffffff
        mask2 = mask_o[1] >> 32
        mask_o1 = [mask_o[0], mask1]
        mask_o2 = [mask_o[0], mask2]
    else:
        # dtype is b16
        mask_o1 = [0, mask_o[1]]
        mask_o2 = [0, mask_o[0]]

    return mask_o1, mask_o2


def check_is_atomic_add_attr(original_shape):
    """
    check whether attr needs to be added

    Parameters
    ----------
    original_shape:the tensor's original shape

    Returns
    -------
    True or False
    """
    if get_soc_name() == "Ascend910B" and \
            (type(original_shape) in (tuple, list)) and isinstance(reduce_mul(original_shape), Expr):
        for element in original_shape:
            if isinstance(element, int) and (element == MAX_INT32_VALUE or element == MAX_INT64_VALUE):
                return False
        return True
    return False