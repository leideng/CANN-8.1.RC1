#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     common_check_func.py
DESC:     tik common check func
CREATED:  2021-12-28 20:17
MODIFIED: 2021-12-28 20:17
"""

import numpy as np
from tbe.tik.tik_lib import Expr
from tbe.tik.common.util import is_basic_expr
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import TikUtil
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import get_mask_len
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import check_scalar_dtype
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.util import check_imme_mask_full_mode
from tbe.tik.common.tik_get_soc_name import get_compatible_rep_size
from tbe.tik.tik_lib.tik_params import ONE_REP_BYTE_SIZE
from tbe.tik.tik_lib.tik_params import MAX_PAD_MODE
from tbe.tik.tik_lib.tik_params import MIN_MODE_NUMBER
from tbe.tik.tik_lib.tik_params import UINT_MIN
from tbe.tik.tik_lib.tik_params import UINT8_MAX
from tbe.tik.tik_lib.tik_params import INT8_MIN
from tbe.tik.tik_lib.tik_params import INT8_MAX
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import MASK_LEN_CONTINOUS_MODE
from tbe.tik.tik_lib.tik_params import MASK_HIGH_IDX
from tbe.tik.tik_lib.tik_params import MIN_INDEX
from tbe.tik.tik_lib.tik_params import MAX_INDEX
from tbe.tik.tik_lib.tik_params import CONST_MASK_VALUE
from tbe.tik.tik_lib.tik_params import MASK_LEN_64
from tbe.tik.tik_lib.tik_params import MASK_LOW_IDX
from tbe.tik.tik_lib.tik_params import MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import SRC_LIST_LEN
from tbe.tik.tik_lib.tik_params import MIN_MASK
from tbe.tik.tik_lib.tik_params import MAX_MASK
from tbe.tik.tik_lib.tik_params import MAX_MASK_64
from tbe.tik.tik_lib.tik_params import MASK_LEN_FULL_MODE
from tbe.tik.tik_lib.tik_params import BIT_LEN_16
from tbe.tik.tik_lib.tik_api_constants import MVF_ELE_SIZE_MAP
from tbe.tik.tik_lib.tik_api_constants import MVF_ELE_SIZE_MAP_V300
from tbe.tik.tik_lib.tik_api_constants import MVF_STRIDE_MAP
from tbe.tik.tik_lib.tik_api_constants import MVF_STRIDE_MAP_V300
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
# byte per proposal for vms4v2
_BYTES_PER_PROPOSAL = 8

# element number per proposal for vms4
_ELEMENTS_PER_PROPOSAL = 8
_MIN_L1_H = 3
_MIN_L1_W = 1
_MAX_L1 = 4095


def check_address_align_with_context(context, tensor_list, name_list, align_size):
    """
    check tensor start address if aligned to given align_size

    Parameters
    ----------
    context: debug context
    tensor_list: tensor list
    name_list: tensor name list
    align_size: align addr

    Returns
    -------
    None
    """
    for tensor, name in zip(tensor_list, name_list):
        tensor_start = context.evaluate_expr(tensor.offset)
        if tensor_start * DTYPE_SIZE[tensor.dtype] % align_size != 0:
            TikCheckUtil.raise_error(
                "Address align error, %s[%s] is not %s Byte align" % (name, tensor_start, align_size))


def split_rep_stride(dst_rep_stride):
    """
    In 910b single src Vector instr, the {Xt[55:52], Xt[39:32]} is the repeat stride size for dst.
    dst_rep_stride needs to be split into high 4bits and low 8bits.

    Parameters
    ----------
    dst_rep_stride: repeat stride size for dst

    Returns
    -------
    high 4bits and low 8bits of dst_rep_stride
    """
    dst_rep_stride_low = dst_rep_stride & 0b11111111
    dst_rep_stride_high = (dst_rep_stride & 0b111100000000) >> 8
    return dst_rep_stride_low, dst_rep_stride_high


def check_mask_valid(mask, tensor_bit_len):
    """
    this func can only be used for tik codegen, cannot be used for tik debug!!
    """
    max_mask = MAX_MASK if not TikSocManager.is_nano_soc() else MAX_MASK_64
    mask_list = TikUtil.to_list(mask)
    if len(mask_list) == MASK_LEN_CONTINOUS_MODE and is_basic_expr(mask_list):
        for msk in mask_list:
            check_scalar_dtype(msk,
                               "scalar_mask should be a scalar's dtype"
                               " of int/uint, input dtype %s" % type(msk))
    elif len(mask_list) == MASK_LEN_CONTINOUS_MODE and \
            is_immediate_number(mask_list):
        for msk in mask_list:
            TikCheckUtil.check_type_match(
                msk, int, "mask should be int type, input type %s" % type(msk))
        # for immediate mask, value should  be in range of [1,128], b16
        if tensor_bit_len == BIT_LEN_16:
            TikCheckUtil.check_in_range_by_dtype(
                mask_list[0], msg="mask value should be in the range of [%d, %d] for b16 "
                "tensor, input mask %s" % (MIN_MASK, max_mask, mask_list[0]), var_range=[MIN_MASK, max_mask])
        # b32 case
        else:
            TikCheckUtil.check_in_range_by_dtype(
                mask_list[0], msg="mask value should be in the range of [%d, %d] for b32 "
                "tensor, input mask %s" % (MIN_MASK, MAX_MASK_64, mask_list[0]), var_range=[MIN_MASK, MAX_MASK_64])
    elif len(mask_list) == MASK_LEN_FULL_MODE and is_basic_expr(mask_list):
        for msk in mask_list:
            check_scalar_dtype(msk,
                               "scalar_mask should be a scalar's dtype"
                               " of int/uint, input dtype %s" % type(msk))
    elif len(mask_list) == MASK_LEN_FULL_MODE and \
            is_immediate_number(mask_list):
        check_imme_mask_full_mode(mask_list, tensor_bit_len)
    else:
        TikCheckUtil.raise_error("not support this type of mask now")


def concate_deqscale_vconv(deqscale_list, bit_46):
    """
    check vmrgsort overlapping

    Parameters
    ----------
    deqscale_list : a list of scale and offset
    bit_46 : bit 46 of deqscale

    Returns
    -------
    deqscale
    """
    _min_offset = -256
    _max_offset = 255
    TikCheckUtil.check_equality(len(deqscale_list), 2,
                                "when deqscale is a list, the length should be 2.")
    scale, offset = deqscale_list
    TikCheckUtil.check_type_match(
        scale, float, "when deqscale is a list, deqscale[0] should be int/float")
    TikCheckUtil.check_in_range_by_dtype(scale, "float32",
                                         "when deqscale is a list, deqscale[0] should in range of float32")
    TikCheckUtil.check_type_match(
        offset, int, "when deqscale is a list, deqscale[1] should be int")
    TikCheckUtil.check_in_range_by_dtype(offset, msg="when deqscale is a list, deqscale[1] should in [%d, %d]]"
                                         % (_min_offset, _max_offset),  var_range=[_min_offset, _max_offset])

    # change float to uint32
    np_dt = getattr(np, "float32")
    target_np = getattr(np, 'uint32')
    scale = np_dt(scale).view(target_np).item()
    # change int to 9-bit complement
    if offset < 0:
        offset += 2 ** 9

    # deqscale[45:37] is viewed as offset
    # deqscale[31:13] is viewed as scale, deqscale[12:0] is reversed
    return (bit_46 << 46) + (offset << 37) + scale


def check_overlapping_vsort32(dst_offset, src0_offset, repeat_times, dtype):
    """
    check vms overlapping

    Parameters
    ----------
    dst_offset : dst tensor offset
    src0_offset : src0 tensor offset
    repeat_times: repeat_times: times of invoke this instrction
    dtype : float16 or float32

    Returns
    -------
    None
    """
    if any(not isinstance(i, int) for i in [dst_offset, src0_offset, repeat_times]):
        return
    read_end = src0_offset + repeat_times * 32
    for i in range(repeat_times - 1):
        write_start = dst_offset + i * 256 // DTYPE_SIZE[dtype]
        write_end = dst_offset + (i + 1) * 256 // DTYPE_SIZE[dtype]
        read_start = src0_offset + (i + 1) * 32
        if not (write_end <= read_start or write_start >= read_end):
            TikCheckUtil.raise_error("dst and src0 tensor address overlapping error.")


def check_overlapping_vmrgsort4(dst_offset, src_offset, element_count_list, repeat_times, idx):
    """
    check vmrgsort4 overlapping

    Parameters
    ----------
    dst_offset : dst tensor offset
    src_offset : src tensor offset
    element_count_list : element_count_list
    repeat_times: repeat_times: times of invoke this instrction
    idx: index of src tensor in src_list

    Returns
    -------
    None
    """
    if any(not isinstance(i, int) for i in [dst_offset, src_offset, repeat_times] + list(element_count_list)):
        return
    repeat_len = sum(element_count_list)
    write_start = dst_offset
    write_end = repeat_times * repeat_len * _ELEMENTS_PER_PROPOSAL + dst_offset
    read_start = src_offset
    read_end = (element_count_list[idx] + (repeat_times - 1) * repeat_len) * _ELEMENTS_PER_PROPOSAL + src_offset

    if not (write_end <= read_start or write_start >= read_end):
        TikCheckUtil.raise_error("dst and src tensor address overlapping error.")


def check_overlapping_vmrgsort(vmrgsort_params):
    """
    check vmrgsort overlapping

    Parameters
    ----------

    vmrgsort_params: contains
    -dst_offset : dst tensor offset
    -src_offset : src tensor offset
    -element_count_list : element_count_list
    -repeat_times: repeat_times: times of invoke this instrction
    -dtype : float16 or float32
    -idx: index of src tensor in src_list

    Returns
    -------
    None
    """
    dst_offset, src_offset, element_count_list, repeat_times, dtype, idx = vmrgsort_params
    if any(not isinstance(i, int) for i in [dst_offset, src_offset, repeat_times] + list(element_count_list)):
        return
    repeat_len = sum(element_count_list)
    write_start = dst_offset
    write_end = repeat_times * repeat_len * _BYTES_PER_PROPOSAL // DTYPE_SIZE[dtype] + dst_offset
    read_start = src_offset
    read_end = (element_count_list[idx] + (
            repeat_times - 1) * repeat_len) * _BYTES_PER_PROPOSAL // DTYPE_SIZE[dtype] + src_offset

    if not (write_end <= read_start or write_start >= read_end):
        TikCheckUtil.raise_error("dst and src tensor address overlapping error.")


def check_vms4v2_repeat_times(repeat_times, element_count_list, if_exhausted_suspension):
    """
    check vms v2 repeat_times range, type

    Parameters
    ----------
    repeat_times: repeat_times: times of invoke this instrction
    element_count_list : length of the proposal list
    if_exhausted_suspension : 0 not stop, 1 stop

    Returns
    -------
    None
    """
    length_list_same_flag = True
    if all(isinstance(value, int) for value in element_count_list):
        if len(set(element_count_list)) != 1:
            length_list_same_flag = False
    valid_bit_15_flag = (len(element_count_list) == SRC_LIST_LEN)
    if if_exhausted_suspension or length_list_same_flag is False or \
            valid_bit_15_flag is False:
        if isinstance(repeat_times, int):
            TikCheckUtil.check_equality(
                repeat_times, MIN_REPEAT_TIMES,
                "When input params cannot meet repeat criterions as follows: 1."
                " if_exhuasted_suspension == False, 2. src_list has 4 source tensor"
                ", 3. elements of element_count_list are equal to each other,"
                " repeat_times should be 1, but input repeat_times: %s" % repeat_times)


def check_vms4_repeat_times(repeat_times, element_count_list, valid_bit,
                            if_exhausted_suspension):
    """
    check vms repeat_times range, type

    Parameters
    ----------
    repeat_times: repeat_times: times of invoke this instrction
    element_count_list : length of the proposal list
    valid_bit:   0001 one lines are valid
    -            0011 two lines are valid
    -            0111 three lines are valid
    -            1111 four lines are valid
    if_exhausted_suspension : 0 not stop, 1 stop

    Returns
    -------
    None
    """
    length_list_same_flag = True
    if all(isinstance(value, int) for value in element_count_list):
        if len(set(element_count_list)) != 1:
            length_list_same_flag = False
    valid_bit_15_flag = True
    if Expr(valid_bit).eval_value() is not None:
        if Expr(valid_bit).eval_value() != 15:
            valid_bit_15_flag = False
    if if_exhausted_suspension or length_list_same_flag is False or \
            valid_bit_15_flag is False:
        if isinstance(repeat_times, int):
            TikCheckUtil.check_equality(
                repeat_times, MIN_REPEAT_TIMES,
                "When input params cannot meet repeat criterions as follows: 1."
                " if_exhuasted_suspension == False, 2. valid_bit in decimal is "
                "15, 3. elements of element_count_list are equal to each other,"
                " repeat_times should be 1, but input repeat_times: %s" % repeat_times)


def get_32bit_dtype_mask_len(mask, mask_mode):
    """
    get mask len when dtype bit length is 32

    Parameters
    ----------
    mask: mask
    mask_mode: mask mode
    """
    mask_len = 0
    if is_basic_expr(mask):
        mask_len = MASK_LEN_64
    elif len(mask) == MASK_LEN_CONTINOUS_MODE:
        mask_len = mask[MASK_HIGH_IDX]
        if mask_len is None:
            mask_len = MASK_LEN_64
        if mask_mode == "normal":
            TikCheckUtil.check_le(
                mask_len, MASK_LEN_64,
                "mask should not be more than 64 when dtype bit length is 32, mask:%s" % mask)
    else:
        TikCheckUtil.check_equality(
            mask[MASK_HIGH_IDX], 0,
            "mask list should be [0, xxx] when dtype bit length is 32")
        for index in range(MIN_INDEX, MAX_INDEX):
            if not mask[MASK_LOW_IDX] & (CONST_MASK_VALUE >> index) == 0:
                mask_len = MAX_INDEX - index
                break
    return mask_len


def calculate_vecotor_max_offset(repeat_times, params, mask_len, stride_unit, nblock):
    """
    calculate vector max offset
    Parameters
    ----------
    repeat_times: repeat times
    params: params
    mask_len: mask len
    stride_unit: stride unit
    nblock: nblock

    Returns
    -------

    """
    blk_stride = params.stride1
    rep_stride = params.stride2
    if isinstance(repeat_times, int) and repeat_times == 0:
        return 0

    blk_num_last_rep = ceil_div(mask_len, params.block_len)
    ele_num_last_blk = mask_len % params.block_len if mask_len % params.block_len else params.block_len

    # last rep has multi blocks, blk_stride is zero, last blk num must be set to block len
    if blk_num_last_rep > 0 and blk_stride == 0:
        ele_num_last_blk = params.block_len

    # blk_stride: stride, rep_stride: stride, unit: 32B
    if stride_unit == 0:
        max_offset = ((repeat_times - 1) * rep_stride +
                      (blk_num_last_rep - 1) * blk_stride) * params.block_len + \
                     ele_num_last_blk
    # blk_stride: stride, rep_stride: gap, unit: 32B
    elif stride_unit == 1:
        max_offset = \
            ((repeat_times - 1) * (rep_stride + (nblock - 1) * blk_stride + 1)
             + (blk_num_last_rep - 1) * blk_stride) * params.block_len + \
            ele_num_last_blk
    # blk_stride: gap, rep_stride: stride, unit: element
    elif stride_unit == 2:
        max_offset = (repeat_times - 1) * rep_stride + \
                     (blk_num_last_rep - 1) * (params.block_len + blk_stride) + \
                     ele_num_last_blk
    # blk_stride: gap, rep_stride: gap, unit: element
    else:
        max_offset = \
            (repeat_times - 1) * \
            (rep_stride + (nblock - 1) * (blk_stride + params.block_len) +
             params.block_len) + \
            (blk_num_last_rep - 1) * (blk_stride + params.block_len) + \
            ele_num_last_blk

    return max_offset


def check_addr_overlap_v4dtrans(v4dtrans_overlap_tuple):
    """
    check address overlap for v4dtrans
    """
    dst, src, m_len, channels, dst_offset, src_offset = v4dtrans_overlap_tuple
    if dst_offset is None or src_offset is None:
        return
    if src.buffer == dst.buffer:
        need_ele = m_len * channels
        total_repeat_size = need_ele * get_bit_len(src.dtype) // ONE_BYTE_BIT_LEN
        src_need_offset = Expr(src_offset + need_ele).eval_value()
        dst_need_offset = Expr(dst_offset + need_ele).eval_value()
        if total_repeat_size > ONE_REP_BYTE_SIZE:
            if src_offset <= src_need_offset <= dst_offset or \
                    dst_offset <= dst_need_offset <= src_offset:
                pass
            else:
                TikCheckUtil.raise_error(
                    "when m_len*channels*dtype*size>256B, v4dtrans"
                    " dst, src not support address overlapping.")
        else:
            if src_offset <= src_need_offset <= dst_offset or \
                    dst_offset <= dst_need_offset <= src_offset or \
                    src_offset == dst_offset:
                pass
            else:
                TikCheckUtil.raise_error(
                    "when m_len*channels*dtype*size<=256B, "
                    "v4dtrans dst, src address overlap error,"
                    " only support 100 percent same.")


def check_overlap_param(mask, repeat, dst_offset, src_offset):
    """
    get a flag of address overlap check
    """
    is_check = True
    if repeat <= 0:
        is_check = False
    if dst_offset is None or src_offset is None:
        is_check = False
    if is_basic_expr(TikUtil.to_list(mask)):
        is_check = False
    return is_check


def check_depthwise_conv_params(src_fm, pad_mode, l1_h, l1_w, feature_offset):
    """
    check depthwise_conv params range

    Parameters
    ----------
    src_fm: source tensor_left
    pad_mode:   0 - no padding
    -           1 - two colume on right side
    -           2 - two colume on left side
    -           3 - one colume on right&left side
    l1_h: height of src_fm
    l1_w: width of src_fm
    feature_offset: the feature map matrix offset, dtype is same as src_fm.
    -               If no offset is needed, set to 8'b0. Only works for
    -               src_fm dtype is b8.

    Returns
    -------
    None
    """
    TikCheckUtil.check_in_range_by_dtype(
        pad_mode, msg="pad_mode should be in the range of [%d, %d], input pad_mode: %s"
        % (MIN_MODE_NUMBER, MAX_PAD_MODE, pad_mode), var_range=[MIN_MODE_NUMBER, MAX_PAD_MODE])
    TikCheckUtil.check_in_range_by_dtype(
        l1_h, msg="l1_h should be in the range of [%d, %d], input l1_h : %s"
                  % (_MIN_L1_H, _MAX_L1, l1_h), var_range=[_MIN_L1_H, _MAX_L1])
    TikCheckUtil.check_in_range_by_dtype(
        l1_w, msg="l1_w should be in the range of [%d, %d], input l1_w : %s"
                  % (_MIN_L1_W, _MAX_L1, l1_w), var_range=[_MIN_L1_W, _MAX_L1])
    if src_fm.dtype == "uint8":
        TikCheckUtil.check_in_range_by_dtype(
            feature_offset, "uint8",
            "when src_fm dtype is uint8, feature_offset should be in the range "
            "of [%d, %d], input feature_offset : %s" % (UINT_MIN, UINT8_MAX, feature_offset))
    elif src_fm.dtype == "int8":
        TikCheckUtil.check_in_range_by_dtype(
            feature_offset, "int8",
            "when src_fm dtype is int8, feature_offset should be in the range "
            "of [%d, %d], input feature_offset : %s" % (INT8_MIN, INT8_MAX, feature_offset))
    else:  # float16
        if isinstance(feature_offset, int):
            TikCheckUtil.check_equality(feature_offset, 0,
                                        "when src_fm dtype is float16, feature_offset is invalid so it "
                                        "should be 0, input feature_offset : %s" % feature_offset)


def check_mvf_data_move_overflow(mv_data_params, index_num, dst_stride, dst_offset, src_offset):
    """
    check mvf mov tensor overflow dst, src_index, index_num, ele_size
    """
    dst_extent = None
    if TikSocManager.is_v300_610l_soc():
        ele_size_byte = MVF_ELE_SIZE_MAP_V300[mv_data_params.ele_size]
    else:
        ele_size_byte = MVF_ELE_SIZE_MAP[mv_data_params.ele_size]

    if isinstance(dst_stride, int):
        if TikSocManager.is_v300_610l_soc():
            dst_stride_byte = MVF_STRIDE_MAP_V300[dst_stride]
        else:
            dst_stride_byte = MVF_STRIDE_MAP[dst_stride]

        dst_extent = Expr(((index_num - 1) * dst_stride_byte +
                           ele_size_byte) // DTYPE_SIZE[
            mv_data_params.dst.dtype]).eval_value()
        TikCheckUtil.check_ge(
            dst_stride_byte, ele_size_byte,
            "dst_stride's Byte number(%s) should not be less than"
            " ele_size's Byte number(%s)" %
            (dst_stride_byte, ele_size_byte))
    # check src_index tensor
    index_extent = Expr(src_offset + index_num).eval_value()
    index_total_size = reduce_mul(mv_data_params.src_index.original_shape)
    if index_extent is not None:
        TikCheckUtil.check_le(
            index_extent, index_total_size,
            "src_index tensor overflow, src_index"
            " need %s element, but only "
            "%s" % (index_extent, index_total_size))
    # check dst
    dst_offset = Expr(dst_offset).eval_value()
    dst_total_size = reduce_mul(mv_data_params.dst.original_shape)
    if dst_extent is not None and dst_offset is not None:
        dst_extent_offset = dst_offset + \
                            (dst_extent // DTYPE_SIZE[mv_data_params.dst.dtype])
        TikCheckUtil.check_le(
            dst_extent_offset, dst_total_size,
            "dst tensor overflow, dst need %s element, but only "
            "%s" % (dst_extent_offset, dst_total_size))


def check_sel_overflow(dst, src0, sel, mask, repeat_times):
    """
    vsel instruction, check sel tensor overflow when mode is 1 or 2

    Parameters
    ----------
    dst : destination operator
    src0 : source operator
    sel : sel operator, when mode is 1 or 2, should be Tensor
    mask : Effective operation on element
    repeat_times : Repeated iterations times

    Returns
    -------
    None
    """
    sel_bit_len = get_bit_len(sel.dtype)
    parallelism = get_compatible_rep_size() * ONE_BYTE_BIT_LEN // max(get_bit_len(dst.dtype), get_bit_len(src0.dtype))
    offset = sel.offset
    total_size = reduce_mul(sel.original_shape)
    sel_num_each_repeat = parallelism // sel_bit_len
    mask_list = TikUtil.to_list(mask)
    if not is_basic_expr(mask_list):
        if len(mask_list) == MASK_LEN_CONTINOUS_MODE:
            mask_len = mask_list[MASK_HIGH_IDX]
        else:
            mask_len = get_mask_len(mask_list)
        extend_offset = (repeat_times - 1) * sel_num_each_repeat + ceil_div(mask_len, sel_bit_len)
        if Expr(extend_offset + offset).eval_value() is not None:
            TikCheckUtil.check_le(
                Expr(extend_offset + offset).eval_value(),
                total_size,
                "sel tensor overflow, expected elements: %s, actual elements: %s"
                % (Expr(extend_offset + offset).eval_value(), total_size))
