#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     common_util.py
DESC:     common util file for tik api and debug
CREATED:  2020-01-10 19:02:50
MODIFIED: 2020-01-10 19:02:50
"""
from collections import namedtuple
import numpy as np
from tbe import tvm
from tbe.common.platform import intrinsic_check_support
from tbe.tik.tik_lib.tik_basic_data import BasicData
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_expr import BasicExpr
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.common.common_check_func import calculate_vecotor_max_offset
from tbe.tik.common.common_check_func import get_32bit_dtype_mask_len
from tbe.tik.common.util import DTYPE_INT_VALUE
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import TikUtil
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import get_mask_len
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import check_scalar_dtype
from tbe.tik.common.tik_api_map import ONLY_TIK_API_MAP
from tbe.tik.common.tik_api_map import int64_support_map
from tbe.tik.common.tik_api_map import int64_intrinsic_map
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import DEFAULT_STRIDE
from tbe.tik.tik_lib.tik_params import MASK_LEN_CONTINOUS_MODE
from tbe.tik.tik_lib.tik_params import MASK_HIGH_IDX
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MASK_VALUE_ZERO
from tbe.tik.tik_lib.tik_params import BIT_LEN_32
from tbe.tik.tik_lib.tik_params import BIT_LEN_16
from tbe.tik.tik_lib.tik_params import BIT_LEN_8
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MASK_LEN_128
from tbe.tik.tik_lib.tik_params import MASK_LOW_IDX
from tbe.tik.tik_lib.tik_params import BLK_16_LIST
from tbe.tik.tik_lib.tik_params import BLK_32_LIST
from tbe.tik.tik_lib.tik_params import TRANS_TIK_API_TO_INSTR_MAP
from tbe.tik.tik_lib.tik_params import MASK_LEN_B8_MODE
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.common.tik_get_soc_name import get_soc_core_type
from tbe.tik.common.tik_get_soc_name import get_compatible_blk_size
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_params import ALIGNED_ADDR
from tbe.tik.tik_lib.tik_params import ALIGNED_ADDR_NANO
from tbe.tik.debug.util import get_dtype_size
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_12_BITS
from tbe.tik.tik_lib.tik_data_move_api.tik_data_move_common import AIPP_INPUT_TYPE_SWAP_ALIGN

TENSORADDR_DTYPE = "int64"
_BYTE_PER_C0 = 32
_MIN_BLK_LEN = 1
_MIN_DST_BLK_STRIDE = 1


DMA_V100_ALIGN = {
    'L1': 32,
    'UB': 32,
    'OUT': 1,
    'L0C16': 512,
    'L0C32': 1024,
    'L0A': 512,
    'L0B': 512
}

TENSOR_START_ADDR_ALIGN = {
    'UB': 32,
    'L1': 512,
    'L0A': 512,
    'L0B': 512,
    'L0C16': 512,
    'L0C32': 1024
}


def vec_template_align(dtype):
    """
    vector instr align
    Parameters
    ----------
    dtype: data type

    Returns
    -------
    align size
    """
    if TikSocManager.is_hisi_soc():
        align_size = get_dtype_size(dtype)
    else:
        if TikSocManager.is_nano_soc():
            align_size = ALIGNED_ADDR_NANO
        else:
            align_size = ALIGNED_ADDR

    return align_size


def check_vector_stride(blk_stride_list, rep_stride_list, blk_stride_range, rep_stride_range, name):
    """
    check blk_stride and rep_stride params of vector instructions

    Parameters
    ----------
    blk_stride_list : list of dst_blk_stride and src_blk_stride
    rep_stride_list : list of dst_rep_stride and src_rep_stride
    blk_stride_range : upper bound of blk_stride
    rep_stride_range : upper bound of rep_stride
    name : list of tensor name

    Returns
    -------
    None
    """
    if rep_stride_list is not None:
        TikCheckUtil.check_type_match(rep_stride_list, (list, tuple),
                                      "rep_stride_list should be list or tuple")
        TikCheckUtil.check_equality(len(rep_stride_list), len(name),
                                    "rep_stride_list and name should contains "
                                    "same number of elements")
        TikCheckUtil.check_var_in_list(rep_stride_range, [MAX_REP_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_12_BITS,
                                                          MAX_REP_STRIDE_SINGLE_BYTE], "rep_stride_range not support!")
        for i, rep_stride in enumerate(rep_stride_list):
            TikCheckUtil.check_type_match(rep_stride, (int, BasicExpr),
                                          "%s_rep_stride should be int, "
                                          "Expr or Scalar, input "
                                          "type is %s" % (name[i], type(rep_stride)))
            check_scalar_dtype(rep_stride,
                               "scalar %s_rep_stride should be "
                               "a scalar of int/uint" % name[i])
            TikCheckUtil.check_in_range_by_dtype(
                rep_stride, msg="%s_rep_stride should be in the range of [%d, %d], "
                "input value is %s" % (name[i], DEFAULT_STRIDE, rep_stride_range, rep_stride),
                var_range=[DEFAULT_STRIDE, rep_stride_range])
    if blk_stride_list is not None:
        TikCheckUtil.check_type_match(blk_stride_list, (list, tuple),
                                      "blk_stride_list should be list  or tuple")
        TikCheckUtil.check_equality(len(blk_stride_list), len(name),
                                    "blk_stride_list and name should contains "
                                    "same number of elements")
        TikCheckUtil.check_var_in_list(blk_stride_range, [MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_BLK_STRIDE_SINGLE_BYTE],
                                       "blk_stride_range not support!")
        for i, blk_stride in enumerate(blk_stride_list):
            TikCheckUtil.check_type_match(
                blk_stride, (int, BasicExpr),
                "%s_blk_stride should be int, Expr or Scalar, input type is %s"
                % (name[i], type(blk_stride)))
            check_scalar_dtype(blk_stride,
                               "scalar_%s_blk_stride should be "
                               "a scalar of int/uint" % name[i])
            TikCheckUtil.check_in_range_by_dtype(
                blk_stride, msg="%s_blk_stride should be in the range of [%d, %d], input value"
                " is %s" % (name[i], DEFAULT_STRIDE, blk_stride_range, blk_stride),
                var_range=[DEFAULT_STRIDE, blk_stride_range])


def _cal_mask_rep_for_counter_mode(params):
    """
    cal mask repeat for counter mode

    Parameters
    ----------
    params: contains mask, nblock, block_len, repeat, tensor_dtype, is_910b

    Returns
    -------

    """
    mask = params.mask
    nblock = params.nblock
    repeat = params.repeat
    TikCheckUtil.check_type_match(
        mask, int, "mask here should be int, "
                   "input type of mask: %s" % type(mask))
    if not params.is_910b:
        repeat = ceil_div(mask, nblock * params.block_len)
        mask = mask % (nblock * params.block_len)
        if mask == MASK_VALUE_ZERO:
            mask = nblock * params.block_len
    else:
        nblock = ceil_div(mask * DTYPE_SIZE[params.tensor.dtype], get_compatible_blk_size())
        mask = mask % (nblock * params.block_len)
        if mask == MASK_VALUE_ZERO:
            mask = nblock * params.block_len
    return mask, nblock, repeat


def vector_tensor_overflow_check(check_params):
    """
    check overflow vector tensor
    Parameters
    ----------
    check_params
    - tensor: tensor object
    - mask: instr's mask
    - nblock: effective block nums in one repeat
    - block_len: number of elements in one block
    - repeat: the times of instr run
    - stride1: block stride
    - stride2: repeat stride
    - msg: error msg
    - stride_unit: stride unit
    - mask_mode: mask mode
    - ori_offset: ori offset
    - imm_offset: tensor's offset from debug
    - imm_original_shape: tensor's original shape from debug
    - is_910b: for vreducev2, vcopy, mask_mode is counter is diff with others

    Returns
    -------
    no return
    """
    tensor_check_param = namedtuple(
        "TensorCheckParam", "tensor mask nblock block_len repeat stride1 stride2 msg stride_unit mask_mode "
                            "ori_offset imm_offset imm_original_shape is_910b")
    params = tensor_check_param(*check_params)
    mask = params.mask
    repeat = params.repeat
    nblock = params.nblock
    if is_basic_expr(TikUtil.to_list(params.mask)) or \
            any(is_basic_expr([value]) for value in [params.repeat, params.stride1, params.stride2]):
        return
    if params.repeat == 0:
        return
    if not isinstance(params.nblock, int):
        return
    if params.mask_mode == "counter":
        mask, nblock, repeat = _cal_mask_rep_for_counter_mode(params)

    offset = params.tensor.offset
    if params.imm_offset is not None:
        offset = params.imm_offset

    if isinstance(offset, (tvm.tir.IntImm, tvm.tir.FloatImm, tvm.tir.StringImm)):
        offset = offset.value
    total_size = reduce_mul(params.tensor.original_shape)
    if params.imm_original_shape is not None:
        total_size = reduce_mul(params.imm_original_shape)
    extend_offset = vector_max_offset_cal((mask, params.tensor.dtype, params.block_len, repeat,
                                           params.stride1, params.stride2),
                                          stride_unit=params.stride_unit, nblock=nblock,
                                          mask_mode=params.mask_mode)
    # offset means offset away from tensor head address, it's 16 for tensor[16]
    # entend_offset means valid data offset
    need_offset = get_need_offset(params.ori_offset, extend_offset, offset)
    if need_offset is not None:
        TikCheckUtil.check_le(
            need_offset, total_size, "%s, expected elements nums: %s, actual elements nums: %s"
                                     % (params.msg, need_offset, total_size))


def get_need_offset(ori_offset, extend_offset, offset):
    """
    get tensor offset
    Parameters
    ----------
    ori_offset: tensor offset
    extend_offset: extend offset
    offset: offset

    Returns
    -------

    """
    need_offset = (Expr(extend_offset) + Expr(offset) + Expr(ori_offset)).eval_value()
    return need_offset


def get_blk_valid_list(mask, dtype, block_len):
    """
    get block id list
    """
    bit_len = get_bit_len(dtype)
    if bit_len == BIT_LEN_32:
        blk_valid_list = []
        blk_valid_ls = compare_mask_val(mask, block_len, blk_valid_list)
    else:
        blk_valid_ls = _get_blk_valid_not_32(mask, block_len, [])
    return blk_valid_ls


def compare_mask_val(mask, block_len, blk_valid_list):
    """
    compare mask and change block valid list
    Parameters
    ----------
    mask: mask
    block_len: block len
    blk_valid_list: block valid list

    Returns
    -------

    """
    if len(mask) == MASK_LEN_CONTINOUS_MODE:
        mask_len = mask[MASK_HIGH_IDX]
        blk_num = ceil_div(mask_len, block_len)
        blk_valid_list = range(blk_num)
    else:
        for blk_id in range(BLK_NUM_PER_REP):
            if mask[MASK_LOW_IDX] & BLK_32_LIST[blk_id] != 0:
                blk_valid_list.append(blk_id)

    return blk_valid_list


def _get_blk_valid_not_32(mask, block_len, blk_valid_list):
    """
    get_blk_valid_not_32
    """
    if len(mask) == MASK_LEN_CONTINOUS_MODE:
        mask_len = mask[MASK_HIGH_IDX]
        blk_num = ceil_div(mask_len, block_len)
        blk_valid_list = range(blk_num)
    else:
        for blk_id in range(BLK_NUM_PER_REP):
            blk_valid_list = _get_blk_valid_list_for_mask(blk_id, mask, blk_valid_list)
    return blk_valid_list


def _get_blk_valid_list_for_mask(blk_id, mask, blk_valid_list):
    """
    get blk valid list for mask
    """
    if blk_id < 4:
        if not mask[MASK_LOW_IDX] & BLK_16_LIST[blk_id] == 0:
            blk_valid_list.append(blk_id)
    else:
        if not mask[MASK_HIGH_IDX] & BLK_16_LIST[blk_id - 4] == 0:
            blk_valid_list.append(blk_id)
    return blk_valid_list


def vector_max_offset_cal(offset_params, stride_unit=0, nblock=BLK_NUM_PER_REP, mask_mode="normal"):
    """
    get max offset of calculate vector
    Parameters
    ----------
    mask_mode: mask mode
    nblock: numbers of block in one repeat
    stride_unit: param indicating gap/stride
    offset_params: contains mask dtype block_len repeat stride1 stride2
    Returns
    -------
    max_offset
    """
    max_offset_cal_params = namedtuple(
        "MaxOffsetParams", "mask dtype block_len repeat stride1 stride2")
    params = max_offset_cal_params(*offset_params)
    mask = params.mask
    repeat = params.repeat
    if not isinstance(mask, (list, tuple)):
        mask = [mask]
    # mask_len, the last effective element
    mask_len = 0
    bit_len = get_bit_len(params.dtype)
    if bit_len == BIT_LEN_32:
        mask_len = get_32bit_dtype_mask_len(mask, mask_mode)
    elif bit_len == BIT_LEN_8 or BIT_LEN_16:
        mask_len = get_8or16bit_dtype_mask_len(mask)

    if mask_mode == "normal":
        max_offset = calculate_vecotor_max_offset(
            repeat, params, mask_len, stride_unit, nblock)
    # counter mode, offset at penultimate repeat may be larger than offset
    # at last repeat

    else:
        full_mask = nblock * params.block_len
        if mask_len == full_mask:
            max_offset = calculate_vecotor_max_offset(repeat, params, mask_len, stride_unit, nblock)
        else:
            offset_0 = calculate_vecotor_max_offset(repeat - 1, params, full_mask, stride_unit, nblock)
            offset_1 = calculate_vecotor_max_offset(repeat, params, mask_len, stride_unit, nblock)
            max_offset = max(offset_0, offset_1)

    return max_offset


def get_b8_dtype_mask_len(total_mask_len, mask):
    """
    Calculates the number of most significant bits in the binary represented by the mask.
    for example:
    mask = [0, 0, 0, 2**3] => mask_len = 3 + 1
    mask = [0, 0, 2**63, 0] => mask_len = (63+1) + 64
    mask = [0, 2**31, 2, 1] => mask_len = (31 + 1) + 64 + 64
    Parameters
    ----------
    total_mask_len: total mask len
    mask: mask

    Returns
    -------

    """
    for mask_64 in mask:
        for index in range(63, -1, -1):
            if (mask_64 >> index) & 1 == 1:
                return total_mask_len
            else:
                total_mask_len -= 1
    return total_mask_len


def get_8or16bit_dtype_mask_len(mask):
    """
    get mask len when dtype bit length is 8 or 16
    Parameters
    ----------
    mask: mask

    Returns
    -------
    mask length
    """
    if is_basic_expr(mask):
        mask_len = MASK_LEN_128
    elif len(mask) == MASK_LEN_CONTINOUS_MODE:
        mask_len = mask[MASK_HIGH_IDX]
        if mask_len is None:
            mask_len = MASK_LEN_128
    elif len(mask) == MASK_LEN_B8_MODE:
        total_mask_len = 256
        mask_len = get_b8_dtype_mask_len(total_mask_len, mask)
    else:
        mask_len = get_mask_len(mask)
    return mask_len


def check_depthwise_conv_l1_w(pad_mode, l1_w):
    """
    check depthwise_conv l1_w

    Parameters
    ----------
    pad_mode:   0 - no padding
    -           1 - two colume on right side
    -           2 - two colume on left side
    -           3 - one colume on right&left side
    l1_w: height of src_fm

    Returns
    -------
    None
    """
    pad_mode = Expr(pad_mode).eval_value()
    l1_w = Expr(l1_w).eval_value()
    if pad_mode is None or l1_w is None:
        return
    if pad_mode == 0:
        if l1_w // 16 == 0 or l1_w % 16 != 2:
            TikCheckUtil.raise_error(
                "In depthwise_conv, when pad_mode is 0, l1_w should be "
                "16*i + 2(i is a positive integer), input l1_w: %s" % l1_w)


def float16format2uint16(numbers):
    """
    transeform data from float16 2 binary uint16
    Parameters
    ----------
    numbers: input

    Returns
    -------
    uint16 result
    """
    if isinstance(numbers, list):
        result = []
        for one in numbers:
            one_np = np.float16(one)
            one_str = one_np.view(np.uint16)
            result.append(np.uint16(one_str))

        return result
    one_np = np.float16(numbers)
    one_str = one_np.view(np.uint16)

    return np.uint16(one_str)


def check_dict_and_not_none(input_dict, name):
    """
    check input dict
    """
    if input_dict is None:
        TikCheckUtil.raise_error(name + " not support None")
    TikCheckUtil.check_type_match(input_dict, dict, name + " must be dict")


def check_aipp_one_src_overflow(src0, input_format, src_horizontal_size, src_vertical_size):
    """
    check aipp one src tensor overflow
    """
    src_size = src_vertical_size * src_horizontal_size * AIPP_INPUT_TYPE_SWAP_ALIGN.get(
        input_format).get("size_channel")
    TikCheckUtil.check_ge(src0.buffer_size, src_size,
                          "src0 buffer_size less than picture size, src0 overflow, src_size: %s" % src_size)


def check_aipp_two_src_overflow(src0, src1, input_format, src_horizontal_size, src_vertical_size):
    """
    check aipp two src tensor overflow
    """
    src0_size = src_horizontal_size * src_vertical_size * AIPP_INPUT_TYPE_SWAP_ALIGN.get(
        input_format).get("src0_size_channel")
    TikCheckUtil.check_ge(src0.buffer_size, src0_size,
                          "src0 buffer_size less than picture size, src0 overflow. src0_size: %s" % src0_size)
    src1_size = src_horizontal_size * src_vertical_size * AIPP_INPUT_TYPE_SWAP_ALIGN.get(
        input_format).get("src1_size_channel")
    TikCheckUtil.check_ge(src1.buffer_size, src1_size,
                          "src1 buffer_size less than picture size, src1 overflow, input: %s" % src1_size)


def _check_access_tread(block_idx, block_idx_cmp, tensor_name, access, access_list_cmp):
    for value in access:
        start, end, atomic_add = value
        for access_cmp in access_list_cmp:
            start_cmp, end_cmp, atomic_add_cmp = access_cmp[0]
            if atomic_add != 0 and atomic_add_cmp != 0:
                continue
            if (start_cmp <= start < end_cmp) or (start_cmp < end <= end_cmp) or \
                    (start <= start_cmp and end >= end_cmp):
                print("[Warning] access tensor: %s block index: %s slice: [%s, %s] has tread with block index: %s "
                      "slice: [%s, %s]" % (tensor_name, block_idx, start, end, block_idx_cmp, start_cmp, end_cmp))


def _check_block_tread(access_info_dict, block_idx, out, accessed_block_idx, access_list):
    # check all access address of all blocks expect block_idx
    for block_idx_cmp in access_info_dict.keys():
        # if has checked, won't compare
        if block_idx_cmp in accessed_block_idx:
            continue
        # check the tensor access tread
        if block_idx_cmp == block_idx:
            continue
        for out_cmp in access_info_dict[block_idx_cmp].keys():
            # only check the same tensor access tread
            if out_cmp != out:
                continue
            access_list_cmp = access_info_dict[block_idx_cmp][out_cmp]
            for access in access_list:
                # access_list and access_list_cmp is [[[x, y], [m, n]]] list
                _check_access_tread(block_idx, block_idx_cmp, out_cmp, access, access_list_cmp)


def check_tensor_access(access_info_dict):
    """
    According the access list info, check whether the multi core access address has tread
    Parameters
    ----------
    access_info_dict: all gm access extent info

    Returns
    -------
    no return
    """
    accessed_block_idx = []
    # check all access address of all blocks
    for block_idx in access_info_dict.keys():
        accessed_block_idx.append(block_idx)
        # get all access address of block_idx, out is gm tensor
        for out in access_info_dict[block_idx].keys():
            # access_list is the access address list of out gm tensor
            access_list = access_info_dict[block_idx][out]
            _check_block_tread(access_info_dict, block_idx, out, accessed_block_idx, access_list)


def check_vshl_vshr_scalar(src_dtype, scalar):
    """
    check vshl/vshr input scalar value

    Parameters
    ----------
    src_dtype: src dtype
    scalar: scalar_value

    Returns
    -------
    None
    """
    _max_value = get_bit_len(src_dtype)
    TikCheckUtil.check_in_range_by_dtype(
        scalar, msg="src_scalar should be in the range of [%d, %d], input value: %s"
        % (0, _max_value, str(scalar)), var_range=[0, _max_value])


def is_scalar(obj):
    """
    check whether input object is scalar

    Parameters
    ----------
    obj: input object

    Returns
    -------
    bool: whether obj is scalar
    """
    if isinstance(obj, BasicData) and obj.is_scalar():
        return True
    return False


def is_vector_register(obj):
    """
    check whether input object is tik Vector
    Parameters
    ----------
    obj: input obj

    Returns
    -------
    bool val
    """
    if isinstance(obj, BasicData) and (obj.is_vector() or obj.is_wide() or obj.is_predicate() or obj.is_address()):
        return True
    return False


def is_scalar_array(obj):
    """
    check whether input object is ScalarArray

    Parameters
    ----------
    obj: input object

    Returns
    -------
    bool: whether obj is ScalarArray
    """
    if isinstance(obj, BasicData) and obj.is_scalar_array():
        return True
    return False


def is_tensor(obj):
    """
    check whether input object is tensor

    Parameters
    ----------
    obj: input object

    Returns
    -------
    bool: whether obj is tensor
    """
    if isinstance(obj, BasicData) and obj.is_tensor():
        return True
    return False


def is_tensor_addr_list(obj):
    """
    check whether input object is tensoraddrlist

    Parameters
    ----------
    obj: input object

    Returns
    -------
    bool: whether obj is tensoraddrlist
    """
    if isinstance(obj, BasicData) and obj.is_tensor_addr():
        return True
    return False


def is_expr(obj):
    """
    check the obj whether is expr
    Parameters
    ----------
    obj:  the object whick will be check.

    Returns
    -------
    True : the obj is expr;False: the obj is not expr
    """

    if isinstance(obj, BasicExpr):
        return True
    return False


def is_predicate(obj):
    """
    check whether input object is predicate register

    Parameters
    ----------
    obj: input object

    Returns
    -------
    bool: whether obj is predicate register
    """
    if isinstance(obj, BasicData) and obj.is_predicate():
        return True
    return False


def check_extent_overflow(tensor, extent, tensor_offset, tensor_name, tensor_original_shape=None):
    """
    check tensor extent overflow
    """
    dst_total_elements = reduce_mul(tensor.original_shape)
    if tensor_original_shape is not None:
        dst_total_elements = reduce_mul(tensor_original_shape)
    if extent is not None and tensor_offset is not None:
        dst_need_elements = tensor_offset + ceil_div(extent, DTYPE_SIZE[tensor.dtype])
        TikCheckUtil.check_le(
            dst_need_elements, dst_total_elements, "%s tensor overflow, %s need %s elements, but only %s" %
                                                   (tensor_name, tensor_name, dst_need_elements, dst_total_elements))


def check_param_type_range(param_list, min_value_list, max_value_list, name_list, api_name):
    """
    check param type and range
    """
    for param, name, min_value, max_value in \
            zip(param_list, name_list, min_value_list, max_value_list):
        TikCheckUtil.check_type_match(
            param, (int, BasicExpr),
            "Intrinsic %s's %s should be int, Scalar or Expr,"
            " input type of line_num %s" % (api_name, name, type(param)))
        check_scalar_dtype(param,
                           "Intrinsic %s's %s should be a Scalar/Expr of int/uint" % (api_name, name))
        TikCheckUtil.check_in_range_by_dtype(
            param, msg="Intrinsic %s's %s should be in the range of [%d, %d], input value: %s" %
            (api_name, name, min_value, max_value, param), var_range=[min_value, max_value])


def get_l0c_align(tensor):
    """
    get address align for l0c scope
    """
    if "32" in tensor.dtype:
        align = 1024
    else:
        align = 512
    return align


def dma_align_fn(src, dst=None):
    """
    get the src and dst align
    """
    src_scope = TikUtil.get_storage_scope(src.scope)
    src_dtype_size = DTYPE_SIZE.get(src.dtype)
    src_scope_name = src_scope
    if src_scope_name == 'L0C':
        if src_dtype_size <= 2:
            src_scope_name = 'L0C16'
        else:
            src_scope_name = 'L0C32'
    src_align = DMA_V100_ALIGN.get(src_scope_name)

    if dst:
        dst_scope = TikUtil.get_storage_scope(dst.scope)
        dst_dtype_size = DTYPE_SIZE.get(dst.dtype)
        dst_scope_name = dst_scope
        if dst_scope_name == 'L0C':
            if dst_dtype_size <= 2:
                dst_scope_name = 'L0C16'
            else:
                dst_scope_name = 'L0C32'
        dst_align = DMA_V100_ALIGN.get(dst_scope_name)

        return src_align, dst_align

    return src_align


def check_address_align(tensor_list, name_list, align_size=None):
    """
    check tensor start address if aligned to given align_size

    Parameters
    ----------
    tensor_list: tensor list
    name_list: tensor name list
    align_size: align addr

    Returns
    -------
    None
    """
    _align_size = get_compatible_blk_size() if align_size is None else align_size
    for tensor, name in zip(tensor_list, name_list):
        if isinstance(tensor.offset, int) and tensor.offset * DTYPE_SIZE[tensor.dtype] % _align_size != 0:
            TikCheckUtil.raise_error(
                "Address align error, %s[%s] is not %s Byte align" % (name, tensor.offset, _align_size))


def check_shape_align(tensor_list, name_list, align_size=None):
    """
    check tensor shape size if aligned to given align_size

    Parameters
    ----------
    tensor_list: tensor list
    name_list: tensor name list
    align_size: align addr

    Returns
    -------
    None
    """
    _align_size = get_compatible_blk_size() if align_size is None else align_size
    for tensor, name in zip(tensor_list, name_list):
        shape_size = Expr(reduce_mul(tensor.original_shape) - tensor.offset).eval_value()
        if shape_size is not None and shape_size % _align_size != 0:
            TikCheckUtil.raise_error(
                "Shape size error, %s is not %s Byte align" % (
                    name, _align_size * DTYPE_SIZE[tensor.dtype]))


def check_begin_end_value(begin, endt, dtype):
    """
    check begin and end value
    """
    if dtype == "uint64":
        max_value = DTYPE_INT_VALUE["int64"][1]
    else:
        if dtype not in DTYPE_INT_VALUE:
            TikCheckUtil.raise_error("dtype %s is not valid!" % dtype)
        max_value = DTYPE_INT_VALUE[dtype][1]

    start = 0
    if isinstance(begin, int):
        if begin < 0 or begin > max_value:
            TikCheckUtil.raise_error("begin should in [0, %d], but get %d" % (max_value, begin))
        start = begin
    if isinstance(endt, int):
        if endt < start or endt > max_value:
            TikCheckUtil.raise_error("endt should in [%d, %d], but get %d" % (start, max_value, endt))


def _deal_tik1_5_api(intrinsic, dtype):
    """
    deal tik1.5 api
    Parameters
    ----------
    intrinsic : str, the intrinsic need to check
    dtype: str, optional args, if not empty, will check the dtype.

    Returns
    -------
    tik_api_instr,dtype
    """
    tik_api_instr = intrinsic.split('.')[1]
    api_deal_dict = {
        "h_cast": "vconv",
        "h_reduce_sum": "vcadd",
        "h_reduce_max": "vcmax",
        "h_reduce_min": "vcmin",
        "h_reduce_argmax": "vcmax",
        "h_reduce_argmin": "vcmin",
        "h_duplicate": "vector_dup",
        "h_data_move": "data_move",
        "h_sin": "h_sin",
        "h_cos": "h_cos",
        "h_quant": "h_quant",
        "h_cmpv": "h_cmpv"
    }
    if api_deal_dict.get(tik_api_instr):
        tik_api_instr = api_deal_dict.get(tik_api_instr)
        if tik_api_instr == "data_move":
            if dtype in ["uint8", "int8", "uint16", "int16", "float16", "uint32", "int32",
                         "float32", "uint64", "int64"]:
                dtype = ""
    else:
        tik_api_instr = tik_api_instr.replace("h_", "v")
    return tik_api_instr, dtype


def _deal_tik_api(intrinsic, dtype):
    """
    deal tik api

    Parameters
    ----------
    intrinsic : str, the intrinsic need to check
    dtype: str, optional args, if not empty, will check the dtype.

    Returns
    -------
    value: bool, True if chip contains such api, else return False
    """
    tik_api_instr = intrinsic[4:]
    soc_total_version = get_soc_name() + get_soc_core_type()

    tik_api_map = ONLY_TIK_API_MAP
    if tik_api_instr in tik_api_map:
        if soc_total_version in tik_api_map.get(tik_api_instr):
            if dtype == "" or dtype in tik_api_map.get(tik_api_instr).get(soc_total_version):
                return True
        return False
    if tik_api_instr in TRANS_TIK_API_TO_INSTR_MAP:
        tik_api_instr = TRANS_TIK_API_TO_INSTR_MAP.get(tik_api_instr)
    if tik_api_instr.startswith("vec_"):
        tik_api_instr = "v" + tik_api_instr[4:]
    if tik_api_instr.startswith("h_"):
        reg = _check_tik1_5_api(intrinsic, dtype, tik_api_map, soc_total_version)
        if reg is not None:
            return reg
        tik_api_instr, dtype = _deal_tik1_5_api(intrinsic, dtype)
    return intrinsic_check_support("Intrinsic_" + tik_api_instr, dtype)


def _check_tik1_5_api(intrinsic, dtype, tik_api_map, soc_total_version):
    """
    check_tik1_5_api
    """
    ret = None
    tik_api_instr, dtype = _deal_tik1_5_api(intrinsic, dtype)
    if tik_api_instr in tik_api_map:
        if soc_total_version in tik_api_map.get(tik_api_instr):
            if dtype == "" or dtype in tik_api_map.get(tik_api_instr).get(soc_total_version):
                ret = True
        else:
            ret = False
    return ret


def tik_api_check_support(intrinsic, dtype):
    """
    check tik api support
    """
    return _deal_tik_api(intrinsic, dtype)


def _merge_intervals(intervals: list, new_interval: list) -> list:
    """
    merge effective interval
    """
    intervals.append(new_interval)
    q = sorted(intervals, key=lambda x: x[0])
    temp = q[0]
    result = []
    for i in range(1, len(q)):
        if temp[1] >= q[i][0]:
            temp[1] = max(temp[1], q[i][1])
        else:
            result.append(temp)
            temp = q[i]
    result.append(temp)
    return result


def set_tensor_addr_list_valid_idx(valid_idx: dict, idx: int, dst_buffer: tvm.schedule.Buffer,
                                   data_size: int) -> None:
    """
    Sets the valid index range of tensoraddrlist.
    """
    dst_valid_idx = valid_idx.get(dst_buffer)
    end_value = idx + data_size // DTYPE_SIZE[TENSORADDR_DTYPE]
    if dst_valid_idx:
        valid_idx[dst_buffer] = _merge_intervals(dst_valid_idx, [idx, end_value])
    else:
        valid_idx[dst_buffer] = [[idx, end_value]]


def check_tensor_addr_list_valid_idx(idx: int, intervals: list, buffer_name: str) -> None:
    """
    check the valid index range of tensoraddrlist.
    """

    check_result = False
    for interval in intervals:
        if interval[0] <= idx < interval[1]:
            check_result = True
            break

    TikCheckUtil.check_is(check_result, True, "The %s[%s] address is invalid." % (buffer_name, idx))


def int64_support_check(intrinsic, dtype):
    """
    check whether the intrinsic supports int64.

    Parameters
    ----------
    intrinsic : str, the intrinsic need to check
    dtype: str, optional args, if not empty, will check the dtype.

    Returns
    -------
    value: bool, True if chip contains such api, else return False
    """
    tik_api_instr = intrinsic[4:]
    soc_total_version = get_soc_name() + get_soc_core_type()
    if soc_total_version in int64_support_map and tik_api_instr in int64_support_map.get(soc_total_version) \
            and dtype in int64_intrinsic_map.get(tik_api_instr):
        return True
    return False
