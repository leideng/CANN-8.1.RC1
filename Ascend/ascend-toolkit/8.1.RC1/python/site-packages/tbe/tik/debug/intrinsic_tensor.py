#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     intrinsic_tensor.py
DESC:     intrinsic_tensor
CREATED:  2021-12-29 12:41 PM
MODIFIED: 2021-12-29 12:41 PM
"""
from functools import reduce
import numpy as np

from tbe.common.platform.platform_info import get_soc_spec
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import SCOPE_SIZE_MAP
from tbe.tik.common.util import flat_list
from tbe.tik.debug.statement import STMT
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_api_constants import SCOPE_MAP


def cal_static_parameters(tensor, context):
    """
    calibrating static parameters
    """
    static_original_shape = [context.evaluate_expr(i) for i in tensor.original_shape]
    static_shape, static_dims = \
        static_shape_dims(tensor.dimensions, context)
    static_strides = [context.evaluate_expr(i) for i in tensor.strides]
    static_data = context.evaluate_expr(tensor.data)

    context.buffer2static_parameters[id(tensor)] = \
        {"static_original_shape": static_original_shape,
         "static_dims": static_dims,
         "static_shape": static_shape,
         "static_strides": static_strides,
         "static_data": static_data}

    for i in static_shape:
        if i <= 0:
            TikCheckUtil.raise_error("invalid shape input, shape: %s" % list(static_shape))
    if tensor.scope in SCOPE_SIZE_MAP:
        get_size = get_soc_spec(SCOPE_SIZE_MAP[tensor.scope])
        if context.evaluate_expr(tensor.size) > get_size:
            TikCheckUtil.raise_error("Tensor is too large for %s" % SCOPE_MAP[tensor.scope])


def static_shape_dims(dimensions, context):
    """
    static shape and dims
    """
    shape = []
    dims = []
    for i in dimensions:
        start = context.evaluate_expr(i.start)
        stop = context.evaluate_expr(i.stop)
        step = context.evaluate_expr(i.step)
        dims.append(slice(start, stop, step))
        shape.append((stop - start + step - 1) // step)
    return shape, dims


def check_getitem_parameter(static_dims, last_static_shape):
    """
    check getitem parameter
    """
    for i, slc in enumerate(static_dims):
        if slc.start < 0:
            TikCheckUtil.raise_error(
                "tensor overflow, the start(%s) of slice should in [0, %s]."
                % (slc.start, last_static_shape[i]))
        if slc.stop <= slc.start:
            TikCheckUtil.raise_error(
                "the stop(%s) of slice should be greater than the start(%s) of slice."
                % (slc.stop, slc.start))
        if slc.stop > last_static_shape[i]:
            TikCheckUtil.raise_error(
                "tensor overflow, the stop(%s) of slice should in [0, %s]."
                % (slc.stop, last_static_shape[i]))


def check_getitem_overflow(static_data, static_shape, max_nums):
    """
    check space required

    Parameters
    ----------
    static_data: static_data
    static_shape: static_shape
    max_nums: Maximum space

    Returns
    -------
    no returns
    """
    # tensor.shape should <= tensor.original_shape,
    # it is checked for dynamic shape
    if static_data + reduce(lambda i, j: i * j, static_shape) > max_nums:
        TikCheckUtil.raise_error(
            "The space required exceeds max_mem_size")


def check_tensor_buffer_shape(tensor_buffer_shape, tensor_buffer, tensor):
    """
    check shape of tensor buffer

    Parameters
    ----------
    tensor_buffer_shape: shape of tensor buffer
    tensor_buffer: tensor buffer
    tensor: the tensor

    Returns
    -------
    None
    """
    TikCheckUtil.check_equality(
        tensor_buffer_shape, tensor_buffer.buffer.shape,
        "%s input shape mismatch %s vs %s" % (
            tensor, tensor_buffer_shape,
            tensor_buffer.buffer.shape))
    TikCheckUtil.check_equality(
        tensor_buffer.dtype, tensor_buffer.buffer.dtype,
        "%s input dtype mismatch %s vs %s" % (
            tensor, tensor_buffer.dtype,
            tensor_buffer.buffer.dtype))


class DelTensor(STMT):
    """
    Class DelTensor inherits from STMT
    To del Tensor
    """

    def __init__(self, source_info, tensor, tik_debugger):
        """
        Initialize class DelTensor

        Parameters
        ----------
        source_info:source code information, It represents the relationship of current node with source code

        tensor:a type of tensor

        Returns
        ----------
        No returns
        """
        super(DelTensor, self).__init__(source_info, tik_debugger)
        self.tensor = tensor

    def eval_(self, context):
        """
        Eval function, evaluate all of self.function

        Parameters
        ----------
        context:information of debugger, store all of debugger's information

        Returns
        ----------
        No returns
        """
        context.tensor_buffer.tensor2buffer[id(self.tensor)] = None


class TensorDef(STMT):
    """
    def tensor
    """

    def __init__(self, source_info, tensor, tik_debugger, kwargs):
        """
        Initialize class TensorDef

        Parameters
        ----------
        source_info:source code information, It represents the relationship of current node with source code

        tensor:source tensor

        Returns
        ----------
        No returns
        """
        super(TensorDef, self).__init__(source_info, tik_debugger)
        self.tensor = tensor
        self.init_value = kwargs.get('init_value')
        self.reuse_list = kwargs.get('reuse_list')
        self.enable_buffer_reuse = kwargs.get('enable_buffer_reuse')

    def eval_(self, context):
        """
        Eval function
        evaluate all of self.function

        Parameters
        ----------
        context:information of debugger, store all of debugger's information

        Returns
        ----------
        No returns
        """
        cal_static_parameters(self.tensor, context)
        static_shape = context.buffer2static_parameters[id(self.tensor)]["static_shape"]
        if not self.tensor.is_static_shape and \
                self.tensor.max_mem_size is not None:
            total_num = reduce(lambda i, j: i * j, static_shape)
            if self.tensor.max_mem_size < total_num * DTYPE_SIZE[self.tensor.dtype]:
                TikCheckUtil.raise_error("max_mem_size is too small.")

        np_array = None
        # for input tensor, the tensor buffer data from input feed_dict
        for key, value in context.placeholders.items():
            if self.tensor.buffer == value:
                input_value = context.feed_dict_tensor[key]
                np_array = np.ascontiguousarray(input_value)
                context.add_tensor(self.tensor, np_array)  # set the buffer array with input_value

                tensor_buffer_shape = tuple(static_shape)
                tensor_buffer = context.tensor_buffer.get_npbuffer_by_tvmbuffer(self.tensor.buffer)

                check_tensor_buffer_shape(tensor_buffer_shape, tensor_buffer, self.tensor)

                return
        # for gm tensor with init_value, the tensor buffer data from init_value
        if self.init_value is not None:
            if not isinstance(self.init_value, (tuple, list)):
                self.init_value = [self.init_value] * self.tensor.size
            # flatten init_value
            self.init_value = list(flat_list(self.init_value))
            input_value = np.array(self.init_value, dtype=self.tensor.dtype).reshape(self.tensor.shape)
            np_array = np.ascontiguousarray(input_value)
            context.add_tensor(self.tensor, np_array)
            return
        # for tensor not input tensor and has no init_value
        self._deal_not_input_uinit(np_array, context)

    def _deal_not_input_uinit(self, np_array, context):
        """
        for tensor not input tensor and has no init_value
        """
        if self.reuse_list is None:
            context.add_tensor(self.tensor, np_array)
            if self.enable_buffer_reuse is True:
                context.reuse_dict[self.tensor.name] = self.tensor
        else:
            if self.enable_buffer_reuse is False:
                return
            for tensor_name in self.reuse_list:
                if tensor_name in context.reuse_dict:
                    np_array = context.get_value(context.reuse_dict[tensor_name]).buffer
                    context.add_tensor(self.tensor, np_array)
                    context.reuse_dict[self.tensor.name] = self.tensor


class TensorProxyDef(STMT):
    """
    def tensor proxy
    """

    def __init__(self, source_info, tensor, tik_debugger):
        """
        Initialize class TensorDef

        Parameters
        ----------
        source_info:source code information, It represents the relationship of current node with source code

        tensor:tensor proxy

        Returns
        ----------
        No returns
        """
        super(TensorProxyDef, self).__init__(source_info, tik_debugger)
        self.tensor = tensor
        self.last_tensor = tensor.last_tensor

    def eval_(self, context):
        """
        Eval function
        evaluate all of self.function

        Parameters
        ----------
        context:information of debugger
        store all of debugger's information

        Returns
        ----------
        No returns
        """
        cal_static_parameters(self.tensor, context)

        # check parameter for getitem tik 1.0 case
        if self.tensor.is_reshape and self.tensor.is_getitem:
            self._check_parameter_for_getitem(context)
        # check parameter for reshape case
        elif self.tensor.is_reshape:
            self._check_param_for_reshape_case(context)
        # check parameter for getitem case
        elif self.tensor.is_getitem:
            self._check_param_for_getitem_case(context)
        # check parameter for reinterpret_cast_to case
        elif self.tensor.dtype != self.last_tensor.dtype:
            self._check_param_for_reinterpret_case(context)

    def _check_param_for_reshape_case(self, context):
        """
        check parameter for reshape case

        Parameters
        ----------
        context: context

        Returns
        -------
        no returns
        """
        static_shape = \
            context.buffer2static_parameters[id(self.tensor)][
                "static_shape"]
        last_static_shape = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_shape"]
        original_static_shape = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_original_shape"]
        total_num = reduce(lambda i, j: i * j, static_shape)
        last_total_num = reduce(lambda i, j: i * j, last_static_shape)
        original_total_num = reduce(lambda i, j: i * j,
                                    original_static_shape)
        if last_total_num != original_total_num:
            TikCheckUtil.raise_error(
                "Reshape operator is needed, but the tensor with shape: "
                "%s has been sliced." % last_static_shape)
        if last_total_num != total_num:
            TikCheckUtil.raise_error(
                "Shape mismatch! reshape operator with shape: %s and "
                "new shape: %s." % (last_static_shape, static_shape))

    def _check_param_for_getitem_case(self, context):
        """
        check parameter for getitem case

        Parameters
        ----------
        context: context

        Returns
        -------
        no returns
        """

        last_static_shape = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_shape"]
        static_dims = context.buffer2static_parameters[id(self.tensor)][
            "static_dims"]
        static_shape = \
            context.buffer2static_parameters[id(self.tensor)][
                "static_shape"]
        static_data = \
            context.buffer2static_parameters[id(self.tensor)][
                "static_data"]
        check_getitem_parameter(static_dims, last_static_shape)
        if self.tensor.max_mem_size is not None:
            check_getitem_overflow(static_data, static_shape,
                                   self.tensor.max_mem_size // DTYPE_SIZE[
                                       self.tensor.dtype])

    def _check_param_for_reinterpret_case(self, context):
        """
        check parameter for reinterpret_cast_to case

        Parameters
        ----------
        context: context

        Returns
        -------
        no returns
        """
        last_static_strides = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_strides"]
        last_static_dims = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_dims"]
        if last_static_strides[-1] != 1:
            TikCheckUtil.raise_error(
                "Last stride should be equal to 1"
                " when do reinterpret_cast_to")

        if DTYPE_SIZE[self.tensor.dtype] > \
                DTYPE_SIZE[self.last_tensor.dtype]:
            dtype_factor = DTYPE_SIZE[self.tensor.dtype] // \
                           DTYPE_SIZE[self.last_tensor.dtype]
            last_dim_offset = last_static_dims[-1].stop - \
                              last_static_dims[-1].start
            if last_dim_offset % dtype_factor != 0:
                TikCheckUtil.raise_error(
                    "Last dimension can't be divided")

    def _check_parameter_for_getitem(self, context):
        """
        check parameter for getitem tik 1.0 case
        """
        last_static_strides = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_strides"]
        last_static_shape = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_shape"]
        last_static_original_shape = \
            context.buffer2static_parameters[id(self.last_tensor)][
                "static_original_shape"]
        static_dims = context.buffer2static_parameters[id(self.tensor)][
            "static_dims"]
        last_total_num = reduce(lambda i, j: i * j, last_static_original_shape)
        static_shape = \
            context.buffer2static_parameters[id(self.tensor)][
                "static_shape"]
        static_data = \
            context.buffer2static_parameters[id(self.tensor)][
                "static_data"]
        for i in range(1, len(last_static_strides)):
            if last_static_strides[i] * last_static_shape[i] != \
                    last_static_strides[i - 1]:
                TikCheckUtil.raise_error(
                    "Getitem need flatten, but the tensor is not "
                    "continuous after slicing.")
        last_static_original_shape = [last_total_num]
        check_getitem_parameter(static_dims, last_static_original_shape)
        if self.tensor.max_mem_size is not None:
            check_getitem_overflow(static_data, static_shape,
                                   self.tensor.max_mem_size // DTYPE_SIZE.get(self.tensor.dtype))
