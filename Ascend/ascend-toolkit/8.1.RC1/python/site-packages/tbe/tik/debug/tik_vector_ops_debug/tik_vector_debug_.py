#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_debug_.py
DESC:     provide params
CREATED:  2019-04-18 18:53:42
MODIFIED: 2020-12-7 19:17:00
"""
import sys

from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.common.common_util import vec_template_align
from tbe.tik.common.common_util import check_vshl_vshr_scalar
from tbe.tik.common.common_util import is_tensor
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.common_check_func import check_addr_overlap_v4dtrans
from tbe.tik.debug.statement import STMT
from tbe.tik.debug.sim import Encoder
from tbe.tik.debug.simd import set_mask_counter_mode
from tbe.tik.debug.simd import _VECTOR_SCALAR_FN_ENCODER
from tbe.tik.debug.simd import _VSHR_DTYPE_ENCODING_V200
from tbe.tik.debug.util import copy_tensor_to_model
from tbe.tik.debug.util import get_dtype_size
from tbe.tik.debug.util import set_vector_mask
from tbe.tik.debug.util import VEC_DATA_TYPE_ENCODING
from tbe.tik.debug.util import get_dtype_bit_width
from tbe.tik.debug.util import cvt_float_to_uint
from tbe.tik.debug.util import copy_tensor_to_model_get_addr
from tbe.tik.debug.sim.util import TempEnv
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_params import SRC_BLOCK_STRIDE_SHIFT_POS
from tbe.tik.tik_lib.tik_params import MIN_STRIDE_UNIT
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import ONE_REP_BYTE_SIZE
from tbe.tik.tik_lib.tik_params import REPEAT_SHIFT_POS
from tbe.tik.tik_lib.tik_params import STRIDE_UNIT_SHIFT_POS
from tbe.tik.tik_lib.tik_params import DST_REPEAT_STRIDE_SHIFT_POS
from tbe.tik.tik_lib.tik_params import SRC_REPEAT_STRIDE_SHIFT_POS
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_STRIDE_UNIT
from tbe.tik.debug.debug_encoder import SPECIAL_DTYPE_INSTR
from tbe.tik.debug.debug_encoder import VECTOR_DTYPE_FN_ENCODER
from tbe.tik.debug.debug_encoder import VSHL_DTYPE_CODE
from tbe.tik.debug.debug_encoder import VEC_RELUCONV_TYPE_ENCODING
from tbe.tik.debug.debug_encoder import VEC_DATA_TYPE_ENCODING_0_
from tbe.tik.debug.debug_encoder import VEC_DATA_TYPE_ENCODING_V200
from tbe.tik.debug.debug_encoder import B16_B32_DTYPE_CODE
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import reduce_mul

_SRC_BLK_STRIDE_SHIFT_POS = 8
_SRC1_BLK_STRIDE_SHIFT_POS = 16
_DST_REPEAT_STRIDE_SHIFT_POS = 24
_SRC_REPEAT_STRIDE_SHIFT_POS = 32
_SRC1_REPEAT_STRIDE_SHIFT_POS = 40
_ENCODER = Encoder()

_VMULCONV_DTYPE_ENCODING = {'int8': 0b01, 'uint8': 0b10}
_VCBD_MAP = {
    "u162u8": 0,
    "s162u8": 1,
    "u322u8": 2,
    "s322u8": 3,
    "u82u16": 4,
    "u322u16": 5,
    "s322u16": 6,
    "u82s16": 7,
    "u322s16": 8,
    "s322s16": 9,
    "u82u32": 10,
    "u162u32": 11,
    "s162u32": 12,
    "u82s32": 13,
    "u162s32": 14,
    "s162s32": 15
}


class NewVectorOnlyTemplate(STMT):
    """
    this template only have vector
    """
    _VECTOR_ONLY_FN_ENCODER = {
        'vrelu': _ENCODER.gen_vrelux,
        'vexp': _ENCODER.gen_vexpx,
        'vln': _ENCODER.gen_vlnx,
        'vabs': _ENCODER.gen_vabsx,
        'vrec': _ENCODER.gen_vrecx,
        'vrsqrt': _ENCODER.gen_vrsqrtx,
        'vnot': _ENCODER.gen_vnotx,
        'vsqrt': _ENCODER.gen_vsqrtx,
        'vcbd': _ENCODER.gen_vcbd,
    }

    def __init__(self, source_info, op_obj, tik_debugger=None):
        super(NewVectorOnlyTemplate, self).__init__(source_info, tik_debugger)
        self.op_obj = op_obj
        if TikSocManager.is_v300_610l_soc():
            self.source_id = op_obj.tik_instance.context.debug_source_id

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        repeat = context.evaluate_expr(self.op_obj.repeat_times)
        self._check_params(context, repeat)

        if TikSocManager.is_v300_610l_soc():
            context.step_next(self.source_id)
            return

        mask = self.op_obj.control_op.mask

        set_vector_mask(mask, context, mask_mode=self.op_obj.control_op.mask_mode,
                        tensor_bit_len=max(get_bit_len(self.op_obj.src_tensor_op.tensor_obj.dtype),
                                           get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype)))
        align = vec_template_align(self.op_obj.src_tensor_op.tensor_obj.dtype)
        temp_env = TempEnv()

        xn_idx, _, src_alloc_size, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src_tensor_op.tensor_obj, align, access_mode='r')

        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.op_obj.dst_tensor_op.tensor_obj, align, access_mode='w')

        param = _ENCODER.new_param()
        if self.op_obj.control_op.mask_mode == "counter":
            orig_ctrl_value = set_mask_counter_mode(context)
        if self.op_obj.print_name == "vcbd":
            param.type = self.gen_vcbd_param_type()
        else:
            self.op_obj.src_tensor_op.check_read_mem_out_of_bounds(src_alloc_size, self.op_obj.control_op)
            param.type = VEC_DATA_TYPE_ENCODING.get(self.op_obj.src_tensor_op.tensor_obj.dtype)

        if self.op_obj.name in SPECIAL_DTYPE_INSTR:
            param.type = VEC_DATA_TYPE_ENCODING_V200.get(self.op_obj.src_tensor_op.tensor_obj.dtype)
            param.type |= SPECIAL_DTYPE_INSTR[self.op_obj.name] << 2
        param.xd = xd_idx
        param.xn = xn_idx
        param.xt = self.create_gpr_x_t(context, temp_env, repeat)

        instr = NewVectorOnlyTemplate._VECTOR_ONLY_FN_ENCODER.get(self.op_obj.name)(param)
        context.model.step(instr)
        temp_env.check_mem_access(context.model, False)
        if self.op_obj.control_op.mask_mode == "counter":
            context.model.write_spr('CTRL', orig_ctrl_value)
        context.model.read_memory(dst_addr, self.op_obj.dst_tensor_op.tensor_obj.scope, dst_ptr,
                                  dst_alloc_size)

    def gen_vcbd_param_type(self):
        """
        gen vcbd param type

        Returns
        -------
        None
        """
        max_dtype_size = max(DTYPE_SIZE.get(self.op_obj.dst_tensor_op.tensor_obj.dtype),
                             DTYPE_SIZE.get(self.op_obj.src_tensor_op.tensor_obj.dtype))
        self.op_obj.dst_tensor_op.check_vcbd_overflow(self.op_obj.control_op, max_dtype_size)
        self.op_obj.src_tensor_op.check_vcbd_overflow(self.op_obj.control_op, max_dtype_size)
        src_type_name = DTYPE_MAP.get(self.op_obj.src_tensor_op.tensor_obj.dtype)
        dst_type_name = DTYPE_MAP.get(self.op_obj.dst_tensor_op.tensor_obj.dtype)
        type_name = "%s2%s" % (src_type_name, dst_type_name)
        return _VCBD_MAP.get(type_name)

    def create_gpr_x_t(self, context, temp_env, repeat):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context
        temp_env : the temp environment
        repeat: repeat times

        Returns
        -------
        xt_idx
        """

        TikCheckUtil.check_in_range_by_dtype(
            context.evaluate_expr(self.op_obj.stride_unit),
            msg="stride_unit should be in the range of [%d, %d], input stride_unit is %s"
                % (MIN_STRIDE_UNIT, MAX_STRIDE_UNIT, context.evaluate_expr(self.op_obj.stride_unit)),
            var_range=[MIN_STRIDE_UNIT, MAX_STRIDE_UNIT])
        tensor_bit_len = max(get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype),
                             get_bit_len(self.op_obj.src_tensor_op.tensor_obj.dtype))
        block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len]
        # check addr overlap
        if self.op_obj.src_tensor_op.tensor_obj.buffer == self.op_obj.dst_tensor_op.tensor_obj.buffer and \
                repeat != 0:
            self.op_obj.dst_tensor_op.check_address_overlapping(
                self.op_obj.print_name, self.op_obj.control_op, self.op_obj.src_tensor_op, block_list)

        xt_idx = temp_env.alloc_register()
        x_t = context.evaluate_expr(self.op_obj.dst_tensor_op.blk_stride)
        x_t |= context.evaluate_expr(self.op_obj.src_tensor_op.blk_stride) << SRC_BLOCK_STRIDE_SHIFT_POS
        stride_unit = context.evaluate_expr(self.op_obj.stride_unit)
        x_t |= stride_unit << STRIDE_UNIT_SHIFT_POS
        x_t |= repeat << REPEAT_SHIFT_POS
        x_t |= context.evaluate_expr(self.op_obj.dst_tensor_op.rep_stride) << DST_REPEAT_STRIDE_SHIFT_POS
        x_t |= context.evaluate_expr(self.op_obj.src_tensor_op.rep_stride) << SRC_REPEAT_STRIDE_SHIFT_POS

        context.model.write_gpr(xt_idx, x_t)

        return xt_idx

    def _check_params(self, context, repeat):

        self.op_obj.dst_tensor_op.set_context(context)
        self.op_obj.src_tensor_op.set_context(context)
        # check repeat
        TikCheckUtil.check_in_range_by_dtype(
            repeat, msg="repeat_times should be in the range of [%d, %d], input repeat_times is %s"
                        % (0, MAX_REPEAT_TIMES, repeat), var_range=[0, MAX_REPEAT_TIMES])

        # check strides
        self.op_obj.dst_tensor_op.set_rep_stride_value()
        self.op_obj.src_tensor_op.set_rep_stride_value()
        self.op_obj.dst_tensor_op.set_blk_stride_value()
        self.op_obj.src_tensor_op.set_blk_stride_value()
        self.op_obj.dst_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_DOUBLE_BYTE)
        self.op_obj.src_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_DOUBLE_BYTE)
        self.op_obj.dst_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)
        self.op_obj.src_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)


class NewVectorVectorTemplate(STMT):
    """
    this template both have vector and vecotr
    """
    VECTOR_VECTOR_FN_ENCODER = {
        'vadd': _ENCODER.gen_vaddx,
        'vsub': _ENCODER.gen_vsubx,
        'vmul': _ENCODER.gen_vmulx,
        'vdiv': _ENCODER.gen_vdivx,
        'vmax': _ENCODER.gen_vmaxx,
        'vmin': _ENCODER.gen_vminx,
        'vand': _ENCODER.gen_vandx,
        'vor': _ENCODER.gen_vorx,
        'vmla': _ENCODER.gen_vmlax,
        'vmadd': _ENCODER.gen_vmaddx,
        'vmaddrelu': _ENCODER.gen_vmaddrelux,
        'vmulconv': _ENCODER.gen_vmulconvx,
        'vadddeqrelu': _ENCODER.gen_vadd_deq_relux,
        'vaddrelu': _ENCODER.gen_vaddrelux,
        'vsubrelu': _ENCODER.gen_vsubrelux,
        'vdp': _ENCODER.gen_vdp
    }

    def __init__(self, source_info, op_obj, tik_debugger, **kwargs):
        super(NewVectorVectorTemplate, self).__init__(source_info, tik_debugger)
        self.op_obj = op_obj
        self.kwargs = kwargs
        if TikSocManager.is_v300_610l_soc():
            self.source_id = op_obj.tik_instance.context.debug_source_id

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        self._check_all_params(context)

        if self.op_obj.name:
            if 'vmulconv' in self.op_obj.name:
                self.op_obj.name, self.op_obj.print_name = 'vmulconv', 'vmulconv'

        # change vmulconv_f162s8, vmulconv_f162u8 to vmulconv
        if self.op_obj.name not in NewVectorVectorTemplate.VECTOR_VECTOR_FN_ENCODER:
            sys.stderr.write("[WARN]: Instruction '%s' not supported in debug system yet!\n" % self.op_obj.name)
            return

        if TikSocManager.is_v300_610l_soc():
            context.step_next(self.source_id)
            return

        align = vec_template_align(self.op_obj.src0_tensor_op.tensor_obj.dtype)
        temp_env = TempEnv()
        set_vector_mask(self.op_obj.control_op.mask, context,
                        tensor_bit_len=max(get_bit_len(self.op_obj.src0_tensor_op.tensor_obj.dtype),
                                           get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype),
                                           get_bit_len(self.op_obj.src1_tensor_op.tensor_obj.dtype)))

        src0_buffer_size, src1_buffer_size, dst_addr, dst_alloc_size, dst_ptr, param = \
            self.gen_param_and_copy_model(context, temp_env, align)

        # check param before check overflow
        self.op_obj.src0_tensor_op.check_read_mem_out_of_bounds(src0_buffer_size, self.op_obj.control_op)
        self.op_obj.src1_tensor_op.check_read_mem_out_of_bounds(src1_buffer_size, self.op_obj.control_op)
        if self.op_obj.name == 'vadddeqrelu':
            deqscale = context.evaluate_expr(self.kwargs.get('deqscale'))
            context.model.write_spr('DEQSCALE', cvt_float_to_uint('float16', deqscale))

        if self.op_obj.name in ('vaddreluconv', 'vsubreluconv'):
            if is_tensor(self.kwargs.get('deqscale')):
                deq_addr = copy_tensor_to_model_get_addr(
                    context, temp_env, self.kwargs.get('deqscale'), align, access_mode='w')
                context.model.write_spr('DEQSCALE', deq_addr // align)
                param.type = 0b11
            else:
                param.type = VEC_RELUCONV_TYPE_ENCODING[
                    (self.op_obj.src0_tensor_op.tensor_obj.dtype, self.op_obj.dst_tensor_op.tensor_obj.dtype)]

            store_h = self.kwargs.get('storeMode')
            if store_h is not None:
                param.h = context.evaluate_expr(store_h)

        context.model.step(NewVectorVectorTemplate.VECTOR_VECTOR_FN_ENCODER.get(self.op_obj.name)(param))
        temp_env.check_mem_access(context.model, False)

        context.model.read_memory(
            dst_addr, self.op_obj.dst_tensor_op.tensor_obj.scope, dst_ptr, dst_alloc_size)

    def gen_param_and_copy_model(self, context, temp_env, align):
        """
        gen param and copy tensor to model

        Parameters
        ----------
        context : the stack context

        temp_env : the temp environment

        align : the align addr

        Returns
        -------
        src0_buffer_size, src1_buffer_size, dst_addr, dst_alloc_size, dst_ptr, param
        """
        xn_idx, _, src0_buffer_size, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src0_tensor_op.tensor_obj, align, access_mode='r')

        xm_idx, _, src1_buffer_size, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src1_tensor_op.tensor_obj, align, access_mode='r')

        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.op_obj.dst_tensor_op.tensor_obj, align, access_mode='w')

        param = _ENCODER.new_param()
        param.type = self.gen_param_type()
        param.xd = xd_idx
        param.xn = xn_idx
        param.xm = xm_idx
        param.xt = self.create_gpr_x_t(context, temp_env)
        return [src0_buffer_size, src1_buffer_size, dst_addr, dst_alloc_size, dst_ptr, param]

    def gen_param_type(self):
        """
        genarate type encoding param

        Returns
        -------
        param_type : the type encoding
        """
        # for vmla instr fmix mode
        if self.op_obj.src0_tensor_op.tensor_obj.dtype != self.op_obj.dst_tensor_op.tensor_obj.dtype \
                and self.op_obj.name == 'vmla':
            data_type = 'fmix'
        else:
            data_type = self.op_obj.src0_tensor_op.tensor_obj.dtype

        param_type = VECTOR_DTYPE_FN_ENCODER.get(data_type)
        if self.op_obj.name in ('vand', 'vor'):
            bit_width = int(get_dtype_bit_width(data_type))
            param_type = B16_B32_DTYPE_CODE.get(bit_width)
            param_type |= SPECIAL_DTYPE_INSTR.get(self.op_obj.name) << 2
        elif self.op_obj.name in SPECIAL_DTYPE_INSTR:
            param_type = VEC_DATA_TYPE_ENCODING_0_.get(data_type)
            param_type |= SPECIAL_DTYPE_INSTR.get(self.op_obj.name) << 2
        elif self.op_obj.name == 'vmulconv':
            param_type = _VMULCONV_DTYPE_ENCODING.get(self.op_obj.dst_tensor_op.tensor_obj.dtype)
        return param_type

    def create_gpr_x_t(self, context, temp_env):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context

        temp_env : the temp environment

        Returns
        -------
        xt_idx
        """
        # check addr overlap
        can_overlap_instr_name = ["vadd", "vsub", "vmul", "vmax", "vmin", "vor", "vand"]
        if self.op_obj.dst_tensor_op.tensor_obj.buffer == self.op_obj.src0_tensor_op.tensor_obj.buffer:
            tensor_bit_len = max(get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype),
                                 get_bit_len(self.op_obj.src0_tensor_op.tensor_obj.dtype))
            block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len]
            self.op_obj.dst_tensor_op.check_address_overlapping(self.op_obj.print_name, self.op_obj.control_op,
                                                                self.op_obj.src0_tensor_op, block_list)
        if self.op_obj.dst_tensor_op.tensor_obj.buffer == self.op_obj.src1_tensor_op.tensor_obj.buffer:
            if self.op_obj.name not in can_overlap_instr_name or self.op_obj.dst_tensor_op.tensor_obj.dtype not in \
                    ("float16", "int32", "float32") or self.op_obj.dst_tensor_op.repeat_times_value == 1:
                tensor_bit_len = max(get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype),
                                     get_bit_len(self.op_obj.src1_tensor_op.tensor_obj.dtype))
                block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len]
                self.op_obj.dst_tensor_op.check_address_overlapping(self.op_obj.print_name, self.op_obj.control_op,
                                                                    self.op_obj.src0_tensor_op, block_list)
            else:
                self.op_obj.dst_tensor_op.check_dst_src1_overlap_other(self.op_obj.print_name, self.op_obj.control_op,
                                                                       self.op_obj.src0_tensor_op,
                                                                       self.op_obj.src1_tensor_op)

        xt_idx = temp_env.alloc_register()
        x_t = self.op_obj.dst_tensor_op.blk_stride_value
        x_t |= self.op_obj.src0_tensor_op.blk_stride_value << _SRC_BLK_STRIDE_SHIFT_POS
        x_t |= self.op_obj.src1_tensor_op.blk_stride_value << _SRC1_BLK_STRIDE_SHIFT_POS
        x_t |= self.op_obj.control_op.stride_unit << STRIDE_UNIT_SHIFT_POS
        x_t |= self.op_obj.dst_tensor_op.repeat_times_value << REPEAT_SHIFT_POS
        x_t |= self.op_obj.dst_tensor_op.rep_stride_value << _DST_REPEAT_STRIDE_SHIFT_POS
        x_t |= self.op_obj.src0_tensor_op.rep_stride_value << _SRC_REPEAT_STRIDE_SHIFT_POS
        x_t |= self.op_obj.src1_tensor_op.rep_stride_value << _SRC1_REPEAT_STRIDE_SHIFT_POS

        context.model.write_gpr(xt_idx, x_t)

        return xt_idx

    def _check_all_params(self, context):
        self.op_obj.dst_tensor_op.set_context(context)
        self.op_obj.src0_tensor_op.set_context(context)
        self.op_obj.src1_tensor_op.set_context(context)
        self.op_obj.dst_tensor_op.set_repeat_times(self.op_obj.control_op.repeat_times)
        # check repeat_times
        TikCheckUtil.check_in_range_by_dtype(
            self.op_obj.dst_tensor_op.repeat_times_value,
            msg="repeat_times should be in the range of [%d, %d], input repeat_times is %s"
                % (0, MAX_REPEAT_TIMES, self.op_obj.dst_tensor_op.repeat_times_value), var_range=[0, MAX_REPEAT_TIMES])
        # check_stride
        self.op_obj.dst_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)
        self.op_obj.src0_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)
        self.op_obj.src1_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)
        self.op_obj.dst_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_SINGLE_BYTE)
        self.op_obj.src0_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_SINGLE_BYTE)
        self.op_obj.src1_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_SINGLE_BYTE)


class NewVectorScalarTemplate(STMT):
    """
    this template both have vector and scalar
    """

    def __init__(self, source_info, op_obj, tik_debugger=None):
        super(NewVectorScalarTemplate, self).__init__(source_info, tik_debugger)
        self.op_obj = op_obj
        if TikSocManager.is_v300_610l_soc():
            self.source_id = op_obj.tik_instance.context.debug_source_id

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        repeat = context.evaluate_expr(self.op_obj.control_op.repeat_times)
        self._check_params(context, repeat)
        if TikSocManager.is_v300_610l_soc():
            context.step_next(self.source_id)
            return

        align = vec_template_align(self.op_obj.src_tensor_op.tensor_obj.dtype)
        if self.op_obj.name == "vlrelu":
            align = ONE_BLK_SIZE
        temp_env = TempEnv()
        orig_ctrl_value = None
        if self.op_obj.control_op.mask_mode == "counter":
            orig_ctrl_value = set_mask_counter_mode(context)

        set_vector_mask(self.op_obj.control_op.mask, context, self.op_obj.control_op.mask_mode,
                        tensor_bit_len=max(get_bit_len(self.op_obj.src_tensor_op.tensor_obj.dtype),
                                           get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype)))

        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.op_obj.dst_tensor_op.tensor_obj, align, access_mode='w')

        param = _ENCODER.new_param()
        param.type = self.gen_param_type()
        param.xd = xd_idx
        param.xt = self.create_gpr_x_t(context, temp_env, repeat)
        param.xn = self.get_xn_idx(context, temp_env, align)
        param.xm = self.create_gpr_x_m(context, temp_env)
        param.out = self.op_obj.round_en

        instr = _VECTOR_SCALAR_FN_ENCODER[self.op_obj.name](param)

        context.model.step(instr)
        temp_env.check_mem_access(context.model, False)

        # mask: counter_mode, reset CTRL as orig_ctrl_value
        if self.op_obj.control_op.mask_mode == "counter":
            context.model.write_spr('CTRL', orig_ctrl_value)

        context.model.read_memory(dst_addr, self.op_obj.dst_tensor_op.tensor_obj.scope, dst_ptr,
                                  dst_alloc_size)

    def get_xn_idx(self, context, temp_env, align):
        """
        check tensor overflow

        Parameters
        ----------
        context : the stack context
        temp_env : the temp environment
        align : the align addr

        Returns
        -------
        xn_idx
        """
        xn_idx, _, src_buffer_size, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src_tensor_op.tensor_obj, align, access_mode='r')
        self.op_obj.src_tensor_op.check_read_mem_out_of_bounds(src_buffer_size, self.op_obj.control_op)
        return xn_idx

    def gen_param_type(self):
        """
        genarate type encoding param

        Returns
        -------
        param_type : the type encoding
        """
        dtype = self.op_obj.src_tensor_op.tensor_obj.dtype
        if dtype != self.op_obj.dst_tensor_op.tensor_obj.dtype:
            dtype = 'fmix'
        param_type = None
        if TikSocManager.is_v100_soc():
            param_type = VEC_DATA_TYPE_ENCODING[dtype]
        else:
            # vadds vmuls vmaxs vmins
            if self.op_obj.name in SPECIAL_DTYPE_INSTR and self.op_obj.name != "vaxpy":
                param_type = VEC_DATA_TYPE_ENCODING_V200[dtype]
                param_type |= SPECIAL_DTYPE_INSTR[self.op_obj.name] << 2
            elif self.op_obj.name == 'vaxpy':
                param_type = VECTOR_DTYPE_FN_ENCODER[dtype]
            elif self.op_obj.name == 'vshl':
                param_type = VSHL_DTYPE_CODE[dtype]
            elif self.op_obj.name == 'vshr':
                param_type = _VSHR_DTYPE_ENCODING_V200[dtype]
        return param_type

    def create_gpr_x_m(self, context, temp_env):
        """
        create general purpose register x_m

        Parameters
        ----------
        context : the stack context

        temp_env : the temp environment

        Returns
        -------
        xm_idx
        """
        scalar = context.evaluate_expr(self.op_obj.scalar)
        if self.op_obj.name in ("vshl", "vshr"):
            check_vshl_vshr_scalar(self.op_obj.src_tensor_op.tensor_obj.dtype, scalar)

        xm_idx = temp_env.alloc_register()
        x_m = cvt_float_to_uint(self.op_obj.src_tensor_op.tensor_obj.dtype, scalar)

        context.model.write_gpr(xm_idx, x_m)
        return xm_idx

    def create_gpr_x_t(self, context, temp_env, repeat):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context
        temp_env : the temp environment
        repeat: repeat times

        Returns
        -------
        xt_idx
        """

        TikCheckUtil.check_in_range_by_dtype(
            context.evaluate_expr(self.op_obj.control_op.stride_unit),
            msg="stride_unit should be in the range of [%s, %s], input stride_unit: %s"
                % (MIN_STRIDE_UNIT, MAX_STRIDE_UNIT, context.evaluate_expr(self.op_obj.control_op.stride_unit)),
            var_range=[MIN_STRIDE_UNIT, MAX_STRIDE_UNIT])

        tensor_bit_len = max(get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype),
                             get_bit_len(self.op_obj.src_tensor_op.tensor_obj.dtype))
        # check addr overlap
        block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len]
        if self.op_obj.src_tensor_op.tensor_obj.buffer == self.op_obj.dst_tensor_op.tensor_obj.buffer:
            self.op_obj.dst_tensor_op.check_address_overlapping(
                self.op_obj.print_name, self.op_obj.control_op, self.op_obj.src_tensor_op, block_list)

        xt_idx = temp_env.alloc_register()
        x_t = context.evaluate_expr(self.op_obj.dst_tensor_op.blk_stride)
        x_t |= context.evaluate_expr(self.op_obj.src_tensor_op.blk_stride) << SRC_BLOCK_STRIDE_SHIFT_POS
        x_t |= context.evaluate_expr(self.op_obj.control_op.stride_unit) << STRIDE_UNIT_SHIFT_POS
        x_t |= repeat << REPEAT_SHIFT_POS
        x_t |= context.evaluate_expr(self.op_obj.dst_tensor_op.rep_stride) << DST_REPEAT_STRIDE_SHIFT_POS
        x_t |= context.evaluate_expr(self.op_obj.src_tensor_op.rep_stride) << SRC_REPEAT_STRIDE_SHIFT_POS

        context.model.write_gpr(xt_idx, x_t)

        return xt_idx

    def _check_params(self, context, repeat):

        self.op_obj.dst_tensor_op.set_context(context)
        self.op_obj.src_tensor_op.set_context(context)
        # check repeat
        if self.op_obj.control_op.mask_mode == "normal":
            TikCheckUtil.check_in_range_by_dtype(
                repeat, msg="repeat_times should be in the range of [%d, %d], input repeat_times: %d"
                            % (0, MAX_REPEAT_TIMES, repeat), var_range=[0, MAX_REPEAT_TIMES])

        # check strides
        self.op_obj.dst_tensor_op.set_rep_stride_value()
        self.op_obj.src_tensor_op.set_rep_stride_value()
        self.op_obj.dst_tensor_op.set_blk_stride_value()
        self.op_obj.src_tensor_op.set_blk_stride_value()
        self.op_obj.dst_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_DOUBLE_BYTE)
        self.op_obj.src_tensor_op.check_tensor_op_blk_stride(MAX_BLK_STRIDE_DOUBLE_BYTE)
        self.op_obj.dst_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)
        self.op_obj.src_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_SINGLE_BYTE)


class V4DTRANS(STMT):
    """
    V4DTRANS instruction
    """
    ALIGNED_ADDR = 32
    _MIN_M_LEN = 1
    _MAX_M_LEN = 4095
    MIN_CHANNELS = 1
    MAX_V4_CHANNELS = 4095
    _CHANNELS_SHIFT_BIT_POS = 12
    _TRANS_DIR_SHIFT_BIT_POS = 63

    def __init__(self, source_info, op_obj, tik_debugger):
        super(V4DTRANS, self).__init__(source_info, tik_debugger)
        self.chw2hwc = op_obj.chw2hwc
        self.dst = op_obj.dst
        self.src = op_obj.src
        self.m_len = op_obj.m_len
        self.channels = op_obj.channels

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        temp_env = TempEnv()
        xn_idx, _, _, _ = copy_tensor_to_model(
            context, temp_env, self.src, V4DTRANS.ALIGNED_ADDR, access_mode='r')
        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.dst, V4DTRANS.ALIGNED_ADDR, access_mode='w')

        # we treat b16 as uint16 ...
        bit_width = get_dtype_bit_width(self.src.dtype)
        dtype = 'uint' + bit_width

        param = context.encoder.new_param()
        param.type = VEC_DATA_TYPE_ENCODING[dtype]
        param.xd = xd_idx
        param.xn = xn_idx
        param.xt = self.create_gpr_x_t(context, temp_env)

        instr = context.encoder.gen_v4dtrans(param)

        context.model.step(instr)
        temp_env.check_mem_access(context.model, False)

        context.model.read_memory(
            dst_addr, self.dst.scope, dst_ptr, dst_alloc_size)

    def create_gpr_x_t(self, context, temp_env):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context

        temp_env : the temp environment

        Returns
        -------
        xt_idx
        """
        if self.chw2hwc:
            trans_dir = 0
        else:
            trans_dir = 1

        m_len = context.evaluate_expr(self.m_len)
        channels = context.evaluate_expr(self.channels)

        # check m_len and channels in range
        TikCheckUtil.check_in_range_by_dtype(
            m_len, msg="m_len should be in the range of [%s,%s], input m_len: %s"
                       % (V4DTRANS._MIN_M_LEN, V4DTRANS._MAX_M_LEN, str(m_len)),
            var_range=[V4DTRANS._MIN_M_LEN, V4DTRANS._MAX_M_LEN])

        # check  image size must be 32B align
        image_size = m_len * get_dtype_size(self.src.dtype)
        TikCheckUtil.check_equality(
            image_size % ONE_BLK_SIZE, 0,
            "H*W*dtype_size should be 32 Byte aligned, "
            "input size is %s" % str(image_size))

        TikCheckUtil.check_in_range_by_dtype(
            channels, msg="channels should be in the range of [%s,%s], input channels: %s"
                          % (V4DTRANS.MIN_CHANNELS, V4DTRANS.MAX_V4_CHANNELS, str(channels)),
            var_range=[V4DTRANS.MIN_CHANNELS, V4DTRANS.MAX_V4_CHANNELS])
        # check tensor overflow
        src_offset = context.evaluate_expr(self.src.offset)
        dst_offset = context.evaluate_expr(self.dst.offset)
        # check address overlap
        check_addr_overlap_v4dtrans((
            self.dst, self.src, m_len, channels, dst_offset, src_offset))

        TikCheckUtil.check_le(
            m_len * channels,
            reduce_mul(self.src.original_shape) - src_offset,
            "src tensor overflow, m_len*channels should be less equal src size")
        TikCheckUtil.check_le(
            m_len * channels,
            reduce_mul(self.dst.original_shape) - dst_offset,
            "dst tensor overflow, m_len*channels should be less equal dst size")

        xt_idx = temp_env.alloc_register()
        x_t = m_len
        x_t |= (channels << V4DTRANS._CHANNELS_SHIFT_BIT_POS)
        x_t |= trans_dir << V4DTRANS._TRANS_DIR_SHIFT_BIT_POS

        context.model.write_gpr(xt_idx, x_t)

        return xt_idx
