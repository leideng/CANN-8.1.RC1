#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     context_vector.py
DESC:     debug context vector
CREATED:  2021-11-29 10:12:13
MODIFIED: 2021-11-29 15:30:23
"""
from tbe.common.platform import scope_wreg
from tbe.common.platform import scope_preg
from tbe.tik.common.common_util import is_predicate
from tbe.tik.debug.util import VecRegType
from tbe.tik.debug.util import VecRegTypeV300
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_params import MIN_SREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import MAX_SREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import MAX_AREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import MAX_UREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import MAX_VREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import MAX_PREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import MAX_WREG_ALLOCATED
from tbe.tik.tik_lib.tik_params import VREG_START_INDEX
from tbe.tik.tik_lib.tik_params import PREG_START_INDEX
from tbe.tik.tik_lib.tik_params import WREG_START_INDEX
from tbe.tik.tik_lib.tik_params import REGISTER_GROUP_NUM
from tbe.tik.tik_lib.tik_params import VL_T_MAP
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import WDTYPE_TO_DTYPE
from tbe.tik.common.util import TikUtil
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.common.tik_get_soc_name import get_soc_core_type
from tbe.tik.debug.vector_buffer import VectorBuffer
from tbe.tik.debug.npbuffer import NumpyBuffer


class ContextVector:
    """
    Vector variable correlation
    """
    def __init__(self):
        self.soc_name = get_soc_name()
        self.soc_core_type = get_soc_core_type()

        self.vector_buffer = VectorBuffer(self)
        self.v_register_alloc = 0
        self.p_register_alloc = 0
        self.a_register_alloc = 0
        self.u_register_alloc = 0
        self.w_register_alloc = 0
        self.s_register_alloc = MIN_SREG_ALLOCATED

        # cache all register
        self.v_register_id_map = {}
        self.p_register_id_map = {}
        self.a_register_id_map = {}
        self.u_register_id_map = {}
        self.w_register_id_map = {}

    def set_vector_buffer(self):
        """
        free vector NumpyBuffer
        """
        self.vector_buffer = None

    def vpd_preg_full(self, mask, dtype):
        """
        vpd preg full
        Parameters
        ----------
        mask: mask
        dtype: data type

        Returns
        -------
        no return
        """
        value_buffer = self.get_vector_value(mask).buffer
        flatten_np = value_buffer.reshape(-1)

        init_value = 17
        if TikUtil.dtype_is_b8(dtype):
            init_value = 255
        elif TikUtil.dtype_is_b16(dtype):
            init_value = 85
        for i in range(len(flatten_np)):
            value_buffer[i] = init_value

    def init_vreg_according_cache(self, vector, vreg_index):
        """
        init vreg according cache
        Parameters
        ----------
        vector: vector
        vreg_index: vreg index
        Returns
        -------
        no return
        """
        if TikSocManager.is_v300_610l_soc():
            self._init_vreg_according_cache_v300(vector, vreg_index, VecRegTypeV300.VREG)
            return
        value_buffer = self.get_vector_value(vector).buffer
        flatten_np = value_buffer.reshape(-1)
        vd_idx_rename = self.model.get_vec_register_index(vreg_index, VecRegType.V_REG)
        src_state = self.get_vector_state(vector)
        if src_state is True:
            for i in range(REGISTER_GROUP_NUM):
                new_index = VREG_START_INDEX + (vd_idx_rename + i * MAX_VREG_ALLOCATED) % \
                            (REGISTER_GROUP_NUM*MAX_VREG_ALLOCATED)
                self.model.write_vec_register(
                    new_index, flatten_np.ctypes.data, len(flatten_np) * DTYPE_SIZE.get(vector.dtype))

    def init_preg_according_cache(self, mask, preg_index, dtype=None):
        """
        init preg register according mask, preg_index and dtype.
        Parameters
        ----------
        mask: valid elements number
        preg_index: preg register index
        dtype: data type

        Returns
        -------
        no return
        """
        if TikSocManager.is_v300_610l_soc():
            self._init_preg_according_cache_v300(mask, preg_index, dtype)
            return
        pd_idx_rename = self.model.get_vec_register_index(preg_index, VecRegType.P_REG)
        # input mask is imm, Scalar, Epxr or None
        if not is_predicate(mask):
            if mask is None:
                self._init_preg_full(pd_idx_rename, dtype)
            else:
                self._init_preg_mask(mask, pd_idx_rename, dtype)
            return

        # input mask is Vector object
        value_buffer = self.get_vector_value(mask).buffer
        flatten_np = value_buffer.reshape(-1)
        src_state = self.get_vector_state(mask)
        if src_state is True:
            for i in range(REGISTER_GROUP_NUM):
                new_index = PREG_START_INDEX + (pd_idx_rename + i * MAX_PREG_ALLOCATED) % \
                            (REGISTER_GROUP_NUM*MAX_PREG_ALLOCATED)
                self.model.write_vec_register(new_index, flatten_np.ctypes.data, len(flatten_np))

    def alloc_v_register(self, vector, is_even=False):
        """
        allocate VR register

        Returns
        -------
        the result code
        """
        # check whether is wide register
        if vector.scope == scope_wreg:
            return self._alloc_w_register(vector)

        if vector in self.v_register_id_map:
            ret = self.v_register_id_map[vector]
            self.init_vreg_according_cache(vector, ret)
            if is_even is True:
                return ret, ret + 1
            return ret

        ret = self.v_register_alloc
        needed_v_nums = 1
        if is_even is True:
            needed_v_nums = 2
            ret += ret % 2

        if ret >= MAX_VREG_ALLOCATED:
            TikCheckUtil.raise_error("all vector register exhausted")
        self.v_register_alloc += needed_v_nums
        # save the vector register ID
        self.v_register_id_map[vector] = ret

        # init the vector register according the cache value
        self.init_vreg_according_cache(vector, ret)

        # if need two register, apply two continuous register
        if is_even is True:
            return ret, ret + 1
        return ret

    def alloc_p_register(self, mask, dtype=None, is_even_start=False, is_even=False):
        """
        allocate Pd register

        Returns
        -------
        if is_even is True, return two Pd register and first register index is even.
        if is_even_start is True, return one Pd register and register index is even.
        else, return one Pd register.
        """
        if is_predicate(mask) and mask in self.p_register_id_map:
            pd_idx = self.p_register_id_map[mask]
            self.init_preg_according_cache(mask, pd_idx, dtype)
            if is_even:
                return pd_idx, pd_idx + 1
            return pd_idx
        pd_idx = self.p_register_alloc
        needed_p_nums = 1
        if is_even:
            needed_p_nums = 2
            pd_idx += pd_idx % 2
        elif is_even_start:
            pd_idx += pd_idx % 2

        if pd_idx >= MAX_PREG_ALLOCATED:
            TikCheckUtil.raise_error("all predicate register exhausted")
        self.p_register_alloc = pd_idx + needed_p_nums
        if is_predicate(mask):
            self.p_register_id_map[mask] = pd_idx
        self.init_preg_according_cache(mask, pd_idx, dtype)

        # if need two register, apply two continuous register
        if is_even is True:
            return pd_idx, pd_idx + 1
        return pd_idx

    def free_all_register(self):
        """
        free all registers
        """
        self.v_register_alloc = 0
        self.w_register_alloc = 0
        self.p_register_alloc = 0
        self.a_register_alloc = 0
        self.u_register_alloc = 0
        self.s_register_alloc = MIN_SREG_ALLOCATED
        self.v_register_id_map = {}
        self.p_register_id_map = {}
        self.a_register_id_map = {}
        self.u_register_id_map = {}
        self.w_register_id_map = {}

    def alloc_a_register(self, vector):
        """
        allocate address register

        Returns
        -------
        the result code
        """
        if vector in self.a_register_id_map:
            return self.a_register_id_map[vector]

        ret = self.a_register_alloc
        if ret >= MAX_AREG_ALLOCATED:
            TikCheckUtil.raise_error("all address register exhausted")
        self.a_register_alloc += 1
        # save the vector address register ID
        self.a_register_id_map[vector] = ret
        return ret

    def alloc_u_register(self, vector):
        """
        allocate alignment register

        Returns
        -------
        the result code
        """
        if vector in self.u_register_id_map:
            ureg_idx = self.u_register_id_map[vector]
            return ureg_idx

        ret = self.u_register_alloc
        if ret >= MAX_UREG_ALLOCATED:
            TikCheckUtil.raise_error("all alignment register exhausted")

        self.u_register_alloc += 1
        # save the vector alignment register ID
        self.u_register_id_map[vector] = ret
        return ret

    def alloc_s_register(self, dtype):
        """
        allocate Sn register

        Returns
        -------
        the result code
        """
        ret = self.s_register_alloc
        needed_s_nums = 1
        if DTYPE_SIZE.get(dtype) > 2:
            needed_s_nums = 2
            ret += ret % 2
        if ret >= MAX_SREG_ALLOCATED:
            TikCheckUtil.raise_error("all share register exhausted")
        self.s_register_alloc += needed_s_nums
        return ret, needed_s_nums

    def add_vector(self, vector):
        """
        add tensor to buffer

        Parameters
        ----------
        vector : the added vector

        Returns
        -------
        no return
        """
        self.vector_buffer.add_vector(vector)

    def get_vector_value(self, vector):
        """
        get vector from buffer_mapping

        Parameters
        ----------
        vector: vector

        Returns
        -------
        buffer
        """
        try:
            return self.vector_buffer.get_npbuffer_by_vector(vector)
        except KeyError:
            return None
        finally:
            pass

    def set_vector_state(self, vector, is_valid=True):
        """
        set vector state
        Parameters
        ----------
        vector: vector
        is_valid: valid flag

        Returns
        -------
        no return
        """
        self.vector_buffer.set_value_valid(vector, is_valid)

    def get_vector_state(self, vector):
        """
        get vector state
        Parameters
        ----------
        vector: vector

        Returns
        -------
        True or False
        """
        return self.vector_buffer.get_value_valid(vector)

    def _init_preg_full_v300(self, preg_index, dtype):
        data_len = self.dprofile.reg_size_query(scope_preg) // DTYPE_SIZE["uint8"]

        npbuf = NumpyBuffer((self, data_len, "uint8", "random", None)).buffer
        flatten_np = npbuf.reshape(-1)

        init_value = 17  # 0b00010001
        if TikUtil.dtype_is_b8(dtype):
            init_value = 255  # 0b11111111
        elif TikUtil.dtype_is_b16(dtype):
            init_value = 85  # 0b01010101

        for i in range(len(flatten_np)):
            npbuf[i] = init_value

        self.model.write_vec_spr(VecRegTypeV300.PREG, preg_index, flatten_np.ctypes.data, len(flatten_np))

    def _init_preg_full(self, pd_idx_rename, dtype):
        data_len = self.dprofile.reg_size_query(scope_preg) // DTYPE_SIZE.get("uint8")
        npbuf = NumpyBuffer((self, data_len, "uint8", "random", None)).buffer
        flatten_np = npbuf.reshape(-1)

        init_value = 17  # 0b00010001
        if TikUtil.dtype_is_b8(dtype):
            init_value = 255  # 0b11111111
        elif TikUtil.dtype_is_b16(dtype):
            init_value = 85  # 0b01010101

        for i in range(len(flatten_np)):
            npbuf[i] = init_value

        for i in range(REGISTER_GROUP_NUM):
            new_index = PREG_START_INDEX + \
                        (pd_idx_rename + i * MAX_PREG_ALLOCATED) % \
                        (REGISTER_GROUP_NUM * MAX_PREG_ALLOCATED)
            self.model.write_vec_register(
                new_index, flatten_np.ctypes.data, len(flatten_np))

    def _set_mask_npbuf(self, data_len, dtype, mask_value):
        npbuf = NumpyBuffer((self, data_len, "uint8", "constant", 0)).buffer
        flatten_np = npbuf.reshape(-1)

        init_value = 0b0001
        mask_bit_len = 4
        if TikUtil.dtype_is_b8(dtype):
            init_value = 0b1
            mask_bit_len = 1
        elif TikUtil.dtype_is_b16(dtype):
            init_value = 0b01
            mask_bit_len = 2

        uint8_bit_len = 8
        uint8_mask_len = uint8_bit_len // mask_bit_len
        for i in range(mask_value):
            mask_index = i // uint8_mask_len
            if i % uint8_mask_len == 0:
                npbuf[mask_index] = init_value
            else:
                npbuf[mask_index] = npbuf[mask_index] << mask_bit_len | init_value
        return flatten_np

    def _init_preg_mask_v300(self, mask, preg_index, dtype):
        mask_value = self.evaluate_expr(mask)
        vl_t = VL_T_MAP[dtype]
        if mask_value < 0 or mask_value > vl_t:
            TikCheckUtil.raise_error(
                "mask should be in range [0, {}] for dtype {}, "
                "but input value: {}".format(vl_t, dtype, mask_value))

        data_len = self.dprofile.reg_size_query(scope_preg) // DTYPE_SIZE["uint8"]
        flatten_np = self._set_mask_npbuf(data_len, dtype, mask_value)
        self.model.write_vec_spr(VecRegTypeV300.PREG, preg_index, flatten_np.ctypes.data, len(flatten_np))

    def _init_preg_according_cache_v300(self, mask, preg_index, dtype=None):
        """
        init preg register according mask, preg_index and dtype.
        Parameters
        ----------
        mask: valid elements number
        preg_index: preg register index
        dtype: data type

        Returns
        -------
        no return
        """
        # input mask is imm, Scalar, Epxr or None
        if not is_predicate(mask):
            if mask is None:
                self._init_preg_full_v300(preg_index, dtype)
            else:
                self._init_preg_mask_v300(mask, preg_index, dtype)
            return

        # input mask is Vector object
        value_buffer = self.get_vector_value(mask).buffer
        flatten_np = value_buffer.reshape(-1)
        src_state = self.get_vector_state(mask)
        if src_state is True:
            self.model.write_vec_spr(VecRegTypeV300.PREG, preg_index, flatten_np.ctypes.data, len(flatten_np))

    def _init_preg_mask(self, mask, pd_idx_rename, dtype):
        mask_value = self.evaluate_expr(mask)
        if mask_value < 0 or mask_value > VL_T_MAP.get(dtype):
            TikCheckUtil.raise_error(
                "mask should be in range [0, {}] for dtype {}, "
                "but input value: {}".format(VL_T_MAP.get(dtype), dtype, mask_value))

        data_len = self.dprofile.reg_size_query(scope_preg) // DTYPE_SIZE.get("uint8")
        flatten_np = self._set_mask_npbuf(data_len, dtype, mask_value)

        for i in range(REGISTER_GROUP_NUM):
            new_index = PREG_START_INDEX + \
                        (pd_idx_rename + i * MAX_PREG_ALLOCATED) % \
                        (REGISTER_GROUP_NUM * MAX_PREG_ALLOCATED)
            self.model.write_vec_register(new_index, flatten_np.ctypes.data, len(flatten_np))

    def _init_vreg_according_cache_v300(self, vector, vreg_index, reg):
        """
        init vreg according cache
        Parameters
        ----------
        vector: vector
        vreg_index: vreg index
        Returns
        -------
        no return
        """
        value_buffer = self.get_vector_value(vector).buffer
        flatten_np = value_buffer.reshape(-1)
        src_state = self.get_vector_state(vector)
        if src_state is True:
            self.model.write_vec_spr(
                reg, vreg_index, flatten_np.ctypes.data, len(flatten_np) * DTYPE_SIZE[vector.dtype])

    def _init_wreg_according_cache(self, w_vector, wreg_index):
        """
        init the wide register with buffer data
        """
        if TikSocManager.is_v300_610l_soc():
            self._init_vreg_according_cache_v300(w_vector, wreg_index, VecRegTypeV300.WREG)
            return
        value_buffer = self.get_vector_value(w_vector).buffer
        flatten_np = value_buffer.reshape(-1)
        src_state = self.get_vector_state(w_vector)
        dst_dtype = WDTYPE_TO_DTYPE.get(w_vector.dtype)
        if src_state is True:
            new_index = WREG_START_INDEX + wreg_index
            self.model.write_vec_register(new_index, flatten_np.ctypes.data, len(flatten_np)*DTYPE_SIZE.get(dst_dtype))

    def _alloc_w_register(self, w_vector):
        """
        allocate a new wide register and init it with buffer data
        """
        if w_vector in self.w_register_id_map:
            ret = self.w_register_id_map[w_vector]
            self._init_wreg_according_cache(w_vector, ret)
            return ret

        ret = self.w_register_alloc
        needed_w_nums = 1

        if ret >= MAX_WREG_ALLOCATED:
            TikCheckUtil.raise_error("all vector register exhausted")
        self.w_register_alloc += needed_w_nums
        # save the wide register ID
        self.w_register_id_map[w_vector] = ret

        # init the wide register according the cache value
        self._init_wreg_according_cache(w_vector, ret)
        return ret
