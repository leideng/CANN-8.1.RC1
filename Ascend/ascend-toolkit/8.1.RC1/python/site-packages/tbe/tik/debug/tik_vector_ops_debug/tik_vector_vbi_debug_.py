#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_vbi_debug_.py
DESC:     provide vector instructions
CREATED:  2021-11-23 10:53:42
MODIFIED: 2021-11-26 10:17:00
"""

from tbe.tik.common.util import TikUtil
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import get_mask_len
from tbe.tik.common.common_util import vec_template_align
from tbe.tik.debug.util import copy_tensor_to_model
from tbe.tik.debug.util import cvt_float_to_uint
from tbe.tik.debug.util import set_vector_mask
from tbe.tik.debug.util import get_flatten_idx
from tbe.tik.debug.util import check_mask_valid_for_debug
from tbe.tik.debug.simd import eval_mask
from tbe.tik.debug.statement import STMT
from tbe.tik.debug.sim.util import TempEnv
from tbe.tik.debug.sim.instr_encoder import Encoder
from tbe.tik.debug.debug_encoder import VEC_DATA_TYPE_ENCODING_V200
from tbe.tik.debug.debug_encoder import SPECIAL_DTYPE_INSTR
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MASK_MODE_MASK
from tbe.tik.tik_lib.tik_params import MASK_COUNTER_MODE_ENABLE_SHIFT_POS
from tbe.tik.tik_lib.tik_params import ONE_REP_BYTE_SIZE
from tbe.tik.tik_lib.tik_params import MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import MIN_STRIDE
from tbe.tik.tik_lib.tik_params import SRC_BLOCK_STRIDE_SHIFT_POS
from tbe.tik.tik_lib.tik_params import REPEAT_SHIFT_POS
from tbe.tik.tik_lib.tik_params import STRIDE_UNIT_SHIFT_POS
from tbe.tik.tik_lib.tik_params import DST_REPEAT_STRIDE_SHIFT_POS
from tbe.tik.tik_lib.tik_params import SRC_REPEAT_STRIDE_SHIFT_POS
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_vector_api.tik_vbi_control import VbiControlOp
from tbe.tik.tik_lib.tik_vector_api.tik_vector_vbi_tensor_op import VbiTensorOp

_ENCODER = Encoder()
_ROUND_TO_NEAREST_ENABLE = 0


def vadds_create_gpr_x_t(context, temp_env):
    """
    create general purpose register x_t for function vadds_gen_param

    Parameters
    ----------
    context : the stack context
    temp_env : the temp environment

    Returns
    -------
    xt_idx
    """
    repeat = 1
    dst_block_stride = 1
    src_block_stride = 1
    dst_repeat_stride = 8
    src_repeat_stride = 8
    stride_unit = 0

    xt_idx = temp_env.alloc_register()
    x_t = dst_block_stride
    x_t |= src_block_stride << SRC_BLOCK_STRIDE_SHIFT_POS
    x_t |= stride_unit << STRIDE_UNIT_SHIFT_POS
    x_t |= repeat << REPEAT_SHIFT_POS
    x_t |= dst_repeat_stride << DST_REPEAT_STRIDE_SHIFT_POS
    x_t |= src_repeat_stride << SRC_REPEAT_STRIDE_SHIFT_POS

    context.model.write_gpr(xt_idx, x_t)

    return xt_idx


class VbiBasic(STMT):
    """
    VBI Basic instruction
    """
    def __init__(self, source_info, op_obj):
        super(VbiBasic, self).__init__(source_info, op_obj.tik_instance.context.tik_debugger)
        self.op_obj = op_obj

    def check_vbi_overflow_overlap(self, mask_len, src0_buffer_size):
        """
        check vbi instruction whether src0_offset and src1 is overflow, check whether src0_offset and src1 is overlap

        Parameters
        ----------
        mask_len: length between lowest digit and top effective digit of mask
        src0_buffer_size: src0_offset tensor buffer size
        Returns
        -------
        None
        """
        # check read mem out of bounds
        if isinstance(self.op_obj.vbi_control_op.mask, (list, tuple)) and \
                (is_basic_expr(self.op_obj.vbi_control_op.mask)):
            src0_offset_valid_mask = BLK_NUM_PER_REP
        else:
            src0_offset_valid_mask = ceil_div(
                mask_len, ONE_BLK_SIZE // DTYPE_SIZE[self.op_obj.src0_tensor_op.tensor_obj.dtype])
        src0_offset_control_op = VbiControlOp(src0_offset_valid_mask,
                                              (self.op_obj.vbi_control_op.vertical_repeat_times,
                                               self.op_obj.vbi_control_op.horizontal_repeat_times),
                                              self.op_obj.vbi_control_op.repeat_mode,
                                              self.op_obj.vbi_control_op.vertical_repeat_offset)

        self.op_obj.src0_offset_tensor_op.check_read_mem_out_of_bounds(src0_buffer_size, src0_offset_control_op)

        # check src1 overflow
        self.op_obj.src1_tensor_op.check_vbi_src1_tensor_overflow(self.op_obj.vbi_control_op)
        # check overlapping between dst, src0, src0_offset and src1
        self.op_obj.vbi_control_op.mask_len = mask_len
        self.op_obj.vbi_check_params.check_vbi_overlap()

    def check_mask(self, context):
        """
        check mask
        """
        # check mask
        check_mask_valid_for_debug(self.op_obj.vbi_control_op.mask, context, mask_mode="normal",
                                   tensor_bit_len=get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype))
        mask_value = eval_mask(self.op_obj.vbi_control_op.mask, context)
        # check repeat_times
        self.op_obj.vbi_control_op.check_vertical_repeat_times()
        self.op_obj.vbi_control_op.check_horizontal_repeat_times()
        if isinstance(mask_value, int):

            mask_len = mask_value
        else:
            mask_len = get_mask_len(mask_value)
        return mask_len


class VBI(VbiBasic):
    """
    VBI instruction
    """

    def __init__(self, source_info, op_obj):
        super(VBI, self).__init__(source_info, op_obj)
        self.op_obj = op_obj

    @staticmethod
    def set_mask_counter_mode(context):
        """
        set mask with counter mode
        """
        # save orig_ctrl_value
        orig_ctrl_value = context.model.read_spr('CTRL')
        # mask: counter_mode, set CTRL[56] as 1
        ctrl_value = orig_ctrl_value & MASK_MODE_MASK
        ctrl_value = ctrl_value | (1 << MASK_COUNTER_MODE_ENABLE_SHIFT_POS)
        context.model.write_spr('CTRL', ctrl_value)
        return orig_ctrl_value

    @staticmethod
    def vadds_gen_param_type(src):
        """
        genarate type encoding param for function vadds_gen_param

        Parameters
        ----------
        src: src operator

        Returns
        -------
        param_type : the type encoding
        """
        dtype = src.dtype
        param_type = VEC_DATA_TYPE_ENCODING_V200[dtype]
        param_type |= SPECIAL_DTYPE_INSTR["vadds"] << 2
        return param_type

    @staticmethod
    def vadds_create_gpr_x_m(context, temp_env, scalar, src):
        """
        create general purpose register x_m for function vadds_gen_param

        Parameters
        ----------
        context : the stack context
        temp_env : the temp environment
        scalar: scalar operator
        src: src operator

        Returns
        -------
        xm_idx
        """
        scalar = context.evaluate_expr(scalar)
        xm_idx = temp_env.alloc_register()
        x_m = cvt_float_to_uint(src.dtype, scalar)

        context.model.write_gpr(xm_idx, x_m)
        return xm_idx

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        self.op_obj.dst_tensor_op.set_context(context)
        self.op_obj.src0_tensor_op.set_context(context)
        self.op_obj.src1_tensor_op.set_context(context)
        self.op_obj.src0_offset_tensor_op.set_context(context)
        self.op_obj.vbi_control_op.set_context(context)

        temp_env = TempEnv()
        align = vec_template_align(self.op_obj.dst_tensor_op.tensor_obj.dtype)
        mask_len = self.cal_src0_really_address(context, temp_env, align)

        set_vector_mask(self.op_obj.vbi_control_op.mask, context, mask_mode="normal",
                        tensor_bit_len=get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype))
        xn_idx, _, src0_buffer_size, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src0_offset_tensor_op.tensor_obj, align, access_mode='r')
        xm_idx, _, _, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src1_tensor_op.tensor_obj, align, access_mode='r')
        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.op_obj.dst_tensor_op.tensor_obj, align, access_mode='w')

        param = _ENCODER.new_param()
        param.type = self.gen_param_type()
        param.xt = self.create_gpr_x_t(context, temp_env)
        param.xd = xd_idx
        param.xn = xn_idx
        param.xm = xm_idx

        self.check_vbi_overflow_overlap(mask_len, src0_buffer_size)

        context.model.step(_ENCODER.gen_vbi(param))
        temp_env.check_mem_access(context.model, False)

        context.model.read_memory(dst_addr, self.op_obj.dst_tensor_op.tensor_obj.scope, dst_ptr, dst_alloc_size)

    def cal_src0_really_address(self, context, temp_env, align):
        """
        for vbi, need calculate src0 block addr according to
        src0 start addr and src0_offset

        Parameters
        ----------
        context : the stack context
        temp_env: the temp environment
        align: the addr align size
        Returns
        -------
        mask_len
        """
        _, src0_addr, _, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src0_tensor_op.tensor_obj, align, access_mode='r')
        src0_flatten_idx = get_flatten_idx(self.op_obj.src0_tensor_op.tensor_obj, context) * \
                           DTYPE_SIZE[self.op_obj.src0_tensor_op.tensor_obj.dtype]
        src0_start_addr = context.evaluate_expr(src0_addr + src0_flatten_idx)
        mask_len, valid_src0_offset_num = self.get_vadds_mask(context)
        self.vadds_actual_eval_(context, temp_env, valid_src0_offset_num, src0_start_addr)
        return mask_len

    def get_vadds_mask(self, context):
        """
        get mask_len corresponding to self.mask and need elements for vadds part as vadds counter mode mask
        """
        # check mask
        check_mask_valid_for_debug(self.op_obj.vbi_control_op.mask, context, mask_mode="normal",
                                   tensor_bit_len=get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype))
        mask_value = eval_mask(self.op_obj.vbi_control_op.mask, context)
        # check repeat_times
        self.op_obj.vbi_control_op.check_vertical_repeat_times()
        self.op_obj.vbi_control_op.check_horizontal_repeat_times()

        if isinstance(mask_value, int):
            mask_len = mask_value
        else:
            mask_len = get_mask_len(mask_value)
        block_value = ONE_BLK_SIZE // DTYPE_SIZE[self.op_obj.dst_tensor_op.tensor_obj.dtype]
        valid_block_len = ceil_div(mask_len, block_value)
        repeat_time = self.op_obj.vbi_control_op.vertical_repeat_times_value * \
                      self.op_obj.vbi_control_op.horizontal_repeat_times_value - 1
        need_src0_block = repeat_time * BLK_NUM_PER_REP + valid_block_len
        return mask_len, need_src0_block

    def vadds_actual_eval_(self, context, temp_env, mask, scalar):
        """
        vadds eval function for vbi instruction

        Parameters
        ----------
        context : the stack context
        temp_env: the temp environment
        mask: Effective operation on element, divided into two model: Continuous and bit by bit.
        scalar: scalar operator
        Returns
        -------
        None
        """
        mask_mode = "counter"
        orig_ctrl_value = self.set_mask_counter_mode(context)

        set_vector_mask(mask, context, mask_mode,
                        tensor_bit_len=get_bit_len(self.op_obj.src0_offset_tensor_op.tensor_obj.dtype))

        align = vec_template_align(self.op_obj.src0_offset_tensor_op.tensor_obj.dtype)
        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.op_obj.src0_offset_tensor_op.tensor_obj, align, access_mode='w')
        xn_idx = self._vadds_gen_xn_and_check_overflow(context, temp_env, align, mask)

        param = _ENCODER.new_param()
        param.type = self.vadds_gen_param_type(self.op_obj.src0_offset_tensor_op.tensor_obj)
        param.xd = xd_idx
        param.xn = xn_idx
        param.xm = self.vadds_create_gpr_x_m(context, temp_env, scalar, self.op_obj.src0_offset_tensor_op.tensor_obj)
        param.xt = vadds_create_gpr_x_t(context, temp_env)
        param.out = _ROUND_TO_NEAREST_ENABLE

        context.model.step(_ENCODER.gen_vaddsx(param))
        temp_env.check_mem_access(context.model, False)

        # mask: counter_mode, reset CTRL as orig_ctrl_value
        context.model.write_spr('CTRL', orig_ctrl_value)

        context.model.read_memory(dst_addr, self.op_obj.src0_offset_tensor_op.tensor_obj.scope, dst_ptr,
                                  dst_alloc_size)

    def gen_param_type(self):
        """
        generate param type encoding
        """
        type_encoding = None
        if self.op_obj.dst_tensor_op.tensor_obj.dtype == "float16":
            type_encoding = 0
        else:
            TikCheckUtil.raise_error("instruction vbi not support this dtype of dst")
        return type_encoding

    def create_gpr_x_t(self, context, temp_env):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context
        temp_env : the temp environment

        Returns
        -------
        xt_idx
        """
        dst_blk_stride = context.evaluate_expr(self.op_obj.dst_tensor_op.blk_stride)
        vertical_repeat_times = context.evaluate_expr(
            self.op_obj.vbi_control_op.vertical_repeat_times)
        horizontal_repeat_times = context.evaluate_expr(
            self.op_obj.vbi_control_op.horizontal_repeat_times)
        vertical_repeat_offset = context.evaluate_expr(self.op_obj.vbi_control_op.vertical_repeat_offset)
        # check strides
        TikCheckUtil.check_in_range_by_dtype(
            dst_blk_stride, msg="dst_blk_stride should be in the range of [%d, %d], "
            "input dst_blk_stride: %s" % (MIN_STRIDE, MAX_BLK_STRIDE_DOUBLE_BYTE, dst_blk_stride),
            var_range=[MIN_STRIDE, MAX_BLK_STRIDE_DOUBLE_BYTE])
        TikCheckUtil.check_in_range_by_dtype(
            vertical_repeat_offset, msg="vertical_repeat_offset should be in the range of [%d, %d], "
            "input vertical_repeat_offset: %s"
            % (0, MAX_REPEAT_TIMES_DOUBLE_BYTE, vertical_repeat_offset),
            var_range=[0, MAX_REPEAT_TIMES_DOUBLE_BYTE])

        xt_idx = temp_env.alloc_register()
        x_t = 0
        x_t |= horizontal_repeat_times << 56
        x_t |= vertical_repeat_times << 48
        x_t |= vertical_repeat_offset << 32
        x_t |= dst_blk_stride << 16
        x_t += self.op_obj.vbi_control_op.repeat_mode

        context.model.write_gpr(xt_idx, x_t)
        return xt_idx

    def _vadds_gen_xn_and_check_overflow(self, context, temp_env, align, mask):
        """
        for vadds_actual_eval_, gen xn_idx and check src overflow
        """
        xn_idx, _, src_buffer_size, _ = copy_tensor_to_model(
            context, temp_env, self.op_obj.src0_offset_tensor_op.tensor_obj, align, access_mode='r')
        src_control_op = VbiControlOp(mask, (self.op_obj.vbi_control_op.vertical_repeat_times,
                                             self.op_obj.vbi_control_op.horizontal_repeat_times),
                                      self.op_obj.vbi_control_op.repeat_mode,
                                      self.op_obj.vbi_control_op.vertical_repeat_offset)
        src_control_op.mask_mode = "counter"
        src_control_op.repeat_times = MIN_REPEAT_TIMES
        src_tensor_op = VbiTensorOp(self.op_obj.src0_offset_tensor_op.tensor_obj, 1, 8, "src0_offset")
        src_tensor_op.set_context(context)
        src_tensor_op.block_len = ONE_REP_BYTE_SIZE // get_bit_len(src_tensor_op.tensor_obj.dtype)
        src_tensor_op.nblock = BLK_NUM_PER_REP
        src_tensor_op.check_read_mem_out_of_bounds(src_buffer_size, src_control_op)

        return xn_idx


class VBIHighApi(VbiBasic):
    """
    VBI instruction
    """

    def __init__(self, source_info, op_obj):
        super(VBIHighApi, self).__init__(source_info, op_obj)
        self.op_obj = op_obj
        if TikSocManager.is_v300_610l_soc():
            self.source_id = op_obj.tik_instance.context.debug_source_id

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        if TikSocManager.is_v300_610l_soc():
            context.step_next(self.source_id)
            return
        TempEnv()
        self.op_obj.dst_tensor_op.set_context(context)
        self.op_obj.src0_tensor_op.set_context(context)
        self.op_obj.src1_tensor_op.set_context(context)
        self.op_obj.src0_offset_tensor_op.set_context(context)
        self.op_obj.vbi_control_op.set_context(context)
        # check UB addr 32B align
        align = vec_template_align(self.op_obj.dst_tensor_op.tensor_obj.dtype)
        self.op_obj.dst_tensor_op.check_tensor_op_address_align(self.op_obj.name, align)
        self.op_obj.src0_tensor_op.check_tensor_op_address_align(self.op_obj.name, align)
        self.op_obj.src1_tensor_op.check_tensor_op_address_align(self.op_obj.name, align)
        src0_offset_align = vec_template_align(self.op_obj.src0_offset_tensor_op.tensor_obj.dtype)
        self.op_obj.src0_offset_tensor_op.check_tensor_op_address_align(self.op_obj.name, src0_offset_align)

        set_vector_mask(self.op_obj.vbi_control_op.mask, context, mask_mode="normal",
                        tensor_bit_len=get_bit_len(self.op_obj.dst_tensor_op.tensor_obj.dtype))
        mask_len = self.check_mask(context)
        # check horizontal_repeat_times
        self.op_obj.vbi_check_params.check_220_h_rep_times_and_v_repeat_offset()

        self.check_dst_params_and_overflow(context, mask_len)
        src0_buffer_size = context.evaluate_expr(reduce_mul(self.op_obj.src0_tensor_op.tensor_obj.original_shape)) \
                           * DTYPE_SIZE[self.op_obj.src0_offset_tensor_op.tensor_obj.dtype]

        self.check_vbi_overflow_overlap(mask_len, src0_buffer_size)

    def get_vbi_dst_need_size(self, context, mask_len):
        """
        get vbi dst tensor need size
        """
        block_len = ONE_BLK_SIZE // DTYPE_SIZE[self.op_obj.dst_tensor_op.tensor_obj.dtype]
        dst_blk_stride = context.evaluate_expr(self.op_obj.dst_tensor_op.blk_stride)
        dst_repeat_offset = context.evaluate_expr(self.op_obj.vbi_control_op.vertical_repeat_offset)
        vertical_repeat_times = context.evaluate_expr(self.op_obj.vbi_control_op.vertical_repeat_times)
        if mask_len is None or is_basic_expr(TikUtil.to_list(mask_len)):
            max_offset = (vertical_repeat_times - 1) * dst_repeat_offset + \
                         (BLK_NUM_PER_REP - 1) * dst_blk_stride * block_len + block_len
            return max_offset
        if mask_len % block_len == 0:
            max_offset = (vertical_repeat_times - 1) * dst_repeat_offset + \
                         (mask_len // block_len - 1) * dst_blk_stride * block_len + block_len
        else:
            max_offset = (vertical_repeat_times - 1) * dst_repeat_offset + \
                         (mask_len // block_len) * dst_blk_stride * block_len + mask_len % block_len
        return max_offset

    def check_dst_params_and_overflow(self, context, mask_len):
        """
        check dst_blk_stride, vertical_repeat_offset and dst tensor overflow
        """
        TikCheckUtil.check_in_range_by_dtype(
            self.op_obj.dst_tensor_op.blk_stride_value,
            msg="dst_blk_stride should be in the range of [%d, %d], input value is %s" % (
                MIN_STRIDE, MAX_BLK_STRIDE_SINGLE_BYTE, self.op_obj.dst_tensor_op.blk_stride_value),
            var_range=[MIN_STRIDE, MAX_BLK_STRIDE_SINGLE_BYTE])
        self.op_obj.vbi_control_op.check_vertical_repeat_offset()

        # check dst overflow
        max_offset = self.get_vbi_dst_need_size(context, mask_len)
        offset = context.evaluate_expr(self.op_obj.dst_tensor_op.tensor_obj.offset)
        total_size = context.evaluate_expr(reduce_mul(self.op_obj.dst_tensor_op.tensor_obj.original_shape))
        need_offset = max_offset + offset
        TikCheckUtil.check_le(need_offset, total_size, "dst tensor overflow, instruction need {} but "
                                                       "only {}".format(need_offset, total_size))
