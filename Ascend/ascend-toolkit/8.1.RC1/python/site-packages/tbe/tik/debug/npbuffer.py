#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
# FILE:     npbuffer.py
# DESC:     To store numpy buffer
# CREATED:  2019-7-04 20:12:13
# MODIFIED: 2020-12-7 19:17:00
"""
import gc
from multiprocessing.sharedctypes import RawArray
from multiprocessing.heap import BufferWrapper
from multiprocessing.heap import Heap
import numpy as np

from tbe.tik.common.util import ceil_div
from tbe.tik.tik_lib.tik_params import BIT_LEN_8
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import WDTYPE_TO_DTYPE
from tbe.tik.common.util import reduce_mul
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil

_MIN_INIT_VALUE = 0
_MAX_INIT_VALUE = 255
_MAX_RANDINT = 255


def create_shared_array_by_np_array(np_array):
    """
    create RawArray for gm tensor, for multi-core multi-process create global shared memory
    Parameters
    ----------
    np_array: input numpy array

    Returns
    -------
    numpy array object of shared memory
    """
    size = reduce_mul(np_array.shape)
    dtype = np_array.dtype
    # dtype for: c:int8/B:uint8/h:int16/H:uint16/i:int32/I:uint32/l:int64/L:uint64/f:fp32/d:fp64
    if dtype.isbuiltin and dtype.char in 'bBhHiIlLfd':
        typecode = dtype.char
        np_array_ravel = np_array.ravel()
    else:
        # only for dtype fp16, typecode is B, so here size must be Bytes
        typecode, size = 'B', size * dtype.itemsize
        np_array_ravel = np_array.ravel().view("uint8")

    # default value is 0, won't init with others value, if the size is large, init shared memory very slow
    # many times, GM memory values from users, and will init with user input value
    shared_array = RawArray(typecode, np_array_ravel)
    shared_np_array = np.frombuffer(shared_array, dtype).reshape(np_array.shape)
    return shared_np_array


def _create_shared_array(context, shape, dtype):
    """
    create RawArray for gm tensor, for multi-core multi-process create global shared memory
    Parameters
    ----------
    context: object of class Context
    shape: gm tensor shape
    dtype: gm tensor dtype

    Returns
    -------
    RawArray object of shared memory
    """
    imm_shape = [context.evaluate_expr(s) for s in shape]
    if any(not isinstance(i, int) for i in imm_shape):
        return None
    size = reduce_mul(imm_shape)
    dtype = np.dtype(dtype)
    # dtype for: c:int8/B:uint8/h:int16/H:uint16/i:int32/I:uint32/l:int64/L:uint64/f:fp32/d:fp64
    if dtype.isbuiltin and dtype.char in 'bBhHiIlLfd':
        typecode = dtype.char
    else:
        # only for dtype fp16, typecode is B, so here size must be Bytes
        typecode, size = 'B', size * dtype.itemsize

    # default value is 0, won't init with others value, if the size is large, init shared memory very slow
    # many times, GM memory values from users, and will init with user input value
    shared_array = RawArray(typecode, size)
    return shared_array, imm_shape


def get_uninited_buffer(params_list, init_mode='random', init_value=None, is_wide_register=False):
    """
    get uninited buffer

    Parameters
    ----------
    params_list: context, shape, dtype
    -            context: object of class Context
    -            shape: data shape
    -            dtype: data type
    init_mode: init mode
    init_value: init value
    is_wide_register: for int24, int48, int64 of wide register

    Returns
    ----------
    _buffer:NumpyBuffer
    """
    # 255 is max value of uint8
    # check init_mode is str
    context, shape, dtype = params_list
    TikCheckUtil.check_type_match(init_mode, str)
    if not isinstance(shape, (list, tuple)):
        shape = [shape]
    if init_mode == 'random':
        shape = [context.evaluate_expr(s) for s in shape]
        if any(not isinstance(i, int) for i in shape):
            return None
        if is_wide_register is True:
            dst_type = WDTYPE_TO_DTYPE[dtype]
            shape[-1] = shape[-1] * DTYPE_SIZE[dtype] // DTYPE_SIZE[dst_type]
            _buffer = np.zeros(shape=shape, dtype=dst_type).view('uint8')
            tmp_shape = _buffer.shape
            rand_buffer = np.random.randint(_MAX_RANDINT, size=tmp_shape,
                                            dtype='uint8').view(dst_type)
        else:
            if dtype == "bool":
                # for bool dtype, bool as 1bit, apply numpy buffer of uint8
                new_shape = ceil_div(reduce_mul(shape), BIT_LEN_8)
                rand_buffer = np.random.randint(_MAX_RANDINT, size=new_shape, dtype='uint8')
            else:
                _buffer = np.zeros(shape=shape, dtype=dtype).view('uint8')
                tmp_shape = _buffer.shape
                rand_buffer = np.random.randint(_MAX_RANDINT, size=tmp_shape,
                                                dtype='uint8').view(dtype)
        _buffer = rand_buffer
        return _buffer
    if init_mode == 'constant':
        TikCheckUtil.check_in_range_by_dtype(
            init_value, msg='init value out of range,value must be [%d, %d], but got %s'
            % (_MIN_INIT_VALUE, _MAX_INIT_VALUE, init_value), var_range=[_MIN_INIT_VALUE, _MAX_INIT_VALUE])
        shape = [context.evaluate_expr(s) for s in shape]
        if any(not isinstance(i, int) for i in shape):
            return None
        _buffer = np.zeros(shape=shape, dtype=dtype).view('uint8')
        rand_buffer = (_buffer + init_value).view(dtype)
        _buffer = rand_buffer
        return _buffer
    return TikCheckUtil.raise_error(
        'unknown init mode %s init node only can be '
        'random or constant' % init_mode)


class NumpyBuffer:
    """
    Store numpy buffer
    """

    def __init__(self, params_list, is_global=False, np_array=None, is_wide_register=False):
        self.context, shape, self.dtype, init_mode, init_value = params_list
        # numpy cannot recognize "bfloat16", use tf.bfloat16.as_numpy_dtype so that numpy can understand
        if self.dtype == 'bfloat16':
            import tensorflow as tf
            self.dtype = tf.bfloat16.as_numpy_dtype
        self._buffer = None
        self.is_wide_register = is_wide_register  # record whether is wide register, special for int64
        # if it's input tensor, needn't create buffer here, Context.eval_ will init it
        if np_array is not None:
            self.buffer = np_array
        elif is_global:
            # GM tensor, for multi core, create shared array for it
            self._buffer, self.shape = _create_shared_array(self.context, shape, self.dtype)
            self.buffer = np.frombuffer(self._buffer, self.dtype).reshape(self.shape)
        else:
            self.buffer = get_uninited_buffer((self.context, shape, self.dtype), init_mode,
                                              init_value, is_wide_register)
            self.shape = self.buffer.shape

    def __del__(self):
        if self._buffer:
            # free the RawArray buffer
            BufferWrapper._heap = Heap()  # create a new heap for BufferWrapper
            gc.collect()  # collect the old _head memory
            self._buffer = None
        self.buffer = None


class NumpyBufferProxy:
    """
    Store numpy buffer proxy
    """

    def __init__(self, context, tensor):
        self.context = context
        self.tensor = tensor
        self.last_tensor = tensor.last_tensor

    @property
    def buffer(self):
        """
        get _buffer attribute

        Returns
        ----------
        np_buffer:NumpyBuffer
        """
        np_buffer = self.context.tensor_buffer.get_npbuffer_by_tvmbuffer(
            self.tensor.buffer).buffer.reshape(-1).view(self.tensor.dtype)
        # for getitem tik 1.0 case, it need to flatten the last_tensor firstly
        if self.tensor.is_reshape and self.tensor.is_getitem:
            static_indice = self.gen_new_dimension(np_buffer)
            return np_buffer[tuple(static_indice)]
        # for reshape case
        if self.tensor.is_reshape:
            static_shape = \
                self.context.buffer2static_parameters[id(self.tensor)][
                    "static_shape"]
            return np_buffer.reshape(static_shape)
        # for getitem and reinterpret_cast_to case
        static_original_shape = \
            self.context.buffer2static_parameters[id(self.tensor)][
                "static_original_shape"]
        np_buffer = np_buffer.reshape(static_original_shape)
        static_indice = self.gen_new_dimension(np_buffer)
        return np_buffer[tuple(static_indice)]

    def gen_new_dimension(self, buffer):
        """
        gen new dimension
        Parameters
        ----------
        buffer: buffer

        Returns
        -------

        """
        static_shape = []
        static_dims = self.context.buffer2static_parameters[id(self.tensor)][
            "static_dims"]
        for i in static_dims:
            static_shape.append((i.stop - i.start + i.step - 1) // i.step)
        static_shape = static_shape[::-1]
        static_strides = self.context.buffer2static_parameters[id(self.tensor)][
                             "static_strides"][::-1]
        offset = self.context.buffer2static_parameters[id(self.tensor)][
            "static_data"]
        list.reverse(static_dims)
        original_shape = buffer.shape[::-1]
        new_dims = []
        strides_acc = 1
        for i in range(len(static_dims)):
            dims = original_shape[i]
            start_i = offset % dims
            step_i = static_strides[i] // strides_acc
            strides_acc *= dims
            stop_i = start_i + step_i * static_shape[i]
            offset = offset // dims
            new_dims.append(slice(start_i, stop_i, step_i))
        list.reverse(new_dims)
        return new_dims
