#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2024-2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
 
"""
fusion_op_utils
"""
import sys
import inspect
import json
import importlib

from tbe.common.buildcfg import build_config
from tbe.tvm.buffer_manager import get_buffer_manager
import tbe.common.register as tbe_register
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.common.context import op_context
from tbe.common.utils import log as logger
import tbe.common.context.op_info as operator_info

from reduce_classify_fusion import ReduceClassifyFusion
from common_classify_fusion import CommonClassifyFusion
from cube_classify_fusion import CubeClassifyFusion
from norm_classify_fusion import NormClassifyFusion
from op_manager import get_compute_op_func
from constant import (OpcOptions, GraphDefParam, CompileParam, OpImplType)
from opc_common import normalize_optional_impl_mode, check_and_normalize_impl_mode


def get_extra_params(extra_params):
    extra_params_dict = {}
    try:
        extra_params_dict = json.loads(extra_params)
    except Exception:  # pylint: disable=bare-except,broad-except
        pass

    if isinstance(extra_params_dict, dict):
        return extra_params_dict
    return {}


def get_check_impl_mode(op_func, op_name, impl_mode):
    impl_mode_default = inspect.signature(op_func).parameters.get(OpcOptions.IMPL_MODE, None)
    if impl_mode_default is None:
        logger.debug("op_name %s impl_mode_default is None", op_name)
        return {}
    if impl_mode_default.kind not in \
        (inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
        logger.error("op_name %s impl_mode_default.kind %s ERROR", op_name, impl_mode_default.kind)
        return {}
    return impl_mode


def get_opt_impl_mode_flag(impl_mode, impl_mode_default):
    opt_impl_mode_flag = False
    if OpImplType.OPTIONAL in impl_mode:
        impl_mode = normalize_optional_impl_mode(impl_mode)
        opt_impl_mode_flag = True
    elif impl_mode_default and (impl_mode_default.default != '') and (impl_mode == impl_mode_default.default):
        opt_impl_mode_flag = True
    return impl_mode, opt_impl_mode_flag


def get_impl_mode(impl_mode_cfg, impl_mode_attr, op_func, op_name, fusion_op_impl_mode):
    impl_mode_default = inspect.signature(op_func).parameters.get(OpcOptions.IMPL_MODE, None)
    if impl_mode_default is None:
        logger.debug("op_name %s impl_mode_default is None", op_name)
    elif impl_mode_default.kind not in \
        (inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
        logger.error("op_name %s impl_mode_default.kind %s ERROR", op_name, impl_mode_default.kind)
        return {}

    if impl_mode_attr is not None:
        impl_mode_cfg = impl_mode_attr
    if impl_mode_cfg is None:
        return {}

    valid_flag, impl_mode = check_and_normalize_impl_mode(impl_mode_cfg)
    logger.debug("op_name %s impl_mode is %s", op_name, str(impl_mode))
    if (impl_mode_default is None) and valid_flag:
        logger.info("op_name %s impl_mode_default is None", op_name)
        return {}
    impl_mode, opt_impl_mode_flag = get_opt_impl_mode_flag(impl_mode, impl_mode_default)
    fusion_op_impl_mode[op_name] = (impl_mode, opt_impl_mode_flag)
    return {OpcOptions.IMPL_MODE: impl_mode}


def set_l1_fusion_type(l1_enable, op_node):
    if l1_enable is False:
        return

    buffer_manager = get_buffer_manager()
    l1_fusion_type = op_node[GraphDefParam.OUTPUT_DESC][0].get(CompileParam.L1_FUSION_TYPE, -1)
    buffer_manager.set_l1_fusion_type(l1_fusion_type)


def check_lx_stride_info(op_list):
    def _check_desc(op_desc):
        for desc in op_desc:
            if not desc.get(CompileParam.SHAPE):
                continue

            addr_type = desc.get(CompileParam.ADDR_TYPE, 0)
            l1_addr_offset = desc.get(CompileParam.L1_ADDR_OFFSET, 0)

            if addr_type != 0 or (addr_type == 2 and l1_addr_offset != 0) or \
                any(desc.get(CompileParam.VALID_SHAPE, [])):
                return True
        return False

    for op_node in op_list:
        if op_node[CompileParam.TYPE] == "Data":
            continue
        if GraphDefParam.INPUT_DESC in op_node:
            if _check_desc(op_node[GraphDefParam.INPUT_DESC]):
                return True
        if GraphDefParam.OUTPUT_DESC in op_node:
            if _check_desc(op_node[GraphDefParam.OUTPUT_DESC]):
                return True
    return False


def add_optional_inputs(op_list):
    """optional input may not be available, need replace"""
    def _create_optional_input(input_desc, count):
        input_name = "___data_replace_input___" + str(count)
        input_desc.update({CompileParam.NAME : input_name})
        input_desc.update({CompileParam.ORI_SHAPE : "NULL"})
        input_desc.update({CompileParam.SHAPE : "NULL"})
        return {CompileParam.NAME : input_name,
                GraphDefParam.OUTPUT_DESC : [input_desc],
                CompileParam.TYPE : "Data",
                GraphDefParam.EXTERNEL_INPUT: True}

    count = 0
    for op_node in op_list:
        if op_node.get(CompileParam.TYPE) == "Data":
            continue
        for input_desc in op_node.get(GraphDefParam.INPUT_DESC, []):
            input_name = input_desc.get(CompileParam.NAME)
            if input_name is None or len(input_name) == 0:
                op_list.append(_create_optional_input(input_desc, count))
                count += 1


def call_all_registered_functions(op_list):
    """call all registered functions

    Parameters
    ----------
    op_list : all op info.
    """
    registered_functions = tbe_register.get_all_fusion_pass()
    if len(registered_functions) > 0:
        for func_info in registered_functions:
            if func_info.get_stage() == tbe_register.class_manager.InvokeStage.STAGE_BUILD:
                func = func_info.get_func()
                func(op_list)


def get_op_inputs_args(op_list, op_node):
    """
    get op inputs
    """
    inputs_args = []
    dyn_inputs = []
    dyn_idx = -1
    desc = op_node[GraphDefParam.INPUT_DESC]
    for op_input in desc:
        if op_input.get(CompileParam.SHAPE, None) is None:
            inputs_args.append(None)
        else:
            data_type = op_input.get("data_type")
            if data_type is not None:
                op_input[CompileParam.DTYPE] = data_type
                del op_input["data_type"]
            idx = op_input.get(CompileParam.DYN_INDEX)
            if idx is not None:
                if dyn_idx == -1:
                    dyn_idx = idx
                elif dyn_idx != idx:
                    logger.warn("get multiple dyn_index: %d, %d", dyn_idx, idx)
                dyn_inputs.append(op_input)
            else:
                inputs_args.append(op_input)
    if len(dyn_inputs) > 0:
        inputs_args.append(dyn_inputs)
    return inputs_args


def set_single_op_info_to_context(op_list, op_node, kernel_name, op_compute_func, json_data):
    """
    set single op_info to context
    """
    attrs, outputs, extra_params_str = None, None, None
    if CompileParam.OP_ATTRS_DESC in op_node:
        attrs = op_node[CompileParam.OP_ATTRS_DESC]
    if GraphDefParam.OUTPUT_DESC in op_node:
        outputs = op_node[GraphDefParam.OUTPUT_DESC]
    if GraphDefParam.EXTRA_PARAMS in op_node:
        extra_params_str = op_node[GraphDefParam.EXTRA_PARAMS]

    context_extra_params = {}
    context_extra_params.update(get_extra_params(extra_params_str))
    fusion_op_impl_mode = dict()
    op_impl_kwds = get_impl_mode(json_data[CompileParam.SOC_INFO].get(OpcOptions.IMPL_MODE, None),
                                 op_node.get(OpcOptions.IMPL_MODE, None), op_compute_func,
                                 str(op_node[CompileParam.NAME]), fusion_op_impl_mode)
    context_extra_params.update(op_impl_kwds)
    options_kwds = get_options_kwds(op_node, op_compute_func, False, None)
    context_extra_params.update(options_kwds)

    op_info = operator_info.OpInfo(str(op_node[CompileParam.NAME]), str(op_node[CompileParam.TYPE]))
    op_info.pattern = str(op_node.get(CompileParam.PATTERN, "Opaque"))
    op_info.inputs = get_op_inputs_args(op_list, op_node)
    op_info.outputs = outputs
    op_info.attrs = attrs
    op_info.kernel_name = kernel_name
    op_info.extra_params = context_extra_params
    op_info.precision_mode = op_impl_kwds.get(OpcOptions.IMPL_MODE, "")
    logger.debug("op_info node[%s] precision_mode: [%s].", op_node.get(CompileParam.NAME), op_info.precision_mode)
    context = op_context.get_context()
    context.add_op_info(op_info)
    return op_info


def set_op_info_to_context(op_list, kernel_name, json_data):
    """
    set opInfo to op context
    """
    for op_node in op_list:
        if op_node[CompileParam.TYPE] == "Data":
            continue

        op_compute_func = get_compute_op_func(op_node[CompileParam.TYPE])
        if op_compute_func is None:
            raise RuntimeError("Compute function of node[%s] is null." % op_node.get(CompileParam.NAME))
        set_single_op_info_to_context(op_list, op_node, kernel_name, op_compute_func, json_data)


def get_fusion_build_cfg():
    fusion_build_config = tbe_register.get_fusion_buildcfg()
    ret_fusion_build_config = {}
    if fusion_build_config:
        for op_type in fusion_build_config.keys():
            for k, v in fusion_build_config[op_type].items():
                if k in ret_fusion_build_config:
                    if ret_fusion_build_config[k] != v:
                        logger.warn("Exception: fusion_build_config of %s is not same: %s and %s",
                                    str(k), str(v), str(ret_fusion_build_config[k]))
                else:
                    ret_fusion_build_config[k] = v

    return ret_fusion_build_config


def collect_out_tensor(op_node, tensor_list, op_output_list, compute_output_tensor_list, output_tensor_cnt):
    """
    collect out tensor
    """
    for output_desc in op_node[GraphDefParam.OUTPUT_DESC]:
        if not output_desc or not isinstance(output_desc, dict) or "name" not in output_desc:
            continue
        if output_desc[CompileParam.NAME] not in tensor_list:
            output_tensor = op_output_list[output_desc.get(CompileParam.OUTPUT_INDEX)]
            output_tensor.op.attrs[CompileParam.ADDR_TYPE] = \
                output_desc.get(CompileParam.ADDR_TYPE, 0)
            tensor_list[output_desc[CompileParam.NAME]] = output_tensor
            compute_output_tensor_list.append(output_tensor)

            # record output tensor called by other tensor
            if output_tensor not in output_tensor_cnt:
                output_tensor_cnt[output_tensor] = 0

            tmp_cnt = output_tensor_cnt[output_tensor]
            output_tensor_cnt[output_tensor] = tmp_cnt + 1
        else:
            raise RuntimeError("Output tensor already exists {}".format(output_desc[CompileParam.NAME]))


def get_sub_graph_output(compute_output_tensor_list, is_used_tensor_list, output_list,
                         output_tensor_cnt, input_tensor_cnt):
    """
    After Compute, find sub-graph output
    """

    for tensor in compute_output_tensor_list:
        if tensor not in is_used_tensor_list:
            output_list.append(tensor)
            is_used_tensor_list.add(tensor)
            input_tensor_cnt[tensor] = output_tensor_cnt[tensor]
        # expose the tensor while input cnt < output cnt
        elif output_tensor_cnt[tensor] > input_tensor_cnt[tensor]:
            output_list.append(tensor)
            input_tensor_cnt[tensor] = output_tensor_cnt[tensor]


def has_dynshape(op_list):
    """
    check if dynamic shape
    """
    for node in op_list:
        if node[CompileParam.TYPE] == "Data":
            for data in node[GraphDefParam.OUTPUT_DESC]:
                if not data[CompileParam.SHAPE]:
                    continue
                if [ele for ele in data[CompileParam.SHAPE] if ele < 0]:
                    return True
    return False


OP_PATTERN_NODE_DICT = {
    Pattern.CONV2D : "cube",
    Pattern.CONV2D_BACKPROP_INPUT : "conv2d_backprop_input",
    Pattern.CONV2D_BACKPROP_FILTER : "conv2d_backprop_filter",
    Pattern.REDUCE : "reduce",
    Pattern.NORM : "norm",
    Pattern.BROADCAST : "broadcast",
    Pattern.ELEMWISE : "elewise"
}


def get_op_mode_by_pattern(op_pattern):
    return OP_PATTERN_NODE_DICT.get(op_pattern) if op_pattern in OP_PATTERN_NODE_DICT.keys() else op_pattern


OP_PATTERN_PRIORITY_DICT = {
    Pattern.CONV2D : 4,
    Pattern.CONV2D_BACKPROP_INPUT : 4,
    Pattern.CONV2D_BACKPROP_FILTER : 4,
    Pattern.MAT_MUL : 4,
    Pattern.BATCH_MATMUL : 4,
    Pattern.CONV3D : 4,
    Pattern.REDUCE : 3,
    Pattern.NORM : 3,
    Pattern.ASCEND_ANTI_QUANT : 2,
    Pattern.ASCEND_QUANT : 2,
    Pattern.BROADCAST : 1
}


def get_fusion_pattern(op_list):
    # FUSION_PATTERN: ELEWISE  ELEWISE_WITH_BROADCAST  REDUCE
    fusion_pattern = Pattern.ELEMWISE
    pattern_priority = 0
    for op_node in op_list:
        if "pattern" not in op_node:
            continue
        if op_node["pattern"] not in OP_PATTERN_PRIORITY_DICT.keys():
            continue
        priority = OP_PATTERN_PRIORITY_DICT.get(op_node["pattern"])
        if priority > pattern_priority:
            pattern_priority = priority
            fusion_pattern = op_node["pattern"]

    return fusion_pattern


CLASSIFY_TYPE_FUNC = {
        Pattern.REDUCE : ReduceClassifyFusion, Pattern.BROADCAST : CommonClassifyFusion,
        Pattern.ELEMWISE : CommonClassifyFusion, Pattern.NORM : NormClassifyFusion,
        Pattern.ASCEND_ANTI_QUANT : CommonClassifyFusion, Pattern.CONV2D : CubeClassifyFusion,
        Pattern.CONV2D_BACKPROP_FILTER : CubeClassifyFusion
    }


def get_classify_info(fusion_pattern, fusion_mode, op_list):
    global CLASSIFY_TYPE_FUNC

    def correct_d_type(op_desc_list):
        for op_desc in op_desc_list:
            data_type = op_desc.get("data_type")
            if data_type is None:
                continue
            op_desc["dtype"] = data_type

    for op in op_list:
        if "input_desc" in op:
            correct_d_type(op["input_desc"])
        if "output_desc" in op:
            correct_d_type(op["output_desc"])

    if fusion_pattern in CLASSIFY_TYPE_FUNC.keys():
        classify_func = CLASSIFY_TYPE_FUNC.get(fusion_pattern)
        return classify_func(op_list, fusion_pattern, fusion_mode)

    return CubeClassifyFusion(op_list, fusion_pattern, fusion_mode)


def check_vector_info(vector_info):
    if vector_info.ins_list is not None:
        return False, vector_info.ins_list

    return True, vector_info.ins_with_attr_list


def get_ins_by_flag(ins_list_none, ins_attrs_options):
    if ins_list_none:
        return ins_attrs_options[0]

    return ins_attrs_options


def get_real_output(sch, output_list):
    """
    get_real_output
    """
    # some schedule will modify out tensor, need update real out tensor
    try:
        if sch.cce_special["real_out_tensor"]:
            return sch.cce_special["real_out_tensor"]

        return output_list
    except Exception:  # 'pylint: disable=broad-except
        return output_list


def get_fusion_op_kernel_name(func_name, kernel_name):
    """
    get kernel_name kwds of the op
    """
    try:
        opfunc = func_name
        if inspect.signature(opfunc).parameters[OpcOptions.OpcOptions].kind in \
                (inspect.Parameter.KEYWORD_ONLY,
                 inspect.Parameter.POSITIONAL_OR_KEYWORD):
            return {OpcOptions.OpcOptions: kernel_name}
        return {}
    except Exception:  # 'pylint: disable=broad-except
        return {}


def get_attrs_from_cls_info(op_node, ins_attrs_options):
    """
    get op outputs and attrs
    """
    attrs = []
    outputs = op_node[GraphDefParam.OUTPUT_DESC] if GraphDefParam.OUTPUT_DESC in op_node else None
    if len(ins_attrs_options) < 2:
        return outputs, attrs

    op_name = op_node.get("name")
    for attr in ins_attrs_options[1]:
        if attr["name"] == op_name:
            for item in attr["val"]:
                attrs.append(item)
            break

    return outputs, attrs


def get_classify_options(op_node, ins_attrs_options):
    if len(ins_attrs_options) < 3:
        return {}
    options = ins_attrs_options[2]
    op_name = op_node.get("name")

    for option in options:
        if option["name"] == op_name:
            return {"options": option["options"]}
    return {}


def get_output_attrs_for_all_args(all_args, op_node, ins_list_none, ins_attrs_options):
    if op_node[CompileParam.TYPE] != "conv2d_data_rm":
        if ins_list_none:
            outputs, attrs = get_attrs_from_cls_info(op_node, ins_attrs_options)
        else:
            attrs = op_node[CompileParam.OP_ATTRS_DESC] if CompileParam.OP_ATTRS_DESC in op_node else None
            outputs = op_node[GraphDefParam.OUTPUT_DESC] if GraphDefParam.OUTPUT_DESC in op_node else None

        if outputs:
            all_args.extend(outputs)
        if attrs:
            all_args.extend(attrs)


def get_options_kwds(op_node, op_compute_func, ins_list_none, ins_attrs_options):
    options_kwds = {}
    if ins_list_none:
        options_kwds = get_classify_options(op_node, ins_attrs_options)
    elif 'options' in op_node:
        options_kwds = get_compute_options(op_compute_func, op_node["options"])
    return options_kwds


def get_compute_options(op_func, options):
    """
    get options kwargs
    """
    if options:
        options_arg = inspect.signature(op_func).parameters.get('options', None)
        if options_arg is not None and \
                options_arg.kind in (inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
            return {'options': options}
    return {}


def modify_duplicated_inputs_opdesc_name(dup_data_names, dup_indesc_names):
    for name, ops in dup_data_names.items():
        indesc_names = dup_indesc_names.get(name)
        if len(ops) != len(indesc_names):
            raise RuntimeError('Duplicated names not match')
        for idx, opdesc in enumerate(zip(ops, indesc_names)):
            new_name = "{}___{}".format(name, str(idx))
            opdesc[0][CompileParam.NAME] = new_name
            opdesc[0][GraphDefParam.OUTPUT_DESC][0][CompileParam.NAME] = new_name
            opdesc[1][CompileParam.NAME] = new_name


def modify_duplicated_inputs(json_data):
    """
    rename names of duplicated inputs
    """
    dup_data_names = {}
    for operator in json_data[CompileParam.OP_LIST]:
        if operator[CompileParam.TYPE] != 'Data':
            continue
        count = dup_data_names.setdefault(operator[CompileParam.NAME], [])
        count.append(operator)

    for key, value in dup_data_names.items():
        if len(value) > 1:
            dup_data_names[key] = value

    dup_indesc_names = {}
    for operator in json_data[CompileParam.OP_LIST]:
        if operator[CompileParam.TYPE] == 'Data':
            continue
        for indesc in operator[GraphDefParam.INPUT_DESC]:
            if indesc[CompileParam.NAME] in dup_data_names.keys():
                count = dup_indesc_names.setdefault(indesc[CompileParam.NAME], [])
                count.append(indesc)

    if len(dup_data_names) != len(dup_indesc_names):
        raise RuntimeError('Duplicated names not match')

    modify_duplicated_inputs_opdesc_name(dup_data_names, dup_indesc_names)