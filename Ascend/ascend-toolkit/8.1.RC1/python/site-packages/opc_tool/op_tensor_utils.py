import json
import functools


from tbe import tvm
from tbe.common.utils import shape_util
from tbe.dsl.unify_schedule.constants import Pattern
from tbe.common.context import op_context
from tbe.common.utils import log as logger
from tbe.common.utils.op_util import op_util_conv2d

from constant import (GraphDefParam, CompileParam)


def replace_attr_for_op_input_list(op_input_list, input_to_attr_list):
    if not input_to_attr_list:
        return

    for key in input_to_attr_list:
        op_input_list[key] = input_to_attr_list[key]


# 'pylint: disable=too-many-arguments
def add_input_tensor(op_node, tensor_list, op_input_list, is_used_tensor_list,
                     input_tensor_cnt, dyn_input_dict, input_desc_names):
    """
    add input tensor
    """
    for input_desc in op_node[GraphDefParam.INPUT_DESC]:
        input_desc_names.append(input_desc[CompileParam.NAME])

        tensor_info = tensor_list.get(input_desc[CompileParam.NAME], None)
        if tensor_info is not None:
            tensor_info.op.attrs[GraphDefParam.CURRENT_SHAPE] = input_desc[CompileParam.SHAPE]
        else:
            op_input_list.append(tensor_info)
            continue

        if CompileParam.DYN_INDEX in input_desc:
            if CompileParam.DYN_INDEX not in dyn_input_dict:
                dyn_input_dict[CompileParam.DYN_INDEX] = []
            dyn_input_dict[CompileParam.DYN_INDEX].append(tensor_info)
        else:
            op_input_list.append(tensor_info)

        is_used_tensor_list.add(tensor_info)
        # count input tensor called by other tensor
        if tensor_info in input_tensor_cnt:
            input_tensor_cnt[tensor_info] += 1
        else:
            input_tensor_cnt[tensor_info] = 1


def _inner_replace_cube_tvm_shapes(transdata_input_name_vec, inputs, cube_inputs, vector_inputs):
    # this situation is [conv2d + transdata]
    if len(transdata_input_name_vec) == 1:
        for input_op in inputs:
            if input_op.get(GraphDefParam.INPUT_PATTERN) == "cube":
                if input_op.get(CompileParam.FORMAT) == "NC1HWC0":
                    input_op[GraphDefParam.INPUT_PATTERN] = "cube_transdata"
                cube_inputs.append(input_op)
            else:
                vector_inputs.append(input_op)
    # [transdata + conv2d + transdata] or other situation.
    else:
        for input_op in inputs:
            if input_op.get(GraphDefParam.INPUT_PATTERN) == "cube":
                cube_inputs.append(input_op)
            elif input_op.get(CompileParam.NAME) in transdata_input_name_vec:
                input_op[GraphDefParam.INPUT_PATTERN] = "cube_transdata"
                cube_inputs.append(input_op)
            else:
                vector_inputs.append(input_op)


def replace_cube_tvm_shapes(inputs, vector_info, op_list, ins_attrs_options):
    cube_inputs = []
    vector_inputs = []

    transdata_op_type = "TransData"
    transdata_input_name_vec = []
    for op_node in op_list:
        if op_node.get(CompileParam.TYPE) == transdata_op_type:
            transdata_input_name_vec.append(op_node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME])

    _inner_replace_cube_tvm_shapes(transdata_input_name_vec, inputs, cube_inputs, vector_inputs)

    tvm_shapes_dict = {}
    cube_tvm_shapes = shape_util.variable_shape(list(cube_inputs), op_mode="cube")
    for i, input_op in enumerate(cube_inputs):
        tvm_shapes_dict[input_op[CompileParam.NAME]] = cube_tvm_shapes[i]

    if len(vector_inputs) > 0:
        if op_util_conv2d.is_conv2d_binary():
            vector_tvm_shapes = op_util_conv2d.replace_conv2d_vector_tvm_shapes(list(vector_inputs), ins_attrs_options)
        else:
            vector_tvm_shapes = shape_util.variable_shape(list(vector_inputs))
        for i, input_op in enumerate(vector_inputs):
            tvm_shapes_dict[input_op[CompileParam.NAME]] = vector_tvm_shapes[i]

    for index in vector_info.placeholder_op.idx:
        op_list[index][GraphDefParam.OUTPUT_DESC][0][CompileParam.SHAPE] = \
            tvm_shapes_dict.get(op_list[index][GraphDefParam.OUTPUT_DESC][0][CompileParam.NAME])
        if op_list[index][GraphDefParam.OUTPUT_DESC][0].get(CompileParam.NAME) in transdata_input_name_vec:
            op_list[index][GraphDefParam.OUTPUT_DESC][0][GraphDefParam.INPUT_PATTERN] = "cube_transdata"


def replace_dw_tvm_shapes(inputs, vector_info, op_list):
    """
    get input shape of dw transdata fusion
    """
    tvm_shapes_dict = {}
    dw_tvm_shapes = shape_util.variable_shape(list(inputs), op_mode="conv2d_backprop_filter")
    for i, input_member in enumerate(inputs):
        tvm_shapes_dict[input_member[CompileParam.NAME]] = dw_tvm_shapes[i]
    for index in vector_info.placeholder_op.idx:
        op_list[index][GraphDefParam.OUTPUT_DESC][0][CompileParam.SHAPE] = \
            tvm_shapes_dict.get(op_list[index][GraphDefParam.OUTPUT_DESC][0][CompileParam.NAME])
        if op_list[index][GraphDefParam.OUTPUT_DESC][0].get(GraphDefParam.INPUT_PATTERN) is None:
            op_list[index][GraphDefParam.OUTPUT_DESC][0][GraphDefParam.INPUT_PATTERN] = "dw_transdata"


def modify_input_shape_range(inputs):
    for _, input_op in enumerate(inputs):
        in_ranges = input_op.get(CompileParam.RANGE)
        if in_ranges is not None:
            tmp_ranges = []
            is_change = False
            for in_range in in_ranges:
                if len(in_range) > 1 and in_range[1] == -1:
                    tmp_range = (in_range[0], None)
                    tmp_ranges.append(tmp_range)
                    is_change = True
                else:
                    tmp_ranges.append(in_range)
            if is_change:
                input_op[CompileParam.RANGE] = tmp_ranges


def replace_tvm_shapes(fusion_pattern, fusion_mode, vector_info, op_list, ins, input_to_attr_list, ins_attrs_options):
    modify_input_shape_range(ins)
    if fusion_pattern in [Pattern.REDUCE, Pattern.BROADCAST, Pattern.ELEMWISE,
                          Pattern.ASCEND_ANTI_QUANT, Pattern.ASCEND_QUANT]:
        if fusion_pattern in [Pattern.REDUCE, ]:
            if vector_info.label in ["D", ]:
                tvm_shapes = shape_util.variable_shape(list(ins), op_mode=fusion_mode)[0: len(ins) - 1]
                # update reduce func
                for index in vector_info.reduce_op.idx:
                    op_list[index][CompileParam.ATTRS][0] = ins[-1].get(CompileParam.VALUE)
            else:
                if vector_info.axis_idx:
                    # axis is one of placeholders
                    axis_attr = ins[vector_info.axis_idx].get(CompileParam.VALUE)
                    input_to_attr_list[vector_info.axis_idx] = axis_attr
                    tvm_shapes = shape_util.variable_shape(list(ins), op_mode=fusion_mode)
                else:
                    # axis not belong to placeholder
                    axis_attr = ins[-1].get(CompileParam.VALUE)
                    input_to_attr_list[-1] = axis_attr
                    tvm_shapes = shape_util.variable_shape(list(ins), op_mode=fusion_mode)[0: len(ins) - 1]
        elif fusion_mode == "broadcast":
            # variable_shape does not support broadcast, using the default value which is elewise
            tvm_shapes = shape_util.variable_shape(list(ins))
        else:
            tvm_shapes = shape_util.variable_shape(list(ins), fusion_mode)

        # update placeholder
        idx = 0
        for index in vector_info.placeholder_op.idx:
            op_list[index][GraphDefParam.OUTPUT_DESC][0][CompileParam.SHAPE] = tvm_shapes[idx]
            idx += 1
    elif fusion_pattern == Pattern.CONV2D_BACKPROP_FILTER:
        replace_dw_tvm_shapes(ins, vector_info, op_list)
    else:
        replace_cube_tvm_shapes(ins, vector_info, op_list, ins_attrs_options)


def compress_node(op_node, op_list):
    """
    specific case for conv2d_compress placeholder
    :param op_node:
    :param op_list:
    """
    for op_operator in op_list:
        if op_operator[CompileParam.TYPE] != "Conv2DCompress" and \
                op_operator[CompileParam.TYPE] != "FullyConnectionCompress" and \
                op_operator[CompileParam.TYPE] != "MatMulV2Compress":
            continue

        compress_index_input = op_operator[GraphDefParam.INPUT_DESC]
        try:
            if op_node[CompileParam.NAME] != compress_index_input[2][CompileParam.NAME]:
                continue
        except Exception:  # 'pylint: disable=broad-except
            logger.error("Caught exception from the option_node: %s", op_node[CompileParam.NAME])
            continue

        compress_index_shape = tvm.var("compress_index_shape", dtype="int32")
        compress_index = tvm.placeholder((compress_index_shape,), dtype="int8", name='compress_index')
        return compress_index

    return None


def _fresh_shape_and_attr(desc, attr):
    """
    when dw transdata fusion node, fuse the h and w dim of trans_data_input
    """
    if desc.get(GraphDefParam.INPUT_PATTERN) == "dw_transdata":
        attr[CompileParam.SHAPE] = desc[CompileParam.SHAPE]
        desc[CompileParam.SHAPE] = [desc[CompileParam.SHAPE][0], \
            desc[CompileParam.SHAPE][1], desc[CompileParam.SHAPE][2] * desc[CompileParam.SHAPE][3]]
    return desc, attr


def aipp_format_change(op_node, op_list):
    """
    specific case for conv2d_compress placeholder
    :param op_node:
    :param op_list:
    """
    for op_operator in op_list:
        if op_operator[CompileParam.TYPE] != "Aipp":
            continue

        aipp_input = op_operator[GraphDefParam.INPUT_DESC]
        if op_node[CompileParam.NAME] != aipp_input[0][CompileParam.NAME]:
            continue

        desc = op_node[GraphDefParam.OUTPUT_DESC][0]

        for op_f, op_s in ((CompileParam.FORMAT, CompileParam.SHAPE),
                           (CompileParam.ORI_FORMAT, CompileParam.ORI_SHAPE)):
            if desc.get(op_f) == "NHWC":
                desc[op_f] = "NCHW"
                desc[op_s] = [desc[op_s][0], desc[op_s][3],
                              desc[op_s][1], desc[op_s][2]]


def create_placeholder_tensor(op_node, tensor_list, input_list, op_list, params_count, is_unknown_shape=False):
    desc = op_node[GraphDefParam.OUTPUT_DESC][0]
    aipp_format_change(op_node, op_list)
    if desc[CompileParam.SHAPE] == "NULL" or desc[CompileParam.SHAPE] == []:
        tensor_list[desc[CompileParam.NAME]] = None
    else:
        sformat = desc.get(CompileParam.FORMAT, "")
        sub_format = desc.get(CompileParam.SUB_FORMAT, "")
        ori_shape = desc.get(CompileParam.ORI_SHAPE, [])
        ori_format = desc.get(CompileParam.ORI_FORMAT, "")
        addr_type = desc.get(CompileParam.ADDR_TYPE, 0)
        valid_shape = desc.get(CompileParam.VALID_SHAPE, [])
        slice_offset = desc.get(CompileParam.SLICE_OFFSET, [])
        l1_fusion_type = desc.get(CompileParam.L1_FUSION_TYPE, -1)
        l1_addr_flag = desc.get(CompileParam.L1_ADDR_FLAG, -1)
        l1_addr_offset = desc.get(CompileParam.L1_ADDR_OFFSET, -1)
        l1_valid_size = desc.get(CompileParam.L1_VALID_SIZE, -1)
        range_value = desc.get(CompileParam.RANGE, [])
        const_value = desc.get(CompileParam.CONST_VALUE, [])
        for idx, range_val in enumerate(range_value):
            if isinstance(range_val, tuple):
                range_value[idx] = list(range_val)
            if len(range_val) == 2 and range_val[1] is None:
                range_value[idx][1] = -1

        para_name = "params_%s" % str(params_count[0])

        if not is_unknown_shape:
            trans_shape(desc, op_list)
            trans_shape_by_pattern(desc, op_list)

        out = compress_node(op_node, op_list)

        if out is not None:
            tensor_list[desc[CompileParam.NAME]] = out
        else:
            attr = {
                CompileParam.FORMAT: sformat,
                CompileParam.SUB_FORMAT: sub_format,
                CompileParam.ORI_SHAPE: ori_shape,
                CompileParam.ORI_FORMAT: ori_format,
                CompileParam.ADDR_TYPE: addr_type,
                CompileParam.VALID_SHAPE: valid_shape,
                CompileParam.SLICE_OFFSET: slice_offset,
                CompileParam.L1_FUSION_TYPE: l1_fusion_type,
                CompileParam.L1_ADDR_FLAG: l1_addr_flag,
                CompileParam.L1_ADDR_OFFSET: l1_addr_offset,
                CompileParam.L1_VALID_SIZE: l1_valid_size,
                CompileParam.RANGE: range_value,
                CompileParam.CONST_VALUE: const_value}
            desc, attr = _fresh_shape_and_attr(desc, attr)
            # 'pylint: disable=unexpected-keyword-arg
            tensor_list[desc[CompileParam.NAME]] = \
                tvm.placeholder(desc[CompileParam.SHAPE], dtype=desc[CompileParam.DTYPE],
                                name=para_name, attrs=attr)

        params_count[0] = params_count[0] + 1

    input_list.append(tensor_list[desc[CompileParam.NAME]])
    
    
    
def trans_shape_fullycompress(data_node, node):
    """
    tansform fullyconnectioncompress op shape, input tensor order is
    x, w, compress_index, b, and so on
    :param data_node:
    :param node:
    """
    # trans x shape
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[0], shape[1] *
                                             shape[2] * shape[3] * shape[4]]

    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][3][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[1] * shape[4]]


def trans_shape(data_node, op_list):
    """
    tansform op shape if necessary
    :param data_node:
    :param op_list:
    """
    tans_node_func = {"FullyConnection": trans_fully_connection,
                        "FullyConnectionCompress": trans_shape_fullycompress,
                        "DepthwiseConv2D": trans_depthwise_conv2d,
                        "MatMulV2": trans_matmul_bias_shape,
                        "MatMulV2Compress": trans_matmulcompress_bias_shape,
                        "BNInferenceD": trans_bninference_shape,
                        "AscendRequantS16": trans_arequant_s16,
                        "BatchMatMul": trans_batch_matmul_shape,
                        "BatchMatMulV2": trans_batch_matmul_shape,
                        "Conv3D": trans_conv3d_shape
                        }
    all_elemwise_flag = True
    has_elemwise_flag = False
    nodes_count_without_data = 0
    for node in op_list:
        pattern = node.get(CompileParam.PATTERN)
        if pattern is not None and pattern != 'ElemWise' and pattern != 'quant':
            all_elemwise_flag = False
        if pattern is not None and pattern == 'ElemWise':
            has_elemwise_flag = True
        if node[CompileParam.TYPE] != "Data":
            nodes_count_without_data = nodes_count_without_data + 1

    # While elemwise and quant do fusion, the shape of quant must be transfer if format is NZ or 5HD
    # If quant is fused with cube node, the output shape of conv has been transfered,
    # so we only deal the scene when elemwise and quant do fusion
    if has_elemwise_flag and nodes_count_without_data == 2:
        tans_node_func["AscendQuant"] = trans_ascend_quant_shape
    if all_elemwise_flag:
        tans_node_func["BiasAdd"] = trans_biasadd_shape
    for node in op_list:
        if node[CompileParam.TYPE] in tans_node_func.keys():
            trans_foo = tans_node_func.get(node[CompileParam.TYPE])
            trans_foo(data_node, node)
        else:
            continue


def trans_shape_by_pattern(data_node, op_list):
    """trans shape"""
    try:
        for node in op_list:
            if node.get(CompileParam.PATTERN) == 'ElemWise':
                trans_elemwise_shape(data_node, node, op_list)
    except Exception:  # 'pylint: disable=broad-except
        pass


def trans_elemwise_shape(data_node, node, op_list):
    """
    broadcast elemwise input shape if necessary
    """

    for operator in op_list:
        if operator.get(CompileParam.PATTERN) in ('Convolution', 'Conv3D',
                                                  'Conv2d_backprop_input'):
            # no need to broadcast in Conv+elemwise fusion
            return

    data_node_name = data_node[CompileParam.NAME]
    max_input_dim = 0

    input_names = [x[CompileParam.NAME] for x in node[GraphDefParam.INPUT_DESC]]
    if data_node_name not in input_names:
        return

    input_shapes = [x[CompileParam.SHAPE] for x in node[GraphDefParam.INPUT_DESC]]
    for shape in input_shapes:
        max_input_dim = max(len(shape), max_input_dim)
    data_dim = len(data_node[CompileParam.SHAPE])

    if data_dim <= 1:
        data_node[CompileParam.SHAPE] = [1] * (max_input_dim - data_dim) + data_node[CompileParam.SHAPE]


def get_shape_bias(input_x_format, shape_x, shape_bias, data_format):
    if input_x_format is not None and input_x_format.upper() == "NC1HWC0":
        shape_bias = (1, shape_x[1], 1, 1, shape_x[4])
    elif input_x_format is not None and input_x_format.upper() == "NDHWC":
        shape_bias = (1, 1, 1, 1, shape_x[4])
    elif input_x_format is not None and input_x_format.upper() == "NCDHW":
        shape_bias = (1, shape_x[1], 1, 1, 1)
    elif input_x_format is not None and input_x_format.upper() == "NDC1HWC0":
        shape_bias = (1, 1, shape_x[2], 1, 1, shape_x[5])
    elif data_format == "NCHW":
        shape_bias = (1, shape_x[1],)
        for _ in range(2, len(shape_x)):
            shape_bias = shape_bias + (1,)
    else:
        shape_bias = ()
        for _ in range(0, len(shape_x) - 1):  # 'pylint: disable=unused-variable
            shape_bias = shape_bias + (1,)
        shape_bias = shape_bias + (shape_x[-1],)
    return shape_bias


def trans_biasadd_shape(data_node, node):
    """
    transform shape for biasadd
    """
    if data_node[CompileParam.NAME] != node[GraphDefParam.INPUT_DESC][1][CompileParam.NAME]:
        return
    input_x = node[GraphDefParam.INPUT_DESC][0]
    bias = node[GraphDefParam.INPUT_DESC][1]
    shape_x = input_x.get(CompileParam.SHAPE)
    shape_bias = bias.get(CompileParam.SHAPE)
    data_format = node[CompileParam.ATTRS][0]
    data_format = data_format.upper()
    shape_bias = get_shape_bias(input_x.get(CompileParam.FORMAT), shape_x, shape_bias, data_format)
    data_node[CompileParam.SHAPE] = shape_bias


def trans_conv3d_shape(data_node, node):
    """
    tranform shape for conv3d
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][1][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]

        if len(shape) == 4:
            data_node[CompileParam.SHAPE] = [shape[0],
                                             shape[1],
                                             shape[2],
                                             shape[3]]


def trans_depthwise_conv2d(data_node, node):
    """
    tranform shape for depthwise_conv2d
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        total_shape = data_node[CompileParam.TOTAL_SHAPE]
        valid_shape = data_node[CompileParam.VALID_SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[0], shape[1],
                                             shape[2], shape[3], shape[4]]
            data_node[CompileParam.TOTAL_SHAPE] = [total_shape[0], total_shape[1], 1,
                                                   total_shape[2], total_shape[3], total_shape[4]]
            if not any(valid_shape):
                data_node[CompileParam.VALID_SHAPE] = [valid_shape[0], valid_shape[1], 1,
                                                       valid_shape[2], valid_shape[3], valid_shape[4]]
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][1][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        total_shape = data_node[CompileParam.TOTAL_SHAPE]
        valid_shape = data_node[CompileParam.VALID_SHAPE]
        if len(shape) == 6:
            data_node[CompileParam.SHAPE] = [shape[0], shape[1], shape[2],
                                             shape[4], shape[5]]
            data_node[CompileParam.TOTAL_SHAPE] = [total_shape[0], total_shape[1],
                                                   total_shape[2], total_shape[4], total_shape[5]]
            if not any(valid_shape):
                data_node[CompileParam.VALID_SHAPE] = [valid_shape[0], valid_shape[1],
                                                       valid_shape[2], valid_shape[4], valid_shape[5]]


def trans_arequant_s16(data_node, node):
    """
    transform AscendRequantS16 shape
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[0], shape[1],
                                  shape[2] * shape[3], shape[4]]
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][2][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[0], shape[1],
                                  shape[2] * shape[3], shape[4]]


def trans_ascend_quant_shape(data_node, node):
    """
    transform AscendQuant shape
    """
    input_format = data_node[CompileParam.FORMAT]
    input_shape = data_node[CompileParam.SHAPE]
    if input_format == "FRACTAL_NZ" and len(input_shape) >= 4:
        batch = 1
        if len(input_shape) > 4:
            batch = functools.reduce(lambda x, y: x * y, input_shape[:-4])
        input_shape = [batch, input_shape[-4], input_shape[-3] * input_shape[-2], input_shape[-1]]
        data_node[CompileParam.SHAPE] = input_shape


def trans_batch_matmul_shape(data_node, node):
    """
    tansform batch_matmul op shape if necessary
    :param data_node:
    :param node:
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) > 5:
            data_node[CompileParam.SHAPE] = [functools.reduce(
                lambda x, y: x * y, shape[:-4])] + shape[-4:]
        return
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][1][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) > 5:
            data_node[CompileParam.SHAPE] = [functools.reduce(
                lambda x, y: x * y, shape[:-4])] + shape[-4:]
        return
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][2][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) > 5:
            data_node[CompileParam.SHAPE] = [functools.reduce(
                lambda x, y: x * y, shape[:-4])] + shape[-4:]
        return


def trans_mul_shape(data_node, node, op_list):
    """
    tansform mul shape
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][1][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        for node_list in op_list:
            if node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME] == \
                    node_list[GraphDefParam.OUTPUT_DESC][0][CompileParam.NAME] and \
                    (node_list[CompileParam.TYPE] == "AvgPool" or
                     node_list[CompileParam.TYPE] == "AscendDequant" or
                     node_list[CompileParam.TYPE] == "Pooling"):
                shape = data_node[CompileParam.SHAPE]
                data_node[CompileParam.SHAPE] = [shape[0], shape[1], 1,
                                                 shape[2] * shape[3], shape[4]]

    # broadcast mul input shape if necessary
    if data_node[CompileParam.SHAPE] != [1, 1, 1] and data_node[CompileParam.SHAPE] != [1, 1]:
        return

    data_node_name = data_node[CompileParam.NAME]
    max_input_dim = 0

    input_names = [x[CompileParam.NAME] for x in node[GraphDefParam.INPUT_DESC]]
    if data_node_name not in input_names:
        return

    input_shapes = [x[CompileParam.SHAPE] for x in node[GraphDefParam.INPUT_DESC]]
    for shape in input_shapes:
        max_input_dim = max(len(shape), max_input_dim)

    data_node[CompileParam.SHAPE] = [1] * max(1, max_input_dim)


def trans_fully_connection(data_node, node):
    """trans fully connection"""
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][0][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[0], shape[1] *
                                  shape[2] * shape[3] * shape[4]]

    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][2][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 5:
            data_node[CompileParam.SHAPE] = [shape[1] * shape[4]]


def trans_bninference_shape(data_node, node):
    """trans bn inference shape"""
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][1][CompileParam.NAME] \
            or data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][2][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if data_node[CompileParam.FORMAT] == "NHWC":
            shape = [1, 1, 1] + shape
        elif data_node[CompileParam.FORMAT] == "NCHW":
            shape = [1] + shape + [1, 1]
        data_node[CompileParam.SHAPE] = shape


def trans_matmul_bias_shape(data_node, node):
    """
    tansform matmul bias op shape if necessary
    :param data_node:
    :param node:
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][2][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) > 0:
            data_node[CompileParam.SHAPE] = _reshape_bias_shape(shape)
            return


def trans_matmulcompress_bias_shape(data_node, node):
    """
    tansform matmulcompress bias op shape if necessary. input tensor order is
    x, w, compress_index, b, and so on
    :param data_node:
    :param node:
    """
    if data_node[CompileParam.NAME] == node[GraphDefParam.INPUT_DESC][3][CompileParam.NAME]:
        shape = data_node[CompileParam.SHAPE]
        if len(shape) == 1:
            data_node[CompileParam.SHAPE] = _reshape_bias_shape(shape)
            return


def _reshape_bias_shape(shape_bias):
    """
    tansform matmul bias op shape
    :param shape_bias:
    """
    bias_length = 1
    for i in shape_bias:
        bias_length *= i
    return [(bias_length + 15) // 16 * 16]