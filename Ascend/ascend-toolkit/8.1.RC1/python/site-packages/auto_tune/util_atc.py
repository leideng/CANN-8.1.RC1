#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.

This program is free software; you can redistribute it and/or modify
it under the terms of the Apache License Version 2.0.You may not use
this file except in compliance with the License.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
Apache License for more details at
http://www.apache.org/licenses/LICENSE-2.0

Define the class for calculate the input/output size and dtype
"""
import os
import stat
import json
import shutil
from collections import namedtuple
from typing import List
from typing import TextIO

from tbe.dsl.static_schedule.conv_schedule import check_conv_bn1
from tbe.tvm.buffer_manager import get_buffer_manager

from auto_tune.common_module.common_util import get_soc_info
from auto_tune.auto_tune_log import LOG_INSTANCE

FILE_FLAG = os.O_WRONLY | os.O_CREAT
FILE_MODE_640 = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP
DIR_MODE_750 = 0o750
OPP_PATH = "ASCEND_OPP_PATH"


BLOCK_SIZE = {'uint1': 1 / 8, 'int4': 0.5, 'int8': 1, 'float16': 2, 'uint8': 1, 'float32': 4,
              'int32': 4, 'bool': 1, 'uint64': 8, 'int16': 2, 'bfloat16': 2}
C0_FOR_BN = 16

INVALID_COST_TIME = -1

# define L1 L2 memory type
DDR_MEMORY_TYPE = 0
L1_MEMORY_TYPE = 1
L2_MEMORY_TYPE = 2

RepoNameParams = namedtuple("RepoNameParams", ["src_file", "soc_version", "aicore_num", "l1_size",
                              "l2_fusion", "l1_fusion"], defaults=[False, False])

# version
VERSION_DICT = {
    "Ascend310": 1,
    "Ascend910": 2,
    "Hi3796CV300ES": 3,
    "Ascend310P": 4,
    "Ascend610": 5,
    "Hi3796CV300CS": 6,
    "SD3403": 7,
    "BS9SX1A": 9,
    "Ascend910B": 13,
    "Ascend910_93": 13,
    "Ascend310B": 15
}

# Adapted to factory mode, corresponding class name is required
OP_TAG_DICT = {
    "AvgPool": "depthwise_conv2d_forward",
    "Deconvolution": "conv2d_backprop_input",
    "Conv2D": "conv2d",
    "Conv2DCompress": "conv2d",
    "Conv2DBackpropInputD": "conv2d_backprop_input",
    "Conv2DBackpropFilterD": "conv2d_backprop_filter",
    "GEMM": "matmul",
    "MatMul": "matmul",
    "MatMulV2": "matmul",
    "MatMulV2Compress": "matmul",
    "BatchMatMul": "matmul",
    "BatchMatMulV2": "matmul",
    "FullyConnection": "matmul",
    "FullyConnectionCompress": "matmul",
    "DepthwiseConv2D": "depthwise_conv2d_forward",
    "DepthwiseConv2DBackpropInputD": "depthwise_bp_input",
    "DepthwiseConv2DBackpropFilterD": "depthwise_bp_filter",
    "Conv3D": "conv3d",
    "Conv3DBackpropInputD": "conv3d_backprop_input",
    "Conv3DBackpropFilterD": "conv3d_backprop_filter",
    "AvgPool3DGradD": "conv3d_backprop_input",
    "Pooling": "conv2d",
    "Conv2DTransposeD": "conv2d_backprop_input",
    "Conv3DTransposeD": "conv3d_backprop_input",
    "Conv2DTransposeDCompress": "conv2d_backprop_input"
}


def enable_auto_tune_support() -> List[str]:
    """
    return the op_list that support enable_auto_tune

    Parameters
    ----------

    Returns
    -------
    """
    return list(OP_TAG_DICT.keys())


def get_output_size_and_dtype(res_tensor_list: list) -> tuple:
    """
    calculate the output tensor size and data_type
    :param res_tensor_list: the list of result tensor
    :return: size list and data_type list
    """
    out_size_list = []
    out_dtype_list = []
    buffer_manager = get_buffer_manager()
    lx_flag = bool(buffer_manager.get_tensor_list())
    for tensor in res_tensor_list:
        compute_shape = buffer_manager.get_tensor_info(tensor).get_buffer_shape() if lx_flag else tensor.shape
        res_dtype = tensor.dtype
        block_size = BLOCK_SIZE[str(res_dtype)]
        total_size = block_size
        for shape in compute_shape:
            total_size *= int(shape)
        out_size_list.append(total_size)
        out_dtype_list.append(res_dtype)
    conv_bn1_flag = check_conv_bn1(res_tensor_list)
    if conv_bn1_flag:
        out_size_list[1] = C0_FOR_BN * out_size_list[1]
        out_size_list[2] = C0_FOR_BN * out_size_list[2]
    swrite_stride = 0
    if res_tensor_list[-1].op.tag == "strided_write":
        swrite_stride = res_tensor_list[-1].op.attrs["stride"].value
    elif res_tensor_list[-1].op.tag == "conv2d_data_rm" \
        and res_tensor_list[-1].op.input_tensors[0].op.tag == "strided_write":
        swrite_stride = res_tensor_list[-1].op.input_tensors[0].op.attrs["stride"].value
    if swrite_stride:
        c_1 = res_tensor_list[-1].shape[1]
        out_size_list[0] = (swrite_stride + c_1 - 1) // c_1 * out_size_list[0]
    return out_size_list, out_dtype_list


def get_input_size_dtype_file(fusion_op_info: dict) -> tuple:
    """
    calculate the input size and dtype
    :param fusion_op_info:the fusion op info from te_fusion
    :return:size and dtype
    """

    # get params from json_data
    op_list = fusion_op_info.get("input_shape_list")
    input_size_list = []
    input_type_list = []
    input_file_list = []
    for var, op_node in enumerate(op_list):
        input_shape = op_node
        if (fusion_op_info.get("strided_read_input") is not None) and (var == 0):
            input_shape = fusion_op_info.get("strided_read_input")
        d_type = fusion_op_info.get("input_dtype_list")[var]
        block_size = BLOCK_SIZE[d_type]
        total_size = block_size
        for single_shape in input_shape:
            total_size *= single_shape
        input_size_list.append(total_size)
        input_type_list.append(d_type)
        input_file_list.append("".join([d_type, "@input.data"]))
    return input_size_list, input_type_list, input_file_list


def get_output_size_dtype(fusion_op_info: dict) -> tuple:
    """
    calculate the output size and dtype
    :param fusion_op_info:the fusion op info from te_fusion
    :return:size and dtype
    """

    # get params from json_data
    op_list = fusion_op_info.get("output_shape_list")
    output_size_list = []
    output_type_list = []
    for var, op_node in enumerate(op_list):
        input_shape = op_node
        d_type = fusion_op_info.get("output_dtype_list")[var]
        total_size = BLOCK_SIZE[d_type]
        for single_shape in input_shape:
            total_size *= single_shape
        output_size_list.append(total_size)
        output_type_list.append(d_type)
    return output_size_list, output_type_list


def create_dir(dir_path: str) -> bool:
    """
    :param dir_path:
    :return: bool
    """
    if os.path.exists(dir_path):
        return True
    try:
        os.makedirs(dir_path, DIR_MODE_750, exist_ok=True)
    except OSError as exception:
        LOG_INSTANCE.error('An error happened while creating {}, error: {}'.format(dir_path, str(exception)))
        return False
    finally:
        pass
    return True


def create_dir_steply(dir_path: str) -> bool:
    """
    :param dir_path:
    :return: bool
    """
    if os.path.exists(dir_path):
        return True
    path_lists = dir_path.split(os.sep)
    paths = os.sep
    for path_item in path_lists:
        paths = os.path.join(paths, path_item)
        if not path_item or os.path.exists(paths):
            continue
        try:
            os.makedirs(paths, DIR_MODE_750, exist_ok=True)
        except OSError as exception:
            LOG_INSTANCE.error('An error happened while creating {}, error: {}'.format(dir_path, str(exception)))
            return False
        finally:
            pass
    return True


def get_dict_from_json_file(file_handle: TextIO) -> dict:
    """
    :param dir_path:
    :return dict:
    """
    try:
        ret_dict = json.load(file_handle)
    except ValueError:
        LOG_INSTANCE.warning('Failed to load JSON files. An empty dict is returned.')
        ret_dict = {}
    finally:
        pass
    if not isinstance(ret_dict, dict):
        ret_dict = {}
    return ret_dict


def remove_files(file_to_remove: List[str]) -> None:
    """
    remove files
    """
    for r_file in file_to_remove:
        if os.path.isfile(r_file):
            os.remove(r_file)


def remove_file(file_path: str) -> None:
    """
    remove file
    """
    if os.path.isfile(file_path):
        try:
            os.remove(file_path)
        except (FileNotFoundError, IOError):
            LOG_INSTANCE.warning("Failed to remove file: %s ", file_path)
        finally:
            pass


def create_file(file_name: str) -> None:
    '''
    create file
    :param file_name: file_path
    :return:
    '''
    if not os.path.exists(file_name):
        try:
            with os.fdopen(os.open(file_name, FILE_FLAG, FILE_MODE_640), 'w'):
                pass
        except (IOError, ValueError):
            raise RuntimeError("Failed to open %s" % file_name)
        finally:
            pass


def copy_repo_files(src_dir: str, dst_dir: str) -> None:
    """
    copy repo files from src_dir to dst_dir
    """
    full_soc_version = get_soc_info().full_soc_version
    lower_soc_version = full_soc_version[:1].lower() + full_soc_version[1:]
    for repo_file in os.listdir(src_dir):
        src_repo_file = os.path.join(src_dir, repo_file)
        if not os.path.isfile(src_repo_file):
            continue
        if full_soc_version in repo_file or lower_soc_version in repo_file:
            shutil.copy(src_repo_file, dst_dir)
            dst_repo_file = os.path.join(dst_dir, repo_file)
            os.chmod(dst_repo_file, FILE_MODE_640)
