#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.

Define the function for update repository
"""

import json
import fcntl
import copy
import os
from pathlib import Path
from typing import List
from typing import TextIO

from auto_tune.auto_tune_log import LOG_INSTANCE
from auto_tune.generate_repository import transfer_json_to_bin_by_handle
from auto_tune.generate_repository import get_bin_from_shape
from auto_tune.generate_repository import get_bin_from_repository_case
from auto_tune.common_module.common_util import GA_DIR_NAME
from auto_tune.util_atc import FILE_FLAG
from auto_tune.util_atc import FILE_MODE_640
from auto_tune.util_atc import create_dir
from auto_tune.util_atc import OPP_PATH
from auto_tune.util_atc import remove_file


SOC_VERSION_TO_SHORT_LIST = {
    "Ascend910A": "ascend910",
    "Ascend910B": "ascend910",
    "Ascend910PremiumA": "ascend910",
    "Ascend910ProA": "ascend910",
    "Ascend910ProB": "ascend910",
    "Ascend310": "ascend310",
    "Ascend910B1": "Ascend910B",
    "Ascend910B2": "Ascend910B",
    "Ascend910B2C": "Ascend910B",
    "Ascend910B3": "Ascend910B",
    "Ascend910B4": "Ascend910B",
    "Ascend910B4-1": "Ascend910B",
    "Ascend910_9391": "Ascend910_93",
    "Ascend910_9381": "Ascend910_93",
    "Ascend910_9392": "Ascend910_93",
    "Ascend910_9382": "Ascend910_93",
    "Ascend910_9372": "Ascend910_93",
    "Ascend910_9362": "Ascend910_93",
    "Ascend310B1": "Ascend310B",
    "Ascend310B2": "Ascend310B",
    "Ascend310B3": "Ascend310B",
    "Ascend310B4": "Ascend310B",
    "AS31XM1X": "AS31XM1",
    "Ascend610": "ascend610",
    "Ascend610Lite": "Ascend610Lite",
    "BS9SX1AA": "bs9sx1a",
    "BS9SX1AB": "bs9sx1a",
    "BS9SX1AC": "bs9sx1a",
    "BS9SX2AA": "BS9SX2A",
    "BS9SX2AB": "BS9SX2A",
    "MC61AM21AA": "MC61AM21A",
    "MC61AM21AB": "MC61AM21A",
    "Ascend310P3": "Ascend310P",
    "Ascend310P1": "Ascend310P",
    "Ascend310P5": "Ascend310P",
    "Ascend310P7": "Ascend310P",
    "Hi3796CV300ES": "Hi3796CV300ES",
    "Hi3796CV300CS": "Hi3796CV300CS",
    "SD3403": "SD3403",
}

BIN_FILE_HEAD_LENGTH = 64
LINE_CNT = 300000
ASCEND_NORMAL_LEN = 9

# file path
THIS_FILE_NAME = __file__
FILE_PATH = os.path.dirname(os.path.realpath(THIS_FILE_NAME))


def build_id_old(input_shape: dict) -> str:
    """
    build old Unique ID for each case
    input_shape: shape dict for each case

    return: ID
    """
    a_shape = input_shape.get("A_shape")
    b_shape = input_shape.get("B_shape")
    c_shape = input_shape.get("C_shape")
    a_dtype = input_shape.get("A_dtype")
    b_dtype = input_shape.get("B_dtype")
    c_dtype = input_shape.get("C_dtype")
    mad_dtype = input_shape.get("mad_dtype")
    padl = input_shape.get("padl")
    padr = input_shape.get("padr")
    padu = input_shape.get("padu")
    padd = input_shape.get("padd")
    strideh = input_shape.get("strideH")
    stridew = input_shape.get("strideW")
    strideh_expand = input_shape.get("strideH_expand")
    stridew_expand = input_shape.get("strideW_expand")
    dilationh = input_shape.get("dilationH")
    dilationw = input_shape.get("dilationW")
    group = input_shape.get("group")
    fused_double_operand_num = input_shape.get("fused_double_operand_num")
    quantize_config = input_shape.get("quantizeConfig", "")
    scale_sqrt = input_shape.get("scaleSqrt", "")
    bias_flag = 1 if input_shape.get("bias_flag") else 0
    op_type = input_shape.get("op_type")
    platform = input_shape.get("platform")

    case_id = str(a_shape) + str(b_shape) + str(c_shape) + \
              str(a_dtype) + str(b_dtype) + str(c_dtype) + str(mad_dtype) + \
              str(padl) + str(padr) + str(padu) + str(padd) + \
              str(strideh) + str(stridew) + \
              str(strideh_expand) + str(stridew_expand) + \
              str(dilationh) + str(dilationw) + \
              str(group) + str(fused_double_operand_num) + \
              str(quantize_config) + str(scale_sqrt) + \
              str(bias_flag) + str(op_type) + str(platform)
    return case_id


def build_id_new(input_shape: dict) -> str:
    """
    build new Unique ID for each case
    input_shape: shape dict for each case

    return: ID
    """
    a_shape = input_shape.get("a_shape")
    b_shape = input_shape.get("b_shape")
    c_shape = input_shape.get("c_shape")
    a_dtype = input_shape.get("a_dtype")
    b_dtype = input_shape.get("b_dtype")
    c_dtype = input_shape.get("c_dtype")
    mad_dtype = input_shape.get("mad_dtype")
    pad = input_shape.get("pad")
    stride = input_shape.get("stride")
    dilation = input_shape.get("dilation")
    group = input_shape.get("group")
    bias_flag = 1 if input_shape.get("bias_flag") else 0
    fused_coefficient = input_shape.get("fused_coefficient")
    fused_channel_wise = input_shape.get("fused_channel_wise")
    in_fm_memory_type = input_shape.get("in_fm_memory_type", [0])
    out_fm_memory_type = input_shape.get("out_fm_memory_type", [0])
    l1_fusion_type = input_shape.get("l1_fusion_type")
    fusion_type = input_shape.get("fusion_type")

    case_id = str(a_shape) + str(b_shape) + str(c_shape) + \
              str(a_dtype) + str(b_dtype) + str(c_dtype) + str(mad_dtype) + \
              str(pad) + str(stride) + str(dilation) + str(group) + \
              str(bias_flag) + str(fused_coefficient) + \
              str(fused_channel_wise) + str(in_fm_memory_type) + \
              str(out_fm_memory_type) + str(l1_fusion_type) + \
              str(fusion_type)
    return case_id


def build_id(input_shape: dict) -> str:
    """
    build Unique ID for each case
    input_shape: shape dict for each case

    return: ID
    """
    op_list = ["conv2d", "convolution", "convolution_3d",
               "conv3d_backprop_input", "conv3d_backprop_filter"]
    if input_shape['op_type'] in op_list:
        case_id = build_id_new(input_shape)
    else:
        case_id = build_id_old(input_shape)
    return case_id


def update_single_case(old_name_set: List[str], old_case_set: List[dict],
                       update_name_set: List[str], update_case_set: List[dict]) -> tuple:
    """
    update single case from old_case_set to new_case_set
    old_name_set: name for each case in old set
    old_case_set: old case set
    update_name_set: name for each case in new set
    update_case_set: update case set

    return: new_name_set, new_case_set
    """
    new_name_set = []
    new_case_set = []
    for old_case in old_case_set:
        new_case = copy.deepcopy(old_case)
        old_name = str(get_bin_from_shape(old_case['shape']))

        if old_name in update_name_set:
            index = update_name_set.index(old_name)
            update_case = update_case_set[index]

            if update_case.get("cost_time") < old_case.get("cost_time"):
                new_case['tiling'] = update_case.get("tiling")
                new_case['cost_time'] = update_case.get("cost_time")
                LOG_INSTANCE.debug("Case %s would be updated in repository." %
                    new_case.get("shape").get('test_case', None))
            else:
                LOG_INSTANCE.debug("The cost time of case %s is worse than the time in repository. It would not " \
                    "be updated." % new_case.get("shape").get('test_case', None))

        new_name_set.append(str(get_bin_from_shape(new_case.get("shape"))))
        new_case_set.append(new_case)

    for update_case in update_case_set:
        new_case = update_case
        update_name = str(get_bin_from_shape(update_case.get("shape")))
        if update_name not in old_name_set:
            new_name_set.append(str(get_bin_from_shape(new_case.get("shape"))))
            new_case_set.append(new_case)
            LOG_INSTANCE.debug("Case %s would be updated in repository."
                % new_case.get("shape").get('test_case', None))
    return new_name_set, new_case_set


def update_all_case(old_name_set: List[str], old_case_set: List[dict],
                    update_name_set: List[str], update_case_set: List[dict]) -> tuple:
    """
    update all case from old_case_set to new_case_set
    old_name_set: name for each case in old set
    old_case_set: old case set
    update_name_set: name for each case in new set
    update_case_set: update case set

    return: new_name_set, new_case_set
    """
    new_name_set = []
    new_case_set = []

    update_name_set_tmp = copy.deepcopy(update_name_set)
    update_case_set_tmp = copy.deepcopy(update_case_set)
    if len(update_case_set_tmp) == 0:
        new_name_set = old_name_set
        new_case_set = old_case_set

    for i, case in enumerate(update_case_set_tmp):
        update_name_set = [update_name_set_tmp[i]]
        update_case_set = [case]
        new_name_set_tmp, new_case_set_tmp = update_single_case(old_name_set, old_case_set,
                                                                update_name_set, update_case_set)
        new_name_set, new_case_set = new_name_set_tmp, new_case_set_tmp
        old_name_set, old_case_set = new_name_set_tmp, new_case_set_tmp
    return new_name_set, new_case_set


def read_json_file(file_handle: TextIO) -> tuple:
    """
    read json file to contact name_set and case_set
    old_name_set: name for each case in old set

    return: name_set, case_set
    """
    name_set = []
    case_set = []
    file_loc = file_handle.tell()
    lines = file_handle.readlines()
    for idx, line in enumerate(lines):
        if 'shape' not in line:
            LOG_INSTANCE.debug("This line is not repo json str")
            continue
        try:
            case = json.loads(line)
            case_id = str(get_bin_from_shape(case.get("shape")))
        except ValueError:
            LOG_INSTANCE.error("Please check the json file. total_lines = {}, err_line_idx = {}, err_line = {}".format(
                               len(lines), idx, line))
            raise
        finally:
            pass
        name_set.append(case_id)
        case_set.append(case)
    file_handle.seek(file_loc)
    return name_set, case_set


def tiling_merge_file(json_handle: TextIO, tiling_dict: dict) -> None:
    """
    merge repository function interface
    json_handle: json file handle
    tiling_dict: tiling_dict

    return: None
    """
    json_handle.seek(0)
    old_name_set, old_case_set = read_json_file(json_handle)
    update_name_set = [str(get_bin_from_shape(tiling_dict.get("shape")))]
    update_case_set = [tiling_dict]
    _, new_case_set = update_all_case(old_name_set, old_case_set, update_name_set, update_case_set)

    json_handle.seek(0)
    json_handle.truncate()
    for new_case in new_case_set:
        new_case = str(new_case).replace('None', 'null')
        new_case = new_case.replace('\'', '\"')
        new_case = new_case.replace('False', 'false')
        new_case = new_case.replace('True', 'true')
        json_handle.write(str(new_case))
        json_handle.write("\n")


def get_file_name(file_in: str) -> str:
    """
    file_name_split ["AICXXX","bp_input"] or ["AICXXXinput", "1"]
    """
    file_name  = file_in.rsplit(os.sep, 1)[1].split('.')[0]
    file_name_split = file_name.rsplit('_', 1)
    if file_name_split[-1].isdigit():
        return file_name_split[0]
    return file_name


def get_bytes_str(byte_list: list) -> object:
    """
    decode_byte_list_to_bytes_list
    byte_list: [[bytes][bytes]]
    """
    ret_bytes = b''
    tmp_bytes_list = []
    for list_item in byte_list:
        tmp_bytes_list.append(ret_bytes.join(list_item))
    ret_bytes = ret_bytes.join(tmp_bytes_list)
    return ret_bytes


def get_repository_file_body(input_json_path: str) -> object:
    """
    generate tiling repository body in memory
    input_json_path: json file path
    """
    with open(input_json_path, 'rb+') as repository_json_file:
        fcntl.flock(repository_json_file.fileno(), fcntl.LOCK_EX)
        all_bin = []
        for line in repository_json_file:
            try:
                case = json.loads(line)
                tmp_bin = []
                tmp_bin, _ = get_bin_from_repository_case(case)
            except ValueError:
                LOG_INSTANCE.warning("{} is not json str".format(line))
                continue
            finally:
                pass
            all_bin.append(tmp_bin)
        fcntl.flock(repository_json_file.fileno(), fcntl.LOCK_UN)
    return get_bytes_str(all_bin)


def compare_json_bin(json_file_path: str, bin_file_path: str) -> bool:
    """
    match json and bin file content
    :param: merge repo path, json file name, bin file name
    Returns
    -------
    True or False
    """
    tmp_file_content = get_repository_file_body(json_file_path)
    bin_file_content = b''
    try:
        if os.path.isfile(bin_file_path):
            with open(bin_file_path, 'rb') as fd_bin:
                fd_bin.seek(BIN_FILE_HEAD_LENGTH)
                bin_file_content = fd_bin.read()
        return tmp_file_content == bin_file_content
    except OSError as err:
        LOG_INSTANCE.warning("Failed to compare json and bin, as {}".format(err))
        return False
    finally:
        pass


def get_json_list(soc_ga_dir: str) -> List[str]:
    """
    get all json file in soc_ga_dir
    """
    custom_json_file_list = []
    for path in Path(soc_ga_dir).glob("*.json"):
        if not os.access(path, os.R_OK):
            LOG_INSTANCE.warning("No permission to read file %s", path)
            continue
        path = os.fspath(path)
        file_name = path.rsplit('.', 1)[0]
        bin_file = '{}.bin'.format(file_name)
        if not compare_json_bin(path, bin_file):
            LOG_INSTANCE.warning("%s json and bin file don't match.", path)
            continue
        custom_json_file_list.append(path)
    return custom_json_file_list


def get_same_file_dict(ga_dir: str, same_file_dict: dict) -> dict:
    """
    merge repository function interface
    custom_json_list: json file path custom_repo_dir

    return:
    """
    json_file_list = get_json_list(ga_dir)
    if not json_file_list:
        LOG_INSTANCE.warning("No qualified ga json to merge in path %s.", ga_dir)
    for json_file in json_file_list:
        file_name = get_file_name(json_file)
        if file_name not in same_file_dict:
            same_file_dict[file_name] = []
        same_file_dict[file_name].append(json_file)


def check_dst_file_exist(dst_file: str) -> bool:
    """
    check if dst_bin or dst_json file exist

    return: true or false
    """
    if os.path.isfile(dst_file):
        LOG_INSTANCE.warning("Object file %s has already exist, it will not merge.", dst_file)
        return True
    return False


def sort_by_cost_time(all_case_dict: dict) -> None:
    """
    sort all case in dict by cost_time in ascend order
    """
    for case in all_case_dict:
        all_case_dict[case] = sorted(all_case_dict[case], key=lambda repo:repo['cost_time'])


def get_custom_cases(single_file: str, all_case_dict: dict, total_cnt: int) -> int:
    """
    get custom cases from single file
    """
    with open(single_file, 'r') as f_handler:
        case_id_list, case_list = read_json_file(f_handler)
        LOG_INSTANCE.debug("%d different cases in file %s." % (len(set(case_id_list)), single_file))
        for idx, case_id in enumerate(case_id_list):
            if case_id not in all_case_dict:
                all_case_dict[case_id] = []
            all_case_dict[case_id].append(case_list[idx])
            total_cnt = total_cnt + 1
            if total_cnt % LINE_CNT == 0:
                LOG_INSTANCE.info("GA bank is merging, please wait.")
    return total_cnt


def get_all_custom_cases(same_file_list: List[str]) -> dict:
    """
    get all cases dict in same files
    key : case_id
    value: case list
    return all_case_dict
    """
    all_case_dict = {}
    total_cnt = 0
    for single_file in same_file_list:
        total_cnt = get_custom_cases(single_file, all_case_dict, total_cnt)
    LOG_INSTANCE.debug("%d different cases will be merged." % (len(all_case_dict)))
    return all_case_dict


def update_all_case_dict(line: str, all_case_dict: dict) -> None:
    """
    update all case dict while reading json lines
    """
    try:
        case = json.loads(line)
        case_id = str(get_bin_from_shape(case.get("shape")))
        if case_id in all_case_dict and case.get("cost_time") <= all_case_dict.get(case_id)[0].get("cost_time"):
            all_case_dict.pop(case_id)
    except (ValueError, KeyError):
        LOG_INSTANCE.debug("%s is not repo json format.", line)
    finally:
        pass


def get_final_cases(built_in_file: str, all_case_dict: dict) -> None:
    """
    compare built-in repo with repo in all_case_dict
    remove the cases no better than built-in
    """
    if not os.path.exists(built_in_file):
        LOG_INSTANCE.info("No built-in ga bank file: %s", built_in_file)
        return
    with open(built_in_file, 'r') as b_handler:
        file_loc = b_handler.tell()
        lines = b_handler.readlines()
        for line in lines:
            update_all_case_dict(line, all_case_dict)
        b_handler.seek(file_loc)


def generate_merge_file(dst_json_file: str, dst_bin_file: str, update_cases: List[str]) -> None:
    """
    generate json and bin file for merge
    """
    if not os.path.isfile(dst_json_file):
        with os.fdopen(os.open(dst_json_file, FILE_FLAG, FILE_MODE_640), 'w'):
            pass
    if not os.path.isfile(dst_bin_file):
        with os.fdopen(os.open(dst_bin_file, FILE_FLAG, FILE_MODE_640), 'w'):
            pass

    with open(dst_json_file, 'r+') as json_handle, open(dst_bin_file, 'ab') as bin_handle:
        fcntl.flock(json_handle.fileno(), fcntl.LOCK_EX)
        fcntl.flock(bin_handle.fileno(), fcntl.LOCK_EX)
        json_handle.seek(0, 0)
        json_handle.truncate()
        for final_case in update_cases:
            final_case = str(final_case).replace('None', 'null')
            final_case = final_case.replace('\'', '\"')
            final_case = final_case.replace('False', 'false')
            final_case = final_case.replace('True', 'true')
            json_handle.write(str(final_case))
            json_handle.write("\n")
        transfer_json_to_bin_by_handle(json_handle, bin_handle)
        fcntl.flock(bin_handle.fileno(), fcntl.LOCK_UN)
        fcntl.flock(json_handle.fileno(), fcntl.LOCK_UN)


def merge_same_files(base_name: str, same_file_list: List[str], built_in_path: str, dst_path: str) -> bool:
    """
    merge all same files for custom_repo and built_in_repo
    """
    base_json = '{}.json'.format(base_name)
    dst_bin = '{}.bin'.format(base_name)
    dst_json_file = os.path.join(dst_path, base_json)
    dst_bin_file = os.path.join(dst_path, dst_bin)
    if check_dst_file_exist(dst_json_file) or check_dst_file_exist(dst_bin_file):
        return False
    built_in_file = os.path.join(built_in_path, base_json)
    all_case_dict = {}
    tmp_path = os.path.join(os.getcwd(), "repo.tmp")
    with os.fdopen(os.open(tmp_path, FILE_FLAG, FILE_MODE_640), 'w') as tmp_file:
        fcntl.flock(tmp_file.fileno(), fcntl.LOCK_EX)
        all_case_dict = get_all_custom_cases(same_file_list)
        if all_case_dict:
            sort_by_cost_time(all_case_dict)
            update_case_set = []
            get_final_cases(built_in_file, all_case_dict)
            if not all_case_dict:
                LOG_INSTANCE.info("All cases in %s are better in built-in ga bank.", base_json)
                fcntl.flock(tmp_file.fileno(), fcntl.LOCK_UN)
                remove_file(tmp_path)
                return True
            for case_id in all_case_dict:
                update_case_set.append(all_case_dict[case_id][0])
            create_dir(dst_path)
            generate_merge_file(dst_json_file, dst_bin_file, update_case_set)
        fcntl.flock(tmp_file.fileno(), fcntl.LOCK_UN)
    remove_file(tmp_path)
    return True


def check_src_type(src_path_list: List[str]) -> List[str]:
    """
    check if the src_path is tune_bank_path
    return :
    """
    src_paths = []
    default_path = [os.path.realpath(os.path.join(FILE_PATH, "../../../data/tiling/")),
                    os.path.realpath(os.path.join(FILE_PATH, "../../../data/rl/"))]
    for src_path in src_path_list:
        if os.path.realpath(src_path) in default_path:
            if "tiling" in src_path:
                LOG_INSTANCE.warning("Input src path can not be default custom bank path: %s", src_path)
            continue
        paths = os.listdir(src_path)
        if not paths or "tiling" in paths or "rl" in paths or "unified_bank" in paths:
            LOG_INSTANCE.warning("Please check your input src_path: %s", src_path)
            continue
        src_paths.append(src_path)
    return src_paths


def check_dir(base_path: str) -> bool:
    """
    check if soc_dir exist
    """
    if not os.path.isdir(base_path):
        LOG_INSTANCE.warning("No ga bank path: %s.", base_path)
        return False
    if not os.access(base_path, os.R_OK):
        LOG_INSTANCE.warning("No permission to read dir %s.", base_path)
        return False
    return True


def merge_single_soc(dir_list: List[str], opp_path: str, soc_version: str, dst_path: str) -> bool:
    """
    merge all cases in tune_bank_path/soc/cube with built-in
    """
    same_file_dict = {}
    for src_dir in dir_list:
        src_ga_dir = os.path.realpath(os.path.join(src_dir, soc_version, GA_DIR_NAME))
        if not check_dir(src_ga_dir):
            continue
        get_same_file_dict(src_ga_dir, same_file_dict)
    # change short soc_version
    soc_version = SOC_VERSION_TO_SHORT_LIST.get(soc_version, soc_version)

    built_in_path = os.path.realpath(os.path.join(opp_path, "built-in", "data", "tiling", soc_version))
    if not os.path.exists(built_in_path):
        built_in_path = os.path.realpath(os.path.join(opp_path, "data", "tiling", soc_version, "built-in"))
    if not os.path.isdir(built_in_path):
        LOG_INSTANCE.error("Please check your built-in ga bank path in opp package.")
        return False
    for base_file in same_file_dict:
        base_name = get_file_name(same_file_dict.get(base_file)[0])
        merge_same_files(base_name,
            same_file_dict.get(base_file), built_in_path, dst_path)
    return True


def tiling_merge_offline(input_src_path_list: List[str], base_dst_path: str) -> None:
    """
    merge repository function interface
    base_src_path: json file path custom_repo_dir
    xxx/data/ or tune_bank_path
    data/tiling/soc/custom tune_bank_path/soc/ga/
    base_dst_path: json file path new_repo_dir

    return: None
    """
    opp_path = os.getenv(OPP_PATH)
    if not opp_path:
        LOG_INSTANCE.error("Please check your ASCEND_OPP_PATH in env.")
        return False
    base_src_path_list = check_src_type(input_src_path_list)
    if not base_src_path_list:
        LOG_INSTANCE.warning("All src paths don't support merge ga bank.")
        return False
    soc_version_list = []
    for base_src_path in base_src_path_list:
        dir_list = os.listdir(base_src_path)
        for soc_version in dir_list:
            if soc_version not in SOC_VERSION_TO_SHORT_LIST.keys():
                continue
            soc_version_list.append(soc_version)
    if not soc_version_list:
        LOG_INSTANCE.warning("No soc_version found in src.")
        return True
    soc_version_set = set(soc_version_list)
    base_dst_path = os.path.realpath(base_dst_path)
    for soc_version in soc_version_set:
        LOG_INSTANCE.info("Start to merge %s ga bank.", soc_version)
        dst_path = os.path.join(base_dst_path, soc_version, GA_DIR_NAME)
        merge_single_soc(base_src_path_list, opp_path, soc_version, dst_path)
        LOG_INSTANCE.info("Ga bank for %s has finished merging.", soc_version)
    return True


def merge_same_files_locally(dst_dir: str, base_name: str, same_file_list: List[str]) -> None:
    """
    merge all same files for tmp_bank and custom_bank
    """
    dst_json_file = os.path.join(dst_dir, '%s.json' % base_name)
    dst_bin_file = os.path.join(dst_dir, '%s.bin' % base_name)
    all_case_dict = get_all_custom_cases(same_file_list)
    if all_case_dict:
        sort_by_cost_time(all_case_dict)
        update_case_set = []
        for case_id in all_case_dict:
            update_case_set.append(all_case_dict[case_id][0])
        generate_merge_file(dst_json_file, dst_bin_file, update_case_set)


def merge_bank_files(dir_list: List[str], soc_version: str) -> bool:
    """
    merge all cases in dir_list
    @param dir_list: custom_bank_path, tmp_bank_path
    @param soc_version: soc_version of the current tune in-param
    @return: Bool
    """
    same_file_dict = {}
    dst_dir = os.path.realpath(os.path.join(dir_list[0], soc_version, "ga"))
    create_dir(dst_dir)

    del_redundant_files(dst_dir)

    for tmp_dir in dir_list:
        tmp_ga_dir = os.path.realpath(os.path.join(tmp_dir, soc_version, "ga"))
        get_same_file_dict(tmp_ga_dir, same_file_dict)
    for base_file in same_file_dict:
        base_name = get_file_name(same_file_dict[base_file][0])
        merge_same_files_locally(dst_dir, base_name, same_file_dict.get(base_file))
    return True


def del_redundant_files(dir_path: str) -> None:
    '''
    delete the .repo_lock and repo.lock file in the dir_path
    '''
    repo_lock1 = os.path.join(dir_path, '.repo_lock')
    repo_lock2 = os.path.join(dir_path, 'repo.lock')
    remove_file(repo_lock1)
    remove_file(repo_lock2)
