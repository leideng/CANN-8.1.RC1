#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.

Define the base class for tuners
"""
import os
import time
import json
import copy
import math
from enum import Enum
from typing import List
from collections import namedtuple
from dataclasses import dataclass

from auto_tune.auto_tune_lock import LocalLock
from auto_tune.auto_tune_log import LOG_INSTANCE
from auto_tune.auto_tune_log import LOG_PROGRESS_INSTANCE
from auto_tune.estimate.python.atc_estimate import atc_build_kernel
from auto_tune.estimate.python.atc_estimate import ESTIMATE_CONFIG_PATH
from auto_tune.estimate.python.parallel_run import parallel_run
from auto_tune.estimate.python.parallel_run import SUCCESS_RATE_CONTROL
from auto_tune.estimate.python.qtest_run import qtest_run
from auto_tune.estimate.python.util import get_kernel_meta_dir
from auto_tune.common_module.common_util import COST_MAX_VALUE
from auto_tune.common_module.common_util import MeasureResult
from auto_tune.common_module.common_util import get_time_str
from auto_tune.common_module.common_util import report_tune_progress
from auto_tune.common_module.common_util import TuneResReport
from auto_tune.common_module.common_util import DEFAULT_OM_RUN_THRESHOLD
from auto_tune.config_module.config import RUN_TYPE_QTEST
from auto_tune.config_module.task_feature import TaskFeature
from auto_tune.util_atc import get_dict_from_json_file
from auto_tune.util_atc import INVALID_COST_TIME

MINIEST_CONFIG_SIZE = 2
BETTER_RATIO = 0.95
SMALL_THRESHOLD = 3
BIG_THRESHOLD = 100
BUILD_OM_WAIT = 3000
BUILD_OM_WAIT_HIGH_PERF = 9000
A_OVERHEAD_OPT_FLAG_LOCATION = 1
B_OVERHEAD_OPT_FLAG_LOCATION = 15
TIME_SLEEP = 0.1
TOPN_FOR_DATACMP = 3
ADJUST_COEFFICIENT = 10

BestIndividual = namedtuple("BestIndividual", ["tiling_entity", "tiling_dict", "measure_result", "origin"])
BestIndividual.__new__.__defaults__ = (None, None, None, "tuning")

TilingResults = namedtuple("TilingResults",
                           ["cost_model_tiling_ret",
                            "tuning_tiling_ret",
                            "auto_tiling_ret",
                            "dsl_cache_tiling_ret",
                            "default_ret"
                            ])
TilingResultsAfterDC = namedtuple("TilingResultsAfterDC",
                           ["tuning_tiling_ret",
                            "cost_model_tiling_ret",
                            "dsl_cache_tiling_ret"
                            ])
TopNItems = namedtuple("TopNItems", ["tuning_topn", "cost_model_topn"])
BestTilingFlag = namedtuple("BestTilingFlag", ["best_tuning", "best_cost_model", "best_dsl_cache"])


class CompIntensity(Enum):
    NORMAL = 0
    LOW = 1
    HIGH = 2


@dataclass
class TunerFilterList:
    '''
    Some Special filter list for tuner
    '''
    def __init__(self):
        # the list support cost compiling with choose tiling and default
        self.cost_compile_op_list = ["matmul", "conv2d_backprop_filter"]


def tiling_better_than(curr_tiling_result: int, target_tiling_result: int,
    intensity: CompIntensity = CompIntensity.NORMAL) -> bool:
    """
    Judge whether curr_tiling_result is better enough

    Parameters
    ----------
    curr_tiling_result: current tiling result
    target_tiling_result: target tiling result
    """
    coefficient = 1.0
    if intensity == CompIntensity.HIGH:
        coefficient = BETTER_RATIO
    if (target_tiling_result - curr_tiling_result >= SMALL_THRESHOLD) and \
            (target_tiling_result - curr_tiling_result >= BIG_THRESHOLD or
             curr_tiling_result <= target_tiling_result * BETTER_RATIO * coefficient):
        return True
    return False


def record_op_ticks(tick: int, pid_time_stamp: str, op_name: str) -> None:
    """
    record performance while tuning
    """
    tune_result_file = os.path.join(os.getcwd(), "tune_result_{}.json".format(pid_time_stamp))
    local_lock = LocalLock(tune_result_file)
    local_lock.lock()
    with open(tune_result_file, 'r+') as f_handle:
        tune_result = get_dict_from_json_file(f_handle)
        tune_result.setdefault(op_name, {})
        time_stamp = time.strftime("%Y-%m-%d-%H:%M:%S", time.localtime())
        best_tick_item = "[%d, %s]" % (tick, time_stamp)
        tune_result[op_name].setdefault('ticks_best', [])
        tune_result[op_name]['ticks_best'].append(best_tick_item)
        f_handle.seek(0)
        f_handle.truncate()
        json.dump(tune_result, f_handle, sort_keys=True, indent=4)
    local_lock.unlock()


class Tuner:
    """
    Base class for tuners
    Parameters
    ----------
    task: autotvm.task.Task Tuning Task
    """
    def __init__(self, task: TaskFeature, **kwargs: object) -> None:
        self.task = task
        self.param = kwargs
        self.feature_config = task.feature_config
        self.tune_res_report = None
        self.build_op_name = None
        self.filter_list = TunerFilterList()
        self.iter_num = 0
        self.best_config = None
        self.best_performance = COST_MAX_VALUE
        self.best_individual = None
        self.topn_result = []
        self.best_iter = 0
        self.history_record = []
        self.tiling_inputs = []
        # Store the params for current generation store the entity of tiling individuals for current generation
        self.inputs = []
        # store the corresponding performance data of tiling individuals for current generation
        self.results = []
        self.can_tune = True
        # the flag of tuning result
        self.better_than_online = False
        # time to leave
        self.n_trial = None
        self.pop_size = None
        self.total_round = 0
        self.update_end_tune_params()
        self.early_stopping = None
        # params about tilings
        self.exist_repository = self.task.common_args.get('exist_repository_flag', False)
        self.auto_tiling = self.task.common_args.get('auto_tiling_dict', [])
        self.cost_model_tiling = self.task.common_args.get('cost_model_tiling_dict', [])
        self.dsl_cache_tiling = self.task.common_args.get('dsl_cache_tiling', [])
        # current op case flag file
        self.op_tune_flag = self.task.common_args.get('op_tune_flag', "")
        self.time_stamp = self.task.common_args.get('time_stamp', None)
        self.pass_param_num = self.task.common_args.get('pass_param_num', 0)
        self.overhead_opt_tune = self.task.common_args.get('overhead_opt_tune', False)
        # run_optune: True means the mode is optune, False means auto_tune
        self.run_optune = False
        self.costtime_baseline = DEFAULT_OM_RUN_THRESHOLD
        self.quit_due_to_malloc_error = False
        self.tmp_total_costtime = None
        self.default_time_bound = BUILD_OM_WAIT_HIGH_PERF if self.feature_config.high_perf else BUILD_OM_WAIT
        self.build_run_time_bound = self.default_time_bound
        if "cb_struct_key" in self.task.common_args.keys():
            self.run_optune = True
            self.cb_struct_key = self.task.common_args.get("cb_struct_key", "")
            self.cb_queue = self.task.common_args.get("cb_task", None)
            self.cb_result = self.task.common_args.get("cb_res", None)
            self.cb_release = self.task.common_args.get("cb_release", None)
            self.timer_queue = self.task.common_args.get("timer", None)
            self.tune_process_q = self.task.common_args.get("tune_progress", None)
            self.cb_datacmp_queue = self.task.common_args.get("cb_datacmp_task", None)
            self.cb_datacmp_result = self.task.common_args.get("cb_datacmp_res", None)
            LOG_INSTANCE.info("Tuner cb_struct_key: %s." % self.cb_struct_key)
            LOG_PROGRESS_INSTANCE.set_log_flag(False)
            self.build_run_time_bound = self.get_build_run_time_bound()
            if self.tmp_total_costtime is not None:
                self.costtime_baseline = self.tmp_total_costtime

    @staticmethod
    def _log_compile_err(kernel_compile_op: List[dict]) -> None:
        for c_op in kernel_compile_op:
            c_item = c_op.get()
            if isinstance(c_item, dict):
                for c_key in c_item:
                    LOG_INSTANCE.error("\"%s\": %s" % (c_key, c_item[c_key]))
            else:
                LOG_INSTANCE.error(c_item)

    def update_end_tune_params(self) -> None:
        if "ga_args" in self.param:
            self.n_trial = min(self.param.get("ga_args").get("trial_num"), len(self.task.feasibility))
            self.pop_size = self.param.get("ga_args").get('pop_size')
            if len(self.task.feasibility) < MINIEST_CONFIG_SIZE:
                LOG_INSTANCE.warning(
                    "Config space don't have enough individuals, the kernel %s doesn't support auto tuning!",
                    self.task.param["kernel_name"])
                self.can_tune = False
                self.total_round = 0
            else:
                self.total_round = 0 if self.pop_size == 0 else math.ceil(self.n_trial / self.pop_size)

    def get_op_cost_time(self, res: dict) -> int:
        """
        get_op_cost_time from res
        Parameters
        ----------
        res: dict
        """
        if not res:
            return 0
        if not self.tmp_total_costtime:
            self.tmp_total_costtime = int(res.get("profE2ECostTime", DEFAULT_OM_RUN_THRESHOLD))
        if res.get("totalCostTime") == INVALID_COST_TIME:
            return INVALID_COST_TIME
        op_name = res.get("opName")
        # save build_op_name for report tune result
        opat_indx = op_name.rfind("_OPAT_")
        if not self.build_op_name and opat_indx >= 0:
            split_str = op_name[opat_indx:]
            self.build_op_name = "".join(op_name.split(split_str))

        op_cost_time = int(res.get("aiCoreCostTime").get(op_name, 0))
        if not op_cost_time:
            op_cost_time = int(res.get("opCostTime").get(op_name, 0))
        return op_cost_time / 1000

    def update(self):
        """
        Update individuals of the tiling according to corresponding results
        Parameters
        ----------
        Implemented in GATuner
        """
        LOG_INSTANCE.debug("update is not Implemented in currrent tuner: %s", self)
        raise NotImplementedError()

    def check_op_tune_flag(self) -> bool:
        """
        check if current op flag file exist
        Parameters
        ----------
        Returns
        """
        if self.run_optune:
            return True
        return os.path.isfile(self.op_tune_flag)

    def get_topn_individuals(self, need_input_and_result: bool) -> list:
        """
        get topn individuals
        """
        topn_individual = []
        if need_input_and_result:
            self.topn_result = sorted(self.topn_result, key = lambda res: res[0].cost)[0 : TOPN_FOR_DATACMP]
            for res in self.topn_result:
                temp_entity = res[1]
                temp_dict = self.task.tiling_transform(temp_entity)
                topn_individual.append(BestIndividual(temp_entity, temp_dict, res[0]))
        else:
            self.topn_result = sorted(self.topn_result, key = lambda res: res.cost)[0 : TOPN_FOR_DATACMP]
            for res in self.topn_result:
                temp_entity = self.task.point2entity(res.point)
                temp_dict = self.task.tiling_transform(temp_entity)
                topn_individual.append(BestIndividual(temp_entity, temp_dict, res))
        return topn_individual

    def get_tmp_best_measure_pair(self, pop_size: int) -> int:
        """
        get tmp_best_measure_pair each round
        """
        # record the best performance each round
        round_best = COST_MAX_VALUE
        for k, res in enumerate(self.results):
            # record the round best performance
            round_best = min(round_best, res.cost)
            # record the best performance
            if res.cost < self.best_performance:
                self.best_performance = res.cost
                temp_entity = self.task.point2entity(res.point)
                temp_dict = self.task.tiling_transform(temp_entity)
                self.best_individual = BestIndividual(temp_entity, temp_dict, res)
                self.best_iter = pop_size + k
            LOG_INSTANCE.debug("No: %s\tTIME(us): %s\t result: %s", (pop_size + k + 1), res.cost, res.cost)

        return round_best

    def tune_early_terminated(self, pop_size: int) -> None:
        """
        early stop record
        """
        if len(self.results) != 0:
            _ = self.get_tmp_best_measure_pair(pop_size)
        # Record the training data
        history_data = self.task.record_history_data(self.inputs, self.results)
        self.history_record.extend(history_data)
        LOG_INSTANCE.error('The task is terminated due to insufficient genes: %s', self.task.param.get('test_case'))

    def enter_next_round(self, pop_size: int, early_stopping: int) -> bool:
        """
        enter next tune round
        """
        # If iterative variable exceeds the upper limit, stop the iterative process
        if pop_size >= self.best_iter + early_stopping:
            LOG_INSTANCE.debug("Early stopped. Best iter: %d.", self.best_iter)
            return False

        if pop_size < self.n_trial:
            self.iter_num += 1
            if self.total_round == self.iter_num:
                self.total_round += 1
                LOG_INSTANCE.info("Due to insufficient individuals, there will be another round! " \
                    "Total tuning round would be %d", self.total_round)
                LOG_PROGRESS_INSTANCE.info("Due to insufficient individuals, there will be another round! " \
                    "Total tuning round would be %d", self.total_round)
            if self.update():
                LOG_INSTANCE.debug("Early stopped. Best iter: %d.", self.best_iter)
                return False
        return True

    def get_default_cost(self, is_data_cmp: bool = False, is_base: bool = False) -> tuple:
        """
        get default cost without tiling
        support op: matmul
        """
        if not self.feature_config.is_type_v300:
            op_type = self.task.param.get("op_type")
            if not is_base and op_type not in self.filter_list.cost_compile_op_list:
                return COST_MAX_VALUE, self.default_time_bound
        default_kernel = {
            "kernel_name": "%s_%s" % (self.task.param.get("kernel_name", ""), get_time_str()),
            "tiling_dict": None,
            "tuning_mode": "GA"
        }
        default_kernel_list = [{}] if self.run_optune else [default_kernel]
        return self.get_tiling_result(default_kernel_list, is_data_cmp, is_base)

    def get_build_run_time_bound(self) -> int:
        _, base_cost_time = self.get_default_cost(False, True)
        return math.ceil(base_cost_time) * ADJUST_COEFFICIENT

    def tune(self, early_stopping: int = None) -> None:
        """
        Begin tuning

        Parameters
        ----------
        early_stopping: Early stop the tuning when not finding better configs in this number of trials
        """
        # set the variable for early stopping,
        # control the early exit of iteration process
        self.early_stopping = early_stopping or 1e9
        need_report = self.run_optune and hasattr(self, "cb_struct_key") and self.cb_struct_key
        # Set iterative variable
        iterative_val = 0
        # Main process for iteration
        while self.check_op_tune_flag() and iterative_val < self.n_trial:
            if self.quit_due_to_malloc_error:
                LOG_INSTANCE.warning("The tuning of %s is stopped due to insufficient memory space.",
                    self.task.param.get('kernel_name'))
                break
            # If the number of configs don't reach the required number, stop the iterative process
            if len(self.inputs) < len(self.tiling_inputs) * SUCCESS_RATE_CONTROL:
                self.tune_early_terminated(iterative_val)
                break
            # If the number of results don't reach the required number, stop the iterative process
            if len(self.results) != len(self.inputs):
                break
            LOG_INSTANCE.debug("current iteration number: %s", iterative_val)
            # Record the training data
            history_data = self.task.record_history_data(self.inputs, self.results)
            self.history_record.extend(history_data)
            # Record best tiling
            round_best = self.get_tmp_best_measure_pair(iterative_val)
            # Record topN results every round
            self.topn_result.extend(sorted(self.results, key = lambda res: res.cost)[0 : TOPN_FOR_DATACMP])
            if need_report:
                report_tune_progress(self.tune_process_q, self.cb_struct_key, self.iter_num + 1, self.total_round)
            LOG_INSTANCE.info("Round %s/%s : time of best tiling is %s us", \
                                self.iter_num + 1, self.total_round, round_best)
            LOG_PROGRESS_INSTANCE.info("Round %s/%s : time of best tiling is %s us", self.iter_num + 1,
                                        self.total_round, round_best)
            if not self.run_optune:
                record_op_ticks(round_best, self.time_stamp, self.task.topi_args.get('ori_op_name', '[[]]'))
            # Update the iterative variable
            iterative_val += len(self.results)
            if not self.enter_next_round(iterative_val, self.early_stopping):
                break
        self.compare_performance()
        if self.best_individual is None and not self.quit_due_to_malloc_error:
            LOG_INSTANCE.error("%s can not find best tiling.", self.task.param.get('test_case'))
        LOG_INSTANCE.info("Auto tuning of kernel %s is finished.", self.task.param.get('kernel_name'))
        LOG_PROGRESS_INSTANCE.info("Auto tuning of kernel %s is finished.", self.task.param.get('kernel_name'))

    def get_kernels_for_overhead(self, base_kernel_name: str) -> List[dict]:
        """
        get kernels that enable overhead opt
        :return: kernels that enable overhead opt
        """
        overhead_strategys = [{"A_overhead_opt_flag": False,
                                "B_overhead_opt_flag": False},
                                {"A_overhead_opt_flag": True,
                                "B_overhead_opt_flag": False},
                                {"A_overhead_opt_flag": False,
                                "B_overhead_opt_flag": True},
                                {"A_overhead_opt_flag": True,
                                "B_overhead_opt_flag": True},
                                ]
        param_2_location = {"A_overhead_opt_flag": A_OVERHEAD_OPT_FLAG_LOCATION,
                            "B_overhead_opt_flag": B_OVERHEAD_OPT_FLAG_LOCATION}

        tuning_kernel_list = []
        tiling_entitys = []
        measure_dicts = []
        if self.feature_config.is_type_v300:
            for val in self.task.config_space.get("special_optimize_flag"):
                measure_dict = copy.deepcopy(self.best_individual.tiling_dict)
                tiling_entity = copy.deepcopy(self.best_individual.tiling_entity)
                measure_dict["special_optimize_flag"] = val
                tiling_entity[-1] = val
                measure_dicts.append(measure_dict)
                tiling_entitys.append(tiling_entity)
        else:
            for strategy in overhead_strategys:
                measure_dict = copy.deepcopy(self.best_individual.tiling_dict)
                tiling_entity = copy.deepcopy(self.best_individual.tiling_entity)
                for param, opt_flag in strategy.items():
                    measure_dict[param] = opt_flag
                    tiling_entity[param_2_location[param]] = opt_flag
                measure_dicts.append(measure_dict)
                tiling_entitys.append(tiling_entity)
        for index, tiling_entity in enumerate(tiling_entitys):
            tuning_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_entity": tiling_entity,
                "tiling_dict": measure_dicts[index],
                "tuning_mode": "GA"
            }
            tuning_kernel_list.append(tuning_kernel)
        return tuning_kernel_list

    def get_kernels_for_passparam(self, base_kernel_name: str, pass_param_num: int) -> List[dict]:
        """
        get kernels for passparams
        """
        kernel_list = []
        for pass_num in range(0, pass_param_num):
            tmp_measure_dict = copy.deepcopy(self.best_individual.tiling_dict)
            tmp_measure_dict["tbe_compile_para"] = pass_num
            tiling_entity = copy.deepcopy(self.best_individual.tiling_entity)
            tiling_entity.append(pass_num)
            tuning_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_entity": tiling_entity,
                "tiling_dict": tmp_measure_dict,
                "tuning_mode": "GA"
            }
            kernel_list.append(tuning_kernel)
        return kernel_list

    def measure_performance(self) -> list:
        """
        measure auto tiling, cost model tiling, tuning tiling performance

        Parameters
        ----------

        Returns
        -------
        auto_tiling_result: auto tiling performance int
        cost_model_tiling_result: cost model tiling performance int
        tuning_tiling_result: tuning tiling performance int

        """
        base_kernel_name = self.task.param.get("kernel_name")
        default_result, _ = self.get_default_cost()

        best_model_tiling_index = 0
        cost_model_tiling_result = COST_MAX_VALUE
        cost_model_tiling_dict = {}
        cost_model_tiling_topn_list = []
        for index, model_tiling in enumerate(self.cost_model_tiling):
            if model_tiling:
                cost_model_kernel = {
                    "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                    "tiling_dict": model_tiling,
                    "tuning_mode": "GA"
                }
                cost_model_tiling_dict[index], _ = self.get_tiling_result([cost_model_kernel])
        if cost_model_tiling_dict:
            cost_model_tiling_topn_list = \
                sorted(cost_model_tiling_dict.items(), key = lambda x: (x[1], x[0]))[0 : TOPN_FOR_DATACMP]
            # the first zero is the best index of cost model list, the sec zero is model tilling index
            best_model_tiling_index = cost_model_tiling_topn_list[0][0]
            cost_model_tiling_result = cost_model_tiling_topn_list[0][1]

        dsl_cache_tiling_result = COST_MAX_VALUE
        if self.dsl_cache_tiling:
            cache_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_dict": self.dsl_cache_tiling,
                "tuning_mode": "GA"
            }
            dsl_cache_tiling_result, _ = self.get_tiling_result([cache_kernel])

        auto_tiling_result = COST_MAX_VALUE
        if self.auto_tiling:
            auto_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_dict": self.auto_tiling[0],
                "tuning_mode": "GA"
            }
            auto_tiling_result, _ = self.get_tiling_result([auto_kernel])

        tuning_tiling_result = COST_MAX_VALUE
        tuning_topn_individuals = self.get_topn_individuals(False)
        if self.best_individual:
            tuning_tiling_result = self.best_individual.measure_result.cost
            if self.pass_param_num > 0:
                self.topn_result.clear()
                tuning_tiling_result = self.get_passnum_and_overhead_results(base_kernel_name)
            else:
                tuning_kernel = {
                    "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                    "tiling_entity": copy.deepcopy(self.best_individual.tiling_entity),
                    "tiling_dict": self.best_individual.tiling_dict,
                    "tuning_mode": "GA"
                }
                tuning_tiling_result, _ = self.get_tiling_result([tuning_kernel])

        LOG_INSTANCE.info("The result of auto_tiling is %s.", auto_tiling_result)
        LOG_INSTANCE.info("The result of tuning_tiling is %s.", tuning_tiling_result)
        LOG_INSTANCE.info("The result of cost_model_tiling is %s.", cost_model_tiling_result)
        LOG_INSTANCE.info("The result of default_tiling is %s.", default_result)
        LOG_INSTANCE.info("The result of cache_tiling is %s.", dsl_cache_tiling_result)
        LOG_INSTANCE.info("measure_performance succ, kernel_name: %s.", base_kernel_name)

        tiling_rets = TilingResults(cost_model_tiling_result, tuning_tiling_result, auto_tiling_result,
            dsl_cache_tiling_result, default_result)
        return tiling_rets, best_model_tiling_index, TopNItems(tuning_topn_individuals, cost_model_tiling_topn_list)

    def update_best_individual_by_cost_model_tiling(self, tiling_result: int, best_model_tiling_index: int):
        """
        update the best_individual by best_model_tiling:
        best_model_tiling_index: the index of best best_model_tiling in task.common_args['cost_model_tiling_dict']
        """
        if not self.best_individual:
            return
        res = self.best_individual.measure_result
        res.cost = tiling_result
        cost_model_tiling_entity = self.task.common_args.get('cost_model_tiling_list', [])[best_model_tiling_index]
        cost_model_tiling_dict = self.task.common_args.get('cost_model_tiling_dict', [])[best_model_tiling_index]
        self.best_individual = BestIndividual(cost_model_tiling_entity, cost_model_tiling_dict, res, "cost_model")
        LOG_INSTANCE.info("update_best_individual_by_cost_model_tiling, kernel_name: %s.",
                          self.task.param.get("kernel_name"))

    def update_best_individual_by_dsl_cache_tiling(self, tiling_result: int) -> None:
        """
        update the best_individual by dsl_cache_tiling:
        """
        if not self.best_individual:
            return
        res = self.best_individual.measure_result
        res.cost = tiling_result
        tiling_entity = self.dsl_cache_tiling
        tiling_dict = self.dsl_cache_tiling
        self.best_individual = BestIndividual(tiling_entity, tiling_dict, res, "cache_tiling")
        LOG_INSTANCE.info("update_best_individual_by_dsl_cache_tiling, kernel_name: %s.",
                          self.task.param.get("kernel_name"))

    def get_tiling_compare_result(self, tiling_results: TilingResults, best_model_tiling_index: int) -> tuple:
        """
        compare the  best tiling in auto_tiling, cost_model_tiling, and tuning_tiling_result
        """
        tuning_tiling_result = tiling_results.tuning_tiling_ret
        cost_model_tiling_result = tiling_results.cost_model_tiling_ret
        dsl_cache_tiling_result = tiling_results.dsl_cache_tiling_ret
        auto_tiling_result = tiling_results.auto_tiling_ret
        tuning_tilig_best = \
            tiling_better_than(tuning_tiling_result, cost_model_tiling_result) and \
            tiling_better_than(tuning_tiling_result, auto_tiling_result) and \
            tiling_better_than(tuning_tiling_result, dsl_cache_tiling_result)
        cost_model_tiling_best = \
            tiling_better_than(cost_model_tiling_result, tuning_tiling_result) and \
            tiling_better_than(cost_model_tiling_result, dsl_cache_tiling_result) and \
            ((self.cost_model_tiling[best_model_tiling_index] == self.auto_tiling[0]) or
             tiling_better_than(cost_model_tiling_result, auto_tiling_result))
        dsl_cache_tiling_best = \
            tiling_better_than(dsl_cache_tiling_result, tuning_tiling_result) and \
            tiling_better_than(dsl_cache_tiling_result, cost_model_tiling_result) and \
            tiling_better_than(dsl_cache_tiling_result, auto_tiling_result)
        repo_update_flag = not self.exist_repository and self.best_individual
        if not tuning_tilig_best and not cost_model_tiling_best and not dsl_cache_tiling_best and self.exist_repository:
            repo_update_flag = \
                tiling_better_than(tuning_tiling_result, auto_tiling_result, intensity = CompIntensity.HIGH) or \
                tiling_better_than(cost_model_tiling_result, auto_tiling_result, intensity = CompIntensity.HIGH) or \
                tiling_better_than(dsl_cache_tiling_result, auto_tiling_result, intensity = CompIntensity.HIGH)
        LOG_INSTANCE.info("get_tiling_compare_result end, "
            "tuning_tilig_best: %s, cost_model_tiling_best: %s, dsl_cache_tiling_best: %s, repo_update_flag: %s.",
            tuning_tilig_best, cost_model_tiling_best, dsl_cache_tiling_best, repo_update_flag)
        best_tiling_flag = BestTilingFlag(tuning_tilig_best, cost_model_tiling_best, dsl_cache_tiling_best)
        return best_tiling_flag, repo_update_flag

    def data_compare(self, tuning_topn_individuals: list, cost_model_tiling_topn_list: List[tuple]) -> tuple:
        """
        returns the optimal result after data_compare
        """
        base_kernel_name = self.task.param.get("kernel_name")
        LOG_INSTANCE.info("GA data_compare[%s] start!", base_kernel_name)
        tuning_tiling_result_after_dc = COST_MAX_VALUE
        cost_model_tiling_result_after_dc = COST_MAX_VALUE
        best_model_tiling_index_after_dc = 0
        dsl_cache_tiling_after_dc = COST_MAX_VALUE
        if self.pass_param_num > 0 and len(tuning_topn_individuals) > 1:
            # Do another traversal of the suboptimal solution
            self.best_individual = tuning_topn_individuals[1]
            tmp_result = self.get_passnum_and_overhead_results(base_kernel_name)
            if int(tmp_result) == COST_MAX_VALUE:
                return TilingResultsAfterDC(tuning_tiling_result_after_dc, cost_model_tiling_result_after_dc,
                    dsl_cache_tiling_after_dc), best_model_tiling_index_after_dc
            tuning_topn_individuals = self.get_topn_individuals(True)

        results = self.get_datacmp_results(tuning_topn_individuals, cost_model_tiling_topn_list, base_kernel_name)
        if not results:
            return TilingResultsAfterDC(tuning_tiling_result_after_dc, cost_model_tiling_result_after_dc,
                dsl_cache_tiling_after_dc), best_model_tiling_index_after_dc

        tuning_topn_result = results[:TOPN_FOR_DATACMP]
        for idx, res in enumerate(tuning_topn_result):
            if res.verify:
                tuning_tiling_result_after_dc = res.cost
                self.best_individual = tuning_topn_individuals[idx]
                break
            else:
                LOG_INSTANCE.warning("GA data can not cmp, tuning tilling is %s.", \
                    str(tuning_topn_individuals[idx].tiling_dict))
        # The last one in results is cache_tiling result, so the cost_model result is in range [TOPN_FOR_DATACMP: -1)
        cost_model_topn_result = results[TOPN_FOR_DATACMP: -1]
        for idx, res in enumerate(cost_model_topn_result):
            if res.verify:
                best_model_tiling_index_after_dc = cost_model_tiling_topn_list[idx][0]
                cost_model_tiling_result_after_dc = cost_model_tiling_topn_list[idx][1]
                break
            else:
                LOG_INSTANCE.warning("GA data can not cmp, costmodel tilling is %s.", \
                    str(self.task.common_args.get('cost_model_tiling_dict', [])[cost_model_tiling_topn_list[idx][0]]))
        # The last one in results is cache_tiling result
        dsl_cache_tiling_result = results[-1]
        if self.dsl_cache_tiling:
            if dsl_cache_tiling_result.verify:
                dsl_cache_tiling_after_dc = dsl_cache_tiling_result.cost
            else:
                LOG_INSTANCE.warning("GA data can not cmp, dsl_cache_tiling is %s.", str(self.dsl_cache_tiling))
        LOG_INSTANCE.info("GA data_compare[%s] end!", base_kernel_name)
        return TilingResultsAfterDC(tuning_tiling_result_after_dc, cost_model_tiling_result_after_dc,
            dsl_cache_tiling_after_dc), best_model_tiling_index_after_dc

    def compare_performance(self) -> None:
        """
        compare performance with auto_tiling

        Parameters
        ----------
        data_set: Previous tuning records, array of result pair
        """
        if self.quit_due_to_malloc_error:
            self.better_than_online = False
            return
        # compare performance of auto tiling, cost model tiling and tuning tiling
        self.costtime_baseline = DEFAULT_OM_RUN_THRESHOLD
        tiling_rets, best_model_tiling_index, topn_ret = self.measure_performance()
        auto_tiling_result = tiling_rets.auto_tiling_ret
        tuning_tiling_result = tiling_rets.tuning_tiling_ret
        cost_model_tiling_result = tiling_rets.cost_model_tiling_ret
        dsl_cache_tiling_result = tiling_rets.dsl_cache_tiling_ret
        default_result = tiling_rets.default_ret
        tuning_topn_individuals, cost_model_tiling_topn_list = topn_ret.tuning_topn, topn_ret.cost_model_topn

        before_res = min(auto_tiling_result, default_result)
        better_than_online = True

        best_tiling_flag, repo_update_flag = self.get_tiling_compare_result(tiling_rets, best_model_tiling_index)
        tuning_tilig_best = best_tiling_flag.best_tuning
        cost_model_tiling_best = best_tiling_flag.best_cost_model
        dsl_cache_tiling_best = best_tiling_flag.best_dsl_cache

        # only support dw and not support strided_write , due to can't get right datacmp result
        is_support_op = "conv2dbackpropfilterd" in self.task.param.get("kernel_name") and \
            "strided_write" not in self.task.param.get("kernel_name")
        # data compare when the repository needs to be updated
        if is_support_op and (tuning_tilig_best or cost_model_tiling_best or dsl_cache_tiling_best or repo_update_flag):
            dc_ret, best_model_tiling_index = self.data_compare(tuning_topn_individuals, cost_model_tiling_topn_list)
            tuning_tiling_result = dc_ret.tuning_tiling_ret
            cost_model_tiling_result = dc_ret.cost_model_tiling_ret
            dsl_cache_tiling_result = dc_ret.dsl_cache_tiling_ret
            if int(tuning_tiling_result) == COST_MAX_VALUE and int(cost_model_tiling_result) == COST_MAX_VALUE and \
                int(dsl_cache_tiling_result) == COST_MAX_VALUE:
                self.better_than_online = False
                self.best_individual = None
                self.tune_res_report = TuneResReport(before_res, COST_MAX_VALUE, False, self.exist_repository)
                return
            best_tiling_flag, repo_update_flag = self.get_tiling_compare_result(tiling_rets, best_model_tiling_index)
            tuning_tilig_best = best_tiling_flag.best_tuning
            cost_model_tiling_best = best_tiling_flag.best_cost_model
            dsl_cache_tiling_best = best_tiling_flag.best_dsl_cache

        if tuning_tilig_best: # tuning_tiling
            self.best_individual.measure_result.cost = tuning_tiling_result
        elif dsl_cache_tiling_best: # dsl_cache_tiling
            self.update_best_individual_by_dsl_cache_tiling(dsl_cache_tiling_result)
        elif cost_model_tiling_best: # cost_model_tiling
            self.update_best_individual_by_cost_model_tiling(cost_model_tiling_result, best_model_tiling_index)
        elif repo_update_flag: # tuning_tiling or cost_model_tiling
            if tiling_better_than(tuning_tiling_result, cost_model_tiling_result) and \
                tiling_better_than(tuning_tiling_result, dsl_cache_tiling_result):
                self.best_individual.measure_result.cost = tuning_tiling_result
            elif tiling_better_than(dsl_cache_tiling_result, cost_model_tiling_result):
                self.update_best_individual_by_dsl_cache_tiling(dsl_cache_tiling_result)
            else:
                self.update_best_individual_by_cost_model_tiling(cost_model_tiling_result, best_model_tiling_index)
        else:
            LOG_INSTANCE.info("The performance of kernel %s is already fine, there's no need to update repository!",
                self.task.param.get("kernel_name"))
            LOG_PROGRESS_INSTANCE.info("The performance of kernel %s is already fine, "
                "there's no need to update repository!", self.task.param.get("kernel_name"))
            better_than_online = False

        # compare the cost between the best tiling and default
        if better_than_online and self.best_individual and self.best_individual.measure_result.cost > default_result:
            LOG_INSTANCE.info(
                "[%s] the best cost[%s] is not better than default[%s], there's no need to update repository.",
                self.task.param.get("kernel_name"), self.best_individual.measure_result.cost, default_result)
            before_res = default_result
            better_than_online = False

        if before_res >= COST_MAX_VALUE:
            better_than_online = False
        # update self.better_than_online
        self.better_than_online = better_than_online
        # update tune res report status
        after_res = COST_MAX_VALUE
        if self.best_individual:
            after_res = self.best_individual.measure_result.cost
        self.tune_res_report = TuneResReport(before_res, after_res, better_than_online, self.exist_repository)

    def performance_evaluate(self, tiling_population: List[tuple]) -> None:
        """
        Evaluate the performance of tiling

        Parameters
        ----------
        topi_args: params of topi interface
        tiling_population: tiling generated by optimization algorithm

        Returns
        -------
        ret:
        state: -1:compile failed; 0:run failed; 1:success
        indicate whether the evaluation is successful or unsuccessful
        total_time: record the runtime cost of .o, us
        """
        base_kernel_name = self.task.topi_args["fusion_op_name"]
        kernel = []
        for tiling_point in tiling_population:
            tiling_entity = self.task.point2entity(tiling_point)
            tiling_dict = self.task.tiling_transform(tiling_entity)
            e_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_point": tiling_point,
                "tiling_entity": tiling_entity,
                "tiling_dict": tiling_dict,
                "tuning_mode": "GA"
            }
            LOG_INSTANCE.debug("Start to build kernel: %s, tiling: %s",
                               e_kernel.get("kernel_name"), e_kernel.get("tiling_dict"))
            kernel.append(e_kernel)
        self.results, self.inputs, _ = self.execute_kernel_list(kernel)

    def execute_kernel_list(self, kernel_list: List[dict], is_data_cmp: bool = False, is_base: bool = False) -> tuple:
        """
        :description excute kernel list
        :param kernel_list
        """
        cost_time = self.default_time_bound
        if self.run_optune:
            res_list, cost_time = self.build_and_execute(kernel_list, is_data_cmp, is_base)
            if res_list:
                result_list, input_list = self.get_build_res(res_list, kernel_list)
            else:
                result_list, input_list = [], []
                LOG_INSTANCE.error("Build and excute get empty results list.")
        else:
            result_list, input_list = self.build_and_run(self.task.topi_args, kernel_list)
        return result_list, input_list, cost_time

    def get_datacmp_results(self, tuning_topn_individuals: list,
            cost_model_tiling_topn_list: List[tuple], base_kernel_name: str) -> list:
        """
        get topn tuning tiling individuals and topn cost model data compare result
        """
        kernel_list = []
        for individual in tuning_topn_individuals:
            tmp_measure_dict = copy.deepcopy(individual.tiling_dict)
            tiling_entity = copy.deepcopy(individual.tiling_entity)
            tuning_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_entity": tiling_entity,
                "tiling_dict": tmp_measure_dict,
                "tuning_mode": "GA"
            }
            kernel_list.append(tuning_kernel)
        for cost_model_tiling_i in enumerate(cost_model_tiling_topn_list):
            cost_model_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_dict": self.cost_model_tiling[cost_model_tiling_i[0]],
                "tuning_mode": "GA"
            }
            kernel_list.append(cost_model_kernel)
        if self.dsl_cache_tiling:
            dsl_cache_kernel = {
                "kernel_name": "%s_%s" % (base_kernel_name, get_time_str()),
                "tiling_dict": self.dsl_cache_tiling,
                "tuning_mode": "GA"
            }
            kernel_list.append(dsl_cache_kernel)
        results, _, _ = self.execute_kernel_list(kernel_list, True)
        if len(kernel_list) != len(results):
            LOG_INSTANCE.error("The length of executed kernels(%s) is not equal to the length of results(%s), "
                "kernel name is %s.", str(len(kernel_list)), str(len(results)), base_kernel_name)
            return []
        if not self.dsl_cache_tiling:
            res_pair = MeasureResult(point=None, cost=COST_MAX_VALUE, verify=False)
            results.append(res_pair)
        return results

    def get_passnum_and_overhead_results(self, base_kernel_name: str) -> int:
        """
        get passparam and overhead kernel run results
        """
        kernel_list = self.get_kernels_for_passparam(base_kernel_name, self.pass_param_num)
        tuning_tiling_result, _ = self.get_tiling_result(kernel_list)
        # tune A/B_overhead_opt_flag
        if self.overhead_opt_tune:
            kernels = self.get_kernels_for_overhead(base_kernel_name)
            tuning_tiling_result, _ = self.get_tiling_result(kernels)
        if len(self.results) != len(self.inputs):
            LOG_INSTANCE.error("The length of executed results(%s) is not equal to the length of inputs(%s), "
                "kernel name is %s.", str(len(self.results)), str(len(self.inputs)), base_kernel_name)
            return COST_MAX_VALUE

        results_and_inputs = []
        for idx in range(len(self.results)):
            results_and_inputs.append([self.results[idx], self.inputs[idx]])
        self.topn_result.extend(sorted(results_and_inputs, key = lambda res: res[0].cost)[0 : TOPN_FOR_DATACMP])
        return tuning_tiling_result

    def get_tiling_result(self, kernel: List[dict], is_data_cmp: bool = False, is_base: bool = False) -> tuple:
        """
        get tiling results
        """
        self.results, self.inputs, cost_time = self.execute_kernel_list(kernel, is_data_cmp, is_base)
        tiling_result = COST_MAX_VALUE
        idx = 0
        if self.results:
            best_tiling_result = min(self.results, key=lambda x: x.cost)
            idx = self.results.index(best_tiling_result)
            tiling_result = best_tiling_result.cost
        # when self.pass_num > 0, the number of tasks executed is greater than 1
        if len(self.inputs) > 1:
            measure_obj = copy.deepcopy(self.best_individual.measure_result)
            self.best_individual = BestIndividual(
                self.inputs[idx], self.task.tiling_transform(self.inputs[idx]), measure_obj)
        return tiling_result, cost_time

    def get_build_res(self, run_result: List[dict], kernel_list: List[dict]) -> tuple:
        """
        get build_and_execute results
        """
        input_list = []
        result_list = []
        success_kernel_cnt = 0
        if not run_result:
            return result_list, input_list

        for idx, res in enumerate(run_result):
            op_cost_time = self.get_op_cost_time(res)
            kernel_dict = kernel_list[idx]
            if op_cost_time == INVALID_COST_TIME:
                self.quit_due_to_malloc_error = True
                return [], []
            if op_cost_time > 0:
                success_kernel_cnt = success_kernel_cnt + 1
                res_pair = MeasureResult(point=kernel_dict.get("tiling_point", None), cost=op_cost_time, \
                    verify=res.get("verify", True))
                result_list.append(res_pair)
                input_list.append(kernel_dict.get('tiling_entity'))
            else:
                LOG_INSTANCE.warning("[%s] op_cost_time is smaller than zero." % str(kernel_dict.get('kernel_name')))
                LOG_INSTANCE.warning("[%s] tiling_entity: %s" % (str(kernel_dict.get('kernel_name')),
                                                                 str(kernel_dict.get('tiling_entity'))))
                LOG_INSTANCE.warning("[%s] tiling_point: %s" % (str(kernel_dict.get('kernel_name')),
                                                                str(kernel_dict.get('tiling_point'))))
                LOG_INSTANCE.warning("[%s] get_b_dtype: %s" % (str(kernel_dict.get('kernel_name')),
                                                               str(self.task.get_b_dtype())))
                LOG_INSTANCE.warning("[%s] tiling_dict: %s" % (str(kernel_dict.get('kernel_name')),
                                                               str(kernel_dict.get('tiling_dict'))))

        success_rate = success_kernel_cnt / len(run_result)
        if success_rate < SUCCESS_RATE_CONTROL:
            LOG_INSTANCE.error("Build_and_execute success_rate[%s] is lower than [%s]." %
                               (str(success_rate), str(SUCCESS_RATE_CONTROL)))
            for idx, res in enumerate(run_result):
                LOG_INSTANCE.error("Tiling index is [%s]: OpName is %s, TotalCostTime is %sns", str(idx),
                    str(res.get("opName", "")), str(res.get("totalCostTime", "")))
                LOG_INSTANCE.error("OpCostTime : %s", str(res.get("opCostTime", "")))
                LOG_INSTANCE.error("AicoreCostTime : %s", str(res.get("aiCoreCostTime", "")))
            result_list = []
            input_list = []
        return result_list, input_list

    def build_and_run(self, topi_args: dict, kernel_list: List[dict]) -> None:
        """
        Build and run the kernel

        Parameters
        ----------
        topi_args: params of topi interface
        kernel_list: tiling generated by optimization algorithm

        Returns
        -------
        ret: state code, three state
        state: -1:compile failed; 0:run failed; 1:success
        indicate whether the evaluation is successful or unsuccessful
        total_time: number (us)
        record the runtime cost of .o
        """
        # generate the .o file and json file
        config_file = os.path.join(ESTIMATE_CONFIG_PATH, 'config.json')
        op_param_info = {}
        is_default_build = len(kernel_list) == 1 and not kernel_list[0].get('tiling_dict')
        with open(config_file, "r") as file_handle:
            op_param_info = json.load(file_handle)
        op_info_list = []
        kernel_compile_op = []
        for e_kernel in kernel_list:
            kernel_tiling_dict = {} if is_default_build else {e_kernel.get('kernel_name'): e_kernel.get('tiling_dict')}
            t_op, cfg_dict = atc_build_kernel(topi_args, e_kernel, kernel_tiling_dict)
            kernel_compile_op.append(t_op)
            op_info_list.append(cfg_dict)
        kernel_flag = False
        while not kernel_flag:
            for c_op in kernel_compile_op:
                if c_op.get() is None:
                    kernel_flag = False
                    break
                kernel_flag = True
        return self.build_run_ret_parse(op_param_info, op_info_list, kernel_compile_op, topi_args)

    def build_run_ret_parse(self, op_param_info: dict, op_info_list: List[dict],
        kernel_compile_op: List[dict], topi_args: dict) -> tuple:
        """
        parsing the result of default build and run
        """
        op_param_info["op_info_list"] = []
        for op_param in op_info_list:
            kernel_path = os.path.join(get_kernel_meta_dir(), "%s.o" % op_param.get('kernel_name', None))
            if os.path.isfile(kernel_path):
                op_param_info["op_info_list"].append(op_param)
        results, inputs = [], []
        if len(op_param_info.get("op_info_list")) < len(op_info_list) * SUCCESS_RATE_CONTROL:
            self._log_compile_err(kernel_compile_op)
            LOG_INSTANCE.error("compile .o failed.")
            return results, inputs

        if self.feature_config.run_type == RUN_TYPE_QTEST:
            ret_status, results, inputs = qtest_run(topi_args, op_param_info)
        else:
            ret_status, results, inputs = parallel_run(topi_args, op_param_info)

        if not ret_status.ret:
            results, inputs = [], []
            ret_info = ret_status.ret_info.replace("\n", "")
            LOG_INSTANCE.error_multi(";", "run .o failed. %s", ret_info)
        return results, inputs

    def build_and_execute(self, kernel_list: List[dict], is_data_cmp: bool = False, is_base: bool = False) -> tuple:
        """
        Build and execute the om

        Parameters
        ----------
        kernel_list: tiling generated by optimization algorithm

        Returns
        -------
        """
        cb_list = []
        for kernel in kernel_list:
            cb_list.append(str(kernel))
        if not cb_list:
            cb_list = ['{}']

        cb_key = "GA_%s_%s" % (os.getpid(), get_time_str())
        cb_dict = {
            cb_key: {"cb_struct_key": self.cb_struct_key,
                     "strategy_list": cb_list,
                     "need_integrate": self.feature_config.need_integrate,
                     "tune_mode": "GA",
                     "costtime_baseline": self.costtime_baseline}
        }
        self.timer_queue.put(("start", os.getpid(), cb_key))
        cb_queue = self.cb_queue if not is_data_cmp else self.cb_datacmp_queue
        cb_result = self.cb_result if not is_data_cmp else self.cb_datacmp_result
        cb_queue.put(cb_dict)
        build_begin = time.time()
        result = None
        strategy_num = max(len(kernel_list), 50)
        while True:
            build_run_time = time.time() - build_begin
            if cb_key in cb_result:
                result = copy.deepcopy(cb_result.get(cb_key))
                if not is_data_cmp:
                    self.cb_release.put(cb_key)
                break
            if not is_base:
                time_bound = max(strategy_num * self.build_run_time_bound, self.default_time_bound)
                if build_run_time > time_bound:
                    result = [{} for _ in kernel_list]
                    LOG_INSTANCE.warning(
                        "Auto_tune build_and_execute[cb_key:%s][cb_struct_key:%s] has been waiting longer than %d!",
                        cb_key, self.cb_struct_key, time_bound)
                    break
            time.sleep(TIME_SLEEP)
        self.timer_queue.put(("stop", os.getpid(), cb_key))
        for idx, _ in enumerate(kernel_list):
            kernel_name = kernel_list[idx].get("kernel_name", "kernel")
            LOG_INSTANCE.debug("build_and_execute kernel[%s] res:%s" % (
                kernel_name, str(result[idx] if (isinstance(result, list) and len(result) > idx) else None)))
            LOG_INSTANCE.debug("[%s] tiling_dict: %s" % (kernel_name, str(kernel_list[idx].get("tiling_dict", None))))
            LOG_INSTANCE.debug("[%s] get_b_dtype: %s" % (kernel_name, str(self.task.get_b_dtype())))
            LOG_INSTANCE.debug("[%s] tiling_entity: %s" % (kernel_name,
                str(kernel_list[idx].get('tiling_entity', None))))
            LOG_INSTANCE.debug("[%s] tiling_point: %s" % (kernel_name, str(kernel_list[idx].get('tiling_point', None))))
        return result, build_run_time
