#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from typing import List, Dict, Tuple, Union
from llm_datadist.utils import log
from llm_datadist.configs import LLMReq
from llm_datadist.v2.llm_types import ModelRunner
from llm_datadist.v2.llm_utils import get_req_desc
from llm_datadist.v2.offline_model_runner import OfflineModelRunner, SwitchableOfflineModelRunner


class MixModel:
    def __init__(self, model_id: int, model_options: Dict[str, str]):
        self._model_id = model_id
        self._model_options = model_options
        self._model_runner = None
        self._need_req_desc = False

    def init(self, model_runner: ModelRunner, _):
        self._model_runner = model_runner
        self._need_req_desc = isinstance(model_runner, (OfflineModelRunner, SwitchableOfflineModelRunner))

    def finalize(self):
        log.info(f'Finalizing model {self._model_id}')

    def predict(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]], inputs: Union[Tuple, List], **kwargs) -> List:
        log.info('[model:%d] [predict] start, req_ids = %s', self._model_id, [llm_req.req_id for llm_req in llm_reqs])
        if self._need_req_desc:
            kwargs['_req_desc'] = get_req_desc(llm_reqs)
        output_tensors = self._model_runner.run_model(None, inputs, **kwargs)
        log.info('[predict] success')
        return output_tensors

    def complete_request(self, llm_req: LLMReq) -> None:
        log.info('[model:%d] [complete_request] success, req_id = %d', self._model_id, llm_req.req_id)
