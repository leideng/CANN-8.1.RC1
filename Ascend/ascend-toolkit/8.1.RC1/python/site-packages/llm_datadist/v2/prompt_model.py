#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from typing import List, Dict, Tuple, Union
from llm_datadist.utils import log
from llm_datadist.status import raise_if_false, LLMStatusCode
from llm_datadist.configs import LLMReq
from llm_datadist.v2.kv_cache_manager import KvCacheManager
from llm_datadist.v2.llm_types import CacheDesc, KvCache, ModelRunner, CacheKey
from llm_datadist.v2.llm_utils import CacheDescParser, clone_cache_desc, is_invalid_id, calc_tensor_size, get_req_desc, \
    is_valid_id
from llm_datadist.v2.offline_model_runner import OfflineModelRunner, SwitchableOfflineModelRunner
from llm_datadist.v2._token_manager import TokenManager, RequestGroup


class PrefixInfo:
    def __init__(self, kv_cache: KvCache, size: int, cache_keys):
        self.kv_cache = kv_cache
        self.size = size
        self.cache_keys = cache_keys


class PromptModel:
    def __init__(self, model_id: int, model_options: Dict[str, str]):
        self._model_id = model_id
        self._model_options = model_options
        self._enable_shared_system_prefix = "llm.EnableSharedSystemPrefix" in model_options and \
                                            model_options["llm.EnableSharedSystemPrefix"] == "1"
        self._cache_desc = CacheDescParser.parse_by_options(model_options)
        self._is_dyn_seq_len = (self._cache_desc.seq_len_dim_index > 0 and
                                self._cache_desc.shape[self._cache_desc.seq_len_dim_index] == -1)
        self._prefix_id_to_prefix_info: Dict[int, PrefixInfo] = {}
        self._req_id_to_cache_key: Dict[int, CacheKey] = {}
        self._model_runner = None
        self._kv_cache_manager = None
        self._need_req_desc = False
        self._token_manager = None
        self._mem_size_per_token = self._get_mem_size_per_token(self._cache_desc)

    @property
    def mem_size_per_token(self) -> int:
        return self._mem_size_per_token

    @staticmethod
    def _get_mem_size_per_token(cache_desc: CacheDesc) -> int:
        kv_tensor_format = cache_desc.kv_tensor_format
        if kv_tensor_format is None:
            return 0
        cache_shape = cache_desc.shape
        raise_if_false(len(cache_shape) == len(kv_tensor_format),
                       f"llm.kvTensorFormat:{kv_tensor_format} not match kv shape:{cache_shape}")
        batch_value = cache_shape[kv_tensor_format.index("B")]
        seq_value = cache_shape[kv_tensor_format.index("S")]
        # per token size include k tensor and v tensor size all layers
        return (cache_desc.size / (batch_value * seq_value)) * cache_desc.num_tensors

    def init(self, model_runner: ModelRunner, kv_cache_manager: KvCacheManager, token_manager: TokenManager):
        self._model_runner = model_runner
        self._kv_cache_manager = kv_cache_manager
        self._need_req_desc = isinstance(model_runner, (OfflineModelRunner, SwitchableOfflineModelRunner))
        self._token_manager = token_manager

    def finalize(self):
        log.info(f'Finalizing model {self._model_id}')

    def predict(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]], inputs: Union[Tuple, List], **kwargs) -> List:
        log.info('[model:%d] [predict] start, req_ids = %s', self._model_id, [llm_req.req_id for llm_req in llm_reqs])
        cache_keys = [CacheKey(llm_req.prompt_cluster_id, llm_req.req_id, self._model_id) for llm_req in llm_reqs]
        cache_desc = self._prepare_cache_desc(llm_reqs)
        kv_cache = self._kv_cache_manager.allocate_cache(cache_desc, cache_keys)
        self._token_manager.allocate(
            RequestGroup(llm_reqs, self._model_id, self.mem_size_per_token, self._cache_desc.batch_size))
        success = False
        try:
            output_tensors = self._predict_with_kv_cache(kv_cache, llm_reqs, inputs, **kwargs)
            success = True
        finally:
            # predict异常, remove绑定的key
            if not success:
                log.error('[model:%d] [predict] failed, release kv cache, req_ids = %s',
                          self._model_id, [llm_req.req_id for llm_req in llm_reqs])
                for cache_key in cache_keys:
                    if is_valid_id(cache_key.req_id):
                        self._kv_cache_manager.remove_cache_key(cache_key)
                for req in llm_reqs:
                    self._token_manager.free(req, self._model_id, self._cache_desc.batch_size)
            # 已通过cache_key持有，释放cache_id到kv_cache的引用
            self._kv_cache_manager.deallocate_cache(kv_cache)
        for req, cache_key in zip(llm_reqs, cache_keys):
            self._req_id_to_cache_key[req.req_id] = cache_key
        log.info('[predict] success')
        return output_tensors

    def _predict_with_kv_cache(self,
                               kv_cache: KvCache,
                               llm_reqs: Union[List[LLMReq], Tuple[LLMReq]],
                               inputs: Union[Tuple, List],
                               **kwargs) -> List:
        for batch_index, llm_req in enumerate(llm_reqs):
            if is_invalid_id(llm_req.req_id):
                continue
            if is_valid_id(llm_req.prefix_id) and not self._enable_shared_system_prefix:
                prefix_info = self._get_prefix_info(llm_req.prefix_id)
                self._kv_cache_manager.copy_cache(kv_cache, prefix_info.kv_cache, batch_index, 0, 0, prefix_info.size)
                log.info('copy prefix kv cache success, prefix_id = %d, '
                         'prefix_cache_id = %d, prefix kv shape = %s, size = %d',
                         llm_req.prefix_id, prefix_info.kv_cache.cache_id,
                         prefix_info.kv_cache.cache_desc.shape, prefix_info.size)
        if self._need_req_desc:
            kwargs['_req_desc'] = get_req_desc(llm_reqs)
        if self._enable_shared_system_prefix:
            prefix_info = self._get_prefix_info(llm_req.prefix_id)
            kwargs['prefix_kv_cache'] = prefix_info.kv_cache
        output_tensors = self._model_runner.run_model(kv_cache, inputs, **kwargs)
        return output_tensors

    def preload_prompt_prefix(self, llm_req: LLMReq, inputs: Union[Tuple, List], **kwargs) -> None:
        log.info('[model:%d] [preload_prompt_prefix] start, prefix_id = %s', self._model_id, llm_req.prefix_id)
        raise_if_false(is_invalid_id(llm_req.req_id), 'invalid req id: {0}, should be 2**64-1', llm_req.req_id)
        raise_if_false(is_valid_id(llm_req.prefix_id), 'invalid prefix id: {0}', llm_req.prefix_id)
        raise_if_false(llm_req.prefix_id not in self._prefix_id_to_prefix_info,
                       '[model:{0}] prefix_id {1} already exist',
                       self._model_id, llm_req.prefix_id,
                       status_code=LLMStatusCode.LLM_PREFIX_ALREADY_EXIST)
        cache_desc = clone_cache_desc(self._prepare_cache_desc((llm_req,)))
        need_shrunk = (cache_desc.batch_size > 1)
        cache_keys = [CacheKey(llm_req.prompt_cluster_id, llm_req.req_id, prefix_id=llm_req.prefix_id)]
        kv_cache = self._kv_cache_manager.allocate_cache(cache_desc, () if need_shrunk else cache_keys)
        if self._need_req_desc:
            kwargs['_req_desc'] = 'prefix_id={0}'.format(llm_req.prefix_id)
        _ = self._model_runner.run_model(kv_cache, inputs, **kwargs)
        if need_shrunk:
            log.info('prefix kv shape = %s, need to shrink batch_size to 1', cache_desc.shape)
            shrunk_cache_desc = clone_cache_desc(cache_desc)
            shrunk_cache_desc.update_dim(shrunk_cache_desc.batch_dim, 1)
            shrunk_kv_cache = self._kv_cache_manager.allocate_cache(shrunk_cache_desc, cache_keys)
            self._kv_cache_manager.copy_cache(shrunk_kv_cache, kv_cache)
            self._kv_cache_manager.deallocate_cache(kv_cache)
            kv_cache = shrunk_kv_cache
            cache_desc = shrunk_cache_desc
            prefix_mem_size_per_token = self._get_mem_size_per_token(shrunk_cache_desc)
            self._token_manager.allocate(
                RequestGroup([llm_req], self._model_id, prefix_mem_size_per_token, 1))  # prefix只占用单batch的大小
        copy_size = -1
        # BSH排布, copy时候按实际大小
        if cache_desc.seq_len_dim_index == 1:
            valid_shape = cache_desc.shape[:]
            valid_shape[cache_desc.seq_len_dim_index] = llm_req.prompt_length
            copy_size = calc_tensor_size(valid_shape, cache_desc.data_type)
            log.info('[preload_prompt_prefix] cache_shape = %s, dtype = %s, prompt_length = %d, copy_size = %d',
                     cache_desc.shape, cache_desc.data_type, llm_req.prompt_length, copy_size)
        prefix_info = PrefixInfo(kv_cache, copy_size, cache_keys)
        self._prefix_id_to_prefix_info[llm_req.prefix_id] = prefix_info
        log.info('[model:%d] [preload_prompt_prefix] success, prefix_id = %d, cache_id = %d, prompt_length = %d',
                 self._model_id, llm_req.prefix_id, kv_cache.cache_id, llm_req.prompt_length)

    def release_prompt_prefix(self, llm_req: LLMReq) -> None:
        raise_if_false(is_valid_id(llm_req.prefix_id),
                       '[model:{0}] [release_prompt_prefix] failed, invalid prefix id: {1}',
                       self._model_id, llm_req.prefix_id)
        raise_if_false(llm_req.prefix_id in self._prefix_id_to_prefix_info,
                       '[model:{0}] prefix_id {1} not exist',
                       self._model_id, llm_req.prefix_id,
                       status_code=LLMStatusCode.LLM_PREFIX_NOT_EXIST)
        prefix_info = self._prefix_id_to_prefix_info.pop(llm_req.prefix_id)
        self._kv_cache_manager.deallocate_cache(prefix_info.kv_cache)
        [self._kv_cache_manager.remove_cache_key(cache_key) for cache_key in prefix_info.cache_keys]
        self._token_manager.free(llm_req, self._model_id, 1) # prefix只占用单batch的大小
        log.info('[model:%d] [release_prompt_prefix] success, prefix_id = %d', self._model_id, llm_req.prefix_id)

    def complete_request(self, llm_req: LLMReq) -> None:
        # user call complete or pull kv from decoder will free cache memory.
        cache_key = self._req_id_to_cache_key.pop(llm_req.req_id, None)
        if cache_key is not None:
            self._kv_cache_manager.remove_cache_key(cache_key)
            self._token_manager.free(llm_req, self._model_id, self._cache_desc.batch_size)
        log.info('[model:%d] [complete_request] success, req_id = %d', self._model_id, llm_req.req_id)

    def _get_prefix_info(self, prefix_id: int) -> PrefixInfo:
        prefix_info = self._prefix_id_to_prefix_info.get(prefix_id, None)
        raise_if_false(prefix_info is not None,
                       '[model:{0}] prefix info not found, prefix id = {1}', self._model_id, prefix_id)
        return prefix_info

    def _prepare_cache_desc(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]]) -> CacheDesc:
        cache_desc = self._cache_desc
        if self._is_dyn_seq_len:
            max_seq_len = 0
            for llm_req in llm_reqs:
                if llm_req.prompt_length > max_seq_len:
                    max_seq_len = llm_req.prompt_length
            cache_desc = clone_cache_desc(self._cache_desc)
            cache_desc.update_dim(self._cache_desc.seq_len_dim_index, max_seq_len)
            log.info('update shape from %s to %s', self._cache_desc.shape, cache_desc.shape)
        return cache_desc
