#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================
import copy
import json
from typing import Any, Dict, List, Tuple, Optional, Union
import atexit
from llm_datadist import llm_wrapper
from llm_datadist.utils.utils import check_isinstance, check_dict, check_uint64, check_int32, check_uint32
from llm_datadist.utils import log
from llm_datadist.status import (code_2_status, handle_llm_status, raise_if_false, raise_if_true, LLMStatusCode,
                                 LLMException)
from llm_datadist.configs import LLMRole, LLMClusterInfo, LLMReq
from llm_datadist.v2.config import EngineConfig
from llm_datadist.v2.decoder_pa_model import DecoderPAModel
from llm_datadist.v2.prompt_pa_model import PromptPAModel
from llm_datadist.v2.llm_types import ModelRunner, CapacityState, RegisterMemStatus, int_to_mem_status_dict
from llm_datadist.v2.offline_model_runner import SwitchableOfflineModelRunner, PAModelRunner
from llm_datadist.v2.decoder_model import DecoderModel
from llm_datadist.v2.prompt_model import PromptModel
from llm_datadist.v2.mix_model import MixModel
from llm_datadist.v2.kv_cache_manager import KvCacheManager
from llm_datadist.v2.cache_manager import CacheManager
from llm_datadist.v2._token_manager import TokenManager

__all__ = ['LLMModel', 'LLMDataDist', 'KvCacheManager']
_MAX_CLUSTER_NAME = 128
_MAX_NODE_NUM = 4


class LLMModel:
    def __init__(self, role: LLMRole, model_id: int, cluster_id: int, model_options: Dict[str, str]) -> None:
        self._role = role
        self._model_id = model_id
        self._cluster_id = cluster_id
        self._enable_pa = "llm.EnablePagedAttention" in model_options and \
                          model_options["llm.EnablePagedAttention"] == "1"
        self._model_runner: Optional[ModelRunner] = None
        self._kv_cache_manager: Optional[KvCacheManager] = None
        self._token_manager: Optional[TokenManager] = None
        self._model_options: Dict[str, str] = model_options
        self._model = None
        self._prompt_model = None
        self._decoder_model = None

    def init(self, model_runner: ModelRunner, kv_cache_manager: KvCacheManager, token_manager: TokenManager = None):
        self._model_runner = model_runner
        self._kv_cache_manager = kv_cache_manager
        self._token_manager = token_manager
        self._switch_role(self._role)

    @property
    def mem_size_per_token(self) -> int:
        if self._role == LLMRole.PROMPT and isinstance(self._prompt_model, PromptModel):
            return self._prompt_model.mem_size_per_token
        return 0

    def finalize(self):
        if self._model is not None:
            self._model.finalize()

    def predict(self,
                llm_req: Union[LLMReq, List[LLMReq], Tuple[LLMReq]],
                inputs: Any,
                **kwargs) -> List:
        """
        执行模型推理

        Args:
            llm_req (Union[LLMReq, List[LLMReq], Tuple[LLMReq]]): 请求,
                对于Decoder，其个数需要与模型的batch_size相等, 空闲的位置需要用req_id = UINT64_MAX的占位，
                对于Prompt，最后一个有效Req之后的无效Req可以省略
            inputs (Any): 模型输入, 会透传给ModelRunner.run_model方法,
                          如果使用默认的ModelRunner, inputs的类型需要为Union[Tuple[Tensor], List[Tensor]]
            kwargs (Optional[Dict]): 可选参数, 会透传给ModelRunner.run_model方法

        Returns:
            模型输出
        """
        llm_reqs = (llm_req,) if isinstance(llm_req, LLMReq) else llm_req
        check_isinstance('llm_reqs', llm_reqs, [list, tuple], LLMReq)
        raise_if_false(len(llm_reqs) > 0, 'llm_reqs can not be empty.')
        return self._model.predict(llm_reqs, inputs, **kwargs)

    def pull_kv(self, llm_req: LLMReq) -> None:
        """
        拉取KV到暂存区, 每个LLMModel对象独占一个暂存区. 仅当LLMRole为DECODER时可以使用

        Args:
            llm_req: LLMReq
        """
        raise_if_false(not self._enable_pa,
                       "This api is not supported when you have 'llm.EnablePagedAttention' option configured.")
        check_isinstance('llm_req', llm_req, LLMReq)
        self._check_role('[pull_kv]', LLMRole.DECODER)
        self._decoder_model.pull_kv(llm_req)

    def pull_blocks(self, llm_req: LLMReq, prompt_blocks: List[int] = [], decoder_blocks: List[int] = []) -> None:
        """
        拉取KV到暂存区, 每个LLMModel对象独占一个暂存区. 仅当LLMRole为DECODER时可以使用

        Args:
            llm_req: LLMReq
            prompt_blocks: prompt block list
            decoder_blocks: decoder block list
        """
        raise_if_false(self._enable_pa,
                       "This api is not supported when you don't have the 'llm.EnablePagedAttention' option configured.")
        check_isinstance('llm_req', llm_req, LLMReq)
        check_isinstance('prompt_blocks', prompt_blocks, list, int)
        check_isinstance('decoder_blocks', decoder_blocks, list, int)
        self._check_role('[pull_kv]', LLMRole.DECODER)
        raise_if_false(len(prompt_blocks) > 0, "prompt_blocks can not be empty.")
        raise_if_false(len(decoder_blocks) > 0, "decoder_blocks can not be empty.")
        self._decoder_model.pull_blocks(llm_req, prompt_blocks, decoder_blocks)

    def merge_kv(self, llm_req: LLMReq, batch_index: int, batch_id: int = 0) -> None:
        """
        将KV从暂存区merge到Kv Cache的指定位置

        Args:
            llm_req: LLMReq
            batch_index: KV Cache的batch index
            batch_id: 使能pipeline stage调度时设置, 指定是第几个KV Cache
        """
        raise_if_false(not self._enable_pa,
                       "This api is not supported when you have 'llm.EnablePagedAttention' option configured.")
        check_isinstance('llm_req', llm_req, LLMReq)
        check_isinstance('batch_index', batch_index, int)
        check_isinstance('batch_id', batch_id, int)
        self._check_role('[merge_kv]', LLMRole.DECODER)
        self._decoder_model.merge_kv(llm_req, batch_index, batch_id)

    def complete_request(self, llm_req: LLMReq) -> None:
        """
        结束请求，释放相关资源
        Args:
            llm_req: LLMReq
        """
        check_isinstance('llm_req', llm_req, LLMReq)
        self._model.complete_request(llm_req)

    def preload_prompt_prefix(self, llm_req: LLMReq, inputs: Any, **kwargs) -> None:
        """
        预加载前缀, 仅当LLMRole为PROMPT时可以使用

        Args:
            llm_req (LLMReq): LLMReq, 需要设置prefix_id
            inputs (Any): 模型输入, 会透传给ModelRunner.run_model方法
            kwargs (Optional[Dict]): 可选参数, 会透传给ModelRunner.run_model方法
        """
        raise_if_false(not self._enable_pa,
                       "This api is not supported when you have 'llm.EnablePagedAttention' option configured.")
        check_isinstance('llm_req', llm_req, LLMReq)
        self._check_role('[preload_prompt_prefix]', LLMRole.PROMPT)
        self._prompt_model.preload_prompt_prefix(llm_req, inputs, **kwargs)

    def release_prompt_prefix(self, llm_req: LLMReq) -> None:
        """
        释放预加载的前缀KV, 仅当LLMRole为PROMPT时可以使用

        Args:
            llm_req: LLMReq, 需要设置prefix_id
        """
        raise_if_false(not self._enable_pa,
                       "This api is not supported when you have 'llm.EnablePagedAttention' option configured.")
        check_isinstance('llm_req', llm_req, LLMReq)
        self._check_role('[release_prompt_prefix]', LLMRole.PROMPT)
        self._prompt_model.release_prompt_prefix(llm_req)

    def _check_role(self, func_name, role: LLMRole) -> None:
        raise_if_false(self._role == role,
                       '{0} is not supported by {1}',
                       func_name, self._role)

    def _switch_role(self, role: LLMRole) -> None:
        self._role = role
        self._prompt_model = None
        self._decoder_model = None
        if self._model is not None:
            self._model.finalize()
        if isinstance(self._model_runner, SwitchableOfflineModelRunner):
            self._model_runner.switch_role(role)
        if role == LLMRole.PROMPT:
            if not self._enable_pa:
                self._model = PromptModel(self._model_id, self._model_options)
                self._prompt_model = self._model
                self._model.init(self._model_runner, self._kv_cache_manager, self._token_manager)
                return
            self._model = PromptPAModel(self._model_id, self._cluster_id, self._model_options)
            self._prompt_model = self._model
        elif role == LLMRole.DECODER:
            self._model = DecoderPAModel(self.model_id, self._model_options) if self._enable_pa else \
                DecoderModel(self._model_id, self._model_options)
            self._decoder_model = self._model
        else:
            self._model = MixModel(self._model_id, self._model_options)
        self._model.init(self._model_runner, self._kv_cache_manager)

    @property
    def model_id(self) -> int:
        """
        获取model_id

        Returns:
            model_id
        """
        return self._model_id


class LLMDataDist(object):
    llm_engine_instance = None
    """
    LLMDataDist

    Args:
        role: role of LLMDataDist
        cluster_id: cluster_id of LLMDataDist
    """

    def __init__(self, role: LLMRole, cluster_id: int):
        check_isinstance("role", role, LLMRole)
        self._models: List[LLMModel] = []
        self._kv_cache_manager = None
        self._cache_manager = None
        self._role = role
        check_uint64("cluster_id", cluster_id)
        self._cluster_id = cluster_id
        self._llm_datadist = llm_wrapper
        self._engine_config = None
        self._is_initialized = False
        self._pending_models = []
        self._engine_options: Dict[str, str] = {}
        self._token_manager: TokenManager = None
        self._enable_cache_mgr = False
        self._enable_free_comm = False

    @staticmethod
    def _get_flow_graph_max_size(options: Dict[str, str]) -> int:
        mem_utilization = float(options.get("llm.MemoryUtilization", "0.95"))
        value = options.get("ge.flowGraphMemMaxSize", None)
        if value is None:
            return 10737418240 * mem_utilization  # 10*1024*1024*1024

        check_isinstance('ge.flowGraphMemMaxSize', value, str)
        raise_if_false(len(value.split(",")) == 1, "ge.flowGraphMemMaxSize only support one mem pool in llm datadist")
        raise_if_false(value.isdigit(), "ge.flowGraphMemMaxSize must be digit")
        return int(value) * mem_utilization

    def init(self, options: Dict[str, str]) -> None:
        """
        初始化LLM Engine

        Args:
            options: Engine相关options
        """
        if self._is_initialized:
            return
        raise_if_false(LLMDataDist.llm_engine_instance is None, 'Cannot init multiple LLM engines',
                       status_code=LLMStatusCode.LLM_FAILED)
        check_isinstance("options", options, dict)
        self._engine_options = options
        self._engine_options['llm.Role'] = self._role_to_str(self._role)
        self._enable_cache_mgr = "llm.EnableCacheManager" in options and options["llm.EnableCacheManager"] == "1"
        log.info('options = %s', self._engine_options)
        # 统计全量空闲的token数
        self._token_manager = TokenManager(self._get_flow_graph_max_size(options))

        if self._enable_cache_mgr:
            ret = self._llm_datadist.initialize_v2(self._cluster_id, self._engine_options)
            handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm datadist, options = {options}')
            self._cache_manager = CacheManager(self._llm_datadist, options)
        else:
            EngineConfig.gen_cluster_info_if_not_exist(self._cluster_id, self._role, self._engine_options)
            ret = self._llm_datadist.initialize(self._cluster_id, self._engine_options)
            handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm datadist, options = {options}')
            self._kv_cache_manager = KvCacheManager(self._llm_datadist, self._role)
            for model, model_options, model_runner in self._pending_models:
                if model_runner is None:
                    model_runner = self._create_offline_model_runner(model_options)
                    log.info('load offline model success, model_id = %d', model.model_id)
                log.info('load pending model: %d start', model.model_id)
                model.init(model_runner, self._kv_cache_manager, self._token_manager)
                log.info('load pending model: %d end', model.model_id)
            self._pending_models = None
        LLMDataDist.llm_engine_instance = self
        self._is_initialized = True

    def _check_is_cache_mgr_mode(self, func_name):
        raise_if_false(self._enable_cache_mgr,
                       '{0} is not supported when llm.EnableCacheManager is not configured.',
                       func_name)

    def _check_is_not_cache_mgr_mode(self, func_name):
        raise_if_false(not self._enable_cache_mgr,
                       '{0} is not supported when llm.EnableCacheManager is configured.',
                       func_name)

    def link(self, comm_name: str, cluster_rank_info: Dict[int, int], rank_table: str) -> int:
        """
        :param cluster_rank_info:
        :param rank_table:
        :return: comm id
        """
        self._check_is_inited()
        self._check_is_cache_mgr_mode('link')
        check_isinstance("comm_name", comm_name, str, allow_none=False)
        raise_if_true(len(comm_name) == 0, "comm_name can not be empty")
        raise_if_false(len(comm_name) < _MAX_CLUSTER_NAME, "cluster_name length should be smaller than 128.")
        raise_if_false(len(cluster_rank_info) <= _MAX_NODE_NUM, "cluster_rank_info size can not be greater than 4.")
        check_isinstance("cluster_rank_info", cluster_rank_info, dict)
        check_dict("cluster_rank_info", cluster_rank_info, int, int)
        for k, v in cluster_rank_info.items():
            check_uint64("cluster_rank_info key", k)
            check_uint32("cluster_rank_info value", v)
        check_isinstance("rank_table", rank_table, str)
        ranks = list(cluster_rank_info.values())
        raise_if_false(len(ranks) > 1, "cluster num must be bigger than 1.")
        ranks_copy = copy.copy(ranks)
        ranks_copy.sort()
        raise_if_false(ranks == ranks_copy, "rank in cluster_rank_info must be ordered")
        cluster_ids = list(cluster_rank_info.keys())
        raise_if_false(len(cluster_ids) == len(set(cluster_ids)),
                       "cluster id in cluster_rank_info can not be duplicated.")
        ret, comm_id = self._llm_datadist.link(comm_name, cluster_rank_info, rank_table)
        handle_llm_status(ret, '[link]', cluster_rank_info)
        self.cache_manager.set_is_call_linked()
        return comm_id

    def unlink(self, comm_id: int):
        """
        :param comm_id:
        :return:
        """
        self._check_is_inited()
        self._check_is_cache_mgr_mode('unlink')
        check_isinstance('comm_id', comm_id, int)
        check_uint64("comm_id", comm_id)
        ret = self._llm_datadist.unlink(comm_id)
        handle_llm_status(ret, '[unlink]', comm_id)

    def query_register_mem_status(self, comm_id: int) -> RegisterMemStatus:
        """
        :param comm_id:
        :return:
        """
        self._check_is_inited()
        self._check_is_cache_mgr_mode('query_register_mem_status')
        check_isinstance('comm_id', comm_id, int)
        check_uint64("comm_id", comm_id)
        ret, status = self._llm_datadist.query_register_mem_status(comm_id)
        handle_llm_status(ret, '[query_register_mem_status]', comm_id)
        self.cache_manager.set_is_memory_prepared()
        return int_to_mem_status_dict[status] if status in int_to_mem_status_dict else RegisterMemStatus.FAILED

    @property
    def cache_manager(self) -> CacheManager:
        """
        获取KvCacheManager

        Returns:
            KvCacheManager
        """
        self._check_is_inited()
        self._check_is_cache_mgr_mode('cache_manager')
        return self._cache_manager

    def finalize(self) -> None:
        """
        释放LLM Engine相关资源
        """
        if not self._is_initialized:
            return
        try:
            for model in self._models:
                model.finalize()
        finally:
            if self._enable_cache_mgr:
                self._llm_datadist.finalize_v2()
            else:
                self._llm_datadist.finalize()
            if self._kv_cache_manager is not None:
                self._kv_cache_manager._initialized = False
            if self._cache_manager is not None:
                self._cache_manager._initialized = False
            self._is_initialized = False
            LLMDataDist.llm_engine_instance = None

    def add_model(self, model_options: Dict[str, str], model_runner: Optional[ModelRunner] = None) -> LLMModel:
        """
        添加Model

        Args:
            model_options: 模型相关options
            model_runner (Optional): ModelRunner, 如果为None，则会默认使用OfflineModelRunner

        Returns:
            LLMModel
        """
        self._check_is_not_cache_mgr_mode('add_model')
        if model_runner is not None:
            check_isinstance('model_runner', model_runner, [ModelRunner])
        check_isinstance('model_options', model_options, dict)
        model_id = len(self._models)
        model = LLMModel(self._role, model_id, self.cluster_id, model_options)
        if self._is_initialized:
            if model_runner is None:
                log.info('model_id = %d, model_runner is None, use offline ModelRunner by default', model_id)
                model_runner = self._create_offline_model_runner(model_options)
                log.info('load offline model success, model_id = %d', model_id)
            log.info('add model after init, model_id = %d', model_id)
            model.init(model_runner, self._kv_cache_manager, self._token_manager)
        else:
            log.info('add model before init, model_id = %d', model_id)
            self._pending_models.append((model, model_options, model_runner))
        self._models.append(model)
        return model

    def _create_offline_model_runner(self, model_options: Dict[str, str]) -> ModelRunner:
        if "llm.EnablePagedAttention" in model_options and model_options["llm.EnablePagedAttention"] == "1":
            ret, model_id = self._llm_datadist.add_model(model_options)
            handle_llm_status(ret, LLMException, "Failed to load model")
            return PAModelRunner(self._llm_datadist, self._cluster_config(), model_id)
        model_runner = SwitchableOfflineModelRunner(self._llm_datadist,
                                                    self._cluster_config(),
                                                    model_options)
        model_runner.switch_role(self._role)
        return model_runner

    def _cluster_config(self):
        if self._engine_config is None:
            self._engine_config = EngineConfig.from_engine_options(self._role == LLMRole.PROMPT, self._engine_options)
        return self._engine_config.cluster_config


    def check_link_status(self, remote_cluster_id: int):
        self._check_is_inited()
        self._check_is_not_cache_mgr_mode('check_link_status')
        check_uint64("remote_cluster_id", remote_cluster_id)
        ret = self._llm_datadist.check_link_status(remote_cluster_id)
        handle_llm_status(ret, '[check_link_status]', f"remote_cluster_id is {remote_cluster_id}")
        log.info('[check_link_status] success')


    def link_clusters(self, clusters: Union[List[LLMClusterInfo], Tuple[LLMClusterInfo]], timeout=3000):
        self._check_is_inited()
        self._check_is_not_cache_mgr_mode('link_clusters')
        check_int32("timeout", timeout)
        raise_if_false(timeout > 0, "Param timeout should be greater than 0.")
        check_isinstance("clusters", clusters, [list, tuple], LLMClusterInfo)
        cluster_list = [(cluster.remote_cluster_id,
                         0,
                         cluster.local_ip_info_list,
                         cluster.remote_ip_info_list) for cluster in clusters]
        ret, rets = self._llm_datadist.link_clusters(cluster_list, timeout)
        return code_2_status(ret), [code_2_status(cluster_ret) for cluster_ret in rets]

    def unlink_clusters(self, clusters: Union[List[LLMClusterInfo], Tuple[LLMClusterInfo]], timeout=3000, force=False):
        self._check_is_inited()
        self._check_is_not_cache_mgr_mode('unlink_clusters')
        check_int32("timeout", timeout)
        raise_if_false(timeout > 0, "Param timeout should be greater than 0.")
        check_isinstance("clusters", clusters, [list, tuple], LLMClusterInfo)
        check_isinstance("force", force, bool)
        cluster_list = [(cluster.remote_cluster_id,
                         0,
                         cluster.local_ip_info_list,
                         cluster.remote_ip_info_list) for cluster in clusters]
        ret, rets = self._llm_datadist.unlink_clusters(cluster_list, timeout, force)
        return code_2_status(ret), [code_2_status(cluster_ret) for cluster_ret in rets]

    def complete_request(self, llm_req: LLMReq):
        self._check_is_not_cache_mgr_mode('complete_request')
        check_isinstance('llm_req', llm_req, LLMReq)
        self._check_is_inited()
        for model in self._models:
            model.complete_request(llm_req)

    def switch_role(self, role: LLMRole, switch_options: Optional[Dict[str, str]] = None):
        self._check_is_inited()
        if self._enable_cache_mgr:
            self._role = role
            return
        check_isinstance('role', role, LLMRole)
        raise_if_false(self._role != role, f'role not changed, role = {role.name}')
        role_str = self._role_to_str(role)
        log.info(f'[switch_role] [{self._role.name}->{role.name}] start, switch_options = {switch_options}')
        check_isinstance('switch_options', switch_options, dict)
        options = switch_options.copy() if switch_options is not None else {}
        if role == LLMRole.PROMPT:
            raise_if_false('llm.listenIpInfo' in switch_options,
                           'Failed to switch to Prompt, option "llm.listenIpInfo" was specified')
            listen_ip_info = switch_options['llm.listenIpInfo']
            ip, port = EngineConfig.parse_listen_ip_info(listen_ip_info)
            options['llm.ListenIp'] = str(ip)
            options['llm.ListenPort'] = str(port)
        ret = self._llm_datadist.switch_role(role_str, options)
        handle_llm_status(ret,
                          '[LLMEngine.switch_role]',
                          f'Failed to switch role, role = {role}, options = {options}')
        self._kv_cache_manager._switch_role(role)
        for model in self._models:
            model._switch_role(role)
            log.info(f'[model:{model.model_id}] [switch_role] [{self._role.name}->{role.name}] success')
        log.info(f'[switch_role] [{self._role.name}->{role.name}] success')
        self._role = role

    @staticmethod
    def _role_to_str(role: LLMRole) -> str:
        role_mapping = {
            LLMRole.PROMPT: 'Prompt',
            LLMRole.DECODER: 'Decoder',
            LLMRole.MIX: 'Mix',
        }
        return role_mapping[role]

    def check_capacity(self, seq_len: int) -> CapacityState:
        self._check_is_inited()
        self._check_is_not_cache_mgr_mode('check_capacity')
        check_isinstance("seq_len", seq_len, int)
        raise_if_false(self._role == LLMRole.PROMPT, 'check_capacity must be called in prompt role')
        raise_if_false(self._models, 'check_capacity must be called after add_model')
        all_model_per_token_size = 0
        for model in self._models:
            raise_if_false(model.mem_size_per_token != 0,
                           'check_capacity must be called after set llm.kvTensorFormat option and not PA prompt model')
            all_model_per_token_size += model.mem_size_per_token
            log.info("model:%d per token size:%d", model.model_id, model.mem_size_per_token)
        log.info("llm datadist all model per token size:%d", all_model_per_token_size)
        return self._token_manager.get_capacity_state(seq_len, all_model_per_token_size)

    def _check_is_inited(self):
        if not self._is_initialized:
            raise RuntimeError('llm engine is not initialized')

    @property
    def kv_cache_manager(self) -> KvCacheManager:
        """
        获取KvCacheManager

        Returns:
            KvCacheManager
        """
        self._check_is_inited()
        self._check_is_not_cache_mgr_mode('kv_cache_manager')
        return self._kv_cache_manager

    @property
    def cluster_id(self):
        return self._cluster_id


def _shutdown_handler():
    if LLMDataDist.llm_engine_instance is not None:
        log.info('[shutdown_handler] finalize llm datadist')
        try:
            LLMDataDist.llm_engine_instance.finalize()
        except LLMException as e:
            log.warn(f'error occurred while finalize llm engine: {e} '
                     f'may cause by already finalized by another framework')


atexit.register(_shutdown_handler)
