#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

import threading
from typing import List, Dict
from llm_datadist.utils import log
from llm_datadist.status import raise_if_false, LLMStatusCode
from llm_datadist.configs import LLMReq
from llm_datadist.v2.llm_types import CapacityState

__all__ = ['RequestGroup', 'LogicalTokenSize', 'TokenManager']

_INVALID_ID = 2 ** 64 - 1


class LogicalTokenSize:
    def __init__(self, size: int) -> None:
        self.size = size
        self.ref_count = 0

    def __repr__(self) -> str:
        return f"LogicalToken(size={self.size}, ref_count={self.ref_count})"


class RequestGroup:
    def __init__(self, reqs: List[LLMReq], model_id: int, per_token_size: int, batch_size: int) -> None:
        self._reqs = reqs
        self._per_token_size = per_token_size
        self._model_id = model_id
        self._batch_size = batch_size

    @property
    def num_valid_reqs(self) -> int:
        return len([req for req in self._reqs if (req.req_id != _INVALID_ID or req.prefix_id != _INVALID_ID)])

    @property
    def max_size(self) -> int:
        max_seq_len_req = max(self._reqs, key=lambda x: x.prompt_length)
        return max_seq_len_req.prompt_length * self._per_token_size

    @property
    def keys(self) -> List[str]:
        res = []
        for req in self._reqs:
            if req.req_id != _INVALID_ID or req.prefix_id != _INVALID_ID:
                req_id = req.req_id if req.req_id != _INVALID_ID else req.prefix_id
                res.append(str(req_id) + str(self._model_id))
        return res

    @property
    def batch_size(self) -> int:
        return self._batch_size


class TokenManager:
    def __init__(self, max_token_size: float) -> None:
        self._max_token_size = max_token_size
        self._free_token_size = max_token_size
        self._token_tables = {}
        self._lock = threading.Lock()

    def allocate(self, req_group: RequestGroup) -> None:
        with self._lock:
            allocate_size = req_group.max_size
            logical_token_size = LogicalTokenSize(allocate_size)
            logical_token_size.ref_count = req_group.num_valid_reqs
            batch_allocate_size = logical_token_size.size * req_group.batch_size
            raise_if_false(((self._free_token_size > 0) and (self._free_token_size >= batch_allocate_size)),
                           "Out of memory! No free tokens are available",
                           status_code=LLMStatusCode.LLM_DEVICE_OUT_OF_MEMORY)
            for key in req_group.keys:
                self._token_tables[key] = logical_token_size
            log.info("llm reqs allocate size:%d", batch_allocate_size)
            self._free_token_size -= batch_allocate_size

    def _free_token_table(self, logical_token_size: int, batch_size: int) -> None:
        raise_if_false(logical_token_size.ref_count != 0, f"Double free! {logical_token_size} is already freed")
        logical_token_size.ref_count -= 1
        if logical_token_size.ref_count == 0:
            log.info("llm reqs free size:%d", logical_token_size.size * batch_size)
            self._free_token_size += logical_token_size.size * batch_size

    def free(self, req: LLMReq, model_id: int, batch_size: int) -> None:
        with self._lock:
            req_id = req.req_id if req.req_id != _INVALID_ID else req.prefix_id
            key = str(req_id) + str(model_id)
            logical_token_size = self._token_tables.get(key, None)
            if logical_token_size is None:
                log.info("req_id:%d, prefix_id:%d has been freed", req.req_id, req.prefix_id)
                return
            self._free_token_table(logical_token_size, batch_size)
            log.info("req_id:%d, prefix_id:%d token freed", req.req_id, req.prefix_id)
            del self._token_tables[key]

    def get_capacity_state(self, seq_len: int, per_token_size: int) -> CapacityState:
        raise_if_false(per_token_size > 0, f"per_token_size:{per_token_size} must be greater than 0")
        max_token_num = int(self._max_token_size // per_token_size)
        with self._lock:
            free_token_num = int(self._free_token_size // per_token_size)
            return CapacityState(seq_len <= free_token_num, free_token_num, max_token_num)
