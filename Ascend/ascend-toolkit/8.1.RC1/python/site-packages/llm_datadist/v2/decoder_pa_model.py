#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from typing import List, Dict, Tuple, Union

from llm_datadist.configs import LLMReq
from llm_datadist.status import raise_if_false
from llm_datadist.utils import log
from llm_datadist.v2.kv_cache_manager import KvCacheManager
from llm_datadist.v2.llm_types import ModelRunner, CacheDesc, BlocksCacheKey, CacheKey
from llm_datadist.v2.llm_utils import CacheDescParser, clone_cache_desc, get_req_desc
from llm_datadist.v2.offline_model_runner import OfflineModelRunner


class DecoderPAModel(object):
    def __init__(self, model_id: int, model_options: Dict[str, str]) -> None:
        self._model_id = model_id
        self._cache_desc: CacheDesc = CacheDescParser.parse_by_options(model_options)
        raise_if_false(-1 not in self._cache_desc.shape,
                       f'Dynamic KV shape is not supported by Decoder, shape = {self._cache_desc.shape}')
        self._blocks_kv_cache = None
        self._kv_cache_manager = None
        self._model_runner = None
        self._need_req_desc = False

    def init(self, model_runner: ModelRunner, kv_cache_manager: KvCacheManager):
        self._model_runner = model_runner
        self._kv_cache_manager = kv_cache_manager
        self._need_req_desc = isinstance(model_runner, OfflineModelRunner)
        self._allocate_kv_caches()

    def _allocate_kv_caches(self):
        pull_kv_cache_desc = clone_cache_desc(self._cache_desc)
        self._blocks_kv_cache = self._kv_cache_manager.allocate_cache(pull_kv_cache_desc)
        self._model_runner.on_cache_allocated(self._blocks_kv_cache)
        log.info(f"KV Cache for PA allocated, desc = {self._cache_desc}")

    def predict(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]], inputs: Union[Tuple, List], **kwargs) -> List:
        log.info('[model:%d] [predict] start, req_ids = %s', self._model_id, [req.req_id for req in llm_reqs])
        prefix_ids = set([llm_req.prefix_id for llm_req in llm_reqs])
        raise_if_false(len(prefix_ids) > 0, 'multiple request should contain same prefix id.')
        if self._need_req_desc:
            kwargs['_req_desc'] = get_req_desc(llm_reqs)
        output_tensors = self._model_runner.run_model(self._blocks_kv_cache, inputs, **kwargs)
        return output_tensors

    def pull_blocks(self, llm_req: LLMReq, prompt_blocks: List[int] = [], decoder_blocks: List[int] = []) -> None:
        prompt_cache_key = BlocksCacheKey(llm_req.prompt_cluster_id, self._model_id)
        self._kv_cache_manager.pull_blocks(prompt_cache_key, self._blocks_kv_cache, prompt_blocks,
                                           decoder_blocks)
        log.info('[model:%d] [pull_blocks] success, req_id = %d', self._model_id, llm_req.req_id)

    def finalize(self):
        log.info(f'Finalizing model {self._model_id}')
        self._kv_cache_manager.deallocate_cache(self._blocks_kv_cache)
        log.info(f'model {self._model_id} finalized')

    def complete_request(self, llm_req: LLMReq) -> None:
        log.info("Paged attention mode do nothing when call complete_request.")
        return
