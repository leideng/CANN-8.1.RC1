#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from typing import List, Dict, Tuple, Union

from llm_datadist.configs import LLMReq
from llm_datadist.status import LLMStatusCode, raise_if_false, raise_if_true
from llm_datadist.utils import log
from llm_datadist.v2.kv_cache_manager import KvCacheManager
from llm_datadist.v2.llm_types import ModelRunner, CacheKey, KvCache, CacheDesc
from llm_datadist.v2.llm_utils import CacheDescParser, clone_cache_desc, calc_tensor_size, is_invalid_id, get_req_desc, \
    is_valid_id
from llm_datadist.v2.offline_model_runner import OfflineModelRunner, SwitchableOfflineModelRunner


class DecoderModel(object):
    def __init__(self, model_id: int, model_options: Dict[str, str]) -> None:
        self._model_id = model_id
        self._cache_desc: CacheDesc = CacheDescParser.parse_by_options(model_options)
        raise_if_false(-1 not in self._cache_desc.shape,
                       f'Dynamic KV shape is not supported by Decoder, shape = {self._cache_desc.shape}')
        self._batch_size = self._cache_desc.shape[self._cache_desc.batch_dim]
        self._batch_num = 1
        self._parse_pipeline_options(model_options)
        self._size_per_seq = -1
        self._max_seq_len = -1
        self._pull_kv_cache = None
        self._prefix_id_2_kv_cache = {}
        self._pull_kv_req_id = -1
        self._decoder_kv_caches: List[KvCache] = []
        self._batch_id_to_req_ids = [[-1 for _ in range(self._batch_size)] for _ in range(self._batch_num)]
        self._kv_cache_manager = None
        self._model_runner = None
        self._need_req_desc = False

    def init(self, model_runner: ModelRunner, kv_cache_manager: KvCacheManager):
        self._model_runner = model_runner
        self._kv_cache_manager = kv_cache_manager
        self._need_req_desc = isinstance(model_runner, (OfflineModelRunner, SwitchableOfflineModelRunner))
        self._allocate_kv_caches()

    def _parse_pipeline_options(self, model_options: Dict[str, str]) -> None:
        if 'llm.pipelineStageNum' in model_options:
            self._batch_num = int(model_options['llm.pipelineStageNum'])

    def _allocate_kv_caches(self):
        self._pull_kv_cache, pull_kv_cache_desc = self._allocate_cache()
        log.info(f"KV Cache for pull kv allocated, desc = {self._cache_desc}")
        # BSH
        if pull_kv_cache_desc.seq_len_dim_index == 1:
            tensor_size = calc_tensor_size(pull_kv_cache_desc.shape, pull_kv_cache_desc.data_type)
            self._max_seq_len = pull_kv_cache_desc.shape[pull_kv_cache_desc.seq_len_dim_index]
            self._size_per_seq = tensor_size // self._max_seq_len

        for _ in range(self._batch_num):
            kv_cache = self._kv_cache_manager.allocate_cache(self._cache_desc)
            self._model_runner.on_cache_allocated(kv_cache)
            self._decoder_kv_caches.append(kv_cache)
            log.info(f'KV cache allocate, cache_desc = {self._cache_desc}')

    def _allocate_cache(self):
        pull_kv_cache_desc = clone_cache_desc(self._cache_desc)
        pull_kv_cache_desc.update_dim(pull_kv_cache_desc.batch_dim, 1)
        return self._kv_cache_manager.allocate_cache(pull_kv_cache_desc), pull_kv_cache_desc

    def finalize(self):
        log.info(f'Finalizing model {self._model_id}')
        self._kv_cache_manager.deallocate_cache(self._pull_kv_cache)
        for kv_cache in self._decoder_kv_caches:
            self._kv_cache_manager.deallocate_cache(kv_cache)
        log.info(f'model {self._model_id} finalized')

    def predict(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]], inputs: Union[Tuple, List], **kwargs) -> List:
        log.info('[model:%d] [predict] start, req_ids = %s', self._model_id, [req.req_id for req in llm_reqs])
        raise_if_false(len(llm_reqs) == self._batch_size,
                       'number of requests ({0}) mismatches batch_size ({1})',
                       len(llm_reqs), self._batch_size)
        prefix_ids = set([llm_req.prefix_id for llm_req in llm_reqs])
        raise_if_false(len(prefix_ids) > 0, 'multiple request should contain same prefix id.')
        batch_id = self._get_batch_id(llm_reqs)
        kv_cache = self._decoder_kv_caches[batch_id]
        if self._need_req_desc:
            kwargs['_req_desc'] = get_req_desc(llm_reqs)
        prefix_id = prefix_ids.pop()
        kwargs['prefix_kv_cache'] = self._prefix_id_2_kv_cache.get(prefix_id, None) if is_valid_id(
            prefix_id) else None
        output_tensors = self._model_runner.run_model(kv_cache, inputs, **kwargs)
        log.info('[predict] finished, batch_id = %d', batch_id)
        return output_tensors

    def pull_kv(self, llm_req: LLMReq) -> None:
        raise_if_true(is_invalid_id(llm_req.req_id) and is_invalid_id(llm_req.prefix_id),
                      'one of req id and prefix id should contain valid value:[0, 2**64-1).')
        raise_if_true(is_valid_id(llm_req.req_id) and is_valid_id(llm_req.prefix_id),
                      'only one of req id and prefix id should contain valid value:[0, 2**64-1).')
        prompt_cache_key = CacheKey(llm_req.prompt_cluster_id, llm_req.req_id, self._model_id, llm_req.prefix_id)
        pull_size = -1
        if self._size_per_seq > 0:
            raise_if_false(llm_req.prompt_length <= self._max_seq_len,
                           'Request[{0}] prompt_length ({1}) > max_seq_len ({2})',
                           llm_req.req_id, llm_req.prompt_length, self._max_seq_len)
            pull_size = self._size_per_seq * llm_req.prompt_length
            log.info('Request[%d] layout is BSH, pull by prompt_length = %d, size = %d',
                     llm_req.req_id, llm_req.prompt_length, pull_size)
        if is_valid_id(llm_req.prefix_id):
            self._prefix_id_2_kv_cache[llm_req.prefix_id], _ = self._allocate_cache()
            self._kv_cache_manager.pull_cache(prompt_cache_key, self._prefix_id_2_kv_cache[llm_req.prefix_id], 0,
                                              pull_size)
        else:
            self._kv_cache_manager.pull_cache(prompt_cache_key, self._pull_kv_cache, 0, pull_size)
        self._pull_kv_req_id = llm_req.req_id
        log.info('[model:%d] [pull_kv] success, req_id = %d, prefix_id = %d, size = %d',
                 self._model_id, llm_req.req_id, llm_req.prefix_id, pull_size)

    def merge_kv(self, llm_req: LLMReq, batch_index: int, batch_id: int = 0) -> None:
        raise_if_false(is_valid_id(llm_req.req_id), '[model:{0}] [merge_kv] failed, req_id = {1} is not valid.',
                       self._model_id, llm_req.req_id)
        raise_if_false(llm_req.req_id == self._pull_kv_req_id,
                       '[model:{0}] [merge_kv] failed, req_id = {1} is not pulled req_id ({2})',
                       self._model_id, llm_req.req_id, self._pull_kv_req_id,
                       status_code=LLMStatusCode.LLM_KV_CACHE_NOT_EXIST)
        raise_if_false(0 <= batch_id < self._batch_num,
                       'batch_id ({0}) out of range, [0, {1})',
                       batch_id, self._batch_num)
        raise_if_false(0 <= batch_index < self._batch_size,
                       'batch_index ({0}) out of range, [0, {1})',
                       batch_index, self._batch_size)
        self._kv_cache_manager.copy_cache(dst=self._decoder_kv_caches[batch_id],
                                          src=self._pull_kv_cache,
                                          dst_batch_index=batch_index,
                                          src_batch_index=0,
                                          offset=0,
                                          size=-1,
                                          req_id=llm_req.req_id)
        self._batch_id_to_req_ids[batch_id][batch_index] = llm_req.req_id
        log.info('[model:%d] [merge_kv] success, req_id = %d, batch_id = %d, batch_index = %d',
                 self._model_id,
                 llm_req.req_id,
                 batch_id,
                 batch_index)

    def complete_request(self, llm_req: LLMReq) -> None:
        if is_valid_id(llm_req.req_id):
            log.warn("decoder only support release prefix, you should not pass a valid req id.")
            return
        kv_cache: KvCache = self._prefix_id_2_kv_cache.pop(llm_req.prefix_id, None)
        if kv_cache is not None:
            self._kv_cache_manager.deallocate_cache(kv_cache)
        log.info('[model:%d] [complete_request] success, prefix_id = %d', self._model_id, llm_req.prefix_id)

    def get_pull_kv_tensors(self) -> KvCache:
        return self._pull_kv_cache

    def get_kv_cache(self, batch_id: int = 0) -> KvCache:
        raise_if_false(0 <= batch_id < len(self._decoder_kv_caches),
                       'batch_id ({0}) out of range, [0, {1}))', batch_id, len(self._decoder_kv_caches))
        return self._decoder_kv_caches[batch_id]

    def _get_batch_id(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]]) -> int:
        distinct_batch_ids = set()
        for i, req in enumerate(llm_reqs):
            if is_invalid_id(req.req_id):
                continue
            match = False
            for batch_id in range(self._batch_num):
                if self._batch_id_to_req_ids[batch_id][i] == req.req_id:
                    distinct_batch_ids.add(batch_id)
                    match = True
                    break
            raise_if_false(match,
                           '[model:{0}] req_index = {1}, req_id = {2} not in any batch, batch_state = {3}',
                           self._model_id, i, req.req_id, self._batch_id_to_req_ids)
        raise_if_false(len(distinct_batch_ids) <= 1,
                       '[model:{0}] requests maps to multiple batch_ids, req_ids = {1}, '
                       'batch_ids = {2}, batch_state = {3}',
                       self._model_id, [req.req_id for req in llm_reqs],
                       distinct_batch_ids, self._batch_id_to_req_ids)
        return 0 if len(distinct_batch_ids) == 0 else distinct_batch_ids.pop()
