#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from abc import ABC
from typing import Any, Dict, List, Optional
from llm_datadist.v2.llm_wrapper import LLMDataDistWrapper
from llm_datadist.configs import LLMRole
from llm_datadist.status import handle_llm_status, LLMException, raise_if_false
from llm_datadist.tensor import Tensor
from llm_datadist.utils import log
from llm_datadist.utils.utils import check_isinstance
from llm_datadist.v2.llm_types import ModelRunner, KvCache
from llm_datadist.v2.llm_utils import build_npu_tensors
from llm_datadist.v2.config import ClusterConfig

TensorId = int
_KEY_PREFIX_KV_CACHE = 'prefix_kv_cache'


class OfflineModelRunner(ModelRunner, ABC):
    def __init__(self, llm_engine: LLMDataDistWrapper, cluster_config: ClusterConfig, model_id: int):
        super().__init__()
        self._llm_engine = llm_engine
        self._device_ids = list(cluster_config.rank_id_to_device_id.values())
        self._model_id = model_id

    def run_model_internal(self, device_id_to_inputs: Dict[int, List[TensorId]], **kwargs) -> List[Tensor]:
        req_desc = kwargs['_req_desc']
        ret, outputs = self._llm_engine.run_model(self._model_id, device_id_to_inputs, req_desc)
        handle_llm_status(ret, LLMException, f"Failed to run model, model_id = {self._model_id}")
        output_tensors = [Tensor.from_tensor_tuple(output_tensor) for output_tensor in outputs]
        return output_tensors

    def append_prefix_kv_cache(self, device_id_to_inputs, **kwargs):
        if _KEY_PREFIX_KV_CACHE in kwargs and kwargs[_KEY_PREFIX_KV_CACHE] is not None:
            prefix_kv_cache = kwargs[_KEY_PREFIX_KV_CACHE]
            raise_if_false(len(prefix_kv_cache.per_device_tensor_addrs) == len(self._device_ids),
                           "unexpected prefix_kv_cache length.")
            for index in range(len(self._device_ids)):
                prefix_kv_tensors: List[TensorId] = build_npu_tensors(prefix_kv_cache, index)
                device_id_to_inputs[self._device_ids[index]].extend(prefix_kv_tensors)


class PromptOfflineModelRunner(OfflineModelRunner):
    def __init__(self, llm_engine: LLMDataDistWrapper, cluster_config: ClusterConfig, model_id: int):
        super().__init__(llm_engine, cluster_config, model_id)

    def run_model(self, kv_cache: KvCache, input_tensors: Any, **kwargs) -> List[Tensor]:
        check_isinstance('input_tensors', input_tensors, [list, tuple], Tensor)
        device_id_to_inputs: Dict[int, List[TensorId]] = {}
        raise_if_false(len(kv_cache.per_device_tensor_addrs) == len(self._device_ids),
                       '[run_model] failed, len(kv_cache.per_device_tensor_addrs) = {0},len(device_ids) = {1}',
                       len(kv_cache.per_device_tensor_addrs), len(self._device_ids))
        for index in range(len(kv_cache.per_device_tensor_addrs)):
            inputs: List[TensorId] = [t._tensor_id for t in input_tensors]
            kv_tensors: List[TensorId] = build_npu_tensors(kv_cache, index)
            inputs.extend(kv_tensors)
            device_id_to_inputs[self._device_ids[index]] = inputs
        self.append_prefix_kv_cache(device_id_to_inputs, **kwargs)
        output_tensors = self.run_model_internal(device_id_to_inputs, **kwargs)
        return output_tensors


class DecoderOfflineModelRunner(OfflineModelRunner):
    def __init__(self, llm_engine: LLMDataDistWrapper, cluster_config: ClusterConfig, model_id: int):
        super().__init__(llm_engine, cluster_config, model_id)
        self._cache_id_to_tensors: Dict[int, List[List[TensorId]]] = {}

    def on_cache_allocated(self, kv_cache: KvCache) -> None:
        cache_id = kv_cache.cache_id
        all_tensors: List[List[TensorId]] = []
        for index in range(len(kv_cache.per_device_tensor_addrs)):
            inputs = build_npu_tensors(kv_cache, index)
            all_tensors.append(inputs)
        self._cache_id_to_tensors[cache_id] = all_tensors
        log.info(f"Cache {kv_cache.cache_id} has been allocated")

    def on_cache_deallocated(self, kv_cache: KvCache) -> None:
        self._cache_id_to_tensors.pop(kv_cache.cache_id)
        log.info(f"Cache {kv_cache.cache_id} has been deallocated")

    def run_model(self, kv_cache: KvCache, input_tensors: List[Tensor], **kwargs) -> List[Tensor]:
        raise_if_false(kv_cache.cache_id in self._cache_id_to_tensors,
                       '[run_model] failed, cache_id ({0}) not exist', kv_cache.cache_id)
        all_kv_tensors = self._cache_id_to_tensors[kv_cache.cache_id]
        device_id_to_inputs: Dict[int, List[TensorId]] = {}
        for index, kv_tensors in enumerate(all_kv_tensors):
            inputs = [t._tensor_id for t in input_tensors]
            inputs.extend(kv_tensors)
            device_id_to_inputs[self._device_ids[index]] = inputs
        self.append_prefix_kv_cache(device_id_to_inputs, **kwargs)
        output_tensors = self.run_model_internal(device_id_to_inputs, **kwargs)
        return output_tensors


class MixOfflineModelRunner(OfflineModelRunner):
    def __init__(self, llm_engine: LLMDataDistWrapper, cluster_config: ClusterConfig, model_id: int):
        super().__init__(llm_engine, cluster_config, model_id)

    def run_model(self, kv_cache: KvCache, input_tensors: List[Tensor], **kwargs) -> List[Tensor]:
        raise_if_false(len(self._device_ids) == 1, 'Only SPMD mode is supported')
        inputs: List[TensorId] = [t._tensor_id for t in input_tensors]
        device_id_to_inputs: Dict[int, List[TensorId]] = {self._device_ids[0]: inputs}
        output_tensors = self.run_model_internal(device_id_to_inputs, **kwargs)
        return output_tensors


class PAModelRunner(OfflineModelRunner):
    def __init__(self, llm_engine: LLMDataDistWrapper, cluster_config: ClusterConfig, model_id: int):
        super().__init__(llm_engine, cluster_config, model_id)
        self.kv_tensors: List[List[TensorId]] = []

    def on_cache_allocated(self, kv_cache: KvCache) -> None:
        for index in range(len(kv_cache.per_device_tensor_addrs)):
            inputs = build_npu_tensors(kv_cache, index)
            self.kv_tensors.append(inputs)
        log.info(f"Cache {kv_cache.cache_id} has been allocated")

    def run_model(self, kv_cache: KvCache, input_tensors: List[Tensor], **kwargs) -> List[Tensor]:
        device_id_to_inputs: Dict[int, List[TensorId]] = {}
        for index, kv_tensors in enumerate(self.kv_tensors):
            inputs = [t._tensor_id for t in input_tensors]
            inputs.extend(kv_tensors)
            device_id_to_inputs[self._device_ids[index]] = inputs
        output_tensors = self.run_model_internal(device_id_to_inputs, **kwargs)
        return output_tensors


class SwitchableOfflineModelRunner(ModelRunner):
    def __init__(self, llm_engine: LLMDataDistWrapper, cluster_config: ClusterConfig, model_options: Dict[str, str]):
        super().__init__()
        self._llm_engine = llm_engine
        self._cluster_config = cluster_config
        self._model_runner: Optional[ModelRunner] = None
        ret, self._model_id = self._llm_engine.add_model(model_options)
        handle_llm_status(ret, LLMException, "Failed to load model")

    def switch_role(self, role: LLMRole):
        if role == LLMRole.PROMPT:
            self._model_runner = PromptOfflineModelRunner(self._llm_engine, self._cluster_config, self._model_id)
        elif role == LLMRole.DECODER:
            self._model_runner = DecoderOfflineModelRunner(self._llm_engine, self._cluster_config, self._model_id)
        else:
            self._model_runner = MixOfflineModelRunner(self._llm_engine, self._cluster_config, self._model_id)

    def on_cache_allocated(self, kv_cache: KvCache) -> None:
        self._model_runner.on_cache_allocated(kv_cache)

    def on_cache_deallocated(self, kv_cache: KvCache) -> None:
        self._model_runner.on_cache_deallocated(kv_cache)

    def run_model(self, kv_cache: KvCache, input_tensors: List[Tensor], **kwargs) -> List[Tensor]:
        return self._model_runner.run_model(kv_cache, input_tensors, **kwargs)
