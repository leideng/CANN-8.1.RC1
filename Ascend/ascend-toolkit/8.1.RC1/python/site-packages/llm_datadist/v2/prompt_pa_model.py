# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from typing import List, Dict, Tuple, Union

from llm_datadist.configs import LLMReq
from llm_datadist.status import raise_if_false
from llm_datadist.utils import log
from llm_datadist.v2.kv_cache_manager import KvCacheManager
from llm_datadist.v2.llm_types import ModelRunner, BlocksCacheKey
from llm_datadist.v2.llm_utils import CacheDescParser, get_req_desc
from llm_datadist.v2.offline_model_runner import OfflineModelRunner


class PromptPAModel(object):
    def __init__(self, model_id: int, cluster_id, model_options: Dict[str, str]):
        self._model_id = model_id
        self._cluster_id = cluster_id
        self._model_options = model_options
        self._cache_desc = CacheDescParser.parse_by_options(model_options)
        raise_if_false(-1 not in self._cache_desc.shape,
                       "PagedAttention mode do not support 'llm.RefInputShapes' option contains -1.")
        self._model_runner = None
        self._kv_cache_manager = None
        self._need_req_desc = False
        self._blocks_kv_cache = None

    def init(self, model_runner: ModelRunner, kv_cache_manager: KvCacheManager):
        self._model_runner = model_runner
        self._kv_cache_manager = kv_cache_manager
        self._need_req_desc = isinstance(model_runner, OfflineModelRunner)
        prompt_cache_key = BlocksCacheKey(self._cluster_id, self._model_id)
        self._blocks_kv_cache = self._kv_cache_manager.allocate_blocks_cache(self._cache_desc, prompt_cache_key)
        self._model_runner.on_cache_allocated(self._blocks_kv_cache)
        log.info(f"KV Cache for PA allocated, desc = {self._cache_desc}")

    def finalize(self):
        log.info(f'Finalizing model {self._model_id}')
        self._kv_cache_manager.deallocate_cache(self._blocks_kv_cache)
        log.info(f'model {self._model_id} finalized')

    def predict(self, llm_reqs: Union[List[LLMReq], Tuple[LLMReq]], inputs: Union[Tuple, List], **kwargs) -> List:
        """
        user pass block_table to inputs
        """
        log.info('[model:%d] [predict] start, req_ids = %s', self._model_id, [llm_req.req_id for llm_req in llm_reqs])
        if self._need_req_desc:
            kwargs['_req_desc'] = get_req_desc(llm_reqs)
        output_tensors = self._model_runner.run_model(self._blocks_kv_cache, inputs, **kwargs)
        log.info('[predict] success')
        return output_tensors

    def complete_request(self, llm_req: LLMReq) -> None:
        log.info("Paged attention mode do nothing when call complete_request.")
        return
