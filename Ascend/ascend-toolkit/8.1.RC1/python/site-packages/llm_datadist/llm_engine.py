#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

from typing import Union, List, Tuple, Dict
from . import llm_wrapper
from .status import LLMStatusCode, code_2_status, Status, handle_llm_status, raise_if_false
from .tensor import Tensor
from .configs import LLMRole, LLMClusterInfo, LLMReq, LLMModelStatus, LLMEngineStatus
from .utils import log
from .utils.utils import check_isinstance, check_dict


def _llm_req_str(llm_req: LLMReq):
    return f"llm_req id: {llm_req.req_id}, prompt_cluster_id: {llm_req.prompt_cluster_id}, " \
           f"decoder_cluster_id: {llm_req.decoder_cluster_id}, prefix_id: {llm_req.prefix_id}, " \
           f"prompt_length: {llm_req.prompt_length}"


def _pack_llm_req(llm_req: LLMReq):
    return (llm_req.req_id, llm_req.prompt_length, llm_req.prompt_cluster_id,
            llm_req.decoder_cluster_id, llm_req.prefix_id, llm_req.sequence_length)


class LLMModel(object):
    def __init__(self, llm_model_id: int, role: LLMRole, batch_mode):
        self._llm_model_id = llm_model_id
        self._role = role
        self._batch_mode = batch_mode

    def _check_role(self, role: LLMRole):
        if self._role != role:
            raise ValueError(f"role {self._role} is not allowed to call this api.")

    def pull_kv(self, llm_req: LLMReq) -> LLMStatusCode:
        """
        拉取KV
        Args:
            llm_req: 请求信息

        Raises:
            失败时会抛出LLMException
        """
        self._check_role(LLMRole.DECODER)
        check_isinstance("llm_req", llm_req, LLMReq)
        status = llm_wrapper.pull_kv(self._llm_model_id, _pack_llm_req(llm_req))
        handle_llm_status(status, "pull_kv", _llm_req_str(llm_req))

    def merge_kv(self, llm_req: LLMReq, batch_index: int, batch_id: int = 0) -> LLMStatusCode:
        """
        合并KV
        Args:
            llm_req: 请求信息
            batch_index: 当前batch所在索引
            batch_id: 多个batch所在的id

        Raises:
            失败时会抛出LLMException
        """
        self._check_role(LLMRole.DECODER)
        check_isinstance("llm_req", llm_req, LLMReq)
        check_isinstance("batch_index", batch_index, int)
        check_isinstance("batch_id", batch_id, int)
        status = llm_wrapper.merge_kv(self._llm_model_id, _pack_llm_req(llm_req), batch_index, batch_id)
        handle_llm_status(status, "merge_kv", _llm_req_str(llm_req))

    def preload_prompt_prefix(self, llm_req: LLMReq, inputs: Union[Tuple[Tensor], List[Tensor]]) -> LLMStatusCode:
        """
        预加载前缀
        Args:
            llm_req: 请求信息
            inputs: 输入Tensor

        Raises:
            失败时会抛出LLMException
        """
        self._check_role(LLMRole.PROMPT)
        check_isinstance("llm_req", llm_req, LLMReq)
        check_isinstance("inputs", inputs, [list, tuple], Tensor)
        status = llm_wrapper.preload_prompt_prefix(self._llm_model_id, _pack_llm_req(llm_req),
                                                   LLMModel.convert_in_tensor(inputs))
        handle_llm_status(status, "preload_prompt_prefix", _llm_req_str(llm_req))

    def release_prompt_prefix(self, llm_req: LLMReq) -> LLMStatusCode:
        """
        释放前缀KV
        Args:
            llm_req: 请求信息

        Raises:
            失败时会抛出LLMException
        """
        self._check_role(LLMRole.PROMPT)
        check_isinstance("llm_req", llm_req, LLMReq)
        status = llm_wrapper.release_prompt_prefix(self._llm_model_id, _pack_llm_req(llm_req))
        handle_llm_status(status, "release_prompt_prefix", _llm_req_str(llm_req))

    @staticmethod
    def convert_out_tensor(outputs):
        return [Tensor.from_tensor_tuple(output) for output in outputs]

    @staticmethod
    def convert_in_tensor(inputs):
        return [input_tensor._tensor_id for input_tensor in inputs]

    def predict(self, llm_req: Union[LLMReq, List[LLMReq], Tuple[LLMReq]],
                inputs: Union[Tuple[Tensor], List[Tensor]]):
        """
        推理
        Args:
            llm_req: 请求信息
            inputs: 输入Tensor

        Raises:
            失败时会抛出LLMException
        """
        check_isinstance("inputs", inputs, [list, tuple], Tensor)
        func_name = "predict"
        if self._role == LLMRole.PROMPT:
            if isinstance(llm_req, LLMReq):
                status, outputs = llm_wrapper.run_prompt(self._llm_model_id, _pack_llm_req(llm_req),
                                                         LLMModel.convert_in_tensor(inputs))
                handle_llm_status(status, func_name, _llm_req_str(llm_req))
                return LLMModel.convert_out_tensor(outputs)
            check_isinstance("llm_req", llm_req, [list, tuple], LLMReq)
            status, outputs = llm_wrapper.run_prompt_batch(self._llm_model_id, [_pack_llm_req(req) for req in llm_req],
                                                           LLMModel.convert_in_tensor(inputs))
            handle_llm_status(status, func_name, [_llm_req_str(req) for req in llm_req])
            return LLMModel.convert_out_tensor(outputs)
        else:
            if isinstance(llm_req, LLMReq):
                status, outputs = llm_wrapper.run_decoder(self._llm_model_id, _pack_llm_req(llm_req),
                                                          LLMModel.convert_in_tensor(inputs))
                handle_llm_status(status, func_name, _llm_req_str(llm_req))
                return LLMModel.convert_out_tensor(outputs)
            check_isinstance("llm_req", llm_req, [list, tuple], LLMReq)
            status, outputs = llm_wrapper.run_decoder_batch(self._llm_model_id,
                                                            [_pack_llm_req(req) for req in llm_req],
                                                            LLMModel.convert_in_tensor(inputs))
            handle_llm_status(status, func_name, [_llm_req_str(req) for req in llm_req])
            return LLMModel.convert_out_tensor(outputs)

    def fetch_status(self) -> LLMModelStatus:
        """
        获取状态信息
        Returns:
            状态信息

        Raises:
            失败时会抛出ValueError或TypeError
        """
        return LLMModelStatus(llm_wrapper.model_fetch_status(self._llm_model_id))


class LLMEngine(object):
    def __init__(self, role: LLMRole, cluster_id: int, batch_mode="auto"):
        """
        初始化LLM Engine
        Args:
            role: 角色
            cluster_id: 集群ID
            batch_mode: 批处理模式，可选auto或manual

        Raises:
            失败时会抛出ValueError或TypeError
        """
        self._inited = False
        self._created = False
        check_isinstance("role", role, LLMRole)
        check_isinstance("cluster_id", cluster_id, int)
        check_isinstance("batch_mode", batch_mode, str)
        self._cluster_id = cluster_id
        self._engine = llm_wrapper
        self._role = role
        self._batch_mode = batch_mode
        if self._batch_mode not in ["auto", "manual"]:
            raise ValueError(f"Param batch_mode only support [auto, manual], but got {self._batch_mode}")

    def __del__(self):
        if self._inited:
            self.finalize()
        if self._created:
            llm_wrapper.destroy_llm_engine()

    def add_model(self, model_paths: Union[Tuple[str], List[str], str], options: Dict[str, str],
                  postprocess_model_path=None) -> LLMModel:
        """
        添加模型
        Args:
            model_paths: 模型路径
            options: 模型配置项
            postprocess_model_path: 后处理模型路径

        Returns:
            LLMModel

        Raises:
            失败时会抛出ValueError或TypeError
        """
        if self._inited:
            raise RuntimeError(f"llm engine has been inited.")
        self._ensure_created()
        check_isinstance("model_paths", model_paths, [list, tuple], str)
        check_isinstance("options", options, dict)
        check_dict("options", options, str, str)
        options["llm.OmCachePath"] = ";".join(model_paths)
        if postprocess_model_path:
            check_isinstance("postprocess_model_path", postprocess_model_path, str)
            options["llm.PostProcessOmCachePath"] = postprocess_model_path
        ret, llm_model_id = self._engine.add_model_v1(options)
        ret = code_2_status(ret)
        if ret != LLMStatusCode.LLM_SUCCESS:
            raise RuntimeError(f"Failed to add model, error:{ret}.")
        return LLMModel(llm_model_id, self._role, self._batch_mode)

    def init(self, options: Dict[str, str]):
        """
        初始化
        Args:
            options: Engine配置项

        Returns:
            初始化状态码

        Raises:
            失败时会抛出ValueError或TypeError
        """
        if self._inited:
            raise RuntimeError(f"llm engine has been inited.")
        self._ensure_created()
        check_isinstance("options", options, dict)
        check_dict("options", options, str, str)
        options["llm.Role"] = "Prompt" if self._role == LLMRole.PROMPT else "Decoder"
        options["llm.batch_mode"] = self._batch_mode
        log.info("llm engine options:%s", options)
        ret = code_2_status(self._engine.init_v1(options))
        if ret != LLMStatusCode.LLM_SUCCESS:
            raise RuntimeError(f"Failed to init llm engine, error:{ret}.")
        self._inited = True
        return ret

    def finalize(self):
        """
        终止
        Raises:
            失败时会抛出RuntimeError
        """
        self._check_is_inited()
        self._engine.finalize_v1()
        self._inited = False

    def complete_request(self, llm_req: LLMReq):
        """
        结束请求
        Raises:
            失败时会抛出RuntimeError
        """
        self._check_is_inited()
        check_isinstance("llm_req", llm_req, LLMReq)
        self._engine.complete_request_v1(_pack_llm_req(llm_req))

    def link_clusters(self, clusters: Union[List[LLMClusterInfo], Tuple[LLMClusterInfo]], timeout=3000):
        """
        和prompt建链
        Args:
            clusters: 集群信息
            timeout: 超时信息

        Returns:
            建链结果

        Raises:
            失败时会抛出ValueError或TypeError

        Examples:
            >>> from llm_datadist import LLMEngine
            >>> llm_engine = LLMEngine(...)
            >>> ...
            >>> ret, cluster_rets = llm_engine.link_clusters(...)
            >>> print(ret.is_ok())
            >>> print([ret.is_ok() for ret in cluster_rets])
        """
        self._check_is_inited()
        check_isinstance("timeout", timeout, int)
        raise_if_false(timeout > 0, "Param timeout should be greater than 0.")
        check_isinstance("clusters", clusters, [list, tuple], LLMClusterInfo)
        cluster_list = [(cluster.remote_cluster_id,
                         0,
                         cluster.local_ip_info_list,
                         cluster.remote_ip_info_list) for cluster in clusters]
        ret, rets = self._engine.link_clusters_v1(cluster_list, timeout)
        return Status(code_2_status(ret)), [Status(code_2_status(cluster_ret)) for cluster_ret in rets]

    def unlink_clusters(self, clusters: Union[List[LLMClusterInfo], Tuple[LLMClusterInfo]], timeout=3000):
        """
        和prompt断链
        Args:
            clusters: 集群信息
            timeout: 超时信息

        Returns:
            断链结果

        Raises:
            失败时会抛出ValueError或TypeError

        Examples:
            >>> from llm_datadist import LLMEngine
            >>> llm_engine = LLMEngine(...)
            >>> ...
            >>> ret, cluster_rets = llm_engine.unlink_clusters(...)
            >>> print(ret.is_ok())
            >>> print([ret.is_ok() for ret in cluster_rets])
        """
        self._check_is_inited()
        check_isinstance("timeout", timeout, int)
        raise_if_false(timeout > 0, "Param timeout should be greater than 0.")
        check_isinstance("clusters", clusters, [list, tuple], LLMClusterInfo)
        cluster_list = [(cluster.remote_cluster_id,
                         0,
                         cluster.local_ip_info_list,
                         cluster.remote_ip_info_list) for cluster in clusters]
        ret, rets = self._engine.unlink_clusters_v1(cluster_list, timeout)
        return Status(code_2_status(ret)), [Status(code_2_status(cluster_ret)) for cluster_ret in rets]

    def fetch_status(self) -> LLMEngineStatus:
        """
        获取状态信息
        Returns:
            状态信息

        Raises:
            失败时会抛出ValueError或TypeError
        """
        self._check_is_inited()
        status = self._engine.engine_fetch_status()
        return LLMEngineStatus(status[0], status[1])

    def _ensure_created(self):
        if not self._created:
            llm_wrapper.create_llm_engine(self._cluster_id)
            self._created = True

    def _check_is_inited(self):
        if not self._inited:
            raise RuntimeError("llm engine is not initialized.")
