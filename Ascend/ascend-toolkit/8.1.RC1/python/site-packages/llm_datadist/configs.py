#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# Copyright (c) 2024 Huawei Technologies Co., Ltd.
# This file is a part of the CANN Open Software.
# Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# ======================================================================================================================

import json
import socket
from enum import Enum
from typing import List, Tuple, Union, Optional

from .data_type import DataType
from .utils.utils import check_isinstance, check_dict, check_uint64, check_int32, check_uint32, check_uint16
from .status import raise_if_false

_INVALID_ID = 2 ** 64 - 1


class LLMReq(object):
    def __init__(self, req_id=_INVALID_ID, prompt_length=0, prompt_cluster_id=0, decoder_cluster_id=0,
                 prefix_id=_INVALID_ID, sequence_length=0):
        check_uint64("req_id", req_id)
        check_uint64("prompt_length", prompt_length)
        check_uint64("prompt_cluster_id", prompt_cluster_id)
        check_uint64("decoder_cluster_id", decoder_cluster_id)
        check_uint64("prefix_id", prefix_id)
        self._req_id = req_id
        self._prompt_length = prompt_length
        self._prompt_cluster_id = prompt_cluster_id
        self._decoder_cluster_id = decoder_cluster_id
        self._prefix_id = prefix_id
        self._sequence_length = sequence_length

    @property
    def req_id(self):
        return self._req_id

    @req_id.setter
    def req_id(self, req_id):
        check_uint64("req_id", req_id)
        self._req_id = req_id

    @property
    def sequence_length(self):
        return self._sequence_length

    @sequence_length.setter
    def sequence_length(self, sequence_length):
        check_uint64("sequence_length", sequence_length)
        self._sequence_length = sequence_length

    @property
    def prompt_length(self):
        return self._prompt_length

    @prompt_length.setter
    def prompt_length(self, prompt_length):
        check_uint64("prompt_length", prompt_length)
        self._prompt_length = prompt_length

    @property
    def prompt_cluster_id(self):
        return self._prompt_cluster_id

    @prompt_cluster_id.setter
    def prompt_cluster_id(self, prompt_cluster_id):
        check_uint64("prompt_cluster_id", prompt_cluster_id)
        self._prompt_cluster_id = prompt_cluster_id

    @property
    def decoder_cluster_id(self):
        return self._decoder_cluster_id

    @decoder_cluster_id.setter
    def decoder_cluster_id(self, decoder_cluster_id):
        check_uint64("decoder_cluster_id", decoder_cluster_id)
        self._decoder_cluster_id = decoder_cluster_id

    @property
    def prefix_id(self):
        return self._prefix_id

    @prefix_id.setter
    def prefix_id(self, prefix_id):
        check_uint64("prefix_id", prefix_id)
        self._prefix_id = prefix_id


class LLMRole(Enum):
    PROMPT = 1
    DECODER = 2
    MIX = 3


def trans_str_ip(ip):
    if isinstance(ip, str):
        try:
            ip_bytes = socket.inet_aton(ip)
            return int.from_bytes(ip_bytes, byteorder="little")
        except:
            raise RuntimeError(f"Can not parse ip str:{ip}")
    return ip


class LLMClusterInfo(object):
    def __init__(self):
        self._remote_cluster_id = None
        self._remote_role_type = None
        self._local_ip_info_list: List[Tuple[int, int]] = []
        self._remote_ip_info_list: List[Tuple[int, int]] = []

    def _check_inputs(self, ip, port):
        check_isinstance("ip", ip, [str, int])
        check_uint16("port", port)
        return trans_str_ip(ip)

    @property
    def remote_role_type(self):
        return self._remote_role_type

    @property
    def remote_cluster_id(self):
        return self._remote_cluster_id

    @property
    def local_ip_info_list(self):
        return self._local_ip_info_list

    @property
    def remote_ip_info_list(self):
        return self._remote_ip_info_list

    @remote_role_type.setter
    def remote_role_type(self, remote_role_type: Union[LLMRole, int]):
        check_isinstance("remote_role_type", remote_role_type, [LLMRole, int])
        self._remote_role_type = remote_role_type

    @remote_cluster_id.setter
    def remote_cluster_id(self, remote_cluster_id):
        check_uint64("remote_cluster_id", remote_cluster_id)
        self._remote_cluster_id = remote_cluster_id

    def append_local_ip_info(self, ip: Union[str, int], port: int):
        """
        添加本地IP信息
        Args:
            ip: IP
            port: 端口
        """
        ip = self._check_inputs(ip, port)
        self._local_ip_info_list.append((ip, port))

    def append_remote_ip_info(self, ip: Union[str, int], port: int):
        """
        添加对端IP信息
        Args:
            ip: IP
            port: 端口
        """
        ip = self._check_inputs(ip, port)
        self._remote_ip_info_list.append((ip, port))


class LlmConfig(object):
    def __init__(self):
        self._options = {}
        self._listen_ip_info = ""
        self._device_id = None
        self._sync_kv_timeout = None
        self._hcom_cluster_config = ""
        self._deploy_res_path = ""
        self._ge_options = {}
        self._enable_switch_role = False

        # below is offline
        self._cluster_info = ""
        self._nn_execute_timeout = ""
        self._process_request_timeout = ""
        self._output_max_size = ""
        self._mem_utilization = 0.95
        self._buf_pool_cfg = ""
        self._mem_pool_cfg = ""
        self._host_mem_pool_cfg = ""
        self._enable_cache_manager = False
        self._enable_remote_cache_accessible = False

    def generate_options(self):
        """
        生成LLM Engine配置项
        Returns:
            配置项dict
        """
        return self.gen_options()

    def gen_options(self):
        if self.ge_options:
            self._options.update(self.ge_options)
        if self.listen_ip_info:
            self._options["llm.listenIpInfo"] = str(self.listen_ip_info)
        if self.device_id is not None:
            if isinstance(self.device_id, int):
                self._options["ge.exec.deviceId"] = str(self.device_id)
                self._options["ge.session_device_id"] = str(self.device_id)
            else:
                self._options["ge.session_device_id"] = str(self.device_id[0])
                self._options["ge.exec.deviceId"] = ";".join([str(dev) for dev in self.device_id])
        if self.sync_kv_timeout is not None:
            self._options["llm.SyncKvCacheWaitTime"] = str(self.sync_kv_timeout)
        if self.hcom_cluster_config:
            self._options["llm.HcomClusterConfig"] = str(self.hcom_cluster_config)
        if self.deploy_res_path:
            self._options["llm.deployResPath"] = str(self.deploy_res_path)
        if self.buf_pool_cfg:
            self._options["llm.BufPoolCfg"] = str(self.buf_pool_cfg)
        if self._mem_pool_cfg:
            self._options["llm.MemPoolConfig"] = str(self._mem_pool_cfg)
        if self._host_mem_pool_cfg:
            self._options["llm.HostMemPoolConfig"] = str(self._host_mem_pool_cfg)
        if self.enable_cache_manager:
            self._options["llm.EnableCacheManager"] = "1"
        if self.enable_remote_cache_accessible:
            self._options["llm.EnableRemoteCacheAccessible"] = "1"

        # below is offline
        if self._cluster_info:
            self._options["llm.ClusterInfo"] = str(self.cluster_info)
        if self._nn_execute_timeout:
            self._options["llm.NnExecuteWaitTime"] = str(self.nn_execute_timeout)
        if self._process_request_timeout:
            self._options["llm.ProcessRequestWaitTime"] = str(self.process_request_timeout)
        if self._output_max_size:
            self._options["llm.OutputMaxSize"] = str(self.output_max_size)
        if self._enable_switch_role:
            self._options["llm.EnableSwitchRole"] = "1"
        if self._mem_utilization is not None:
            self._options["llm.MemoryUtilization"] = str(self.mem_utilization)
        return self.options

    @property
    def ge_options(self):
        return self._ge_options

    @ge_options.setter
    def ge_options(self, ge_options):
        check_isinstance("ge_options", ge_options, dict)
        check_dict("ge_options", ge_options, str, str)
        self._ge_options = ge_options

    @property
    def device_id(self):
        return self._device_id

    @device_id.setter
    def device_id(self, device_id):
        check_isinstance("device_id", device_id, [list, tuple, int])
        if isinstance(device_id, list) or isinstance(device_id, tuple):
            check_isinstance("device_id", device_id, [list, tuple], int)
            [raise_if_false(dev_id >= 0, "device_id should be greater than or equal to zero.") for dev_id in device_id]
            [check_int32('device_id', dev_id) for dev_id in device_id]
        else:
            check_isinstance("device_id", device_id, int)
            raise_if_false(device_id >= 0, "device_id should be greater than or equal to zero.")
            check_int32('device_id', device_id)
        self._device_id = device_id

    @property
    def listen_ip_info(self):
        return self._listen_ip_info

    @listen_ip_info.setter
    def listen_ip_info(self, listen_ip_info):
        check_isinstance("listen_ip_info", listen_ip_info, str)
        self._listen_ip_info = listen_ip_info

    @property
    def deploy_res_path(self):
        return self._deploy_res_path

    @deploy_res_path.setter
    def deploy_res_path(self, deploy_res_path):
        check_isinstance("deploy_res_path", deploy_res_path, str)
        self._deploy_res_path = deploy_res_path

    @property
    def buf_pool_cfg(self):
        return self._buf_pool_cfg

    @buf_pool_cfg.setter
    def buf_pool_cfg(self, buf_pool_cfg):
        check_isinstance("buf_pool_cfg", buf_pool_cfg, str)
        self._buf_pool_cfg = buf_pool_cfg

    @property
    def output_max_size(self):
        return self._output_max_size

    @output_max_size.setter
    def output_max_size(self, output_max_size):
        check_isinstance("output_max_size", output_max_size, int)
        self._output_max_size = output_max_size

    @property
    def mem_utilization(self):
        return self._mem_utilization

    @mem_utilization.setter
    def mem_utilization(self, mem_utilization):
        check_isinstance("mem_utilization", mem_utilization, float)
        raise_if_false(((mem_utilization >= 0.0) and (mem_utilization <= 1.0)),
                       f"mem_utilization must be in range [0,1], current:{mem_utilization}")
        self._mem_utilization = mem_utilization

    @property
    def options(self):
        return self._options

    @property
    def cluster_info(self):
        return self._cluster_info

    @property
    def hcom_cluster_config(self):
        return self._hcom_cluster_config

    @property
    def nn_execute_timeout(self):
        return self._nn_execute_timeout

    @property
    def sync_kv_timeout(self):
        return self._sync_kv_timeout

    @property
    def process_request_timeout(self):
        return self._process_request_timeout

    @cluster_info.setter
    def cluster_info(self, cluster_info):
        check_isinstance("cluster_info", cluster_info, str)
        cluster_info_dict = json.loads(cluster_info)
        if "listen_ip_info" in cluster_info_dict:
            for ip_info in cluster_info_dict["listen_ip_info"]:
                ip_info["ip"] = trans_str_ip(ip_info["ip"])
        self._cluster_info = json.dumps(cluster_info_dict)

    @hcom_cluster_config.setter
    def hcom_cluster_config(self, hcom_cluster_config):
        check_isinstance("hcom_cluster_config", hcom_cluster_config, str)
        self._hcom_cluster_config = hcom_cluster_config

    @nn_execute_timeout.setter
    def nn_execute_timeout(self, nn_execute_timeout):
        check_isinstance("nn_execute_timeout", nn_execute_timeout, [int, str])
        self._nn_execute_timeout = nn_execute_timeout

    @sync_kv_timeout.setter
    def sync_kv_timeout(self, sync_kv_timeout):
        check_isinstance("sync_kv_timeout", sync_kv_timeout, [int, str])
        if isinstance(sync_kv_timeout, str):
            raise_if_false(sync_kv_timeout.isdigit(), "sync_kv_timeout must be digit.")
        raise_if_false(int(sync_kv_timeout) > 0, "sync_kv_timeout should be greater than zero.")
        check_int32('sync_kv_timeout', int(sync_kv_timeout))
        self._sync_kv_timeout = sync_kv_timeout

    @process_request_timeout.setter
    def process_request_timeout(self, process_request_timeout):
        check_isinstance("process_request_timeout", process_request_timeout, [int, str])
        self._process_request_timeout = process_request_timeout

    @property
    def enable_switch_role(self):
        return self._enable_switch_role

    @enable_switch_role.setter
    def enable_switch_role(self, enable_switch_role: bool):
        check_isinstance("enable_switch_role", enable_switch_role, [bool])
        self._enable_switch_role = enable_switch_role

    @property
    def enable_cache_manager(self):
        return self._enable_cache_manager

    @enable_cache_manager.setter
    def enable_cache_manager(self, enable_cache_manager: bool):
        check_isinstance("enable_cache_manager", enable_cache_manager, [bool])
        self._enable_cache_manager = enable_cache_manager

    @property
    def enable_remote_cache_accessible(self):
        return self._enable_remote_cache_accessible

    @enable_remote_cache_accessible.setter
    def enable_remote_cache_accessible(self, enable_remote_cache_accessible: bool):
        check_isinstance("enable_remote_cache_accessible", enable_remote_cache_accessible, [bool])
        self._enable_remote_cache_accessible = enable_remote_cache_accessible

    @property
    def mem_pool_cfg(self) -> str:
        return self._mem_pool_cfg

    @mem_pool_cfg.setter
    def mem_pool_cfg(self, mem_pool_cfg: str):
        check_isinstance("mem_pool_cfg", mem_pool_cfg, str)
        self._mem_pool_cfg = mem_pool_cfg

    @property
    def host_mem_pool_cfg(self) -> str:
        return self._host_mem_pool_cfg

    @host_mem_pool_cfg.setter
    def host_mem_pool_cfg(self, host_mem_pool_cfg: str):
        check_isinstance("host_mem_pool_cfg", host_mem_pool_cfg, str)
        self._host_mem_pool_cfg = host_mem_pool_cfg


class ModelConfig(object):
    def __init__(self):
        self._options = {}
        self._ge_options = {}
        self._kv_shapes = []
        self._kv_dtypes = []
        self._kv_seq_len_dim_index = None
        self._enable_page_attention = False
        self._enable_shared_system_prefix = False
        self._om_paths = []

        self._input_batch_dim_index = ""
        self._input_shapes = ""
        self._input_dtypes = []
        self._ref_input_shapes = ""
        self._ref_input_dtypes = []
        self._output_nums = ""
        self._kv_cache_counts = ""
        self._is_flow_model = False
        self._pipeline_input_indices = ""
        self._input_paddings = ""
        self._postprocess_input_shapes = ""
        self._postprocess_input_dtypes = []
        self._postprocess_output_nums = ""
        self._output_mapping = ""
        self._pipeline_execution = None
        self._page_attention_single_block_size = ""
        self._page_attention_block_num = ""
        self._page_attention_max_seq_len = ""
        self._page_attention_max_seq_num = ""
        self._page_attention_max_prompt_len = ""
        self._kv_tensor_format = None

    def generate_options(self):
        """
        生成模型配置项
        Returns:
            配置项dict
        """
        if self.kv_shapes:
            self._options["llm.RefInputShapes"] = ";".join(self.kv_shapes)
        if self.kv_dtypes:
            self._options["llm.RefInputDtypes"] = ";".join([str(val.value) for val in self.kv_dtypes])
        if self.kv_seq_len_dim_index is not None:
            self._options["llm.RefInputSeqLenDimIndex"] = str(self.kv_seq_len_dim_index)
        if self.om_paths:
            self._options["llm.OmCachePath"] = ";".join(self.om_paths)
        return self.gen_options()

    def gen_options(self):
        if self.enable_shared_system_prefix:
            self._options["llm.EnableSharedSystemPrefix"] = "1"
        if self.ge_options:
            self._options.update(self.ge_options)
        if self.is_flow_model:
            self._options["llm.IsFlowModel"] = "1"
        if self.enable_page_attention:
            self._options["llm.EnablePagedAttention"] = "1"
        if self._page_attention_single_block_size:
            self._options["llm.PagedAttentionBlockSize"] = str(self.page_attention_single_block_size)
        if self._page_attention_block_num:
            self._options["llm.PagedAttentionBlocksNum"] = str(self.page_attention_block_num)
        if self._page_attention_max_seq_len:
            self._options["llm.PagedAttentionMaxSeqLen"] = str(self.page_attention_max_seq_len)
        if self._page_attention_max_seq_num:
            self._options["llm.PagedAttentionMaxSeqsNum"] = str(self.page_attention_max_seq_num)
        if self._page_attention_max_prompt_len:
            self._options["llm.PagedAttentionMaxPromptLen"] = str(self.page_attention_max_prompt_len)
        if self.input_batch_dim_index:
            self._options["llm.InputsBatchSizeDimIndex"] = str(self.input_batch_dim_index)
        if self.input_shapes:
            self._options["llm.InputShapes"] = str(self.input_shapes)
        if self.input_dtypes:
            self._options["llm.InputDtypes"] = ";".join([str(val.value) for val in self.input_dtypes])
        if self.ref_input_shapes:
            self._options["llm.RefInputShapes"] = str(self.ref_input_shapes)
        if self.ref_input_dtypes:
            self._options["llm.RefInputDtypes"] = ";".join([str(val.value) for val in self.ref_input_dtypes])
        if self.kv_tensor_format:
            self._options["llm.kvTensorFormat"] = self.kv_tensor_format
        if self.output_nums:
            self._options["llm.OutputNums"] = str(self.output_nums)
        if self.kv_cache_counts:
            self._options["llm.KvCacheCounts"] = str(self.kv_cache_counts)
        self._gen_extra_options()
        return self.options

    def _gen_extra_options(self):
        if self._output_mapping:
            self._options["llm.OutputMapping"] = str(self.output_mapping)
        if self.pipeline_input_indices:
            self._options["llm.PipelineInputIndices"] = str(self.pipeline_input_indices)
        if self._postprocess_input_shapes:
            self._options["llm.PostProcessInputShapes"] = str(self.postprocess_input_shapes)
        if self._postprocess_input_dtypes:
            self._options["llm.PostProcessInputDtypes"] = ";".join(
                [str(val.value) for val in self.postprocess_input_dtypes])
        if self._postprocess_output_nums:
            self._options["llm.PostProcessOutputNums"] = str(self.postprocess_output_nums)
        if self._pipeline_execution is not None:
            self._options["llm.PipelineExecution"] = "enable" if self.pipeline_execution else "disable"

    @property
    def om_paths(self):
        return self._om_paths

    @om_paths.setter
    def om_paths(self, om_paths):
        check_isinstance("om_paths", om_paths, list, str)
        self._om_paths = om_paths

    @property
    def kv_shapes(self):
        return self._kv_shapes

    @kv_shapes.setter
    def kv_shapes(self, kv_shapes):
        check_isinstance("kv_shapes", kv_shapes, list, str)
        self._kv_shapes = kv_shapes

    @property
    def kv_dtypes(self):
        return self._kv_dtypes

    @kv_dtypes.setter
    def kv_dtypes(self, kv_dtypes):
        check_isinstance("kv_dtypes", kv_dtypes, list, DataType)
        self._kv_dtypes = kv_dtypes

    @property
    def kv_seq_len_dim_index(self):
        return self._kv_seq_len_dim_index

    @kv_seq_len_dim_index.setter
    def kv_seq_len_dim_index(self, kv_seq_len_dim_index):
        check_isinstance("kv_seq_len_dim_index", kv_seq_len_dim_index, int)
        raise_if_false(kv_seq_len_dim_index >= 0, "kv_seq_len_dim_index should be greater than or equal to zero.")
        self._kv_seq_len_dim_index = kv_seq_len_dim_index

    @property
    def is_flow_model(self):
        return self._is_flow_model

    @is_flow_model.setter
    def is_flow_model(self, is_flow_model):
        check_isinstance("is_flow_model", is_flow_model, bool)
        self._is_flow_model = is_flow_model

    @property
    def enable_shared_system_prefix(self):
        return self._enable_shared_system_prefix

    @enable_shared_system_prefix.setter
    def enable_shared_system_prefix(self, enable_shared_system_prefix):
        check_isinstance("enable_shared_system_prefix", enable_shared_system_prefix, bool)
        self._enable_shared_system_prefix = enable_shared_system_prefix

    @property
    def postprocess_input_shapes(self):
        return self._postprocess_input_shapes

    @property
    def postprocess_input_dtypes(self):
        return self._postprocess_input_dtypes

    @property
    def postprocess_output_nums(self):
        return self._postprocess_output_nums

    @property
    def output_mapping(self):
        return self._output_mapping

    @property
    def pipeline_execution(self):
        return self._pipeline_execution

    @property
    def enable_page_attention(self):
        return self._enable_page_attention

    @property
    def page_attention_single_block_size(self):
        return self._page_attention_single_block_size

    @property
    def page_attention_block_num(self):
        return self._page_attention_block_num

    @property
    def page_attention_max_seq_len(self):
        return self._page_attention_max_seq_len

    @property
    def page_attention_max_seq_num(self):
        return self._page_attention_max_seq_num

    @property
    def page_attention_max_prompt_len(self):
        return self._page_attention_max_prompt_len

    @property
    def options(self):
        return self._options

    @property
    def ge_options(self):
        return self._ge_options

    @property
    def input_batch_dim_index(self):
        return self._input_batch_dim_index

    @property
    def input_shapes(self):
        return self._input_shapes

    @property
    def input_dtypes(self):
        return self._input_dtypes

    @property
    def ref_input_shapes(self):
        return self._ref_input_shapes

    @property
    def ref_input_dtypes(self):
        return self._ref_input_dtypes

    @property
    def output_nums(self):
        return self._output_nums

    @property
    def kv_cache_counts(self):
        return self._kv_cache_counts

    @property
    def pipeline_input_indices(self):
        return self._pipeline_input_indices

    @property
    def kv_tensor_format(self):
        return self._kv_tensor_format

    @kv_tensor_format.setter
    def kv_tensor_format(self, kv_tensor_format):
        check_isinstance("kv_tensor_format", kv_tensor_format, str)
        self._kv_tensor_format = kv_tensor_format

    @output_mapping.setter
    def output_mapping(self, output_mapping):
        check_isinstance("output_mapping", output_mapping, str)
        self._output_mapping = output_mapping

    @pipeline_execution.setter
    def pipeline_execution(self, pipeline_execution):
        check_isinstance("pipeline_execution", pipeline_execution, bool)
        self._pipeline_execution = pipeline_execution

    @enable_page_attention.setter
    def enable_page_attention(self, enable_page_attention):
        check_isinstance("enable_page_attention", enable_page_attention, [bool])
        self._enable_page_attention = enable_page_attention

    @page_attention_single_block_size.setter
    def page_attention_single_block_size(self, page_attention_single_block_size):
        check_isinstance("page_attention_single_block_size", page_attention_single_block_size, [int, str])
        self._page_attention_single_block_size = page_attention_single_block_size

    @page_attention_block_num.setter
    def page_attention_block_num(self, page_attention_block_num):
        check_isinstance("page_attention_block_num", page_attention_block_num, [int, str])
        self._page_attention_block_num = page_attention_block_num

    @page_attention_max_seq_len.setter
    def page_attention_max_seq_len(self, page_attention_max_seq_len):
        check_isinstance("page_attention_max_seq_len", page_attention_max_seq_len, [int, str])
        self._page_attention_max_seq_len = page_attention_max_seq_len

    @page_attention_max_seq_num.setter
    def page_attention_max_seq_num(self, page_attention_max_seq_num):
        check_isinstance("page_attention_max_seq_num", page_attention_max_seq_num, [int, str])
        self._page_attention_max_seq_num = page_attention_max_seq_num

    @page_attention_max_prompt_len.setter
    def page_attention_max_prompt_len(self, page_attention_max_prompt_len):
        check_isinstance("page_attention_max_prompt_len", page_attention_max_prompt_len, [int, str])
        self._page_attention_max_prompt_len = page_attention_max_prompt_len

    @ge_options.setter
    def ge_options(self, ge_options):
        check_isinstance("ge_options", ge_options, dict)
        check_dict("ge_options", ge_options, str, str)
        self._ge_options = ge_options

    @input_batch_dim_index.setter
    def input_batch_dim_index(self, input_batch_dim_index):
        check_isinstance("input_batch_dim_index", input_batch_dim_index, str)
        self._input_batch_dim_index = input_batch_dim_index

    @input_shapes.setter
    def input_shapes(self, input_shapes):
        check_isinstance("input_shapes", input_shapes, str)
        self._input_shapes = input_shapes

    @input_dtypes.setter
    def input_dtypes(self, input_dtypes):
        check_isinstance("input_dtypes", input_dtypes, list, DataType)
        self._input_dtypes = input_dtypes

    @ref_input_shapes.setter
    def ref_input_shapes(self, ref_input_shapes):
        check_isinstance("ref_input_shapes", ref_input_shapes, str)
        self._ref_input_shapes = ref_input_shapes

    @ref_input_dtypes.setter
    def ref_input_dtypes(self, ref_input_dtypes):
        check_isinstance("ref_input_dtypes", ref_input_dtypes, list, DataType)
        self._ref_input_dtypes = ref_input_dtypes

    @output_nums.setter
    def output_nums(self, output_nums):
        check_isinstance("output_nums", output_nums, str)
        self._output_nums = output_nums

    @kv_cache_counts.setter
    def kv_cache_counts(self, kv_cache_counts):
        check_isinstance("kv_cache_counts", kv_cache_counts, str)
        self._kv_cache_counts = kv_cache_counts

    @pipeline_input_indices.setter
    def pipeline_input_indices(self, pipeline_input_indices):
        check_isinstance("pipeline_input_indices", pipeline_input_indices, str)
        self._pipeline_input_indices = pipeline_input_indices

    @postprocess_input_shapes.setter
    def postprocess_input_shapes(self, postprocess_input_shapes):
        check_isinstance("postprocess_input_shapes", postprocess_input_shapes, str)
        self._postprocess_input_shapes = postprocess_input_shapes

    @postprocess_input_dtypes.setter
    def postprocess_input_dtypes(self, postprocess_input_dtypes):
        check_isinstance("postprocess_input_dtypes", postprocess_input_dtypes, list, DataType)
        self._postprocess_input_dtypes = postprocess_input_dtypes

    @postprocess_output_nums.setter
    def postprocess_output_nums(self, postprocess_output_nums):
        check_isinstance("postprocess_output_nums", postprocess_output_nums, [int, str])
        self._postprocess_output_nums = postprocess_output_nums


class LLMModelStatus:
    def __init__(self, num_free_blocks):
        self._num_free_blocks = num_free_blocks

    @property
    def num_free_blocks(self):
        return self._num_free_blocks

    @num_free_blocks.setter
    def num_free_blocks(self, num_free_blocks):
        check_isinstance("num_free_blocks", num_free_blocks, int)
        self._num_free_blocks = num_free_blocks


class LLMEngineStatus:
    def __init__(self, empty_max_prompt_kv, num_free_blocks):
        self._empty_max_prompt_kv = empty_max_prompt_kv
        self._num_free_blocks = num_free_blocks

    @property
    def empty_max_prompt_kv(self):
        return self._empty_max_prompt_kv

    @property
    def num_free_blocks(self):
        return self._num_free_blocks

    @empty_max_prompt_kv.setter
    def empty_max_prompt_kv(self, empty_max_prompt_kv):
        check_isinstance("empty_max_prompt_kv", empty_max_prompt_kv, [int])
        self._empty_max_prompt_kv = empty_max_prompt_kv

    @num_free_blocks.setter
    def num_free_blocks(self, num_free_blocks):
        check_isinstance("num_free_blocks", num_free_blocks, int)
        self._num_free_blocks = num_free_blocks
