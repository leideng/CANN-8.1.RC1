#!/usr/bin/env python
# coding=utf-8
"""
Function:
This file mainly involves op file content.
Copyright Information:
Huawei Technologies Co., Ltd. All Rights Reserved Â© 2020-2021
"""


class OPTmpl:
    """
    The class for OP template
    """
    # ==================1.ini file==================
    INI_OP = """[{op_type}]
"""
    INI_INPUT = """input{index}.name={name}
input{index}.dtype={dtype}
input{index}.paramType={paramType}
input{index}.format={format}
"""
    INI_OUTPUT = """output{index}.name={name}
output{index}.dtype={dtype}
output{index}.paramType={paramType}
output{index}.format={format}
"""
    INI_ATTR_LIST = """attr.list={attr_info}
"""
    INI_ATTR_TYPE_VALUE = """attr_{name}.type={type}
attr_{name}.value=all
"""
    INI_ATTR_PARAM_TYPE = """attr_{name}.paramType={paramType}
"""
    INI_ATTR_DEFAULT_VALUE = """attr_{name}.defaultValue={defaultValue}
"""
    INI_BIN_FILE = """opFile.value={name}
opInterface.value={name}
"""
    # =============================================
    # ==================2.IR file==================
    IR_H_HEAD = """/**
 * Copyright (C)  2020-2021. Huawei Technologies Co., Ltd. All rights reserved.

 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the Apache License Version 2.0.You may not use this file except in compliance with the License.

 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * Apache License for more details at
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * @brief
 *
 * @version 1.0
 *
 */

#ifndef GE_OP_{op_type_upper}_H
#define GE_OP_{op_type_upper}_H
#include "graph/operator_reg.h"
namespace ge {left_braces}

REG_OP({op_type})
"""
    IR_H_INPUT = """    .INPUT({name}, TensorType({{{type}}}))
"""
    IR_H_DYNAMIC_INPUT = """    .DYNAMIC_INPUT({name}, TensorType({{{type}}}))
"""
    IR_H_OUTPUT = """    .OUTPUT({name}, TensorType({{{type}}}))
"""
    IR_H_DYNAMIC_OUTPUT = """    .DYNAMIC_OUTPUT({name}, TensorType({{{type}}}))
"""
    IR_H_ATTR_WITHOUT_VALUE = """    .REQUIRED_ATTR({name}, {type})
"""
    IR_H_ATTR_WITH_VALUE = """    .ATTR({name}, {type}, {value})
"""
    IR_H_END = """    .OP_END_FACTORY_REG({op_type})
{right_braces}
#endif //GE_OP_{op_type_upper}_H
"""
    IR_CPP_HEAD = """#include "{fix_op_type}.h"
namespace ge {left_braces}

IMPLEMT_COMMON_INFERFUNC({op_type}InferShape)
{left_braces}
    return GRAPH_SUCCESS;
{right_braces}

IMPLEMT_VERIFIER({op_type}, {op_type}Verify)
{left_braces}
    return GRAPH_SUCCESS;
{right_braces}

COMMON_INFER_FUNC_REG({op_type}, {op_type}InferShape);
VERIFY_FUNC_REG({op_type}, {op_type}Verify);

{right_braces}  // namespace ge
"""
    # =================================================
    # ==================3.plugin file==================
    PLUGIN_COPYRIGHT = """/* Copyright (C) 2020-2021. Huawei Technologies Co., Ltd. All
rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the Apache License Version 2.0.
 * You may not use this file except in compliance with the License.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * Apache License for more details at
 * http://www.apache.org/licenses/LICENSE-2.0
 */

"""
    TF_PLUGIN_CPP = PLUGIN_COPYRIGHT + """#include "register/register.h"

namespace domi {left_braces}
// register op info to GE
REGISTER_CUSTOM_OP("{name}")
    .FrameworkType({fmk_type})   // type: CAFFE, TENSORFLOW
    .OriginOpType("{name}")      // name in tf module
    .ParseParamsByOperatorFn(AutoMappingByOpFn);
{right_braces}  // namespace domi
"""

    ONNX_PLUGIN_CPP = PLUGIN_COPYRIGHT + """#include "register/register.h"

namespace domi {left_braces}
// Onnx ParseParams
Status ParseParam{name}(const ge::Operator& op_src, ge::Operator& op_dest) {left_braces}
    // To do: Implement the operator plugin by referring to the Onnx Operator Development Guide.
    return SUCCESS;
{right_braces}

// register {name} op info to GE
REGISTER_CUSTOM_OP("{name}")     // Set the registration name of operator
    .FrameworkType({fmk_type})   // Operator name with the original framework
    .OriginOpType("")      // Set the original frame type of the operator
    .ParseParamsByOperatorFn(ParseParam{name}); // Registering the callback function for parsing operator parameters
{right_braces}  // namespace domi
"""

    CAFFE_PLUGIN_CPP = PLUGIN_COPYRIGHT + """#include "register/register.h"
#include "graph/operator.h"

using namespace ge;

namespace domi
 {left_braces}
// Caffe ParseParams
Status ParseParam{name}(const Operator& op_src, ge::Operator& op_dest)
{left_braces}
    // To do: Implement the operator plug-in by referring to the TBE Operator Development Guide.
    return SUCCESS;
{right_braces}

// register op info to GE
REGISTER_CUSTOM_OP("{name}")
    .FrameworkType({fmk_type})    // type: CAFFE, TENSORFLOW
    .OriginOpType("{name}")       // name in caffe module
    .ParseParamsByOperatorFn(ParseParam{name});
{right_braces} // namespace domi
"""

    PLUGIN_CMAKLIST = """# Copyright (c) Huawei Technologies Co., Ltd. 2020-2021. All rights reserved.
aux_source_directory(. SRCS)
message(STATUS "SRCS = ${SRCS}")

if("x${SRCS}" STREQUAL "x")
    add_custom_target(${TF_PLUGIN_TARGET}
            COMMAND mkdir -p ${TF_PLUGIN_TARGET_OUT_DIR}
            COMMAND echo "no source to make lib${TF_PLUGIN_TARGET}.so")
    return(0)
endif()

set(LIBRARY_OUTPUT_PATH ${TF_PLUGIN_TARGET_OUT_DIR})

add_library(${TF_PLUGIN_TARGET} SHARED ${SRCS})

target_compile_definitions(${TF_PLUGIN_TARGET} PRIVATE
    google=ascend_private
)

target_link_libraries(${TF_PLUGIN_TARGET} ${ASCEND_INC}/../lib64/libgraph.so)
"""

    ONNX_PLUGIN_CMAKLIST = """# Copyright (c) Huawei Technologies Co., Ltd. 2020-2021. All rights reserved.
aux_source_directory(. SRCS)
message(STATUS "SRCS = ${SRCS}")

if("x${SRCS}" STREQUAL "x")
    add_custom_target(${ONNX_PLUGIN_TARGET}
            COMMAND mkdir -p ${ONNX_PLUGIN_TARGET_OUT_DIR}
            COMMAND echo "no source to make lib${ONNX_PLUGIN_TARGET}.so")
    return(0)
endif()

set(LIBRARY_OUTPUT_PATH ${ONNX_PLUGIN_TARGET_OUT_DIR})

add_library(${ONNX_PLUGIN_TARGET} SHARED ${SRCS})

target_compile_definitions(${ONNX_PLUGIN_TARGET} PRIVATE
    google=ascend_private
)

target_link_libraries(${ONNX_PLUGIN_TARGET} ${ASCEND_INC}/../lib64/libgraph.so)
"""

    CAFFE_PLUGIN_CMAKLIST = """# Copyright (c) Huawei Technologies Co., Ltd. 2020-2021. All rights reserved.

aux_source_directory(. SRCS)
aux_source_directory(./proto/caffe PROTO_SRCS)
list(APPEND SRCS ${PROTO_SRCS})

message(STATUS "SRCS = ${SRCS}")

if("x${SRCS}" STREQUAL "x")
    add_custom_target(${CAFFE_PLUGIN_TARGET}
            COMMAND mkdir -p ${CAFFE_PLUGIN_TARGET_OUT_DIR}
            COMMAND echo "no source to make lib${CAFFE_PLUGIN_TARGET}.so")
    return(0)
endif()

set(LIBRARY_OUTPUT_PATH ${CAFFE_PLUGIN_TARGET_OUT_DIR})

include_directories(./proto/caffe)
add_library(${CAFFE_PLUGIN_TARGET} SHARED ${SRCS})
"""

    COMMON_PLUGIN_CMAKLIST = """
aux_source_directory(${CMAKE_CURRENT_SOURCE_DIR} plugin_srcs)
add_library(cust_PLUGINTYPE_parsers SHARED ${plugin_srcs})
target_compile_definitions(cust_PLUGINTYPE_parsers PRIVATE google=ascend_private)
if(ENABLE_CROSS_COMPILE)
    target_link_directories(cust_PLUGINTYPE_parsers PRIVATE
                            ${CMAKE_COMPILE_COMPILER_LIBRARY}
                            ${CMAKE_COMPILE_RUNTIME_LIBRARY}
    )
endif()
target_link_libraries(cust_PLUGINTYPE_parsers PRIVATE intf_pub graph)
install(TARGETS cust_PLUGINTYPE_parsers
        LIBRARY DESTINATION packages/vendors/${vendor_name}/framework/PLUGINNAME
)
"""

    CAFFE_CUSTOM_PROTO = """
syntax = "proto2";
package domi.caffe;
message NetParameter {
  optional string name = 1; // consider giving the network a name
  // The layers that make up the net.  Each of their configurations, including
  // connectivity and behavior, is specified as a LayerParameter.
  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.

}
message LayerParameter {
  optional string name = 1;  // the layer name
  optional string type = 2;  // the layer type

  // Add new LayerParameter here.
  optional CustomTestParameter custom_test_param = 1000;
}

// Add the definition of LayerParameter here.
message CustomTestParameter {
    optional bool adj_x1 = 1 [default = false];
    optional bool adj_x2 = 2 [default = false];
}
"""
    # =================================================
    # ==================4.impl file==================
    PY_HEAD = """import tbe.dsl as tbe
from tbe import tvm
from tbe.common.register import register_op_compute
from tbe.common.utils import para_check

"""
    PY_COMPUTE_WITHOUT_ATTR = """
@register_op_compute("{name}")
def {name}_compute({input_name}, {output}, kernel_name="{name}"):
    \"""
    To do: Implement the operator by referring to the
           TBE Operator Development Guide.
    \"""
"""
    PY_COMPUTE_WITH_ATTR = """
@register_op_compute("{name}")
def {name}_compute({input_name}, {output}, {attr}, kernel_name="{name}"):
    \"""
    To do: Implement the operator by referring to the
           TBE Operator Development Guide.
    \"""
"""
    PY_COMPUTE_END = """
    res = tbe.XXX({input_name})
    return res
"""
    PY_DEF_WITHOUT_ATTR = """
@para_check.check_op_params({op_params})
def {name}({input_name}, {output}, kernel_name="{name}"):
    \"""
    To do: Implement the operator by referring to the
           TBE Operator Development Guide.
    \"""
"""
    PY_DEF_WITH_ATTR = """
@para_check.check_op_params({op_params})
def {name}({input_name}, {output}, {attr}, kernel_name="{name}"):
    \"""
    To do: Implement the operator by referring to the
           TBE Operator Development Guide.
    \"""
"""
    PY_PLACEHOLDER = \
        """    data_{name} = tvm.placeholder({name}.get(\"shape\"), dtype={name}.get(\"dtype\"), name=\"data_{name}\")
"""

    PY_RES_WITHOUT_ATTR = """
    res = {name}_compute({input_data}, {output_data}, kernel_name)
"""
    PY_RES_WIT_ATTR = """
    res = {name}_compute({input_data}, {output_data}, {attr}, kernel_name)
"""
    PY_TARGET_CCE = """
    # auto schedule
    with tvm.target.cce():
        schedule = tbe.auto_schedule(res)
"""
    PY_BUILD = """
    # operator build
    config = {left_braces}"name": kernel_name,
              "tensor_list": [{input_data}, res]{right_braces}
    tbe.build(schedule, config)
    """
    # ==================4.2 MindSpore python file================
    PY_MS_HEAD = """from __future__ import absolute_import
from tbe import tvm
import tbe.dsl as tbe
from tbe.common.register import register_op_compute
from tbe.common.utils import shape_refine
from mindspore.ops.op_info_register import op_info_register, TBERegOp, DataType


"""
    PY_MS_COMPUTE = """@register_op_compute("{name}")
def {name}_compute({input_name}, {output}):
    \"""
    The compute function of the {up_name} implementation.
    \"""
    res = tbe.XXX({input_name})
    return res

"""
    PY_MS_ATTR_WITHOUT_VALUE_INFO = \
        """.attr("{attr_name}", "{param_type}", "{attr_type}", "all")\\"""
    PY_MS_DATA_DESC_INFO = """.{data_desc}({index}, "{key_name}", False, "{param_type}", "all")\\"""
    PY_MS_DATA_TYPE = """DataType.{data_type}"""
    PY_MS_DTYPE_FORMAT = """.dtype_format({data_types_join})\\"""
    PY_MS_OP_INFO = """
# Define the kernel info of {up_name}.
{name}_op_info = TBERegOp("{up_name}") \\
    .fusion_type("OPAQUE") \\
    .partial_flag(True) \\
    .async_flag(False) \\
    .binfile_name("{name}.so") \\
    .compute_cost(10) \\
    .kernel_name("{name}_impl") \\
    {inputs}
    {outputs}
    {data_types}
    .get_op_info()

"""
    PY_MS_OP_INFO_REGISTER_TVM = \
        """input{index} = tvm.placeholder(shape, name="input{index}", dtype=dtype.lower())"""
    PY_MS_OP_INFO_REGISTER = """
# Binding kernel info with the kernel implementation.
@op_info_register({name}_op_info)
def {name}_impl({input_name}, {output}, kernel_name="{name}_impl"):
    \"""
    The entry function of the {up_name} implementation.
    \"""
    shape = {input_x}.get("shape")
    dtype = {input_x}.get("dtype").lower()

    shape = shape_refine(shape)
    {tvm_placeholder}

    with tvm.target.cce():
        res = {name}_compute({all_inputs}, {output})
        sch = tbe.auto_schedule(res)
"""
    PY_MS_OP_INFO_REGISTER_CONFIG = """
    config = {{"print_ir": False,
              "name": kernel_name,
              "tensor_list": [{all_inputs}, res]}}

    tbe.build(sch, config)
"""
    PY_MS_PROTO_HEAD = """from mindspore.ops import prim_attr_register, \
PrimitiveWithInfer
import mindspore.ops as ops
# description


class {up_name}(PrimitiveWithInfer):
    \"""
    The definition of the {up_name} primitive.
    \"""
    @prim_attr_register
    def __init__({init_attr}):
        self.init_prim_io_names(inputs={input_name}, outputs={output})
        # Import the entry function of the kernel implementation from relative
        #  path or PYTHONPATH.
        from {name}_impl import {op_register_func}

    def infer_shape(self, {input_shapes}):
        return {first_input_shape}

    def infer_dtype(self, {input_dtypes}):
        return {first_input_dtype}"""
    # ==================5.AICPU ini file==================
    AICPU_INI_STRING = """[{op_type}]
opInfo.engine=DNN_VM_AICPU
opInfo.flagPartial=False
opInfo.computeCost=100
opInfo.flagAsync=False
opInfo.opKernelLib=CUSTAICPUKernel
opInfo.kernelSo=libcust_aicpu_kernels.so
opInfo.functionName=RunCpuKernel
opInfo.workspaceSize=1024
"""
    # =============================================
    # ==================6.AICPU impl cc file==================
    AICPU_IMPL_CPP_STRING = """
/*
 * Copyright (c) Huawei Technologies Co., Ltd. 2020-2021. All rights reserved.
 * Description: implement of {op_type}
 */
#include "{fix_op_type}_kernels.h"

namespace  {left_braces}
const char *{op_type_upper} = "{op_type}";
{right_braces}

namespace aicpu  {left_braces}
uint32_t {op_type}CpuKernel::Compute(CpuKernelContext &ctx)
{left_braces}
    return 0;
{right_braces}

REGISTER_CPU_KERNEL({op_type_upper}, {op_type}CpuKernel);
{right_braces} // namespace aicpu
"""
    # =============================================
    # ==================7.AICPU impl h file==================
    AICPU_IMPL_H_STRING = """
/*
 * Copyright (c) Huawei Technologies Co., Ltd. 2020-2021. All rights reserved.
 * Description: api of {op_type}
 */

#ifndef _{op_type_upper}_KERNELS_H_
#define _{op_type_upper}_KERNELS_H_

#include "cpu_kernel.h"

namespace aicpu {left_braces}
class {op_type}CpuKernel : public CpuKernel {left_braces}
public:
    ~{op_type}CpuKernel() = default;
    virtual uint32_t Compute(CpuKernelContext &ctx) override;
{right_braces};
{right_braces} // namespace aicpu
#endif
"""

    # ==================4.4 MindSpore AICPU python file================
    PY_MS_AICPU_HEAD = """from mindspore.ops.op_info_register import op_info_register, AiCPURegOp, DataType"""

    PY_MS_AICPU_ATTR_INFO = \
        """.attr("{attr_name}", "{attr_type}")\\"""

    PY_MS_AICPU_CUST_ATTR_INFO = \
        """.attr("cust_aicpu", "str")\\"""

    PY_MS_AICPU_DATA_DESC_INFO = """.{data_desc}({index}, "{key_name}", "{param_type}")\\"""

    PY_MS_AICPU_REGISTER_OP_INFO = """
# Define the kernel info of {up_name}.
# .attr("cust_aicpu", "str") indicate that the attribute is used to obtain *.so file for the kernel name.
{name}_op_info = AiCPURegOp("{up_name}") \\
    .fusion_type("OPAQUE") \\
    {inputs}
    {outputs}
    {data_types}
    .get_op_info()


@op_info_register({name}_op_info)
def _{name}_aicpu():
    \"\"\"{up_name} AiCPU register\"\"\"
    return
    """

    OP_HOST_TILING_DEF_H = """
#include \"register/tilingdata_base.h\"

namespace optiling {{
BEGIN_TILING_DATA_DEF({op_type}TilingData)
  TILING_DATA_FIELD_DEF(uint32_t, size);
END_TILING_DATA_DEF;

REGISTER_TILING_DATA_CLASS({op_type}, {op_type}TilingData)
}}
"""

    OP_HOST_CPP_HEADER = """
#include \"{op_fix}_tiling.h\"
#include \"register/op_def_registry.h\"

"""

    OP_HOST_TILING_FUNC = """
namespace optiling {{
static ge::graphStatus TilingFunc(gert::TilingContext* context)
{{

  {op_type}TilingData tiling;
  const gert::StorageShape* x1_shape = context->GetInputShape(0);
  int32_t data_sz = 1;
  for (int i = 0; i < x1_shape->GetStorageShape().GetDimNum(); i++)
    data_sz *= x1_shape->GetStorageShape().GetDim(i);
  tiling.set_size(data_sz);
  context->SetBlockDim(8);
  tiling.SaveToBuffer(context->GetRawTilingData()->GetData(), context->GetRawTilingData()->GetCapacity());
  context->GetRawTilingData()->SetDataSize(tiling.GetDataSize());

  return ge::GRAPH_SUCCESS;
}}
}}

"""

    OP_HOST_INFER_FUNC = """
namespace ge {
static ge::graphStatus InferShape(gert::InferShapeContext* context)
{
    const gert::Shape* x1_shape = context->GetInputShape(0);
    gert::Shape* y_shape = context->GetOutputShape(0);
    *y_shape = *x1_shape;
    return GRAPH_SUCCESS;
}
static ge::graphStatus InferDataType(gert::InferDataTypeContext *context)
{
const auto inputDataType = context->GetInputDataType(0);
context->SetOutputDataType(0, inputDataType);
return ge::GRAPH_SUCCESS;
}
}

"""

    OP_HOST_DEF_BEGIN = """
namespace ops {{
class {op_type} : public OpDef {{
public:
    explicit {op_type}(const char* name) : OpDef(name)
    {{
"""

    OP_HOST_INFER_REG = """
        this->SetInferShape(ge::InferShape).SetInferDataType(ge::InferDataType);
"""

    OP_HOST_TILING_REG = """
        this->AICore()
            .SetTiling(optiling::TilingFunc);
"""

    OP_HOST_DEF_END = """
    }}
}};

OP_ADD({op_type});
}}
"""

    CMAKE_KERNEL_CMAKELISTS_FILE = """
# set custom compile options
if ("${{CMAKE_BUILD_TYPE}}x" STREQUAL "Debugx")
    add_ops_compile_options(ALL OPTIONS -g -O0 --cce-ignore-always-inline=true)
endif()

# Multi-operator should add OpName and kernel file
add_kernel_compile({op_name} ${{CMAKE_CURRENT_SOURCE_DIR}}/{kernel_name}.cpp)

if (ENABLE_TEST AND EXISTS ${{CMAKE_CURRENT_SOURCE_DIR}}/testcases)
    add_subdirectory(testcases)
endif()
    """

    # =======================================================

    def get_ini_op(self: any) -> str:
        """
        get ini op
        """
        return self.INI_OP

    def get_ini_input(self: any) -> str:
        """
        get ini input
        """
        return self.INI_INPUT
