#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

RL-TUNE
"""
import datetime
import json
import os
import time
import traceback
from collections import namedtuple

from schedule_search import log
from schedule_search import tune_util
from schedule_search import util
from schedule_search import soc_cfg
from schedule_search.global_manager import MultiprocessManager
from schedule_search.rl import tune
from schedule_search.rl_online_tune import get_tensor_list
from schedule_search.rl_progress_report import report_rl_tune_progress
from schedule_search.ts_env.estimator import om_runner
from schedule_search.tune_util import rl_check_sch
from schedule_search.tune_util import load_json_str
from tbe.common.rl_bank import bank_manager
from tbe.common.rl_bank.rl_op_filter import rl_op_filter

MEANINGLESS_TIME = 1
COMPILE_LIMIT = 30
PostRetList = namedtuple("PostRetList", ["sch_list", "cce_special_list", "tik_tensor_list"])


def get_op_res(kernel_name: str) -> (list, list, list):
    """
    获取算子对应的op_schedule_info.schedule_obj以及cce special list写进文件
    :param kernel_name: 算子的kernel name
    :return: (sch_list, cce_special_list)
    """
    sch_list = bank_manager.get_op_res(kernel_name)
    if not sch_list:
        log.warn("Call bank_manager.get_op_res: kernel name is %s, but get None.", kernel_name)
        return [], [], []

    cce_special_list = []
    for sch in sch_list:
        cce_special_list.append(sch.cce_special)

    tik_tensor_info = bank_manager.get_tik_tensor(kernel_name)
    tik_tensor_list = tune_util.parse_tik_tensor_info(tik_tensor_info)

    return sch_list, cce_special_list, tik_tensor_list


def base_build_compile(kernel_name: str,
                       op_json_str: str,
                       option: dict) -> dict:
    """
    base build编译部分
    :param kernel_name: 算子的kernel name
    :param op_json_str: 单个算子描述信息
    :param option: 算子对应的调优配置信息
    :return: compile_key, compile_res_dict, compile_release_q
    """
    json_dict = json.loads(op_json_str)
    if rl_op_filter.is_conv2d_l1fusion(json_dict):
        json_dict["is_rl_conv2d_l1fusion"] = True
        op_json_str = json.dumps(json_dict)

    func_in_compile = {
        'prerun': (bank_manager.set_current_op_name, [kernel_name]),
        'postrun': (get_op_res, [kernel_name])
        }

    compile_res = {}
    time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    compile_key = "rl_compile_%s_%s_%s" % (kernel_name, os.getpid(), time_stamp)
    compile_dict = {}
    compile_dict[compile_key] = {
        "json_str": op_json_str,
        "pre_and_post": func_in_compile
        }

    compile_task_queue = option.get("global_mgr").get("compile_task_d")
    compile_res_dict = option.get("global_mgr").get("compile_result_d")
    compile_release_q = option.get("global_mgr").get("release_compile_q")
    compile_task_queue.put(compile_dict)

    compile_begin = time.time()
    # 编译超时30s
    while time.time() - compile_begin < COMPILE_LIMIT:
        if compile_key in compile_res_dict:
            compile_res = compile_res_dict.get(compile_key)
            compile_release_q.put(compile_key)
            break
    return compile_res


def base_build(kernel_name: str,
               op_json_str: str,
               option: dict) -> (bool, namedtuple):
    """
    base_build 编译
    :param kernel_name: 算子的kernel name
    :param op_json_str: 单个算子描述信息
    :param option: 算子对应的调优配置信息
    :return: compile_ret, sch_list, cce_special_list
    """
    try:
        compile_res = base_build_compile(kernel_name, op_json_str, option)
    except Exception as exception:  # pylint: disable=broad-except
        log.warn("RL exception occur: op %s can not base build.", repr(exception))
        return False, PostRetList([], [], [])
    finally:
        pass

    if not compile_res.get("post_res", None):
        log.warn("RL warn: %s can not base build, post_res is empty.", kernel_name)
        return False, PostRetList([], [], [])

    sch_list, cce_special_list, tik_tensor_list = compile_res.get("post_res")
    if not sch_list or not cce_special_list or len(sch_list) != len(cce_special_list):
        log.warn("RL warn: %s can not base build, sch_list: %s, cce_special_list: %s.",
                 kernel_name, sch_list, cce_special_list)
        return False, PostRetList(sch_list, cce_special_list, tik_tensor_list)

    return True, PostRetList(sch_list, cce_special_list, tik_tensor_list)


def get_res_by_json(op_json_str: str, op_desc: dict, option: dict) -> (list, dict):
    """
    compile op by op_json to get output tensors
    :param op_json_str: 单个算子描述信息
    :param option: 算子对应的调优配置信息
    :return: res_list, op_config_option
    """
    op_name_list = []
    op_type_list = []
    for op_info in op_desc.get("op_list", []):
        if op_info.get("type") != "Data" and "func_name" in op_info:
            op_name_list.append(op_info.get("func_name"))
            op_type_list.append(op_info.get("type").lower())

    kernel_name = str(op_desc.get("fusion_op_name"))
    op_config_option = {"op_name": "__".join(op_name_list),
                        "profiling_name": option.get("profiling_name", ""),
                        "kernel_name": kernel_name,
                        "op_type": "__".join(op_type_list)}

    # base build
    compile_ret, post_ret_list = base_build(kernel_name, op_json_str, option)
    if not compile_ret:
        log.warn("RL tune info: op %s can not base build, skip it.", kernel_name)
        return None, op_config_option
    if not post_ret_list.sch_list:
        log.warn("RL tune info: op %s can not get auto_schedule, skip it.", kernel_name)
        return None, op_config_option

    is_conv2d_l1fusion = option.get("is_conv2d_l1fusion", False)
    op_config_option["base_kernel"] = os.path.join(soc_cfg.kernel_meta_dir(), kernel_name + ".o")
    res_list = []
    for idx, sch in enumerate(post_ret_list.sch_list):
        try:
            op_config_option["tensor_list"] = \
                get_tensor_list(post_ret_list.cce_special_list[idx].get("tensor_list", []))
        except AttributeError as exception:
            log.warn("RL tune info: can not get tensor list. exception msg: %s, kernel_name: %s.", repr(exception),
                     kernel_name)
            return None, op_config_option
        finally:
            pass
        res = tune_util.get_res_by_output_name(sch, post_ret_list.cce_special_list[idx]["op_outputs"])
        res_list.append(res)
        if not is_conv2d_l1fusion and not rl_check_sch(sch, kernel_name):
            log.warn("RL tune info: op %s can not check auto_schedule, skip it.", kernel_name)
            return None, op_config_option

    if post_ret_list.tik_tensor_list:
        op_config_option["tik_tensor"] = post_ret_list.tik_tensor_list
    else:
        res_list = res_list[0]
    log.info("RL tune info: get output tensors end, res_list: %s, op_config_option: %s.", res_list, op_config_option)
    return res_list, op_config_option


def check_dyn_impl_and_dyn_shape(cbkey: str, op_desc: dict) -> bool:
    """
    check whether op is implemented by dynamic-static template or dynamic shape
    :param cbkey
    :param op_desc
    """
    is_dynamic_impl_flag = False
    is_dyn_flag = False
    for op_info in op_desc.get("op_list", []):
        if op_info.get("is_dynamic_impl", False):
            is_dynamic_impl_flag = True
        if op_info.get("dyn_flag", False):
            is_dyn_flag = True
    if is_dynamic_impl_flag or is_dyn_flag:
        log.warn("RL tune info: op %s is_dynamic_impl %s, dyn_flag: %s, skip!", cbkey,
                is_dynamic_impl_flag, is_dyn_flag)
        return True
    return False


def get_op_res_and_config_option(op_json_str_list: list,
                                 option_list: list,
                                 tune_option: dict,
                                 communicate_option: dict) -> (list, list):
    """
    get op res_list and config_option_list for rl tune
    :param op_json_str_list: 算子描述信息列表
    :param option_list: 调优配置信息列表
    :param tune_option: 调优推理选项
    :param communicate_option: 通信选项
    :return: res_list, config_option_list
    """
    res_list = []
    op_config_option_list = []
    tune_progress_q = communicate_option.get("tune_progress_q")
    for op_idx, op_json_str in enumerate(op_json_str_list):
        option = option_list[op_idx]
        log.dbg("RL tune info: op index: %d, option: %s, op_json_str: %s.", op_idx, option, op_json_str)
        cbkey = option.get("cb_struct_key")
        op_desc = load_json_str(op_json_str)
        if not op_desc:
            log.warn("RL tune info: Cannot load op_json_str: %s, skip!", op_json_str)
            report_rl_tune_progress(tune_progress_q, cbkey, MEANINGLESS_TIME, "skip")
            continue
        if check_dyn_impl_and_dyn_shape(cbkey, op_desc):
            report_rl_tune_progress(tune_progress_q, cbkey, MEANINGLESS_TIME, "skip")
            continue
        option["is_conv2d_l1fusion"] = rl_op_filter.is_conv2d_l1fusion(op_desc)

        # compile op by op_json to get output tensors
        res, op_config_option = get_res_by_json(op_json_str, op_desc, option)
        kernel_name = op_config_option.get("kernel_name")
        if not res:
            log.warn("RL tune info: rl tune not support %s, skip!", kernel_name)
            report_rl_tune_progress(tune_progress_q, cbkey, MEANINGLESS_TIME, "skip")
            continue

        is_tune, hit_bank, bank_key = tune_util.check_repeat_tune(res, kernel_name)
        if not is_tune:
            log.info("RL tune info: op %s has existed in rl bank, and REPEAT_TUNE is false, skip!", kernel_name)
            report_rl_tune_progress(tune_progress_q, cbkey, MEANINGLESS_TIME, "skip")
            continue

        log.dbg("RL tune info: op %s compile auto_schedule and get output tensors succ.", kernel_name)
        res_list.append(res)

        ori_op_name = tune_util.get_ori_op_name("", kernel_name, op_json_str)
        if ori_op_name:
            op_config_option["ori_op_name"] = ori_op_name
            op_config_option["tune_result_key"] = ori_op_name
            util.write_to_file(os.path.join(tune_option.get("tune_show_dir"), "%s.flag" % kernel_name), ori_op_name)

        op_config_option["core_type"] = op_desc["SocInfo"]["coreType"]
        op_config_option["core_num"] = op_desc["SocInfo"]["coreNum"]
        op_config_option["l1_fusion"] = op_desc["SocInfo"]["l1Fusion"]
        op_config_option["bank_key"] = bank_key
        op_config_option["cb_struct_key"] = cbkey
        op_config_option["is_conv2d_l1fusion"] = option["is_conv2d_l1fusion"]
        op_config_option["hit_bank"] = hit_bank
        op_config_option["op_desc"] = op_desc
        op_config_option_list.append(op_config_option)
    log.info("RL tune info: %d ops compile auto_schedule and get output tensors succ.", len(res_list))
    return res_list, op_config_option_list


def _init_tune_option(ori_option: dict) -> dict:
    """
    _init_tune_option
    :param ori_option: original option
    :return: tune option
    """
    # 创建tune路径
    tmp_file_path = os.path.realpath(ori_option.get("tmp_file_path"))
    suboptimal_path = os.path.realpath(ori_option.get("suboptimal_path")) if ori_option.get("suboptimal_path") else ''
    default_task_time_stamp = "%s_%s" % (datetime.datetime.now().strftime('%Y%m%d_%H%M%S_%f'), os.getpid())
    task_time_stamp = ori_option.get("pid_timestamp", default_task_time_stamp)
    curr_tune_workspace = os.path.join(tmp_file_path, "tune_workspace_%s" % task_time_stamp)
    tune_show_dir = os.path.join(tmp_file_path, "tune_show_%s" % task_time_stamp)
    util.create_dir(curr_tune_workspace)
    util.create_dir(tune_show_dir)
    high_perf = ori_option.get("high_perf", False)

    tune_option = {
        "WORKSPACE": tmp_file_path,
        "auto_schedule_golden": True,
        "init_evb": False,
        "rl_mode": "rl_tune_om",
        "main_pid": os.getpid(),
        "run_by_om": True,
        "is_store_cheque": True,
        "not_store_sch": True,
        "tune_workspace": curr_tune_workspace,
        "suboptimal_path": suboptimal_path,
        "tune_show_dir": tune_show_dir,
        "timeout": tune_util.get_timeout(tune_by_om=True, high_perf=high_perf),
        "soc_version": soc_cfg.get_full_soc_version(),
        "job_id": ori_option.get("job_id", "")
    }
    log.info("RL tune info: init tune_option: %s", tune_option)
    return tune_option


def rl_tune_om(op_json_str_list: list, option_list: list) -> bool:
    """
    rl_tune_om
    :param op_json_str_list: 算子描述信息列表
    :param option_list: 调优配置信息列表
    :return: rl调优结果
    """
    if not op_json_str_list:
        log.warn("RL tune info: no op to tune!")
        return False

    main_pid = os.getpid()
    log.event("RL tune info: enter rl_tune_om, main_pid: %d, op count: %d.", main_pid, len(op_json_str_list))

    # 系统环境变量设置
    first_option = option_list[0]
    cbkey = first_option.get("cb_struct_key")
    tune_option = _init_tune_option(first_option)
    communicate_option = {
        "tune_progress_q": first_option.get("global_mgr").get("tune_progress_q"),
        "timer_q": first_option.get("global_mgr").get("timer_q"),
        "cb_key_task_clean_q": first_option.get("global_mgr").get("cb_key_task_clean_q"),
        "cb_key_clean_res_d": first_option.get("global_mgr").get("cb_key_clean_res_d"),
        "tune_report_info_q": first_option.get("global_mgr").get("tune_report_info_q"),
        "datacmp_task_q": first_option.get("global_mgr").get("datacmp_task_q"),
        "datacmp_res_d": first_option.get("global_mgr").get("datacmp_res_d"),
        "tune_task_d": first_option.get("global_mgr").get("tune_task_d"),
        "tune_result_d": first_option.get("global_mgr").get("tune_result_d"),
        "release_tune_q": first_option.get("global_mgr").get("release_tune_q")
    }

    om_runner.CB_TASK_QUEUE = first_option.get("global_mgr").get("tune_task_d")
    om_runner.CB_RES_DICT = first_option.get("global_mgr").get("tune_result_d")
    om_runner.CB_RELEASE_QUEUE = first_option.get("global_mgr").get("release_tune_q")
    timer_queue = communicate_option.get("timer_q")

    timer_queue.put(("start", main_pid, "rl tune om main process"))
    timer_queue.put(("start", os.getpid(), "get_op_res_and_config_option"))
    res_list, op_config_option_list = get_op_res_and_config_option(op_json_str_list, option_list,
                                                                   tune_option, communicate_option)
    timer_queue.put(("stop", os.getpid(), "get_op_res_and_config_option"))
    if not res_list:
        log.event("RL tune info: res list is empty, no op to tune.")
        report_rl_tune_progress(communicate_option.get("tune_progress_q"), cbkey, MEANINGLESS_TIME, "all_finish")
        return False

    # rl tune
    ret = False
    global_multiprocess_mgr = MultiprocessManager()
    try:
        ret = tune(res_list, tune_option, communicate_option, op_config_option_list, global_multiprocess_mgr)
    except Exception as exception:  # pylint: disable=broad-except
        log.err("RL exception occur: rl tune failed. error msg: %s, traceback info: %s.", repr(exception),
                traceback.format_exc())
    finally:
        global_multiprocess_mgr.shutdown()
        log.event("RL tune info: rl_tune_om main process end, ret: %s, main_pid: %d.", str(ret), main_pid)

    report_rl_tune_progress(communicate_option.get("tune_progress_q"), cbkey, MEANINGLESS_TIME, "all_finish")
    timer_queue.put(("stop", main_pid, "rl tune om main process"))
    return ret
