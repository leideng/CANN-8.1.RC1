#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

RL-TUNE
"""
import datetime
import json
import multiprocessing
import os
import pickle
import queue
import signal
import sys
import time

from schedule_search import config
from schedule_search import log
from schedule_search import tune_util
from schedule_search import util
from schedule_search.atc_rl_manager import atc_rl_adapter
from schedule_search.log import LOGGING_INSTANCE
from schedule_search.ts_env import env_util
from schedule_search.ts_env.estimator.evb import evb_util
from schedule_search.ts_env.estimator.evb import evb_host
from schedule_search.util import OPEN_FILE_MODES_644
from schedule_search.util import WRITE_FILE_FLAGS

from tbe import tvm
from tbe.common.rl_bank.rl_bank import get_bank_name
from tbe.common.rl_bank.rl_bank import query_rl_bank
from tbe.common.rl_bank.rl_op_filter import rl_op_filter
from tbe.common.rl_bank import bank_manager
from tbe.common.buildcfg import get_current_build_config
from te_fusion.parallel_compilation import FusionOpTask
from te_fusion.parallel_compilation import gen_task_res
from te_fusion.parallel_compilation import check_task_succ
from te_fusion.parallel_compilation import SingleOpTask
from te_fusion.parallel_compilation import update_running_task
from te_fusion.compile_task_manager import compile_op_sync


def excepthook_silent(etype, value, tback):  # pylint: disable=unused-argument
    """
    excepthook to print nothing
    """


def is_tune_dump():
    """
    判断是否需要dump数据
    :return:True
    """
    if os.getenv("ENABLE_TUNE_DUMP", "").lower() == "true":
        return True
    return False


def get_tensor_list(ori_tensor_list):
    """
    生成tensor list
    :param ori_tensor_list:
    :return:list
    """
    tensor_list = []
    for ori_tensor in ori_tensor_list:
        shape_list = env_util.tvm_shape_trans(ori_tensor.shape)
        tensor_list.append({
            "shape": shape_list,
            "name": ori_tensor.name,
            "dtype": ori_tensor.dtype
        })
    return tensor_list


def signal_handler(signum, frame):  # pylint: disable=unused-argument
    """
    rl tune process just quit and clean proc when Ctrl-C pressed
    """
    log.info('RL tune info: Ctrl-C pressed, rl tune process quiting...')
    try:
        tune_util.tune_workspace_clean(TUNE_MANAGER.tune_workspace,
                                       TUNE_MANAGER.tune_show_dir,
                                       TUNE_MANAGER.option)
    except Exception as exception:  # pylint: disable=broad-except
        log.dbg("exception: %s", str(exception))
    os._exit(1)  # pylint: disable=protected-access


class TuneManager:
    """
    管理rl tune loop进程
    """
    def __init__(self):
        self.task_queue = None
        self.tune_proc = None
        self.soc_info = None
        self.tune_show_dir = None
        self._tune_workspace = None
        self.option = {}

    @property
    def tune_workspace(self):
        """

        :param tune_workspace:
        """
        return self._tune_workspace

    def start(self, res_queue, live_checker, termin_event):
        """
        开始loop进程，等候任务
        :param res_queue:放置任务结果的队列
        :param live_checker:主进程是否活着
        :param termin_event:终止信号
        """
        self.task_queue = multiprocessing.Queue()
        # Do not flush queue data when closing, or current thread may being blocked at writing queue pipe
        self.task_queue.cancel_join_thread()
        tune_task_proc = TuneTaskProc(self.task_queue, res_queue, live_checker[0], termin_event)
        self.tune_proc = multiprocessing.Process(target=tune_task_proc.loop)
        self.tune_proc.start()
        log.info("RL tune info: tune_proc %s start.", self.tune_proc.pid)

    def destory(self):
        """
        #销毁rl tune进程
        """
        if is_tune_dump():
            return

        tune_util.tune_workspace_clean(self._tune_workspace, self.tune_show_dir, self.option)

        if not self.tune_proc.is_alive():
            return

        self.tune_proc.terminate()
        time.sleep(0.1)
        if self.tune_proc.is_alive():
            os.kill(self.tune_proc.pid, signal.SIGKILL)

    def set_soc_info(self, soc_info):
        """

        :param soc_info:
        """
        self.soc_info = soc_info

    def set_tune_show_dir(self, tune_show_dir):
        """

        :param tune_show_dir:
        """
        self.tune_show_dir = tune_show_dir

    @tune_workspace.setter
    def tune_workspace(self, tune_workspace):
        """

        :param tune_workspace:
        """
        self._tune_workspace = tune_workspace


TUNE_MANAGER = TuneManager()


def rl_tune_init(soc_info,  # pylint: disable=R0913
                 res_queue, live_checker, termin_event, log_level, pid_ts):
    """
    初始化rl tune 进程
    :param soc_info:
    :param res_queue:放置任务结果的队列
    :param live_checker:主进程是否活着
    :param termin_event:终止信号
    :param log_level:日志级别
    :param pid_ts:pid及时间
    """
    log.dbg("RL tune info: the logging log level assigned by TE is %s.", log_level)
    TUNE_MANAGER.set_soc_info(soc_info)
    tune_util.vector_custom_repo_migrate()
    if not tune_util.check_tune_bank_path_valid():
        return False

    if is_tune_dump():
        LOGGING_INSTANCE.info("RL tune info: tune_dump is enabled, rl_tune_init succ.")
        log.info("RL tune info: tune_dump is enabled, rl_tune_init succ.")
        return True

    try:
        tune_show_dir = os.path.join(os.getcwd(), "tune_show_%s" % pid_ts)
        util.create_dir(tune_show_dir)
        # 在tune_show_dir中添加in_use的FLag
        util.write_to_file(os.path.join(tune_show_dir, 'RL_in_use.flag'),
                           "pid=%s, ts=%s" % (os.getpid(), datetime.datetime.now()))
        TUNE_MANAGER.set_tune_show_dir(tune_show_dir)
        tune_workspace = os.path.join(
            config.WORKSPACE,
            "tune_workspace_%s" % pid_ts)
        util.create_dir(tune_workspace)
        TUNE_MANAGER.tune_workspace = tune_workspace

        tss_workspace = os.path.basename(tune_workspace)
        TUNE_MANAGER.option = util.init_option({})
        for evb_info in TUNE_MANAGER.option['evbs']:
            evb_info["tss_workspace"] = tss_workspace
            evb_host.update_evb_info(evb_info)

        ret, output = evb_util.env_check(TUNE_MANAGER.option)
        if not ret:
            log.err("RL tune info: rl tune init failed, because env check failed: %s.", output)
            return False
        TUNE_MANAGER.start(res_queue, live_checker, termin_event)
    except (OSError, ValueError, TypeError) as exception:
        log.err("RL exception occur: rl tune init failed, exception: %s.", repr(exception))
        return False
    finally:
        pass

    LOGGING_INSTANCE.info("RL tune info: rl_tune_init succ.")
    log.info("RL tune info: rl_tune_init succ.")
    return True


def rl_tune_deinit():
    """
    资源回收
    """
    TUNE_MANAGER.destory()
    LOGGING_INSTANCE.info("RL tune info: rl_tune_deinit succ.")
    log.info("RL tune info: rl_tune_deinit succ.")


class TuneOpTask:  # pylint: disable=R0902,R0913
    """
    算子进行tune的任务对象
    """
    def __init__(self,
                 sch_list,
                 base_kernel,
                 kernel_name,
                 op_name,
                 op_args,
                 op_task,
                 extra_info_dict=None):
        self.op_task = op_task
        self.sch_list = sch_list
        self.base_kernel = base_kernel
        self.kernel_name = kernel_name
        self.op_name = op_name
        self.op_args = op_args
        self.extra_info_dict = extra_info_dict
        self.tensor_list = []
        self.op_model_name = None
        self.ori_op_name = None
        self.op_outputs_list = []
        self.init()

    def __str__(self):
        return "TuneOpTask {} {}".format(self.kernel_name, self.op_task)

    def init(self):
        """
        get_op_outputs
        :return:
        """
        if isinstance(self.extra_info_dict, dict) and \
                self.ignore_reason() in ['', 'hit_bank']:
            for sch in self.sch_list:
                self.op_outputs_list.append(
                    sch.cce_special.get("op_outputs", []))
            # 多schedule的场景也不需要tensor_list，直接取[0]即可
            if self.sch_list:
                self.tensor_list = self.sch_list[0].cce_special.get(
                    "tensor_list", [])

            self.op_model_name = self.extra_info_dict.get("op_model_name", "")
            op_json = self.extra_info_dict.get('op_json', "")
            self.ori_op_name = tune_util.get_ori_op_name(self.op_model_name,
                                                         self.kernel_name,
                                                         op_json)
            # ignore的不会进行调优，所以无须生成flag
            if self.ori_op_name and not self.ignore_reason():
                util.write_to_file(os.path.join(TUNE_MANAGER.tune_show_dir,
                                                "%s.flag" % self.kernel_name),
                                   self.ori_op_name)

    def ignore_reason(self):
        """
        ignore的是指那些不会进行调优的算子，比如不支持或已经命中Bank
        :return:
        """
        return self.extra_info_dict.get("ignore_reason", '')


class TuneTaskProc:
    """
    实际进行tune的进程
    """
    def __init__(self, task_queue, res_queue, live_checker, termin_event):
        self.res_queue = res_queue
        self.live_checker = live_checker
        self.termin_event = termin_event
        self.task_queue = task_queue
        self.global_res_dict = {}
        # 有的网络可能有多个子图，或者多个Batch，从而会起多次，要分开
        self.run_index = 0

    def get_sch_res_op_lists(self, task_list: list) -> tuple:
        """
        get sch_dump_list, res_names_list, op_config_list for tune_task
        """
        sch_dump_list = []
        res_names_list = []
        op_config_list = []
        # 每个task都是TuneOpTask对象
        for task in task_list:
            if task.ignore_reason() not in ['', 'hit_bank']:
                continue
            kernel_name = task.kernel_name
            if kernel_name not in self.global_res_dict:
                self.global_res_dict[kernel_name] = {"op_task": task.op_task}
            sch_list = []
            for sch_dump in task.sch_list:
                sch_list.append(pickle.dumps(sch_dump, protocol=2))
            sch_dump_list.append(sch_list)
            res_names_list.append(task.op_outputs_list)
            op_config = {
                "op_name": task.op_name,
                "base_kernel": task.base_kernel,
                "kernel_name": task.kernel_name,
                "inputs": task.op_args,
                "tensor_list": get_tensor_list(task.tensor_list),
                "tune_show_dir": TUNE_MANAGER.tune_show_dir,
                "op_model_name": task.op_model_name,
                "opp_path": task.extra_info_dict.get("opp_path", ""),
                "op_json": task.extra_info_dict.get("op_json", ""),
                "ori_op_name": task.ori_op_name,
                "tune_result_key": task.ori_op_name,
                "ignore_reason": task.extra_info_dict.get("ignore_reason", ""),
                "hit_bank": task.extra_info_dict.get("hit_bank", False),
                "soc_version": task.extra_info_dict.get("soc_info", ["Ascend310"])[0]
            }
            if 'tik_tensor' in task.extra_info_dict:
                op_config['tik_tensor'] = task.extra_info_dict['tik_tensor']
            if not os.path.exists(task.base_kernel):
                op_config['no_base_tune'] = True
            no_base_tune = op_config.get('no_base_tune', False)
            op_config["auto_schedule_golden"] = not no_base_tune
            op_config["base_ignore"] = no_base_tune
            op_config_list.append(op_config)
        return sch_dump_list, res_names_list, op_config_list

    def run(self, task_list, tune_type, run_index):  # pylint: disable=R0914
        """
        执行tune任务
        :param task_list:任务列表
        :param tune_type:tune的类别，infer、search
        """
        log.info("RL tune info: TuneTaskProc run begin, tune_type: %s, task_list: %s.", tune_type, task_list)
        sch_dump_list, res_names_list, op_config_list = self.get_sch_res_op_lists(task_list)
        # 没任务直接返回
        if not sch_dump_list:
            return
        tune_workspace = '%s_%03d' % (TUNE_MANAGER.tune_workspace, run_index)
        util.create_dir(tune_workspace)
        tune_res_dict = atc_rl_adapter.online_tune_proc(tune_type, sch_dump_list, res_names_list, op_config_list,
            tune_workspace=tune_workspace)
        log.info("RL tune info: TuneTaskProc run end, tune_type: %s, task_list: %s, tune_res_dict: %s.",
                 tune_type, task_list, tune_res_dict)
        if tune_res_dict:
            # 从结果文件中读取最优py
            for kernel_name in tune_res_dict:
                self.global_res_dict[kernel_name][tune_type] = tune_res_dict.get(kernel_name)
        else:
            # 结果文件不存在则认为tune失败，最优结果路径置为None
            for kernel_name in [op_cfg.get("kernel_name") for op_cfg in op_config_list]:
                self.global_res_dict[kernel_name][tune_type] = None

    def put_result(self):  # pylint: disable=protected-access
        """
        将算子tune结果放到全局结果队列中
        """
        log.info("RL tune info: start to process tune result, global_res_dict: %s.", self.global_res_dict)
        for kernel_name in self.global_res_dict:
            # 如果任务已经处理过了，不再重复处理了
            if self.global_res_dict[kernel_name].get("task_finished", False):
                log.dbg("RL tune info: %s task has been tuned and send result.", kernel_name)
                continue
            if "infer" in self.global_res_dict[kernel_name] and "search" in \
                    self.global_res_dict[kernel_name]:
                # 根据infer和search获取最终tune的结果
                best_tick, best_sch_path, base_tick = \
                    tune_util.get_best_from_infer_search(
                        self.global_res_dict[kernel_name]["infer"],
                        self.global_res_dict[kernel_name]["search"])
                task_obj = self.global_res_dict[kernel_name]["op_task"]
                # 尝试放到bank里
                bank_name = get_bank_name()
                time_stamp = datetime.datetime.now().strftime(
                    '%Y%m%d_%H%M%S_%f')
                tmp_kernel_name = "{}@{}".format(kernel_name, time_stamp)
                bank_info = tune_util.BankInfo('{}_{}'.format(bank_name, time_stamp), 'custom', '')
                ret = tune_util.add_best_to_bank((best_tick, base_tick),
                                                 best_sch_path,
                                                 tmp_kernel_name,
                                                 bank_info)
                if ret:
                    # 再调用一次算子编译
                    task_obj.run()

                # 生成tune task结果对象，放到结果队列里
                # base已经存在，如果有更优的会覆盖生成新的同名.o和.json
                # 所以这里无脑成功就对了
                res_info_msg = "kernel[{}] compile success. ".format(
                    task_obj._kernel_name)  # pylint: disable=W0212
                tune_res = gen_task_res(
                    task_obj.build_type,
                    task_obj.graph_id,
                    task_obj.task_id,
                    0,
                    task_obj._kernel_name,  # pylint: disable=W0212
                    res_info_msg)
                log.info("RL tune info: process tune res succ, kernel_name: %s, tune_res: %s.", kernel_name, tune_res)
                self.res_queue.put(tune_res)
                # 设置任务结束标识符
                self.global_res_dict[kernel_name]["task_finished"] = True

    def update_infer_search_task(self, tune_task: object, infer_task_list: list, search_task_list: list) -> tuple:
        """
        更新infer_task_list 和search_task_list
        """
        if tune_task:
            infer_task_list.append(tune_task)
            search_task_list.append(tune_task)
        else:
            if infer_task_list:
                self.run(infer_task_list, "infer", self.run_index)
                # 执行完任务后置为空
                infer_task_list = []
            elif search_task_list:
                self.run(search_task_list, "search", self.run_index)
                # 执行完任务后置为空
                search_task_list = []
        return infer_task_list, search_task_list

    def loop(self):
        """
        获取算子编译任务和执行rl tune的循环
        """
        infer_task_list = []
        search_task_list = []
        signal.signal(signal.SIGINT, signal_handler)

        while not self.termin_event.is_set():
            try:
                # check dispatcher process is alive
                if self.live_checker.poll():
                    self.live_checker.recv()

                tune_task = self._get_tune_task()
                infer_task_list, search_task_list = self.update_infer_search_task(
                    tune_task, infer_task_list, search_task_list)
                # 将执行结束的任务结果放入res_queue
                self.put_result()
            except EOFError:
                log.warn("RL exception occur: Master process dead, worker process quiting.")
                # Avoid 'Broken PIPE' exception msg of multiprocessing module,
                # we are quiting anyway.
                sys.excepthook = excepthook_silent
                break

        tune_util.tune_workspace_clean(TUNE_MANAGER.tune_workspace, TUNE_MANAGER.tune_show_dir, TUNE_MANAGER.option)

    def _get_tune_task(self) -> object:
        log.info("RL tune info: get op tune task begin.")
        try:
            tune_task = self.task_queue.get(True, 60)
        except queue.Empty:
            tune_task = None
        finally:
            pass
        log.info("RL tune info: get op tune task end, tune_task: %s.", tune_task)
        return tune_task


def get_tik_tensor(kernel_name: str, extra_info_dict: dict) -> None:
    """

    :param kernel_name:
    :param extra_info_dict:
    :return:
    """
    # get input&output tensor
    tik_tensor_info = bank_manager.get_tik_tensor(kernel_name)
    tik_tensor_list = tune_util.parse_tik_tensor_info(tik_tensor_info)
    if tik_tensor_list:
        extra_info_dict["tik_tensor"] = tik_tensor_list


def compile_op_by_mp(json_str: str) -> None:
    """
    compile op by dispatching a task to compilation worker process
    """
    op_desc = json.loads(json_str)
    kernel_name = op_desc["fusion_op_name"]
    run_cfg = {
        "prerun": (bank_manager.set_current_op_name, [kernel_name]),
        "postrun": (bank_manager.collect_op_res, [kernel_name])
    }

    if rl_op_filter.is_conv2d_l1fusion(op_desc):
        log.info("RL tune info: base_build_compile: op is conv2d L1fusion.")
        op_desc["is_rl_conv2d_l1fusion"] = True
        json_str = json.dumps(op_desc)

    res = compile_op_sync(json_str, run_cfg)
    if not check_task_succ(res):
        raise RuntimeError("compile failed. result: {}".format(res))
    post_res = res["post_res"]
    bank_manager.restore_op_res(post_res)


def get_res_list(sch_list: list, extra_info_dict: dict, op_type: str, op_name: str) -> list:
    """
    get res_list for dispatch_tune_task
    """
    res_list = []
    is_conv2d_l1fusion = extra_info_dict["kernel_name"].lower().count("conv2d") > 1
    for sch in sch_list:
        if not is_conv2d_l1fusion and not tune_util.tune_comm_check(sch, op_type):
            extra_info_dict['ignore_reason'] = 'comm_check fail'
            break

        op_outputs = sch.cce_special.get("op_outputs", [])
        if not op_outputs:
            log.err("RL tune info: schedule of %s has no op_outputs!", op_name)
            extra_info_dict['ignore_reason'] = 'no op_outputs'
            break

        res = tune_util.get_res_by_output_name(sch, op_outputs)
        res_list.append(res)
    return res_list


def dispatch_tune_task(  # pylint: disable=R0913,R0914
        base_kernel,
        kernel_name,
        op_name,
        l1size,
        op_args,
        task,
        param_dict,
        op_model_name):
    """
    下发算子tune任务
    :param op_args:
    :param base_kernel:任务base.o
    :param kernel_name:任务kernel_name
    :param op_name:任务算子名
    :param l1size:任务l1size
    :param task:任务的OpTask对象，用来设置上下文和生成算子编译结果字典
    :param param_dict:任务对应的算子参数字典
    :return:bool
    """
    log.info("RL tune info: dispatch_tune_task begin, kernel_name: %s, op_name: %s.", kernel_name, op_name)
    task.set_l1size(l1size)
    base_json = base_kernel.replace(".o", ".json")
    op_type = param_dict["op_type"]
    extra_info_dict = {
        "op_model_name": op_model_name,
        "opp_path": param_dict.get("opp_path", ""),
        "op_json": param_dict.get("op_json", ""),
        "kernel_name": kernel_name,
    }

    sch_list = bank_manager.get_op_res(kernel_name)
    if not sch_list:
        log.info("RL tune info: op %s can not get default schedule, skip.", op_type)
        return False

    get_tik_tensor(kernel_name, extra_info_dict)

    res_list = get_res_list(sch_list, extra_info_dict, op_type, op_name)

    if res_list:
        if "tik_tensor" not in extra_info_dict:
            res_list = res_list[0]

        # 如果命中了bank，却没有打开REPEAT_TUNE，则不重复tune了
        repeat_tune, hit_bank, _ = tune_util.check_repeat_tune(res_list, op_name)
        if not repeat_tune:
            extra_info_dict['ignore_reason'] = 'hit_bank'
        # 判断是否命中Bank
        extra_info_dict['hit_bank'] = hit_bank

    # Dump模式，就会将支持算子的sch、base_kernel dump到本地文件中
    if is_tune_dump():
        log.info("RL tune info: tune dump begin, kernel_name: %s, op_name: %s.", kernel_name, op_name)
        if extra_info_dict.get('ignore_reason', '') in ['', 'hit_bank']:
            tune_dump_proc(sch_list, base_kernel, base_json, op_name, kernel_name, TUNE_MANAGER.soc_info, op_args,
                           param_dict, extra_info_dict)
        return False

    # Tune模式下，不支持的算子要直接返回False了
    if extra_info_dict.get('ignore_reason'):
        log.info("RL tune info: op %s is not support, skip.", op_name)
        return False

    extra_info_dict["soc_info"] = TUNE_MANAGER.soc_info
    # put task in task_queue
    tune_task = TuneOpTask(sch_list, base_kernel, kernel_name, op_name, op_args, task, extra_info_dict)
    log.info("RL tune info: put tune task %s in task_queue", tune_task)
    TUNE_MANAGER.task_queue.put(tune_task, True)
    update_running_task(task)
    return True


def dispatch_single_tune_task(  # pylint: disable=R0913
        graph_id,
        task_id,
        l1size,
        base_kernel,
        kernel_name,
        op_model_name,
        op_module,
        op_func,
        op_type,
        op_args,
        build_option=None):
    """
    下发单算子tune任务
    :param op_type: 任务op_type
    :param graph_id:任务graph_id
    :param task_id:任务task_id
    :param l1size:任务l1size
    :param base_kernel:任务base.o
    :param kernel_name:任务kerne_name
    :param op_module:任务op_module
    :param op_func:任务算子compute的函数
    :param op_args:任务算子的入参
    :return:bool
    """
    if build_option is not None:
        if build_option.get("is_dynamic_impl", False):
            log.warn("RL tune info: %s is implemented by dynamic-static template, skip.", kernel_name)
            return False
        if build_option.get("dyn_flag", False):
            log.warn("RL tune info: %s is dynamic_shape op, skip.", kernel_name)
            return False
    op_module, op_py_path = op_module.split("@")
    opp_path = op_py_path.replace(op_module.replace(".", "/"), "")
    # gen rl tune task
    task = SingleOpTask(graph_id, task_id, op_module, op_type, op_func,
                        kernel_name, *op_args)

    param_dict = {
        "l1size": l1size,
        "op_module": op_module,
        "op_func": op_func,
        "op_args": op_args,
        "op_type": op_type,
        "opp_path": opp_path,
    }

    return dispatch_tune_task(base_kernel, kernel_name, op_func, l1size,
                              op_args, task, param_dict, op_model_name)


def dispatch_fusion_tune_task(  # pylint: disable=R0913,R0914
        graph_id,
        task_id,
        l1size,
        base_kernel,
        kernel_name,
        op_model_name,
        op_json):
    """
    下发融合算子tune任务
    :param graph_id:任务graph_id
    :param task_id:任务task_id
    :param l1size:任务l1size
    :param base_kernel:任务base.o
    :param kernel_name:任务kerne_name
    :param op_json:任务融合算子的信息
    :return:bool
    """
    # gen rl_tune_task
    task = FusionOpTask(graph_id, task_id, op_json, kernel_name)
    op_name_set = set()
    op_type_set = set()
    fusion_dict = json.loads(op_json)
    op_args = []

    is_dynamic_impl_flag = False
    is_dyn_flag = False
    for op_info in fusion_dict.get("op_list", []):
        if op_info["type"] == "Data":
            desc = op_info["output_desc"][0]
            op_arg = {"shape": desc["shape"], "dtype": desc["data_type"]}
            op_args.append(op_arg)
        elif "func_name" in op_info:
            op_name_set.add(op_info["func_name"])
            op_type_set.add(op_info["type"])
        if op_info.get("is_dynamic_impl", False):
            is_dynamic_impl_flag = True
        if op_info.get("dyn_flag", False):
            is_dyn_flag = True

    if is_dynamic_impl_flag or is_dyn_flag:
        log.warn("RL tune info: op %s is_dynamic_impl %s, dyn_flag: %s, skip!", kernel_name,
                is_dynamic_impl_flag, is_dyn_flag)
        return False

    op_name = "__".join(list(op_name_set))
    op_type = "__".join(list(op_type_set))

    param_dict = {"l1size": l1size, "op_json": op_json, "op_type": op_type}

    return dispatch_tune_task(base_kernel, kernel_name, op_name, l1size,
                              op_args, task, param_dict, op_model_name)


def tune_dump_proc(  # pylint: disable=R0913,R0914
        sch_list,
        base_kernel,
        base_json,
        op_name,
        kernel_name,
        soc_info,
        op_args,
        params_dict,
        extra_info_dict):
    """
    将rl tune的信息dump到文件里
    :param op_args:
    :param sch_list:default schedule对象
    :param base_kernel:base.o路径
    :param base_json:base.o对应的base.json
    :param op_name:算子名
    :param kernel_name:最重要生成的kernel_name
    :param soc_info:soc信息
    :param params_dict:算子入参
    :return:True
    """
    env_dump_dir = os.getenv("TUNE_DUMP_PATH", "")
    if not env_dump_dir:
        env_dump_dir = "./tune_dump"
    dump_dir = os.path.realpath(env_dump_dir)
    base_kernel_name = os.path.basename(base_kernel)
    base_json_name = os.path.basename(base_json)
    util.create_dir(dump_dir)
    sch_dump_file_name = base_kernel_name.split(".")[0]
    op_outputs_list = []
    sch_file_list = []
    for i, sch in enumerate(sch_list):
        sch_file = "%s.%s.sch" % (sch_dump_file_name, i)
        sch_file_list.append(sch_file)
        op_outputs_list.append(sch.cce_special["op_outputs"])
        try:
            with os.fdopen(os.open(
                os.path.join(dump_dir, sch_file), WRITE_FILE_FLAGS, OPEN_FILE_MODES_644), "wb") as file_handler:
                pickle.dump(sch, file_handler, protocol=2)
        except PermissionError as excp:
            log.warn("RL exception occur: tune dump proc exit: %s.", excp)
            return False

    tensor_list = get_tensor_list(sch_list[0].cce_special.get("tensor_list", []))
    tune_dump_dict = {
        "sch": sch_file_list,
        "op_name": op_name,
        "base_kernel": base_kernel_name,
        "base_json": base_json_name,
        "op_outputs": op_outputs_list,
        "tensor_list": tensor_list,
        "kernel_name": kernel_name,
        "soc_info": soc_info,
        "op_args": op_args,
    }
    tune_dump_dict.update(params_dict)
    tune_dump_dict.update(extra_info_dict)

    tune_info_file = os.path.join(dump_dir, base_kernel_name.replace(".o", ".tune_info.json"))

    with os.fdopen(os.open(tune_info_file, WRITE_FILE_FLAGS, OPEN_FILE_MODES_644), "w") as file_handler:
        json.dump(tune_dump_dict, file_handler, indent=4)
    stat_info = os.stat(tune_info_file)
    util.cp_src_to_dst(base_kernel, os.path.join(dump_dir, base_kernel_name),
                       mode=stat_info.st_mode)
    util.cp_src_to_dst(base_json, os.path.join(dump_dir, base_json_name),
                       mode=stat_info.st_mode)
    LOGGING_INSTANCE.info("RL tune info: enable ENABLE_TUNE_DUMP, dump to %s.", tune_info_file)
    log.info("RL tune info: enable ENABLE_TUNE_DUMP, dump to %s.", tune_info_file)
    return True


def rl_tune(res):  # pylint: disable=R0914
    """
    RL调优，供单算子文件调用
    :param res: 算子输出tensor
    :return: sch obj
    """
    err_res = None
    if get_current_build_config("enable_op_prebuild"):
        return err_res

    # for RL tune getting res
    bank_manager.set_op_res(res)

    ret, sch = query_rl_bank(res)
    if ret:
        log.info("rl bank hit, return sch directly!")
        return sch

    res_op = []
    op_outputs = []
    if isinstance(res, list):
        for single_res in res:
            res_op.append(single_res.op)
            op_outputs.append(single_res.op.name)
    else:
        res_op.append(res.op)
        op_outputs.append(res.op.name)
    sch = tvm.create_schedule(res_op)
    sch_dump_list = [pickle.dumps(sch, protocol=2)]
    log.info("rl bank not hit, need to tune!")
    time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    kernel_name = "rl_tune_{}_{}".format(os.getpid(), time_stamp)
    tune_res_dict = atc_rl_adapter.online_tune_proc(
        "infer", sch_dump_list, [op_outputs], [{
            "kernel_name": kernel_name,
            "auto_schedule_golden": False,
            "base_ignore": True,
            "no_base_tune": True,
        }])
    sch = None
    best_py = tune_res_dict.get(kernel_name, None)
    if not best_py:
        return sch

    best_tick, best_sch_path, base_tick = tune_util.parse_tune_res(best_py)

    # 尝试放到bank里
    bank_name = get_bank_name()
    bank_info = tune_util.BankInfo('{}_{}'.format(bank_name, time_stamp), 'custom', '')
    ret = tune_util.add_best_to_bank((best_tick, base_tick),
                                     best_sch_path,
                                     kernel_name,
                                     bank_info)
    if ret:
        _, sch = query_rl_bank(res)
    return sch
