#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import glob
import json
import os
import time
import shutil
from collections import namedtuple

from schedule_search import log
from schedule_search import util
from schedule_search.cce_intrin_map import OP_INTRIN_KEY_TAG
from schedule_search.soc_cfg import get_soc_version
from schedule_search.soc_cfg import get_full_soc_version
from schedule_search.soc_cfg import kernel_meta_dir
from schedule_search.ts_env.tensor_cfg import MAX_ORIGIN_STAGE_CNT
from schedule_search.util import OPEN_FILE_MODES_640
from tbe import tvm
from tbe.common.rl_bank.add_cheque import try_add_cheque
from tbe.common.rl_bank.rl_bank import get_bank_name
from tbe.common.rl_bank.rl_bank import check_bank_hit
from tbe.common.rl_bank.rl_bank import get_rl_bank_key
from tbe.common.rl_bank.rl_bank import get_default_rl_path
from tbe.common.rl_bank.rl_bank import get_previous_bank_paths
from tbe.common.rl_bank.rl_bank import satisfy_bank
from tbe.common.rl_bank.rl_bank import update_bank
from tbe.common.rl_bank.rl_bank import OLD_USR_ASCEND_PATH


TUNE_TIME_PER_TASK = 300
TUNE_TIME_PER_TASK_OM = 200
TUNE_TIME_HIGH_PERF = 1200

WORKSPACE_MAX_LIFE_TIME = 3600 * 24 * 10

# RL_TUNE明确不支持的算子黑名单OpType, 后面统一lower，这里不区分大小写
OP_BLACK_LIST = [
    "batchnorm", "transdata", "batchnormgrad", "batchnormext2", "maxpool",
    "conv2d", "depthwiseconv2d", "depthwiseconv2dbackpropfilterd",
    "depthwiseconv2dbackpropinputd", "conv2dbackpropfilterd",
    "bninferconv2dbackpropinputd", "ascenddequant", "ascendquant",
    "deconvolution", "bninfergrad", "bninference", "bninferenced",
    "fullyconnection", "yolo", "yolov3detectionoutputv2d", "upsample",
    "fsrdetectionoutput", "concatd", "shufflechannel",
    "yolov2detectionoutputd", "proposald", "roipooling", "dequant",
    "quant", "convolution", "msra", "pooling", "innerproduct",
    "dropout", "xavier", "crop", "gaussian", "lrn", "concat", "slice",
    "flatten", "reshape", "proposal", "psroipooling",
    "priorboxdv2", "transposed", "ssddecodebbox", "ssddetectionoutput",
    "antiquant", "gather", "gathernd", "gatherv2", "gatherv2d",
    "batchmatmul", "batchmatmulv2", "matmul", "matmulv2", "select",
]

FUSED_OP_BLACK_LIST = ["mul", "tanhgrad", "sigmoidgrad"]
LOWER_OP_BLACK_LIST = [black_op.lower() for black_op in OP_BLACK_LIST]

# TAG黑名单为出厂设置 放在代码里即可
TAG_BLACK_LIST = [
    "segment_sum", "load2d", "im2col", "conv_mad", "matmul_v2", "matmul_gemv",
    "matmul_v2_gemv", "pooling2d_avg", "pooling2d_max", "pooling2d_gap",
    "pooling2d_gmp", "convolution_C", "convolution_C_UB", "convolution_c_col",
    "convolution_im2col_row_major", "convolution_im2col_fractal",
    "conv_vector_bias_add"
]

# 已知tag不在黑名单里即是支持白名单
SUPPORT_TAG_LIST = [
    op_intrin.ori_tag
    for op_intrin in OP_INTRIN_KEY_TAG.values()
    if op_intrin.ori_tag not in TAG_BLACK_LIST
]


BankInfo = namedtuple('BankInfo', ['bank_file', 'bank_type', 'bank_prefix'])


def add_best_to_bank(
        ticks: tuple,
        best_sch_path: str,
        op_info_str: str,
        bank_info: namedtuple,
        best_cheque_info: dict = None) -> bool:
    """
    将最优结果注册到bank中
    :param ticks:
    :param best_sch_path:最优schedule文件路径
    :param op_info_str: kernel_name@op_type@shape
    :param bank_info: namedtuple, including 'bank_file', 'bank_type', 'bank_prefix'
    :param best_cheque_info: a dict, including "base_tick" "best_tick" "bank_key" "cheque"
    :return: bool
    """
    best_tick, base_tick = ticks

    # "tune best_tick" is smoke condition, don't modify
    log.info("RL tune info: add cheque to bank, op_info: %s, tune best_tick: %s, base_tick: %s.",
             op_info_str, best_tick, base_tick)
    if not satisfy_bank(base_tick, best_tick, 'in'):
        log.info("RL tune info: op_info %s not satisfy bank update conditions, stop to add to bank.", op_info_str)
        return False

    bank_file = bank_info.bank_file
    if bank_info.bank_type != 'custom':
        bank_file = get_bank_name(bank_info.bank_prefix)
    ret, output = try_add_cheque(best_sch_path, bank_info.bank_type, bank_file,
                                 op_info_str, best_cheque_info=best_cheque_info)
    if not ret:
        log.warn("RL tune info: op_info %s can not add search result to bank: %s.", op_info_str, output)
        return False
    log.info("RL tune info: add cheque to rl %s bank succ, op_info: %s, bank_file_name: %s, best_cheque_info: %s.",
             bank_info.bank_type, op_info_str, bank_file, best_cheque_info)
    update_bank(bank_info.bank_prefix)
    return True


def parse_tune_res(best_path):
    """
    根据最优schedule文件名解析tune的结果
    :param best_path:最优schedule文件路径
    :return: best_tick, best_path, base_tick
    """
    if not best_path:
        return None
    best_tick, base_tick = os.path.basename(best_path).split("_")[:2]
    return int(best_tick), best_path, int(base_tick)


def get_best_from_infer_search(infer_best_path, search_best_path):
    """
    比较infer结果和search结果得到最优结果
    :param infer_best_path:infer出来的最优文件路径
    :param search_best_path:search出来的最优文件路径
    :return: best_tick, best_sch_path, base_tick
    """
    log.dbg("RL tune info: get_best_from_infer_search begin, infer_best_path: %s, search_best_path: %s.",
             infer_best_path, search_best_path)
    infer_best_info = parse_tune_res(infer_best_path)
    search_best_info = parse_tune_res(search_best_path)

    if infer_best_info is None and search_best_info is None:
        log.warn("RL tune info: can not get_best_from_infer_search, best result is empty.")
        return None, None, None

    if infer_best_info is None:
        best_tick, best_sch_path, base_tick = search_best_info
    elif search_best_info is None:
        best_tick, best_sch_path, base_tick = infer_best_info
    else:
        infer_best_tick, infer_best_path, infer_base_tick = infer_best_info
        search_best_tick, search_best_path, search_base_tick = search_best_info
        if infer_best_tick > search_best_tick:
            best_tick = search_best_tick
            best_sch_path = search_best_path
            base_tick = search_base_tick
        else:
            best_tick = infer_best_tick
            best_sch_path = infer_best_path
            base_tick = infer_base_tick
    log.info("RL tune info: get_best_from_infer_search end, best_tick: %s, base_tick: %s.",
             best_tick, base_tick)
    return best_tick, best_sch_path, base_tick


def get_curr_res(stage: object, curr_res: list) -> None:
    """
    get curr_res
    """
    for idx in range(stage.op.num_outputs):
        if stage.op.output(idx) in curr_res:
            continue
        curr_res.append(stage.op.output(idx))


def get_res_by_output_name(sch, output_name_list):
    """
    :param sch:
    :param output_name_list:
    :return:
    """
    log.dbg("RL tune info: get_res_by_output_name begin, sch: %s, output_name_list: %s.", sch, output_name_list)
    curr_res = []
    for output_name in output_name_list:
        for stage in sch.stages:
            if stage.op.name != output_name:
                continue
            get_curr_res(stage, curr_res)
            break

    log.dbg("RL tune info: get_res_by_output_name end, sch: %s, curr_res: %s.", sch, curr_res)
    return curr_res


def filter_black_op_type(op_type: str) -> set:
    """
    check_black_op_type
    :param op_type:
    :return:
    """
    op_type_list = [x.lower() for x in op_type.split("__")]
    op_matched_num = 0
    for op in op_type_list:
        if op in FUSED_OP_BLACK_LIST:
            op_matched_num += 1
    if op_matched_num == len(FUSED_OP_BLACK_LIST):
        return set(op_type)

    inter_black_list = set(op_type_list) & set(LOWER_OP_BLACK_LIST)
    if inter_black_list:
        log.warn("RL tune info: op %s is not supported.", op_type)
    return inter_black_list


def stage_check(sch, op_type):
    """
    check stage
    :param sch:
    :param op_type:
    :return:
    """
    for stage in sch.stages:
        if stage.op.tag and stage.op.tag.split("|")[0] not in SUPPORT_TAG_LIST:
            log.info("RL tune info: rl_tune not support %s tag, skip!", stage.op.tag)
            return False
        if isinstance(stage.op, tvm.ExternOp):
            log.info("RL tune info: rl_tune not support %s extern op, skip!", op_type)
            return False

    if len(sch.stages) > MAX_ORIGIN_STAGE_CNT:
        log.info("RL tune info: current op stage_num:%d, rl_tune supported max stage_num is %d, skip!",
                 len(sch.stages), MAX_ORIGIN_STAGE_CNT)
        return False

    return True


def rl_check_sch(sch: object, op_type: str) -> bool:
    """
    check schedule
    :param sch: default schedule object
    :param op_type: op type, fused op is joined by "__"
    :return: bool
    """
    if sch is None:
        log.info("RL tune info: can not get default schedule of %s, skip!", op_type)
        return False

    if not stage_check(sch, op_type):
        log.warn("RL tune info: %s stage can not check, skip!", op_type)
        return False
    return True


def tune_comm_check(sch: object, op_type: str) -> bool:
    """
    检查是否支持rl tune
    :param sch: default schedule object
    :param op_type: op type, fused op is joined by "__"
    :return: bool
    """
    if filter_black_op_type(op_type):
        log.info("RL tune info: rl_tune not support %s op, skip!", op_type)
        return False

    return rl_check_sch(sch, op_type)


def check_repeat_tune(res_tensor: list, kernel_name: str) -> (bool, bool, list):
    """
    check_repeat_tune
    :param res_tensor:
    :param kernel_name:
    :return: is_tune, hit_bank, bank_key
    """
    # get_rl_bank_key by op_name
    rl_bank_key = get_rl_bank_key(res_tensor)
    if not rl_bank_key:
        log.warn("RL tune info: op %s can not get rl bank key, will not tune.", kernel_name)
        return False, False, rl_bank_key

    hit_bank = check_bank_hit(rl_bank_key, kernel_name)
    if not hit_bank:
        log.info("RL tune info: op %s not hit bank.", kernel_name)
        return True, hit_bank, rl_bank_key

    if os.getenv("REPEAT_TUNE", "False").lower() == "true":
        log.dbg("enable REPEAT_TUNE!")
        return True, hit_bank, rl_bank_key
    # 没有打开REPEAT_TUNE，但命中了bank，就不重复tune了
    log.info("op %s hit bank and REPEAT_TUNE is False, will not tune.", kernel_name)
    return False, hit_bank, rl_bank_key


def get_timeout(tune_by_om: bool = False, high_perf: bool = False) -> int:
    """
    get_timeout
    :return:
    """
    # 设置调优超时时间
    default_timeout = TUNE_TIME_PER_TASK_OM if tune_by_om else TUNE_TIME_PER_TASK
    if high_perf:
        default_timeout = TUNE_TIME_HIGH_PERF
    try:
        return int(os.getenv("TUNE_TIMEOUT", str(default_timeout)))
    except ValueError:
        log.err("TUNE_TIMEOUT error, must be a number. Now use default timeout:%d", default_timeout)
        return default_timeout
    finally:
        pass


def clean_evb_tss_workspace(evb_infos):
    """
    # 清理evb机器上的临时文件
    :param tune_workspace:
    :param tune_show_dir:
    :return:
    """
    for evb_info in evb_infos:
        evb_clean_cmd = "rm -rf %s" % evb_info["EVB_HOST_BASE_DIR"]
        evb_clean_cmd = util.get_exec_cmd(evb_clean_cmd, evb_info["host_ip"],
                                          evb_info["host_user"],
                                          evb_info.get("host_password", ""))
        util.run_cmd_comm(evb_clean_cmd)


def tune_workspace_clean(  # pylint: disable=R0914
        tune_workspace, tune_show_dir, option):
    """
    clean tune workspace
    :param tune_workspace:
    :param tune_show_dir:
    :return:
    """
    # 等2秒，待子进程全走完
    time.sleep(2)
    evb_info_list = option.get("evbs", [])
    clean_evb_tss_workspace(evb_info_list)
    # 删除tune_show_dir下面的in_use.Flag
    util.rm_proc(os.path.join(tune_show_dir, 'RL_in_use.flag'))
    # 如果当前tune_show目录下已经没有in_use Flag了，就删除它，否则不删除
    in_use_flags = glob.glob(os.path.join(tune_show_dir, '*_in_use.flag'))
    if not in_use_flags:
        util.rm_proc(tune_show_dir)
    # 删除kernel_meta下面的东西
    util.run_cmd_comm("rm -rf %s" %
                      os.path.join(kernel_meta_dir(), 'rltune__*'))

    # 清除tune_workspace
    workspace_parent_dir = os.path.dirname(tune_workspace.rstrip('/'))
    workspace_name = os.path.basename(tune_workspace.rstrip('/'))
    tune_workspaces = []
    for subdir in os.listdir(workspace_parent_dir):
        if subdir.startswith(workspace_name):
            tune_workspaces.append(os.path.join(workspace_parent_dir, subdir))

    # 删除无用目录
    for workspace in tune_workspaces:
        util.rm_proc(os.path.join(workspace, "work_dir"))
        util.rm_proc(os.path.join(workspace, "valid"))
        util.rm_proc(os.path.join(workspace, "result_dir"))
        outputs_dir = os.path.join(workspace, "outputs")
        generation_logs = glob.glob(os.path.join(outputs_dir,
                                                 "*/generation_*.log"))
        for generation_log in generation_logs:
            util.mv_src_to_dst(generation_log, workspace)
        util.rm_proc(outputs_dir)
        # 远程的场景不打包
        if evb_info_list and evb_info_list[0]["host_ip"] != "localhost":
            return True
        # 压缩
        ret, _ = util.run_cmd_comm(
            "tar -zcvf %s.tar.gz -C %s %s" %
            (workspace, os.path.dirname(workspace),
             os.path.basename(workspace)))
        if ret:
            util.rm_proc(workspace)

    workspace_list = glob.glob(
        os.path.join(os.path.dirname(tune_workspace), "tune_workspace_*"))

    # 删除10天前的workspace目录和压缩包
    curr_time = time.time()
    for workspace_dir in workspace_list:
        modify_time = os.stat(workspace_dir).st_mtime
        if curr_time - modify_time > WORKSPACE_MAX_LIFE_TIME:
            util.rm_proc(workspace_dir)

    return True


def get_ori_op_name(op_model_name, kernel_name, op_json):
    """

    :param op_model_name:
    :param kernel_name:
    :param op_json:
    :return:
    """
    if op_model_name:
        default_ori_name = op_model_name
    elif kernel_name:
        default_ori_name = kernel_name

    if not op_json:
        return '[%s]' % default_ori_name

    dict_op_json = json.loads(op_json)
    op_list = dict_op_json.get('op_list', [])
    ori_name_list = []
    for op_info in op_list:
        if op_info.get('type') == 'Data':
            continue

        cur_ori_name = op_info.get('ori_name', [])
        if cur_ori_name:
            ori_name_list.append(str(cur_ori_name))

    if ori_name_list:
        return "[%s]" % ','.join(ori_name_list)

    return default_ori_name


def load_json_str(op_json_str: str) -> dict:
    """
    load json from str to dict
    :param op_json_str: str of json
    :return: dict of json
    """
    try:
        op_desc = json.loads(op_json_str)
    except (ValueError, AttributeError) as exception:
        log.err("RL tune info: Cannot load op_json_str, %s", exception)
        return {}
    finally:
        pass
    return op_desc


def create_dir_steply(dir_path: str) -> bool:
    """
    :param dir_path:
    :return: bool
    """
    if os.path.exists(dir_path):
        return True

    path_list = dir_path.split(os.sep)
    obj_path = os.sep
    for path_item in path_list:
        if not path_item:
            continue
        obj_path = os.path.join(obj_path, path_item)
        try:
            if not os.path.exists(obj_path):
                os.mkdir(obj_path, util.DIR_PERMISSION_750)
        except OSError as exception:
            log.err('An error happened while creating {}, error: {}'.format(dir_path, str(exception)))
            return False
        finally:
            pass
    return True


def copy_repo_files(src_dir: str, dst_dir: str) -> None:
    """
    copy repo files from src_dir to dst_dir
    """
    full_soc_version = get_full_soc_version()
    for repo_file in os.listdir(src_dir):
        src_repo_file = os.path.join(src_dir, repo_file)
        if not os.path.isfile(src_repo_file):
            continue
        if full_soc_version in repo_file:
            shutil.copy(src_repo_file, dst_dir)
            dst_repo_file = os.path.join(dst_dir, repo_file)
            os.chmod(dst_repo_file, OPEN_FILE_MODES_640)


def migrate_vector_repo_dirs(rl_data_path: str, customer_repo_path: str) -> None:
    """
    copy repo files from rl_data_path to customer_repo_path
    """
    if not os.access(rl_data_path, os.R_OK | os.W_OK | os.X_OK):
        log.warn("Current user doesn't have rwx permission for dir[%s], "
            "please copy vector repo files to dir[%s] manually!", rl_data_path, customer_repo_path)
        return True

    if not os.listdir(rl_data_path):
        return False

    create_dir_steply(customer_repo_path)
    log.dbg("Try to copy vector repository files under dir[%s] to customer path.", rl_data_path)
    try:
        copy_repo_files(rl_data_path, customer_repo_path)
    except (OSError) as e:
        log.warn("Failed to copy src_path[%s] to dst_path[%s]. Error is %s",
                 rl_data_path, customer_repo_path, e)
    finally:
        pass

    log.info("Copy vector repository files under dir[%s] to customer path[%s] success.",
            rl_data_path, customer_repo_path)
    return True


def get_aoe_data_paths(soc_version: str) -> list:
    """
    get aoe default repo path for migrate
    """
    aoe_data_path = "aoe/data"
    old_ascend_aoe_data_path = os.path.join(OLD_USR_ASCEND_PATH, aoe_data_path)
    old_home_aoe_data_path = os.path.join(os.getenv("HOME", ""), "Ascend", aoe_data_path)
    old_ascend_home_aoe_data_path = os.path.join(os.getenv("ASCEND_HOME_PATH", ""), aoe_data_path)
    old_aoe_data_paths = [old_ascend_home_aoe_data_path, old_ascend_aoe_data_path, old_home_aoe_data_path]

    old_rl_aoe_custom_path = []
    for path in old_aoe_data_paths:
        old_rl_aoe_custom_path.append(os.path.realpath(os.path.join(path, soc_version, "rl")))
    return old_rl_aoe_custom_path


def vector_custom_repo_migrate() -> None:
    """
    custom repo migrate to user home path
    :param: None
    :return: None
    """
    if os.getenv("TUNE_BANK_PATH", "") or os.getenv("ASCEND_CACHE_PATH", ""):
        log.info("TUNE_BANK_PATH or ASCEND_CACHE_PATH is set,"
            "default vector customer repository files won't be migrated.")
        return
    full_soc_version = get_full_soc_version()
    customer_repo_path = os.path.realpath(os.path.join(get_default_rl_path(custom=True), full_soc_version, "vector/"))
    if os.path.exists(customer_repo_path) and os.access(customer_repo_path, os.R_OK | os.W_OK | os.X_OK) and \
        os.path.isdir(customer_repo_path) and os.listdir(customer_repo_path):
        log.info("customer repo path[%s] already has repo files, won't migrate.", customer_repo_path)
        return

    soc_version = get_soc_version()
    old_rl_data_paths = get_aoe_data_paths(soc_version)
    old_rl_data_paths.extend(get_previous_bank_paths())

    for rl_data_path in old_rl_data_paths:
        if not os.path.exists(rl_data_path) or not os.path.isdir(rl_data_path):
            log.dbg("vector repo path[%s] not exists.", rl_data_path)
            continue
        if migrate_vector_repo_dirs(rl_data_path, customer_repo_path):
            return

    log.info("No qualified vector repository files to migrate.")


def check_path_valid(custom_bank_path: str) -> bool:
    custom_bank_path = os.path.realpath(custom_bank_path)
    if not os.path.isdir(custom_bank_path) or not os.access(custom_bank_path, os.R_OK | os.W_OK | os.X_OK):
        log.err("PATH :%s is without access, please check!" % custom_bank_path)
        return False
    return True


def check_tune_bank_path_valid() -> bool:
    """
    check if TUNE_BANK_PATH or ASCEND_CACHE_PATH available
    """
    tune_bank_path = os.getenv("TUNE_BANK_PATH", "")
    ascend_cache_path = os.getenv("ASCEND_CACHE_PATH", "")
    if not tune_bank_path and not ascend_cache_path:
        return True
    if tune_bank_path:
        return check_path_valid(tune_bank_path)
    return check_path_valid(ascend_cache_path)


def parse_tik_tensor_info(tik_tensor_info: list) -> list:
    """
    :param tik_tensor_info
    :return tik_tensor_list
    """
    if tik_tensor_info is None:
        return []
    input_tensors, output_tensors = tik_tensor_info
    input_info_list = []
    for input_tensor in input_tensors:
        input_info_list.append(
            (input_tensor.name, input_tensor.shape, input_tensor.dtype))
    output_info_list = []
    for output in output_tensors:
        output_info_list.append((output.name, output.shape, output.dtype))

    return [input_info_list, output_info_list]
