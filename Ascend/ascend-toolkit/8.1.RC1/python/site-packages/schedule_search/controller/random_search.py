#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
import math
import multiprocessing
import os
import pickle
import random
import time
from functools import reduce as functools_reduce

import numpy as np

from schedule_search import log
from schedule_search.controller.search_comm import dump_params_to_file
from schedule_search.controller.search_comm import get_action_mask
from schedule_search.controller.search_comm import translate_action
from schedule_search.ts_env.code_to_tensor.at_analysis import get_at_info
from schedule_search.ts_env.env_util import get_init_action_tensor
from schedule_search.ts_env.te_auto_schedule_env import gen_sch
from schedule_search.ts_env.te_auto_schedule_env import get_op_schedule_info
from schedule_search.ts_env.te_auto_schedule_env import get_tick
from schedule_search.ts_env.te_auto_schedule_env import update_op_schedule_info
from schedule_search.ts_env.tensor_cfg import AXIS_CNT
from schedule_search.util import get_op_layers
from schedule_search.util import OPEN_FILE_MODES_640
from schedule_search.util import WRITE_FILE_FLAGS


def update_for_at_proc(op_schedule_info: object, ori_op_schedule_info: object, workspace_info: object) -> tuple:
    """
    update op_schedule_info
    """
    did_at_choice = False
    for consumer in workspace_info.consumers:
        if len(consumer.at_candidates) == 1 or consumer.sampled_target is not None:
            continue
        # 随机选一个At Target
        action_value = random.randint(0, len(consumer.at_candidates) - 1)
        consumer.set_sampled_target(consumer.at_candidates[action_value])
        new_stages_info = []
        for stage_info in op_schedule_info.stages_info:
            if not {'CacheWrite', 'CacheRead'} & set(stage_info.get('type', [])):
                new_stages_info.append(stage_info)
        op_schedule_info = copy.deepcopy(ori_op_schedule_info)
        op_schedule_info.stages_info = new_stages_info
        did_at_choice = True
        break
    return did_at_choice, op_schedule_info


def at_proc(op_schedule_info: object, ori_op_schedule_info: object) -> None:
    """
    :param op_schedule_info:
    :param ori_op_schedule_info:
    :return:
    """
    while True:
        get_at_info(op_schedule_info)
        did_at_choice = False
        for stage_index in range(len(op_schedule_info.stages_info) - 1, -1, -1):
            workspace_info = op_schedule_info.stages_info[stage_index].get('at_info', None)
            did_at_choice, op_schedule_info = update_for_at_proc(op_schedule_info, ori_op_schedule_info, workspace_info)
            if did_at_choice:
                break
        if not did_at_choice:
            break


def update_best_action_dict(op_schedule_info: object, best_action_dict: dict,
                            batch_size: int, global_step: int) -> None:
    """
    update_best_action_dict after run_one_step
    """
    action_tensors, ticks = run_one_step(batch_size, op_schedule_info)
    real_tick = [i for i in ticks if i]
    if real_tick:
        min_tick = min(real_tick)
        if best_action_dict["tick"] > min_tick:
            best_action_dict["tick"] = min_tick
            best_action_dict["action_tensor"] = action_tensors[ticks.index(min_tick)]
            best_action_dict["step"] = global_step
        log.dbg("best_action_dict:%s", best_action_dict)


def search_loop(op_schedule_info: object, best_action_dict: dict, evb_error_tick: bool) -> dict:
    """
    find best action dict
    """
    option = op_schedule_info.option
    tick_threshold = option.get("evb_error", 10) if evb_error_tick else 10
    # 随机尝试的次数
    batch_size = int(option.get('batch_size', 3))
    max_step = int(option.get('max_step', 1))
    timeout = int(option.get('timeout', 1800))
    garbage_steps = int(option.get('garbage_steps', 20))
    base_tick = op_schedule_info.base_tick
    # 如果base_tick获取失败则赋值为0，防止后面打印失败
    if not base_tick:
        base_tick = 0
    global_step = 0
    # 随机生成相应的ActionTensor，并行获取其Reward和Tick等信息
    log.info("Gen random actions.")
    begin_time = time.time()
    while True:
        global_step += 1
        # 每一轮更新best_action_dict
        update_best_action_dict(op_schedule_info, best_action_dict, batch_size, global_step)

        # 如果超过时间限制或者步数限制，退出循环
        if global_step >= max_step or (time.time() - begin_time) > timeout:
            log.info(
                "search run %s steps and %ss, beyond max_step:%s or max_time:%ss, stop!", global_step,
                time.time() - begin_time, max_step, timeout)
            break
        # 至少搜够5倍的garbage_steps，再去看下一个garbage_steps步内有没有搜到更好的，
        # 若无则early_stop，退出循环
        if global_step >= garbage_steps * 5 and global_step - best_action_dict["step"] >= garbage_steps:
            log.info(
                "current step:%s, best_action step:%s, no improvement for %s steps, ealy stop!", global_step,
                best_action_dict["step"], garbage_steps)
            break

        # 已经搜到比base好的，停下
        if option.get("stop_better_than_base", False) and best_action_dict["tick"] < base_tick + tick_threshold:
            log.info("best tick %s is better than base %s, early stop!",
                     best_action_dict["tick"], base_tick)
            break

    log.info("search result: %d step, %ds , %d action_tensor\n"
        "best_action_dict:%s", global_step, time.time() - begin_time, global_step * batch_size, best_action_dict)

    best_tick = best_action_dict["tick"]
    log.info("Shape:%s. Base tick: %s. Best Tick: %s. Delta tick: %s.",
             op_schedule_info.shape, base_tick, best_tick, (base_tick - best_tick))

    return best_action_dict


def search(res, option):  # pylint: disable=R0912,R0914,R0915
    """
    这个函数的调用者可能是Python2也可能是Python3，
    但这个函数调用的接口只支持Python3。
    :param res: TVM compute 的输出Tensor
    :param option:  参数信息
    :return: 1. 搜索是否成功
    2. 最优的Schedule对象
    """
    log.info("Random search in. option: %s.", str(option))
    dump_params_to_file(option)
    params_file = os.path.join(option['output_dir'], "params.pk")
    output_dir = os.path.dirname(params_file)
    ori_op_schedule_info = get_op_schedule_info(res, option)[0]
    op_schedule_info = copy.deepcopy(ori_op_schedule_info)

    at_proc(op_schedule_info, ori_op_schedule_info)
    action_tensor = get_init_action_tensor(len(op_schedule_info.schedule_obj.stages))
    # 要更新一下相关信息
    update_op_schedule_info(op_schedule_info, action_tensor, 10, 30)

    # 随机生成相应的ActionTensor，并行获取其Reward和Tick等信息
    new_stages_info = []
    for stage_info in op_schedule_info.stages_info:
        if not {'CacheWrite', 'CacheRead'} & set(stage_info.get('type', [])):
            new_stages_info.append(stage_info)
    best_action_dict = {
        "action_tensor": None,
        "tick": int(1e8),
        "step": -1,
        "stages_info": new_stages_info
    }
    best_action_dict = search_loop(op_schedule_info, best_action_dict, False)

    log.dbg("search for best action_tensor succ, save it to search_result.pk")
    result_file = os.path.join(output_dir, 'search_result.pk')
    with os.fdopen(os.open(result_file, WRITE_FILE_FLAGS, OPEN_FILE_MODES_640), 'wb') as file_handler:
        pickle.dump((True, best_action_dict), file_handler, protocol=2)
    return True, best_action_dict


def gen_schedule(best_action_dict, output_tensor, option):
    """

    :param best_action_dict:
    :param output_tensor:
    :param option:
    :return:
    """
    ret, best_sch_path, _ = gen_sch(best_action_dict, output_tensor, option)
    log.dbg("gen_schedule result is %s!", ret)
    return ret, best_sch_path


def gen_factor(factor_label_num):
    """

    :param factor_label_num:
    :return:
    """
    factor = 0
    for i in range(factor_label_num):
        label_choice = random.randint(0, 1)
        factor += 2**i * label_choice
    factor += 1
    return factor


def get_sample(j: int, is_mask: bool, factor_label_num: int, reorder_choice_cnt: int) -> int:
    """
    get sample for get_random_actions
    """
    # 如果mask是True，就不需要重复采了
    if is_mask is True:
        return 0
    # 根据多label方式采样
    if j < 2 * AXIS_CNT:
        return gen_factor(factor_label_num)
    # 采样reorder
    if j < 2 * AXIS_CNT + 2:
        return random.randint(0, reorder_choice_cnt - 1)
    # 采样at
    return random.randint(0, 4)


def get_random_actions(op_schedule_info: object, step: int, batch: int) -> list:
    """
    :param op_schedule_info:
    :param step:
    :param batch:
    :return:
    """
    feature = op_schedule_info.feature_tensor
    op_layer = get_op_layers(op_schedule_info.schedule_obj)
    action_mask = get_action_mask(
        feature, AXIS_CNT, op_layer,
        op_schedule_info.option.get("at_align", True))
    reorder_choice_cnt = functools_reduce(lambda x, y: x * y, range(1, op_schedule_info.shape_cnt + 1))
    factor_label_num = int(math.ceil(math.log(op_schedule_info.shape_max, 2)))

    r_action_steps = []
    for _ in range(step * batch):
        # 先初始化一个全0的actions
        actions = np.zeros([len(action_mask), (2 * AXIS_CNT + 2 + 1)]).tolist()
        for stage_index, mask_info in enumerate(action_mask):
            for j, is_mask in enumerate(mask_info):
                actions[stage_index][j] = get_sample(j, is_mask, factor_label_num, reorder_choice_cnt)
        # 转化成ts_env可以看懂的action
        actions = translate_action(actions, 2 * AXIS_CNT, feature,
                                   factor_label_num, reorder_choice_cnt)
        r_action_steps.append(actions)
    return r_action_steps


def run_one_step(batch_size, op_schedule_info):
    """

    :param batch_size:
    :param op_schedule_info:
    :return:
    """
    # 生成一步的action
    action_tensors = get_random_actions(op_schedule_info, 1, batch_size)
    # 多进程获取tick
    multiproc_pool = multiprocessing.Pool(processes=batch_size)
    pool_result_list = []
    for action_tensor in action_tensors:
        pool_result_list.append(
            multiproc_pool.apply_async(get_tick,
                                       args=(
                                           action_tensor,
                                           op_schedule_info,
                                       )))

    multiproc_pool.close()
    multiproc_pool.join()

    ticks = []
    for pool_result in pool_result_list:
        _, tick, _, _ = pool_result.get()
        ticks.append(tick)
    return action_tensors, ticks


def search_proc(params_file: str) -> tuple:
    """
    :param params_file:
    :return:
    """
    # 先解析参数
    if not os.path.exists(params_file) or not os.path.isfile(params_file):
        log.err("params_file %s not exists!", params_file)
        return False, {}
    with open(params_file, "rb") as file_handler:
        res, option, _ = pickle.load(file_handler)
    output_dir = os.path.dirname(params_file)
    op_schedule_info = get_op_schedule_info(res, option)[0]
    best_action_dict = {"action_tensor": None, "tick": int(1e8), "step": -1}
    best_action_dict = search_loop(op_schedule_info, best_action_dict, True)

    log.dbg("search for best action_tensor succ, save it to search_result.pk")
    result_file = os.path.join(output_dir, 'search_result.pk')
    with os.fdopen(os.open(result_file, WRITE_FILE_FLAGS, OPEN_FILE_MODES_640), 'wb') as file_handler:
        pickle.dump((True, best_action_dict), file_handler, protocol=2)
    return True, best_action_dict
