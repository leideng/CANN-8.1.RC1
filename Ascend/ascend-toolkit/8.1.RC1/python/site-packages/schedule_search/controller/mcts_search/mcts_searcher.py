#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import multiprocessing
import os
import sys
import time
from absl import flags
from schedule_search import log
from schedule_search import util
from schedule_search import global_manager
from schedule_search.ts_env.estimator import om_runner
from schedule_search.controller.mcts_search.infer import SamplePara
from schedule_search.controller.mcts_search.infer import sample_once
from schedule_search.controller.mcts_search.infer import dfs_search
from schedule_search.controller.mcts_search.validate import validation
from schedule_search.controller.search_comm import get_best_sch_path

flags.DEFINE_string('workspace', None, '')
flags.DEFINE_string('load_file_rl', 'work_dir/init', 'Path to model save files.')
flags.DEFINE_string('infer_dir_rl', 'outputs', 'Where to write search data.')
flags.DEFINE_string('param_dir', "./replay_dir/param_dir",
                    'Config file of multi shape list.')
flags.DEFINE_integer('cpu_process_num',
                     util.cpu_num() // 4, 'cpu process num.')
flags.DEFINE_bool('timer', False, 'if True, gen timer statics')
flags.DEFINE_bool(
    'stop_better_than_base', False, 'if True, stop search if current best tick'
                                    'is better than base tick.')
flags.DEFINE_integer('num_readouts_rl', 30, 'MCTS readouts/roll outs.')
flags.register_validator('num_readouts_rl', lambda x: x > 0)
flags.DEFINE_integer('val_num_readouts_rl', 20,
                     'MCTS readouts/roll outs in valid.')
FLAGS = flags.FLAGS

CPU_OCCUPY_RATE = 0.5
MAX_PROCESS_COUNT = 8
SAMPLE_POLL_INTERVAL = 0.5
SINGLE_PROCESS_MAX_TIME = 1800


class ProcessShareInfos:
    def __init__(self):
        """
        infos shared in different processes
        """
        self.best_cheque_dict = global_manager.BEST_CHEQUE_DICT
        self.action_history_dict = global_manager.ACTION_HISTORY_DICT
        self.cheque_history_dict = global_manager.CHEQUE_HISTORY_DICT
        self.best_tick_dict = global_manager.BEST_TICK_DICT
        self.cb_task_queue = om_runner.CB_TASK_QUEUE
        self.cb_res_dict = om_runner.CB_RES_DICT
        self.cb_release_queue = om_runner.CB_RELEASE_QUEUE


class RlPool:
    """
    Class which supports an async version of applying functions to arguments.
    """
    def __init__(self, processes: int = None, time_out: int = sys.maxsize,
                 single_max_time: int = None):
        """
        :param processes: process_num
        :param time_out
        :param single_max_time
        """
        if processes is None or processes < 1:
            processes = os.cpu_count() or 1
        # 处理器的数量
        self._processes_num = processes
        # 进程池，存储当前运行的进程
        self._pool = []
        # 进程池中所有进程最大可用时间
        self._pool_time_out = time_out
        # 单个进程的最长执行时间
        self._single_process_max_time = single_max_time
        if single_max_time is None:
            self._single_process_max_time = time_out + SINGLE_PROCESS_MAX_TIME
        # 任务池，存储所有待处理的任务
        self._all_tasks_list = []
        self.dfs_process = None

    @staticmethod
    def _stop_processes(process_info_list: list) -> list:
        """
        stop all processes in process_list
        :param process_info_list:
        """
        process_to_remove = []
        for process_info in process_info_list:
            process_handler, _, _ = process_info
            if process_handler and process_handler.is_alive():
                process_pid = process_handler.pid
                process_handler.kill()
                process_handler.join()
                process_handler.close()
                process_to_remove.append(process_info)
                log.dbg("RL search info: process %s is stopped.", process_pid)
        return process_to_remove

    def apply_work(self, func_name: object, func_args: SamplePara):
        """
        add all tasks to all_processes_list
        :param func_name
        :param func_args
        """
        self._all_tasks_list.append([func_name, func_args])

    def wait_works(self):
        """
        wait all processes to finish or timeout
        """
        for task_one in self._all_tasks_list:
            # create a new process to process task
            func_name, func_args = task_one
            func_args_tuple = (func_args,)
            self._create_new_process(func_name, func_args_tuple)
        # wait processes in pool to end or run out its time
        while True:
            self._clean_pool()
            # check if all processes is not alive or not
            no_process_alive_flag = True
            for process_info in self._pool:
                process_handler, _, _ = process_info
                if process_handler.is_alive():
                    no_process_alive_flag = False
            if no_process_alive_flag:
                log.info("RL search info: all processes ended.")
                break
            time.sleep(SAMPLE_POLL_INTERVAL)

    def apply_all_works(self, func_name: object, func_args: SamplePara, valid: bool, dfs_res: object):
        """
        apply_sync
        :param func_name
        :param func_args
        :param valid
        """
        index = 1
        pool_start_time = time.time()
        early_stop_flag = False

        # multiple mcts trees
        while True:
            if self.dfs_process and \
                dfs_res.value and \
                (self.dfs_process not in self._pool or not self.dfs_process[0].is_alive()):
                log.event("RL tune info: dfs search process end, early stop!")
                early_stop_flag = True

            if time.time() - pool_start_time > self._pool_time_out or early_stop_flag:
                process_to_remove = self._stop_processes(self._pool)
                self._remove_process(process_to_remove)
                break
            self._clean_pool()
            # if resources are available, create more processes
            while self._processes_num > len(self._pool):
                func_args.index = index
                func_args_tuple = (func_args, valid)
                self._repopulate_pool(func_name, func_args_tuple)
                index += 1

    def apply_dfs_work(self, dfs_func_name: object, op_schedule_info: object, process_share_infos: object,
                       dfs_res: object):
        """
        apply one dfs search process before multiple mcts search processes
        :param dfs_func_name:
        :param op_schedule_info:
        :param process_share_infos:
        """
        # one dfs tree
        func_args_tuple = (op_schedule_info, process_share_infos, dfs_res)
        self._repopulate_pool(dfs_func_name, func_args_tuple)
        self.dfs_process = self._pool[0]
        log.dbg("RL search info: dfs process %s is added to pool.", self.dfs_process[0].pid)

    def _repopulate_pool(self, func_name: object, func_args_tuple: tuple):
        """
        create a new process and add to the pool
        :param func_name
        :param func_args_tuple
        """
        ctx = multiprocessing.get_context()
        sample_process = ctx.Process(target=func_name, args=func_args_tuple, daemon=True)
        sample_process.start()
        self._pool.append([sample_process, time.time(), self._single_process_max_time])
        log.dbg("RL search info: new process %s add to pool, timeout %s.",
                sample_process.pid, self._single_process_max_time)

    def _clean_pool(self):
        """
        delete process handler when process is not alive
        """
        # stop some process when it run out of its time
        process_to_remove = []
        for process_info in self._pool:
            process_handler, start_time, process_time_out = process_info
            if time.time() - start_time > process_time_out:
                process_to_remove.extend(self._stop_processes([process_info]))
        self._remove_process(process_to_remove)

        # clean one process when it is not alive
        process_dead = []
        for process_info in self._pool:
            process_handler, _, _ = process_info
            if process_handler and not process_handler.is_alive():
                process_handler.close()
                process_dead.append(process_info)
        self._remove_process(process_dead)

    def _remove_process(self, process_to_remove: list):
        """
        remove process in self._pool
        :param process_to_remove:
        """
        if process_to_remove:
            for process in process_to_remove:
                self._pool.remove(process)
                del process

    def _create_new_process(self, func_name: object, func_args_tuple: tuple):
        """
        try to create a new process, return True when succ, else return False
        """
        start_ts = time.time()
        while True:
            # clean processes pool
            self._clean_pool()
            if self._processes_num > len(self._pool):
                self._repopulate_pool(func_name, func_args_tuple)
                log.dbg("RL search info: create new process successfully, duration: %s.",
                        time.time() - start_ts)
                return True
            # wait for _pool_time_out seconds at most for one process
            if time.time() - start_ts > self._single_process_max_time:
                log.dbg("RL search info: create new process with timeout: %s.",
                        self._single_process_max_time)
                return False
            time.sleep(SAMPLE_POLL_INTERVAL)


def mcts_search_valid(op_schedule_info_list_ori: list) -> None:
    """
    search on mcts tree one time
    :param op_schedule_info_list_ori
    """
    log.event("RL search info: mcts_search_valid begin.")
    op_schedule_infos_list = [op_info for op_info in op_schedule_info_list_ori if op_info]
    if not op_schedule_infos_list:
        return

    process_num = min(int(multiprocessing.cpu_count() * CPU_OCCUPY_RATE), MAX_PROCESS_COUNT)
    log.event("RL search info: cpu total count: %s, used count: %s.", multiprocessing.cpu_count(), process_num)

    valid_dir = FLAGS.workspace
    util.ensure_dir_exists(valid_dir, False)
    valid_result_dir = os.path.join(FLAGS.workspace, "result_dir", "%s_%s" % (os.getpid(), time.time()))
    util.ensure_dir_exists(valid_result_dir, reset=False)

    pool = RlPool(processes=process_num)
    process_share_infos = ProcessShareInfos()
    for index, op_schedule_info in enumerate(op_schedule_infos_list):
        valid_para = SamplePara(None, op_schedule_info, valid_dir, "", valid_result_dir, index, process_share_infos)
        pool.apply_work(validation, valid_para)
    pool.wait_works()
    del pool

    util.rm_proc(valid_result_dir)
    log.event("RL search info: mcts_search_valid end.")


def mcts_search(op_schedule_info: list, time_out: int) -> None:
    """
    启动搜索所有相关进程：mcts_search, sample one op once
    :param op_schedule_info: 算子调度信息列表
    :param time_out:
    :return:
    """
    log.event("RL search info: mcts search begin, time out is %s.", time_out)
    if is_tune_stop(op_schedule_info[0]):
        return

    process_num = min(int(multiprocessing.cpu_count() * CPU_OCCUPY_RATE), MAX_PROCESS_COUNT)
    log.event("RL search info: cpu total count: %s, used count: %s.", multiprocessing.cpu_count(), process_num)

    workspace = op_schedule_info[0].option.get("TUNE_WORKSPACE", FLAGS.workspace)
    infer_dir = os.path.join(workspace, "infer_dir")
    infer_result_dir = os.path.join(workspace, "result_dir", "%s_%s" % (os.getpid(), time.time()))
    util.ensure_dir_exists(infer_dir, False)
    util.ensure_dir_exists(infer_result_dir, reset=False)

    pool = RlPool(processes=process_num, time_out=time_out)
    process_share_infos = ProcessShareInfos()
    sample_para = SamplePara(None, op_schedule_info, infer_dir, "", infer_result_dir, 0, process_share_infos)
    dfs_res = multiprocessing.Value("b", False)
    pool.apply_dfs_work(dfs_search, op_schedule_info, process_share_infos, dfs_res)
    pool.apply_all_works(sample_once, sample_para, False, dfs_res)
    del pool


def is_tune_stop(op_schedule_info: object) -> bool:
    """

    :param op_schedule_info:
    :return:
    """
    if op_schedule_info.tune_flag is not None \
            and not os.path.exists(op_schedule_info.tune_flag):
        return True

    if FLAGS.stop_better_than_base:
        shape_list_str = op_schedule_info.shape_list_str
        best_sch_path = get_best_sch_path(op_schedule_info, dst_dir=None, silent=True)
        if best_sch_path:
            curr_best_tick = int(os.path.basename(best_sch_path).split("_")[0])
            tmp_info_str = "op_name: %s, shape: %s, curr_best_tick: %s" % (
                op_schedule_info.op_name, shape_list_str, curr_best_tick)
            if op_schedule_info.base_tick not in [0, sys.maxsize] and \
                    curr_best_tick <= op_schedule_info.base_tick + 5:
                log.info("RL search info: %s is better than base %s, skip it!", tmp_info_str,
                         op_schedule_info.base_tick)
                if op_schedule_info.tune_flag:
                    util.rm_proc(op_schedule_info.tune_flag)
                return True
            log.info("RL search info: %s.", tmp_info_str)

    return False
