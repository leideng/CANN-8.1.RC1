#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
from itertools import permutations
from typing import NoReturn
from typing import List
import numpy as np

from schedule_search import log
from schedule_search import comm
from schedule_search.op_cfg import TIK_TO_DSL_OP_LIST
from schedule_search.controller.mcts_search.features import FEATURE_LEN
from schedule_search.controller.mcts_search.features import \
    ITERVAR_ATTR_FEATURE_LEN
from schedule_search.controller.mcts_search.features import ITERVAR_NUM
from schedule_search.controller.mcts_search.features import ITERVAR_SHAPE_LEN
from schedule_search.controller.mcts_search.features import ITERVAR_TODO_LEN
from schedule_search.controller.mcts_search.features import SEARCH_N
from schedule_search.controller.mcts_search.mask_rules import masker
from schedule_search.controller.mcts_search.procedure.action import ActionType
from schedule_search.controller.mcts_search.procedure.comm import \
    need_search_at_axis
from schedule_search.controller.mcts_search.procedure.director import direct
from schedule_search.controller.mcts_search.procedure.director import get_ready
from schedule_search.ts_env.env_util import get_init_action_tensor
from schedule_search.ts_env.tensor_cfg import AXIS_CNT
from schedule_search.ts_env.tensor_cfg import ActionTensorCfg
from schedule_search.ts_env.tensor_to_code.t2c_util import gemm_identify
from schedule_search.util import get_op_layers
from schedule_search.controller.mcts_search.procedure.comm import \
    is_mad_leaf


class ProgressChain:
    """
    有些算子比如RNN它可能有不止一个op_schedule_info，所以Progress也有不止一个
    因此在这里封装一下
    """
    def __init__(self, op_schedule_infos, valid=False):
        """

        :param op_schedule_infos:
        :param index:
        """
        self.index = 0
        self.num = len(op_schedule_infos)
        self.p_list = []
        for op_schedule_info in op_schedule_infos:
            self.p_list.append(Progress(op_schedule_info, 0, valid=valid))

    def has_next(self):
        """

        :return:
        """
        if self.num > self.index + 1:
            return True
        return False

    def next(self):
        """

        :return:
        """
        self.index += 1

    def replace_p(self, new_progress):
        """

        :param new_progress:
        :return:
        """
        self.p_list[self.index] = new_progress


class Progress:  # pylint: disable=R0902
    """
    Progress
    """
    def __init__(self,  # pylint: disable=R0913
                 op_schedule_info, step_n, state=None, todo=None,
                 valid=False):
        """
        :param param_file: 算子的有效信息用Pickle序列化之后的文件
        :param step_n: 已经完成的Action个数，同时也是MCTS树当前的深度
        :param op_schedule_info: 一个OpScheduleInfo类的对象
        """
        # 算子相关信息
        self.ori_op_schedule_info = op_schedule_info
        self.op_schedule_info = copy.deepcopy(op_schedule_info)
        self.op_layers = get_op_layers(self.op_schedule_info.schedule_obj)
        # 判断是否包含规格轴或者不能切的轴
        self.c_op = self.op_schedule_info.c_op
        self.no_need_at_stages = []
        self.axis_info_list = []
        self.at_choices = []
        self.recent = ()
        # 已经走了几步了
        self.step_n = step_n

        # todo是一个SearcherAction类的对象
        self.action_tensor = get_init_action_tensor(self.stage_num)
        self.todo = todo
        if not self.todo:
            get_ready(self)
        log.dbg('self.todo is: %s', self.todo)

        if state is None:
            self.state = self.action_tensor_to_state()
        else:
            self.state = copy.deepcopy(state)
        # Ticks只有最后才会跑出来
        self.tick = 0
        self.valid = valid

    def __str__(self):
        return str(self.todo)

    def __repr__(self):
        return repr(self.todo)

    def __deepcopy__(self, memodict=None):  # pylint: disable=R0912
        # atomic会新增2个stage，新的progress需要把action_tensor维度降到原始
        todo = copy.deepcopy(self.todo)
        action_tensor = copy.deepcopy(self.action_tensor)
        if self.op_schedule_info.tiling_case > 0 and \
                (len(action_tensor) - 2) \
                == len(self.ori_op_schedule_info.schedule_obj.stages):
            todo.stage_index -= 2
            atomic_dict = self.op_schedule_info.reduce_atomic_dict
            rfactor_stage_index = atomic_dict["rfactor_stage_index"]
            reduce_write_stage_index = atomic_dict["reduce_write_stage_index"]
            action_tensor = np.delete(
                action_tensor, [rfactor_stage_index, reduce_write_stage_index],
                axis=0)

        # 更新一下ori_op_schedule_info的stages_info, 以向后传递
        ori_stage_names = [
            info.get('name')
            for info in self.ori_op_schedule_info.stages_info
        ]
        for stage_info in self.op_schedule_info.stages_info:
            stage_name = stage_info.get('name')
            if stage_name in ori_stage_names:
                ori_stage_index = ori_stage_names.index(stage_name)
                self.ori_op_schedule_info.stages_info[ori_stage_index].update(
                    stage_info)

        new_progress = Progress(self.ori_op_schedule_info, self.step_n,
                                self.state, todo, self.valid)
        new_progress.action_tensor = action_tensor
        new_progress.recent = copy.deepcopy(self.recent)
        return new_progress

    @property
    def stage_num(self):
        """

        :return:
        """
        return len(self.op_schedule_info.schedule_obj.stages)

    @property
    def reduce_stage_index(self):
        """

        :return:
        """
        # 获取reduce_index信息
        reduce_stage_index = -1
        for tmp_idx in range(self.stage_num):
            if self.op_schedule_info.feature_tensor[tmp_idx][AXIS_CNT] > 0:
                reduce_stage_index = tmp_idx
                break
        return reduce_stage_index

    @staticmethod
    def _transpose_choose_axis_apply(next_progress: object, stage_info: dict,
                                     stage_index: int, start_factor_index: int) -> NoReturn:
        """
        transpose choose axis
        :param next_progress:
        :param stage_info:
        :param stage_index:
        :param start_factor_index:
        :return:
        """
        # for recording chosen_axes info
        stage_info.setdefault('chosen_axes', [])
        stage_info["chosen_axes"] = next_progress.todo.choose_axis_and_split_map.get("chosen_axes")
        chosen_axes_index = sorted(set(next_progress.todo.choose_axis_and_split_map.get("chosen_axes")))
        nonzero_axis = next_progress.get_nonzero_axes(stage_index)
        nonzero_axis_index = list(range(len(nonzero_axis)))
        need_set_axis_index = [x for x in nonzero_axis_index if x not in chosen_axes_index]

        # if choose same axis, use last factor
        if len(chosen_axes_index) == 1:
            next_progress.action_tensor[stage_index][chosen_axes_index[0]] = 1

        for index in need_set_axis_index:
            factor_index = start_factor_index + index
            if index < chosen_axes_index[-1]:
                next_progress.action_tensor[stage_index][factor_index] = 1
            else:
                next_progress.action_tensor[stage_index][factor_index] = nonzero_axis[index]

    @staticmethod
    def _normal_choose_axis_apply(next_progress: object, start_factor_index: int, split_broadcast_axis: List,
                                  no_split_broadcast_axis: List, action_value: int) -> NoReturn:
        """
        normal choose axis
        :param start_factor_index:
        :param split_broadcast_axis:
        :param no_split_broadcast_axis:
        :param action_value:
        :return:
        """
        stage_index = next_progress.todo.stage_index
        nonzero_axis = next_progress.get_nonzero_axes(stage_index)
        for idx, axis in enumerate(nonzero_axis):
            # 被选中需要切的轴，切分因子置为0，没有被选中切的轴，
            # 切分因子置为轴长本身
            factor_index = start_factor_index + idx
            set_axis_flag = (idx > action_value and idx not in split_broadcast_axis) \
                            or idx in no_split_broadcast_axis
            if set_axis_flag:
                next_progress.action_tensor[stage_index][factor_index] = axis
            else:
                # 否则初始设置为1
                next_progress.action_tensor[stage_index][factor_index] = 1

    @staticmethod
    def _get_broadcast_axis_reorder(ordered_broadcast_groups: list) -> list:
        broadcast_axis_reorder = []
        for broadcast_group in ordered_broadcast_groups:
            for broadcast_axis in broadcast_group:
                if broadcast_axis not in broadcast_axis_reorder:
                    broadcast_axis_reorder.append(broadcast_axis)
        return broadcast_axis_reorder

    def add_todo_to_state(self, state):
        """

        :param state:
        :return:
        """
        if self.todo is None:
            todo_vector = np.array([-ITERVAR_TODO_LEN] * ITERVAR_TODO_LEN,
                                   np.float32)
        else:
            todo_vector = self.todo.vector()

        splited_parts = np.split(state, [ITERVAR_ATTR_FEATURE_LEN], axis=1)
        state_attr_feature, state_buffer = splited_parts[0], splited_parts[1]
        todo_array = np.tile(todo_vector, (len(state_attr_feature), 1))
        state = np.concatenate((state_attr_feature, todo_array, state_buffer),
                               axis=1)
        return state

    def add_shape_to_state(self, state):
        """

        :param state:
        :return:
        """
        shape = []
        for shape_info in self.op_schedule_info.input_info_list:
            shape += shape_info.shape + [0
                                         ] * (AXIS_CNT - len(shape_info.shape))
        if len(shape) > ITERVAR_SHAPE_LEN:
            shape = shape[:ITERVAR_SHAPE_LEN]
        else:
            shape = shape + [0] * (ITERVAR_SHAPE_LEN - len(shape))
        splited_parts = np.split(state, [ITERVAR_ATTR_FEATURE_LEN], axis=1)
        state_attr_feature, state_buffer = splited_parts[0], splited_parts[1]
        shape_array = np.tile(shape, (len(state_attr_feature), 1))
        state = np.concatenate((state_attr_feature, shape_array, state_buffer),
                               axis=1)
        return state

    def action_tensor_to_state(self):
        """

        :return:
        """
        log.dbg("RL search info: action tensor is %s.", self.action_tensor)
        return np.ones([ITERVAR_NUM, FEATURE_LEN], dtype="float64")

    def all_legal_actions(self):
        """
        非法的Action有以下这些：
        1，可at的轴只有N个，却要at到大于或等于N的轴上
        2，Factor的取值再加上目标Action就大于Axis了
        3，reorder_0的后3个，reorder_1的后2个，reorder_2的最后1个
        """
        if self.todo is None:
            return np.array([0] * SEARCH_N, dtype=int)
        try:
            action_mask = masker.proc(self)
        except (RuntimeError, ValueError, OSError) as exception:
            log.warn("RL exception occur: Failed to get legal actions, raise exception: %s", repr(exception))
        finally:
            pass

        return action_mask

    def apply_for_at_stage(self, next_progress, action_value):
        """

        :param next_progress:
        :param action_value:
        :return:
        """
        cur_stage_index = self.todo.stage_index
        cur_stage_info = copy.deepcopy(
            self.op_schedule_info.stages_info[cur_stage_index])
        consumer_id = self.todo.sub_action_index
        at_info = cur_stage_info.get('at_info')
        consumer = at_info.consumers[consumer_id]
        consumer.set_sampled_target(consumer.at_candidates[action_value])
        new_stages_info = []
        for stage_info in self.op_schedule_info.stages_info:
            no_need_type = {
                'CacheWrite', 'CacheRead', 'reduce_atomic_rfactor',
                'reduce_atomic_write'
            }
            if not no_need_type & set(stage_info.get('type', [])):
                if stage_info['name'] == cur_stage_info['name']:
                    new_stages_info.append(cur_stage_info)
                else:
                    new_stages_info.append(copy.deepcopy(stage_info))
        next_progress.op_schedule_info.stages_info = new_stages_info

    def apply_for_choose_axis_type(self, next_progress, action_value):
        """

        :param next_progress:
        :param action_value:
        :return:
        """
        log.dbg("stage_index: [%s, %s]",
                self.todo.stage_index, next_progress.todo.stage_index)
        stages_info = next_progress.op_schedule_info.stages_info
        for stage_index, stage_info in enumerate(stages_info):
            if 'reduce_atomic' in stage_info.get('type', []):
                if action_value == 1:
                    stages_info[stage_index]['split_axis_type'] = 'axis'
                    stages_info[stage_index]['type'].remove('reduce')
                else:
                    stages_info[stage_index]['split_axis_type'] = \
                        'reduce_axis'
        stage_index = next_progress.todo.stage_index
        log.dbg("split_axis_type: %s",
                stages_info[stage_index]['split_axis_type'])

    def apply_for_choose_axis(self,  # pylint: disable=R0914
                              next_progress,
                              action_value):
        """

        :param next_progress:
        :param action_value:
        :return:
        """
        log.dbg("stage_index: [%s, %s]",
                self.todo.stage_index, next_progress.todo.stage_index)
        # broadcast_axis_reorder中切分轴之后的轴不切，之前的轴切factor 1
        stage_index = next_progress.todo.stage_index
        stage_info = next_progress.op_schedule_info.stages_info[stage_index]
        broadcast_axis_reorder = []
        broadcast_groups = stage_info.get('broadcast_groups', [])
        if len(broadcast_groups) > 1:
            broadcast_axis_reorder = stage_info.get('broadcast_axis_reorder',
                                                    [])
        start_index = 0
        if action_value in broadcast_axis_reorder:
            start_index = broadcast_axis_reorder.index(action_value) + 1
        split_broadcast_axis = broadcast_axis_reorder[:start_index]
        no_split_broadcast_axis = broadcast_axis_reorder[start_index:]

        start_factor_index = ActionTensorCfg.split_factor_s + \
                             self.todo.cache_layer * AXIS_CNT
        next_progress.todo.choose_axis_and_split_map.get("chosen_axes").append(action_value)

        if next_progress.todo.choose_axis_and_split_map.get("times") == 1:
            if next_progress.op_schedule_info.option.get('op_type') in TIK_TO_DSL_OP_LIST.keys():
                # for transpose choose axis
                Progress._transpose_choose_axis_apply(next_progress, stage_info, stage_index, start_factor_index)
            else:
                # for normal choose axis
                Progress._normal_choose_axis_apply(next_progress, start_factor_index, split_broadcast_axis,
                                                   no_split_broadcast_axis, action_value)
        # 记录一下用于分析定位
        if broadcast_axis_reorder:
            stage_info['choose_axis'] = action_value
            sample_action = next_progress.action_tensor[stage_index][
                start_factor_index:start_factor_index + AXIS_CNT]
            sample_action = sample_action.tolist()
            stage_info['sample_action'] = sample_action
            log.dbg('[%s] %s %s --> %s', stage_index, broadcast_axis_reorder,
                    action_value, sample_action)

    def apply_for_at_axis(self, next_progress, action_value):
        """

        :param next_progress:
        :param action_value:
        :return:
        """
        log.dbg("stage_index: [%s, %s]",
                self.todo.stage_index, next_progress.todo.stage_index)
        stage_index = next_progress.todo.stage_index
        op_schedule_info = next_progress.op_schedule_info
        if not need_search_at_axis(stage_index, op_schedule_info):
            return

        next_progress.action_tensor[stage_index][ActionTensorCfg.at_s] = \
            action_value

        # broadcast groups的at axis采样需要特殊处理
        stage_info = next_progress.op_schedule_info.stages_info[stage_index]
        if stage_info.get('followed_bc_info', []):
            # 从-1开始，但是采样是从0开始的，所以要-1
            next_progress.action_tensor[stage_index][ActionTensorCfg.at_s] = \
                action_value - 1
            log.dbg('[%s at %s]at_axis_list: %s, choose_axis: %s, at_axis: %s',
                    stage_index, op_schedule_info.at_dict[stage_index],
                    op_schedule_info.stages_info[stage_index].get(
                        'at_axis_list'), op_schedule_info.stages_info[
                            op_schedule_info.at_dict[stage_index]].get(
                                'choose_axis'), action_value - 1)

    def apply_for_reorder(self,  # pylint: disable=R0912,R0914
                          next_progress,
                          action_value):
        """

        :param next_progress:
        :param action_value:
        :return:
        """
        log.dbg("stage_index: [%s, %s]",
                self.todo.stage_index, next_progress.todo.stage_index)
        stage_index = next_progress.todo.stage_index
        stage_info = next_progress.op_schedule_info.stages_info[stage_index]
        broadcast_groups = stage_info.get('broadcast_groups')

        # broadcast_groups的reorder采样,broadcast_groups的排列组合
        if broadcast_groups:
            stage_info.setdefault('broadcast_groups_reorder', 0)
            sub_action_index = next_progress.todo.sub_action_index
            delta = pow(AXIS_CNT, sub_action_index) * action_value
            stage_info['broadcast_groups_reorder'] += delta
            # 采样完成
            if sub_action_index == 0:
                # 1、获取reorder顺序
                reorder_num = stage_info['broadcast_groups_reorder']
                broadcast_groups_perms = sorted(permutations(broadcast_groups))
                # mask rule没有每层处理
                if reorder_num >= len(broadcast_groups_perms):
                    reorder_num = stage_info['broadcast_groups_reorder'] = 0
                ordered_broadcast_groups = broadcast_groups_perms[reorder_num]
                broadcast_axis_reorder = Progress._get_broadcast_axis_reorder(ordered_broadcast_groups)
                stage_info['broadcast_axis_reorder'] = \
                    broadcast_axis_reorder
                log.dbg('[%s] broadcast_groups_reorder: %s,'
                        'broadcast_axis_reorder: %s', stage_index,
                        stage_info['broadcast_groups_reorder'],
                        stage_info['broadcast_axis_reorder'])
                stage_info['sample_action'] = None

        # gemm的reorder采样，mn or nm
        elif next_progress.c_op in comm.MAD_OP_ID_LIST:
            cache_layer = next_progress.todo.cache_layer
            reorder_index = ActionTensorCfg.reorder_s + cache_layer * AXIS_CNT
            reorder_list = list(range(AXIS_CNT))
            if action_value:
                reorder_list[0], reorder_list[1] = reorder_list[1], \
                                                   reorder_list[0]
            next_progress.action_tensor[stage_index][
            reorder_index:reorder_index + AXIS_CNT] = reorder_list

        # transpose reorder: first input reorder, second output reorder
        # this reorder is different from broadcast_reorder, it has an effect on input/output tensor
        elif next_progress.op_schedule_info.option.get('op_type') in TIK_TO_DSL_OP_LIST.keys() and \
                "trs_reorder_times" in \
                TIK_TO_DSL_OP_LIST.get(next_progress.op_schedule_info.option.get('op_type')).keys():
            stage_info.setdefault('trs_reorder_num', 0)
            sub_action_index = next_progress.todo.sub_action_index
            stage_info['trs_reorder_num'] += pow(AXIS_CNT, sub_action_index) * action_value

            # 采样完成
            if sub_action_index == 0:
                # 获取reorder顺序
                reorder_num = stage_info['trs_reorder_num']
                nonzero_axes = next_progress.get_nonzero_axes(stage_index)
                # get axes_index
                axes_index = list(range(len(nonzero_axes)))
                axes_perms = sorted(permutations(axes_index))
                if reorder_num >= len(axes_perms):
                    reorder_num = 0
                ordered_axes = axes_perms[reorder_num]
                reorder_index = ActionTensorCfg.trs_reorder_s + (next_progress.todo.reorder_times - 1) * AXIS_CNT
                next_progress.action_tensor[stage_index][reorder_index:reorder_index + len(nonzero_axes)] = ordered_axes

                # clear trs_reorder_num for next reorder
                next_progress.op_schedule_info.stages_info[stage_index]["trs_reorder_num"] = 0

    def apply_for_split(self, next_progress, action_value):
        """

        :param next_progress:
        :param action_value:
        :return:
        """
        log.dbg("stage_index: [%s, %s]",
                self.todo.stage_index, next_progress.todo.stage_index)
        stage_index = next_progress.todo.stage_index
        start_factor_index = ActionTensorCfg.split_factor_s + \
                             self.todo.cache_layer * AXIS_CNT
        factor_index = start_factor_index + next_progress.todo.axis_index
        sub_action_index = next_progress.todo.sub_action_index
        factor_delta = pow(AXIS_CNT, sub_action_index) * action_value
        next_progress.action_tensor[stage_index][factor_index] += factor_delta

        # gemm不切mn，将L0对应的切分因子置为MN的切分因子
        need_split_l0 = next_progress.op_schedule_info.stages_info[
            stage_index].get('need_split_l0')
        if next_progress.c_op in comm.MAD_OP_ID_LIST and is_mad_leaf(
                next_progress) and not need_split_l0:
            next_progress.action_tensor[stage_index][factor_index +
                                                     AXIS_CNT] += factor_delta

        # 记录一下，用于分析定位
        stage_info = next_progress.op_schedule_info.stages_info[stage_index]
        broadcast_axis_reorder = stage_info.get('broadcast_axis_reorder')
        if broadcast_axis_reorder:
            sample_action = next_progress.action_tensor[stage_index][
                start_factor_index:start_factor_index + AXIS_CNT]
            sample_action = sample_action.tolist()
            stage_info['sample_action'] = sample_action
            log.dbg('[%s] %s %s --> %s', stage_index, broadcast_axis_reorder,
                    action_value, sample_action)

    def apply_for_need_split_l0(self, next_progress, action_value):
        '''

        :param next_progress:
        :param action_value:
        :return:
        '''

        stage_index = self.todo.stage_index
        last_stage_index = self.stage_num - 1

        if next_progress.c_op not in comm.MAD_OP_ID_LIST or \
                stage_index != last_stage_index:
            return

        need_split_l0 = bool(action_value)
        stage_info = next_progress.op_schedule_info.stages_info[stage_index]
        stage_info['need_split_l0'] = need_split_l0

        # gemm切mn则不切大K，需要把切分因子设置为轴长
        if need_split_l0:
            stages_info = next_progress.op_schedule_info.stages_info
            for stage_idx, stage_info in enumerate(stages_info):
                if stage_info.get('tag') != 'matmul':
                    continue
                start_factor_index = ActionTensorCfg.split_factor_s
                nonzero_axes = next_progress.get_nonzero_axes(stage_idx)
                for axis_idx, axis_len in enumerate(nonzero_axes):
                    next_progress.action_tensor[stage_idx][
                        start_factor_index + axis_idx] = axis_len
                log.dbg('[%s]K action_tensor: %s', stage_idx,
                        next_progress.action_tensor[stage_idx][
                            start_factor_index:start_factor_index + AXIS_CNT])
                break

    def apply_for_emit_insn(self: object, next_progress: object, action_value: int) -> NoReturn:
        """
        emit insn action value apply
        :param next_progress: Progress
        :param action_value: action value
        :return: no return
        """
        log.dbg("stage_index: [%s, %s]",
                self.todo.stage_index, next_progress.todo.stage_index)
        stages_info = next_progress.op_schedule_info.stages_info
        for stage_idx, stage_info in enumerate(stages_info):
            if stage_info.get('tag') in ["transpose"]:
                next_progress.action_tensor[stage_idx][ActionTensorCfg.emit_insn_s] = action_value


    def apply_action_value(self,  # pylint: disable=R0912,R0914,R0915
                           action_value):
        """
        应用Action，更新相应的todo, action_tensor, n
        :param action_value:
        :return:
        """
        next_progress = copy.deepcopy(self)
        # n直接+1
        next_progress.step_n += 1

        # 注意：这些apply_for_xxxx函数的self和next_progress不要随便替换
        # 比如，reduce_atomic场景下，self.todo.stage_index和
        # next_progresstodo.stage_index是不同的值，替换会有问题

        # 如果是采at_stage，则先把它记住
        if self.todo.action_type == ActionType.at_stage:
            self.apply_for_at_stage(next_progress, action_value)

        # 记录reduce_atomic的切分轴类型
        elif self.todo.action_type == ActionType.choose_axis_type:
            self.apply_for_choose_axis_type(next_progress, action_value)

        # action_tensor更新
        elif self.todo.action_type == ActionType.choose_axis:
            self.apply_for_choose_axis(next_progress, action_value)

        elif self.todo.action_type == ActionType.split:
            self.apply_for_split(next_progress, action_value)

        elif self.todo.action_type == ActionType.at_axis:
            self.apply_for_at_axis(next_progress, action_value)

        elif self.todo.action_type == ActionType.reorder:
            self.apply_for_reorder(next_progress, action_value)

        elif self.todo.action_type == ActionType.need_split_l0:
            self.apply_for_need_split_l0(next_progress, action_value)

        elif self.todo.action_type == ActionType.emit_insn:
            self.apply_for_emit_insn(next_progress, action_value)

        next_progress.todo.action_value = action_value
        log.dbg('[%s] todo: %s', self.step_n, next_progress.todo)

        # Review：查询所有魔鬼数字
        # 当前没有cache机制，暂时不实时跑ticks
        direct(next_progress)

        next_progress.append_recent(self.todo, action_value)

        log.dbg("%s apply action after,action_type: %s, %s", next_progress, self.todo.action_type,
                next_progress.action_tensor)
        stages = next_progress.op_schedule_info.schedule_obj.stages
        log.dbg('next progress tensor to state. stages: %d, all_iter_vars: %s',
                len(stages), stages[-1].all_iter_vars)
        next_progress.state = next_progress.action_tensor_to_state()

        return next_progress

    def append_recent(self, todo_obj, action_value):
        """

        :param todo_obj:
        :param action_value:
        """
        todo_tmp = copy.deepcopy(todo_obj)
        todo_tmp.action_value = action_value
        self.recent += tuple([str(todo_tmp)])

    def is_search_over(self):
        """

        :return:
        """
        return self.todo is None

    def get_nonzero_axes(self,
                         stage_index,
                         split_axis_type=None):
        """
        :param split_axis_type:
        :param stage_index:
        :return: 非0的轴列表
        """
        stages_info = self.op_schedule_info.stages_info
        stage_types = stages_info[stage_index].get('type', [])
        if 'reduce_atomic' in stage_types:
            nonzero_axes = self._get_nonzero_axes_atomic(stage_index,
                                                         split_axis_type)
        else:
            nonzero_axes = self._get_nonzero_axes_normal(stage_index)
        return nonzero_axes

    def _get_nonzero_axes_atomic(self,  # pylint: disable=R0912
                                 stage_index, split_axis_type):
        """

        :param stage_index:
        :param split_axis_type:
        :return:
        """
        stages_info = self.op_schedule_info.stages_info
        stage_info = stages_info[stage_index]
        if not split_axis_type:
            split_axis_type = stage_info.get('split_axis_type',
                                             'reduce_axis')
        shape_before_reduce = stage_info.get("shape_before_reduce", [])
        reduce_axis_indexs = stage_info.get("reduce_axis_indexs", [])
        is_keepdims = stage_info.get('is_keepdims', False)
        nonzero_axes = []
        if split_axis_type == 'reduce_axis':
            for i, dim in enumerate(shape_before_reduce):
                if i in reduce_axis_indexs:
                    nonzero_axes.append(dim)
        else:
            for i, dim in enumerate(shape_before_reduce):
                if i not in reduce_axis_indexs:
                    nonzero_axes.append(dim)
                elif is_keepdims:
                    nonzero_axes.append(1)
        return nonzero_axes

    def _get_nonzero_axes_normal(self, stage_index):
        """

        :param stage_index:
        :return:
        """
        # 普通轴是前8根，Reduce是第二个8根
        start_pos = 0
        if self.op_schedule_info.feature_tensor[stage_index][AXIS_CNT] > 0:
            start_pos = AXIS_CNT
        stage_fea = self.op_schedule_info.feature_tensor[stage_index]
        axis_list = stage_fea[start_pos:start_pos + AXIS_CNT]
        nonzero_axes = [dim for dim in axis_list if dim > 0]

        # 判断是否为规格轴
        if self.c_op == 'argmax_last-like':
            nonzero_axes = nonzero_axes[:len(nonzero_axes) - 1]
        elif self.c_op in comm.MAD_OP_ID_LIST:
            has_batch, _ = gemm_identify(self.op_schedule_info)
            if start_pos == 0 and has_batch:
                nonzero_axes = nonzero_axes[:len(nonzero_axes) // 2 + 1]
            else:
                nonzero_axes = nonzero_axes[:len(nonzero_axes) // 2]
        return nonzero_axes


if __name__ == '__main__':
    pass
