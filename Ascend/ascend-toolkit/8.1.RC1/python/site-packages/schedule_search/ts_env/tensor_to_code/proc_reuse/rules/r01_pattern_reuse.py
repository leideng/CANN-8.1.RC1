#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
from schedule_search import log
from schedule_search import op_cfg
from schedule_search.ts_env import env_util
from schedule_search.ts_env.env_consts import VIRTUAL_LEAF_OUT_TAG
from schedule_search.ts_env.tensor_to_code.proc_reuse.rules import pattern
from tbe import tvm


def get_mid_tensor_info(t2c_params):  # pylint: disable=R0912, R0914
    """

    :param t2c_params:
    :return:
    """
    # 所有的中间tensor列表，去掉输入和最后的输出
    mid_tensors = []
    # 所有的中间tensor的目的tensor，去掉CacheRead、CacheWrite
    mid_tensor_dst_dict = {}
    # 所有的中间tensor的CacheWritetensor
    mid_tensor_cachew_dict = {}

    visited_tensors = []
    output_stage_op = t2c_params.schedule.stages[-1].op
    output_tensors = [
        output_stage_op.output(idx)
        for idx in range(output_stage_op.num_outputs)
    ]
    log.dbg("output_tensors: %s", output_tensors)
    stage_outputs = []
    for stage in t2c_params.schedule.stages:
        outputs = []
        for idx in range(stage.op.num_outputs):
            outputs.append(stage.op.output(idx))
        stage_outputs.append(outputs)

    def _get_mid_tensor_info(cur_tensor):  # pylint: disable=R0912
        if cur_tensor in visited_tensors:
            return
        visited_tensors.append(cur_tensor)
        for input_tensor in cur_tensor.op.input_tensors:
            if isinstance(input_tensor.op, tvm.PlaceholderOp):
                continue
            if input_tensor not in mid_tensors:
                mid_tensors.append(input_tensor)
            mid_tensor_dst_dict.setdefault(input_tensor, [])
            if cur_tensor not in mid_tensor_dst_dict.get(input_tensor):
                mid_tensor_dst_dict.get(input_tensor).append(cur_tensor)
            _get_mid_tensor_info(input_tensor)

    for output_tensor in output_tensors:
        _get_mid_tensor_info(output_tensor)

    # 去掉CacheRead、CacheWrite
    real_mid_tensors = []
    for mid_tensor in mid_tensors:
        stage_index = None
        for index, stage_output in enumerate(stage_outputs):
            if mid_tensor in stage_output:
                stage_index = index
                break
        stage_info = t2c_params.stages_info[stage_index]
        if set(stage_info.get('type', [])) & {'CacheRead', 'CacheWrite'}:
            continue
        real_mid_tensors.append(mid_tensor)

    # 更新mid_tensor_dst_dict, 替换CacheWrite为原始tensor
    real_mid_tensor_dst_dict = {}
    for mid_tensor in real_mid_tensors:
        dst_tensors = mid_tensor_dst_dict.get(mid_tensor)
        real_mid_tensor_dst_dict.setdefault(mid_tensor, dst_tensors)
        for dst_tensor in dst_tensors:
            dst_stage_index = None
            for index, stage_output in enumerate(stage_outputs):
                if dst_tensor in stage_output:
                    dst_stage_index = index
                    break
            dst_stage_info = t2c_params.stages_info[dst_stage_index]
            if 'CacheWrite' in dst_stage_info.get('type', []) \
                    and len(mid_tensor_dst_dict.get(dst_tensor, [])) == 1:
                real_mid_tensor_dst_dict.get(mid_tensor).remove(dst_tensor)
                real_mid_tensor_dst_dict.get(mid_tensor).append(
                    mid_tensor_dst_dict.get(dst_tensor)[0])

    # 记录CacheWrite关系
    for src_tensor, dst_tensors in mid_tensor_dst_dict.items():
        src_stage_index = None
        for index, stage_output in enumerate(stage_outputs):
            if src_tensor in stage_output:
                src_stage_index = index
                break
        src_stage_info = t2c_params.stages_info[src_stage_index]
        if 'CacheWrite' in src_stage_info.get('type', []) \
                and len(mid_tensor_dst_dict.get(src_tensor)) == 1:
            mid_tensor_cachew_dict[mid_tensor_dst_dict.get(src_tensor)[0]] \
                = src_tensor

    return real_mid_tensors, real_mid_tensor_dst_dict, mid_tensor_cachew_dict


def pattern_identify(mid_tensors):  # pylint: disable=R0912
    """
    pattern identify
    """
    # tag_list
    tag_list = []
    former_broadcast = False
    for mid_tensor in mid_tensors:
        # 不添加连续的broadcast
        if "broadcast_" in mid_tensor.op.tag:
            if former_broadcast:
                continue
            former_broadcast = True
        else:
            former_broadcast = False
        if mid_tensor.op.tag:
            tag_list.append(mid_tensor.op.tag)

    # pattern
    for cur_pattern in pattern.PATTERN_DICT:
        if pattern.PATTERN_DICT[cur_pattern] == tag_list:
            return cur_pattern
    return pattern.PATTERN_NONE


def proc(t2c_params, reuse_dict):  # pylint: disable=R0912
    """
    规则内容：指定的算子结构添加reused_by
    """
    if t2c_params.op_schedule_info.op_name in op_cfg.RNN_OP_LIST:
        return True

    # mid_tensors
    mid_tensors, mid_tensor_dst_dict, mid_tensor_cachew_dict \
        = get_mid_tensor_info(t2c_params)

    # compute_pattern
    compute_pattern = pattern_identify(mid_tensors)
    log.dbg("compute_pattern: %s", compute_pattern)
    if compute_pattern not in pattern.BUFFER_REUSE_PATTERN:
        return True

    # common reuse
    used_dst_tensors = []
    for src_tensor in mid_tensors:
        if len(mid_tensor_dst_dict.get(src_tensor)) != 1:
            log.dbg("multi dst_tensor cannot be reused_by.")
            continue
        if src_tensor not in mid_tensor_cachew_dict:
            log.dbg("no cache_write src_tensor cannot be reused_by.")
            continue
        dst_tensor = mid_tensor_dst_dict.get(src_tensor)[0]
        if dst_tensor in used_dst_tensors:
            log.dbg("used dst_tensor cannot be reused_by.")
            continue
        if dst_tensor.op.tag == VIRTUAL_LEAF_OUT_TAG:
            log.dbg("virtual_leaf_out cannot be reused_by.")
            continue
        if src_tensor.dtype != dst_tensor.dtype:
            log.dbg("dtype not same cannot be reused_by")
            continue
        if env_util.tvm_shape_trans(src_tensor.shape) \
                != env_util.tvm_shape_trans(dst_tensor.shape):
            log.dbg("shape not same cannot be reused_by")
            continue
        src_buffer = mid_tensor_cachew_dict.get(src_tensor)
        dst_buffer = mid_tensor_cachew_dict.get(dst_tensor)
        reuse_dict[src_buffer] = dst_buffer
        used_dst_tensors.append(dst_tensor)

    return True
