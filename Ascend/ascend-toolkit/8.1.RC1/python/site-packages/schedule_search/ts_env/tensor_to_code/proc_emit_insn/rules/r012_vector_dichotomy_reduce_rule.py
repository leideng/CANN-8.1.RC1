#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
from schedule_search.ts_env.env_consts import MODE_RUNTIME
from schedule_search.ts_env.tensor_cfg import FeatureTensorCfg
from schedule_search.ts_env.tensor_to_code.proc_emit_insn.rules.comm import \
    get_emit_insn_axis
from schedule_search.ts_env.cheque_generator import get_emit_insn_cheque


VECTOR_ONE_BLOCK_UNIT = 16
VECTOR_ONE_REPEAT_UNIT = 128


def need_dichotomy_reduce(t2c_params):  # pylint: disable=R0914, R0912
    """

    :param t2c_params:
    :return:
    """
    features = t2c_params.features
    stage_index = t2c_params.stage_index
    axis_info_list = t2c_params.axis_info_list
    stage = t2c_params.schedule.stages[stage_index]
    op_intrin_key_index = t2c_params.op_schedule_info.op_intrin_key_index

    # 1、暂时只处理reduce_sum，reduce_max、reduce_min再说
    compute = op_intrin_key_index[features[stage_index][
        FeatureTensorCfg.compute_s]].op_tag
    if not compute.startswith('reduce_sum'):
        return False, None

    # 2、获取reduce轴、reduce之前的shape、reduce大小
    reduce_axis_indexs = []
    reduce_tensor = stage.op.output(0)
    source_axis_vars = reduce_tensor.op.body[0].source[0].indices
    for reduce_axis in reduce_tensor.op.reduce_axis:
        for index, source_axis_var in enumerate(source_axis_vars):
            if source_axis_var.same_as(reduce_axis.var):
                reduce_axis_indexs.append(index)
                break
    shape_before_reduce = [
        i.value
        for i in reduce_tensor.op.input_tensors[0].shape
    ]
    for axis_index, axis_len in reversed(list(enumerate(shape_before_reduce))):
        if axis_len == 1 and axis_index not in reduce_axis_indexs:
            shape_before_reduce.pop()
        else:
            break
    reduce_size = 1
    for index, shape in enumerate(shape_before_reduce):
        if index in reduce_axis_indexs:
            reduce_size *= shape
    dichotomy_reduce_times = reduce_size // VECTOR_ONE_REPEAT_UNIT

    # 4、不是指定形式的reduce不处理
    if not (len(shape_before_reduce) > 2 and
            len(reduce_axis_indexs) == 2 and
            (len(shape_before_reduce) - 1) in reduce_axis_indexs and
            (len(shape_before_reduce) - 2) not in reduce_axis_indexs and
            shape_before_reduce[-1] % VECTOR_ONE_BLOCK_UNIT == 0 and
            reduce_size % VECTOR_ONE_REPEAT_UNIT == 0 and
            dichotomy_reduce_times & (dichotomy_reduce_times - 1) == 0):
        return False, None

    # 5、还有与切分相关的限制，暂时先不加
    if axis_info_list[stage_index]:
        return False, None

    dichotomy_reduce_intrin = 'vector_dichotomy_reduce'
    return True, dichotomy_reduce_intrin


def proc(t2c_params):
    """
    判断是否使用二分法指令vector_dichotomy_reduce
    """
    mode = t2c_params.mode
    stage = t2c_params.stage
    stage_index = t2c_params.stage_index
    stage_name = t2c_params.stage_name
    axis_info_list = t2c_params.axis_info_list

    proc_sch = bool(mode == MODE_RUNTIME)

    # 前面规则处理过的stage直接跳过
    if t2c_params.proc_flag_dict.get(stage_index, False):
        return True

    need_dichotomy, dichotomy_reduce_intrin = need_dichotomy_reduce(t2c_params)
    if not need_dichotomy:
        return True

    intrin = dichotomy_reduce_intrin
    axis_num, emit_insn_axis, emit_insn_axis_obj = get_emit_insn_axis(
        stage, stage_index, axis_info_list,
        t2c_params.stages_info[stage_index])
    t2c_params.code_lines.append("sch[%s].emit_insn(%s, '%s')" %
                                 (stage_name, emit_insn_axis, intrin))
    if proc_sch:
        stage.emit_insn(emit_insn_axis_obj, intrin)

    # 生成emit_insn对应的cheque
    get_emit_insn_cheque(t2c_params, stage_index, intrin,
                         (emit_insn_axis, axis_num))
    t2c_params.proc_flag_dict[stage_index] = True
    return True
