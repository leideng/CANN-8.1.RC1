#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
from io import StringIO
from contextlib import redirect_stdout
from contextlib import redirect_stderr

from schedule_search import log
from schedule_search.controller.mcts_search.features import ana_lower
from schedule_search.ts_env.cache_manager import \
    CACHE_SCHEDULE_TUPLE_REDUCE_SUM
from schedule_search.ts_env.env_consts import MODE_RUNTIME
from schedule_search.ts_env.estimator.estimate import actions_to_schedule
from schedule_search.ts_env.estimator.estimate import get_all_tensors
from schedule_search.ts_env.estimator.estimate import \
    get_tensors_by_tensors_str
from schedule_search.ts_env.tensor_cfg import FeatureTensorCfg
from schedule_search.ts_env.tensor_to_code import t2c_util
from schedule_search.ts_env.tensor_to_code.proc_emit_insn.rules.comm import \
    get_emit_insn_axis
from schedule_search.ts_env.cheque_generator import get_emit_insn_cheque
from tbe import tvm


def _need_dichotomy_add(t2c_params):  # pylint: disable=R0914, R0912
    """
    需要解析stmt，然后根据emit_insn轴下的For循环结构判断是否可以使用
    WILLDO: 这里暂时做简单的临时处理
    """
    features = t2c_params.features
    stage_index = t2c_params.stage_index
    stage_info = t2c_params.op_schedule_info.stages_info[stage_index]
    op_intrin_key_index = t2c_params.op_schedule_info.op_intrin_key_index

    # 1、只处理reduce_sum、tuple_reduce_sum的stage
    compute = op_intrin_key_index[features[stage_index][
        FeatureTensorCfg.compute_s]].op_tag
    if not compute.startswith('reduce_sum') \
            and not compute.startswith('tuple_reduce_sum'):
        return False, None

    # 2、只支持reduce nist
    reduce_type = stage_info.get('reduce_type')
    if reduce_type not in t2c_util.REDUCE_NIST_KEYWORD:
        return False, None

    # 3、获取二分法指令
    intrin = op_intrin_key_index[features[stage_index][
        FeatureTensorCfg.compute_s]].intrin
    dichotomy_add_intrin = intrin
    if compute.startswith('reduce_sum'):
        dichotomy_add_intrin = 'vector_dichotomy_add'
    if compute.startswith('tuple_reduce_sum'):
        dichotomy_add_intrin = 'vector_dichotomy_add_for_bn_reduce'

    # 4、判断是否可用vector_dichotomy_add、vector_dichotomy_add_for_bn_reduce
    # 使用vector_dichotomy_add/vector_dichotomy_add_for_bn_reduce，
    # 然后lower一下看看有没有错，有错就不用，没错就用
    # 因为reorder会把reduce nist的reduce轴全部放在普通轴前面，
    # 所以emit_insn的轴一定是切分后的reduce轴，
    # 直接复制一个schedule对象，将无法找到这根reduce轴，所以只好重新走一遍t2c
    op_schedule_info = copy.deepcopy(t2c_params.op_schedule_info)
    action_tensors = copy.deepcopy(t2c_params.actions)
    sch_info = actions_to_schedule(
        op_schedule_info,
        action_tensors,
        proc_index_end=60,
        cache_name=CACHE_SCHEDULE_TUPLE_REDUCE_SUM)
    stage = sch_info.sch.stages[stage_index]
    _, _, emit_insn_axis_obj = get_emit_insn_axis(
        stage, stage_index, op_schedule_info.axis_info_list,
        op_schedule_info.stages_info[stage_index])
    stage.emit_insn(emit_insn_axis_obj, dichotomy_add_intrin)
    all_tensors, all_tensor_names = get_all_tensors(
        sch_info.sch, op_schedule_info.special_tensor_dict)
    binds = get_tensors_by_tensors_str(op_schedule_info.tensor_list_str,
                                       all_tensors, all_tensor_names)

    from tbe.common.buildcfg import build_config
    with build_config():
        stmt = ana_lower(sch_info.sch, binds)
    try:
        f_out = StringIO()
        f_err = StringIO()
        with redirect_stderr(f_err), redirect_stdout(f_out):
            mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([], stmt))
            mod = tvm.tir.transform.InjectPrefetch()(mod)
            tvm.tir.transform.transform_extended.EmitInsn()(mod)  # pylint: disable=no-member
        return True, dichotomy_add_intrin
    except Exception as exception:  # pylint: disable=broad-except
        log.dbg('no %s\n%s', dichotomy_add_intrin, str(exception))
        return False, None


def proc(t2c_params):  # pylint: disable=R0912
    """
    1、reduce_sum需要判断是否使用二分法指令vector_dichotomy_add
    2、tuple_reduce_sum需要判断是否使用二分法指令vector_dichotomy_add_for_bn_reduce
    """
    mode = t2c_params.mode
    stage = t2c_params.stage
    stage_index = t2c_params.stage_index
    stage_name = t2c_params.stage_name
    axis_info_list = t2c_params.axis_info_list

    proc_sch = bool(mode == MODE_RUNTIME)

    # 前面规则处理过的stage直接跳过
    if t2c_params.proc_flag_dict.get(stage_index, False):
        return True

    need_dichotomy, dichotomy_add_intrin = _need_dichotomy_add(t2c_params)
    if not need_dichotomy:
        return True

    intrin = dichotomy_add_intrin
    axis_num, emit_insn_axis, emit_insn_axis_obj = get_emit_insn_axis(
        stage, stage_index, axis_info_list,
        t2c_params.stages_info[stage_index])
    t2c_params.code_lines.append("sch[%s].emit_insn(%s, '%s')" %
                                 (stage_name, emit_insn_axis, intrin))
    if proc_sch:
        stage.emit_insn(emit_insn_axis_obj, intrin)

    # 生成emit_insn对应的cheque
    get_emit_insn_cheque(t2c_params, stage_index, intrin,
                         (emit_insn_axis, axis_num))
    t2c_params.proc_flag_dict[stage_index] = True
    return True
