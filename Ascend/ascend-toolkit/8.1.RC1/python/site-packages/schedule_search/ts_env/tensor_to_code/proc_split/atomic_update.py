#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
from schedule_search import log
from schedule_search.ts_env import env_util
from schedule_search.ts_env.env_classes import TensorInfo
from schedule_search.ts_env.tensor_cfg import ActionTensorCfg
from schedule_search.ts_env.code_to_tensor import tvm_compute as \
    tvm_compute_to_tensor
from schedule_search.timer import timer


@timer()
def update_stage_index_map(t2c_params, reduce_stage_index):
    '''
    reduce_stage_index加2，调整一下stage_index_map
    '''
    for key, value in t2c_params.op_schedule_info.stage_index_map.items():
        if value == reduce_stage_index - 2:
            del t2c_params.op_schedule_info.stage_index_map[key]
            t2c_params.op_schedule_info.stage_index_map[value] \
                = reduce_stage_index
            return True
    return False


@timer()
def update_feature(t2c_params,
                   block_split_axis_index,
                   rfactor_stage_index):
    '''
    stage个数变化，需要调整feature33
    '''
    feature_tensor, attr_dict, reduce_axis_dict = \
        tvm_compute_to_tensor.proc(t2c_params.schedule,
                                   t2c_params.stages_info,
                                   t2c_params.op_schedule_info.op_name)
    if block_split_axis_index \
            not in reduce_axis_dict.get(rfactor_stage_index)["axis"]:
        reduce_axis_dict.get(rfactor_stage_index)["axis"].append(
            block_split_axis_index)
    log.dbg("reduce_axis_dict: %s", reduce_axis_dict)
    t2c_params.op_schedule_info.feature_tensor = feature_tensor
    t2c_params.op_schedule_info.attr_dict = attr_dict
    t2c_params.op_schedule_info.reduce_axis_dict = reduce_axis_dict
    t2c_params.features = feature_tensor
    log.dbg("stage_num: %s", len(t2c_params.features))
    return True


@timer()
def update_actions(t2c_params, rfactor_stage_index, reduce_write_stage_index):
    '''
    新增2个stage，插入2维空action
    '''
    action = env_util.get_init_action_tensor(1)
    action = action.tolist()[0]
    t2c_params.actions.insert(rfactor_stage_index, action)
    t2c_params.actions.insert(reduce_write_stage_index, action)


@timer()
def update_non_reduce_actions(t2c_params,
                              rfactor_stage_index,
                              block_split_info,
                              ub_split_info,
                              reduce_axis_info):
    '''
    更新rfactor之前的stage的cleaned_actions：
    ub和block tiling切过的设置为切分因子，其它设置为轴长
    '''
    # 初始化
    non_reduce_action = env_util.get_init_action_tensor(1)
    non_reduce_action = non_reduce_action.tolist()[0]

    for i in range(ActionTensorCfg.split_factor_s,
                   ActionTensorCfg.split_factor_e + 1):
        if i >= len(reduce_axis_info.shape_before_reduce):
            non_reduce_action[i] = 0
            continue
        non_reduce_action[i] = reduce_axis_info.shape_before_reduce[i]
        if i in reduce_axis_info.reduce_axis_indexs:
            reduce_axis_index = reduce_axis_info.reduce_axis_indexs.index(i)
            if reduce_axis_index == ub_split_info.axis_index:
                non_reduce_action[i] = ub_split_info.factor
            elif reduce_axis_index == block_split_info.axis_index:
                non_reduce_action[i] = block_split_info.factor
    log.dbg("non_reduce_action: %s", non_reduce_action)

    # 此处的设置是为了在emit insn的get factor中通过倒推方式获取未切分stage的factor
    non_reduce_stage_index = rfactor_stage_index - 1
    if rfactor_stage_index - 1 in t2c_params.inlined_stages:
        non_reduce_stage_index = rfactor_stage_index - 2

    t2c_params.cleaned_actions[non_reduce_stage_index] = non_reduce_action


@timer()
def update_rfactor_actions(t2c_params,  # pylint: disable=R0912
                           rfactor_stage_index,
                           block_split_info,
                           ub_split_info):
    '''
    更新rfactor stage的cleaned_actions：
    reduce_axis没切，设置为轴长，切了设置为切分因子
    '''
    # 初始化
    rfactor_action = env_util.get_init_action_tensor(1)
    rfactor_action = rfactor_action.tolist()[0]

    # 获取切分轴
    rfactor_stage = t2c_params.schedule.stages[rfactor_stage_index]
    ub_split_axis_type = t2c_params.stages_info[rfactor_stage_index + 2].get(
        'split_axis_type', 'reduce_axis')
    if ub_split_axis_type == 'axis':
        split_axis_index = None
    elif block_split_info.axis_index == ub_split_info.axis_index:
        split_axis_index = len(rfactor_stage.op.reduce_axis) - 1
    else:
        split_axis_index = \
            ub_split_info.axis_index - (block_split_info.axis_index + 1)

    for i in range(ActionTensorCfg.split_factor_s,
                   ActionTensorCfg.split_factor_e + 1):
        if split_axis_index is not None and i == split_axis_index:
            rfactor_action[i] = ub_split_info.factor
        elif i < len(rfactor_stage.op.reduce_axis):
            rfactor_action[i] \
                = rfactor_stage.op.reduce_axis[i].dom.extent.value
        else:
            rfactor_action[i] = 0

    t2c_params.cleaned_actions.insert(rfactor_stage_index, rfactor_action)


@timer()
def update_reduce_write_actions(t2c_params, write_stage_index):
    '''
    更新reduce_write stage的cleaned_actions：
    没有切分，直接设置为轴长
    '''
    # 初始化
    write_action = env_util.get_init_action_tensor(1)
    write_action = write_action.tolist()[0]

    write_stage = t2c_params.schedule.stages[write_stage_index]

    for i in range(ActionTensorCfg.split_factor_s,
                   ActionTensorCfg.split_factor_e + 1):
        if i < len(write_stage.op.reduce_axis):
            write_action[i] = write_stage.op.reduce_axis[i].dom.extent.value
        else:
            write_action[i] = 0

    t2c_params.cleaned_actions.insert(write_stage_index, write_action)


@timer()
def update_cleaned_actions(t2c_params,
                           reduce_stage_index,
                           block_split_info,
                           ub_split_info,
                           reduce_axis_info):
    '''
    1、rfactor将reduce轴转为普通轴，需要更新一下cleaned_actions
    2、新增2个stage，需要插入对应的action
    '''
    reduce_write_stage_index = reduce_stage_index - 1
    rfactor_stage_index = reduce_stage_index - 2

    update_non_reduce_actions(t2c_params,
                              rfactor_stage_index,
                              block_split_info,
                              ub_split_info,
                              reduce_axis_info)

    update_rfactor_actions(t2c_params,
                           rfactor_stage_index,
                           block_split_info,
                           ub_split_info)

    update_reduce_write_actions(t2c_params,
                                reduce_write_stage_index)

    log.dbg("stage_num: %s", len(t2c_params.cleaned_actions))


@timer()
def update_output_info_list(t2c_params,
                            reduce_tensors,
                            reduce_write_tensor_names,
                            reduce_write_tensors):
    '''
    输出tensor需要替换为cache write的tensor
    '''
    for output_info in t2c_params.op_schedule_info.output_info_list:
        log.dbg("[before update]output_info: %s", output_info)

    for reduce_index, reduce_tensor in enumerate(reduce_tensors):
        reduce_tensor_name = reduce_tensor.op.name
        if len(reduce_tensors) > 1:
            reduce_tensor_name += "_v%d" % reduce_index

        reduce_output_index = -1
        for index, output_info in enumerate(
                t2c_params.op_schedule_info.output_info_list):
            if output_info.name == reduce_tensor_name:
                reduce_output_index = index
                break

        # output_info_list
        tensor_info = TensorInfo(reduce_write_tensor_names[reduce_index],
                                 reduce_write_tensors[reduce_index].shape,
                                 reduce_write_tensors[reduce_index].dtype)
        t2c_params.op_schedule_info.output_info_list[reduce_output_index] \
            = tensor_info

    for output_info in t2c_params.op_schedule_info.output_info_list:
        log.dbg("[after update]output_info: %s", output_info)


@timer()
def update_tensor_list_str(t2c_params,
                           reduce_tensors,
                           reduce_write_tensor_names):
    '''
    输出tensor需要替换为cache write的tensor
    '''
    log.dbg("[before update]tensor_list_str: %s",
            t2c_params.op_schedule_info.tensor_list_str)

    for reduce_index, reduce_tensor in enumerate(reduce_tensors):
        reduce_tensor_name = reduce_tensor.op.name
        if len(reduce_tensors) > 1:
            reduce_tensor_name += "_v%d" % reduce_index

        # tensor_list_str
        if reduce_write_tensor_names[reduce_index] not in \
                t2c_params.op_schedule_info.tensor_list_str:
            t2c_params.op_schedule_info.tensor_list_str = \
                t2c_params.op_schedule_info.tensor_list_str.replace(
                    reduce_tensor_name,
                    reduce_write_tensor_names[reduce_index])

    log.dbg("[after update]tensor_list_str: %s",
            t2c_params.op_schedule_info.tensor_list_str)


@timer()
def update_real_out_tensor_str(t2c_params,
                               reduce_tensors,
                               reduce_write_tensor_names):
    '''
    输出tensor需要替换为cache write的tensor
    '''
    log.dbg("[before update]real_out_tensor_str: %s",
            t2c_params.op_schedule_info.real_out_tensor_str)

    for reduce_index, reduce_tensor in enumerate(reduce_tensors):
        reduce_tensor_name = reduce_tensor.op.name
        if len(reduce_tensors) > 1:
            reduce_tensor_name += "_v%d" % reduce_index
        # real_out_tensor_str
        if reduce_write_tensor_names[reduce_index] not in \
                t2c_params.op_schedule_info.real_out_tensor_str:
            t2c_params.op_schedule_info.real_out_tensor_str = \
                t2c_params.op_schedule_info.real_out_tensor_str.replace(
                    reduce_tensor_name,
                    reduce_write_tensor_names[reduce_index])

    log.dbg("[after update]real_out_tensor_str: %s",
            t2c_params.op_schedule_info.real_out_tensor_str)


@timer()
def update_special_tensor_dict(t2c_params,
                               reduce_write_tensor_names,
                               reduce_write_tensors):
    '''
    输出tensor需要替换为cache write的tensor
    '''
    for reduce_write_tensor_name, reduce_write_tensor in zip(
            reduce_write_tensor_names, reduce_write_tensors):
        t2c_params.op_schedule_info.special_tensor_dict[
            reduce_write_tensor_name] = reduce_write_tensor
    log.dbg("special_tensor_dict: %s",
            t2c_params.op_schedule_info.special_tensor_dict)


@timer()
def update_output(t2c_params,
                  reduce_tensors,
                  reduce_write_tensor_names,
                  reduce_write_tensors):
    '''
    输出tensor需要替换为cache write的tensor
    '''
    update_output_info_list(t2c_params,
                            reduce_tensors,
                            reduce_write_tensor_names,
                            reduce_write_tensors)

    update_tensor_list_str(t2c_params,
                           reduce_tensors,
                           reduce_write_tensor_names)

    update_real_out_tensor_str(t2c_params,
                               reduce_tensors,
                               reduce_write_tensor_names)

    update_special_tensor_dict(t2c_params,
                               reduce_write_tensor_names,
                               reduce_write_tensors)


@timer()
def update_reduce_atomic_dict(t2c_params, # pylint: disable=R0913
                              code_lines,
                              reduce_stage_index,
                              stage_axis_infos,
                              reduce_axis_info,
                              block_split_info,
                              ub_split_info):
    '''
    将一些变量存储到字典中，后面不用重复获取
    '''
    ub_outer_axis_info = None
    ub_inner_axis_info = None
    for axis_info in stage_axis_infos[0]:
        if axis_info.attr == 'o':
            ub_outer_axis_info = axis_info
        if axis_info.attr == 'i':
            ub_inner_axis_info = axis_info

    reduce_atomic_dict = {
        "code_lines": code_lines,
        "cheque_list": t2c_params.cheque_list,
        "rfactor_stage_index": reduce_stage_index - 2,
        "reduce_write_stage_index": reduce_stage_index - 1,
        "stage_axis_infos": stage_axis_infos,
        "reduce_op_tag": t2c_params.stages_info[reduce_stage_index - 2].get(
            'op_tag'),
        "reduce_dtype": t2c_params.schedule.outputs[0].output(0).dtype,
        "shape_before_reduce": reduce_axis_info.shape_before_reduce,
        "reduce_axis_indexs": reduce_axis_info.reduce_axis_indexs,
        "block_split_axis_index": block_split_info.axis_index,
        "block_split_factor": block_split_info.factor,
        "ub_split_axis_type": t2c_params.stages_info[reduce_stage_index].get(
            'split_axis_type', 'reduce_axis'),
        "ub_split_axis_index": ub_split_info.axis_index,
        "ub_split_factor": ub_split_info.factor,
        "ub_outer_axis_info": ub_outer_axis_info,
        "ub_inner_axis_info": ub_inner_axis_info
    }

    t2c_params.op_schedule_info.reduce_atomic_dict.update(reduce_atomic_dict)


@timer()
def do_update(t2c_params, # pylint: disable=R0913
              code_lines,
              reduce_stage_index,
              block_split_info,
              ub_split_info,
              reduce_info,
              reduce_write_tensor_names,
              reduce_write_tensors,
              stage_axis_infos):
    '''
    新增两个stage，需要更新一些东西
    '''
    reduce_write_stage_index = reduce_stage_index - 1
    rfactor_stage_index = reduce_stage_index - 2

    update_stage_index_map(t2c_params,
                           reduce_stage_index)

    update_feature(t2c_params,
                   block_split_info.axis_index,
                   rfactor_stage_index)

    update_actions(t2c_params,
                   rfactor_stage_index,
                   reduce_write_stage_index)

    update_cleaned_actions(t2c_params,
                           reduce_stage_index,
                           block_split_info,
                           ub_split_info,
                           reduce_info)

    update_output(t2c_params,
                  reduce_info.reduce_tensors,
                  reduce_write_tensor_names,
                  reduce_write_tensors)

    update_reduce_atomic_dict(t2c_params,
                              code_lines,
                              reduce_stage_index,
                              stage_axis_infos,
                              reduce_info,
                              block_split_info,
                              ub_split_info)

    t2c_params.op_schedule_info.update_dependency_dict()
