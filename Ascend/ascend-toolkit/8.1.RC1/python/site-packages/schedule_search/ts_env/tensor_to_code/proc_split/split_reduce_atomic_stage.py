#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
from schedule_search import log
from schedule_search import util
from schedule_search.ts_env.tensor_to_code.atomic_comm import \
    gen_reduce_info
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_cache_write \
    import do_gm_cache_write
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_do_tiling import \
    do_block_tiling
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_do_tiling import \
    do_ub_tiling
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_get_tiling \
    import get_tiling
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_rfactor import \
    do_rfactor
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_update import \
    do_update
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_axis_info import \
    get_rfactor_stage_axis_info
from schedule_search.ts_env.tensor_to_code.proc_split.atomic_axis_info import \
    get_write_stage_axis_info
from schedule_search.ts_env.cheque_generator import get_axis_cheque
from schedule_search.timer import timer


@timer()
def repeat_proc(t2c_params, stage_index):
    '''
    不支持重复执行
    '''
    log.warn("no reduce atomic again")
    stages_info = t2c_params.stages_info
    reduce_atomic_dict = t2c_params.op_schedule_info.reduce_atomic_dict
    stage_axis_infos = reduce_atomic_dict["stage_axis_infos"]
    code_lines = reduce_atomic_dict["code_lines"]
    if 'reduce_atomic_rfactor' in stages_info[stage_index].get('type', []):
        return [], stage_axis_infos[0], None
    if 'reduce_atomic_write' in stages_info[stage_index].get('type', []):
        return [], stage_axis_infos[1], None
    if 'reduce_atomic' in stages_info[stage_index].get('type', []):
        return code_lines, None, None

    return [], None, None


@timer()
def get_reduce_info(sch):
    '''
    获取reduce信息：
    reduce_tensors、shape_before_reduce、reduce_axis_indexs
    默认只有最后一个stage是reduce才会进行reduce_atomic
    '''
    # 支持tuple_reduce_sum
    reduce_tensors = []
    for index in range(sch.outputs[0].num_outputs):
        reduce_tensors.append(sch.outputs[0].output(index))
    input_tensor_shape = reduce_tensors[0].op.input_tensors[0].shape
    shape_before_reduce = [i.value for i in input_tensor_shape]
    reduce_axis_indexs = util.get_reduce_axis_index(reduce_tensors[0].op)
    reduce_info = gen_reduce_info(reduce_tensors=reduce_tensors,
                                  shape_before_reduce=shape_before_reduce,
                                  reduce_axis_indexs=reduce_axis_indexs)
    return reduce_info


@timer()
def split_reduce_atomic_stage(t2c_params,  # pylint: disable=R0914
                              stage_index):
    """
    进行reduce atomic切分
    """
    # 避免重复执行
    if t2c_params.op_schedule_info.reduce_atomic_dict:
        return repeat_proc(t2c_params, stage_index)

    # bind reduce轴时，只切最终的reduce输出
    if 'reduce_atomic' not in \
            t2c_params.stages_info[stage_index].get('type', []):
        log.dbg("not reduce_atomic stage")
        return [], None, None

    # 进行rfactor、cache write会新增stage，stage_index会变化
    reduce_stage_index = stage_index

    # split的stage的所有普通轴和reduce轴生成对应的cheque
    get_axis_cheque(t2c_params, reduce_stage_index)

    code_lines = []

    # 1、获取reduce信息
    reduce_info = get_reduce_info(t2c_params.schedule)

    # 2、获取切分轴和切分因子
    ub_split_info, block_split_info = get_tiling(t2c_params,
                                                 code_lines,
                                                 reduce_stage_index,
                                                 reduce_info)

    # 3、进行多核切分
    block_split_info = do_block_tiling(t2c_params,
                                       code_lines,
                                       reduce_stage_index,
                                       block_split_info)

    # 4、进行rfactor
    do_rfactor(t2c_params,
               code_lines,
               reduce_stage_index,
               block_split_info,
               reduce_info.reduce_tensors)
    # rfactor后新增rfactor_stage，stage_index变化
    rfactor_stage_index = reduce_stage_index
    reduce_stage_index = rfactor_stage_index + 1

    # 5、进行ub切分
    ub_split_info = do_ub_tiling(t2c_params,
                                 code_lines,
                                 rfactor_stage_index,
                                 block_split_info.axis_index,
                                 ub_split_info)

    # 6、获取rfactor_stage的axis_info，该stage需要进行reorder
    rfactor_stage_axis_info = get_rfactor_stage_axis_info(
        t2c_params,
        code_lines,
        rfactor_stage_index,
        block_split_info.axis_index,
        ub_split_info,
        reduce_info)

    # 7、reduce stage进行cache_write，需要在rfactor之后进行cache write
    reduce_write_tensor_names, reduce_write_tensors =\
        do_gm_cache_write(t2c_params,
                          code_lines,
                          reduce_stage_index,
                          reduce_info.reduce_tensors)
    # cache_write后新增cache_write stage，stage_index变化
    reduce_write_stage_index = reduce_stage_index
    reduce_stage_index = reduce_write_stage_index + 1

    # 8、获取cache_write stage的axis_info，该stage需要进行reorder
    reduce_write_stage_axis_info = \
        get_write_stage_axis_info(t2c_params,
                                  code_lines,
                                  reduce_write_stage_index)

    # 最终3个stage的axis_info
    stage_axis_infos = [rfactor_stage_axis_info,
                        reduce_write_stage_axis_info,
                        None]

    # 9、新增两个stage，需要更新一些东西
    do_update(t2c_params,
              code_lines,
              reduce_stage_index,
              block_split_info,
              ub_split_info,
              reduce_info,
              reduce_write_tensor_names,
              reduce_write_tensors,
              stage_axis_infos)

    log.dbg("atomic split end.")

    return code_lines, stage_axis_infos, None
