#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
from typing import NoReturn

from schedule_search import log
from schedule_search.op_cfg import TIK_TO_DSL_OP_LIST
from schedule_search.timer import timer
from schedule_search.cce_intrin_map import OP_INTRIN_KEY_TAG
from schedule_search.ts_env.tensor_to_code.t2c_util import T2cParams
from schedule_search.util import get_dtype_size
from schedule_search.ts_env.tensor_cfg import ActionTensorCfg
from schedule_search.ts_env.tensor_cfg import AXIS_CNT
from schedule_search.ts_env.code_to_tensor.at_info import AtInfo
from schedule_search.ts_env.code_to_tensor.consumer_info import ConsumerInfo
from schedule_search.ts_env.cheque_generator import get_reorder_cheque
from schedule_search.ts_env.cheque_generator import get_cache_read_cheque
from schedule_search.ts_env.cheque_generator import get_cache_write_cheque
from schedule_search.ts_env.cheque_generator import get_cache_write_cheque_spec
from schedule_search.ts_env.cheque_generator import \
    get_set_buffer_size_cheque
from schedule_search.ts_env.cheque_generator import get_set_scope_cheque
from tbe.common.platform import platform_info
from tbe.tvm import Tensor

CACHE_RW_CFG_FILE = "proc_cache_rw.yaml"


def is_dma_intrin(op_tag, dma_list=None, l1_fusion_type=-1, tensor_name=''):
    """

    :param op_tag:
    :param dma_list:
    :return:
    """
    # eltwise的l1 fusion，若read_select+eltwise融合
    # placeholder做cache read, "output_ub_5d"的tensor不做cache_write
    if l1_fusion_type == 0 and tensor_name.find("output_ub_5d") >= 0:
        return False

    if op_tag == "":
        return True

    # requant_vector比较特殊，需要做inline，所以这里不当dma处理
    if op_tag == "requant_vector":
        return False

    op_tag = op_tag.split("|")[0]
    if op_tag not in OP_INTRIN_KEY_TAG\
            or op_tag in ["concat", "transpose"]:
        return False

    intrin = OP_INTRIN_KEY_TAG[op_tag].intrin
    if dma_list is None:
        dma_list = ["dma_copy", "mov_backup"]
    if intrin in dma_list:
        return True

    return False


def get_cache_rw_name(ori_tensor_name, scope, index=None, ub2l1=False):
    """

    :param ori_tensor_name:
    :param scope:
    :param index:
    :return:
    """
    head = ''
    tail = ''
    if ub2l1:
        head = 'sub'
    elif scope in [platform_info.scope_cbuf, platform_info.scope_ubuf]:
        tail = '_l'
    elif scope in [platform_info.scope_cc, platform_info.scope_ca,
                   platform_info.scope_cb]:
        head = 'sub'

    if index is not None:
        index_str = "_%03d" % index
    else:
        index_str = ""

    return head + ori_tensor_name + tail + index_str


def gen_cache_rw_info(cur_tensor,  # pylint: disable=R0913
                      all_tensors,
                      stage_type,
                      scope,
                      stage_info,
                      ub2l1=False):
    """

    :param cur_tensor:
    :param all_tensors:
    :param stage_type:
    :param scope:
    :param stage_info:
    :return:
    """
    tensor_name = cur_tensor.op.name
    base_cache_rw_info = {
        "type": stage_type,
        "scope": scope,
        "name": stage_info["name"],
    }

    # 初始compute中的tensor name
    ori_tensor_name = stage_info.get("ori_name", tensor_name)
    cache_rw_infos = []
    if stage_type == 'CacheRead':
        worksapce_info = stage_info["at_info"]
        consumers_in_group = worksapce_info.consumers_in_group()
        for i, group in enumerate(consumers_in_group):
            cache_rw_info = copy.deepcopy(base_cache_rw_info)
            cache_rw_info['consumers'] = [consumer.index for consumer in group]
            cache_rw_info['fanout_tensors'] = [
                all_tensors[consumer.index]
                for consumer in group
            ]

            cache_rw_info['rw_name'] = get_cache_rw_name(ori_tensor_name,
                                                         scope,
                                                         index=i, ub2l1=ub2l1)
            cache_rw_info["tensor"] = cur_tensor
            cache_rw_info["ub2l1"] = ub2l1
            cache_rw_infos.append(cache_rw_info)
    else:
        cache_rw_info = base_cache_rw_info
        cache_rw_info["tensor"] = cur_tensor
        cache_rw_info['rw_name'] = get_cache_rw_name(ori_tensor_name, scope)
        cache_rw_info["ub2l1"] = ub2l1
        cache_rw_infos.append(cache_rw_info)

    base_cache_rw_info = {
        "type": stage_type,
        "scope": scope,
        "name": cache_rw_infos[0]['rw_name']
    }


    return cache_rw_infos


def gen_trs_cache_write_info(cur_tensor: Tensor, stage_type: str, scope: str, stage_info: dict) -> dict:
    """
    generate transpose cache write information
    :param cur_tensor:
    :param stage_type:
    :param scope:
    :param stage_info:
    :return:
    """
    # 初始compute中的tensor name
    ori_tensor_name = stage_info.get("name", cur_tensor.op.name)
    cache_rw_info = {
        "type": stage_type,
        "scope": scope,
        "name": ori_tensor_name,
        "tensor": cur_tensor,
        "rw_name": get_cache_rw_name(ori_tensor_name, scope)
    }
    return cache_rw_info


def pre_cache_rw(input_dict, rules):
    """
    获取tensor的cache read/write信息
    字典的信息包括: {type: read or write,
                     scope: UB/l1/L0A/L0B/L0C,
                     consumers: [index0, index1],
                     name: tensor名
                     rw_name: cache read write之后的tensor名}
    """
    # 遍历所有的规则，生成cache rw信息
    cache_rw_infos = []
    for rule in rules:
        rule_module = __import__(rule, fromlist=['1'])
        ret = rule_module.proc(input_dict)
        if ret:
            cache_rw_infos.extend(ret)

    return cache_rw_infos


def do_cache_rw(t2c_params, cache_rw_info): # pylint: disable=R0912,R0914,R0915
    """
    进行cache rw操作，并生成stage info
    """
    tensor = cache_rw_info['tensor']
    tensors = cache_rw_info['tensors']
    tensor_name = cache_rw_info['name']
    rw_tensor_name = cache_rw_info['rw_name']
    buf = cache_rw_info['scope']
    buf_str = str(buf)
    stage_type = cache_rw_info["type"]
    sch = t2c_params.schedule
    stage_index_map = t2c_params.op_schedule_info.stage_index_map
    l1_fusion_dict = t2c_params.op_schedule_info.l1_fusion_dict
    rw_tensor = None

    stages = list(sch.stages)
    stage_op_names = (stage.op.name for stage in stages)
    stage_names = (stage_info["name"] for stage_info in t2c_params.stages_info)
    tensor_ori_index = list(stage_names).index(tensor_name)

    code_lines = []
    if stage_type == 'CacheRead':
        # matmul的l1_fusion([input_l1_flag]=1)
        # 1，增加tensor_a的cache read 到platform_info.scope_cbuf_fusion
        # 2，然后set_buffer_size
        if l1_fusion_dict['input_l1_flag'] == 1 and \
                l1_fusion_dict['tensor_a_name'] == tensor.op.name:
            log.dbg("l1_fusion([input_l1_flag]=1), tensor_a cache_read to "
                    "scope_cbuf_fusion")
            buf = platform_info.scope_cbuf_fusion
            buf_str = str(buf)

        out_name_list = []
        consumer_stage_idx_list = []
        real_fanouts = []
        for fanout_tensor in cache_rw_info['fanout_tensors']:
            fanout_index = list(sch.stages).index(sch[fanout_tensor])
            if cache_rw_info['ub2l1']:
                fanout_index -= 1
                real_fanouts.append(
                    t2c_params.stages_info[fanout_index]["rw_tensor"])
            else:
                real_fanouts.append(fanout_tensor)
            fanout_name = t2c_params.stages_info[fanout_index]["name"]
            out_name_list.append(fanout_name)
            consumer_stage_idx_list.append(fanout_index)

        code_line = "%s = sch.cache_read(%s, '%s', [%s])" % (
            rw_tensor_name, tensor_name, buf_str, ",".join(out_name_list))
        code_lines.append(code_line)
        rw_tensor = sch.cache_read(tensor, buf, real_fanouts)

        # 生成cache_read对应的cheque
        get_cache_read_cheque(t2c_params, tensor_ori_index, buf,
                              consumer_stage_idx_list)
        if buf == platform_info.scope_cbuf:
            t2c_params.l1_cache_read_map[rw_tensor.op.name] = list(
                sch.stages).index(sch[rw_tensor])
        # l1_fusion 2,set_buffer_size: 将算子传过来的input_l1_size传给后端
        if l1_fusion_dict['input_l1_flag'] == 1 and \
                l1_fusion_dict['input_l1_size'] > 0 and \
                l1_fusion_dict['tensor_a_name'] == tensor.op.name:
            log.dbg("l1_fusion([input_l1_flag]=1, [input_l1_size]>0), "
                    "tensor_a set_buffer_size")
            code_lines.append("sch[%s].set_buffer_size(%s)" % (
                rw_tensor_name, l1_fusion_dict['input_l1_size']))
            sch[rw_tensor].set_buffer_size(l1_fusion_dict['input_l1_size'])
            # 生成set_buffer_size对应的cheque
            new_stage_names = [stage.op.name for stage in list(sch.stages)]
            rw_tensor_index = new_stage_names.index(rw_tensor.op.name)
            get_set_buffer_size_cheque(t2c_params, rw_tensor_index,
                                         l1_fusion_dict['input_l1_size'])

        # eltwise的l1 fusion，深度融合，输入来自L1，set_scope到l1 fusion
        if l1_fusion_dict['l1_fusion_type'] == 0 \
                and "addr_type" in tensor.op.attrs \
                and tensor.op.attrs["addr_type"].value == 1:
            buf = platform_info.scope_cbuf_fusion
            buf_str = str(buf)
            code_lines.append("sch[%s].set_scope('%s')" % (tensor_name, buf))
            sch[tensor].set_scope(buf)
            get_set_scope_cheque(t2c_params, tensor_ori_index, buf)

    elif stage_type == 'CacheWrite':
        # tuple_reduce_sum需要对多个输出做cache_write
        if tensor.op.num_outputs > 1:
            tensor_names = []
            written_tensor_names = []
            for idx in range(tensor.op.num_outputs):
                tensor_names.append(tensor_name + "_v%s" % idx)
                written_tensor_names.append(tensor_name + "_v%s_l" % idx)
            code_line = "%s = sch.cache_write([%s], '%s')" % (', '.join(
                written_tensor_names), ', '.join(tensor_names), buf_str)
            code_lines.append(code_line)
            code_lines.append('%s = %s' %
                              (rw_tensor_name, written_tensor_names[0]))
            rw_tensors = sch.cache_write(tensors, buf)
            rw_tensor = rw_tensors[0]
            # 生成cache_write对应的cheque
            get_cache_write_cheque_spec(t2c_params, tensor_ori_index,
                                        buf, tensor.op.num_outputs)

        else:
            # l1_fusion([addr_type]=1),输入tensor_a在L1,
            # 增加tensor_a的set_scope,并且后面inline掉tensor_a_l1
            if l1_fusion_dict['in_addr_type'] == 1 \
                    and tensor_name == 'tensor_a_l1':
                log.dbg("l1_fusion [in_addr_type]=1, tensor_a set_scope")
                rw_tensor = tensor.op.input_tensors[0]
                rw_tensor_name = rw_tensor.op.name
                buf = platform_info.scope_cbuf_fusion
                buf_str = str(buf)
                code_lines.append("sch[%s].set_scope('%s')" % (
                    rw_tensor_name, buf))
                sch[rw_tensor].set_scope(buf)
                stage_type = "SetScope"
                # 生成set_scope对应的cheque
                tensor_ori_index = list(stage_op_names).index(rw_tensor_name)
                get_set_scope_cheque(t2c_params, tensor_ori_index, buf)
            else:
                code_line = "%s = sch.cache_write(%s, '%s')" % (
                    rw_tensor_name, tensor_name, buf_str)
                code_lines.append(code_line)
                rw_tensor = sch.cache_write(tensor, buf)
                # 生成cache_write对应的cheque
                get_cache_write_cheque(t2c_params, tensor_ori_index, buf)

                # eltwise的l1 fusion，深度融合，输出到L1，set_scope到l1 fusion
                if l1_fusion_dict['l1_fusion_type'] == 0 \
                        and l1_fusion_dict['out_l1_flag'] \
                        and l1_fusion_dict['eltwise_res_name'] == tensor_name:
                    buf = platform_info.scope_cbuf_fusion
                    buf_str = str(buf)
                    code_lines.append(
                        "sch[%s].set_scope('%s')" % (tensor_name, buf))
                    sch[tensor].set_scope(buf)
                    get_set_scope_cheque(t2c_params, tensor_ori_index, buf)

    else:
        code_line = ""
        log.warn("Unknown action:%s", stage_type)
        return [code_line]

    for ori_index, cur_index in stage_index_map.items():
        if stage_type == 'SetScope':
            break
        update_index = cur_index
        if update_index > tensor_ori_index:
            update_index += 1
        elif update_index == tensor_ori_index and stage_type == 'CacheWrite':
            update_index += 1
        stage_index_map[ori_index] = update_index

    stages_info = t2c_params.stages_info
    stages = list(sch.stages)

    src_stage_info = stages_info[tensor_ori_index]
    src_at_info = src_stage_info['at_info']

    # 先插入新增的cache rw的stage info
    # index使用原来的index
    rw_at_info = AtInfo(src_at_info.index)
    rw_stage_types = [stage_type]
    need_update_index = None
    if stage_type == 'CacheRead':
        for consumer in src_at_info.consumers:
            if consumer.index in cache_rw_info['consumers']:
                # 复制源tensor的consumer
                consumer_info = copy.deepcopy(consumer)
                rw_at_info.add_consumer(consumer_info)
    else:
        # 1.2 cache write
        if src_at_info.is_fork():
            # 如果src是workspace, at到src
            consumer = ConsumerInfo(src_at_info.index)
            consumer.set_sampled_target(src_at_info.index)
            rw_at_info.add_consumer(consumer)
        else:
            # consumer是自己
            rw_at_info = copy.deepcopy(src_at_info)
            # 如果L0C依赖的是placeholder，需要更新L0C父节点的consumer信息
            if buf == platform_info.scope_cc:
                for input_tensor in rw_tensor.op.input_tensors:
                    if input_tensor.op.name in t2c_params.l1_cache_read_map:
                        need_update_index = t2c_params.l1_cache_read_map[
                            input_tensor.op.name]
                        break

        if 'reduce_gm' in src_stage_info.get('type', []):
            rw_stage_types.append('reduce')

    rw_stage_index = stages.index(sch[rw_tensor])
    rw_stage_info = {
        'name': rw_tensor_name,
        'type': rw_stage_types,
        'scope': buf_str,
        'at_info': rw_at_info,
        'ori_name': src_stage_info.get('ori_name'),
        'tag': src_stage_info.get('tag'),
        "rw_tensor": rw_tensor
    }

    rw_tensor_info = {
        "tensor": rw_tensor,
        "stage_info": rw_stage_info
    }

    # set_scope操作不需要新增stage
    if stage_type == 'SetScope':
        stages_info[rw_stage_index] = rw_stage_info
    else:
        if need_update_index:
            update_at_info = AtInfo(need_update_index)
            consumer = ConsumerInfo(rw_stage_index)
            at_atrget = stages_info[need_update_index][
                "at_info"].get_sampled_target()[0]
            consumer.set_sampled_target(at_atrget)
            update_at_info.add_consumer(consumer)
            stages_info[need_update_index]["at_info"] = update_at_info
        stages_info.insert(rw_stage_index, rw_stage_info)

    # 若是L1 fusion([out_addr_type]=1)，最后输出res做set scope
    if l1_fusion_dict['out_addr_type'] == 1 and \
            tensor_ori_index == len(stages) - 3:
        log.dbg("l1_fusion [out_addr_type]=1, res set_scope")
        rw_tensor = sch.outputs[0].output(0)
        buf = platform_info.scope_cbuf_fusion
        code_lines.append("sch[%s].set_scope('%s')" % (
            rw_tensor.op.name, buf))
        get_set_scope_cheque(t2c_params, len(stages) - 1, buf)
        sch[rw_tensor].set_scope(buf)
        stages_info[-1]['scope'] = str(buf)

    t2c_params.code_lines.extend(code_lines)
    return code_lines, rw_tensor_info


def get_tensor_list(sch: object) -> (list, list):
    """
    get tensor list
    :param sch:
    :return:
    """
    tensor_list = []
    all_tensors = []
    for stage in sch.stages:
        tmp_tensor_list = []
        for idx in range(stage.op.num_outputs):
            tmp_tensor_list.append(stage.op.output(idx))
            all_tensors.append(stage.op.output(idx))
        tensor_list.append(tmp_tensor_list)
    return tensor_list, all_tensors


def do_trs_reorder(t2c_params: T2cParams, rw_tensor_info: dict, reorder_action: list) -> NoReturn:
    """
    do trs reorder
    :param t2c_params:
    :param reorder_tensor:
    :param reorder_action:
    :return:
    """
    rw_tensor_info = rw_tensor_info["stage_info"]
    stage_index = t2c_params.op_schedule_info.stages_info.index(rw_tensor_info)
    stage = t2c_params.schedule.stages[stage_index]
    # get nonzero axes
    axis_list = t2c_params.op_schedule_info.feature_tensor[-1][:AXIS_CNT]
    nonzero_axes = [dim for dim in axis_list if dim > 0]
    # get axis info
    reorder_axis_order = reorder_action[:len(nonzero_axes)]
    axes_info = [stage.op.axis[axis_index] for axis_index in reorder_axis_order]
    stage.reorder(*axes_info)

    # 记录reorder信息到py文件中
    t2c_params.code_lines.append('# reorder_index_%s: %s' % (rw_tensor_info['name'], reorder_axis_order))

    code_line = []
    code_line.append('sch[%s].reorder(' % rw_tensor_info['name'])
    for axis_num in reorder_axis_order:
        code_line.append("%s.op.axis[%s], " % (rw_tensor_info['name'], axis_num))
    code_line.append('%s' % ')')

    # 生成reorder操作对应的cheque
    get_reorder_cheque(t2c_params, stage_index, reorder_axis_order)

    t2c_params.code_lines.append(''.join(code_line))


def trs_reorder_cache_rw(t2c_params: T2cParams, curr_cache_rw_info: dict, trs_reorder_action: list) -> NoReturn:
    """
    for transpose optimize by reorder and cache_rw
    :param t2c_params:
    :return:
    """
    input_reorder_action = trs_reorder_action[: AXIS_CNT]
    output_reorder_action = trs_reorder_action[AXIS_CNT :]

    _, all_tensors = get_tensor_list(t2c_params.schedule)

    # 清洗last轴不做transpose场景的input_reorder_action为顺序index
    ori_permute = []
    for _, stage in enumerate(t2c_params.schedule.stages):
        if "permute" in stage.op.attrs:
            ori_permute = [int(i) for i in stage.op.attrs["permute"]]
            break

    out_tensor = t2c_params.schedule.stages[-1].op.output(0)
    # 缩小解空间规则：last轴不做trs或数据byte大于2时，输入tensor不做reorder
    # 该规则源自手写模板
    if int(ori_permute[len(ori_permute) - 1]) == len(ori_permute) - 1 or \
            get_dtype_size(out_tensor.dtype) > 2:
        input_reorder_action[:len(ori_permute)] = list(range(len(ori_permute)))

    # manual_debug reorder_action
    debug_params = t2c_params.op_schedule_info.option.get("trs_debug", [False, {}])
    if debug_params[0]:
        input_reorder_action = debug_params[1]["trs_reorder_action"][:AXIS_CNT]
        output_reorder_action = debug_params[1]["trs_reorder_action"][AXIS_CNT:]

    if curr_cache_rw_info["type"] == "CacheRead":
        # cache read
        _, rw_tensor_info = do_cache_rw(t2c_params, curr_cache_rw_info)

        # trs reorder
        do_trs_reorder(t2c_params, rw_tensor_info, input_reorder_action)

        # cache write
        next_cache_write_info = gen_trs_cache_write_info(
            rw_tensor_info["tensor"], "CacheWrite", platform_info.scope_ubuf,
            rw_tensor_info['stage_info'])
        next_cache_write_info["tensors"] = all_tensors
        do_cache_rw(t2c_params, next_cache_write_info)

    elif curr_cache_rw_info["type"] == "CacheWrite":
        # trs reorder
        stage_names = [stage.op.name for stage in t2c_params.schedule.stages]
        do_trs_reorder(t2c_params,
                       {
                           "tensor": curr_cache_rw_info["tensor"],
                           "stage_info": t2c_params.op_schedule_info.stages_info[
                               stage_names.index(curr_cache_rw_info["tensor"].op.name)]
                        },
                       output_reorder_action)

        # cache write
        do_cache_rw(t2c_params, curr_cache_rw_info)

        # 将trs的参数信息保存到最后一个stage_info里
        # 经过cache_rw后，stage_info的个数不会改变了
        if "trs_params" not in t2c_params.op_schedule_info.stages_info[-1].keys():
            t2c_params.op_schedule_info.stages_info[-1].setdefault('trs_params', {})
        # recorde reorder info
        t2c_params.op_schedule_info.stages_info[-1]["trs_params"].update(
            {
                "input_reorder_action": input_reorder_action,
                "output_reorder_action": output_reorder_action,
                "ori_permute": ori_permute
            }
        )


@timer('cache_rw')
def proc(t2c_params, rules):  # pylint: disable=R0914,R0912
    """

    :param t2c_params:
    :param rules:
    :return:
    """
    sch = t2c_params.schedule
    stages_info = t2c_params.stages_info

    rule_input_dict = {'schedule': sch}

    t2c_params.code_lines.extend(['\n', '# cache_read/cache_write code'])
    t2c_params.op_schedule_info.stage_index_map = {}
    stage_index_map = t2c_params.op_schedule_info.stage_index_map
    l1_fusion_dict = t2c_params.op_schedule_info.l1_fusion_dict

    # 获取原始的未做cache rw的stage consumer信息，供后续获取consumer使用
    consumer_stages_info_dict = {}
    stages_info_map = {}
    ori_all_tensors = []
    for i, stage_info in enumerate(t2c_params.stages_info):
        stage = sch.stages[i]
        for idx in range(stage.op.num_outputs):
            ori_all_tensors.append(stage.op.output(idx))
        for consumer in t2c_params.stages_info[i]["at_info"].consumers:
            consumer_stage_info = t2c_params.stages_info[consumer.index]
            consumer_stages_info_dict.setdefault(
                stage_info["name"], []).append(consumer_stage_info["name"])
            stages_info_map[stage_info["name"]] = stage_info

    for i in range(len(sch.stages)):
        stage_index_map[i] = i

    # 遍历所有stage，进行cache read/write操作, 两次循环分别为一级缓存和二级缓存
    for i in range(2):
        # 获取需要操作的tensor列表，这里需要考虑tuple_reduce_sum
        tensor_list = []
        all_tensors = []
        base_stages_info = copy.deepcopy(stages_info)
        for index, stage in enumerate(sch.stages):
            tmp_tensor_list = []
            for idx in range(stage.op.num_outputs):
                tmp_tensor_list.append(stage.op.output(idx))
                all_tensors.append(stage.op.output(idx))
            tensor_list.append(tmp_tensor_list)
            base_stages_info[index]["rw_tensor"] = stages_info[index].get(
                "rw_tensor", None)

        rule_input_dict["cache_level"] = i
        rule_input_dict["all_tensors"] = all_tensors
        rule_input_dict["ori_all_tensors"] = ori_all_tensors

        tmp_stage_index_map = copy.deepcopy(stage_index_map)
        # 遍历所有tensor
        for j, tensors in enumerate(tensor_list):
            if not base_stages_info[j].get("at_info"):
                continue
            if base_stages_info[j]['tag'] == 'reshape':
                continue
            if 'virtual_leaf_out' in base_stages_info[j].get('type', []):
                continue
            # bind reduce轴时，reduce输出暂时不进行cache_write，
            # 要在rfactor之后进行cache_write
            if 'reduce_atomic' in base_stages_info[j].get('type', []):
                continue
            # 获取cache read 信息, 一个列表
            rule_input_dict["tensor"] = tensors[0]
            rule_input_dict["stage_info"] = base_stages_info[j]
            # 这里取的consumer的stage_info是原始的（未做cache rw的）
            ori_tensor_name = tensors[0].name.split(".")[0]
            consumer_stage_names = consumer_stages_info_dict[ori_tensor_name]
            consumer_stages_info = []
            for consumer_stage_name in consumer_stage_names:
                consumer_stages_info.append(
                    stages_info_map[consumer_stage_name])
            rule_input_dict["consumer_stages_info"] = consumer_stages_info
            # Lv0_write_l0、Lv0_write_l1、Lv0_write_l12l0处理过的tensor，
            # Lv0_write_ub2out不用再处理，用这个标记记录一下
            rule_input_dict["write_proc_flag"] = False
            # l1_fusion {input_l1_flag=1},在规则out1l1中输入tensor_a需要
            # cache read到L1_Fusion，将该属性放在rule_input_dict传进去
            rule_input_dict["input_l1_flag"] = l1_fusion_dict["input_l1_flag"]
            rule_input_dict["tensor_a_name"] = l1_fusion_dict["tensor_a_name"]
            rule_input_dict["l1_fusion_type"] = l1_fusion_dict[
                "l1_fusion_type"]
            rule_input_dict['stage_index_map'] = tmp_stage_index_map
            cache_rw_infos = pre_cache_rw(rule_input_dict, rules)

            for cache_rw_info in cache_rw_infos:
                cache_rw_info["tensors"] = tensors
                op_type = t2c_params.op_schedule_info.option.get('op_type')
                # 目前只考虑lv1 cache read的场景
                if cache_rw_info["ub2l1"]:
                    cache_rw_info["tensor"] = base_stages_info[j]['rw_tensor']
                    do_cache_rw(t2c_params, cache_rw_info)
                elif op_type in TIK_TO_DSL_OP_LIST.keys() and \
                        "trs_reorder_times" in TIK_TO_DSL_OP_LIST.get(op_type).keys():
                    # for trs reorder optimize, only last stage exist trs_reorder_action
                    trs_reorder_action = t2c_params.cleaned_actions[-1][
                                         ActionTensorCfg.trs_reorder_s:ActionTensorCfg.trs_reorder_e + 1]
                    trs_reorder_cache_rw(t2c_params, cache_rw_info, trs_reorder_action)
                else:
                    do_cache_rw(t2c_params, cache_rw_info)

    log.dbg('\ncache_rw done.\n%s', "\n".join(t2c_params.code_lines))

    return True
