#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
from schedule_search import log
from schedule_search.ts_env.tensor_to_code.proc_cache_rw.main import \
    gen_cache_rw_info
from schedule_search.ts_env.tensor_to_code.proc_cache_rw.main import \
    is_dma_intrin
from schedule_search.ts_env.depend import get_fanouts_from_sch
from tbe.common.platform import platform_info
from tbe import tvm


def is_broadcast_not_last_tensor(tensor): # pylint: disable=R0912
    """

    :param tensor:
    :return:
    """
    # 非last的broadcast不做cache_write,包含两种情形：最后一根轴为1和不为1
    # shape全为1的broadcast也不做cache_write。
    # 因为shape全为1的tensor不需要真正做broadcast，可以使用tensor+scalar的指令，
    # 如vadds/vmuls等(该后端指令会自动识别，不用这里手动修改)
    if tensor.op.tag in ["broadcast_for_tensor"]:
        for input_tensor in tensor.op.input_tensors:
            input_shape = [x.value for x in input_tensor.shape]
            broadcast_shape = [x.value for x in tensor.shape]
            # broadcast nist 情形（1）broadcast前shape的最后一根轴不为1，
            # 例如：[2, 1, 1, 10] -->[2, 3, 9, 10]
            # 以及broadcast shape全为1的情形
            if input_shape[len(input_shape) - 1] != 1 \
                    or sum(input_shape[:]) == len(input_shape):
                log.dbg("None Last broadcast_for_tensor Don't Do Cache Write!")
                return True
            # broadcast nist 情形（2）broadcast前后shape的最后一根轴
            # （或者最后的连续几根轴）都为1，
            # 例如：
            # [2, 1, 9, 1] -->[2, 3, 9, 1]；[2, 1, 9, 1，1]-->[2, 3, 9, 1, 1]
            for i in range(len(input_shape) - 1, -1, -1):
                if input_shape[i] == 1 \
                        and input_shape[i] == broadcast_shape[i]:
                    continue
                if input_shape[i] != 1 \
                        and input_shape[i] == broadcast_shape[i]:
                    log.dbg("None Last broadcast_for_tensor "
                            "Don't Do Cache Write!")
                    return True
                if input_shape[i] == 1 \
                        and input_shape[i] != broadcast_shape[i]:
                    return False

    return False


def _spec_last_stage(sch,  # pylint: disable=R0912
                     tensor_name, tensor, stage_tag, l1_fusion_type):
    """

    :param sch:
    :param tensor_name:
    :param tensor:
    :param stage_tag:
    :return:
    """
    stages = list(sch.stages)
    stage_index = stages.index(sch[tensor])
    last_stage = stages[-1]
    virtual_last = False
    if last_stage.op.name == "virtual_leaf_out":
        for input_tensor in last_stage.op.input_tensors:
            if tensor_name == input_tensor.op.name:
                virtual_last = True
                break
    if stage_index == len(stages) - 1 or virtual_last:
        for input_tensor in tensor.op.input_tensors:
            # 最后一个Tensor，如果自己就是dma,且其input是placeholder，
            # 则不需要添加cache write
            if isinstance(input_tensor.op, tvm.PlaceholderOp):
                if is_dma_intrin(stage_tag):
                    return True
            else:
                # 最后一个tensor，且其依赖的tensor是dma则不需要添加cache write
                if is_dma_intrin(input_tensor.op.tag, None,
                                 l1_fusion_type, input_tensor.op.name):
                    log.dbg(
                        "input of last tensor: %s is : %s, "
                        "found mem_copy!", tensor_name,
                        input_tensor.op.name)
                    return True
            # 只看临近节点
            break
    return False


def condition_check(input_dict):  # pylint: disable=R0912, R0911
    """

    :param input_dict:
    :return:
    """
    tensor = input_dict["tensor"]
    tensor_name = tensor.op.name
    cache_level = input_dict["cache_level"]
    sch = input_dict["schedule"]
    stage_info = input_dict["stage_info"]
    stage_tag = stage_info.get("tag")
    write_proc_flag = input_dict['write_proc_flag']
    l1_fusion_type = input_dict['l1_fusion_type']

    if stage_tag in ["ub_to_out", "conv_mad"]:
        log.dbg("ub_to_out op: %s no need to cache write!", tensor_name)
        return False

    if 'gemm_out' in stage_info.get('type', []):
        return False

    op_type = type(tensor.op)
    if tvm.ComputeOp != op_type:
        log.dbg("Only Compute Tensor Do Cache Write, Tensor Op type: %s",
                op_type)
        return False

    if cache_level != 0:
        log.dbg("Only Cache L0 Do Cache Write From UB To Out, Cache Level: %s",
                cache_level)
        return False

    # Lv0_write_l0、Lv0_write_l1、Lv0_write_l12l0处理过的tensor，
    # Lv0_write_ub2out不用再处理
    if write_proc_flag is True:
        return False

    # 最后一个stage，或者后面只有虚节点
    if _spec_last_stage(sch, tensor_name, tensor, stage_tag, l1_fusion_type):
        return False

    # 非last的broadcast不做cache_write
    if is_broadcast_not_last_tensor(tensor):
        return False

    if l1_fusion_type == 0 and tensor_name.find("output_ub_5d") >= 0:
        return False

    # eltwise的l1 fusion，若eltwise+write_select融合,
    # write_select算子的输入tensor不做cache write
    if l1_fusion_type == 0:
        stages = list(sch.stages)
        fanout_list = get_fanouts_from_sch(sch, sch[tensor])
        for fanout in fanout_list:
            if stages[fanout].op.name.find("write_select") >= 0:
                return False

    log.dbg('Op_name: %s, stage_tag: %s, need write UB->OUT.',
            tensor_name, stage_tag)

    return True


def proc(input_dict):
    """

    :param input_dict:
    :return:
    """
    if not condition_check(input_dict):
        return []

    return gen_cache_rw_info(input_dict["tensor"], input_dict["all_tensors"],
                             "CacheWrite", platform_info.scope_ubuf,
                             input_dict['stage_info'])
