#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
import glob
import hashlib
import os
import time
from functools import reduce as functools_reduce
from typing import NoReturn

import numpy as np

from schedule_search import log
from schedule_search import soc_cfg
from schedule_search import util
from schedule_search.ts_env import env_consts
from schedule_search.ts_env import tensor_cfg
from schedule_search.ts_env.env_classes import LocalLock
from schedule_search.ts_env.env_classes import TensorInfo
from schedule_search.ts_env.estimator.evb import evb_host
from schedule_search.util import OPEN_FILE_MODES_640
from schedule_search.util import WRITE_FILE_FLAGS
from schedule_search.util import write_to_file
from tbe import tvm


def get_stage_broadcast_axis(stage):
    """

    :param stage:
    :return:
    """
    broadcast_axis_list = []
    if "broadcast" not in stage.op.tag:
        return broadcast_axis_list
    output_shape = stage.op.output(0).shape
    # 没有input tensor 说明是对scalar进行broadcast，那每一根轴都是broadcast_axis
    if not stage.op.input_tensors:
        return list(range(len(output_shape)))
    input_shape = stage.op.input_tensors[0].shape
    for i in range(len(input_shape) - 1, -1, -1):
        if input_shape[i].value == 1 and output_shape[i].value != 1:
            broadcast_axis_list.append(i)

    broadcast_axis_list.sort()
    return broadcast_axis_list


def user_data_proc(user_file):
    """

    :param user_file:
    :return:
    """
    # 经用户指定的数据，标记下md5值，重命名复制一份，避免重复
    ret, output = util.run_cmd_comm("md5sum " + user_file)
    if not ret:
        raise RuntimeError("md5sum %s failed, %s!" % (user_file, output))
    file_prefix, file_suffix = os.path.splitext(user_file)
    user_file_final = file_prefix + "@" + output.split(" ")[0] + file_suffix
    if os.path.exists(user_file_final):
        return user_file_final
    ret, output = util.run_cmd_comm("cp %s %s" % (user_file, user_file_final))
    if not ret:
        raise RuntimeError("cp %s to %s failed, %s!" %
                           (user_file, user_file_final, output))
    return user_file_final


def classify_outs(outs):
    """
    将输出分为inter输出和leaf输出返回
    """
    operation_list = copy.deepcopy(outs)
    visited_tensor = set()
    inter_out = set()
    while operation_list:
        tmp_operation_list = []
        for current_tensor in operation_list:
            current_op = current_tensor.op
            current_name = current_op.name
            if current_name in visited_tensor:
                log.dbg("tensor:%s has been visited", current_tensor.op.name)
                continue
            visited_tensor.add(current_name)
            if isinstance(current_op, tvm.PlaceholderOp):
                continue
            for i in range(len(current_op.input_tensors)):
                input_tensor = current_op.input_tensors[i]
                input_name = input_tensor.op.name
                if input_name not in inter_out:
                    inter_out.add(input_name)
                tmp_operation_list.append(input_tensor)
        operation_list = list(set(tmp_operation_list))
    leaf_outs = []
    inter_outs = []
    for tensor in outs:
        if tensor.name in inter_out:
            inter_outs.append(tensor)
            log.dbg("inter_out: %s", tensor.name)
        else:
            leaf_outs.append(tensor)
            log.dbg("leaf_out:%s", tensor.name)
    return leaf_outs, inter_outs


def find_none_reduce_axis_index(axis_index,
                                shape_before_reduce,
                                reduce_axis_indexs,
                                is_keepdims=False):
    """
    获取reduce轴在reduce之前的index
    """
    if is_keepdims:
        return axis_index
    after_reduce_axis_index = 0
    for index in range(len(shape_before_reduce)):
        if index == axis_index:
            return after_reduce_axis_index
        if index not in reduce_axis_indexs:
            after_reduce_axis_index += 1
    return after_reduce_axis_index


def find_last_none_reduce_axis(shape_before_reduce, reduce_axis_indexs):
    """

    :param shape_before_reduce:
    :param reduce_axis_indexs:
    :return:
    """
    end_index = 0
    for i in range(len(shape_before_reduce) - 1, -1, -1):
        if shape_before_reduce[i] != 1:
            if i not in reduce_axis_indexs:
                end_index = i
                break
    start_index = 0
    for i in range(end_index, -1, -1):
        if i in reduce_axis_indexs:
            start_index = i + 1
            break
    return start_index, end_index


def find_last_reduce_axis(  # pylint: disable=R0912,R0915,R0911
        shape_before_reduce, reduce_axis_indexs):
    """

    :param shape_before_reduce:
    :param reduce_axis_indexs:
    :return:
    """
    end_index = 0
    for i in range(len(shape_before_reduce) - 1, -1, -1):
        if shape_before_reduce[i] != 1:
            if i in reduce_axis_indexs:
                end_index = i
                break
    start_index = end_index
    for i in range(end_index, -1, -1):
        if i not in reduce_axis_indexs:
            start_index = i + 1
            break
    return start_index, end_index


def select_tiling_case(  # pylint: disable=R0912,R0915,no-member,R0914,R0911
        output_tensors):
    """

    :param output_tensors:
    :return:
    """
    # 0：不bind reduce axis
    # 1、2、3：bind reduce axis；1：reduce all；2：reduce nist；3：reduce last
    # 除部分tuple_reduce_sum多输出可走atomic，
    # 其余多输出添加虚节点，不bind reduce轴
    if not isinstance(output_tensors, list):
        output_tensors = [output_tensors]
    # 考虑中间输出
    leaf_outs, _ = classify_outs(output_tensors)
    if len(leaf_outs) > 1:
        for leaf_out in leaf_outs:
            if leaf_out.op.tag.find("tuple_reduce_sum") == -1:
                log.dbg("no tuple_reduce_sum multi output, no need atomic.")
                return 0
    # 2、只有叶子输出有reduce操作，且没有broadcast
    # 多个reduce的，如cosine_embedding_loss，也需要支持

    reduce_tensor = leaf_outs[0]
    if reduce_tensor.dtype != "float32":
        log.dbg("not float32, no need atomic.")
        return 0
    # 4、必须是reduce_sum
    if reduce_tensor.op.tag.find("reduce_sum") == -1:
        log.dbg("not reduce_sum, no need atomic.")
        return 0
    # 5、必须是cloud
    product = soc_cfg.get_soc_version()
    if product not in [soc_cfg.SocVersion.ASCEND910]:
        log.dbg("not cloud, no need atomic.")
        return 0
    # 6、不同的reduce场景还有一些不同的限制
    # WILLDO: 如果之前还有reduce操作，这里就不能这么简单处理了
    reduce_axis_indexs = []
    source_axis_vars = reduce_tensor.op.body[0].source[0].indices
    for reduce_axis in reduce_tensor.op.reduce_axis:
        for index, source_axis_var in enumerate(source_axis_vars):
            if source_axis_var.same_as(reduce_axis.var):
                reduce_axis_indexs.append(index)
                break
    shape_before_reduce = [
        i.value
        for i in reduce_tensor.op.input_tensors[0].shape
    ]

    core_num = soc_cfg.get_core_num()
    size = 1
    reduce_size = 1
    for index, shape in enumerate(shape_before_reduce):
        if index in reduce_axis_indexs:
            reduce_size *= shape
        else:
            size *= shape
    last_none_reduce_axis_s, last_none_reduce_axis_e \
        = find_last_none_reduce_axis(shape_before_reduce, reduce_axis_indexs)

    # 6.1 所有的轴都reduce
    if len(shape_before_reduce) == len(reduce_axis_indexs):
        tiling_case = 1
    # 6.2 nist
    elif len(shape_before_reduce) - 1 not in reduce_axis_indexs:
        tiling_case = 2
        last_none_reduce_size = 1
        for index in range(last_none_reduce_axis_s,
                           last_none_reduce_axis_e + 1):
            last_none_reduce_size *= shape_before_reduce[index]
        size = size / last_none_reduce_size
        # (ak+1)*ak*..*a2 > core_num
        if size >= core_num:
            log.dbg("axis >= core_num, no need atomic.")
            return 0
        # (ak+1)*ak*..*a2 > rk*..*r2*r1，但tuple_reduce_sum没有这个要求
        if size > reduce_size and len(leaf_outs) < 2:
            log.dbg("axis >= reduce_axis, no need atomic.")
            return 0
    # 6.3 last
    else:
        tiling_case = 3
        # ak*..*a2*a1/core_num > rk*..*r2*r2,
        last_reduce_axis_s, last_reduce_axis_e = find_last_reduce_axis(
            shape_before_reduce, reduce_axis_indexs)
        last_reduce_size = 1
        for index in range(last_reduce_axis_s, last_reduce_axis_e + 1):
            last_reduce_size *= shape_before_reduce[index]
        reduce_size = reduce_size / last_reduce_size
        if size / core_num > reduce_size:
            log.dbg("axis >= reduce_axis, no need atomic.")
            return 0
    # WILLDO: 这里保持与模板相同的处理，因为atomic会有精度误差，
    #  与模板不一致会导致比对失败
    try:
        from tbe.tvm.topi import generic  # pylint: disable=C0415
        with tvm.target.cce():
            auto_sch = generic.auto_schedule(leaf_outs)
            for stage_index, stage in enumerate(auto_sch.stages):
                stage_name = stage.op.name
                next_stage_name = auto_sch.stages[stage_index + 1].op.name
                if stage_name.endswith(".rf") and (
                        next_stage_name.endswith(".repl.")
                        or next_stage_name.endswith(".repl.global")):
                    return tiling_case
    except Exception as exception:  # pylint: disable=broad-except
        log.dbg("call auto schedule failed: %s", str(exception))
        return 0
    return 0


def gen_kernel_name(op_name, unique_id=None):
    """

    :param op_name:
    :param unique_id:
    :return:
    """
    # profiling解析脚本中对kernel_name要求不能超过63个字符，
    # 因此op name限制只取前30个字符
    op_name = op_name[:30]
    if unique_id is None:
        curr_pid = os.getpid()
        curr_time = int(time.time() * 1000)
        unique_id = str(curr_pid) + "_" + str(curr_time)
        kernel_name = "rltune__%s_%s" % (op_name, unique_id)
    elif unique_id == "":
        kernel_name = op_name
    else:
        kernel_name = "rltune__%s_%s" % (op_name, unique_id)
    return kernel_name, unique_id


def build_data_cmp_op(op_schedule_info, cmp_kernel_name):
    """

    :param op_schedule_info:
    :param cmp_kernel_name:
    :return:
    """
    # 用EVB 来跑的话，我们需要生成一个比较的算子
    data_cmp_datas = []
    unsupport_dtype_list = ["uint32"]
    for output in op_schedule_info.output_info_list:
        output_dtype = output.dtype
        # uint32类型只能使用CPU比对
        if output_dtype in unsupport_dtype_list:
            return False
        # reshape成一维数据进行比较
        output_shape = [functools_reduce(lambda x, y: x * y, output.shape)]
        data_cmp_datas.append({"shape": output_shape, "dtype": output_dtype})

    accuracy_tolerance = op_schedule_info.option.get("accuracy_tolerance",
                                                     5.0 * 0.0009765625)
    soc_info_dict = soc_cfg.get_soc_info()
    data_cmp_str = "%s\ndata_cmp(%s, %s, %s, '%s', True)" % (
        op_schedule_info.data_cmp_str, data_cmp_datas, accuracy_tolerance,
        str(soc_info_dict), cmp_kernel_name)
    data_cmp_file = os.path.join(op_schedule_info.store_dir,
                                 "%s.py" % cmp_kernel_name)
    fd = os.open(data_cmp_file, os.O_RDWR | os.O_CREAT, 0o640)
    with os.fdopen(fd, 'w') as file_handler:
        file_handler.write(data_cmp_str)
    cmd = "python3 %s" % data_cmp_file
    ret, output = util.run_cmd_comm(cmd, timeout=120, shell=False, quiet=True)
    os.remove(data_cmp_file)
    if not ret:
        log.warn("can not run cmd: %s, output: %s.", cmd, output)
        return False

    return True


def get_check_output_type():
    """

    :return:
    """
    # 迭代1不做比对方式的动态选择，由用户指定
    return env_consts.CHECK_OUTPUT_TYPE_ELEMENT_CHECK


def tvm_shape_trans(tvm_shape):
    """

    :param tvm_shape:
    :return:
    """
    shape_list = []
    for value in tvm_shape:
        shape_list.append(int(value))
    return shape_list


def get_input_info(option, input_info_list):
    """

    :param option:
    :param input_info_list:
    :return:
    """
    tensor_list = option.get("tensor_list", [])
    if not tensor_list:
        return update_input_seq(option, input_info_list)

    input_update_list = []
    for tensor_dict in tensor_list:
        if tensor_dict.get("shape", "NULL") == "NULL":
            continue
        input_update_list.append(TensorInfo(**tensor_dict))
        if len(input_update_list) == len(input_info_list):
            break
    return input_update_list


def update_input_seq(option, input_info_list):
    """

    :param option:
    :param input_info_list:
    :return:
    """
    op_args = option.get("inputs", [])
    if not op_args:
        return input_info_list

    input_update_list = []
    input_temp_list = copy.deepcopy(input_info_list)
    input_cnt = len(input_info_list)
    dtype_align = {"int8": ["bool"], "bool": ["int8"]}
    curr_cnt = 0
    for arg_dict in op_args:
        if not isinstance(arg_dict, dict):
            return input_info_list
        if arg_dict.get("shape", "NULL") == "NULL":
            continue
        input_shape = arg_dict["shape"]
        input_dtype = arg_dict["dtype"]
        dtype_align_list = [input_dtype] + dtype_align.get(input_dtype, [])
        for input_info in input_temp_list:
            input_size = get_tensor_size(input_shape, input_dtype)
            tensor_size = get_tensor_size(input_info.shape, input_dtype)
            if input_info.dtype in dtype_align_list and \
                    tensor_size == input_size:
                input_temp_list.remove(input_info)
                input_update_list.append(input_info)
                break
        curr_cnt += 1
        if curr_cnt == input_cnt:
            break
    if not input_temp_list:
        return input_update_list

    log.warn("input tensor can not be parsed, %s.", input_temp_list[0])
    return input_info_list


def get_tik_input_output(tik_tensors):
    """

    :param tik_tensors:
    :return:
    """
    tensor_info_list = []
    for tik_tensor in tik_tensors:
        tensor_info = TensorInfo(*tik_tensor)
        tensor_info_list.append(tensor_info)
    return tensor_info_list


def get_input_output_info(schedule_obj, option, output_tensors):
    """

    :param option:
    :param schedule_obj:
    :param stages_info:
    :return:
    """
    if 'tik_tensor' in option:
        input_info_list = get_tik_input_output(option['tik_tensor'][0])
        output_info_list = get_tik_input_output(option['tik_tensor'][1])
        return input_info_list, output_info_list

    input_info_list = []
    output_info_list = []
    input_size = 0
    for stage in schedule_obj.stages:
        if str(stage.op).startswith("placeholder"):
            tensor_info = TensorInfo(stage.op.name,
                                     tvm_shape_trans(stage.op.shape),
                                     stage.op.dtype)
            input_info_list.append(tensor_info)
            input_size += get_tensor_size(tensor_info.shape, tensor_info.dtype)

    input_info_list = get_input_info(option, input_info_list)

    output_size = 0
    if not isinstance(output_tensors, list):
        output_tensors = [output_tensors]
    for output_tensor in output_tensors:
        if output_tensor.op.tag == "conv_virtual_res":
            continue
        tensor_name = output_tensor.name
        if output_tensor.name.startswith("%s.v" % output_tensor.op.name):
            tensor_idx = output_tensor.name.split('.v')[-1]
            tensor_name = '%s_v%s' % (output_tensor.op.name, tensor_idx)
        tensor_info = TensorInfo(tensor_name,
                                 tvm_shape_trans(output_tensor.shape),
                                 output_tensor.dtype)
        output_info_list.append(tensor_info)
        output_size += get_tensor_size(tensor_info.shape, tensor_info.dtype)

    max_size = option.get("host_mem")
    if max_size and input_size + output_size >= max_size:
        raise RuntimeError("input_size:%sB & output_size: %sB is too large, "
                           "shape: %s!" %
                           (input_size, output_size, input_info_list[0].shape))
    return input_info_list, output_info_list


def gen_shape_tag(input_info_list, option, op_md5=None):
    """

    :param input_info_list:
    :param option:
    :param op_md5:
    :return:
    """
    shape_str_list = []
    # 只取前两个shape 不然名字太长了，WILLDO:这里有没有风险
    for input_info in input_info_list[:2]:
        shape_item_list = []
        for shape_vaule in input_info.shape:
            shape_item_list.append(str(shape_vaule))
        shape_str_list.append("_".join(shape_item_list))
        shape_str_list.append(input_info.dtype)
    shape_str = "@".join(shape_str_list)
    # 如果shape在compute中发生变化，在前面加个前缀表示原始shape
    if option.get("shape", None) and option.get("dtype", None):
        shape_list = option["shape"]
        if not isinstance(shape_list[0], list):
            shape_list = [shape_list]
        dtype = option["dtype"]
        shape_dtype_list = []
        for shape in shape_list[:2]:
            shape_dtype_list.extend(["_".join([str(x) for x in shape]), dtype])
        option_shape_str = "@".join(shape_dtype_list)
        if not shape_str.startswith(option_shape_str):
            shape_str = option_shape_str + "-" + shape_str
    if op_md5:
        shape_str += "@" + op_md5
    shape_str += "@" + soc_cfg.get_soc_version()
    return shape_str


def get_block_num(dtype):
    """

    :param dtype:
    :return:
    """
    # D平台DMA单位是32Byte
    return 32 // util.get_dtype_size(dtype)


def get_tensor_size(tensor_shape, data_type):
    """

    :param tensor_shape:
    :param data_type:
    :return:
    """
    element_cnt = functools_reduce(lambda x, y: x * y, tensor_shape)
    data_size = util.get_dtype_size(data_type) * element_cnt
    return data_size


def get_golden_kernel_path(op_schedule_info, default_sch=True):
    """

    :param op_schedule_info:
    :param default_sch:
    :return:
    """
    base_kernel = op_schedule_info.option.get("base_kernel")
    if base_kernel and not default_sch:
        return base_kernel
    op_name = op_schedule_info.op_name
    op_data_path = os.path.join(op_schedule_info.replay_dir, "data", op_name)
    if default_sch:
        kernel_suffix = ".py"
    else:
        kernel_suffix = ".o"
    golden_kernel_path = os.path.join(
        op_data_path, op_schedule_info.shape_list_str + kernel_suffix)
    return golden_kernel_path


def gen_input_data(input_info_list, option):
    """

    :param input_info_list:
    :param option:
    """
    data_dir = os.path.join(option.get("WORKSPACE", ""), "replay_dir", "data")
    data_require = option.get("data_require", "")
    for input_info in input_info_list:
        input_dtype = input_info.dtype
        util.create_dir(data_dir)
        # 从本地先看下是否已经生成了对应dtype的数据文件
        input_file_list = glob.glob(
            os.path.join(
                data_dir, env_consts.DEFAULT_INPUT_DATA_HEADER +
                "@%s%s@*.data" % (input_dtype, data_require)))
        # default input data不存在的时候需要新建
        if not input_file_list:
            # 需要获取锁
            lock_file = os.path.join(
                option.get("WORKSPACE", ""), "local_lock",
                env_consts.DEFAULT_INPUT_DATA_HEADER + "@%s%s@*.data" %
                (input_dtype, data_require))
            local_lock = LocalLock(lock_file)
            input_data = evb_host.gen_random_input_data(
                evb_host.DEFAULT_INPUT_SIZE, input_dtype, data_require)
            hash_obj = hashlib.sha256()
            hash_obj.update(input_data)
            data_md5 = hash_obj.hexdigest()[:util.HASH_LEN]
            default_input_data = os.path.join(
                data_dir, env_consts.DEFAULT_INPUT_DATA_HEADER +
                "@%s%s@%s.data" % (input_dtype, data_require, data_md5))
            fd = os.open(default_input_data, os.O_RDWR | os.O_CREAT, 0o640)
            with os.fdopen(fd, 'wb') as file_handler:
                file_handler.write(input_data)
            local_lock.unlock()


def get_init_action_tensor(stage_num, axis_cnt=tensor_cfg.AXIS_CNT, layers=2):
    """

    :param stage_num:
    :param axis_cnt:
    :param layers:
    :return:
    """
    # Factor都是1
    factors = np.ones([stage_num, axis_cnt], dtype=np.int32)
    # Reorder都是0~轴长
    reorders = np.ones([stage_num, axis_cnt],
                       dtype=np.int32).cumsum(axis=1) - 1

    # at初始化为非法值，通过采样和规则更新
    at_axis = np.full([stage_num, 1], tensor_cfg.INVALID_AT_AXIS)

    # emit_insn init
    emit_insn_index = np.full([stage_num, 1], tensor_cfg.DEFAULT_INSN_INDEX)

    # trs_reorder都是0~轴长
    trs_reorders = np.ones([stage_num, axis_cnt],
                       dtype=np.int32).cumsum(axis=1) - 1

    # 根据Layers添加Factor、Reorder和at emit_insn trs_reorder
    tensor_list = [factors] * layers + [reorders] * layers + [at_axis] + \
                  [emit_insn_index] + [trs_reorders] * 2

    action_tensor = np.concatenate(tensor_list, axis=1)
    return action_tensor


def gen_schedule_py(op_schedule_info, schedule_code, kernel_name, op_file):
    """

    :param op_schedule_info:
    :param schedule_code:
    :param kernel_name:
    :param op_file:
    :return:
    """
    import_str = env_consts.OP_IMPLEMENT_IMPORT_HEADER.format(
        set_product=soc_cfg.set_product_code())
    head_str = env_consts.OP_IMPLEMENT_FUNC_HEADER.format(
        api_def_args=op_schedule_info.api_def_args, kernel_name=kernel_name)
    if op_schedule_info.option.get('op_mode', '') in ['static']:
        tail_str = env_consts.OP_DYNAMIC_COMPILE_TAIL.format(
            tensor_list=op_schedule_info.tensor_list_str)
    else:
        tail_str = env_consts.OP_IMPLEMENT_TAIL.format(
            tensor_list=op_schedule_info.tensor_list_str)
    func_call_str = env_consts.OP_FUNC_CALL.format(
        api_run_args=op_schedule_info.api_run_args,
        kernel_name=kernel_name,
        need_build=True,
        need_print=False)
    if op_schedule_info.tik_op:
        tik_dsl_func_str = env_consts.TIK_DSL_CALL.format(
            op_name=op_schedule_info.op_name,
            input=op_schedule_info.option.get("inputs", []))
        op_code_str = "%s%s%s%s%s" % (import_str, schedule_code, head_str,
                                      tik_dsl_func_str, func_call_str)
    else:
        op_code_str = "%s%s%s%s%s%s" % (import_str, head_str,
                                        op_schedule_info.compute_code,
                                        schedule_code, tail_str, func_call_str)

    # 生成文件
    log.dbg("RL tune info: gen schedule file: %s.", op_file)
    write_to_file(op_file, op_code_str)
    return True


AUTO_SCHEDULE_TAIL = '''
    config = {{"print_ir": need_print,
              "need_build": need_build,
              "name": kernel_name,
              "tensor_list": {tensor_list},
              "bool_storage_as_1bit": False,}}
    with build_config(kernel_meta_parent_dir=kmp_dir):
        tbe.dsl.build(s, config)

op_cce({api_run_args},
       kernel_name="{kernel_name}",
       need_build={need_build},
       need_print=False)
'''
DYNAMIC_AUTO_SCHEDULE_TAIL = '''
    with tvm.target.cce():
        sch = tbe_dsl.auto_schedule({out})
    config = {{"name": kernel_name, "tensor_list": {tensor_list},}}
    tbe_dsl.build(sch, config)

with tbe.common.context.op_context.OpContext("{op_mode}"):
    op_cce({api_run_args},
           kernel_name="{kernel_name}",
           need_build={need_build},
           need_print=False)
'''
COMPUTE_CONTEXT = '''
    from impl.util.platform_adapter import tbe as tbe_dsl
    with tbe_dsl.compute():
        {compute_code}
'''


def gen_static_auto_schedule_py(op_schedule_info: object, op_file: str, kernel_name: str) -> NoReturn:
    """
    for static auto schedule
    :param op_schedule_info:
    :param op_file:
    :param kernel_name:
    """
    compute_code = "\n".join(op_schedule_info.compute_code.split("\n")[:-1])
    head_str = env_consts.OP_IMPLEMENT_HEADER.format(
        set_product=soc_cfg.set_product_code(),
        api_def_args=op_schedule_info.api_def_args,
        kernel_name=kernel_name)
    tail_str = AUTO_SCHEDULE_TAIL.format(
        tensor_list=op_schedule_info.tensor_list_str,
        api_run_args=op_schedule_info.api_run_args,
        kernel_name=kernel_name,
        need_build=True)
    output_tensor_name = "["
    output_name_list = [
        output_tensor.name
        for output_tensor in op_schedule_info.output_info_list
    ]
    output_tensor_name += ", ".join(output_name_list)
    output_tensor_name += "]"
    schedule_code = "\n    from tbe.tvm.topi import generic\n" \
                    "    with tvm.target.cce():"
    schedule_code += "\n         s = generic.auto_schedule(%s)" % (
        output_tensor_name)
    no_bank_code = ""
    op_code_str = "%s%s%s%s%s" % (head_str, no_bank_code, compute_code,
                                  schedule_code, tail_str)

    util.ensure_dir_exists(os.path.dirname(op_file), reset=False)
    fd = os.open(op_file, os.O_RDWR | os.O_CREAT, 0o640)
    with os.fdopen(fd, 'w') as file_handler:
        file_handler.write(op_code_str)


def gen_dynamic_auto_schedule_py(op_schedule_info: object, op_file: str, kernel_name: str) -> NoReturn:
    """
    for dynamic auto schedule
    :param op_schedule_info:
    :param op_file:
    :param kernel_name:
    """
    compute_code = "\n    ".join(op_schedule_info.compute_code.split("\n")[:-1])
    compute_context = COMPUTE_CONTEXT.format(compute_code=compute_code)

    head_str = env_consts.OP_IMPLEMENT_HEADER.format(
        set_product=soc_cfg.set_product_code(),
        api_def_args=op_schedule_info.api_def_args,
        kernel_name=kernel_name)

    output_tensor_name = "["
    output_name_list = []
    for output_tensor in op_schedule_info.output_info_list:
        output_name_list.append(output_tensor.name)
    output_tensor_name += ", ".join(output_name_list)
    output_tensor_name += "]"

    op_mode = op_schedule_info.option.get('op_mode')

    tail_str = DYNAMIC_AUTO_SCHEDULE_TAIL.format(
        out=output_tensor_name,
        op_mode=op_mode,
        tensor_list=op_schedule_info.tensor_list_str,
        api_run_args=op_schedule_info.api_run_args,
        kernel_name=kernel_name,
        need_build=True)

    no_bank_code = ""
    op_code_str = "%s%s%s%s" % (head_str, no_bank_code, compute_context, tail_str)

    util.ensure_dir_exists(os.path.dirname(op_file), reset=False)
    with os.fdopen(os.open(op_file, WRITE_FILE_FLAGS, OPEN_FILE_MODES_640), "w") as file_handler:
        file_handler.write(op_code_str)


def gen_auto_schedule_py(op_schedule_info, op_file, kernel_name):
    """
    auto schedule py
    :param op_schedule_info:
    :param op_file:
    :param kernel_name:
    :return:
    """
    if op_schedule_info.option.get('op_mode', '') in ['static']:
        gen_dynamic_auto_schedule_py(op_schedule_info, op_file, kernel_name)
    else:
        gen_static_auto_schedule_py(op_schedule_info, op_file, kernel_name)


def get_l1_fusion_type(tensor):
    '''
    获取l1_fusion属性L1_fusion_type
    :param tensor:
    :return:
    '''
    l1_fusion_type = -1
    if 'L1_fusion_type' in tensor.op.attrs:
        l1_fusion_type = tensor.op.attrs['L1_fusion_type'].value
    return l1_fusion_type


def get_addr_type(tensor):
    '''
    获取l1_fusion属性addr_type
    :param tensor:
    :return:
    '''
    addr_type = 0
    if 'addr_type' in tensor.op.attrs and \
            tensor.op.attrs['addr_type'].value == 1:
        addr_type = 1
    return addr_type


def get_input_l1_paras(tensor):
    '''
    获取l1_fusion属性L1_addr_flag，L1_valid_size
    :param tensor:
    :return:
    '''
    input_l1_flag = -1
    input_l1_size = None
    if 'L1_addr_flag' in tensor.op.attrs:
        input_l1_flag = tensor.op.attrs['L1_addr_flag'].value

    if input_l1_flag == 0:
        input_l1_size = -1
    elif input_l1_flag == 1:
        if 'L1_valid_size' in tensor.op.attrs:
            input_l1_size = tensor.op.attrs['L1_valid_size'].value
        else:
            input_l1_flag = -1
    else:
        pass

    return input_l1_flag, input_l1_size


def get_from_global_dict(global_dict, key, default_val):
    """
    :param global_dict:
    :param key:
    :return:
    """
    try:
        return global_dict.get(key, default_val)
    except Exception:  # pylint: disable=broad-except
        return default_val


def set_global_dict(global_dict, key, val):
    """
    :param global_dict:
    :param key:
    :param val:
    :return:
    """
    try:
        global_dict[key] = val
    except Exception:  # pylint: disable=broad-except
        return
