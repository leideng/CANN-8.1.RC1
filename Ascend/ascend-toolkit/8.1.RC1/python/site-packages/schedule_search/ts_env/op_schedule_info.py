#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
import glob
import os
from functools import reduce as functools_reduce

from schedule_search import log
from schedule_search import util
from schedule_search import comm
from schedule_search import soc_cfg
from schedule_search.ts_env import broadcast_analysis
from schedule_search.ts_env import depend
from schedule_search.ts_env import env_consts
from schedule_search.ts_env import env_util
from schedule_search.ts_env.env_classes import KernelRunArgsInfo
from schedule_search.ts_env.tensor_cfg import AXIS_CNT
from schedule_search.ts_env.estimator.estimate import get_all_tensors
from schedule_search.timer import timer
from schedule_search.cce_intrin_map import OP_INTRIN_KEY_INDEX
from tbe import tvm

FILE_PATH = os.path.dirname(os.path.realpath(__file__))


def _init_shape_info(input_info_list):
    """

    :param input_info_list:
    :return:
    """
    # shape信息自动从输入中获取
    shape_str_list = []
    all_shape_list = []
    shape_cnt = 0
    for input_info in input_info_list:
        shape_item_list = []
        all_shape_list.extend(input_info.shape)
        for shape_vaule in input_info.shape:
            shape_item_list.append(str(shape_vaule))
        shape_str_list.append("_".join(shape_item_list))
        if shape_cnt < len(shape_item_list):
            shape_cnt = len(shape_item_list)
    shape_str = "_".join(shape_str_list)
    return shape_str, max(all_shape_list), shape_cnt


class OpScheduleInfo:  # pylint: disable=R0902
    """
    OpScheduleInfo
    """
    def __init__(  # pylint: disable=R0912,R0913,R0914,R0915
            self,
            op_name,
            feature_tensor,
            schedule_obj,
            input_info_list,
            output_info_list,
            op_md5,
            compute_code,
            check_output,
            option,
            reduce_axis_dict,
            stages_info,
            complexity=2,
            op_attr_dict=None,
            tiling_case=None,
            base_kernel=None,
            res=None):
        self.op_name = op_name
        self.feature_tensor = feature_tensor
        self.schedule_obj = schedule_obj
        # ori_schedule_obj记录一下做完cache rw、double buffer、
        # inline的schedule对象
        self.ori_schedule_obj = None
        self.compute_code = compute_code
        self.check_output = check_output
        self.input_info_list = input_info_list
        self.output_info_list = output_info_list
        self.reduce_axis_dict = reduce_axis_dict
        self.broadcast_dict = {}
        self.op_md5 = op_md5
        self.option = option
        self.complexity = complexity
        self.tune_flag = self.init_tune_flag()
        self.shape_list_str = env_util.gen_shape_tag(input_info_list, option,
                                                     op_md5)
        self.tensor_list_str = ""
        self.input_file_list = []
        self.input_size_list = []
        self.base_tick = 0
        self.best_tick = 0
        self.base_totaltime = 0
        self.base_run_time = 0
        self.base_run_err_code = None
        self.axis_info_list = []
        self.at_choices = []
        self.stages_info = stages_info
        # ori_stages_info记录一下做完cache rw、double buffer、inline的stages_info
        self.ori_stages_info = None
        self.ori_at_dict = {}
        self.at_dict = {}
        self.at_name_dict = {}
        self.emit_insn_code_dict = {}
        self.workspace = self.option.get("WORKSPACE", "")
        self.lock_dir = os.path.join(self.workspace, "local_lock")
        self.op_attr_dict = op_attr_dict
        self.stage_index_map = {}
        self.workspace_list_str = ""
        self.base_kernel = base_kernel
        self.fanin_dict = {}
        self.fanout_dict = {}
        self.real_fanin_dict = {}
        self.real_fanout_dict = {}
        self.all_fanin_dict = {}
        self.broadcast_groups_dict = {}
        self.l1_fusion_dict = {}
        self.tik_op = 'tik_tensor' in option
        self.ori_op_name = self.get_ori_op_name()
        self.tune_result_key = self.option.get('tune_result_key', self.ori_op_name)
        self.res = res

        self.store_dir = os.path.realpath(os.path.join(util.get_store_tmp_dir(op_name, option), self.shape_list_str))
        util.create_dir(self.store_dir)
        util.create_dir(os.path.join(self.store_dir, "tmp"))

        all_stage_list = self._get_all_stage_list()
        self.tensor_list_str = "[" + ", ".join(all_stage_list) + "]"
        self.code_lines = None
        # 记录一下做完cache rw、double buffer、compute inline的cheque以及stage信息
        self.cheque_list = None
        self.inlined_stages = []
        self.tiling_case = 0 if tiling_case is None else tiling_case
        if self.tiling_case > 0:
            log.info("RL tune info: tiling_case: %s", self.tiling_case)
        self.reduce_atomic_dict = {}
        self.shape, self.shape_max, self.shape_cnt = _init_shape_info(input_info_list)
        self.axis_info_list = []
        self.special_tensor_dict = {}
        if schedule_obj:
            # reduce_last的max/min后端指令，暂不支持float32 需要转维前端指令
            self.update_intrin_map()
            self.update_broadcast_dict()
            self.update_concat_dict()
            self.update_l1_fusion_dict()

        self.c_op = comm.c_op_identify(schedule_obj)
        self.conv_param_dict = {}

        origin_outputs = self._get_origin_outputs()
        self.orign_out_tensor_str = "[" + ", ".join(origin_outputs) + "]"
        self.real_out_tensor_str = "[" + ", ".join(origin_outputs) + "]"

        input_def_args_list, input_run_args_list, data_type = self._get_input_def_run_args()
        self.api_def_args = ", ".join(input_def_args_list) + ", dtype"
        self.api_run_args = ", ".join(input_run_args_list) + ", '%s'" % data_type

        # 判断conv+conv场景fmap+filter是否超过l1size，超过使用深度融合，不超使用广度融合
        self.overflow_l1size_flag = False
        self._update_overflow_l1size_flag(stages_info)

        if option.get("run_by_om", False):
            return
        # auto_tune2.0 do not define the following attributes
        self.replay_dir = os.path.realpath(os.path.join(self.workspace, "replay_dir"))
        data_cmp_py = os.path.realpath(os.path.join(FILE_PATH, "estimator", "data_cmp.py"))
        with open(data_cmp_py, 'r', encoding="utf-8") as file_handler:
            self.data_cmp_str = file_handler.read()

        input_md5_list = self._init_input(option)
        self.expect_output_file_list, self.output_size_list = self._init_output(op_name, input_md5_list)
        self.cmp_kernel = self._init_cmp_kernel(option)

    def __str__(self):
        print_str = ["op_name: %s\noption:%s\ncheck_output:%s\nop_md5:%s\n" %
                     (self.op_name, self.option, self.check_output, self.op_md5)]
        for input_info in self.input_info_list:
            print_str.append("input name: %s, input shape: %s, input dtype: %s\n" %
                             (input_info.name, input_info.shape, input_info.dtype))
        for output_info in self.output_info_list:
            print_str.append("output name: %s, output shape: %s, output dtype: %s" %
                             (output_info.name, output_info.shape, output_info.dtype))
        return ''.join(print_str)

    def update_intrin_map(self):
        """
        update_intrin_map
        """
        self.op_intrin_key_index = copy.deepcopy(OP_INTRIN_KEY_INDEX)
        stages = self.schedule_obj.stages
        for stage_index, stage in enumerate(stages):
            # reduce_last的max/min后端指令，暂不支持float32 需要转维前端指令
            if stage.op.input_tensors \
                    and stage.op.input_tensors[0].dtype == "float32" \
                    and stage.op.tag in ["reduce_min", "reduce_max"] \
                    and self.stages_info[stage_index].get("reduce_type", "") \
                    == "last":
                for op_intrin in self.op_intrin_key_index.values():
                    if op_intrin.intrin \
                            in ["vector_reduce_min", "vector_reduce_max"]:
                        op_intrin.intrin = op_intrin.intrin.replace(
                            "vector", "reduce_last_axis")

    def update_worksapce_list(self):
        """
        update_worksapce_list
        """
        workspace_list = []
        for stage_info in self.stages_info:
            if 'workspace' in stage_info.get('type', []):
                workspace_list.append(stage_info['name'])
        self.workspace_list_str = "[" + ", ".join(workspace_list) + "]"

    def update_at_dict(self):
        """
        update_at_dict
        """
        for i, stage_info in enumerate(self.stages_info):
            at_info = stage_info.get('at_info')
            # 没有inline的stage, 非最后一个stage
            if at_info and i not in self.inlined_stages \
                    and i != len(self.stages_info) - 1:
                at_target = at_info.get_at_target()
                # 如果为-1，表示最后一个stage, 进行更新
                self.at_dict[i] = at_target if at_target != -1 else len(self.stages_info) - 1
                self.at_name_dict[stage_info['name']] = \
                    self.stages_info[at_info.get_at_target()]['name'] if at_target else ''
            else:
                self.at_dict[i] = None
                self.at_name_dict[stage_info['name']] = ''
            if {'CacheRead', 'CacheWrite'} & set(stage_info.get('type', [])):
                continue
            self.ori_at_dict[at_info.ori_index] = at_info.get_sampled_target()

    def update_moves(self, moves):
        """

        :param moves:
        """
        self.compute_code += "\n    # moves: %s" % str(moves)

    def update_stages_info(self):
        """
        1, 更新stage info中的workspace属性；2, 更新comsumer的at target
        """
        stage_index_map = self.stage_index_map
        for i, stage_info in enumerate(self.stages_info):
            at_info = stage_info['at_info']
            for consumer in at_info.consumers:
                if consumer.sampled_target is not None:
                    new_sampled_target = stage_index_map[
                        consumer.sampled_target]
                    stage_types = self.stages_info[new_sampled_target].get(
                        'type', [])
                    if 'reduce_gm' in stage_types and \
                            'reduce_atomic' not in stage_types and \
                            'CacheWrite' not in stage_types:
                        # 如果at target是reduce, at到其cache write,
                        # 需要at到reduce轴
                        new_sampled_target = new_sampled_target - 1
                        # 如果是reduce的cache write, at
                        if new_sampled_target == i:
                            new_sampled_target += 1
                    elif 'l1fuse_leaf' in stage_types:
                        # 向上一格，向下两格，找到L1的stage即为目标stage
                        for step in [-1, 1, 2]:
                            curr_scope = self.stages_info[new_sampled_target +
                                                          step].get("scope")
                            if curr_scope == "local.L1":
                                new_sampled_target += step
                                break
                else:
                    new_sampled_target = None
                consumer.set_updated_sampled_target(new_sampled_target)

    def update_broadcast_dict(self):  # pylint: disable=R0912
        """
        update_broadcast_dict
        """
        broadcast_last_tensors, broadcast_nist_tensors \
            = broadcast_analysis.get_broadcast_tensor(self.schedule_obj)
        # 只有broadcast_last
        if broadcast_last_tensors and not broadcast_nist_tensors:
            # 是否所有的都只broadcast了最后一根轴
            only_broadcast_last = True
            for broadcast_last_tensor in broadcast_last_tensors:
                # broadcast last中全1的shape做broadcast，
                # 不应该算在last_with_nist里面
                if broadcast_analysis.is_scale_broadcast(
                        broadcast_last_tensor):
                    continue
                if broadcast_analysis.is_broadcast_multi_axes(
                        broadcast_last_tensor):
                    only_broadcast_last = False
                    break
            if only_broadcast_last:
                self.broadcast_dict['type'] = 'last_without_nist'
            else:
                self.broadcast_dict['type'] = 'last_with_nist'
                # 最后一根非broadcast轴
                max_last_none_broadcast_axis = \
                    broadcast_analysis.get_last_no_bc_axis(
                        broadcast_last_tensors)
                # 可以切最后一根非broadcast轴
                self.broadcast_dict['max_axis'] \
                    = max_last_none_broadcast_axis - 1
        # 只有broadcast_nist
        elif broadcast_nist_tensors:
            self.broadcast_dict['type'] = 'nist'
            # 最后一根broadcast轴
            max_last_broadcast_axis = broadcast_analysis. \
                get_max_last_broadcast_axis(broadcast_nist_tensors)
            self.broadcast_dict['max_axis'] = max_last_broadcast_axis
            # 下面的逻辑不适配broadcast stage和最后stage轴不完全匹配的场景
            for broadcast_nist_tensor in broadcast_nist_tensors:
                if len(broadcast_nist_tensor.shape) != len(
                        self.schedule_obj.stages[-1].op.axis):
                    return
            # 最后的非broadcast轴是不是32b对齐
            last_stage_dtype = self.schedule_obj.stages[-1].op.output(0).dtype
            block_size = env_util.get_block_num(last_stage_dtype)
            last_stage_nonzero_axes = []
            for axis_len in self.feature_tensor[-1][:AXIS_CNT]:
                if axis_len > 0:
                    last_stage_nonzero_axes.append(axis_len)
                else:
                    break
            total_size = functools_reduce(
                lambda x, y: x * y,
                last_stage_nonzero_axes[max_last_broadcast_axis + 1:])
            self.broadcast_dict['is_align'] = total_size % block_size == 0
            # 是否为特殊的broadcast
            special_broadcast = broadcast_analysis.is_special_broadcast_sence(
                broadcast_nist_tensors, broadcast_last_tensors, self)
            self.broadcast_dict['special_broadcast'] = special_broadcast
        # 没有broadcast
        else:
            self.broadcast_dict['type'] = None

    def update_broadcast_info(self):
        """
        update_broadcast_info
        """
        broadcast_index_dict = {}
        stages = self.schedule_obj.stages
        for stage_index, stage in enumerate(stages):
            if isinstance(stage.op, tvm.PlaceholderOp):
                continue
            if stage.op.tag not in ['broadcast_for_tensor', 'broadcast']:
                continue
            if not stage.op.input_tensors:
                continue
            broadcast_axis = env_util.get_stage_broadcast_axis(stage)
            if not broadcast_axis:
                continue
            broadcast_index_dict.setdefault(stage_index, {})
            broadcast_index_dict.get(stage_index)['axis'] = broadcast_axis

            # 从fanin中找到第一个与之对应的reduce, 记录index
            all_fanin_stage_indices = self.all_fanin_dict[stage_index]
            reduce_fanins = []
            for fanin_index in all_fanin_stage_indices:
                if not isinstance(stages[fanin_index].op,
                                  tvm.PlaceholderOp) \
                        and stages[fanin_index].op.reduce_axis:
                    reduce_fanins.append(fanin_index)

            broadcast_index_dict.get(stage_index)['reduce'] = reduce_fanins
        self.broadcast_dict['info'] = broadcast_index_dict

    def update_concat_dict(self):
        """
        update_concat_dict
        """
        last_stage = self.schedule_obj.stages[-1]
        # 非concat算子
        if "concat" not in last_stage.op.tag:
            self.concat_dict = {}
        # concat算子，
        # 将多个输入shape和输出shape、以及concat_axis记录在concat_dict中
        else:
            self.concat_dict = {}
            concat_axis = 0
            shapes = []
            for in_tensor in last_stage.op.input_tensors:
                shapes.append(util.shape_to_list(in_tensor.shape))
            concat_shape = util.shape_to_list(last_stage.op.output(0).shape)
            shapes.append(concat_shape)
            self.concat_dict['shapes'] = shapes
            for i, concat_shape_value in enumerate(concat_shape):
                if concat_shape_value != shapes[0][i]:
                    concat_axis = i
                    break
            self.concat_dict['concat_axis'] = concat_axis
            # concat最后一根轴时，base可能超时，暂时不用base生成golden数据
            if concat_axis == len(concat_shape) - 1:
                self.option["auto_schedule_golden"] = False

    @timer()
    def update_dependency_dict(self):
        """
        update_dependency_dict
        """
        stage_num = len(self.feature_tensor)
        for i in range(stage_num):
            self.fanin_dict[i], self.real_fanin_dict[i] = \
                depend.get_real_fanin_fanout_stages(
                    i, self.feature_tensor, self.inlined_stages, 'fanin')
            self.fanout_dict[i], self.real_fanout_dict[i] = \
                depend.get_real_fanin_fanout_stages(
                    i, self.feature_tensor, self.inlined_stages, 'fanout')

        self.all_fanin_dict = depend.get_fanin_sub_tree_indices(
            self.feature_tensor)

    def init_tune_flag(self):
        """
        初始化是否需要继续tune的标记
        """
        tune_show = self.option.get('tune_show_dir', '')
        flag_name = self.option.get('kernel_name', None)
        if not os.path.isdir(tune_show) or not flag_name:
            return None
        return os.path.join(tune_show, flag_name + ".flag")

    def update_l1_fusion_dict(self):
        '''
        获取l1_fusion的一些属性
        :return:
        '''
        all_tensors, _ = get_all_tensors(self.schedule_obj,
                                         self.special_tensor_dict)
        tensor_a_l1 = None
        attr_tensor = None
        for tensor in all_tensors:
            if tensor.op.name == 'tensor_a_l1':
                tensor_a_l1 = tensor
                break

        # out_addr_type:判断输出是DDR还是L1，0:DDR,1:L1
        # in_addr_type:判断输入是DDR还是L1，0:DDR,1:L1
        res = self.schedule_obj.outputs[0].output(0)
        self.l1_fusion_dict['out_addr_type'] = env_util.get_addr_type(res)
        self.l1_fusion_dict['l1_fusion_type'] = -1
        self.l1_fusion_dict['in_addr_type'] = 0
        self.l1_fusion_dict['input_l1_flag'] = -1
        self.l1_fusion_dict['input_l1_size'] = None
        self.l1_fusion_dict['tensor_a_name'] = ''
        self.l1_fusion_dict['out_l1_flag'] = False
        self.l1_fusion_dict['eltwise_res_name'] = ''
        # matmul的l1 fusion
        if tensor_a_l1 is not None:
            tensor_a = tensor_a_l1.op.input_tensors[0]

            # 若属性【l1_fusion_type】属于[0,1],属于l1_fusion,只能使能单核
            self.l1_fusion_dict[
                'l1_fusion_type'] = env_util.get_l1_fusion_type(tensor_a)
            self.l1_fusion_dict['in_addr_type'] = env_util.get_addr_type(
                tensor_a)
            input_l1_flag, input_l1_size = env_util.get_input_l1_paras(
                tensor_a)
            self.l1_fusion_dict['input_l1_flag'] = input_l1_flag
            self.l1_fusion_dict['input_l1_size'] = input_l1_size
            self.l1_fusion_dict['tensor_a_name'] = tensor_a.op.name
        # eltwise的l1 fusion只有深度融合场景，即L1_fusion_type=0
        elif "ele_fusion_params" in res.op.attrs:
            attr_tensor = res
        elif "ele_fusion_params" in res.op.input_tensors[0].op.attrs:
            attr_tensor = res.op.input_tensors[0]
        if attr_tensor is not None:
            fusion_params_map = attr_tensor.op.attrs["ele_fusion_params"]
            if fusion_params_map:
                for key in ["l1_fusion_type", "out_l1_flag"]:
                    value = fusion_params_map[key]
                    if hasattr(value, "value"):
                        self.l1_fusion_dict[key] = value.value
                    else:
                        self.l1_fusion_dict[key] = value
            self.l1_fusion_dict['eltwise_res_name'] = res.op.name
        log.dbg("op_schedule_info.l1_fusion_dict: %s", self.l1_fusion_dict)

    def get_ori_op_name(self):
        """

        :param self
        :return:
        """
        ori_op_name = self.option.get('ori_op_name', None)
        if ori_op_name:
            return ori_op_name

        kernel_name = self.option.get('kernel_name',
                                      self.shape_list_str)

        return kernel_name

    def get_compute_op_list(self):
        """

        :param self
        :return:
        """
        compute_op_list = []
        for i, stage in enumerate(self.schedule_obj.stages):
            if isinstance(stage.op, tvm.PlaceholderOp):
                continue
            compute_op_list.append((stage.op, i))
        return compute_op_list

    def update_conv_params(self):
        """
        update conv_params for set_fmatrix stage
        """
        def get_conv_outpt(input_x, padding, kernel, dilate, stride):
            return (input_x + padding -
                    ((kernel - 1) * dilate + 1)) // (stride) + 1

        if self.c_op not in comm.CONV_OP_ID_LIST:
            return

        stages = self.schedule_obj.stages
        for stage_index, stage in enumerate(stages):
            stage_info = self.stages_info[stage_index]
            stage_type = stage_info.get("type", [])
            if stage_info[
                    "tag"] == "set_fmatrix" and "CacheWrite" in stage_type:
                self.conv_param_dict[stage_index] = stage.op.attrs
                h_o = get_conv_outpt(
                    stage.op.attrs['conv_fm_h'],
                    stage.op.attrs['conv_padding_top'] +
                    stage.op.attrs['conv_padding_bottom'],
                    stage.op.attrs['conv_kernel_h'],
                    stage.op.attrs['conv_dilation_h'],
                    stage.op.attrs['conv_stride_h'])
                w_o = get_conv_outpt(
                    stage.op.attrs['conv_fm_w'],
                    stage.op.attrs['conv_padding_left'] +
                    stage.op.attrs['conv_padding_right'],
                    stage.op.attrs['conv_kernel_w'],
                    stage.op.attrs['conv_dilation_w'],
                    stage.op.attrs['conv_stride_w'])
                self.conv_param_dict[stage_index]["ho"] = h_o
                self.conv_param_dict[stage_index]["wo"] = w_o
                if stage.op.axis[0].var.name == "group.c":
                    self.conv_param_dict[stage_index]["group"] = stage.op.axis[
                        0].dom.extent.value
            elif "l1fuse_leaf" in stage_type or "leaf" in stage_type:
                tensor_shape = stage.op.output(0).shape
                data_type = stage.op.output(0).dtype
                output_size = env_util.get_tensor_size(tensor_shape, data_type)
                dma_full = False
                if output_size.value // soc_cfg.get_core_num(
                ) <= soc_cfg.get_l1_size():
                    dma_full = True
                set_fmatrix_list = sorted(self.conv_param_dict.keys(),
                                          reverse=True)
                for set_fmatrix in set_fmatrix_list:
                    if stage_index > set_fmatrix:
                        self.conv_param_dict[set_fmatrix][
                            "dma_full"] = dma_full
                        break

    def _update_overflow_l1size_flag(self, stages_info):
        """
        calculate the used l1 size
        """
        for stage_info in stages_info:
            if "l1fuse_leaf" in stage_info.get("type", []) and not self.overflow_l1size_flag:
                real_l1_size = soc_cfg.get_l1_size()
                core_num = soc_cfg.get_core_num()
                used_l1_size = 0
                all_tensors, _ = get_all_tensors(self.schedule_obj, self.special_tensor_dict)
                log_tensors = []
                for tensor in all_tensors:
                    if "mad1" in tensor.op.name:
                        input_tensors = tensor.op.input_tensors
                        log_tensors.append(input_tensors)
                        for input_tensor in input_tensors:
                            dtype = input_tensor.dtype
                            shape = input_tensor.shape
                            used_l1_size += env_util.get_tensor_size(shape, dtype)
                if int(used_l1_size) > real_l1_size * core_num:
                    self.overflow_l1size_flag = True
                log.dbg("conv+conv the shape: %s, used_l1_size: %s, real_l1_size(muls core_num): %s",
                        log_tensors, used_l1_size, real_l1_size * core_num)
                break

    def _get_all_stage_list(self) -> list:
        """
        get all stage list
        :return all_stage_list
        """
        if self.tik_op:
            all_stage_list = []
            for stage in self.schedule_obj.stages:
                if isinstance(stage.op, tvm.PlaceholderOp):
                    all_stage_list.append(stage.op.name)
            for output_tensor in self.res:
                tensor_name = output_tensor.name
                if output_tensor.name.startswith("%s.v" % output_tensor.op.name):
                    tensor_idx = output_tensor.name.split('.v')[-1]
                    tensor_name = '%s_v%s' % (output_tensor.op.name, tensor_idx)
                all_stage_list.append(tensor_name)
            return all_stage_list

        all_stage_list = []
        for input_info in self.input_info_list:
            all_stage_list.append(input_info.name)
        for output_info in self.output_info_list:
            all_stage_list.append(output_info.name)
        return all_stage_list

    def _init_input(self, option):
        """
        :param input_info_list:
        :param option:
        :return:
        """
        input_md5_list = []
        input_data_dir = os.path.join(self.replay_dir, "data")
        # 默认设置为float16
        data_type = "float16"
        for i, input_info in enumerate(self.input_info_list):
            data_type = input_info.dtype
            # 获取input_file_list
            if self.option.get("golden_input", []):
                input_file = env_util.user_data_proc(self.option["golden_input"][i])
            else:
                data_require = option.get("data_require", "")
                input_file_list = glob.glob(
                    os.path.join(
                        input_data_dir, env_consts.DEFAULT_INPUT_DATA_HEADER +
                        "@%s%s@*.data" % (data_type, data_require)))
                input_md5_list.append(input_file_list[0].split("@")[-1].split(".data")[0])
                input_file = input_file_list[0]
            self.input_file_list.append(input_file)
            input_size = env_util.get_tensor_size(input_info.shape, data_type)
            self.input_size_list.append(input_size)
        return input_md5_list

    def _get_input_def_run_args(self) -> (list, list):
        """
        _get_input_def_run_args
        :return input_def_args_list, input_def_args_list
        """
        input_def_args_list = []
        input_run_args_list = []
        data_type = "float16"
        for i, input_info in enumerate(self.input_info_list):
            data_type = input_info.dtype
            input_def_args_list.append("input%d_shape" % i)
            input_run_args_list.append(str(input_info.shape).replace(" ", ""))
        return input_def_args_list, input_run_args_list, data_type

    def _get_origin_outputs(self) -> list:
        """
        get origin outputs
        """
        origin_outputs = []
        for output_info in self.output_info_list:
            origin_outputs.append(output_info.name)
        return origin_outputs

    def _init_output(self, op_name, input_md5_list):
        """

        :param op_name:
        :param output_info_list:
        :param all_stage_list:
        :param input_md5_list:
        :return:
        """
        op_data_path = os.path.join(self.replay_dir, "data", op_name)
        expect_output_file_list = []
        output_size_list = []
        for i, output_info in enumerate(self.output_info_list):
            if self.option.get("golden_output", []):
                output_file = env_util.user_data_proc(
                    self.option["golden_output"][i])
            else:
                output_prefix = ""
                if self.option.get("auto_schedule_golden", False):
                    output_prefix = "auto_sch_"
                output_file = os.path.join(op_data_path, "%s%s@%s@%s.data" %
                                           (output_prefix, output_info.name, self.shape_list_str,
                                            ("@".join(list(set(input_md5_list))))))
            expect_output_file_list.append(output_file)
            output_size = env_util.get_tensor_size(output_info.shape,
                                                   output_info.dtype)
            output_size_list.append(output_size)
        return expect_output_file_list, output_size_list

    def _init_cmp_kernel(self, option):
        """

        :param option:
        :param output_info_list:
        :return:
        """
        cmp_kernel = None
        if self.option.get("cmp_device", "0") == "0":
            cmp_kernel_name, _ = env_util.gen_kernel_name(self.op_name)
            cmp_kernel_name = 'rltune__data_cmp_' + cmp_kernel_name
            # 尝试3次
            ret = False
            for i in range(env_consts.RETRY_TIMES):
                ret = env_util.build_data_cmp_op(self, cmp_kernel_name)
                if not ret:
                    log.err("[%s]build_data_cmp_op failed.", i)
                    continue
                break
            if not ret:
                # 失败则用cpu进行比对
                option["cmp_device"] = "1"
            else:
                cmp_bin_path = os.path.join(soc_cfg.kernel_meta_dir(),
                                            "%s.o" % cmp_kernel_name)
                cmp_kernel = KernelRunArgsInfo("data_cmp",
                                               len(self.output_info_list),
                                               cmp_bin_path, "")
        return cmp_kernel
