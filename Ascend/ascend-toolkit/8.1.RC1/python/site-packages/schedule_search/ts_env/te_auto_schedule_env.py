#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import copy
import hashlib
import json
import os
import sys
import time
import math
from typing import NoReturn
from typing import Optional

import numpy as np

from schedule_search import comm
from schedule_search import global_manager
from schedule_search import log
from schedule_search import op_cfg
from schedule_search import retry_proc
from schedule_search import soc_cfg
from schedule_search import util
from schedule_search.controller.history import generate_md5
from schedule_search.controller.history import query_history_dict
from schedule_search.controller.history import update_history_dict
from schedule_search.controller.history import update_hit_num
from schedule_search.timer import timer
from schedule_search.ts_env import env_classes
from schedule_search.ts_env import env_util
from schedule_search.ts_env.cache_manager import CACHE_MODE_READ_UPDATE
from schedule_search.ts_env.cache_manager import TickCacheFactory
from schedule_search.ts_env.code_to_tensor import tvm_compute as \
    tvm_compute_to_tensor
from schedule_search.ts_env.env_consts import MODE_RUNTIME
from schedule_search.ts_env.env_consts import OP_IMPLEMENT_HEADER
from schedule_search.ts_env.env_consts import OP_IMPLEMENT_TAIL
from schedule_search.ts_env.env_consts import OP_DYNAMIC_COMPILE_TAIL
from schedule_search.ts_env.estimator import estimate
from schedule_search.ts_env.estimator.om_runner import get_om_base_tick
from schedule_search.ts_env.estimator.kernel_runner import ErrorCode
from schedule_search.ts_env.op_schedule_info import OpScheduleInfo
from schedule_search.ts_env.tensor_cfg import AXIS_CNT
from schedule_search.ts_env.tensor_cfg import MAX_ORIGIN_STAGE_CNT
from schedule_search.ts_env.tensor_to_code import generator as \
    generate_te_schedule
from schedule_search.ts_env.tensor_to_code import t2c_util
from schedule_search.ts_env.tensor_to_code.t2c_util import refine_at
from schedule_search.util import OPEN_FILE_MODES_640
from schedule_search.util import WRITE_FILE_FLAGS
from tbe.common.platform import intrinsic_check_support


def clear_shared_dict():
    """
    clear_shared_dict
    """
    global_manager.BEST_TICK_DICT.clear()
    global_manager.CHEQUE_HISTORY_DICT.clear()


def get_tolerance(option, output_tensors, sch, op_identify):
    """
    使用cpu生成golden数据时，获取误差允许范围
    :param option:
    :param output_tensors:
    :param sch:
    :param op_identify:
    :return:
    """
    dtype = output_tensors[0].dtype
    # 不能只看输出的dtype
    for stage in sch.stages:
        if stage.op.output(0).dtype == "float16":
            dtype = "float16"
            break
    cmp_dict = {
        "cube": {
            "float16": (0.01, 0.01),
            "float32": (0.001, 0.001)
        },
        "vector": {
            "float16": (0.001, 0.001),
            "float32": (0.0001, 0.0001)
        },
        "reduce_sum": {
            "float16": (0.001, 0.01),
            "float32": (0.0001, 0.01)
        },
    }
    if op_identify in comm.MAD_OP_ID_LIST:
        option["error_tolerance"], option["accuracy_tolerance"] \
            = cmp_dict.get("cube").get(dtype, (0.001, 0.001))
    elif option["op_name"] in ["reduce_sum_d", "gn_training_reduce"]:
        option["error_tolerance"], option["accuracy_tolerance"] \
            = cmp_dict.get("reduce_sum").get(dtype, (0.001, 0.001))
    else:
        option["error_tolerance"], option["accuracy_tolerance"] \
            = cmp_dict.get("vector").get(dtype, (0.001, 0.001))


def update_op_cmp_cfg(output_tensors, option, op_identify, sch):
    """
    数据比对的配置，不同算子可能比对标准不同
    :param output_tensors:
    :param option:
    :param op_identify:
    :param sch:
    :return:
    """
    tiling_case = option["tiling_case"]
    # 因为有累加顺序导致的精度问题，atomic处理时golden为auto_schedule
    if tiling_case > 0:
        option["auto_schedule_golden"] = True
        return

    # 不支持vsqrt指令的精度太低，只能和手写aicore算子进行比对
    if option["op_name"] in ["bn_training_update"]:
        if not intrinsic_check_support("Intrinsic_vsqrt"):
            option["auto_schedule_golden"] = True
            return

    if 'tik_tensor' in option and option.get("no_base_tune", False):
        opp_path = option.get("opp_path", "")
        if os.path.exists(opp_path) and opp_path not in sys.path:
            sys.path.append(opp_path)
        op_name = option["op_name"]
        op_impl_module = __import__('impl.' + op_name, fromlist=['1'])
        if hasattr(op_impl_module, "%s_np" % op_name):
            option["tik_np_golden"] = True
            option["auto_schedule_golden"] = False

    # 使用cpu生成golden数据时，允许一定的误差
    if not option.get("auto_schedule_golden", False):
        get_tolerance(option, output_tensors, sch, op_identify)
    return


def get_complexity(sch, feature_tensor, stages_info, output_tensors):
    """

    :param sch:
    :param reduce_axis_dict:
    :param output_tensors:
    :return:
    """
    _, output_dict = comm.get_depends(sch)
    shape_vec = np.ones([AXIS_CNT], np.int32)
    reduce_num = 0
    broadcast_num = 0
    split_node_num = 0
    for i, stage_info in enumerate(stages_info):
        shape_i = feature_tensor[i][:AXIS_CNT]
        shape_vec = np.maximum(shape_vec, shape_i)
        if len(output_dict[i]) > 1:
            split_node_num += 1
        if 'reduce' in stage_info.get('type', []):
            reduce_num += 1
        if 'broadcast' in stage_info.get('tag', ''):
            broadcast_num += 1

    complexity = 0
    for axis in shape_vec:
        complexity += math.ceil(math.log(axis, 8))
    complexity += reduce_num * 4
    complexity += broadcast_num * 2
    if isinstance(output_tensors, list):
        complexity += len(output_tensors)
    complexity *= split_node_num
    return complexity


@timer('gen_op_schedule_info')
def gen_op_schedule_info(  # pylint: disable=R0914
        output_tensors,
        option,
        add_virtual_leaf_out=True,
        op_md5=None):
    """
    :param output_tensors:
    :param option:
    :param add_virtual_leaf_out:
    :param op_md5:
    :return:
    """
    op_name = option["op_name"]
    # 获取tiling_case
    tiling_case = env_util.select_tiling_case(output_tensors)
    option["tiling_case"] = tiling_case

    # compute_code只包含到default schedule的代码
    sch, compute_code, stages_info = tvm_compute_to_tensor.gen_compute_code(
        output_tensors,
        add_virtual_leaf_out=add_virtual_leaf_out,
        tiling_case=tiling_case,
        option=option)
    if sch is None:
        return None
    if len(stages_info) > MAX_ORIGIN_STAGE_CNT:
        raise Exception('stage_num[%d] > max_stages[%d].' %
                        (len(stages_info), MAX_ORIGIN_STAGE_CNT))

    op_identify = comm.c_op_identify(sch)
    update_op_cmp_cfg(output_tensors, option, op_identify, sch)
    # 获取输入输出Reduce信息
    feature_tensor, attr_dict, reduce_axis_dict \
        = tvm_compute_to_tensor.proc(sch, stages_info, op_name)
    input_info_list, output_info_list = env_util.get_input_output_info(
        sch, option, output_tensors)
    complexity = get_complexity(sch, feature_tensor, stages_info,
                                output_tensors)
    # 用feature_tensor&reduce_axis_dict来当成算子的op_md5,可以确保唯一&不变性
    # 按理说，feature_tensor一样，算子就应该一样，
    # 所以reduce_axis_dict这样的信息应该放在FeatureTensor内的
    if not op_md5:
        op_md5_str = feature_tensor.tobytes()
        op_md5_str += str([
            json.dumps(reduce_axis_dict.get(x), sort_keys=True)
            for x in sorted(reduce_axis_dict.keys())
        ]).encode()
        for stage in sch.stages:
            if str(stage.op).startswith('placeholder'):
                continue
            op_md5_str += str(stage.op.body).encode()
        op_md5_str += ";".join([str(x) for x in input_info_list]).encode()
        op_md5_str += ";".join([str(x) for x in output_info_list]).encode()
        op_md5 = hashlib.sha256(op_md5_str).hexdigest()[:util.HASH_LEN]
    rl_schedule_key = util.gen_rl_schedule_key(input_info_list,
                                               output_info_list, sch)
    op_yaml_config = json.dumps(option.get("op_config", []))
    # compute_code里带上shape、dtype以及reduce轴信息
    compute_code += "\n\n    #rl_schedule_key: " + rl_schedule_key + \
                    "\n    #reduce_axis: " + \
                    str([json.dumps(reduce_axis_dict.get(x), sort_keys=True)
                         for x in sorted(reduce_axis_dict.keys())]) + \
                    "\n    #op_config: " + op_yaml_config

    # 默认不比对
    compare_flag = option.get("verify", False)
    if compare_flag is False:
        check_output = 0
    else:
        check_output = env_util.get_check_output_type()

    # 存在规则轴外取消强制at拉齐
    if op_identify != "no":
        option["at_align"] = False

    if not option.get("run_by_om", False):
        # 检验特殊的IR
        if option["op_name"] not in op_cfg.NO_DATA_REQ_OP_LIST:
            ret = t2c_util.check_data_require(option, sch)
            if ret is False:
                raise Exception('"golden_input" must be configured. %s' % op_name)
        # 新生成input输入数据
        env_util.gen_input_data(input_info_list, option)
    op_schedule_info = OpScheduleInfo(op_name,
                                      feature_tensor,
                                      sch,
                                      input_info_list,
                                      output_info_list,
                                      op_md5,
                                      compute_code,
                                      check_output,
                                      option,
                                      reduce_axis_dict,
                                      stages_info,
                                      complexity,
                                      attr_dict,
                                      tiling_case,
                                      res=output_tensors)

    op_schedule_info.compute_code += "\n    #broadcast_dict: " \
                                     + str(op_schedule_info.broadcast_dict) + \
                                     "\n    #l1_fusion_dict: " \
                                     + str(op_schedule_info.l1_fusion_dict)

    return op_schedule_info


def get_base_tick(op_schedule_info: object, base_tick: Optional[int]) -> tuple:
    """
    :param op_schedule_info:
    :param base_tick:
    :return:
    """
    if base_tick is not None:
        log.info("RL tune info: base_tick is not None, base_tick: %s.", base_tick)
        return base_tick, ErrorCode.RUN_SUCC
    err_code = ErrorCode.RUN_SUCC
    if op_schedule_info.option.get("run_by_om", False):
        base_tick, err_code = get_om_base_tick(op_schedule_info)
    else:
        _, base_tick = estimate.golden_proc(op_schedule_info)
    log.info("RL tune info: op %s get_base_tick end, base_tick: %s, err_code: %s.",
             op_schedule_info.option.get('op_config', {}).get("kernel_name", "default"), base_tick, err_code)
    return base_tick, err_code


def get_op_schedule_info(output_tensors, option):
    """

    :param output_tensors:
    :param option:
    :return:
    """
    log.dbg("RL schedule info: get_op_schedule_info begin, output_tensors: %s, option: %s.", output_tensors, option)
    if str(option.get("verify", True)).lower() != "false":
        option["verify"] = True

    try:
        op_schedule_infos = []
        # TIK可能会额外多一层List
        if isinstance(output_tensors, list) and isinstance(output_tensors[0], list):
            base_tick = None
            if len(output_tensors) > 1:
                option["multi_sch"] = True
            for tik_tensors in output_tensors:
                op_schedule_info = gen_op_schedule_info(tik_tensors, option, add_virtual_leaf_out=True)
                op_schedule_info.base_tick, op_schedule_info.base_run_err_code = \
                    get_base_tick(op_schedule_info, base_tick)
                op_schedule_infos.append(op_schedule_info)
        else:
            op_schedule_info = gen_op_schedule_info(output_tensors,
                                                    option,
                                                    add_virtual_leaf_out=True)
            if op_schedule_info is None:
                return []

            # 生成golden数据，不添加virtual_out
            base_op_schedule_info = op_schedule_info

            if "virtual_leaf_out" in op_schedule_info.stages_info[-1].get('type', []):
                base_op_schedule_info = gen_op_schedule_info(
                    output_tensors,
                    option,
                    add_virtual_leaf_out=False,
                    op_md5=op_schedule_info.op_md5)

            op_schedule_info.base_tick, op_schedule_info.base_run_err_code = get_base_tick(base_op_schedule_info, None)
            op_schedule_infos = [op_schedule_info]
        log.dbg("RL schedule info: get_op_schedule_info end, op_schedule_info: %s.", op_schedule_info)
        return op_schedule_infos
    except Exception as exception:  # pylint: disable=broad-except
        log.err("RL exception occur: %s get_op_schedule_info failed: %s.", option.get("op_name", ""), repr(exception))
        return []


def actions_oom_proc(op_schedule_info,  # pylint: disable=R0912
                     action_tensors, oom_times=30):
    """
    action_tensors -> schedule -> build，生成.o、schedule对象、
    schedule代码、cleaned_actions
    如果oom，减小factor，再试
    """
    error_code = ErrorCode.BUILD_FAIL
    loop_count = 0
    # tik暂时不使能oom
    if isinstance(op_schedule_info, list):
        oom_times = 1
    else:
        oom_times = oom_times if op_schedule_info.option.get('enable_oom', False) else 1

    while loop_count < oom_times:
        loop_count += 1
        output = estimate.actions_to_kernel_bin(action_tensors, op_schedule_info, print_output=False)
        if isinstance(output, list):
            error_code = output[0].error_code
        else:
            error_code = output.error_code
        log.dbg("[%s] actions_to_kernel_bin return error_code: %s", loop_count, error_code)

        # oom处理次数达上限
        if loop_count == oom_times:
            break
        # 不是oom错误
        if error_code not in env_classes.OomError.ALL:
            break
        old_actions = copy.deepcopy(action_tensors)
        # 先做将减小at轴, 如果at轴无法减小了，则factor减小
        action_tensors = refine_at(action_tensors)
        # 如果等于np格式则进行转换
        if isinstance(action_tensors, np.ndarray):
            action_tensors = action_tensors.tolist()
            old_actions = old_actions.tolist()
        if action_tensors == old_actions:
            op_schedule_info = copy.deepcopy(op_schedule_info)
            action_tensors = t2c_util.refine_oom(action_tensors, error_code,
                                                 op_schedule_info)
            log.dbg("decay split, action_tensors: %s", action_tensors)
            # factor无法再减小，则break
            if action_tensors == old_actions:
                break

    return output


def evb_run_tick(  # pylint: disable=R0914,unsubscriptable-object
        op_schedule_info, process_share_infos, kernel_info, print_output):
    """
    :param op_schedule_info:
    :param kernel_info:
    :param print_output:
    :return:
    """
    if isinstance(op_schedule_info, list):
        single_op_schedule_info = op_schedule_info[0]
        single_kernel_info = kernel_info[0]
    else:
        single_op_schedule_info = op_schedule_info
        single_kernel_info = kernel_info

    cleaned_actions = single_kernel_info.sch_info.cleaned_actions

    err_code = ""
    tick = None
    if process_share_infos is not None:
        # 1、can use dict cache
        op_md5 = single_op_schedule_info.op_md5
        cheque_md5 = generate_md5(single_op_schedule_info.option, single_kernel_info.sch_info.cheque_list)
        log.dbg("op_md5: %s cheque_md5: %s try to query and update cheque_history_dict.", op_md5, cheque_md5)
        err_code, tick, _ = query_history_dict(op_md5, cheque_md5, process_share_infos.cheque_history_dict)

    if not err_code:
        # 2、can use redis cache
        cache_mode = single_op_schedule_info.option.get(
            'cache_mode', CACHE_MODE_READ_UPDATE)
        tick_cache_factory = TickCacheFactory()
        tick_cache_obj = tick_cache_factory.get_cache_manager(
            single_op_schedule_info,
            cleaned_actions,
            single_kernel_info.sch_info.code,
            mode=cache_mode)
        _, history_tuple = tick_cache_obj.read_cache()
        # cache命中，且为succ的才复用，其余都要重新跑
        if history_tuple and history_tuple[0] == ErrorCode.RUN_SUCC:
            err_code = history_tuple[0]
            tick = history_tuple[1]
            log.dbg('redis hit, succ!')
    # 3、没有命中的情况，执行run
    if not err_code:
        log.dbg('cache miss!')
        err_code, tick = estimate.proc(op_schedule_info,
                                       kernel_info,
                                       process_share_infos,
                                       store_tmp_sch=True,
                                       print_output=print_output)
        # 只更新成功的cache
        if err_code == ErrorCode.RUN_SUCC:
            tick_cache_obj.update_cache(err_code + "@" + str(tick))

        # 由于是多batch跑的，run的过程中，可能已经更新
        if process_share_infos is not None:
            tick, err_code = update_history_dict(op_md5, cheque_md5, tick, err_code,
                                                 process_share_infos.cheque_history_dict)

    if process_share_infos is not None:
        # update hit_num
        update_hit_num(op_md5, cheque_md5, process_share_infos.cheque_history_dict)

    return err_code, tick


def _get_tick(action_tensors,
              op_schedule_info,
              process_share_infos,
              print_output=False,
              greedy=False,
              moves=None):
    """
    :param action_tensors:
    :param op_schedule_info:
    :param print_output:
    :param greedy:
    :param moves:
    :return:
    """
    op_cfg.MULTI_SCH_BLOCKDIM = []
    # 1、注释中记录一下moves
    if moves and not isinstance(op_schedule_info, list):
        op_schedule_info.update_moves(moves)

    # 2、actions_oom_proc
    kernel_info = actions_oom_proc(op_schedule_info, action_tensors)

    # 3、greedy_proc
    kernel_infos = [kernel_info]
    if greedy and not isinstance(op_schedule_info, list):
        greedy_proc(op_schedule_info, action_tensors, kernel_infos)

    # 4、获取tick
    result_list = []
    for kernel_info in kernel_infos:
        if isinstance(kernel_info, list):
            single_kernel_info = kernel_info[0]
        else:
            single_kernel_info = kernel_info
        # build失败就没必要再获取tick了
        if single_kernel_info.error_code != ErrorCode.BUILD_SUCC_:
            log.dbg("RL tune info: build_failed, status: %s.", single_kernel_info.error_code)
            result_list.append([
                False, 0, single_kernel_info.sch_info.cleaned_actions,
                single_kernel_info.error_code,
                single_kernel_info.sch_info.retry_t2c_rules
            ])
            continue
        err_code, tick = evb_run_tick(op_schedule_info, process_share_infos, kernel_info,
                                      print_output)
        ret = err_code == ErrorCode.RUN_SUCC
        result_list.append([
            ret, tick, single_kernel_info.sch_info.cleaned_actions, err_code,
            single_kernel_info.sch_info.retry_t2c_rules
        ])
        if err_code == ErrorCode.BUILD_OOM:
            break
    # 返回tick最小的一个
    return sorted(result_list, key=lambda x: x[1])[0]


def retry_get_tick(err_code, tick,  # pylint: disable=R0913,R0914
                   action_tensors,
                   op_schedule_info,
                   retry_t2c_rules,
                   process_share_infos,
                   print_output=False):
    """
    :param err_code:
    :param tick:
    :param action_tensors:
    :param op_schedule_info:
    :param print_output:
    :return:
    """
    if process_share_infos is None:
        return tick
    if isinstance(op_schedule_info, list):
        op_schedule_info_list = op_schedule_info
        op_md5 = op_schedule_info[0].op_md5
    else:
        op_schedule_info_list = [op_schedule_info]
        op_md5 = op_schedule_info.op_md5

    if op_schedule_info_list[0].option.get("no_retry", False):
        return tick

    if not tick:
        return tick

    ever_best = env_util.get_from_global_dict(process_share_infos.best_tick_dict, op_md5, sys.maxsize)
    # RNN算子每个succ都retry一下
    if op_schedule_info_list[0].op_name not in op_cfg.RNN_OP_LIST:
        if ever_best <= tick:
            return tick

    if err_code == ErrorCode.RUN_SUCC:
        env_util.set_global_dict(process_share_infos.best_tick_dict, op_md5, tick)

    retry_option_list = retry_proc.get_retry_option(retry_t2c_rules)
    log.dbg("retry_options: %s", retry_option_list)
    for retry_option in retry_option_list:
        old_option_list = []
        for curr_op_schedule in op_schedule_info_list:
            old_option_list.append(curr_op_schedule.option)
            curr_op_schedule.option = copy.deepcopy(curr_op_schedule.option)
            curr_op_schedule.option.update(retry_option)
        ret, curr_tick, _, _, _ = _get_tick(action_tensors,
                                            op_schedule_info,
                                            process_share_infos,
                                            print_output=print_output)
        log.info("retry once, curr tick: %s, ori tick: %s", curr_tick, tick)
        if ret and curr_tick < tick:
            tick = curr_tick

        for i, curr_op_schedule in enumerate(op_schedule_info_list):
            curr_op_schedule.option = old_option_list[i]

    return tick


def get_tick(action_tensors,  # pylint: disable=R0914
             op_schedule_info,
             process_share_infos=None,
             print_output=False,
             greedy=False,
             moves=None):
    """
    :param action_tensors:
    :param op_schedule_info:
    :param print_output:
    :param greedy:
    :param moves:
    :return:
    """
    def gen_excp_info(op_sch_obj, action_tensor):
        content = "# compute_at_dict:"
        content += json.dumps(op_sch_obj.ori_at_dict, ensure_ascii=False)
        content += '\n# broadcast_groups: %s' % op_sch_obj.broadcast_groups_dict
        content += "\n" + str(action_tensor) + "\n"
        return content

    if isinstance(op_schedule_info, list):
        op_cfg.tik_dsl_action_adjust(op_schedule_info, action_tensors)
        new_action_tensors = []
        for action_tensor in action_tensors:
            if isinstance(action_tensor, np.ndarray):
                action_tensor = action_tensor.tolist()
            new_action_tensors.append(action_tensor)
        action_tensors = new_action_tensors
    else:
        if isinstance(action_tensors, np.ndarray):
            action_tensors = action_tensors.tolist()

    try:
        ret, tick, cleaned_actions, err_code, retry_t2c_rules = _get_tick(
            action_tensors, op_schedule_info, process_share_infos, print_output=print_output,
            greedy=greedy, moves=moves)
        tick = retry_get_tick(err_code,
                              tick,
                              action_tensors,
                              op_schedule_info,
                              retry_t2c_rules,
                              process_share_infos,
                              print_output=print_output)
        log.dbg("RL tune info: tick %s, err_code %s, ret %s, cleaned_actions: %s.",
                 tick, err_code, ret, cleaned_actions)
        return ret, tick, cleaned_actions, err_code
    except (KeyboardInterrupt, BrokenPipeError) as exp:
        log.dbg("RL exception occur: msg: %s", repr(exp))
        return None, None, None, ""
    except Exception as exception:  # pylint: disable=broad-except
        # 保存action tensor到store tmp目录
        log.dbg("RL exception occur: msg: %s", repr(exception))
        ret, tick, cleaned_actions, err_code = False, 0, action_tensors, ErrorCode.ENV_EXCEPTION
        content = ""
        if isinstance(op_schedule_info, list):
            store_dir = op_schedule_info[0].store_dir
            for i, op_sch_obj in enumerate(op_schedule_info):
                content += gen_excp_info(op_sch_obj, action_tensors[i])
        else:
            store_dir = op_schedule_info.store_dir
            content += gen_excp_info(op_schedule_info, action_tensors)
        content += str(exception)
        store_tmp_dir = os.path.join(store_dir, err_code)
        unique_id = str(os.getpid()) + "_" + str(int(time.time() * 1000))
        except_record = os.path.join(store_tmp_dir, "action_%s.txt" % unique_id)
        util.write_to_file(except_record, content=content)
        return ret, tick, cleaned_actions, err_code


def gen_sch(  # pylint: disable=R0914,no-member
        best_action_dict,
        res,
        option):
    """
    :param best_action_dict:
    :param res:
    :param option:
    :return:
    """
    # 这个函数拆分成2个部分，分别是生成原始Schedule和生成带Cache操作的Schedule
    # 同时再获取op_schedule_info中的信息
    op_schedule_info = get_op_schedule_info(res, option)[0]

    # 生成代码时用的schedule_obj是通过序列化来的，现在需要用主语义空间的对象
    # 这时的schedule_obj已经是完整版的了，包含了AL，BL之类的cache操作Stage
    best_action = best_action_dict["action_tensor"]
    if op_schedule_info is None or best_action is None:
        log.warn("op_schedule_info or best_action is None, search completed.")
        return False, None, None

    best_tick = best_action_dict["tick"]
    use_base = False
    best_src_path = ""
    best_sch = None

    # 最优schedule python文件路径
    best_code_path = os.path.join(
        op_schedule_info.store_dir,
        "best_%s@%s.py" % (str(best_tick), op_schedule_info.shape_list_str))

    # 使用兜底，把base文件从run_succ里拷出来作为最优
    if use_base:
        util.cp_src_to_dst(best_src_path, best_code_path)
        return True, "", best_sch

    # 使用搜索结果，run_scc里面已经有了，直接生成best文件即可
    # validation的时候，将enable_oom设置为True，需要做clean
    op_schedule_info.option['enable_oom'] = True
    op_schedule_info.stages_info = best_action_dict['stages_info']
    # 先做cache rw, double buffer, inline, 然后更新
    update_op_schedule_info(op_schedule_info, best_action, 10, 30)

    # oom + 用runtime模式获取sch
    kernel_info = actions_oom_proc(op_schedule_info, best_action)
    op_schedule_info.schedule_obj = kernel_info.sch_info.sch
    head_str = OP_IMPLEMENT_HEADER.format(
        set_product=soc_cfg.set_product_code(),
        api_def_args=op_schedule_info.api_def_args,
        kernel_name=op_schedule_info.op_name)

    if op_schedule_info.option.get('op_mode', '') in ['static']:
        tail_str = OP_DYNAMIC_COMPILE_TAIL.format(
            tensor_list=op_schedule_info.tensor_list_str)
    else:
        tail_str = OP_IMPLEMENT_TAIL.format(
            tensor_list=op_schedule_info.tensor_list_str,
            api_run_args=op_schedule_info.api_run_args,
            kernel_name=op_schedule_info.op_name,
            need_build=True,
            need_print=False)
    best_code = "%s%s%s%s" % (head_str, op_schedule_info.compute_code,
                              kernel_info.sch_info.code, tail_str)
    log.dbg("best code is:%s", best_code)
    with os.fdopen(os.open(best_code_path, WRITE_FILE_FLAGS, OPEN_FILE_MODES_640), "w") as file_handler:
        file_handler.write(best_code)

    return True, best_code_path, op_schedule_info


def greedy_proc(
        op_schedule_info,  # pylint: disable=R0914,R0915
        sample_actions,
        kernel_infos):
    """
    :param op_schedule_info:
    :param sample_actions:
    :param kernel_infos:
    :return:
    """
    op_schedule_info = copy.deepcopy(op_schedule_info)

    last_down_actions = sample_actions
    for _ in range(1):
        down_actions = t2c_util.decay_factor(last_down_actions,
                                             op_schedule_info,
                                             decay_divisor=16)
        if down_actions == last_down_actions:
            break
        output = estimate.actions_to_kernel_bin(down_actions,
                                                op_schedule_info,
                                                sample_actions=sample_actions,
                                                print_output=False)
        kernel_infos.append(output)
        last_down_actions = down_actions

    last_up_actions = sample_actions
    for _ in range(30):
        up_actions = t2c_util.update_factor(last_up_actions, op_schedule_info)
        if up_actions == last_up_actions:
            break
        output = estimate.actions_to_kernel_bin(up_actions,
                                                op_schedule_info,
                                                sample_actions=sample_actions,
                                                print_output=False)
        if output.error_code in env_classes.OomError.ALL:
            log.dbg('oom: %s', up_actions[-1][:AXIS_CNT])
            break

        kernel_infos.append(output)
        last_up_actions = up_actions


def action_tensor_to_schedule(action_tensor,
                              op_schedule_info,
                              end_index=75,
                              start_index=31):
    """
    :param action_tensor:
    :param op_schedule_info:
    :param end_index:
    :param start_index:
    :return:
    """
    # 基于action_tensor生成schedule对象返回
    # end_index截止的rule的index
    # start_index开始的rule的index
    if isinstance(action_tensor, np.ndarray):
        action_tensor = action_tensor.tolist()
    sample_actions = copy.deepcopy(action_tensor)

    # 只处理70及以下的Proc, 排除bind的规则
    generate_te_schedule.proc(op_schedule_info,
                              action_tensor,
                              mode=MODE_RUNTIME,
                              sample_actions=sample_actions,
                              proc_index_start=start_index,
                              proc_index_end=end_index,
                              excluded_index=[70])
    return np.array(action_tensor, dtype=np.int32)


def update_workspace_type(stages_info):
    """
    :param stages_info:
    """
    for stage_info in stages_info:
        at_info = stage_info.get('at_info', None)
        # workspace节点设置属性为workspace
        if at_info and at_info.is_fork() \
                and 'placeholder' not in stage_info.get('type', []):
            stage_info.setdefault('type', [])
            if 'workspace' not in stage_info['type']:
                stage_info['type'].append('workspace')


def update_progress(progress, start_index, end_index):
    """

    :param progress:
    :param start_index:
    :param end_index:
    """
    op_schedule_info = progress.op_schedule_info
    update_op_schedule_info(op_schedule_info, progress.action_tensor,
                            start_index, end_index)
    progress.ori_op_schedule_info = copy.deepcopy(op_schedule_info)
    log.dbg(
        '!!!!!!!!!Ori changed. progress stages: %d, stages_info[%d]: ',
        len(op_schedule_info.schedule_obj.stages),
        len(op_schedule_info.stages_info),
    )
    progress.action_tensor = env_util.get_init_action_tensor(
        progress.stage_num)
    # cache_rw后需要更新op_layers
    progress.op_layers = util.get_op_layers(op_schedule_info.schedule_obj)


def trs_update_progress(progress: object, start_index: int, end_index: int) -> NoReturn:
    """

    :param progress:
    :param start_index:
    :param end_index:
    """
    op_schedule_info = progress.op_schedule_info
    ori_stages = copy.deepcopy(progress.op_schedule_info.schedule_obj.stages)
    ori_action_tensor = copy.deepcopy(progress.action_tensor)

    update_op_schedule_info(op_schedule_info, progress.action_tensor,
                            start_index, end_index)

    progress.ori_op_schedule_info = copy.deepcopy(op_schedule_info)

    curr_stages = progress.op_schedule_info.schedule_obj.stages

    progress.action_tensor = env_util.get_init_action_tensor(
        progress.stage_num)

    # update action_tensor
    for stage_index, stage in enumerate(curr_stages):
        for ori_index, ori_stage in enumerate(ori_stages):
            if stage.op.name == ori_stage.op.name:
                progress.action_tensor[stage_index] = \
                    ori_action_tensor[ori_index]

    # cache_rw后需要更新op_layers
    progress.op_layers = util.get_op_layers(op_schedule_info.schedule_obj)


def is_broadcast_followed(op_schedule_info, broadcast_index, stage_index):
    """
    :param op_schedule_info:
    :param broadcast_index:
    :param stage_index:

    """
    # 当前stage和at target之间的broadcast info记录下来
    # 1，fanout是broadcast
    fanouts = op_schedule_info.fanout_dict[stage_index]
    real_fanouts = op_schedule_info.real_fanout_dict[stage_index]

    # 2，如果自身是broadcast的cachewrite，fanout的fanout是broadcast
    stage_info = op_schedule_info.stages_info[stage_index]
    if 'CacheWrite' in stage_info.get('type', []):
        fanouts = op_schedule_info.fanout_dict[stage_index + 1]
        real_fanouts = op_schedule_info.real_fanout_dict[stage_index + 1]

    if broadcast_index in fanouts or broadcast_index in real_fanouts:
        return True

    return False


def bc_axises_clean(  # pylint: disable=R0914
        bc_axises,
        op_schedule_info,
        stage_index,
        at_index):
    """

    :param bc_axises:
    :param op_schedule_info:
    :param stage_index:
    :param at_index:
    :return:
    """
    feature_tensor = op_schedule_info.feature_tensor
    stage_shape = tvm_compute_to_tensor.get_stage_shape(
        feature_tensor, stage_index)
    at_stage_shape = tvm_compute_to_tensor.get_stage_shape(
        feature_tensor, at_index)
    reduce_axis_dict = op_schedule_info.reduce_axis_dict
    exclude_axises = []

    # 1. stage与at target 的shape长度不一样
    if len(stage_shape) != len(at_stage_shape):
        # 如果stage和at target之间有keep_dims=False的reduce，则排除掉
        # 遍历所有的reduce stage:
        # 1)keep_dims为False;
        # 2)at target相同;
        # 3)是reduce fanin
        for index, reduce_info in reduce_axis_dict.items():
            if not reduce_info['keep_dim'] \
                    and op_schedule_info.at_dict.get(index) == at_index \
                    and stage_index in op_schedule_info.all_fanin_dict.\
                    get(index, []):
                # reduce掉broadcast axis排除掉
                exclude_axises.extend(reduce_info['axis'])
    # 2. 维度一样，但axis的值不一样
    else:
        # 维度一样，如果axis不相同且不等于1，则排除掉
        for i, axis in enumerate(stage_shape):
            if axis != at_stage_shape[i]\
                    and axis != 1 \
                    and at_stage_shape[i] != 1:
                exclude_axises.append(axis)

    cleaned_bc_axises = []
    exclude_axises = list(set(exclude_axises))
    for axis in bc_axises:
        if axis in exclude_axises:
            continue

        axises_ahead_num = 0
        for exclude_axis in exclude_axises:
            if axis > exclude_axis:
                axises_ahead_num += 1
        cleaned_bc_axises.append(axis - axises_ahead_num)

    return cleaned_bc_axises


def update_broadcast_group(op_schedule_info):
    """

    :param op_schedule_info:
    """
    # gn_training_update, 不采样reorder
    if t2c_util.gn_training_update_nchw(op_schedule_info):
        return

    # 遍历所有的at target
    broadcast_info = op_schedule_info.broadcast_dict['info']
    for stage_index, target_index in op_schedule_info.at_dict.items():
        # inline掉的stage， target_index为None
        if target_index is None:
            continue
        # reduce stage暂时不处理
        my_stage_info = op_schedule_info.stages_info[stage_index]
        at_stage_info = op_schedule_info.stages_info[target_index]
        if 'reduce' in at_stage_info.get('type', []):
            continue
        # stage是broadcast stage,或者stage的输出是broadcast
        fanins = op_schedule_info.fanin_dict[stage_index]
        fanouts = op_schedule_info.fanout_dict[stage_index]
        for bc_index in broadcast_info:
            bc_axises = bc_axises_clean(broadcast_info[bc_index]['axis'],
                                        op_schedule_info, stage_index,
                                        target_index)

            if bc_index in fanins + [stage_index] + fanouts:
                at_stage_info.setdefault('broadcast_groups', [])
                if bc_axises \
                        and bc_axises not in at_stage_info['broadcast_groups']:
                    at_stage_info['broadcast_groups'].append(bc_axises)

            # 后面紧接着是broadcast，记录broadcast的信息，用于判断采样axis
            if is_broadcast_followed(op_schedule_info, bc_index, stage_index):
                my_stage_info.setdefault('followed_bc_info', [])
                my_stage_info['followed_bc_info'].extend(bc_axises)


def update_op_schedule_info(op_schedule_info, action_tensor, start_index,
                            end_index):
    """

    :param op_schedule_info:
    :param action_tensor:
    :param start_index:
    :param end_index:
    """
    # 如果是workspace,更新type,在action_tensor_to_schedule之前
    update_workspace_type(op_schedule_info.stages_info)

    # 如果是Workspace转到切分，要把Workspace的结束掉
    # 需要搞一个新的OpScheduleInfo
    action_tensor_to_schedule(action_tensor, op_schedule_info, end_index,
                              start_index)

    old_sch = op_schedule_info.schedule_obj
    stages_info = op_schedule_info.stages_info

    # 遍历所有stage info
    new_feature_tensor, new_attr_dict, new_reduce_axis_dict \
        = tvm_compute_to_tensor.proc(old_sch, stages_info,
                                     op_schedule_info.op_name)

    op_schedule_info.feature_tensor = new_feature_tensor
    op_schedule_info.op_attr_dict = new_attr_dict
    op_schedule_info.reduce_axis_dict = new_reduce_axis_dict
    op_schedule_info.update_stages_info()

    # 1，更新workspace_node信息
    op_schedule_info.update_worksapce_list()
    # 2，更新at dict
    op_schedule_info.update_at_dict()
    # 3，更新dependency dict
    op_schedule_info.update_dependency_dict()
    # 4，更新update_broadcast_info
    op_schedule_info.update_broadcast_info()
    # 5, 更新stages_info的broadcast groups
    update_broadcast_group(op_schedule_info)
    # 更新卷积参数
    op_schedule_info.update_conv_params()
    # 更新卷积L1融合参数
    comm.update_stage_tag_type(op_schedule_info)

if __name__ == '__main__':
    sys.exit(0)
