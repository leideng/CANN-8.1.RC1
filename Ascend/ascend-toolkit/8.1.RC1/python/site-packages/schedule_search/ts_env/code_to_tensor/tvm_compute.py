#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import numpy as np

from schedule_search import log
from schedule_search import util
from schedule_search import comm
from schedule_search.ts_env import env_util
from schedule_search.ts_env.env_consts import COMM_REDUCE_TAG
from schedule_search.ts_env.env_consts import VIRTUAL_LEAF_OUT_TAG
from schedule_search.ts_env.tensor_cfg import FeatureTensorCfg
from schedule_search.ts_env.tensor_cfg import AXIS_CNT
from schedule_search.timer import timer
from schedule_search.cce_intrin_map import OP_INTRIN_KEY_TAG
from tbe import tvm


INPUT_TENSOR_GEN = "input_tensor_gen.py"
SCHEDULE_DUMP = "schedule.dump"
REAL_AXIS_DUMP = "real_axis.dump"
DTYPE_INDEX = {'int8': 1,
               'uint8': 2,
               'int16': 3,
               'uint16': 4,
               'int32': 5,
               'uint32': 6,
               'int64': 7,
               'uint64': 8,
               'float16': 9,
               'bfloat16': 10,
               'float32': 11,
               'float64': 12,
               'bool': 13,
               'uint1': 14,
               'unknown': 0}

# conv类算子的tag规则与RL不一致，且现有模板依赖tag，不方便修改，这里额外维护tag与期望tag之间的映射关系
CONV_TAG_AND_TAG_MAP = {"convolution_im2col_row_major": "set_fmatrix",
                        "convolution__im2col_fractal": "im2col",
                        "convolution_im2col_fractal": "im2col",
                        "convolution_c_col": "conv_mad",
                        "dequant_vector": "mem_copy",
                        "dequant_remove_pad": "remove_pad",
                        "quant": "mem_copy",
                        "res_out_fp16": "mem_copy",
                        "requant_remove_pad": "remove_pad",
                        "conv_vector_remove_pad": "remove_pad",
                        "convolution_row_major_reshape": "reshape",
                        "dequant_s16_vector": "mem_copy",
                        "dequant_s16_remove_pad": "remove_pad",
                        "requant_s16_vaddrelu": "elewise_binary_addrelu",
                        "requant_s16_vector": "requant_vector",
                        "conv_virtual_res": "phony_insn",
                        "res_remove_pad_u8": "ub_to_out",
                        "res_remove_pad_s16": "ub_to_out"}


# 和上述类似，维护stage name与期望tag之间的映射关系
CONV_NAME_AND_TAG_MAP = {"reform_by_vmuls": "reform_by_vector",
                         "cast_i8_ub": "elewise_single_round_d",
                         "convolution_bias_l0c": 'ub_to_l0',
                         "convolution_c_col_bias": "mad_add_bias",
                         "fmap_l1": "out_to_l1",
                         "offset_ub": "vector_auto"}


def gen_depends_tensor(schedule_obj, input_tensor): # pylint: disable=R0912
    """
    :param schedule_obj:
    :param input_tensor:
    :return:
    """
    dict_depend, dict_depended = comm.get_depends(schedule_obj)
    # 每个depend 8位，共支持16*8=128个stages
    # 因此非零的depend个数等于stage数除以8，再加1
    stage_len = len(schedule_obj.stages)
    inner_iter_num = stage_len // 8 + 1

    # 遍历所有stage
    for i in range(stage_len):
        # 排下序
        dict_depend[i].sort()
        if i in dict_depended:
            dict_depended[i].sort()
        # 内层循环，每8位1个数
        for j in range(inner_iter_num):
            depend_value = 0
            depended_value = 0
            for depend in dict_depend[i]:
                if j * 8 <= depend < (j + 1) * 8:
                    depend_value = 1 << (depend % 8) | depend_value

            if i in dict_depended:
                for depended in dict_depended[i]:
                    if j * 8 <= depended < (j + 1) * 8:
                        depended_value = 1 << (depended % 8) | depended_value

            deps_int = depend_value | depended_value
            input_tensor[i][FeatureTensorCfg.deps_e - j] = deps_int


def gen_axis_tensor(schedule_obj, input_tensor): # pylint: disable=R0912
    """
    :param schedule_obj:
    :param input_tensor:
    :return:
    """
    for stage_idx, stage in enumerate(schedule_obj.stages):
        if isinstance(stage.op, tvm.PlaceholderOp):
            # Placeholder肯定没有Reduce轴，一起填上0
            for i, axis in enumerate(stage.op.shape):
                input_tensor[stage_idx][i] = axis
        else:
            for i, axis in enumerate(stage.op.axis):
                input_tensor[stage_idx][i] = axis.dom.extent.value

            for i, reduce_axis in enumerate(stage.op.reduce_axis):
                input_tensor[stage_idx][AXIS_CNT + i] = \
                    reduce_axis.dom.extent.value


def get_empty_tag_idx(stages_info, stage, stage_index, c_op):
    """

    :param stages_info:
    :param stage:
    :param stage_index:
    :param c_op:
    :return:
    """
    # 有些算子会赋值op_tag
    if c_op in comm.CONV_OP_ID_LIST:
        for key, expect_tag in CONV_NAME_AND_TAG_MAP.items():
            if key in stage.op.name and '.local.' in stage.op.name:
                op_index = OP_INTRIN_KEY_TAG[expect_tag].op_index
                return op_index

    key = stages_info[stage_index].get("op_tag", "mem_copy")
    if stage.op.reduce_axis and c_op in comm.GEMM_OP_ID_LIST:
        key = 'matmul'
        stages_info[stage_index]['tag'] = key
    op_index = OP_INTRIN_KEY_TAG[key].op_index
    return op_index


def get_matmul_tag_idx(stage):
    """
    :param stage:
    :return:
    """
    if stage.op.reduce_axis:
        op_index = OP_INTRIN_KEY_TAG["matmul"].op_index
    else:
        op_index = 0
    return op_index


def get_mov_backup_tag_idx(stage):
    """
    :param stage:
    :return:
    """
    return OP_INTRIN_KEY_TAG[stage.op.tag].op_index


def get_comm_reduce_tag_idx(op_name, reduce_type):
    """
    COMM_REDUCE_TAG特殊处理
    :param op_name:
    :param reduce_type:
    :return:
    """
    key = op_name
    # 没有指定last nist的场景要补充上
    if reduce_type and not key.endswith(reduce_type):
        key = '%s_%s' % (key, reduce_type)

    op_index = OP_INTRIN_KEY_TAG[key].op_index
    return op_index


def get_local_stage_idx(stage, reduce_type):
    """
    :param stage:
    :param reduce_type:
    :return:
    """
    if (stage.op.tag in ["conv_mad"] and 'local.L0' not in stage.op.name):
        op_index = 0
    elif stage.op.tag in CONV_TAG_AND_TAG_MAP:
        op_index = OP_INTRIN_KEY_TAG[CONV_TAG_AND_TAG_MAP[
            stage.op.tag]].op_index
    else:
        key = stage.op.tag.split("|")[0]
        if reduce_type and \
                not key.endswith(reduce_type) and \
                key not in OP_INTRIN_KEY_TAG:
            key = '%s_%s' % (key, reduce_type)
        op_index = OP_INTRIN_KEY_TAG[key].op_index
    return op_index


def get_stage_op_index(stage_index,
                       stages_info, sch, c_op, op_name):
    """

    :param stage_index:
    :param stages_info:
    :param sch:
    :param c_op:
    :param op_name:
    :return:
    """
    stage = sch.stages[stage_index]
    reduce_type = stages_info[stage_index].get('reduce_type', '')
    op_index = 0

    # 每个元素由三个数据组成：条件、函数、参数，即满足该条件，则等于函数（参数）
    cond_func_list = [
        [
            isinstance(stage.op, tvm.PlaceholderOp),  # placeholder
            int,
            (0, )
        ],
        [
            stage.op.tag == '',  # Tag为空
            get_empty_tag_idx,
            (stages_info, stage, stage_index, c_op)
        ],
        [
            stage.op.tag == 'matmul',  # Tag为Matmul，其实可能是memCpy
            get_matmul_tag_idx,
            (stage, )
        ],
        [
            stage.op.tag in ["mov_backup"],  # 最后一个stage可能本身就是DMA操作，
            get_mov_backup_tag_idx,  # 但emit_insn对应的key可能不是mem_copy
            (stage, )
        ],
        [
            stage_index == len(sch.stages) - 1,  # 最后一个stage都是mem_copy
            int,
            (0, )
        ],
        [
            stage.op.tag == COMM_REDUCE_TAG,  # comm redduce
            get_comm_reduce_tag_idx,
            (op_name, reduce_type)
        ],
        [
            '.local.' in stage.op.name,  # .local stage
            get_local_stage_idx,
            (stage, reduce_type)
        ]
    ]

    for cond, func, params in cond_func_list:
        if cond:
            op_index = func(*params)
            break

    return op_index


def proc_gemm_tag(stage, stage_index, stages_info):  # pylint: disable=R0912
    """
    mat_mul的tag需要特殊处理
    :param stage:
    :param stage_index:
    :param stages_info:
    :return:
    """
    if stage.op.name == 'tensor_c_gm' and stage.op.tag == 'matmul':
        stages_info[stage_index]['tag'] = 'mem_copy'
        stages_info[stage_index].setdefault('type', []).append('gemm_out')
    elif stage.op.name == 'tensor_a_l1' and not stage.op.tag:
        stages_info[stage_index]['tag'] = 'out_to_l1'
    elif stage.op.name == 'tensor_a_l0a' and not stage.op.tag:
        stages_info[stage_index]['tag'] = 'l1_to_l0'
    elif stage.op.name == 'tensor_b_l1' and not stage.op.tag:
        stages_info[stage_index]['tag'] = 'out_to_l1'
    elif stage.op.name == 'tensor_b_l0b' and not stage.op.tag:
        stages_info[stage_index]['tag'] = 'l1_to_l0'
    elif stage.op.name == 'tensor_bias_l0c' and not stage.op.tag:
        stages_info[stage_index]['tag'] = 'ub_to_l0'
    elif stage.op.name == 'tensor_c_add_bias' and not stage.op.tag:
        stages_info[stage_index]['tag'] = 'mad_add_bias'


def proc_conv_tag(stage, stage_index, stages_info):  # pylint: disable=R0912
    """
    conv的tag需要特殊处理
    :param stage:
    :param stage_index:
    :param stages_info:
    :return:
    """
    if stage.op.tag in CONV_TAG_AND_TAG_MAP:
        stages_info[stage_index]['tag'] = CONV_TAG_AND_TAG_MAP[stage.op.tag]
    else:
        for key, expect_tag in CONV_NAME_AND_TAG_MAP.items():
            if key in stage.op.name and not stage.op.tag:
                stages_info[stage_index]['tag'] = expect_tag


def get_op_tensor(sch, stage,  # pylint: disable=R0913
                  stage_index, stages_info, op_name, c_op, input_tensor):
    """
    :param sch:
    :param stage:
    :param stage_index:
    :param stages_info:
    :param op_name:
    :param c_op:
    :param input_tensor:
    :return:
    """
    # matmul的tag打在了最后一个stage，需要挪到reduce stage
    # 因此将tag记录到stages_info，进行特殊处理，tag为空赋值为mem_copy
    if not stages_info[stage_index].get('tag'):
        stages_info[stage_index]['tag'] = \
            stage.op.tag if stage.op.tag else 'mem_copy'
    op_index = get_stage_op_index(stage_index, stages_info, sch, c_op,
                                  op_name)
    input_tensor[stage_index][FeatureTensorCfg.compute_s] = op_index
    # mat_mul的tag需要特殊处理
    if c_op in comm.GEMM_OP_ID_LIST:
        proc_gemm_tag(stage, stage_index, stages_info)
    elif c_op in comm.CONV_OP_ID_LIST:
        proc_conv_tag(stage, stage_index, stages_info)


def get_reduce_axis_dict(stage,  # pylint: disable=R0912
                         stage_index, stages_info, reduce_axis_dict):
    """

    :param stage:
    :param stage_index:
    :param stages_info:
    :param reduce_axis_dict:
    :return:
    """
    if isinstance(stage.op, tvm.PlaceholderOp) \
            or not stage.op.reduce_axis:
        return

    reduce_axis_list = util.get_reduce_axis_index(stage.op)
    keep_dim = len(stage.op.input_tensors[0].shape) == len(
        stage.op.output(0).shape)
    reduce_axis_dict[stage_index] = {
        "axis": reduce_axis_list,
        "keep_dim": keep_dim,
        "stage_name": stage.op.name,
        "bc_followed": {}, }

    stages_info[stage_index].setdefault('type', [])
    if 'reduce_atomic' in stages_info[stage_index].get('type', []):
        stages_info[stage_index]['type'].append('reduce')
        # 记录一下shape_before_reduce、reduce_axis_indexs、is_keepdims
        input_tensor_shape = stage.op.input_tensors[0].shape
        shape_before_reduce = [i.value for i in input_tensor_shape]
        stages_info[stage_index]['shape_before_reduce'] \
            = shape_before_reduce
        stages_info[stage_index]['reduce_axis_indexs'] \
            = reduce_axis_list
        is_keepdims = len(shape_before_reduce) == len(stage.op.axis)
        stages_info[stage_index]['is_keepdims'] = is_keepdims
    else:
        stages_info[stage_index]['type'].append('reduce_gm')

    if not stages_info[stage_index].get('reduce_type', ''):
        if len(stage.op.input_tensors[0].shape) - 1 in reduce_axis_list:
            reduce_type = 'last'
        else:
            reduce_type = 'nist'
        stages_info[stage_index]['reduce_type'] = reduce_type
        reduce_axis_dict[stage_index]["type"] = reduce_type


def get_attr_dict(stage, attr_dict):  # pylint: disable=R0912
    """

    :param stage:
    :param attr_dict:
    :return:
    """
    for attr_name, attr_value in stage.op.attrs.items():
        if isinstance(attr_value, tvm.container.Map):
            attr_dict[str(attr_name)] = {}
            attr_dict[str(attr_name)].update(attr_value.items())
        elif isinstance(attr_value, tvm.tir.expr.IntImm):
            attr_dict[str(attr_name)] = attr_value.value
        elif isinstance(attr_value, tvm.runtime.container.String):
            attr_dict[str(attr_name)] = str(attr_value)
        else:
            attr_dict[str(attr_name)] = attr_value
        # 不存在融合轴时，可以自行计算轴长放置到axis_info中
        if attr_name in ["axis", "reduce_axis"]:
            stage_op_dict = {
                "axis": stage.op.axis,
                "reduce_axis": stage.op.reduce_axis
            }
            axis_tag_list = attr_dict[attr_name]
            attr_dict.setdefault("axis_info", {})
            for i, axis_tag in enumerate(axis_tag_list):
                axis_tag_str = axis_tag.replace(' ', '')
                if axis_tag_str in attr_dict["axis_info"] \
                        or "*" in axis_tag_str \
                        or "/" in axis_tag_str:
                    continue
                attr_dict["axis_info"][axis_tag_str] = \
                    stage_op_dict[attr_name][i].dom.extent


def gen_compute_tensor(sch, stages_info, op_name, input_tensor):
    """
    获取Stage的compute信息
    """
    reduce_axis_dict = {}
    attr_dict = {}
    c_op = comm.c_op_identify(sch)
    for stage_index, stage in enumerate(sch.stages):
        get_reduce_axis_dict(stage, stage_index, stages_info, reduce_axis_dict)
        get_attr_dict(stage, attr_dict)
        get_op_tensor(sch, stage, stage_index, stages_info, op_name, c_op,
                      input_tensor)
    return reduce_axis_dict, attr_dict


def gen_input_data_type(sch, input_tensor):
    """
    输入数据类型：3
    :return:
    """
    # 获取输入关系
    dict_fanin, _ = comm.get_depends(sch)
    # 获取所有Stages
    # 取前三个输入的Dtype
    for stage_index in range(len(sch.stages)):
        for i, input_stage_index in enumerate(dict_fanin[stage_index][:3]):
            dtype = sch.stages[input_stage_index].op.output(0).dtype
            input_tensor[stage_index][FeatureTensorCfg.input_dtype_s + i] \
                = DTYPE_INDEX.get(dtype, 0)


def gen_output_data_type(sch, input_tensor):
    """
    输出数据类型:3
    :return:
    """
    for stage_index, stage in enumerate(sch.stages):
        dtype = stage.op.output(0).dtype
        input_tensor[stage_index][FeatureTensorCfg.output_dtpye_s] \
            = DTYPE_INDEX.get(dtype, 0)


def gen_action_tensor(stage_num):
    """

    :param stage_num:
    :return:
    """
    return np.zeros((stage_num, 32), dtype=np.int32)


def parse_stages(sch,  # pylint: disable=R0914,R0912
                 inter_out_names, tiling_case=0):
    """
    根据Sch信息，分析出相应的feature,
    """
    stages_info = [{
        'name': stage.op.name,
        'tag': stage.op.tag,
        'ori_name': stage.op.name
    } for stage in sch.stages]

    # Stage依赖关系
    dict_fanin, dict_fanout = comm.get_depends(sch)

    c_op = comm.c_op_identify(sch)

    # 中间输出Stage和叶子Stage
    inter_out_stage_indices = []
    leaf_stage_indices = []
    l1fuse_leaf_stage_indices = []
    for i, stage in enumerate(sch.stages):
        if stage.op.name in inter_out_names:
            inter_out_stage_indices.append(i)
        if not dict_fanout[i]:
            leaf_stage_indices.append(i)
        elif c_op in comm.MAD_OP_ID_MAP.values() and \
                stage.op.tag == "conv_l1fuse_reshape":
            l1fuse_leaf = dict_fanin[i][0]
            l1fuse_leaf_stage_indices.append(l1fuse_leaf)
            stages_info[l1fuse_leaf].setdefault('type',
                                                []).append('l1fuse_leaf')

    # 所有叶子向上递归出各自的子树
    leaf_subtree = {}
    for leaf_stage_index in leaf_stage_indices:
        stages_info[leaf_stage_index].setdefault('type', []).append('leaf')
        if tiling_case > 0:
            stages_info[leaf_stage_index].setdefault('type', [])
            stages_info[leaf_stage_index]['type'].append('reduce_atomic')
        leaf_subtree[leaf_stage_index] = []
        comm.get_sub_tree(leaf_stage_index, dict_fanin,
                          leaf_subtree.get(leaf_stage_index))
        # 倒着排一下序，符合Topo排序
        leaf_subtree.get(leaf_stage_index).sort(reverse=True)

    # 然后看中间输出节点
    for index in inter_out_stage_indices:
        stages_info[index].setdefault('type', []).append('inter_out')

    # 然后看PlaceHolder
    tensors = [stage.op.output(0) for stage in sch.stages]
    for index, tensor in enumerate(tensors):
        if str(tensor.op).startswith('placeholder'):
            stages_info[index].setdefault('type', []).append('placeholder')

    return stages_info


def gen_virtual_leaf_out(leaf_outs: list, is_conv2d_l1fusion: bool = False) -> object:
    """
    :param leaf_outs:
    :return:
    """
    def _get_virtual_out_shape(leaf_outs):  # pylint: disable=R0912
        # 原始叶子输出shape完全一致时，直接看叶子输出的shape
        shape_set = set()
        for leaf_out in leaf_outs:
            shape_set.add(str(leaf_out.shape))
        if len(shape_set) == 1:
            return [x.value for x in leaf_outs[0].shape]

        # 获取所有的input
        input_shapes = []
        tensors = leaf_outs[:]
        all_tensors = leaf_outs[:]
        while tensors:
            new_tensors = []
            for tensor in tensors:
                if isinstance(tensor.op, tvm.PlaceholderOp):
                    input_shapes.append([int(axis) for axis in tensor.shape])
                    continue
                new_tensors.extend(tensor.op.input_tensors)
            tensors = list(set(new_tensors) - set(all_tensors))
            all_tensors.extend(tensors)

        # 获取最终virtual_out的shape：包含所有input的axis
        virtual_out_shape = input_shapes[0]
        for input_shape in input_shapes[1:]:
            if len(input_shape) > len(virtual_out_shape):
                virtual_out_shape = input_shape
            elif len(input_shape) == len(virtual_out_shape):
                # 取每个轴最大的值拼成shape
                for i, curr_value in enumerate(input_shape):
                    virtual_out_shape[i] = max(curr_value,
                                               virtual_out_shape[i])
        log.dbg("gen_virtual_leaf_out, virtual_out_shape:%s.", virtual_out_shape)
        return virtual_out_shape

    def _gen_virtual_leaf_out_by_broadcast(leaf_outs, virtual_out_shape) -> object:
        # 针对不keepdim
        import tbe  # pylint: disable=C0415

        def _get_broadcast_leaf_out(virtual_out_shape, index_list, leaf_out):
            broadcast_leaf_out = tvm.te.compute(
                virtual_out_shape,
                lambda *index: leaf_out(
                    *[x for i, x in enumerate(index) if i in index_list]),
                name='%s_broadcast' % leaf_out.name,
                tag='broadcast_for_tensor')
            return broadcast_leaf_out
        # 将leaf_outs broadcast到virtual_out的shap
        broadcast_leaf_outs = []
        for leaf_out in leaf_outs:
            leaf_out_shape = [int(axis_len) for axis_len in leaf_out.shape]
            if not set(leaf_out_shape).issubset(set(virtual_out_shape)):
                log.warn("The operator output tensor shapes are inconsistent, and tuning is not supported !")
                return None
            if leaf_out_shape == virtual_out_shape:
                broadcast_leaf_outs.append(leaf_out)
                continue
            if len(leaf_out_shape) == len(virtual_out_shape):
                # keepdim
                index_list = list(range(len(leaf_out_shape)))
            else:
                # 不keepdim
                index_list = []
                index_dict = {}
                for axis_len in leaf_out_shape:
                    index_dict.setdefault(axis_len, -1)
                    index = virtual_out_shape.index(axis_len,
                                                    index_dict.get(axis_len) + 1)
                    index_dict[axis_len] = index
                    index_list.append(index)
            broadcast_leaf_out = _get_broadcast_leaf_out(
                virtual_out_shape, index_list, leaf_out)
            broadcast_leaf_outs.append(broadcast_leaf_out)

        # broadcast后的leaf_outs相加得到virtual_out
        virtual_out = broadcast_leaf_outs[0]
        for broadcast_leaf_out in broadcast_leaf_outs[1:]:
            virtual_out = tbe.dsl.vadd(virtual_out, broadcast_leaf_out)
        return virtual_out

    def _gen_virtual_leaf_out(leaf_outs, virtual_out_shape):
        # 不支持不keepdim
        tensors = leaf_outs[:]
        all_tensors = leaf_outs[:]
        while tensors:
            new_tensors = []
            for tensor in tensors:
                if len(tensor.shape) != len(virtual_out_shape):
                    log.info("gen virtual_leaf_out by broadcast.")
                    # 尝试一下broadcast + add
                    return _gen_virtual_leaf_out_by_broadcast(
                        leaf_outs, virtual_out_shape)
                if not isinstance(tensor.op, tvm.PlaceholderOp):
                    new_tensors.extend(tensor.op.input_tensors)
            tensors = list(set(new_tensors) - set(all_tensors))
            all_tensors.extend(tensors)
        log.dbg("gen virtual_leaf_out not by broadcast.")

        def phony_insn_fuse(*indice):
            """
            :param indice:
            :return:
            """
            dtype = leaf_outs[0].dtype
            virtual_out = tvm.const(1, dtype)
            for leaf_out in leaf_outs:
                cur_index = []
                for i, virtual_out_shape_i in enumerate(virtual_out_shape):
                    if leaf_out.shape[i].value == virtual_out_shape_i:
                        cur_index.append(indice[i])
                    else:
                        cur_index.append(indice[i] % leaf_out.shape[i].value)
                virtual_out *= tvm.tir.Cast(dtype, leaf_out(*cur_index))
            return virtual_out

        with tvm.tag_scope(VIRTUAL_LEAF_OUT_TAG):
            virtual_out = tvm.te.compute(virtual_out_shape,
                                      phony_insn_fuse,
                                      name="virtual_leaf_out")
        return virtual_out

    def _process_doubleout_conv_l1_fusion(leaf_outs: list) -> object:
        """
        process the out tensors in conv + dequants16 + requant16 doubleout.
        """
        if leaf_outs[0].dtype == "int8":
            res_int8, res_int16 = leaf_outs
        else:
            res_int16, res_int8 = leaf_outs
        virtual_out_shape = res_int8.shape
        log.dbg("_process_doubleout_conv_l1_fusion: virtual_out_shape:%s", virtual_out_shape)

        def phony_insn_fuse(*indice):
            """
            :param indice:
            :return:
            """
            output_dtype = "int8"
            virtual_out = tvm.const(1, output_dtype)
            for leaf_out in leaf_outs:
                cur_index = []
                for i, virtual_out_shape_i in enumerate(virtual_out_shape):
                    if leaf_out.shape[i].value == virtual_out_shape_i:
                        cur_index.append(indice[i])
                    else:
                        cur_index.append(indice[i] % leaf_out.shape[i].value)
                virtual_out *= tvm.tir.Cast(output_dtype, leaf_out(*cur_index))
            return virtual_out

        with tvm.tag_scope(VIRTUAL_LEAF_OUT_TAG):
            virtual_out = tvm.te.compute(virtual_out_shape,
                                      phony_insn_fuse,
                                      name="virtual_leaf_out")
        log.dbg("_process_doubleout_conv_l1_fusion, virtual_out: %s", virtual_out)
        return virtual_out

    if is_conv2d_l1fusion:
        log.info("gen_virtual_leaf_out: the case is conv2d L1fusion.")
        virtual_out = _process_doubleout_conv_l1_fusion(leaf_outs)
    else:
        virtual_out_shape = _get_virtual_out_shape(leaf_outs)
        virtual_out = _gen_virtual_leaf_out(leaf_outs, virtual_out_shape)
    return virtual_out


def get_op_outputs(op_info):  # pylint: disable=R0912
    """

    :param output_tensors:
    :param op_info:
    :return:
    """
    from tbe.dsl.static_schedule.cce_schedule import \
        verify_compute_tensor  # pylint: disable=C0415
    from tbe.dsl.static_schedule.cce_schedule import \
        check_is_need_cast  # pylint: disable=C0415
    from tbe.dsl import cast_to # pylint: disable=C0415

    input_tensors = op_info["input_tensors"]
    mid_tensors = op_info["mid_tensors"]
    output_tensors = op_info["output_tensors"]
    compute_tensors = mid_tensors + output_tensors
    op_outputs = []
    for index, out in enumerate(output_tensors):
        # tuple_reduce_sum一个op可能有多个output
        if out.name.startswith("%s.v" % out.op.name):
            out_idx = out.name.split('.v')[-1]
            op_outputs.append('%s_v%s' % (out.op.name, out_idx))
        else:
            op_outputs.append(out.op.name)
        # 判断是否需要将输出数据转换成输入数据的类型
        need_cast_tags = ["matmul", "matmul_gemv", "elewise_binary_logic|and"]
        if (verify_compute_tensor(compute_tensors)) \
                and input_tensors \
                and (out.op.tag not in need_cast_tags):
            in_dtype = input_tensors[0].dtype
            if in_dtype in ("float32", "float16") and out.dtype == "int32":
                continue
            if in_dtype == 'bool':
                continue
            if not check_is_need_cast(out):
                continue
            if in_dtype and out.dtype != in_dtype:
                real_out = cast_to(out, in_dtype)
                output_tensors[index] = real_out
    return op_outputs


def get_tensors_code(op_outputs,
                     indent, output_op_names, op_list, output_tensor_names):
    """

    :param op_outputs:
    :param indent:
    :param output_op_names:
    :param op_list:
    :param output_tensor_names:
    :return:
    """
    import pickle  # pylint: disable=C0415
    # 创建default schedule
    sch = tvm.create_schedule(op_list)
    # 多个Tensor如果放在一起Dump的话，每个Tensor都会有自己完整的栈，
    # 而二者必然是有重叠的，所以就会有重复的Stage
    # 所以这里要改成Dump Schedule对象
    # 原始的output name 写入文件里面，用于debug manual获取完整的输出tensor列表
    sch_dump = pickle.dumps(sch, protocol=2)
    base_compute_code = """
    #op_outputs:%s
    import pickle
    sch = pickle.loads(%s)
    """ % (",".join(op_outputs), sch_dump)
    # 从Sch中提取输出
    code_lines = [
        base_compute_code,
        "%s%s, = sch.outputs" % (indent, ", ".join(output_op_names))
    ]
    output_tensor_name_idx = 0
    for op_idx, stage_op in enumerate(op_list):
        op_name = output_op_names[op_idx]
        for output_idx in range(stage_op.num_outputs):
            code_lines.append(
                '%s%s = %s.output(%d)' %
                (indent, output_tensor_names[output_tensor_name_idx], op_name,
                 output_idx))
            output_tensor_name_idx += 1
        # tuple_reduce_sum添加一下不带_v的
        if stage_op.num_outputs > 1:
            code_lines.append('%s%s = %s' %
                              (indent, stage_op.name,
                               output_tensor_names[output_tensor_name_idx -
                                                   stage_op.num_outputs]))
    return code_lines


def mark_stage_type(has_virtual_leaf_out,
                    ori_leaf_outs, leaf_outs, sch, stages_info):
    """

    :param has_virtual_leaf_out:
    :param ori_leaf_outs:
    :param leaf_outs:
    :param sch:
    :param stages_info:
    :return:
    """
    # 遍历所有的stage：
    # 1、找到原始叶子输出stage标记type为“origin_leaf_out”
    # 2、找到新添加的virtual_out的stage标记type为“virtual_leaf_out”
    c_op = comm.c_op_identify(sch)
    if c_op in comm.CONV_OP_ID_LIST:
        stage_num = len(sch.stages) - 1
        if sch.stages[stage_num].op.tag == "conv_virtual_res":
            stages_info[stage_num].setdefault('type',
                                              []).append('virtual_leaf_out')
    if has_virtual_leaf_out:
        ori_leaf_out_names = [x.name for x in ori_leaf_outs]
        leaf_out_names = [x.name for x in leaf_outs]
        for stage_index, stage in enumerate(sch.stages):
            if stage.op.output(0).name in ori_leaf_out_names:
                stages_info[stage_index].setdefault('type', []). \
                    append('origin_leaf_out')
            if stage.op.output(0).name in leaf_out_names:
                stages_info[stage_index].setdefault('type', []). \
                    append('virtual_leaf_out')


def add_virtual_leaf(output_tensors: list, leaf_outs: list,
                     add_virtual_leaf_out: bool = True, is_conv2d_l1fusion: bool = False) -> (bool, bool):
    """

    :param output_tensors:
    :param leaf_outs:
    :param add_virtual_leaf_out:
    :param is_conv2d_l1fusion:
    :return: has_virtual_leaf_out, gen_virtual_leaf_succ
    """
    # 如果有多个叶子输出，需要添加虚节点，tuple_reduce_sum不添加虚节点
    is_tuple_reduce_sum = False
    for leaf_out in leaf_outs:
        if 'tuple_reduce_sum' in leaf_out.op.tag:
            is_tuple_reduce_sum = True
    has_virtual_leaf_out = False
    if add_virtual_leaf_out and len(leaf_outs) > 1 and not is_tuple_reduce_sum:
        virtual_leaf_out = gen_virtual_leaf_out(leaf_outs, is_conv2d_l1fusion)
        if virtual_leaf_out is None:
            return has_virtual_leaf_out, False
        output_tensors = list(set(output_tensors) - set(leaf_outs))
        output_tensors.append(virtual_leaf_out)
        leaf_outs = [virtual_leaf_out]
        has_virtual_leaf_out = True

    return has_virtual_leaf_out, True


def gen_compute_code(output_tensors: list, add_virtual_leaf_out: bool = True,
                     tiling_case: int = 0, option: dict = None) -> tuple:
    """

    :param output_tensors:
    :param add_virtual_leaf_out:
    :param tiling_case:
    :return:
    """
    log.dbg("gen_compute_code: output_tensors:%s, add_virtual_leaf_out:%s, tiling_case:%s, option: %s.",
        output_tensors, add_virtual_leaf_out, tiling_case, option)
    from tbe.dsl.static_schedule.cce_schedule import \
        get_op_info  # pylint: disable=C0415
    # 多输出要求output_tensors是个list，不然会卡死
    if not isinstance(output_tensors, list):
        output_tensors = [output_tensors]

    # 有些输出数据与输入数据的类型不一致时，
    # 需要将输出数据的类型转换成输入数据的类型
    # 获取输入数据，中间数据以及输出数据
    op_info = get_op_info(output_tensors)
    output_tensors = op_info["output_tensors"]
    op_outputs = get_op_outputs(op_info)

    # 每行代码的缩进
    indent = '    '
    log.dbg("output_tensors: %s", output_tensors)
    leaf_outs, inter_outs = env_util.classify_outs(output_tensors)
    ori_leaf_outs = leaf_outs[:]

    is_conv2d_l1fusion = option.get("is_conv2d_l1fusion", False) if option else False
    has_virtual_leaf_out, gen_virtual_leaf_succ = add_virtual_leaf(output_tensors, leaf_outs,
                                                                   add_virtual_leaf_out, is_conv2d_l1fusion)
    if not gen_virtual_leaf_succ:
        return None, "", []

    # tuple_reduce_sum要避免添加重复的op
    op_list = []
    for leaf_out in leaf_outs:
        if leaf_out.op not in op_list:
            op_list.append(leaf_out.op)
    output_op_names = [op.name + '_op' for op in op_list]
    # tuple_reduce_sum一个op可能有多个output
    output_tensor_names = []
    for stage_op in op_list:
        if stage_op.num_outputs > 1:
            for idx in range(stage_op.num_outputs):
                output_tensor_names.append(stage_op.name + "_v%s" % idx)
        else:
            output_tensor_names.append(stage_op.name)

    code_lines = get_tensors_code(
        op_outputs, indent, output_op_names, op_list, output_tensor_names)

    # 1, 从output tensor递归遍历，还原compute 代码
    def rec_def_tensor(tensors, code_lines, visited_tensors):
        """
        :param tensors:
        :param code_lines:
        :param visited_tensors:
        """
        new_tensors = []
        for tensor in tensors:
            for i, input_tensor in enumerate(tensor.op.input_tensors):
                input_name = input_tensor.op.name
                code_line = "%s%s = %s.op.input_tensors[%s] # input tensor info: shape:%s, tag:%s, dtype:%s" % (
                    indent, input_name, tensor.op.name, i, input_tensor.shape, input_tensor.op.tag, input_tensor.dtype)
                if input_name in visited_tensors:
                    continue
                log.dbg("gen_compute_code, code_line: %s.", code_line)
                code_lines.append(code_line)
                visited_tensors.append(input_name)
                new_tensors.append(input_tensor)
        if new_tensors:
            rec_def_tensor(new_tensors, code_lines, visited_tensors)

    visited_tensors = []
    rec_def_tensor(leaf_outs, code_lines, visited_tensors)
    # 添加创建schedule代码
    sch_code_line = indent + "sch = tvm.create_schedule([%s])" % (
        ', '.join(output_op_names))
    code_lines.append(sch_code_line)
    compute_code = "\n".join(code_lines)
    # 创建default schedule
    sch = tvm.create_schedule(op_list)
    inter_out_names = [tensor.op.name for tensor in inter_outs]
    stages_info = parse_stages(sch, inter_out_names, tiling_case)
    # 遍历所有的stage：
    # 1、找到原始叶子输出stage标记type为“origin_leaf_out”
    # 2、找到新添加的virtual_out的stage标记type为“virtual_leaf_out”
    mark_stage_type(has_virtual_leaf_out,
                    ori_leaf_outs, leaf_outs, sch, stages_info)
    log.dbg("compute_code: %s", compute_code)
    return sch, compute_code, stages_info


def get_stage_shape(feature_tensor, stage_index):
    """
    :param feature_tensor:
    :param stage_index:
    :return:
    """
    if stage_index >= len(feature_tensor):
        return []
    shape = []
    for axis in feature_tensor[stage_index][0:AXIS_CNT]:
        if axis != 0:
            shape.append(axis)
        else:
            break
    return shape


@timer('tvm_compute_proc')
def proc(schedule_obj, stages_info, op_name):

    """
    :param schedule_obj:
    :param stages_info:
    :param op_name:
    :return:
    """
    # 指定一个运行目录, 把Libs放在这个运行目录下, 配置环境变量
    # 把上述代码，写到运行目录下的一个.py文件里，并在该文件最后加一行return s
    # 运行上述代码，拿到它的返回值：s
    # 根据s的stages，以及stage的axis, reduce_axis，生成
    # 输入是tvm的compute代码，输出是schedule_tensor
    stage_num = len(schedule_obj.stages)
    input_tensor = np.zeros([stage_num, FeatureTensorCfg.featurn_len],
                            dtype=np.int32)
    gen_axis_tensor(schedule_obj, input_tensor)
    reduce_axis_dict, attr_dict = gen_compute_tensor(
        schedule_obj, stages_info, op_name, input_tensor)
    gen_depends_tensor(schedule_obj, input_tensor)
    gen_input_data_type(schedule_obj, input_tensor)
    gen_output_data_type(schedule_obj, input_tensor)
    return input_tensor, attr_dict, reduce_axis_dict
