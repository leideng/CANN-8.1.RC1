#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
import gzip
import hashlib
import os
import pathlib
import pickle
import random
import time
from abc import abstractmethod

import fcntl
import numpy as np

from schedule_search import config
from schedule_search import log
from schedule_search import soc_cfg
from schedule_search.config import WORKSPACE
from schedule_search.ts_env import env_util
from schedule_search.ts_env.env_classes import LocalLock
from schedule_search.util import HASH_LEN
from schedule_search.util import ensure_dir_exists
from schedule_search.util import create_dir

TICK_CACHE_VERSION = "v330"
LOCAL_CACHE_VERSION = "v008"
# 设置过期时间6*30天
EXPIRE_SECONDS = 15552000

# 是否读写cache分成4个模式
CACHE_MODE_NONE = '0'
CACHE_MODE_ONLY_READ = '1'
CACHE_MODE_ONLY_UPDATE = '2'
CACHE_MODE_READ_UPDATE = '3'

# schedule的本地缓存可以有多种，根据走的t2c范围不同区分
# 获取tick时需要的schedule对象，会走所有t2c
CACHE_SCHEDULE_TICK = 'cache_schedule_tick'
# 进行storage_align时需要的schedule对象，会走storage_align之前所有的t2c
CACHE_SCHEDULE_STORAGE_ALIGN = 'cache_schedule_storage_align'
# tuple_reduce_sum判断是否使用二分法需要的schedule对象，会走at之前所有的t2c
CACHE_SCHEDULE_TUPLE_REDUCE_SUM = 'cache_schedule_tuple_reduce_sum'


class BaseTickCacheManager:
    """
    BaseTickCacheManager
    """
    def __init__(self,
                 op_schedule_info,
                 clean_action_tensor,
                 schedule_code,
                 mode=CACHE_MODE_READ_UPDATE):
        """
        Constructor
        """

        self.tick_type = "job_profiling"

        feature_tensor = op_schedule_info.feature_tensor
        check_output = op_schedule_info.check_output
        hash_md5 = hashlib.sha256()
        if isinstance(feature_tensor, list):
            hash_md5.update(str(feature_tensor).encode())
        else:
            hash_md5.update(
                str(feature_tensor.astype(np.int32).tolist()).encode())

        if isinstance(clean_action_tensor, list):
            hash_md5.update(str(clean_action_tensor).encode())
        else:
            hash_md5.update(
                str(clean_action_tensor.astype(np.int32).tolist()).encode())

        self.md5_str = hash_md5.hexdigest()[:HASH_LEN]
        product_version = soc_cfg.get_soc_version()
        op_md5 = op_schedule_info.op_md5
        '''
        cache存储的key为：
        version@md5(str(feature) + str(clean action))@tick_type@check_output
        @product@op_md5,
                  value为：ret@tick
        '''
        key_list = [
            TICK_CACHE_VERSION, self.md5_str, self.tick_type,
            str(check_output),
            str(product_version), op_md5
        ]
        self.key = "@".join(key_list)
        self.mode = mode
        self.store_tmp_dir = op_schedule_info.store_dir
        self.schedule_code = schedule_code
        self.op_schedule_info = op_schedule_info

    @staticmethod
    def _read_cache():
        # 读取cache操作，子类中去实现
        return False, None

    def read_cache(self):
        """
        read_cache
        :return:
        """
        if self.mode in [CACHE_MODE_NONE, CACHE_MODE_ONLY_UPDATE]:
            return True, []
        ret, cache_result = self._read_cache()
        # cache成功的场景需要保存下来对应的action tensor到本地
        if ret and cache_result is not None and cache_result[0] == "succ":
            store_dir = os.path.join(self.store_tmp_dir, "run_succ")
            create_dir(store_dir)
            _, tick = cache_result
            # 只有成功的才会保存cache
            unique_id = str(os.getpid()) + "_" + str(int(
                time.time() * 1000)) + "_" + self.md5_str
            op_name = self.op_schedule_info.option.get("op_name")
            op_file = os.path.join(
                store_dir, "%d_%s_cached_%s.py" % (tick, op_name, unique_id))
            env_util.gen_schedule_py(self.op_schedule_info, self.schedule_code,
                                     unique_id, op_file)
        return ret, cache_result

    def update_cache(self, value):
        """

        :param value:
        :return:
        """
        if self.mode in [CACHE_MODE_NONE, CACHE_MODE_ONLY_READ]:
            return True
        return self._update_cache(value)

    @abstractmethod
    def _update_cache(self, value):
        """
        _update_cache
        :param value:
        :return:
        """
        # 更新cache操作，子类中去实现
        return False


class SqliteTickCacheManager(BaseTickCacheManager):  # pylint: disable=R0902
    """
    SqliteTickCacheManager
    """
    def __init__(self,
                 op_schedule_info,
                 clean_action_tensor,
                 op_code_str,
                 mode=CACHE_MODE_READ_UPDATE):
        """
        Constructor
        """
        # for rule 6.7
        self.tick_type = None
        self.md5_str = None
        self.key = None
        self.mode = None
        self.store_tmp_dir = None
        self.schedule_code = None
        self.op_schedule_info = None
        BaseTickCacheManager.__init__(self,
                                      op_schedule_info,
                                      clean_action_tensor,
                                      op_code_str,
                                      mode=mode)

        if mode == CACHE_MODE_NONE:
            return
        # 调用sqlite接口
        import sqlite3  # pylint: disable=C0415
        op_name = op_schedule_info.option.get('op_name', "default")
        self.conn = sqlite3.connect(  # pylint: disable=no-member
            os.path.join(WORKSPACE, "%s_cache.db" % op_name))
        self.lock_file = os.path.join(WORKSPACE, "%s_sqlite.lock" % op_name)
        file_obj = pathlib.Path(self.lock_file)
        if not file_obj.exists():
            file_obj.touch()
        table_name = op_name + "_" + op_schedule_info.op_md5
        # 创建表格
        create_tb_cmd = '''
        CREATE TABLE IF NOT EXISTS %s
        (id INTEGER PRIMARY KEY,
        key TEXT UNIQUE,
        value TEXT);
        ''' % table_name
        self.cursor = self.conn.cursor()
        local_lock = LocalLock(self.lock_file)
        local_lock.lock()
        self.cursor.execute(create_tb_cmd)
        self.conn.commit()
        local_lock.unlock()
        self.table = table_name

    def __del__(self):
        self.cursor.close()
        self.conn.close()

    def _read_cache(self):
        if self.cursor is None:
            return False, None
        query_cmd = "select * from %s where key='%s'" % (self.table, self.key)
        log.dbg('query_cmd: %s!', query_cmd)
        self.cursor.execute(query_cmd)
        item_tuple = self.cursor.fetchone()
        if item_tuple is None:
            return True, None
        _, _, value = item_tuple
        ret, tick = value.split("@")
        log.dbg('get key: %s, value: %s succ!', self.key, value)
        return True, (ret, int(float(tick)))

    def _update_cache(self, value):
        if self.cursor is None:
            return False
        inser_cmd = "insert or replace into %s values(NULL,'%s','%s');" % (
            self.table, self.key, value)
        log.dbg('inser_cmd: %s!', inser_cmd)
        local_lock = LocalLock(self.lock_file)
        local_lock.lock()
        self.cursor.execute(inser_cmd)
        self.conn.commit()
        local_lock.unlock()
        log.dbg('update key: %s, value: %s succ!', self.key, value)
        return True



class TickCacheFactory:
    """
    TickCacheFactory
    """
    @staticmethod
    def get_cache_manager(op_schedule_info,
                          clean_action_tensor,
                          op_code_str,
                          mode=CACHE_MODE_READ_UPDATE):
        """
        get_cache_manager
        :param op_schedule_info:
        :param clean_action_tensor:
        :param op_code_str:
        :param mode:
        :return:
        """
        cache_type = op_schedule_info.option.get('cache_type', "sqlite")
        if cache_type == 'sqlite':
            return SqliteTickCacheManager(op_schedule_info,
                                          clean_action_tensor,
                                          op_code_str,
                                          mode=mode)
        return BaseTickCacheManager(op_schedule_info,
                                    clean_action_tensor,
                                    op_code_str,
                                    mode=mode)

    def useless_func1(self):
        """

        :return:
        """
        return self.__str__()


class LocalCacheManager:
    """
    LocalCacheManager
    """
    def __init__(self, cache_key, cache_name, mode=CACHE_MODE_READ_UPDATE):
        self.cache_key = LOCAL_CACHE_VERSION + "@" + cache_key
        self.cache_name = cache_name
        self.fname = os.path.join(config.WORKSPACE, "replay_dir",
                                  self.cache_name, self.cache_key)
        self.lock_file = os.path.join(config.WORKSPACE, "replay_dir",
                                      self.cache_name,
                                      self.cache_key + ".lock")
        ensure_dir_exists(os.path.dirname(self.fname), reset=False)
        self.mode = mode

    def read_cache(self, retry_times=3):
        """

        :param retry_times:
        :return:
        """
        # 尝试3次
        if self.mode in [CACHE_MODE_NONE, CACHE_MODE_ONLY_UPDATE]:
            return None
        if not os.path.exists(self.fname):
            return None
        output = None
        # 为了防止多进程写的时候，将出现读异常，增加retry机制
        for _ in range(retry_times):
            try:
                with gzip.open(self.fname, "rb", compresslevel=1) \
                        as file_handler:
                    output = pickle.load(file_handler)
                # 成功读写则退出
                break
            except EOFError as exception:
                log.warn('can not pickle load successfully. %s', exception)
                if not os.path.exists(self.fname):
                    return None
        return output

    def update_cache(self, cache_content):
        """

        :param cache_content:
        :return:
        """
        # 更新之前先拿锁
        if self.mode in [CACHE_MODE_NONE, CACHE_MODE_ONLY_READ]:
            return
        with open(self.lock_file, 'w') as file_h:
            fid = file_h.fileno()
            fcntl.lockf(fid, fcntl.LOCK_EX)
            with gzip.open(self.fname, "wb", compresslevel=1) as file_handler:
                pickle.dump(cache_content, file_handler)
            fcntl.flock(fid, fcntl.LOCK_UN)


class StateCacheManager(LocalCacheManager):
    """
    StateCacheManager
    """
    def __init__(self,
                 feature_tensor,
                 action_tensor,
                 cache_name='state_cache',
                 mode=CACHE_MODE_READ_UPDATE):
        # state太大了，暂时不用redis存放，先放本地
        hash_md5 = hashlib.sha256()
        hash_md5.update(str(feature_tensor).encode())
        hash_md5.update(str(action_tensor).encode())
        cache_key = hash_md5.hexdigest()[:HASH_LEN]
        LocalCacheManager.__init__(self, cache_key, cache_name, mode)


class ScheduleCacheManager(LocalCacheManager):
    """
    ScheduleCacheManager
    """
    def __init__(self,
                 feature_tensor,
                 action_tensor,
                 cache_name='schedule_cache',
                 mode=CACHE_MODE_READ_UPDATE):
        hash_md5 = hashlib.sha256()
        hash_md5.update(str(feature_tensor).encode())
        hash_md5.update(str(action_tensor).encode())
        cache_key = hash_md5.hexdigest()[:HASH_LEN]
        LocalCacheManager.__init__(self, cache_key, cache_name, mode)
