#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.

rl schedule search, tss
"""
from schedule_search import log
from schedule_search import soc_cfg
from schedule_search import util
from schedule_search.cce_intrin_map import OP_INTRIN_KEY_TAG
from tbe import tvm

BROADCAST_ENHANCE_INSN_MAP = {
    "vector_mul": "vector_mul_with_broadcast_enhance",
    "vector_div": "vector_div_with_broadcast_enhance",
    "vector_add": "vector_add_with_broadcast_enhance",
    "vector_sub": "vector_sub_with_broadcast_enhance",
    "vector_min": "vector_min_with_broadcast_enhance"
}


def is_broadcast_nist(tensor):
    """

    :param tensor:
    :return:
    """
    # 最后一根轴不是broadcast轴，则返回True
    if tensor.op.tag == "broadcast_for_tensor":
        # broadcast not last axis
        if list(tensor.op.input_tensors):
            original_tensor = tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            broadcast_shape = util.shape_to_list(tensor.shape)
            log.dbg("original_shape: %s, broadcast_shape: %s", original_shape,
                    broadcast_shape)
            if original_shape[-1] == broadcast_shape[-1]:
                # not include (1,1,1,1,1,1,1)->(10, 10, 5, 2, 3, 9,1)
                if sum(original_shape[:]) != len(original_shape):
                    return True
    return False


def is_scale_broadcast(tensor):
    """

    :param tensor:
    :return:
    """
    # 全1的shape做broadcast，则返回True，
    # 例如(1,1,1,1,1,1,1)->(10, 10, 5, 2, 3, 9,1)
    if tensor.op.tag == "broadcast_for_tensor":
        if list(tensor.op.input_tensors):
            original_tensor = tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            if sum(original_shape[:]) == len(original_shape):
                return True
    return False


def is_broadcast_last(tensor):
    """
    支持(1,1,1,1,1,1,1)->(10, 10, 5, 2, 3, 9,1)
    支持(3,1,1) -> (3,2,1)
    :param tensor:
    :return:
    """
    # 最后一根轴是broadcast轴，则返回True
    if tensor.op.tag == "broadcast_for_tensor":
        # broadcast not last axis
        if list(tensor.op.input_tensors):
            original_tensor = tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            broadcast_shape = util.shape_to_list(tensor.shape)
            log.dbg("original_shape: %s, broadcast_shape: %s", original_shape,
                    broadcast_shape)
            if original_shape[-1] == 1 and broadcast_shape[-1] != 1:
                return True

            for i in reversed(range(len(original_shape))):
                if original_shape[i] != 1 and broadcast_shape[i] != 1:
                    return False
                if original_shape[i] == 1 and broadcast_shape[i] != 1:
                    return True
    return False


def is_broadcast_multi_axes(tensor):
    """

    :param tensor:
    :return:
    """
    # broadcast多根轴，则返回True
    if tensor.op.tag == "broadcast_for_tensor":
        if list(tensor.op.input_tensors):
            original_tensor = tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            broadcast_shape = util.shape_to_list(tensor.shape)
            broadcast_axis_number = 0
            for i in range(len(original_shape) - 1, -1, -1):
                if (original_shape[i] != broadcast_shape[i]) and \
                        (original_shape[i] == 1):
                    broadcast_axis_number = broadcast_axis_number + 1
                if broadcast_axis_number >= 2:
                    return True
    return False


def get_broadcast_tensor(schedule):
    """

    :param schedule:
    :return:
    """
    # 获取所有的broadcast tensor，有reduce就返回空
    broadcast_last_tensors = []
    broadcast_nist_tensors = []
    tensors = [
        schedule.outputs[0].output(idx)
        for idx in range(schedule.outputs[0].num_outputs)
    ]
    all_tensors = tensors[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if tensor.op.tag.find('reduce') >= 0:
                return None, None
            if tensor.op.tag.find('broadcast') >= 0:
                if is_broadcast_last(tensor):
                    broadcast_last_tensors.append(tensor)
                elif is_broadcast_nist(tensor):
                    broadcast_nist_tensors.append(tensor)
            if not isinstance(tensor.op, tvm.PlaceholderOp):
                new_tensors.extend(tensor.op.input_tensors)
        tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(tensors)
    log.dbg("broadcast_last_tensors: %s, broadcast_nist_tensors: %s",
            broadcast_last_tensors, broadcast_nist_tensors)
    return broadcast_last_tensors, broadcast_nist_tensors


def get_last_broadcast_axis(broadcast_tensor):
    """

    :param broadcast_tensor:
    :return:
    """
    # 获取最后一根broadcast轴
    last_broadcast_axis = -1
    if list(broadcast_tensor.op.input_tensors):
        original_tensor = broadcast_tensor.op.input_tensors[0]
        original_shape = util.shape_to_list(original_tensor.shape)
        broadcast_shape = util.shape_to_list(broadcast_tensor.shape)
        for i in range(len(original_shape) - 1, -1, -1):
            if original_shape[i] == 1 \
                    and original_shape[i] != broadcast_shape[i]:
                last_broadcast_axis = i
                break
    return last_broadcast_axis


def get_max_last_broadcast_axis(broadcast_tensors):
    """

    :param broadcast_tensors:
    :return:
    """
    # 输入的tensors，每一个都获取最后一根broadcast轴，返回max
    max_broadcast_axis = 0
    for broadcast_tensor in broadcast_tensors:
        last_broadcast_axis = get_last_broadcast_axis(broadcast_tensor)
        if last_broadcast_axis > max_broadcast_axis:
            max_broadcast_axis = last_broadcast_axis
    return max_broadcast_axis


def get_last_none_broadcast_axis(broadcast_tensor):
    """

    :param broadcast_tensor:
    :return:
    """
    # 获取最后一根非broadcast轴
    last_none_broadcast_axis = -1
    if list(broadcast_tensor.op.input_tensors):
        original_tensor = broadcast_tensor.op.input_tensors[0]
        original_shape = util.shape_to_list(original_tensor.shape)
        broadcast_shape = util.shape_to_list(broadcast_tensor.shape)
        for i in range(len(original_shape) - 1, -1, -1):
            if original_shape[i] == broadcast_shape[i] \
                    and original_shape[i] != 1:
                last_none_broadcast_axis = i
                break
    return last_none_broadcast_axis


def get_last_no_bc_axis(broadcast_tensors):
    """

    :param broadcast_tensors:
    :return:
    """
    # 输入的tensors，每一个都获取最后一根非broadcast轴，返回max
    max_none_broadcast_axis = 0
    for broadcast_tensor in broadcast_tensors:
        last_none_broadcast_axis \
            = get_last_none_broadcast_axis(broadcast_tensor)
        if last_none_broadcast_axis > max_none_broadcast_axis:
            max_none_broadcast_axis = last_none_broadcast_axis
    return max_none_broadcast_axis


def is_special_broadcast_nist(  # pylint: disable=R0911,R0912
        broadcast_nist_tensors,
        op_schedule_info):
    """

    :param broadcast_nist_tensors:
    :param op_schedule_info:
    :return:
    """
    for broadcast_tensor in broadcast_nist_tensors:
        if not list(broadcast_tensor.op.input_tensors):
            return False
        original_tensor = broadcast_tensor.op.input_tensors[0]
        original_shape = util.shape_to_list(original_tensor.shape)
        if original_shape[-1] > 64:
            return False
        for i in range(0, len(original_shape) - 1, 1):
            if original_shape[i] == 1:
                continue
            if original_shape[i] != 1:
                # WILLDO: process like (32,1,4) (32,68,4) scene
                # process like (32,32,32,3,2) (1,1,1,3,2) scene
                if i == len(original_shape) - 2 and original_shape[i + 1] != 1:
                    last_dim = original_shape[-1]
                    second_last_dim = original_shape[-2]
                    if last_dim * second_last_dim < 16:
                        continue
                    return False
                return False
            return False
        # like xdiv operator, broadcast destination are vadd and vabs,
        # as long as one of the destination is not supported broadcast
        # enhance, disable broadcast enhance function
        # 注意，这里默认没做cache read/write，feature里面的compute默认都是0
        is_input_tensor = False
        for stage in op_schedule_info.schedule_obj.stages:
            if broadcast_tensor in stage.op.input_tensors:
                is_input_tensor = True
                op_tag = stage.op.tag.split("|")[0]
                log.dbg("op_tag: %s", op_tag)
                if op_tag not in OP_INTRIN_KEY_TAG:
                    return False
                intrinsic_func_name = OP_INTRIN_KEY_TAG[op_tag].intrin
                log.dbg("intrinsic_func_name: %s", intrinsic_func_name)
                if intrinsic_func_name not in BROADCAST_ENHANCE_INSN_MAP:
                    return False
        if not is_input_tensor:
            return False

    return True


def is_less_32core_bc_out(broadcast_nist_tensors):
    """

    :param broadcast_nist_tensors:
    :return:
    """
    # WILLDO: 这个函数还有问题，没有使能
    # (a, b, 1, 4) -> (a, b, 2, 4) where a <= 32, 16 <= b <= 4080
    if not broadcast_nist_tensors:
        return False

    is_out = False
    for broadcast_tensor in broadcast_nist_tensors:
        if list(broadcast_tensor.op.input_tensors):
            original_tensor = broadcast_tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            broadcast_shape = util.shape_to_list(broadcast_tensor.shape)
            # (x <= 32, x, x, x) only
            if original_shape[0] > 32:
                continue
            # (x <= 32, x, x, 4) only
            if original_shape[-1] != 4:
                continue
            # fp32 only
            if original_tensor.dtype != "float32":
                continue
            # (x <= 32, x, 1, 4) to (x <= 32, x, 2, 4) only
            if original_shape[-2] != 1 or broadcast_shape[-2] != 2:
                continue
            # data per core must be larger than MULTI_CORE_THRESHOLD
            if original_shape[-3] \
                    * original_shape[-1] \
                    * util.get_dtype_size(original_tensor.dtype) < 1024:
                continue
            # data per core must be smaller than max_repeat
            if original_shape[-3] \
                    * original_shape[-1] \
                    * util.get_dtype_size(original_tensor.dtype) > 255 * 32:
                continue
    return is_out


def is_special_broadcast_sence(broadcast_nist_tensors, broadcast_last_tensors,
                               op_schedule_info):
    """

    :param broadcast_nist_tensors:
    :param broadcast_last_tensors:
    :param op_schedule_info:
    :return:
    """
    # 不是cloud，不处理
    product = soc_cfg.get_soc_version()
    if product not in [soc_cfg.SocVersion.ASCEND910]:
        log.dbg("not cloud, no need atomic.")
        return None
    # 有broadcast nist且无broadcast last才处理
    if not broadcast_nist_tensors or broadcast_last_tensors:
        return None
    if is_special_broadcast_nist(broadcast_nist_tensors, op_schedule_info):
        return "special_broadcast_nist"
    if is_less_32core_bc_out(broadcast_nist_tensors):
        return "less_32_core_middle_broadcast_out"
    return None
