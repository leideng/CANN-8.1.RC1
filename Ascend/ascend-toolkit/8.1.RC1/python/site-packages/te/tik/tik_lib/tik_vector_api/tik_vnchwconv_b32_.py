#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vnchwconv_b32_.py
DESC:     provide vector instructions
CREATED:  2021-09-8 18:53:42
"""

from tbe.common.platform import scope_ubuf
from tbe.tik.common.common_util import DTYPE_SIZE
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.tik_get_soc_name import is_compatible_mode
from tbe.tik.debug.tik_vector_ops_debug.tik_vnchwconv_b32_debug import vnchwconv_b32_decorator
from tbe.tik.tik_lib.tik_vector_api.tik_vector_fills_api_ import VnchwconvApi
from tbe.tik.tik_lib.tik_params import BIT_LEN_16
from tbe.tik.tik_lib.tik_params import BIT_LEN_32
from tbe.tik.tik_lib.tik_params import BIT_LEN_8


class VnchwconvB32Api(VnchwconvApi):
    """
    vector vnchwconv b32 ops
    """

    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        self.vnchwconv_check_obj.check_all()
        with self.tik_instance.context.freeze():
            float16_shape = (16, 16)
            dst_tensor_list = []
            for i in range(16):
                dst_tensor_list.append(self.tik_instance.Tensor("float16", float16_shape,
                                                                name="dst%s_ub" % i, scope=scope_ubuf))
            dst_shape = (32, 16)
            dst_ub_fp16_high = self.tik_instance.Tensor("float16", dst_shape, name="dst_ub_fp16_high",
                                                        scope=scope_ubuf)
            dst_ub_fp16_lower = self.tik_instance.Tensor("float16", dst_shape, name="dst_ub_fp16_lower",
                                                         scope=scope_ubuf)
        self.v100_b32_gen_code(dst_tensor_list, dst_ub_fp16_high, dst_ub_fp16_lower)
        self.tik_instance.set_high_level_api_state()

    @vnchwconv_b32_decorator
    def v100_b32_gen_code(self, dst_tensor_list, dst_ub_fp16_high, dst_ub_fp16_lower):
        """
        v100_b32 code gen

        Parameter
        ----------
        dst_tensor_list: 16 intermediate tensor for transposition
        dst_ub_fp16_high: high half tensor used to transpose
        dst_ub_fp16_lower: lower half tensor used to transpose

        Returns
        ----------
        None
        """
        repeat_times_16 = 16
        repeat_times_8 = 8
        vnchwconv_b32_sid = 0
        vnchwconv_b32_nburst_1 = 1
        vnchwconv_b32_burst = 1
        vnchwconv_b32_stride = 0
        # code gen
        with self.tik_instance.context.freeze():
            src_list = self.src_tensor_op.tensor_obj
            src_list_b16 = []
            for i in range(repeat_times_16):
                offset = self.tik_instance.Scalar(init_value=self.src_tensor_op.tensor_obj[i].offset)
                ub = src_list[0].reinterpret_cast_to("float16")
                src_list_b16.append(ub[offset * 2])

            config = [self._dtype_convert(1, "int64"), 0, 0]
            extents = self._get_extents(1, 0, 0)
            dst_ub_b32_type = self.dst_tensor_op.tensor_obj[0].dtype
            dst_ub_result_list = []
            with self.tik_instance.for_range(0, self.control_op.repeat_times) as imm_k:
                for i in range(repeat_times_16):
                    src_list_b16[i] = src_list_b16[i][(imm_k * self.src_tensor_op.rep_stride * BIT_LEN_16):]
                    dst_ub_result_list.append(
                        self.dst_tensor_op.tensor_obj[i][(imm_k * self.dst_tensor_op.rep_stride * BIT_LEN_8):])
                # Perform 16 transposes, and move every 16 lines to 16 tensors.
                for i in range(repeat_times_16):
                    src1_list = [src_list_b16[i] for _ in range(repeat_times_16)]
                    dst1_list = [dst_tensor_list[i][BIT_LEN_16 * j] for j in range(repeat_times_16)]
                    self.tik_instance.config_vas(
                        [dst1_list, src1_list], 'f16f16', config, extents)
                # Divide the first column of the 16 tensors into two parts and transpose them to a 32 * 16 tensor
                for i in range(repeat_times_8):
                    src2_list_high = [dst_tensor_list[j // 2][(j % 2) * BIT_LEN_16 + BIT_LEN_32 * i]
                                      for j in range(repeat_times_16)]
                    dst2_list_high = [dst_ub_fp16_high[j * BIT_LEN_16 + i * BIT_LEN_16] for j in range(repeat_times_16)]
                    self.tik_instance.config_vas(
                        [dst2_list_high, src2_list_high], 'f16f16', config, extents)
                    src2_list_lower = [dst_tensor_list[BIT_LEN_8 + j // 2][(j % 2) * BIT_LEN_16 + BIT_LEN_32 * i]
                                       for j in range(repeat_times_16)]
                    dst2_list_lower = [dst_ub_fp16_lower[j * BIT_LEN_16 + i * BIT_LEN_16]
                                       for j in range(repeat_times_16)]
                    self.tik_instance.config_vas(
                        [dst2_list_lower, src2_list_lower], 'f16f16', config, extents)

                for i in range(repeat_times_8):
                    dst_ub_fp32_high = dst_ub_fp16_high.reinterpret_cast_to(dst_ub_b32_type)
                    dst_ub_fp32_lower = dst_ub_fp16_lower.reinterpret_cast_to(dst_ub_b32_type)

                    self.tik_instance.data_move(dst_ub_result_list[2 * i], dst_ub_fp32_high[BIT_LEN_8 * i],
                                                vnchwconv_b32_sid, vnchwconv_b32_nburst_1, vnchwconv_b32_burst,
                                                vnchwconv_b32_stride, vnchwconv_b32_stride)
                    self.tik_instance.data_move(dst_ub_result_list[2 * i + 1],
                                                dst_ub_fp32_lower[BIT_LEN_8 * i], vnchwconv_b32_sid,
                                                vnchwconv_b32_nburst_1, vnchwconv_b32_burst,
                                                vnchwconv_b32_stride, vnchwconv_b32_stride)


class VnchwconvNanoApi(VnchwconvApi):
    """
    vector vnchwconv b32 ops
    """

    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        compatible = is_compatible_mode()
        self.vnchwconv_check_obj.check_all()
        if not compatible:
            self.blklen_16_gen_code((self.dst_tensor_op.rep_stride, self.src_tensor_op.rep_stride),
                                    self.control_op.repeat_times, self.dst_tensor_op.tensor_obj,
                                    self.src_tensor_op.tensor_obj)
        elif compatible and self.dst_tensor_op.tensor_obj[0].dtype in ["int16", "uint16", "float16"]:
            self.blklen_32_b16_gen_code()
        elif compatible and self.dst_tensor_op.tensor_obj[0].dtype in ["int8", "uint8"]:
            self.blklen_32_b8_gen_code()
        self.tik_instance.set_high_level_api_state()

    def add_dummy_ub(self):
        """
        If src and dst are the same tensor, add a dummy ub

        Returns
        ----------
        None
        """
        if self.src_tensor_op.tensor_obj[0].is_same_tensor(self.dst_tensor_op.tensor_obj[0]):
            one_blk_ele_32 = 32 // DTYPE_SIZE[self.src_tensor_op.tensor_obj[0].dtype]
            ele_nums = reduce_mul(self.dst_tensor_op.tensor_obj[0].original_shape)
            aligned_ele_num = (ele_nums + one_blk_ele_32 - 1) // one_blk_ele_32 * one_blk_ele_32
            dummy_ub = self.tik_instance.Tensor(self.dst_tensor_op.tensor_obj[0].dtype,
                                                (aligned_ele_num,), name="dummy_ub", scope=scope_ubuf)
            self.tik_instance.data_move(dummy_ub, self.src_tensor_op.tensor_obj[0].last_tensor, 0,
                                        1, aligned_ele_num // one_blk_ele_32, 0, 0)
            src_list = []
            for i in range(16):
                dummy_offset = self.src_tensor_op.tensor_obj[i].offset
                src_list.append(dummy_ub[dummy_offset:])
            return src_list
        else:
            return self.src_tensor_op.tensor_obj

    def blklen_32_b8_gen_code(self):
        """
        compatible, b8 code gen

        Returns
        ----------
        None
        """
        original_src_list = self.add_dummy_ub()
        one_blk_ele = 16 // DTYPE_SIZE[self.src_tensor_op.tensor_obj[0].dtype]
        src_interval_elements = self.src_tensor_op.rep_stride * one_blk_ele * 2
        dst_interval_elements = self.dst_tensor_op.rep_stride * one_blk_ele * 2
        dst_list = []
        src_list = []

        scalar_offset = self.tik_instance.Scalar(name="scalar_offset", init_value=one_blk_ele)

        for i in range(16):
            if self.src_high_half:
                src_list.append(original_src_list[i][scalar_offset])
            else:
                src_list.append(original_src_list[i])
            if self.dst_high_half:
                dst_list.append(self.dst_tensor_op.tensor_obj[i][scalar_offset])
            else:
                dst_list.append(self.dst_tensor_op.tensor_obj[i])

        with self.tik_instance.for_range(0, self.control_op.repeat_times) as i:
            new_src_list = [src[src_interval_elements * i:] for src in src_list]
            new_dst_list = [dst[dst_interval_elements * i:] for dst in dst_list]
            self.blklen_16_gen_code((0, 0), 1, new_dst_list, new_src_list)
            self.blklen_16_gen_code((0, 0), 1, new_dst_list, new_src_list)

    def blklen_32_b16_gen_code(self):
        """
        compatible, b16 code gen

        Returns
        ----------
        None
        """
        original_src_list = self.add_dummy_ub()
        one_blk_ele = 16 // DTYPE_SIZE[self.src_tensor_op.tensor_obj[0].dtype]
        src_interval_elements = self.src_tensor_op.rep_stride * one_blk_ele * 2
        dst_interval_elements = self.dst_tensor_op.rep_stride * one_blk_ele * 2
        dst_list1 = []
        dst_list2 = []

        scalar_one_blk_ele = self.tik_instance.Scalar(init_value=one_blk_ele)

        for i in range(16):
            if i < 8:
                dst_list1.append(self.dst_tensor_op.tensor_obj[i])
                dst_list1.append(self.dst_tensor_op.tensor_obj[i][scalar_one_blk_ele:])
            else:
                dst_list2.append(self.dst_tensor_op.tensor_obj[i])
                dst_list2.append(self.dst_tensor_op.tensor_obj[i][scalar_one_blk_ele:])

        with self.tik_instance.for_range(0, self.control_op.repeat_times) as i:
            new_src_list = [src[src_interval_elements * i:] for src in original_src_list]
            new_src_list2 = [src[src_interval_elements * i + one_blk_ele:] for src in original_src_list]

            dst_list = [dst[dst_interval_elements * i:] for dst in dst_list1]
            dst_list2 = [dst[dst_interval_elements * i:] for dst in dst_list2]

            self.blklen_16_gen_code((0, 0), 1, dst_list, new_src_list)
            self.blklen_16_gen_code((0, 0), 1, dst_list2, new_src_list2)

    def blklen_16_gen_code(self, all_rep_stride, repeat_times, dst_list, src_list):
        """
        not compatible, b8 and b16, gen_code

        Parameter
        ----------
        all_rep_stride: offset of dst & src operator in the same block between adjacent iterations
        repeat_times: repeat times
        dst_list: dst ub list
        src_list: src ub list

        Returns
        ----------
        None
        """
        dst_rep_stride, src_rep_stride = all_rep_stride
        with self.tik_instance.context.freeze():
            dtype_str = self.dst_tensor_op.get_dtype_str(
                self.src_tensor_op.tensor_obj, self.dst_tensor_op.tensor_obj, self.name)

            config = [self._dtype_convert(repeat_times, "int64"), dst_rep_stride, src_rep_stride]
            # The fourth parameter is [dst_extent, src_extent]
            extents = self._get_extents(repeat_times, dst_rep_stride, src_rep_stride)
            self.tik_instance.config_vas([dst_list, src_list], dtype_str, config, extents)
