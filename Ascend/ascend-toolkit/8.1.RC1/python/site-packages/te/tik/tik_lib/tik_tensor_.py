#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_tensor_.py
DESC:     get address of tensor for v210
CREATED:  2019-04-18 18:53:42
MODIFIED: 2020-12-7 19:17:00
"""
from collections import namedtuple
from functools import lru_cache

from tbe import tvm
from tbe.tvm import tir
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_ca
from tbe.common.platform import scope_cb
from tbe.common.platform import scope_cc
from tbe.common.platform import intrinsic_check_support
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import TikUtil
from tbe.tik.tik_lib.tik_params import BIT_LEN_8
from tbe.tik.tik_lib.tik_backend import tik_access_ptr
from tbe.tik import debug
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.tik_lib.tik_util import need_check_out_of_scope
from tbe.tik.tik_lib.tik_expr_convert import type_convert
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_params import INDEX_IN_STOP
from tbe.tik.tik_lib.tik_params import INDEX_IN_START
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_params import MIN_LRU_CACHE_SIZE
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.common.util import tvm_immediate_number
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import instance_judge
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.common_util import TENSOR_START_ADDR_ALIGN
from tbe.tik.tik_lib.tik_check_util import print_error_msg


class TensorAddress:
    """
    scalar expression

    content :
    self.ir_generator
    self.buffer
    """
    BUFFER_STORAGE_COUNT = 0

    def __init__(self, ir_generator):
        """
        tensor register initialization
        Parameters
        ----------
        ir_generator:Halide IR generator
        """
        self.ir_generator = ir_generator
        self.available = True
        self.buffer = None
        self.dtype = None

    @source_info_decorator()
    @debug.tensor_get_addr_decorator
    def set_address_value(self, value):
        """
        construct tensor's constructor function
        Parameters
        ----------
        value:tensor addr

        Returns
        ----------
        return:tensor's constructor
        """
        self.check_tensor_attr(value)
        # according src tensor's buffer to find buffer_storage_id
        # if the tensor has buffer_storage_id, use it
        # if the tensor hasn't buffer_storage_id, create a new id according
        # BUFFER_STORAGE_COUNT
        if value.buffer.data in self.ir_generator.buffer_buffer_id_dict:
            buffer_storage_id = self.ir_generator.buffer_buffer_id_dict[value.buffer.data]
        else:
            buffer_storage_id = TensorAddress.BUFFER_STORAGE_COUNT
            TensorAddress.BUFFER_STORAGE_COUNT += 1
            # bind src buffer with buffer_storage_id
            self.ir_generator.buffer_buffer_id_dict.update({value.buffer.data: buffer_storage_id})
        buffer_storage_id = [buffer_storage_id, ]
        if buffer_storage_id not in self.ir_generator.buffer_no_reuse_list:
            self.ir_generator.buffer_no_reuse_list.append(buffer_storage_id)

        self.ir_generator.assign_address(self, value)

    def check_tensor_attr(self, value):
        """
        check tensor attr
        Parameters
        ----------
        value: value
        """
        instr_name = "get_ub_virtual_address"
        api_name = "set_address_value"
        msg = "Tensor %s is not defined in this scope." % self.buffer.name
        if need_check_out_of_scope(self.ir_generator):
            TikCheckUtil.check_equality(self.available, True, msg)
        TikCheckUtil.check_type_match(value, TensorAddress, "value should be Tensor, but get type: %s" % type(value))
        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + instr_name, self.dtype),
                                    True, gen_api_check_statement(value.dtype, api_name))
        if not value.is_single_point():
            print_error_msg("Instruction set_address_value input tensor's size must be 1.")


def tensor_type_convert(value):
    """
    tensor type convert
    """
    if isinstance(value, int):
        return value
    return type_convert(value)


def has_scalar(shape):
    """
    Check whether the shape is Scalar or Expr
    """
    for i in shape:
        if tvm_immediate_number([i]) or isinstance(i, (Scalar, Expr)):
            return True
    return False


def normalize_shape(shape):
    """
    normalize input shape
    :param shape: the input shape of new tensor
    :return: shape_new
    """
    # change type of input shape to slice
    shape_new = []
    for i in shape:
        if tvm_immediate_number([i]):
            shape_new.append(slice(0, int(i.value), 1))
        elif is_immediate_number([i]):
            shape_new.append(slice(0, int(i), 1))
        elif isinstance(i, Scalar):
            shape_new.append(slice(0, i, 1))
        elif isinstance(i, Expr):
            shape_new.append(slice(0, i, 1))
        else:
            TikCheckUtil.raise_error("only support Immediate, Scalar and Expr for Tensor shape, "
                                     "while %s is not" % str(i))

    return shape_new


def _normalize_slice(slc, shape_num):
    """
    Slice tensor

    Parameters
    ----------
    slc : slice or the slice index
    shape_num: the shape index of slice

    Returns
    ----------
    return: the new slice of tensor
    """
    if not isinstance(slc, slice):
        return slice(slc, slc + 1, 1)
    start_tmp = 0
    stop_tmp = shape_num
    step_tmp = 1
    if slc.start is not None:
        start_tmp = slc.start
    if slc.stop is not None:
        stop_tmp = slc.stop
    if slc.step is not None:
        step_tmp = slc.step
    return slice(tensor_type_convert(start_tmp), tensor_type_convert(stop_tmp), tensor_type_convert(step_tmp))


def _slice_div(sls, div_num):
    """
    Slice divide

    Parameters
    ----------
    sls: slice of tensor
    div_num: the divide number of slice

    Returns
    ----------
    return: range of the slice
    """
    value = sls.step
    if tvm_immediate_number(value):
        value = int(sls.step.value)
    TikCheckUtil.check_type_match(value, int, "value should be int")
    TikCheckUtil.check_equality(value, 1, "value should be equal to 1")
    return slice(sls.start // div_num, sls.stop // div_num, 1)


@lru_cache(maxsize=MIN_LRU_CACHE_SIZE)
def dtype_bits_map(input_dtype):
    """
    A Map for TIK: from data type to int

    Parameters
    ----------
    input_dtype: str
    """
    output_bits = {
        'int8': 8, 'int16': 16, 'int32': 32, 'uint8': 8, 'uint16': 16, 'uint32': 32, 'bfloat16': 16,
        'float16': 16, 'float32': 32, 'int': 32, 'float': 32, 'uint64': 64, 'int64': 64
    }
    if input_dtype in output_bits.keys():
        return output_bits.get(input_dtype)
    return TikCheckUtil.raise_error('Unexpected dtype: %s' % input_dtype)


class TensorBase:
    """
    def Tensor or TensorAddrList base class
    """
    COUNT = 0
    ORIGIN_DTYPE = 'uint8'
    ORIGIN_DTYPE_BITS = 8

    def __init__(self, ir_generator, scope=None):
        # when scalar variable "available" is True, this scalar can be accessed.
        # when scalar variable "available" is False this scalar can't be accessed and assert
        self.available = True
        self.ir_generator = ir_generator
        self.__initial_param()
        self.source_loc = None
        # self.dimensions is the shape of tensor.
        # each dim is a slice. which include start ,stop and step.
        # the value of start ,stop, step maybe imm, expr, scalar
        self.dimensions = []
        # it is a stride of each dim. the value maybe imm, expr, scalar.
        self.strides = []
        # self.data is the start position of buffer.
        self.data = None
        self.buffer = None
        self.buffer_shape_dtype = None
        self.tensor_scope = scope
        self.old_data = 0
        self.convet_to_var_count = 0
        self.is_static_shape = None

    def __call__(self, index):
        """
        get tensor register item
        Parameters
        ----------
        index:tensor register index


        Returns
        ----------
        return:the index tensor register
        """
        return self.__getitem__(index)

    @property
    def offset(self):
        """
        return tensor offset
        Parameters
        ----------

        Returns
        ----------
        return:tensor offset
        """
        self.check_tensor_scope()
        return self.data

    @property
    def scope(self):
        """
        return tensor memory scope
        Parameters
        ----------

        Returns
        ----------
        return:buffer scope
        """
        return self.tensor_scope

    @property
    def buffer_size(self):
        """
        ret unit : byte
        """
        self.check_tensor_scope()
        buffer_shape = []
        for i in self.buffer.shape:
            if tvm_immediate_number([i]):
                buffer_shape.append(i.value)
            else:
                buffer_shape.append(i)
        buffer_size = reduce_mul(buffer_shape) * DTYPE_SIZE.get(self.buffer.dtype)
        if not isinstance(buffer_size, int):
            return Expr(buffer_size)
        return buffer_size

    @staticmethod
    def calc_strides(dims):
        """
        calc strides

        Parameters
        ----------
        dims: the shape

        Returns
        -------
        the new strides
        """
        res = [1]
        value = [(dim.stop - dim.start) * dim.step for dim in dims]
        # after "*" or "+" operator,
        # the result may exceed the range of dtype
        if has_scalar(value):
            acc_value = tvm.const(1, "int64")
        else:
            acc_value = 1
        for i in value[::-1][:-1]:
            if isinstance(i, (Scalar, Expr)):
                acc_value = i * acc_value
            else:
                acc_value = acc_value * i
            acc_value_eval = Expr(acc_value).eval_value()
            if acc_value_eval is not None:
                res.append(acc_value_eval)
            else:
                res.append(acc_value)
        return res[::-1]

    @staticmethod
    def calc_slice_length(slc):
        """
        calc slice length
        Parameters
        ----------
        slc: the shape

        Returns
        -------
        return:slice length
        """
        TikCheckUtil.check_type_match(slc, slice, "slc should be slice, input type %s" % type(slc))
        res = (slc.stop - slc.start + slc.step - 1) // slc.step
        res_eval = Expr(res).eval_value()
        if res_eval is not None:
            return res_eval
        return res

    @staticmethod
    def calc_new_strides(new_dims, strides):
        """
        calc new strides

        Parameters
        ----------
        new_dims: new dim
        strides: strides

        Returns
        -------
        return:new strides
        """
        res = []
        for dim, stride in zip(new_dims, strides):
            if isinstance(stride, (Scalar, Expr)):
                res.append(stride * dim.step)
            else:
                res.append(dim.step * stride)
        return res

    @staticmethod
    def calc_new_data(new_dims, strides, data):
        """
        calc new data

        Parameters
        ----------
        new_dims: new dim
        strides: strides
        data: data

        Returns
        -------
        return:new data
        """
        res = data
        for dim, stride in zip(new_dims, strides):
            dim_start = Expr(dim.start).eval_value()
            if dim_start is None:
                dim_start = dim.start
            if isinstance(stride, (Scalar, Expr)):
                tmp = stride * dim_start
            else:
                tmp = dim_start * stride
            if isinstance(res, (Scalar, Expr)):
                res = res + tmp
            else:
                res = tmp + res
        return res

    @staticmethod
    def is_version1():
        """
        Check whether the version is version 1
        """
        return True

    @staticmethod
    def get_items(index_in):
        """
        get index_in position items

        Parameters
        ----------
        index_in: index

        Returns
        -------
        return:items
        """
        if isinstance(index_in, slice) and instance_judge([index_in.stop, index_in.start], (int,)) and \
                (index_in.stop == INDEX_IN_STOP) and (index_in.start == INDEX_IN_START):
            items = slice(None, None, None)
        else:
            items = index_in

        if not isinstance(items, (tuple, list)):
            items = [items]

        return items

    @staticmethod
    def check_start_addr(scope, name, dtype, start_addr):
        """
        check Tensor start addr align and overflow
        :param scope: Tensor scope
        :param name: Tensor name
        :param dtype: Tesnor dtype
        :param start_addr: Tensor start addr, Byte
        :return: None
        """
        if start_addr is None:
            return
        TikCheckUtil.check_var_in_list(scope, [scope_ubuf, scope_cbuf, scope_ca, scope_cb, scope_cc],
                                       "start_addr of tensor %s should be None" % name)

        # check align
        scope_name = TikUtil.get_storage_scope(scope)
        if scope_name == 'L0C':
            if DTYPE_SIZE[dtype] <= 2:
                scope_name = 'L0C16'
            else:
                scope_name = 'L0C32'
        align_size = TENSOR_START_ADDR_ALIGN[scope_name]
        if start_addr % align_size != 0:
            TikCheckUtil.raise_error(
                "start_addr align error, %s[%s] is not %s Byte align" % (name, start_addr, align_size))

    @staticmethod
    def __initial_param():
        # parameters initialization
        TensorBase.COUNT += 1

    @staticmethod
    def __check_start_tmp(item, dim_len):
        start_tmp = 0
        item_start = item.start
        if item_start is not None:
            item_start_expr = Expr(item_start)
            if not item_start_expr.dtype.startswith(("int", "uint")):
                TikCheckUtil.raise_error("the start(%s) of slice should be int." % item.start)
            if isinstance(item_start, int):
                if item_start < 0:
                    item_start += dim_len
                # if item_start_eval is not Imm, the compare result is False
                if item_start < 0:
                    TikCheckUtil.raise_error(
                        "tensor overflow, the start(%s) of slice should in [0, %s]." % (item_start, dim_len))
            start_tmp = item_start
        return start_tmp

    @staticmethod
    def __check_step_tmp(item):
        step_tmp = 1
        item_step = item.step
        if item_step is not None:
            item_step_expr = Expr(item_step)
            if not item_step_expr.dtype.startswith(("int", "uint")):
                TikCheckUtil.raise_error("the step(%s) of slice should be int." % item_step)
            if isinstance(item_step, int):
                step_tmp = item_step
            else:
                TikCheckUtil.raise_error("the step(%s) of slice cannot be Scalar/Expr." % item_step)
        return step_tmp

    @staticmethod
    def __check_item_stop(start_tmp, item_stop, dim_len, item):
        if isinstance(start_tmp, int) and item_stop <= start_tmp:
            TikCheckUtil.raise_error(
                "the stop(%s) of slice should be greater than the start(%s) of slice." % (item_stop, item.start))
        if isinstance(dim_len, int) and item_stop > dim_len:
            TikCheckUtil.raise_error(
                "tensor overflow, the stop(%s) of slice should in [0, %s]." % (item_stop, dim_len))

    def convet_to_var(self, value, var_name=None):
        """
        convent value to var
        """

        def let_value_var(buffer_var, value):
            def __let_stmt(body):
                let_node = tir.LetStmt(buffer_var, value, body)
                return let_node

            self.ir_generator.emit(__let_stmt)

        value = Expr(value)
        if value.eval_value() is not None:
            ret = int(value.eval_value())
        else:
            if var_name is None:
                buffer_var = tvm.var("%s_var%s" % (self.name, str(self.convet_to_var_count)), value.dtype)
                self.convet_to_var_count += 1
            elif isinstance(var_name, str):
                buffer_var = tvm.var("%s_%s" % (self.name, var_name), value.dtype)
            else:
                TikCheckUtil.check_type_match(
                    var_name, str, "Var name must be a str, input Var name type: %s" % type(var_name))
            let_value_var(buffer_var, value.get())
            ret = buffer_var
        return ret

    def get_shape(self, max_mem_size, shape, dtype):
        """
        get total size
        Parameters
        ----------
        max_mem_size:max
        shape:TensorAddress's or Tensor's shape
        dtype:TensorAddress's or Tensor's dtype

        Returns
        -------
        tensor shape size
        """
        shape = reduce_mul(shape)
        if max_mem_size is None:
            shape_value = Expr(shape).eval_value()
            if shape_value is not None:
                self.is_static_shape = True
                shape = int(shape_value)
        else:
            if dtype == 'bool':
                # for bool dtype, need to align to 8 bits
                total_size = ceil_div(shape, BIT_LEN_8)
                shape = max_mem_size
            else:
                total_size = shape * DTYPE_SIZE[dtype]
                shape = max_mem_size // DTYPE_SIZE[dtype]
            self.__check_total_size_max_mem_size(total_size, max_mem_size)

        return shape

    def access_ptr(self, access_mask, offset=0, cast_dtype="handle", extent=None, is_implicit=False):
        """
        Get an access pointer to the head of buffer.
        Support cast feature for TIK.
        Add a Parameter: cast_dtype
        Parameters
        ----------
        access_mask : int
        The access pattern MASK. Indicate whether the access will read or write to the data content.
        The data type of the result pointer. Do not specify unless we want to cast pointer to specific type.
        The number of lanes for the data type. This value is greater than one for vector types.
        offset: Expr, optional
        The offset of pointer. We can use it to offset by the number of elements from the addr of ptr.
        cast_dtype: str, optional
        The data type of the result pointer. Do not specify unless we want to cast pointer to specific type.
        extent: Expr, optional. tensor access extent in unit of byte
        The maximum size required for the tensor, unit is Byte
        is_implicit: Whether is implicit dependence
        """
        ptr_type, content_lanes, cast_dtype, access_mask_value, \
            offset_temp = self.__access_ptr_gen_params(access_mask, offset, cast_dtype)
        return self.__get_access_ptr_args(offset_temp, access_mask_value, ptr_type,
                                          (content_lanes, cast_dtype, extent, is_implicit))

    def access_ptr_vbi_src0(self, access_mask, offset=0, cast_dtype="handle", extent=None):
        """
        Get an access pointer to the head of buffer for vbi. Support cast feature for TIK. Add a Parameter: cast_dtype
        """
        is_implicit = True
        ptr_type, content_lanes, cast_dtype, access_mask_value, \
            offset_temp = self.__access_ptr_gen_params(access_mask, offset, cast_dtype)
        return self.__get_access_ptr_args(offset_temp, access_mask_value, ptr_type,
                                          (content_lanes, cast_dtype, extent, is_implicit))

    def check_tensor_scope(self):
        """
        check tensor scope

        Returns
        -------
        return:None
        """
        if need_check_out_of_scope(self.ir_generator):
            msg = "Tensor %s is not defined in this scope." % self.name
            TikCheckUtil.check_equality(self.available, True, msg)

    def double_buffer_size(self, thread_num_value):
        """
        ret unit : byte
        """
        self.check_tensor_scope()
        return self.buffer_size * thread_num_value

    def disable_tensor(self):
        """
        when this tensor lifecycle is in the end,
        this tensor condition parameter should be changed from true to false.
        Parameters
        ----------
        No parameter

        Returns
        ----------
        return:no return
        """
        self.available = False

    def calc_new_dims(self, items, dims):
        """
        calc new_dims

        Parameters
        ----------
        items:items
        dims:dims

        Returns
        -------
        the new_dims
        """
        if self.is_version1() and len(items) != len(dims):
            TikCheckUtil.raise_error("length of index_in should be equal to the dimension of Tensor when getitem.")
        res = []
        for i, v in enumerate(items):
            res.append(self.__normalize_dims(v, dims[i]))
        for i in range(len(items), len(dims)):
            res.append(self.__normalize_dims(slice(None, None, None), dims[i]))
        return res

    def __normalize_dims(self, item, dim):
        dim_len = self.calc_slice_length(dim)
        if not isinstance(item, slice):
            if isinstance(item, int) and item == -1:
                item = slice(tensor_type_convert(dim_len - 1), tensor_type_convert(dim_len), 1)
            else:
                item = slice(tensor_type_convert(item), tensor_type_convert(item + 1), 1)

        start_tmp = self.__check_start_tmp(item, dim_len)
        step_tmp = self.__check_step_tmp(item)
        stop_tmp = self.__check_stop_tmp(item, start_tmp, dim_len)

        return slice(tensor_type_convert(start_tmp), tensor_type_convert(stop_tmp), tensor_type_convert(step_tmp))

    def __check_stop_tmp(self, item, start_tmp, dim_len):
        stop_tmp = dim_len
        item_stop = item.stop
        if item_stop is not None:
            item_stop_expr = Expr(item_stop)
            if not item_stop_expr.dtype.startswith(("int", "uint")):
                TikCheckUtil.raise_error("the stop(%s) of slice should be int." % item_stop)
            if isinstance(item_stop, int):
                if item_stop < 0:
                    item_stop += dim_len
                # if item_stop_eval is not Imm, the compare result is False
                self.__check_item_stop(start_tmp, item_stop, dim_len, item)
            stop_tmp = item_stop
        return stop_tmp

    def __check_total_size_max_mem_size(self, total_size, max_mem_size):
        if not isinstance(max_mem_size, int):
            TikCheckUtil.raise_error("max_mem_size should be int.")
        if max_mem_size % DTYPE_SIZE[self.dtype] != 0:
            TikCheckUtil.raise_error("max_mem_size should be an integer multiple dtype_size.")
        total_size = Expr(total_size).eval_value()
        if total_size is not None:
            self.is_static_shape = True
            if total_size > max_mem_size:
                TikCheckUtil.raise_error("max_mem_size is too small.")
            if total_size < max_mem_size:
                TikCheckUtil.raise_error("max_mem_size is too big.")

    def __get_access_ptr_args(self, offset_temp, access_mask, ptr_type, args):
        # Offset, extent need to be casted
        # Offset: cast_dtype to buffer.dtype
        buffer_dtype_bits = dtype_bits_map(args[1])
        # offset_buffer_dtype should have same dtype with self.buffer.shape
        if self.buffer_shape_dtype == "int64":
            if dtype_bits_map(self.dtype) >= buffer_dtype_bits:
                offset_buffer_dtype = (Expr(offset_temp) *
                                       Expr(dtype_bits_map(self.dtype) // buffer_dtype_bits, "int64")).get()
            else:
                offset_buffer_dtype = (Expr(offset_temp) //
                                       Expr(buffer_dtype_bits // dtype_bits_map(self.dtype), "int64")).get()
        else:
            offset_buffer_dtype = offset_temp * dtype_bits_map(self.dtype) // buffer_dtype_bits
        # elem_offset, extent, e_dtype: buffer.dtype to cast_dtype
        cast_dtype_bits = dtype_bits_map(args[1])
        return tik_access_ptr((self.buffer, access_mask, buffer_dtype_bits, args[2], cast_dtype_bits, args[1]),
                              ptr_type, args[0], offset_buffer_dtype, args[3])

    def __get_access_mask(self, access_mask):
        if isinstance(access_mask, str):
            mask = 0
            for value in access_mask:
                if value == "r":
                    mask = mask | self.buffer.READ
                elif value == "w":
                    mask = mask | self.buffer.WRITE
                else:
                    TikCheckUtil.raise_error("Unknown access_mask %s" % access_mask, exception_type=ValueError)
            access_mask = mask
        return access_mask

    def __access_ptr_gen_params(self, access_mask, offset, cast_dtype):
        ptr_type = "handle"
        content_lanes = 1
        self.check_tensor_scope()
        if cast_dtype == "handle":
            cast_dtype = self.dtype
        access_mask_value = self.__get_access_mask(access_mask)
        if isinstance(self.offset, tvm.ir.PrimExpr):
            offset_temp = self.offset + Expr(offset).get()
        else:
            offset_temp = self.offset + offset

        return [ptr_type, content_lanes, cast_dtype, access_mask_value, offset_temp]


class TensorInner(TensorBase):
    """
    Tensor inner class
    """
    new_tensor_api = namedtuple('NewTensorApi', ['ir_generator', 'buffer_', 'dtype', 'dims', 'stride',
                                                 'is_workspace', "is_atomic_add", "data", "max_mem_size",
                                                 "original_shape", "old_data", "last_tensor", "is_reshape",
                                                 "is_getitem", "is_global_tensor", "scope", "is_tiling_tensor"])

    def __init__(self, ir_generator, scope):
        TensorBase.__init__(self, ir_generator, scope=scope)
        self.original_shape = None

    @property
    def info_node(self):
        """
        get the tensor's info
        Returns
        -------
        tensor's info of tvm call node
        """
        dimensions = tvm.call_cce_intrin("int32", "dimensions", *[self.convet_to_var(s) for s in self.shape])
        strides = tvm.call_cce_intrin("int32", "strides", *[self.convet_to_var(s) for s in self.strides])
        original_shape = tvm.call_cce_intrin("int32", "original_shape",
                                             *[self.convet_to_var(s) for s in self.original_shape])
        args = [self.buffer.data, self.dtype, self.convet_to_var(self.data), dimensions, strides, original_shape]
        return tvm.call_intrin("int32", "tir.tvm_access_ptr", *args)

    @property
    def size(self):
        """
        return tensor size

        Returns
        ----------
        return:tensor size
        """
        self.check_tensor_scope()
        return reduce_mul(self.shape)

    def is_same_tensor(self, second_tensor):
        """
        judge if it's the same tensor

        Parameters
        ----------
        second_tensor: second tensor

        Returns
        -------
        return:True or False
        """
        return self._get_last_tensor() is second_tensor._get_last_tensor()

    @source_info_decorator()
    def change_shape(self, new_shape):
        """
        change shape
        Parameters
        ----------
        new_shape:the new tensor register shape
        """
        self.check_tensor_scope()
        TikCheckUtil.check_type_match(new_shape, (list, tuple), "new_shape should be list or tuple")
        self.check_tensor_reshapeable(new_shape)
        self.dimensions = normalize_shape(new_shape)
        self.strides = self.calc_strides(self.dimensions)
        self.original_shape = new_shape

    @source_info_decorator()
    def clean_shape(self):
        """
        clean tensor shape

        Returns
        ----------
        return:the i tensor register
        """
        self.check_tensor_scope()
        new_dims = [slice(0, reduce_mul(self.shape), 1)]
        new_strides = [1]
        temp_data = self.data
        new_data = self.data
        if isinstance(new_data, (Scalar, Expr)):
            old_data = new_data - temp_data
        else:
            old_data = -1 * temp_data + new_data
        new_original_shape = [reduce_mul(self.original_shape)]
        new_tensor_obj = TensorInner.new_tensor_api(
            self.ir_generator, self.buffer, self.dtype, new_dims, new_strides, self.is_workspace, self.is_atomic_add,
            new_data, self.max_mem_size, new_original_shape, old_data=old_data, last_tensor=self,
            is_reshape=False, is_getitem=False, is_global_tensor=self.is_global_tensor, scope=self.tensor_scope,
            is_tiling_tensor=self.is_tiling_tensor)
        return self._new_tensor(new_tensor_obj)

    @source_info_decorator()
    def flatten(self):
        """
        tensor flatten shape

        Returns
        ----------
        return:tensor register reshape
        """
        self.check_tensor_scope()
        return self.reshape([reduce_mul(self.shape)])

    def is_single_point(self):
        """
        judging whether tensor is one point

        Returns
        ----------
        return:judging whether tensor is one point
        """
        self.check_tensor_scope()
        shape_eval = [Expr(i).eval_value() for i in self.shape]
        if any(i is None for i in shape_eval):
            return False
        if all(i == 1 for i in shape_eval):
            return True
        return False

    def is_tensor_slice(self):
        """
        judging whether tensor is sliced

        Returns
        ----------
        return:judging whether tensor is sliced
        """
        self.check_tensor_scope()
        shape_eval = [Expr(i).eval_value() for i in self.shape]
        original_shape_eval = [Expr(i).eval_value() for i in self.original_shape]
        if any(i is None for i in shape_eval + original_shape_eval):
            return True
        if reduce_mul(shape_eval) == reduce_mul(original_shape_eval):
            return False
        return True

    def reshape_(self, new_shape):
        """
        tensor register reshape
        note: use this function to call tensor.reshape inside!!
        """
        self.check_tensor_scope()
        self.check_tensor_reshapeable(new_shape)
        # we will change the old shape to new shape
        dimensions = normalize_shape(new_shape)
        strides = self.calc_strides(dimensions)

        new_tensor_obj = TensorInner.new_tensor_api(
            self.ir_generator, self.buffer, self.dtype, dimensions, strides, self.is_workspace,
            self.is_atomic_add, self.data, self.max_mem_size, new_shape, old_data=0, last_tensor=self,
            is_reshape=True, is_getitem=False, is_global_tensor=self.is_global_tensor, scope=self.tensor_scope,
            is_tiling_tensor=self.is_tiling_tensor)

        tmp_t = self._new_tensor(new_tensor_obj)
        return tmp_t

    def check_tensor_reshapeable(self, new_shape=None):
        """
        check  tensor reshape able
        Parameters
        ----------
        new_shape
        default value None
        Returns
        -------
        None
        """
        shape_eval = [Expr(i).eval_value() for i in self.shape]
        for i in shape_eval:
            if i is None:
                return
        original_shape_eval = [Expr(i).eval_value() for i in self.original_shape]
        for i in original_shape_eval:
            if i is None:
                return
        if reduce_mul(shape_eval) != reduce_mul(original_shape_eval):
            print_error_msg(
                "Reshape operator is needed, but the tensor with shape: "
                "%s has been sliced." % shape_eval)
        if new_shape is not None:
            new_shape_eval = [Expr(i).eval_value() for i in new_shape]
            for i in new_shape_eval:
                if i is None:
                    return
            if reduce_mul(shape_eval) != reduce_mul(new_shape_eval):
                print_error_msg(
                    "Shape mismatch! reshape operator with shape: %s and "
                    "new shape: %s." % (shape_eval, new_shape_eval))

    def check_tensor_getitem_flatten(self):
        """
        def check tensor getitem flatten func
        Returns
        -------
        None
        """
        strides_eval = self.strides
        shape_eval = self.shape
        for i in shape_eval:
            if not isinstance(i, int):
                return
        for i in strides_eval:
            if not isinstance(i, int):
                return
        for i in range(1, len(strides_eval)):
            if strides_eval[i] * shape_eval[i] != strides_eval[i - 1]:
                TikCheckUtil.raise_error("Getitem need flatten, but the tensor is not continuous after slicing.")

    def update_buffer_storage(self, ir_generator, enable_buffer_reuse, start_addr):
        """
        create a map record the buffer and buffer reuse id,
        for set_address_value to set src tensor cannot
        reuse with each other
        """
        if enable_buffer_reuse:
            ir_generator.buffer_buffer_id_dict.update({self.buffer.data: self.buffer_storage_id})
        if start_addr:
            ir_generator.start_addr_dict.update({self.buffer.data: [self.buffer_storage_id, start_addr]})
