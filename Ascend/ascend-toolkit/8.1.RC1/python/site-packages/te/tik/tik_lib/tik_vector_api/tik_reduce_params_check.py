#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_reduce_params_check.py
DESC:     provide params
CREATED:  2021-11-4 18:53:42
MODIFIED: 2021-11-5 19:17:00
"""
from tbe.common.platform import intrinsic_check_support
from tbe.tik.common.common_util import vec_template_align
from tbe.tik.common.common_util import int64_support_check
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.common.tik_get_soc_name import get_compatible_rep_size
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import VREDUCE_DST_ALIGN
from tbe.tik.tik_lib.tik_params import MAXMIN_CNT_INDEX_LEN_1
from tbe.tik.tik_lib.tik_params import MAXMIN_CNT_INDEX_LEN_3
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_api_util import check_repeat_times
from tbe.tik.tik_lib.tik_api_util import check_stride_unit
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.tik_lib.tik_mask_concat_ import mask_concat
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager

_DEFAULT_NBLOCK = 1
_DEFAULT_SRC_STRIDE = 0


class ReduceCheck:
    """
    Whole Reduce Check
    """
    def __init__(self, print_name, check_params, maxmin_cnt_index=None):
        self.print_name = print_name
        self.dst_tensor_op, self.src_tensor_op, self.control_op = check_params
        if self.print_name in {"vec_cpadd", "vcpadd"}:
            self.name = "vcpadd"
        else:
            self.name = self.print_name
        self.maxmin_cnt_index = maxmin_cnt_index
        self.one_rep_size = get_compatible_rep_size()

    def check_dtype_support(self):
        """
        check single_dtype support

        Returns
        -------
        None
        """
        dtype = self.dst_tensor_op.tensor_obj.dtype
        if dtype == "int64":
            TikCheckUtil.check_equality(int64_support_check("tik." + self.name, dtype), True,
                                        gen_api_check_statement(dtype, self.print_name))
        else:
            TikCheckUtil.check_equality(self.dst_tensor_op.tensor_obj.dtype, self.src_tensor_op.tensor_obj.dtype,
                                        "Instruction %s's src's "
                                        "dtype should be equal to dst's dtype" % self.print_name)
            TikCheckUtil.check_equality(
                intrinsic_check_support("Intrinsic_" + self.name, self.dst_tensor_op.tensor_obj.dtype), True,
                gen_api_check_statement(self.dst_tensor_op.tensor_obj.dtype, self.print_name))

    def check_maxmin_cnt_index(self):
        """
        check maxmin_cnt_index

        Returns
        -------
        None
        """
        # cannot recognize context member, so disable it
        TikCheckUtil.check_is(TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc() or TikSocManager.is_910b_soc(),
                              True, "%s doesn't support maxmin_cnt_index." % get_soc_name())
        TikCheckUtil.check_type_match(self.maxmin_cnt_index, (list, tuple))
        if TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc():
            TikCheckUtil.check_equality(
                len(self.maxmin_cnt_index), MAXMIN_CNT_INDEX_LEN_1, "maxmin_cnt_index must be a list of one Scalar")
            TikCheckUtil.check_type_match(self.maxmin_cnt_index[0], Scalar, "maxmin_cnt_index must be a list of Scalar")
        else:
            TikCheckUtil.check_equality(
                len(self.maxmin_cnt_index), MAXMIN_CNT_INDEX_LEN_3, "maxmin_cnt_index must be a list of three Scalar")
            for mci_scalar in self.maxmin_cnt_index:
                TikCheckUtil.check_type_match(mci_scalar, Scalar, "maxmin_cnt_index must be a list of Scalar")
            TikCheckUtil.check_var_in_list(self.maxmin_cnt_index[1].dtype, ["int16", "int32", "int64"],
                                           "maxmin_cnt_index only support int16, int32, int64")
            TikCheckUtil.check_var_in_list(self.maxmin_cnt_index[2].dtype, ["int16", "int32", "int64"],
                                           "maxmin_cnt_index only support int16, int32, int64")
        TikCheckUtil.check_equality(self.maxmin_cnt_index[0].dtype, self.src_tensor_op.tensor_obj.dtype)

    def check_debug_common_params(self, context=None):
        """
        check debug and ca common params

        Parameters
        ----------
        context: debug context

        Returns
        -------
        None
        """
        if context:
            repeat = context.evaluate_expr(self.control_op.repeat_times)
            TikCheckUtil.check_in_range_by_dtype(
                repeat, msg="repeat_times should be in the range of [%d, %d], input repeat_times: %s"
                % (0, MAX_REPEAT_TIMES, repeat), var_range=[0, MAX_REPEAT_TIMES])
        else:
            check_repeat_times(self.control_op.repeat_times)
        self.dst_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.set_blk_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.check_tensor_op_stride(MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_DOUBLE_BYTE)
        self.src_tensor_op.check_tensor_op_stride(MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_DOUBLE_BYTE)
        # gen block_list
        block_list = [_DEFAULT_NBLOCK, self.one_rep_size // max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                                get_bit_len(self.src_tensor_op.tensor_obj.dtype))]
        if self.src_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
            value_range = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride,
                           self.src_tensor_op.blk_stride, self.dst_tensor_op.rep_stride,
                           self.src_tensor_op.rep_stride)
            if all(isinstance(value, int) for value in value_range) or context:
                self.dst_tensor_op.check_address_overlapping(self.print_name, self.control_op,
                                                             self.src_tensor_op, block_list)

    def all_check(self, tik_instance):
        """
        check all

        Parameters
        ----------
        tik_instance: tik_instance

        Returns mask_o
        -------
        """

        dst_align = VREDUCE_DST_ALIGN[self.name]
        self.dst_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_DOUBLE_BYTE,
                                                 dst_align)
        src_align = vec_template_align(self.src_tensor_op.tensor_obj.dtype)
        self.src_tensor_op.set_blk_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.src_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_DOUBLE_BYTE,
                                                 src_align)
        self.check_dtype_support()
        check_stride_unit(self.control_op.stride_unit)
        if self.name in ("vcmax", "vcmin") and self.maxmin_cnt_index is not None:
            self.check_maxmin_cnt_index()
        if self.name in ("vcmin", "vcmax"):
            dst_block_len = 2
        elif self.name in ("vcgadd", "vcgmax", "vcgmin"):
            dst_block_len = 8
        elif self.name == "vcpadd":
            dst_block_len = self.one_rep_size // DTYPE_SIZE.get(self.dst_tensor_op.tensor_obj.dtype) // 2
        else:
            dst_block_len = 1
        mask_o = mask_concat(tik_instance, self.control_op.mask,
                             tensor_bit_len=max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                                                get_bit_len(self.src_tensor_op.tensor_obj.dtype)))
        self.check_debug_common_params()
        block_list_dst = [_DEFAULT_NBLOCK, dst_block_len]
        self.dst_tensor_op.blk_stride = _DEFAULT_SRC_STRIDE
        self.dst_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_dst)
        src_bit_len = get_bit_len(self.src_tensor_op.tensor_obj.dtype)
        parallelism = self.one_rep_size * ONE_BYTE_BIT_LEN // src_bit_len
        block_list_src = [parallelism // (self.one_rep_size // src_bit_len), self.one_rep_size // src_bit_len]

        self.src_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_src)

        return mask_o
