#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_reduce_common.py
DESC:     reduce op common file
CREATED:  2021-11-09 9:13
MODIFIED: 2021-11-09 9:13
"""
from collections import namedtuple
from tbe import tvm
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_tensor import Tensor
from tbe.common.platform import scope_ubuf
from tbe.common.platform import intrinsic_check_support
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import check_scalar_int32
from tbe.tik.common.util import check_scalar_dtype
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import TikUtil
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import ceil_div
from tbe.tik.common.common_util import vec_template_align
from tbe.tik.common.common_util import get_blk_valid_list
from tbe.tik.common.common_util import get_8or16bit_dtype_mask_len
from tbe.tik.common.common_util import get_need_offset
from tbe.tik.common.common_util import vector_tensor_overflow_check
from tbe.tik.common.common_check_func import get_32bit_dtype_mask_len
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.common.tik_get_soc_name import get_rep_size
from tbe.tik.tik_lib.tik_params import VREDUCE_DST_ALIGN
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_params import VREDUCE_DEFAULT_DST_BLK_STRIDE
from tbe.tik.tik_lib.tik_params import VREDUCE_DEFAULT_SRC_BLK_STRIDE
from tbe.tik.tik_lib.tik_params import VREDUCE_DEFAULT_DST_REP_STRIDE
from tbe.tik.tik_lib.tik_params import MAX_VREDUCE_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MASK_VALUE_ZERO
from tbe.tik.tik_lib.tik_params import MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import BIT_LEN_32
from tbe.tik.tik_lib.tik_params import BIT_LEN_16
from tbe.tik.tik_lib.tik_params import BIT_LEN_8
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_api_util import check_address_align
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.tik_lib.tik_vector_api.tik_vector_name_map import REDUCE_NAME_DICT
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.common.platform.platform_info import api_check_support

DEFAULT_STRIDE = 1
DEFAULT_REP_STRIDE = 8


class BaseCheckParams:
    """
    reduce add and reduce all common check
    """

    def __init__(self, params):
        self.dst, self.src, self.work_tensor, self.control_op, self.tik_instance, self.src_rep_stride = params
        self.one_blk_size = get_block_size()
        self.one_rep_size = get_rep_size()

    @staticmethod
    def get_extend_interval(blk_valid_list, time, extend_interval_params):
        """
        get tensor each blk interval
        """
        extend_interval = []
        for blk_id in blk_valid_list:
            # blk_stride, rep_stride, unit: 32B
            if extend_interval_params.stride_unit == 0:
                begin = blk_id * extend_interval_params.blk_stride * extend_interval_params.block_len + \
                        time * extend_interval_params.rep_stride * extend_interval_params.block_len + \
                        extend_interval_params.offset
            # blk_stride, rep_gap, unit: 32B
            elif extend_interval_params.stride_unit == 1:
                begin = blk_id * extend_interval_params.blk_stride * extend_interval_params.block_len + \
                        time * (((extend_interval_params.nblock - 1) * extend_interval_params.blk_stride + 1) *
                                extend_interval_params.block_len +
                                extend_interval_params.rep_stride * extend_interval_params.block_len) + \
                        extend_interval_params.offset
            # stride1: blk_gap, stride2: rep_stride, unit: elements
            elif extend_interval_params.stride_unit == 2:
                begin = blk_id * (extend_interval_params.blk_stride + extend_interval_params.block_len) + time * \
                        extend_interval_params.rep_stride + extend_interval_params.offset
            # blk_gap, rep_gap, unit: elements
            else:
                begin = blk_id * (extend_interval_params.block_len + extend_interval_params.blk_stride) + \
                        time * (((extend_interval_params.nblock - 1) * extend_interval_params.blk_stride + 1) *
                                extend_interval_params.block_len +
                                extend_interval_params.rep_stride * extend_interval_params.block_len) + \
                        extend_interval_params.offset

            end = begin + extend_interval_params.block_len
            extend_interval.append(
                [begin * DTYPE_SIZE[extend_interval_params.dtype], end * DTYPE_SIZE[extend_interval_params.dtype]])
        return extend_interval

    @staticmethod
    def get_src_dst_blk_list(all_params):
        """
        get blk valid list for check overlap
        """
        if not isinstance(all_params.mask, (list, tuple)):
            mask = [all_params.mask]
        else:
            mask = all_params.mask
        dst_blk_valid_list = get_blk_valid_list(mask, all_params.dst.dtype, all_params.dst_block_len)
        if all_params.src_mask is not None:
            if not isinstance(all_params.src_mask, (list, tuple)):
                mask = [all_params.src_mask]
            else:
                mask = all_params.src_mask
        src_blk_valid_list = get_blk_valid_list(mask, all_params.src.dtype, all_params.src_block_len)
        return dst_blk_valid_list, src_blk_valid_list

    @staticmethod
    def check_overlap_param(mask, repeat, dst_offset, src_offset):
        """
        get a flag of address overlap check
        """
        is_check = True
        if repeat <= 0:
            is_check = False
        if dst_offset is None or src_offset is None:
            is_check = False
        if is_basic_expr(TikUtil.to_list(mask)):
            is_check = False
        return is_check

    @staticmethod
    def check_overlap_interval(dst_extend_interval, src_extend_interval, error_msg):
        """
        check overlap interval
        :param error_msg:
        :param dst_extend_interval:
        :param src_extend_interval:
        :return:
        """
        for interval_dst in dst_extend_interval:
            for interval_src in src_extend_interval:
                if max(interval_src[0], interval_dst[0]) < min(interval_src[1], interval_dst[1]):
                    TikCheckUtil.raise_error(error_msg)

    def check_tensor_overflow(self, tensor_info_seq, stride_unit=0, mask_mode="normal"):
        """
        check tensor overflow

        Parameters
        ----------
        tensor_info_seq: contains
        - tensor_list
        - mask : Effective operation on element, divided into two model: Continuous and bit by bit
        - repeat_times : Repeated iterations times
        - blk_stride_list : offset of src operator between different block in one repeat
        - rep_stride_list : offset of src operator in the same block between two repeats
        - tensor_name_list : tensor name list

        stride_unit : strideunit
        mask_mode : mask mode

        Returns
        -------
        None
        """
        bit_len = []
        tensor_params = namedtuple(
            "TensorParams", "tensor_list mask repeat_times blk_stride_list rep_stride_list tensor_name_list")
        tensor_params_ins = tensor_params(*tensor_info_seq)
        ori_offset = 0
        imm_offset = None
        imm_original_shape = None
        for tensor_in in tensor_params_ins.tensor_list:
            tensor_bit_len = get_bit_len(tensor_in.dtype)
            bit_len.append(tensor_bit_len)
        parallelism = self.one_rep_size * ONE_BYTE_BIT_LEN // max(bit_len)
        for tensor_in, blk_stride, rep_stride, tensor_bit_len, name in \
                zip(tensor_params_ins.tensor_list, tensor_params_ins.blk_stride_list,
                    tensor_params_ins.rep_stride_list, bit_len, tensor_params_ins.tensor_name_list):
            vector_tensor_overflow_check(
                (tensor_in, tensor_params_ins.mask,
                 parallelism // (self.one_rep_size // tensor_bit_len),
                 self.one_rep_size // tensor_bit_len, tensor_params_ins.repeat_times, blk_stride,
                 rep_stride, "%s tensor overflow" % name, stride_unit,
                 mask_mode, ori_offset, imm_offset, imm_original_shape, False))

    def check_overlap_single(self, repeat_time, dst_overlap_single_params, src_overlap_single_params, all_params):
        """
        check overlap for single repeat
        """
        dst_blk_valid_list, src_blk_valid_list = \
            self.get_src_dst_blk_list(all_params)
        single_dst_extend_interval = self.get_extend_interval(
            dst_blk_valid_list, repeat_time - 1,
            dst_overlap_single_params)

        single_src_extend_interval = self.get_extend_interval(
            src_blk_valid_list, repeat_time - 1,
            src_overlap_single_params)

        # check for single
        check_overlap_single_msg = "%s address overlapping error. " \
                                   "when repeat_times=1, only support %s 100 percent same," \
                                   " and stride must be same too." % (all_params.name, all_params.msg)
        self.check_overlap_interval(single_dst_extend_interval, single_src_extend_interval, check_overlap_single_msg)

    def check_overlap_many(self, dst_params_ins, src_params_ins, overlap_many_params):
        """
        check overlap for not scatter instr
        """
        dst_blk_valid_list, src_blk_valid_list = \
            self.get_src_dst_blk_list(overlap_many_params)

        for time in range(overlap_many_params.repeat_times - 1):
            dst_extend_interval = self.get_extend_interval(
                dst_blk_valid_list, time, dst_params_ins)

            src_extend_interval = self.get_extend_interval(
                src_blk_valid_list, time + 1, src_params_ins)

            # check
            check_overlap_many_msg = "When repeat_times>1, %s %s address " \
                                     "overlapping error. It is not support" \
                                     " iteration N's destination is the source " \
                                     "of next iteration" % (overlap_many_params.name, overlap_many_params.msg)
            self.check_overlap_interval(dst_extend_interval, src_extend_interval, check_overlap_many_msg)

    def check_address_overlapping(self, check_addr_overlap_params, msg="dst and src"):
        """
        check address overlap
        """
        check_params = namedtuple(
            "AddressOverlapping", "name mask dst src nblock block_len dst_block_len src_block_len "
                                  "repeat_times dst_blk_stride src_blk_stride dst_rep_stride src_rep_stride"
                                  " dst_offset src_offset"
                                  " stride_unit mask_mode msg src_mask dst_nblock src_nblock")
        check_params.__new__.__defaults__ = (0, "normal", msg, None, BLK_NUM_PER_REP, BLK_NUM_PER_REP)

        check_params_ins = check_params(*check_addr_overlap_params)
        if not self.check_overlap_param(
                check_params_ins.mask, check_params_ins.repeat_times,
                check_params_ins.dst_offset, check_params_ins.src_offset):
            return
        if check_params_ins.mask_mode == "counter":
            if not TikSocManager.is_910b_soc():
                check_params_ins.repeat_times = ceil_div(
                    check_params_ins.mask, check_params_ins.nblock * check_params_ins.block_len)
                check_params_ins.mask = check_params_ins.mask % (check_params_ins.nblock * check_params_ins.block_len)
                if check_params_ins.mask == MASK_VALUE_ZERO:
                    check_params_ins.mask = check_params_ins.nblock * check_params_ins.block_len
            else:
                # for vcopy, counter mode is special
                check_params_ins.dst_nblock = ceil_div(
                    check_params_ins.mask * DTYPE_SIZE[check_params_ins.dst.dtype], self.one_blk_size)
                check_params_ins.src_nblock = ceil_div(
                    check_params_ins.mask * DTYPE_SIZE[check_params_ins.src.dtype], self.one_blk_size)
                check_params_ins.mask = check_params_ins.nblock * check_params_ins.block_len

        tensor_params = namedtuple("IntervalParams", "blk_stride rep_stride stride_unit offset dtype nblock block_len")
        dst_params_ins = tensor_params(check_params_ins.dst_blk_stride, check_params_ins.dst_rep_stride,
                                       check_params_ins.stride_unit, check_params_ins.dst_offset,
                                       check_params_ins.dst.dtype,
                                       check_params_ins.dst_nblock, check_params_ins.dst_block_len)
        src_params_ins = tensor_params(check_params_ins.src_blk_stride, check_params_ins.src_rep_stride,
                                       check_params_ins.stride_unit, check_params_ins.src_offset,
                                       check_params_ins.src.dtype,
                                       check_params_ins.src_nblock, check_params_ins.src_block_len)
        # check address overlap
        if check_params_ins.repeat_times == 1:
            if check_params_ins.src_offset == check_params_ins.dst_offset and \
                    check_params_ins.src_blk_stride == check_params_ins.dst_blk_stride:
                # 100 percent same, support
                pass
            else:
                self.check_overlap_single(
                    check_params_ins.repeat_times, dst_params_ins, src_params_ins, check_params_ins)
        else:
            self.check_overlap_many(dst_params_ins, src_params_ins, check_params_ins)


class ReduceCheckParams(BaseCheckParams):
    """
    reduce check params
    """

    def __init__(self, name, params, cal_index):
        super().__init__(params)
        self.name = name
        self.print_name = REDUCE_NAME_DICT.get(name)
        self.cal_index = cal_index

    @staticmethod
    def align_start_pos(start_pos, dtype_size):
        """
        Align the src address in the per iteration of vreduce

        Parameters
        ----------
        start_pos: int/Scalar, the start position in work_tensor
        dtype_size: int, the size of element in src. unit is Byte

        Returns
        -------
        align start position
        """
        block_size = get_block_size()
        return ceil_div(start_pos * dtype_size, block_size) * block_size // dtype_size

    @staticmethod
    def vreduce_create_mask(data_len):
        """
        get mask in the "0101010101" format

        Parameters
        ----------
        data_len: int, src_element // 2, src_element is the element count of vreduce instruction will cover

        Returns
        -------
        the new mask
        """
        # mask_len record the valid data num of low mask
        # B16: max data num of low mask is 64
        # B32: max data num of low mask is 64, and all data can select by low mask
        # out data saved as: Data1Index1Data2Index2....DatanIndexn
        # so valid data is half of max data num
        mask_len = 32
        high_mask_bit = data_len - mask_len
        high_mask = 0
        low_mask = 0
        for _ in range(0, high_mask_bit):
            high_mask = high_mask << 2
            high_mask = high_mask | 1

        if data_len >= mask_len:
            low_mask_bit = mask_len
        else:
            low_mask_bit = data_len
        for _ in range(0, low_mask_bit):
            low_mask = low_mask << 2
            low_mask = low_mask | 1
        return [high_mask, low_mask]

    @staticmethod
    def check_vreduce_repeat_times(repeat_times, cal_index, dtype):
        """
        check repeat_times of vec_reduce_max/vec_reduce_min instruction

        Parameters
        ----------
        repeat_times: int/Scalar, the times of instrction run
        cal_index: bool, if calculate index
        dtype: s16/f16/f32, tensor's dtype

        Returns
        -------
        None
        """
        if cal_index:
            if dtype == "int16":
                TikCheckUtil.check_in_range_by_dtype(
                    repeat_times, msg="int16 data type repeat_times should be in [%d, %d] but get %s"
                                      % (MIN_REPEAT_TIMES, MAX_REPEAT_TIMES, repeat_times),
                    var_range=[MIN_REPEAT_TIMES, MAX_REPEAT_TIMES])
            elif dtype == "float16":
                max_repeat_times = 511
                TikCheckUtil.check_in_range_by_dtype(
                    repeat_times, msg="float16 data type repeat_times should be in [%d, %d] but get %s"
                                      % (MIN_REPEAT_TIMES, max_repeat_times, repeat_times),
                    var_range=[MIN_REPEAT_TIMES, max_repeat_times])
            else:
                TikCheckUtil.check_in_range_by_dtype(
                    repeat_times, msg="float32 data type repeat_times should be in [%d, %d] but get %s"
                                      % (MIN_REPEAT_TIMES, MAX_VREDUCE_REPEAT_TIMES, repeat_times),
                    var_range=[MIN_REPEAT_TIMES, MAX_VREDUCE_REPEAT_TIMES])
        else:
            TikCheckUtil.check_in_range_by_dtype(
                repeat_times, msg="repeat_times should be in [%d, %d] but get %s"
                                  % (MIN_REPEAT_TIMES, MAX_VREDUCE_REPEAT_TIMES, repeat_times),
                var_range=[MIN_REPEAT_TIMES, MAX_VREDUCE_REPEAT_TIMES])

    @staticmethod
    def _calculate_vecotor_max_offset(repeat_times, max_offset_params, mask_len):
        """
        calculate vector max offset
        Parameters
        ----------
        repeat_times: repeat times
        max_offset_params: max offset params
        mask_len: mask len

        Returns
        -------

        """
        if isinstance(repeat_times, int) and repeat_times == 0:
            return 0
        blk_num_last_rep = ceil_div(mask_len, max_offset_params.block_len)
        if mask_len % max_offset_params.block_len:
            ele_num_last_blk = mask_len % max_offset_params.block_len
        else:
            ele_num_last_blk = max_offset_params.block_len
        # last rep has multi blocks, blk_stride is zero, last blk num must be set to block len
        if blk_num_last_rep > 0 and max_offset_params.blk_stride == 0:
            ele_num_last_blk = max_offset_params.block_len
        # blk_stride: stride, rep_stride: stride, unit: 32B
        max_offset = \
            ((repeat_times - 1) * max_offset_params.rep_stride + (blk_num_last_rep - 1)
             * max_offset_params.blk_stride) * max_offset_params.block_len + ele_num_last_blk
        return max_offset

    def reduce_cal_extent_stride_unit_mask(self, repeat_times, rep_stride):
        """
        calculate extent, based on mask, stride_unit

        Parameters
        ----------
        repeat_times: Repeated iterations times
        rep_stride: offset of dst/src operator in the same block between adjacent iterations

        Returns
        -------
        extent
        """
        blk_stride = 1
        # blk_stride: stride, rep_stride: stride, unit: 32B
        extent = ((repeat_times - 1) * rep_stride + (BLK_NUM_PER_REP - 1) * blk_stride + 1) * self.one_blk_size
        return Expr(extent).get()

    def check_base_params(self):
        """
        check base params
        :return:
        """
        # check tensor
        TikCheckUtil.check_type_match(self.dst, Tensor, "dst should be tensor but get %s" % type(self.dst))
        if not TikSocManager.is_v300_610l_soc():
            TikCheckUtil.check_type_match(
                self.work_tensor, Tensor, "work_tensor should be tensor but get %s" % type(self.work_tensor))
            TikCheckUtil.check_equality(
                self.work_tensor.scope, scope_ubuf,
                "work_tensor's scope must be UB but get %s" % self.work_tensor.scope)
            TikCheckUtil.check_equality(
                self.dst.dtype, self.work_tensor.dtype,
                "work_tensor's type should be %s but get %s" % (self.dst.dtype, self.work_tensor.dtype))
        TikCheckUtil.check_type_match(self.src, Tensor, "src should be tensor but get %s" % type(self.src))
        # check scope
        TikCheckUtil.check_equality(
            self.src.scope, scope_ubuf, "src's scope must be UB but get %s" % self.src.scope)
        TikCheckUtil.check_equality(
            self.dst.scope, scope_ubuf, "dst's scope must be UB but get %s" % self.dst.scope)
        # dtype of dst, work_tensor and src must be same
        TikCheckUtil.check_equality(
            self.dst.dtype, self.src.dtype,
            "dst's type should be %s but get %s" % (self.src.dtype, self.dst.dtype))
        # check tensor dtype
        TikCheckUtil.check_equality(
            self.dst.dtype, self.src.dtype,
            "Intrinsic %s's src's dtype should be equal to dst's dtype" % self.print_name)
        TikCheckUtil.check_equality(
            intrinsic_check_support("Intrinsic_%s" % self.name, self.dst.dtype), True,
            gen_api_check_statement(self.dst.dtype, self.print_name))

        TikCheckUtil.check_type_match(
            self.control_op.repeat_times, (int, Scalar, Expr),
            "repeat_times should be int, Scalar or Expr but get %s" % type(self.control_op.repeat_times))
        TikCheckUtil.check_type_match(
            self.src_rep_stride, (int, Scalar, Expr),
            "src_rep_stride should be int, Scalar or Expr but get %s" % type(self.src_rep_stride))
        TikCheckUtil.check_type_match(
            self.cal_index, bool,
            "cal_index should be bool but get %s" % type(self.cal_index))
        check_scalar_int32(
            self.control_op.repeat_times, "repeat_times should be a scalar of int32")

    def check_address_overlap(self, params, dst_offset=0, src_offset=0):
        """
        check address overlap
        :param params: contains variable mask, dst, src, repeat_times, src_rep_stride
        :param dst_offset:
        :param src_offset:
        :return:
        """
        n_block = 1
        mask, dst, src, repeat_times, src_rep_stride = params
        # check address overlap
        if src.buffer == dst.buffer:
            if all(isinstance(value, int) for value in (repeat_times, 1, 1, src_rep_stride, 0)):
                block_len = 2
                if isinstance(dst_offset, int) and isinstance(src_offset, int):
                    self.check_address_overlapping((self.name, mask, dst, src, n_block,
                                                    self.one_rep_size // max(
                                                        get_bit_len(dst.dtype), get_bit_len(src.dtype)),
                                                    block_len, self.one_rep_size // get_bit_len(src.dtype),
                                                    repeat_times, VREDUCE_DEFAULT_DST_BLK_STRIDE,
                                                    VREDUCE_DEFAULT_SRC_BLK_STRIDE,
                                                    VREDUCE_DEFAULT_DST_REP_STRIDE, src_rep_stride,
                                                    Expr(dst.offset + dst_offset).eval_value(),
                                                    Expr(src.offset + src_offset).eval_value()))

    def check_reduce_params(self):
        """
        check the args of vreduce instructions

        Parameters
        ----------

        Returns
        -------
        None
        """
        self.check_base_params()
        check_scalar_dtype(self.src_rep_stride, "src_rep_stride should be a scalar of int/uint")

        # if dtype is B16, current algorithm support max repeat_times is 16320
        # if dtype is B32, current algorithm support max repeat_times is 8160
        # if max repeat times over the limit, should update the algorithm
        self.check_vreduce_repeat_times(self.control_op.repeat_times, self.cal_index, self.src.dtype)

        TikCheckUtil.check_in_range_by_dtype(
            self.src_rep_stride, msg="src_rep_stride should be in the range of [%d, %d], "
                                     "input src_rep_stride: %s" % (0, MAX_REP_STRIDE_DOUBLE_BYTE, self.src_rep_stride),
            var_range=[0, MAX_REP_STRIDE_DOUBLE_BYTE])

    def cal_index_is_true(self, reduce_op_params, it1_output_count, rep_is_scalar, work_tensor_elements):
        """
        work tensor offset is not none branching and cal index is True
        Parameters
        ----------
        reduce_op_params: reduce op params
        it1_output_count: it1 output count
        rep_is_scalar: repeat time is scalar
        work_tensor_elements: work tensor elements

        Returns
        -------

        """
        per_rep_output = 2
        dtype_size = DTYPE_SIZE[reduce_op_params.src.dtype]
        it2_align_start = self.align_start_pos(it1_output_count, dtype_size)
        element_num_per_rep = self.one_rep_size // dtype_size

        it2_output_count, it3_start_pos = self._vreduce_body_cal(
            (it1_output_count, it2_align_start, element_num_per_rep, per_rep_output, dtype_size))
        if it2_output_count == per_rep_output and not rep_is_scalar:
            it3_start_pos = it2_align_start
            res_index = it3_start_pos + 1
        else:
            res_index = it3_start_pos + 1
            if it2_output_count > element_num_per_rep or rep_is_scalar:
                _, it4_start_pos = self._vreduce_body_cal(
                    (it2_output_count, it3_start_pos, element_num_per_rep, per_rep_output, dtype_size))
                res_index = it4_start_pos + 1

        need_size = res_index + 1

        TikCheckUtil.check_ge(work_tensor_elements, need_size,
                              "work_tensor's element should not be less than %s"
                              " but get %s" % (need_size,
                                               work_tensor_elements))

    def work_tensor_offset_not_none(self, reduce_op_params, cal_index, rep_is_scalar):
        """
        work tensor offset is not none branching
        Parameters
        ----------
        reduce_op_params: check params
        rep_is_scalar: repeat time is scalar
        cal_index: is get max/min value's index

        Returns
        -------

        """
        per_rep_output = 2
        if reduce_op_params.work_tensor_offset is not None:
            work_tensor_elements = reduce_mul(reduce_op_params.work_tensor.original_shape) - \
                                   reduce_op_params.work_tensor_offset
            it1_output_count = per_rep_output * reduce_op_params.repeat_times

            if not cal_index:
                TikCheckUtil.check_ge(work_tensor_elements, it1_output_count,
                                      "work_tensor's element should be "
                                      "more than %s but get %s" % (it1_output_count,
                                                                   work_tensor_elements))
            else:
                self.cal_index_is_true(reduce_op_params, it1_output_count, rep_is_scalar, work_tensor_elements)

    def check_space_overflow(self, check_params, cal_index=False, rep_is_scalar=False):
        """
        check space is overflow
        Parameters
        ----------
        check_params: check params
        rep_is_scalar: repeat time is scalar
        cal_index: is get max/min value's index

        Returns
        -------

        """

        reduce_op_param = namedtuple(
            "ReduceOpParams", "mask dst work_tensor src dst_offset work_tensor_offset src_offset repeat_times "
                              "src_rep_stride")

        reduce_op_param_ins = reduce_op_param(*check_params)

        # check dst tensor size
        if reduce_op_param_ins.dst_offset is not None:
            # if need save index, need 2 element, else only need 1 elememt
            if not cal_index:
                required_dst_size = 1
            else:
                required_dst_size = 2

            dst_size = reduce_mul(reduce_op_param_ins.dst.original_shape) - reduce_op_param_ins.dst_offset
            TikCheckUtil.check_ge(dst_size, required_dst_size,
                                  "dst's element should be more than %s"
                                  " but input %s" % (required_dst_size,
                                                     dst_size))

        # check src tensor size
        if reduce_op_param_ins.src_offset is not None:
            # check tensor overflow
            # cause tensor has at least one element, dst does not need to check.
            src_blk_stride = 1
            self.check_tensor_overflow(((reduce_op_param_ins.src,), reduce_op_param_ins.mask,
                                        reduce_op_param_ins.repeat_times, (src_blk_stride,),
                                        (reduce_op_param_ins.src_rep_stride,), ("src",)))

        self.work_tensor_offset_not_none(reduce_op_param_ins, cal_index, rep_is_scalar)

    def check_dtype_overflow(self, repeat_times, src, rep_stride):
        """
        check overflow
        """

        element_byte = self.reduce_cal_extent_stride_unit_mask(repeat_times, rep_stride)
        element_byte = Expr(element_byte).eval_value()
        if element_byte is not None:
            bit_len = get_bit_len(src.dtype)
            byte_len = bit_len // 8
            element = element_byte // byte_len
            if src.dtype == "int16":
                limit = 2 ** 15 - 1
            elif src.dtype == "float16":
                limit = 65504
            else:
                limit = 2 ** 32 - 1

            TikCheckUtil.check_le(element, limit,
                                  "Elements number should be less than the limit of"
                                  " unsigned int type corresponding to %s which is "
                                  "%s but get %s" % (src.dtype,
                                                     limit + 1, element))

    def check_operator_address_align(self):
        """
        check operator address align
        :return: None
        """
        src_align = vec_template_align(self.src.dtype)
        check_address_align((self.src,), ("src",), src_align)
        dst_align = VREDUCE_DST_ALIGN[self.name]
        check_address_align((self.dst,), ("dst",), dst_align)
        if not TikSocManager.is_v300_610l_soc():
            work_align = vec_template_align(self.work_tensor.dtype)
            check_address_align((self.work_tensor,), ("work_tensor",), work_align)

    def check_all(self):
        """
        encapsulation all check
        :return:
        """
        self.check_reduce_params()
        self.check_operator_address_align()

    def vector_tensor_overflow_check(self, check_params, ori_offset=0):
        """
        check overflow vector tensor
        Parameters
        ----------
        ori_offset: tensor offset
        check_params: check params

        Returns
        -------
        no return
        """
        vector_overflow_check = namedtuple("VectorOverflowCheck",
                                           "tensor mask nblock block_len repeat blk_stride rep_stride")
        vector_overflow_check_ins = vector_overflow_check(*check_params)
        msg = "tensor overflow"
        value_range = [vector_overflow_check_ins.repeat, vector_overflow_check_ins.blk_stride,
                       vector_overflow_check_ins.rep_stride]
        if is_basic_expr(TikUtil.to_list(vector_overflow_check_ins.mask)) \
                or any(is_basic_expr([value]) for value in value_range):
            return
        if vector_overflow_check_ins.repeat == 0:
            return
        if not isinstance(vector_overflow_check_ins.nblock, int):
            return

        offset = vector_overflow_check_ins.tensor.offset

        if isinstance(offset, (tvm.tir.IntImm, tvm.tir.FloatImm, tvm.tir.StringImm)):
            offset = offset.value
        total_size = reduce_mul(vector_overflow_check_ins.tensor.original_shape)
        extend_offset = self.vector_max_offset_cal(vector_overflow_check_ins)
        # offset means offset away from tensor head address, it's 16 for tensor[16]
        # entend_offset means valid data offset
        need_offset = get_need_offset(ori_offset, extend_offset, offset)
        if need_offset is not None:
            TikCheckUtil.check_le(
                need_offset, total_size, "%s, expected elements nums: %s, actual elements nums: %s"
                                         % (msg, need_offset, total_size))

    def vector_max_offset_cal(self, offset_cal_params):
        """
        get max offset of calculate vector
        Parameters
        ----------

        offset_cal_params: contain
        -    stride1: blk_stride
        -    stride2: rep_stride
        -    stride_unit: param indicating gap/stride
        -    nblock: numbers of block in one repeat

        Returns
        -------
        max_offset
        """

        if not isinstance(offset_cal_params.mask, (list, tuple)):
            mask = [offset_cal_params.mask]
        else:
            mask = offset_cal_params.mask
        # mask_len, the last effective element
        mask_len = 0
        bit_len = get_bit_len(offset_cal_params.tensor.dtype)
        if bit_len == BIT_LEN_32:
            mask_len = get_32bit_dtype_mask_len(mask, "normal")
        elif bit_len == BIT_LEN_8 or BIT_LEN_16:
            mask_len = get_8or16bit_dtype_mask_len(mask)

        # nano soc, block_size/repeat_size/mask is only half of others.
        if TikSocManager.is_nano_soc():
            mask_len = ceil_div(mask_len, 2)
        max_offset = self._calculate_vecotor_max_offset(offset_cal_params.repeat, offset_cal_params, mask_len)

        return max_offset

    def _vreduce_body_cal(self, cal_params):
        """
        vreduce body cal
        Parameters
        ----------
        cal_params: tuple contains pre_data_count, cur_start_pos, element_num_per_rep, per_rep_output, dtype_size params

        Returns
        -------

        """
        body_cal_params = namedtuple(
            "BodyCalParams", "pre_data_count cur_start_pos element_num_per_rep per_rep_output dtype_size")
        body_cal_params_ins = body_cal_params(*cal_params)
        body_rep_times = body_cal_params_ins.pre_data_count // body_cal_params_ins.element_num_per_rep
        body_output_count = body_cal_params_ins.per_rep_output * body_rep_times
        has_tail = (body_cal_params_ins.pre_data_count % body_cal_params_ins.element_num_per_rep) != 0
        tail_output_count = body_cal_params_ins.per_rep_output if has_tail else 0
        output_count = body_output_count + tail_output_count
        next_start_pos = self.align_start_pos(
            body_cal_params_ins.cur_start_pos + output_count, body_cal_params_ins.dtype_size)
        return output_count, next_start_pos


class ReduceAddCheckParams(BaseCheckParams):
    """
    Reduce add check params
    """

    def check_reduce_add_params(self):
        """
        check reduce add params
        :return: None
        """
        # check dst/src/work_tensor
        TikCheckUtil.check_type_match(self.dst, Tensor, "dst should be tensor, input type is:%s" % type(self.dst))
        TikCheckUtil.check_type_match(self.src, Tensor, "src should be tensor, input type is:%s" % type(self.src))
        if not TikSocManager.is_v300_610l_soc():
            TikCheckUtil.check_type_match(
                self.work_tensor, Tensor, "work_tensor should be tensor, input type is:%s" % type(self.work_tensor))
            TikCheckUtil.check_equality(
                self.work_tensor.scope, scope_ubuf,
                "work_tensor's scope must be UB, input scope is:%s" % self.work_tensor.scope)
            # check work_tensor dtype
            TikCheckUtil.check_equality(
                self.src.dtype, self.work_tensor.dtype,
                "work_tensor should have the same dtype as src")
        TikCheckUtil.check_equality(
            self.dst.scope, scope_ubuf, "dst's scope must be UB, input scope is:%s" % self.dst.scope)
        TikCheckUtil.check_equality(
            self.src.scope, scope_ubuf, "src's scope must be UB, input scope is:%s" % self.src.scope)
        # check dtype
        TikCheckUtil.check_equality(
            self.dst.dtype, self.src.dtype, "Intrinsic vec_reduce_add's src's dtype should be equal to dst's dtype")
        TikCheckUtil.check_equality(
            api_check_support("tik." + "vec_reduce_add", self.dst.dtype),
            True, gen_api_check_statement(self.dst.dtype, "vec_reduce_add"))
        # check src_rep_stride
        TikCheckUtil.check_type_match(
            self.src_rep_stride, (int, Expr, Scalar),
            "src_rep_stride should be int, Expr or Scalar")
        check_scalar_dtype(
            self.src_rep_stride,
            "scalar_src_rep_stride should be a scalar of int/uint")
        TikCheckUtil.check_in_range_by_dtype(
            self.src_rep_stride, msg="src_rep_stride should be in the range of [%d, %d], input value is %s"
                                     % (0, MAX_REP_STRIDE_DOUBLE_BYTE, str(self.src_rep_stride)),
            var_range=[0, MAX_REP_STRIDE_DOUBLE_BYTE])
        # check repeat times
        TikCheckUtil.check_type_match(
            self.control_op.repeat_times, (int, Scalar, Expr),
            "repeat_times should be int, Scalar or Expr, input type of "
            "repeat_times: %s" % type(self.control_op.repeat_times))
        check_scalar_int32(self.control_op.repeat_times,
                           "scalar_repeat_time should be a scalar of int32")
        TikCheckUtil.check_in_range_by_dtype(
            self.control_op.repeat_times,
            msg="repeat_times should be in the range of [%d, %d], input repeat_times: %s" % (
                MIN_REPEAT_TIMES, MAX_VREDUCE_REPEAT_TIMES, self.control_op.repeat_times),
            var_range=[MIN_REPEAT_TIMES, MAX_VREDUCE_REPEAT_TIMES])

    def check_reduce_add_tensor_overflow(self):
        """
        check tensor overflow
        cause tensor has at least one element, dst does not need to check.
        :return: None
        """
        src_blk_stride = 1
        self.check_tensor_overflow(((self.src,), self.control_op.mask,
                                    self.control_op.repeat_times, (src_blk_stride,),
                                    (self.src_rep_stride,), ("src",)))

    def check_work_tensor(self):
        """
        check work_tensor size, when work_tensor's src and dst overlap
        :return:
        """
        if TikSocManager.is_v300_610l_soc():
            return
        if TikSocManager.is_910b_soc():
            work_tensor_need = Expr(
                ceil_div(self.control_op.repeat_times, MAX_REPEAT_TIMES) + self.work_tensor.offset).eval_value()
        else:
            work_tensor_need = Expr(self.control_op.repeat_times + self.work_tensor.offset).eval_value()
        if work_tensor_need is not None:
            work_tensor_total_size = reduce_mul(self.work_tensor.original_shape)
            TikCheckUtil.check_ge(
                work_tensor_total_size, work_tensor_need,
                "work_tensor tensor overflow, instruction need %s but only "
                "%s" % (work_tensor_need, work_tensor_total_size))

    def check_address_aligned(self):
        """
        check operator address 32B aligned
        :return: None
        """
        src_align = vec_template_align(self.src.dtype)
        check_address_align((self.src,), ("src",), src_align)
        dst_align = vec_template_align(self.dst.dtype)
        check_address_align((self.dst,), ("dst",), dst_align)
        if not TikSocManager.is_v300_610l_soc():
            work_align = vec_template_align(self.work_tensor.dtype)
            check_address_align((self.work_tensor,), ("work_tensor",), work_align)

    def check_tensor_overlap(self):
        """
        check tensor overlap
        :return:
        """
        self._check_vec_reduce_add_operator_overlap()

    def all_add_reduce_check(self):
        """
        encapsulation all check, v300 The work tensor is invalid and is not verified.
        :return:
        """
        self.check_reduce_add_tensor_overflow()

        self.check_work_tensor()
        self.check_address_aligned()
        self.check_tensor_overlap()

    def _check_vec_reduce_add_operator_overlap(self):
        """
        check vector reduce add operator overlap

        Parameters
        ----------

        Returns None
        -------
        """

        default_dst_rep_stride = 0
        self._check_vec_reduce_add_two_operator_overlap(self.src, self.dst, default_dst_rep_stride,
                                                        "src and dst")
        if not TikSocManager.is_v300_610l_soc():
            default_dst_rep_stride = 1
            self._check_vec_reduce_add_two_operator_overlap(self.src, self.work_tensor, default_dst_rep_stride,
                                                            "src and work_tensor")
            if self.work_tensor.buffer == self.dst.buffer:
                work_tensor_start = Expr(self.work_tensor.offset).eval_value()
                work_tensor_stop = Expr(
                    self.control_op.repeat_times + self.work_tensor.offset).eval_value()
                dst_start = Expr(self.dst.offset).eval_value()
                dst_stop = dst_start + 1
                is_all_not_none = all((value is not None)
                                      for value in (work_tensor_start, work_tensor_stop, dst_start, dst_stop))
                if is_all_not_none and max(work_tensor_start, dst_start) < min(work_tensor_stop, dst_stop):
                    TikCheckUtil.raise_error("vec_reduce_add work_tensor and dst "
                                             "address overlapping error.")

    def _check_vec_reduce_add_two_operator_overlap(self, src, dst, dst_rep_stride, msg):
        """
        check vec_reduce_add if two of the tensor overlapping
        """
        src_blk_stride = 1
        default_dst_blk_stride = 0
        block_len = 1
        default_nblock = 1
        if src.buffer == dst.buffer:
            if all(isinstance(value, int) for
                   value in (self.control_op.repeat_times, self.src_rep_stride)):
                if self.control_op.repeat_times == 1:
                    _default_dst_blk_stride = src_blk_stride
                _default_dst_rep_stride = dst_rep_stride

                self.check_address_overlapping(
                    ("vec_reduce_add", self.control_op.mask, dst, src, default_nblock,
                     self.one_rep_size // max(
                         get_bit_len(dst.dtype), get_bit_len(src.dtype)),
                     block_len, self.one_rep_size // get_bit_len(src.dtype),
                     self.control_op.repeat_times, default_dst_blk_stride, src_blk_stride,
                     dst_rep_stride, self.src_rep_stride,
                     Expr(dst.offset).eval_value(),
                     Expr(src.offset).eval_value()), msg=msg)
