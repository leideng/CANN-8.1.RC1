#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_build_check.py
DESC:     tik_inner_
CREATED:  2021-12-13 6:50 AM
MODIFIED: 2021-12-13 6:50 AM
"""
from tbe.tik.api.tik_scalar import InputScalar
from tbe.tik.api.tik_tensor_addr_list import TensorAddrList
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.common.util import get_check_feed_dict
from tbe.tik.common.util import DTYPE_INT_VALUE
from tbe.tik.common.util import DTYPE_FLOAT_VALUE
from tbe.tik.tik_lib.tik_params import DTYPE_REL_TOL
from tbe.tik.tik_lib.tik_check_util import float_in_range
from tbe.tik.tik_lib.tik_check_util import print_error_msg
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil


def _gen_inputscalar_num(inputs_tmp, input_output_name_set, input_output_tensor_name_set,
                         non_flowtable_set=None):
    """
    use to gen inputscalar number
    Parameters
    ----------
    inputs_tmp: total inputs
    input_output_name_set: input output name set
    input_output_tensor_name_set: name set
    non_flowtable_set: input output name set Excluding flowtable

    Returns
    -------
    no return
    """
    input_scalar_name_set = set()
    input_scalar_num = 0
    input_scalar_index_list = []
    input_tensor_index_list = []
    for i, tmp_tensor in enumerate(inputs_tmp):
        input_output_name_set.add(tmp_tensor.name)
        if non_flowtable_set is not None:
            non_flowtable_set.add(tmp_tensor.name)
        if isinstance(tmp_tensor, InputScalar):
            input_scalar_name_set.add(tmp_tensor.name)
            input_scalar_num += 1
            input_scalar_index_list.append(i)
        elif isinstance(tmp_tensor, (Tensor, TensorAddrList)):
            input_output_tensor_name_set.add(tmp_tensor.name)
            input_tensor_index_list.append(i)
        else:
            TikCheckUtil.raise_error("BuildCCE's input can only be Tensor or InputScalar!")
    return [input_scalar_name_set, input_scalar_num, input_scalar_index_list, input_tensor_index_list]


def _check_scalar_behind_tensor(tensor_index_list, scalar_index_list):
    """
    use to check scalar is behind tensor
    Parameters
    ----------
    tensor_index_list:
    scalar_index_list:

    Returns
    -------
    no return
    """
    if tensor_index_list and scalar_index_list:
        # only compare when two list are not empty
        TikCheckUtil.check_ge(
            scalar_index_list[0], tensor_index_list[-1],
            "All InputScalar in inputs should be put behind the Tensor!"
            " But find Tensor index: {tensor_index}, InputScalar index: {inputscalar_index}.".format(
                tensor_index=tensor_index_list[-1], inputscalar_index=scalar_index_list[0]))


def check_build_cce_inputs_outpus(inputs_tmp, outputs_tmp, flowtable_tmp):
    """
    check build_cce api input and output and get input_output_tensor_name_set
    """
    input_output_name_set = set()
    input_output_tensor_name_set = set()
    non_flowtable_info = set()
    input_info = _gen_inputscalar_num(
        inputs_tmp, input_output_name_set, input_output_tensor_name_set, non_flowtable_info)
    output_info = _gen_inputscalar_num(
        outputs_tmp, input_output_name_set, input_output_tensor_name_set, non_flowtable_info)
    flowtable_info = _gen_inputscalar_num(flowtable_tmp, input_output_name_set, input_output_tensor_name_set)

    # add check input name is all different
    TikCheckUtil.check_equality(
        len(input_info[0] | output_info[0] | flowtable_info[0]),
        input_info[1] + output_info[1] + flowtable_info[1],
        "Duplicate name found in InputScalar! Please make all the InputScalar with different name!")

    # check tensor name and input scalar name has same!
    TikCheckUtil.check_equality(
        len(input_output_tensor_name_set &
            (input_info[0] | output_info[0] | flowtable_info[0])),
        0, "InputScalar's name is used in input or output Tensor!")
    # check tensor name has not same!
    TikCheckUtil.check_equality(
            len(input_output_tensor_name_set),
            len(input_info[3] + output_info[3] + flowtable_info[3]),
            "Tensor's name is duplicate in input or output!")

    # check if sequence of inputs is Tensor first and inputScalar second.
    _check_scalar_behind_tensor(input_info[3], input_info[2])
    _check_scalar_behind_tensor(output_info[3], output_info[2])
    _check_scalar_behind_tensor(flowtable_info[3], flowtable_info[2])
    return input_output_name_set, non_flowtable_info


def check_start_profiling_feed_dict(last_inputs, feed_dict, last_flowtable):
    """
    for startProfiling function
    once last_inputs not empty, we should check key is same
    """
    if last_inputs or feed_dict or last_flowtable:
        build_cce_input_list_tensor = []
        build_cce_input_list_var = []
        last_inputs = last_inputs + last_flowtable
        for i in last_inputs:
            if isinstance(i, InputScalar):
                build_cce_input_list_var.append(i.name)
            else:
                build_cce_input_list_tensor.append(i.name)
        build_cce_input_names = " ".join(i.name for i in last_inputs)
        build_cce_input_tensor_names = " ".join(build_cce_input_list_tensor)
        build_cce_input_var_names = " ".join(build_cce_input_list_var)
        build_list_tensor = {i.name for i in last_inputs}
        get_check_feed_dict(
            feed_dict, build_cce_input_list_tensor, build_cce_input_list_var, build_list_tensor,
            (build_cce_input_names, build_cce_input_tensor_names, build_cce_input_var_names))


def check_input_tensor_type_match(input_tensor, feed_dict, desc_tensor=None):
    """
    use to check input tensor type match between last_inputs and feed_dict
    Parameters
    ----------
    input_tensor: last_inputs
    feed_dict: feed_dict
    desc_tensor: A list is used to specify a special tensor's name for save the gm tensor info.
    Returns
    -------
    no return
    """
    if desc_tensor and input_tensor.name in desc_tensor:
        temp_arr = feed_dict[input_tensor.name][0]
    else:
        temp_arr = feed_dict[input_tensor.name]

    if all(isinstance(i, int) for i in input_tensor.shape):
        TikCheckUtil.check_equality(
            len(input_tensor.shape), len(temp_arr.shape),
            "%s input shape mismatch %s vs feed_dict %s" % (input_tensor.name, input_tensor.shape, temp_arr.shape))
        if any(a != b for a, b in zip(input_tensor.shape, temp_arr.shape)):
            TikCheckUtil.raise_error(
                "%s input shape mismatch %s vs feed_dict %s" % (input_tensor.name, input_tensor.shape, temp_arr.shape))
    # for bfloat16 scenarioï¼š temp_arr.dtype is tf.bfloat16.as_numpy_dtype
    # and input_tensor.dtype is "bfloat16", check_equal will fail, need to convert to string
    TikCheckUtil.check_equality(
        str(input_tensor.dtype), str(temp_arr.dtype),
        "%s input dtype mismatch %s vs %s" % (input_tensor.name, input_tensor.dtype, temp_arr.dtype))


def check_input_tensor_addr_list_type_match(_input, feed_dict):
    """
    use to check input TensorAddrList type match between last_inputs and feed_dict
    Parameters
    ----------
    _input: last_inputs
    feed_dict: feed_dict

    Returns
    -------
    no return
    """
    if all(isinstance(i, int) for i in _input.shape):
        TikCheckUtil.check_le(
            len(feed_dict[_input.name]), _input.size,
            "%s input shape mismatch %s vs feed_dict %s" % (_input.name, _input.size, len(feed_dict[_input.name])))


def check_input_scalar_type_match(input_scalar, feed_dict):
    """
    use to check input scalar type match between last_inputs and feed_dict
    Parameters
    ----------
    input_scalar: last_inputs
    feed_dict: feed_dict

    Returns
    -------
    no return
    """
    if input_scalar.dtype.startswith("int") or input_scalar.dtype.startswith("uint"):
        TikCheckUtil.check_type_match(feed_dict[input_scalar.name], int,
                                      input_scalar.name + " is " + input_scalar.dtype + ", but value is float!")
        TikCheckUtil.check_in_range_by_dtype(
            feed_dict[input_scalar.name], msg="%s is %s type, should in [%s, %s], but get %s"
            % (input_scalar.name, input_scalar.dtype, DTYPE_INT_VALUE[input_scalar.dtype][0],
               DTYPE_INT_VALUE[input_scalar.dtype][1], feed_dict[input_scalar.name]),
            var_range=[DTYPE_INT_VALUE[input_scalar.dtype][0], DTYPE_INT_VALUE[input_scalar.dtype][1]])
    if input_scalar.dtype.startswith("float"):
        TikCheckUtil.check_type_match(feed_dict[input_scalar.name], float,
                                      input_scalar.name + " is " + input_scalar.dtype + ", but value is int!")


def check_scalar_value_map(scalar_value_map):
    """
    check scalar value and dtype
    """
    TikCheckUtil.check_type_match(scalar_value_map, dict, "evaluates should be dict for BuildCCE")
    if len(scalar_value_map) > 16:
        print_error_msg("The evaluates supports up to 16 elements ,but get %s evaluates" % len(scalar_value_map))
    for scalar, value in scalar_value_map.items():
        TikCheckUtil.check_type_match(scalar, Scalar, "evaluates key should be Scalar")
        if scalar.dtype in DTYPE_INT_VALUE:
            if not isinstance(value, int):
                print_error_msg("When scalar type is %s, value should be int." % scalar.dtype)
            TikCheckUtil.check_in_range_by_dtype(
                value, msg="Scalar is %s type, should in [%s, %s], but get %s"
                % (scalar.dtype, DTYPE_INT_VALUE[scalar.dtype][0], DTYPE_INT_VALUE[scalar.dtype][1], value),
                var_range=[DTYPE_INT_VALUE[scalar.dtype][0], DTYPE_INT_VALUE[scalar.dtype][1]])
        elif scalar.dtype in DTYPE_FLOAT_VALUE:
            if not isinstance(value, float):
                print_error_msg("When scalar type is %s, value should be float." % scalar.dtype)
            if not float_in_range(value, scalar.dtype, DTYPE_REL_TOL[scalar.dtype]):
                print_error_msg(
                    "Scalar is %s type, should in [%s, %s], but get %s" %
                    (scalar.dtype, DTYPE_FLOAT_VALUE[scalar.dtype][0], DTYPE_FLOAT_VALUE[scalar.dtype][1], value))
        else:
            print_error_msg("Scalar type not support %s" % scalar.dtype)
