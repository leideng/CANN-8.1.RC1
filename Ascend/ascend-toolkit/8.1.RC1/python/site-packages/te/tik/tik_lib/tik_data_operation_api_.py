#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_data_operation_api_.py
DESC:     tik tensor explanation
CREATED:  2019-04-18 18:53:42
MODIFIED: 2020-12-7 14:04:45
"""
from collections import namedtuple
import numpy as np

from tbe import tvm
from tbe.common.platform import intrinsic_check_support
from tbe.common.platform.platform_info import api_check_support
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_cb
from tbe.common.platform import scope_smask
from tbe.common.platform import scope_ca
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_gm
from tbe.common.platform import VEC_610
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_tensor import get_addr_list
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.api.tik_ir_builder import TikIRBuilder
from tbe.tik.api.tik_tensor_addr_list import TensorAddrList
from tbe.tik.common.tik_api_map import VEC_310P
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.common_nametuple_util import V4dtransApi
from tbe.tik.common.common_nametuple_util import VtransposeApi
from tbe.tik.common.common_util import check_address_align
from tbe.tik.common.common_util import vec_template_align
from tbe.tik.common.common_check_func import check_addr_overlap_v4dtrans
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.common.tik_get_soc_name import get_soc_core_type
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import v4dtrans_decorator
from tbe.tik import debug
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_expr_convert import type_convert
from tbe.tik.tik_lib.tik_util import concat_params
from tbe.tik.tik_lib.tik_util import change_dtype_str
from tbe.tik.tik_lib.tik_util import emit_scatter_instr
from tbe.tik.tik_lib.tik_params import VA_REG
from tbe.tik.tik_lib.tik_params import INT8_MAX
from tbe.tik.tik_lib.tik_params import MIN_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MAX_VA_ADDR_NUM
from tbe.tik.tik_lib.tik_params import PIPE_V
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import PIPE_MTE1
from tbe.tik.tik_lib.tik_params import VA0_INDEX
from tbe.tik.tik_lib.tik_params import VA2_INDEX
from tbe.tik.tik_lib.tik_params import PIPE_S
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import MIN_M_LEN
from tbe.tik.tik_lib.tik_params import MAX_M_LEN
from tbe.tik.tik_lib.tik_params import MIN_CHANNELS
from tbe.tik.tik_lib.tik_params import PIPE_MTE3
from tbe.tik.tik_lib.tik_params import PIPE_MTE2
from tbe.tik.tik_lib.tik_params import LOAD_SMASK_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import LOAD_SMASK_SEGMENT_LIST
from tbe.tik.tik_lib.tik_params import TENSOR_PADDING_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import TENSOR_PADDING_SEGMENT_LIST
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_params import HAS_PARAM_CONCAT
from tbe.tik.tik_lib.tik_params import NEED_PARAM_CONCAT
from tbe.tik.tik_lib.tik_params import PADDING_LEFT_IDX
from tbe.tik.tik_lib.tik_params import PADDING_RIGHT_IDX
from tbe.tik.tik_lib.tik_params import PADDING_TOP_IDX
from tbe.tik.tik_lib.tik_params import PADDING_BOT_IDX
from tbe.tik.tik_lib.tik_params import FMATRIX_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import FMATRIX_SEGMENT_LIST
from tbe.tik.tik_lib.tik_params import PADDING_ONE_BYTE_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import PADDING_ONE_BYTE_SEGMENT_LIST
from tbe.tik.tik_lib.tik_params import PADDING_TWO_BYTE_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import PADDING_TWO_BYTE_SEGMENT_LIST
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_api_constants import VNCHWCONV_INSTR_APPENDIX_MAP
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_vector_api.tik_vector_fills_api_ import VnchwconvApi
from tbe.tik.tik_lib.tik_vector_api.tik_vnchwconv_b32_ import VnchwconvB32Api
from tbe.tik.tik_lib.tik_vector_api.tik_vnchwconv_b32_ import VnchwconvNanoApi
from tbe.tik.tik_lib.tik_vector_api.tik_params_check_fills import check_list_vnchwconv
from tbe.tik.tik_lib.tik_vector_api.tik_vector_fills_api_ import VecTransApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_fills_api_ import NanoVecTransApi

_DEFAULT_STRIDE = 0
_STRIDE_UNIT_ZERO = 0
_STRIDE_UNIT_ONE = 1
_MAX_CHANNEL_V4 = 4095


def _get_addr_list(dst_list, src_list, extents):
    """
    get addr list
    """
    dst_addr_list0 = []
    dst_addr_list1 = []
    for index in range(MAX_VA_ADDR_NUM):
        get_addr_list(dst_addr_list0, dst_list[index], "w", extent=extents[0])  # dst_extent
        get_addr_list(dst_addr_list1, dst_list[index + MAX_VA_ADDR_NUM], "w", extent=extents[0])  # dst_extent

    src_addr_list0 = []
    src_addr_list1 = []
    for index in range(MAX_VA_ADDR_NUM):
        get_addr_list(src_addr_list0, src_list[index], "r", extent=extents[1])  # src_extent
        get_addr_list(src_addr_list1, src_list[index + MAX_VA_ADDR_NUM], "r", extent=extents[1])  # src_extent
    return [dst_addr_list0, dst_addr_list1, src_addr_list0, src_addr_list1]


def _check_vscatter_vgather_operator_scope(src, dst, offset, offset_name):
    """
    check scope for vscatter and vgather

    Parameters
    ----------
    src: src operator
    dst: dst operator
    offset: addr offset tensor
    offset_name: offset tensor name

    Returns
    -------
    None
    """
    TikCheckUtil.check_equality(src.scope,
                                scope_ubuf,
                                "src's scope must be UB. "
                                "input scope: {}".format(src.scope))
    TikCheckUtil.check_equality(dst.scope,
                                scope_ubuf,
                                "dst's scope must be UB. "
                                "input scope: {}".format(dst.scope))
    TikCheckUtil.check_equality(offset.scope,
                                scope_ubuf,
                                "{}'s scope must be UB. "
                                "input scope: {}".format(offset_name,
                                                         offset.scope))


class TikDataOpApi(TikIRBuilder):
    """
    Data convert, Data fill, Data move Api
    """
    vnchwconv_api = namedtuple('VnchwconvApi', ['dst_high_half', 'src_high_half', 'dst_list', 'src_list',
                                                'repeat_times', 'dst_rep_stride', 'src_rep_stride', 'name'])

    def __init__(self):
        super().__init__()
        self.mem_stamp = []

    def _set_padding_value_emit(self, value, dst):
        if not isinstance(value, Scalar):  # immediate
            self._set_padding_emit_with_imm(dst, value)
        else:  # scalar
            with self.new_scope():
                # one ir is call_extern
                instr = tvm.call_extern(
                    "float16", "set_l0_set_value", tvm.call_intrin("float16", "tir.reinterpret", value.get()))
                self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_MTE1)
                self.emit(instr)

    @staticmethod
    def _check_padding_scope_dtype(dst, value):
        """check tensor padding input scope and dtype"""
        if TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc() or TikSocManager.is_910b_soc():
            TikCheckUtil.check_var_in_list(
                dst.scope, [scope_ca, scope_cb, scope_cbuf],
                "dst scope should be L0A, L0B or L1, input dst scope: %s." % dst.scope)
        else:
            TikCheckUtil.check_var_in_list(
                dst.scope, [scope_ca, scope_cb], "dst scope should be L0A or L0B, input dst scope: %s." % dst.scope)
        # check dst dtype
        if isinstance(value, Scalar):
            dtype_str = DTYPE_MAP[dst.dtype] + DTYPE_MAP[value.dtype]
        else:
            # dtype_str: dst_dtype add dst_dtype
            dtype_str = DTYPE_MAP[dst.dtype] * 2
        TikCheckUtil.check_var_in_list(
            dtype_str, ["u16u16", "s16s16", "f16f16"], "dtype of dst and src should be u16u16, s16s16 or f16f16")

    @staticmethod
    def _check_vector_scalar_operator_and_get_dst_name(name, dst, scalar, print_name):
        """
        check operator for vector_scalar_elewise_func and
        get special dst name for different instructions
        """
        # check instruction
        if name == "vci":
            TikCheckUtil.check_var_in_list(
                get_soc_name() + get_soc_core_type(), [VEC_610, VEC_310P],
                "only VEC support instruction vci")
            dst_name = "dst_index"
            scalar_name = "start_point"
        else:
            dst_name = "dst"
            scalar_name = "scalar"
        # check dst
        TikCheckUtil.check_type_match(
            dst, Tensor,
            "{} should be tensor, input type is {}".format(dst_name, type(dst)))
        TikCheckUtil.check_equality(
            dst.scope, "local.UB",
            "{}'s scope must be UB, not support scope: {}".format(dst_name, dst.scope))
        # check UB address 32B align
        align = vec_template_align(dst.dtype)
        check_address_align((dst,), ("dst",), align)
        # check scalar
        TikCheckUtil.check_type_match(
            scalar, (int, float, Expr, Scalar),
            "{} should be int, float, Expr or Scalar, input type is {}".format(scalar_name, type(scalar)))
        # check dtype
        if isinstance(scalar, Scalar):
            TikCheckUtil.check_equality(
                dst.dtype, scalar.dtype,
                "Intrinsic {}'s scalar's dtype should be equal to dst's dtype".format(print_name))
        TikCheckUtil.check_equality(
            api_check_support("tik." + name, dst.dtype), True, gen_api_check_statement(dst.dtype, print_name))
        if "int" in dst.dtype:
            TikCheckUtil.check_not_equality(
                type(scalar), float,
                "{} should not be float when {}.dtype is {}".format(scalar_name, dst_name, dst.dtype))
        return dst_name

    @source_info_decorator()
    def vtranspose(self, dst, src):
        """
        Transpose a continuous 16*16 two-dimensional matrix data block

        Parameters
        ----------
        src : destination operator
        dst : destination operator

        Returns
        -------
        None
        """
        repeat_times = 1
        dst_rep_stride = 0
        src_rep_stride = 0
        vec_trans_api = VtransposeApi(dst, src, repeat_times, dst_rep_stride, src_rep_stride)
        if TikSocManager.is_nano_soc():
            vtranspose_obj = NanoVecTransApi(self, vec_trans_api, "vtranspose")
        else:
            vtranspose_obj = VecTransApi(self, vec_trans_api, "vtranspose")
        vtranspose_obj.run_all()

    def config_vas(self, dst_list_src_list, dtype_str, config, extents=None):
        """
        Transpose a continuous 16*16 two-dimensional matrix data block

        Parameters
        ----------
        dst_list_src_list : dst_list_src_list
        dtype_str : dtype_str
        config : config
        extents : extents

        Returns
        -------
        None
        """
        # can't find the function in library, so disable it
        # config VAs
        with self.new_scope():
            intrin = tvm.call_extern("uint64", "scatter_vnchwconv_" +
                                     VNCHWCONV_INSTR_APPENDIX_MAP[dtype_str],
                                     VA_REG[VA0_INDEX], VA_REG[VA2_INDEX],
                                     *type_convert(config))
            addr_list_tuple = _get_addr_list(dst_list_src_list[0],  # dst_list
                                             dst_list_src_list[1],  # src_list
                                             extents)
            intrin_block = tvm.tir.Evaluate(0)
            self.source_info.set_node_span(intrin_block)
            for index, addr_list in enumerate(addr_list_tuple):
                intrin_setva = tvm.call_extern("uint64", "VA_reg_set", VA_REG[index], *addr_list)
                tmp_instr = tvm.tir.Evaluate(intrin_setva)
                self.source_info.set_node_span(tmp_instr)
                intrin_block = tvm.tir.stmt.SeqStmt([intrin_block, tmp_instr])
                self.source_info.set_node_span(intrin_block)

            tmp_instr = tvm.tir.Evaluate(intrin)
            self.source_info.set_node_span(tmp_instr)
            intrin_block = tvm.tir.stmt.SeqStmt([intrin_block, tmp_instr])
            self.source_info.set_node_span(intrin_block)
            emit_scatter_instr(self, intrin_block)

    def v210_config_var(self, dst_list_src_list, dtype_str, config):
        """
        Transv210_config_var

        Parameters
        ----------
        dst_list_src_list : dst_list_src_list
        dtype_str : dtype_str
        config : config
        extents : extents

        Returns
        -------
        None
        """
        dst_tmp = []
        src_tmp = []
        with self.new_scope():
            for index in range(len(dst_list_src_list[0])):
                dst_tmp.append(dst_list_src_list[0][index].access_ptr("w"))
                src_tmp.append(dst_list_src_list[1][index].access_ptr("r"))
            intrin = tvm.call_extern("uint32", "vec_trans_scatter_" + VNCHWCONV_INSTR_APPENDIX_MAP[dtype_str],
                                     *dst_tmp, *src_tmp, *type_convert(config))
            tmp_instr = tvm.tir.Evaluate(intrin)
            self.source_info.set_node_span(tmp_instr)
            emit_scatter_instr(self, tmp_instr)

    def assign(self, dst, src, dst_offset=0, src_offset=None):
        """
        assign src to dst

        Parameters
        ----------
        dst : destination tensor
        src : source tensor
        dst_offset: dst tensor offset
        src_offset: src tensor offset

        Returns
        -------
        None
        """
        type_list = (Tensor, Scalar, Expr, TensorAddrList)
        TikCheckUtil.check_type_match(dst, type_list,
                                      "assign only support load or "
                                      "store data between UB and REG")
        TikCheckUtil.check_type_match(src, (type_list, (int, float)),
                                      "assign only support load or "
                                      "store data between UB and REG")
        with self.new_scope():
            self.scope_attr(tvm.thread_axis("cce"), "ScalarReplace", 0)
            self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_S)
            if isinstance(dst, (Scalar, Expr)):
                dst_side = tvm.call_extern(dst.dtype, "reg", dst.get())
            else:
                dst_side = dst.access_ptr("w", extent=DTYPE_SIZE[dst.dtype], offset=dst_offset)
            if isinstance(src, (Scalar, Expr)):
                src_side = tvm.call_extern(src.dtype, "reg", src.get())
            elif isinstance(src, (int, float)):
                src_side = tvm.call_extern(dst.dtype, "reg", src)
            else:
                if src_offset is None:
                    src_side = src.access_ptr("r", extent=DTYPE_SIZE[src.dtype])
                else:
                    src_side = src.access_ptr("r", extent=DTYPE_SIZE[src.dtype], offset=src_offset)
                if self.check_tiling_scalar(dst, src):
                    dst.is_tiling_scalar = True
                    self._set_tiling_scalar_value(dst, src)

            # one ir is reg_mov
            self.emit(tvm.call_extern(dst.dtype, "reg_mov", dst_side, src_side, ))

    def check_tiling_scalar(self, dst, src):
        """
        if scalar.set_as(tiling_ub[idx]), and idx is const value, scalar is tiling_scalar

        Parameters
        ----------
        dst : destination scalar
        src : source tensor

        Returns
        -------
        whether scalar is tiling_scalar
        """
        if not self._has_tiling_ub:
            return False
        if not isinstance(dst, Scalar):
            return False
        if isinstance(src, Tensor) and src.scope == scope_ubuf and \
                src._get_last_tensor().is_tiling_tensor and isinstance(src.old_data, int):
            return True
        return False

    def assign_address(self, dst, src):
        """
        assign src addr to dst

        Parameters
        ----------
        dst : destination tensor
        src : source tensor

        Returns
        -------
        None
        """
        type_list = (Tensor, Scalar, Expr)
        TikCheckUtil.check_type_match(dst, type_list,
                                      "assign only support load or "
                                      "store data between UB and REG")
        TikCheckUtil.check_type_match(src, type_list,
                                      "assign only support load or "
                                      "store data between UB and REG")
        with self.new_scope():
            self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_S)
            if isinstance(dst, (Scalar, Expr)):
                dst_side = tvm.call_extern(dst.dtype, "reg", dst.get())
            else:
                dst_side = dst.access_ptr("w", extent=Expr(DTYPE_SIZE[dst.dtype]).get())
            if isinstance(src, (Scalar, Expr)):
                src_side = tvm.call_extern(src.dtype, "reg", src.get())
            else:
                src_side = src.access_ptr("r", extent=Expr(DTYPE_SIZE[src.dtype]).get())
            # one ir is reg_mov
            self.emit(tvm.call_extern(dst.dtype, "address_mov", dst_side, src_side, ))

    @source_info_decorator()
    @debug.set_2d_decorator
    def tensor_padding_with_matrix(self, dst, repeat_times, value=None):
        """
        Move value to dst tensor

        Parameters
        ----------
        dst : destination tensor
        value : the value
        repeat_times : [1, 255] the invoke times

        Returns
        -------
        None
        """
        # subclass has the member but parent class call it, so disable E1101
        scope_map = {
            scope_ca: "l0a",
            scope_cb: "l0b",
            scope_cc: "l0c",
            scope_cbuf: "l1",
            scope_ubuf: "ub",
            scope_gm: "out"
        }
        # check repeat_times
        TikCheckUtil.check_in_range_by_dtype(
            repeat_times, msg="repeat_times should be in the range of [{}, {}], input value is "
                              "{}".format(MIN_REPEAT_TIMES, MAX_REPEAT_TIMES, repeat_times),
            var_range=[MIN_REPEAT_TIMES, MAX_REPEAT_TIMES])
        # check dst scope
        TikCheckUtil.check_not_contains(
            get_soc_name() + get_soc_core_type(), (VEC_610, VEC_310P),
            "%s doesn't support tensor_padding_with_matrix." % (get_soc_name() + get_soc_core_type()))
        self._check_padding_scope_dtype(dst, value)
        # padding value
        if value is not None:
            TikCheckUtil.check_type_match(value, (int, float, Scalar), "value should be int or float or Scalar")
            with self.context.freeze():
                self._set_padding_value_emit(value, dst)
        # code gen
        args = concat_params([repeat_times], TENSOR_PADDING_OFFSET_LIST, TENSOR_PADDING_SEGMENT_LIST)
        with self.new_scope():
            self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_MTE1)
            instr = tvm.call_extern(
                dst.dtype, "set_" + scope_map.get(dst.scope) + "_2d", dst.access_ptr("w"), type_convert(args))
            # one ir is call_extern
            self.emit(instr)

    @source_info_decorator()
    def v4dtrans(self, chw2hwc, dst, src, m_len, channels):
        """
        transform data between chw and hwc

        Parameters
        ----------
        chw2hwc : bool, True - chw->hwc; False - hwc->chw
        dst : destination operator
        src : source operation
        m_len : H*W direction dimension
        channels: size of C

        Returns
        -------
        None
        """
        # check tensor
        TikCheckUtil.check_type_match(src, Tensor, "src's type should be tensor, input type: %s" % type(src))
        TikCheckUtil.check_type_match(dst, Tensor, "dst's type should be tensor, input type: %s" % type(dst))

        # check scope
        TikCheckUtil.check_equality(src.scope, scope_ubuf, "src's scope must be UB, input scope is: %s" % src.scope)
        TikCheckUtil.check_equality(dst.scope, scope_ubuf, "dst's scope must be UB, input scope is: %s" % dst.scope)

        # check UB address 32B align
        align = vec_template_align(src.dtype)
        check_address_align((src, dst), ("src", "dst"), align)

        # check dtype
        TikCheckUtil.check_equality(dst.dtype, src.dtype,
                                    "Intrinsic {}'s src's dtype should be equal to dst's dtype".format("v4dtrans"))
        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + "v4dtrans", dst.dtype), True,
                                    gen_api_check_statement(dst.dtype, "v4dtrans"))

        # check mode
        TikCheckUtil.check_type_match(chw2hwc, bool, "chw2hwc should be bool.")
        if chw2hwc:
            dir_v4dtrans = 0
        else:
            dir_v4dtrans = 1
        # check m_len
        if isinstance(m_len, (int, float)):
            TikCheckUtil.check_in_range_by_dtype(m_len, msg="m_len should be in the range of [%d,%d], input m_len: %s"
                                                            % (MIN_M_LEN, MAX_M_LEN, str(m_len)),
                                                 var_range=[MIN_M_LEN, MAX_M_LEN])
            image_size = m_len * get_bit_len(src.dtype) // ONE_BYTE_BIT_LEN
            TikCheckUtil.check_equality(image_size % ONE_BLK_SIZE, 0,
                                        "H*W*dtype_size should be 32 Byte aligned, input size is %s" % str(image_size))
        # check channels
        if isinstance(channels, (int, float)):
            TikCheckUtil.check_in_range_by_dtype(
                channels, msg="channels should be in the range of [%d,%d], input channels: %s"
                              % (MIN_CHANNELS, _MAX_CHANNEL_V4, str(channels)),
                var_range=[MIN_CHANNELS, _MAX_CHANNEL_V4])

        # check tensor overflow, only for immediate
        if all(Expr(value).eval_value() is not None
               for value in (m_len, channels, src.offset, dst.offset)):
            # check address overlap
            check_addr_overlap_v4dtrans((dst, src, m_len, channels,
                                         Expr(dst.offset).eval_value(), Expr(src.offset).eval_value()))

            TikCheckUtil.check_le(m_len * channels, reduce_mul(src.original_shape) - Expr(src.offset).eval_value(),
                                  "src tensor overflow, m_len*channels is too big")
            TikCheckUtil.check_le(m_len * channels, reduce_mul(dst.original_shape) - Expr(dst.offset).eval_value(),
                                  "dst tensor overflow, m_len*channels is too big")
        # code gen
        v4dtrans_obj = V4dtransApi(chw2hwc, dst, src, m_len, channels, dir_v4dtrans)
        self._v4dtrans_code_gen(v4dtrans_obj)

    # VA mode - vnchwconv
    @source_info_decorator()
    def vnchwconv(self, dst_high_half, src_high_half, dst_list, src_list, repeat_times, dst_rep_stride,
                  src_rep_stride, name=None):
        """
        used for NCHW to NHWC

        Parameters
        ----------
        dst_high_half : bool, specify the place of the data store
        src_high_half : bool, specify where data is read
        src_list : list, the src operation list
        dst_list : list, the des operation list
        repeat_times : int, Repeated iterations times
        dst_rep_stride : int, offset of dst operator in the same block between adjacent iterations
        src_rep_stride : int, offset of src operator in the same block between adjacent iterations
        name : str, the name of this directive

        Returns
        -------
        Nones
        """
        if name is None:
            name = "vnchwconv"
        vnchwconv_api = TikDataOpApi.vnchwconv_api(dst_high_half, src_high_half, dst_list, src_list, repeat_times,
                                                   dst_rep_stride, src_rep_stride, name)
        check_list_vnchwconv(src_list, dst_list)
        if TikSocManager.is_v100_soc() and src_list[0].dtype in ["float32", "int32", "uint32"]:
            vnchwconv_obj = VnchwconvB32Api(self, vnchwconv_api)
        elif TikSocManager.is_nano_soc():
            vnchwconv_obj = VnchwconvNanoApi(self, vnchwconv_api)
        else:
            vnchwconv_obj = VnchwconvApi(self, vnchwconv_api)
        vnchwconv_obj.run_all()

    @source_info_decorator()
    @debug.load_smask_decorator
    def load_smask(self, dst, src, load_size, sid=0):
        """
        load src to smask

        Parameters
        ----------
        dst: destination operator
        src: source operator
        load_size: load size, unit: 2B
        sid: SID for OUTSMMU

        Returns
        -------
        None
        """
        instr_map = {
            scope_ubuf: ["load_smask_table_from_ub", PIPE_MTE3],
            scope_gm: ["load_smask_table_from_gm", PIPE_MTE2]
        }
        TikCheckUtil.check_type_match(
            dst, Tensor, "dst should be Tensor, input type: {}".format(type(dst)))
        TikCheckUtil.check_equality(
            dst.scope, scope_smask,
            "dst scope should be SMASK, input scope: {}".format(dst.scope))
        TikCheckUtil.check_type_match(
            src, Tensor,
            "src should be Tensor, input type: {}".format(type(src)))
        TikCheckUtil.check_var_in_list(
            src.scope, (scope_ubuf, scope_gm),
            "src scope should be gm or ub, input scope: {}".format(src.scope))
        if src.scope == scope_ubuf:
            check_address_align((src,), ("src",))
        check_address_align((dst,), ("dst",))
        TikCheckUtil.check_type_match(
            load_size, int,
            "load_size should be int, input type: {}".format(type(load_size)))
        TikCheckUtil.check_in_range_by_dtype(
            load_size, msg="load_size should be in the range of [{}, {}], input load_size: {}".format(
                0, INT8_MAX, load_size), var_range=[0, INT8_MAX])
        instr_name, pipe_line = instr_map[src.scope]
        len_1 = 0
        len_7 = load_size & INT8_MAX
        params = [len_1, len_7, sid]
        args = [concat_params(params, LOAD_SMASK_OFFSET_LIST, LOAD_SMASK_SEGMENT_LIST)]
        # load_size, unit:2B
        smask_extent = load_size * 2
        with self.new_scope():
            instr = tvm.call_extern("uint16", instr_name,
                                    dst.access_ptr("w", extent=smask_extent),
                                    src.access_ptr("r", extent=smask_extent),
                                    *args)
            self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", pipe_line)
            self.emit(instr)

    def inject_sync(self, dst_blk_stride, v_rep_offset, one_blk_size):
        """
        inject sync
        """
        if isinstance(dst_blk_stride, int) and isinstance(v_rep_offset, int):
            if (dst_blk_stride * 7 + 1) * one_blk_size <= v_rep_offset:
                self.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))
        else:
            with self.if_scope((dst_blk_stride * 7 + 1) * one_blk_size <= v_rep_offset):
                self.emit(tvm.tir.Call("int32", tvm.ir.Op.get("cce.coproc_sync"), type_convert([PIPE_V])))

    def _set_fmatrix(self, *value):
        if len(value) == HAS_PARAM_CONCAT:
            fmatrix_value = value[0]
        elif len(value) == NEED_PARAM_CONCAT:
            pad, l1_h, l1_w = value
            TikCheckUtil.check_type_match(pad, list, "pad should be list")
            TikCheckUtil.check_type_match(l1_h, int, "l1_h should be int")
            TikCheckUtil.check_type_match(l1_w, int, "l1_w should be int")
            params = [l1_w, l1_h, pad[PADDING_LEFT_IDX],
                      pad[PADDING_RIGHT_IDX], pad[PADDING_TOP_IDX],
                      pad[PADDING_BOT_IDX]]
            offset_list = FMATRIX_OFFSET_LIST
            segment_list = FMATRIX_SEGMENT_LIST
            fmatrix_value = concat_params(params, offset_list, segment_list)
        # one ir is call_extern
        self.emit(tvm.call_extern("int64", "set_fmatrix", fmatrix_value))

    def _set_padding(self, value, dtype):
        if not is_basic_expr(value):
            TikCheckUtil.check_type_match(
                value, (int, float),
                "set value should be float16, uint8 or int8")
        if dtype in ("uint8", "int8"):
            params = [value, value]
            offset_list = PADDING_ONE_BYTE_OFFSET_LIST
            segment_list = PADDING_ONE_BYTE_SEGMENT_LIST
        else:
            params = [value]
            offset_list = PADDING_TWO_BYTE_OFFSET_LIST
            segment_list = PADDING_TWO_BYTE_SEGMENT_LIST
        padding = concat_params(params, offset_list, segment_list)
        with self.new_scope():
            # one ir is call_extern
            self.emit(tvm.call_extern("uint64", "set_padding", padding))

    def _set_padding_emit_with_imm(self, dst, value):
        if "l0_set_2d" in self.global_dict:
            l0_set_2d = self.global_dict["l0_set_2d"]
        else:
            l0_set_2d = self.global_scalar(dtype="int16")
            self.global_dict["l0_set_2d"] = l0_set_2d
        t_l0_set_2d = self.scalar_(dtype="int16")
        if dst.dtype == "float16":
            l0_set_2d_value = np.float16(value)
        elif dst.dtype == "int16":
            l0_set_2d_value = np.int16(value)
        else:
            l0_set_2d_value = np.uint16(value)
        l0_set_2d_value = l0_set_2d_value.view(np.int16)
        l0_set_2d_value = int(l0_set_2d_value)
        t_l0_set_2d.set_as(l0_set_2d_value)
        self.scope_attr(tvm.thread_axis("cce"), "if_protect",
                        PIPE_MTE1)
        with self.if_scope_(l0_set_2d != t_l0_set_2d):
            l0_set_2d.set_as(t_l0_set_2d)
            with self.new_scope():
                instr = tvm.call_extern(
                    "float16", "set_l0_set_value",
                    tvm.call_intrin("float16", "tir.reinterpret", l0_set_2d.get()))
                self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_MTE1)
                # one ir is call_extern
                self.emit(instr)

    @v4dtrans_decorator
    def _v4dtrans_code_gen(self, op_obj):
        # change dtype_str
        dtype_str = change_dtype_str(op_obj.dst)
        # code gen
        config = [op_obj.m_len, op_obj.channels, op_obj.dir_v4dtrans]
        args = type_convert(config)
        # cal extent
        src_extent = Expr(op_obj.m_len * op_obj.channels * DTYPE_SIZE[op_obj.src.dtype]).get()
        dst_extent = Expr(op_obj.m_len * op_obj.channels * DTYPE_SIZE[op_obj.dst.dtype]).get()
        # issue instruction
        with self.new_scope():
            instr = tvm.call_extern(op_obj.dst.dtype, "v4dtrans",
                                    op_obj.dst.reinterpret_cast_to(dtype_str).access_ptr("w", extent=dst_extent),
                                    op_obj.src.reinterpret_cast_to(dtype_str).access_ptr("r", extent=src_extent), *args)
            self.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.emit(instr)
