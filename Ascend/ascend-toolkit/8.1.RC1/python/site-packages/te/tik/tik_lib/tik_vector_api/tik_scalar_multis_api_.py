#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_double_ops_api_.py
DESC:     provide params
CREATED:  2019-04-18 18:53:42
MODIFIED: 2021-11-17 17:24:32
"""
from tbe import tvm
from tbe.common.platform import scope_ubuf
from tbe.tik.common.tik_get_soc_name import is_compatible_mode
from tbe.tik.common.util import reassign_mask
from tbe.tik.common.util import check_mask1_mask2
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_debug import vec_scalar_single_elewise_dec_new
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_util import type_convert
from tbe.tik.tik_lib.tik_params import PIPE_V
from tbe.tik.tik_lib.tik_vector_api.tik_compute_control import ControlOp
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp
from tbe.tik.tik_lib.tik_vector_api.tik_params_check import ScalarMultisCheckParams
from tbe.tik.tik_lib.tik_vector_api.tik_vector_name_map import MULTI_NAME_DICT
from tbe.tik.tik_lib.tik_vector_api.vector_common_util import gen_b64_mask_mode
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_util import concat_params
from tbe.tik.tik_lib.tik_params import VEC_SCALAR_OFFSET_LIST
from tbe.tik.tik_lib.tik_params import VEC_SCALAR_SEGMENT_LIST
from tbe.tik.tik_lib.tik_util import dtype_convert
from tbe.tik.tik_lib.tik_api_util import set_ctrl_counter_mask
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_api_util import reset_ctrl_value
from tbe.tik.tik_lib.tik_mask_concat_ import mask_concat


class ScalarMultisOps:
    """
    Double Vector Ops
    """

    def __init__(self, tik_instance, scalar_multis_api, mask_o=None):
        super().__init__()
        self.tik_instance = tik_instance
        self.name = MULTI_NAME_DICT.get(scalar_multis_api.name)
        self.print_name = scalar_multis_api.name
        if hasattr(scalar_multis_api, "api_name"):
            self.name = scalar_multis_api.api_name
        self.control_op = ControlOp(scalar_multis_api.mask, scalar_multis_api.repeat_times,
                                    scalar_multis_api.stride_unit, scalar_multis_api.mask_mode)
        self.dst_tensor_op = TensorOp(scalar_multis_api.dst, scalar_multis_api.dst_blk_stride,
                                      scalar_multis_api.dst_rep_stride, "dst")
        self.src_tensor_op = TensorOp(scalar_multis_api.src, scalar_multis_api.src_blk_stride,
                                      scalar_multis_api.src_rep_stride, "src")
        self.scalar = scalar_multis_api.scalar
        self.round_en = scalar_multis_api.round_en
        # debug check instance
        self.check_params = (self.dst_tensor_op, self.src_tensor_op, self.scalar, self.control_op)
        self.scalar_multis_check_obj = ScalarMultisCheckParams(self.print_name, self.check_params, self.round_en,
                                                               self.name)
        self.mask_o = mask_o
        self.mask_mode = scalar_multis_api.mask_mode

    @vec_scalar_single_elewise_dec_new
    def code_gen(self):
        """
        code gen

        Returns
        -------
        None
        """
        args = [self.control_op.repeat_times, self.dst_tensor_op.blk_stride, self.src_tensor_op.blk_stride,
                self.dst_tensor_op.rep_stride, self.src_tensor_op.rep_stride]
        if TikSocManager.is_v200_soc() or (TikSocManager.is_v210_soc() and TikSocManager.is_aicore_core()):
            args.append(self.control_op.stride_unit & 0b01)
            args.append((self.control_op.stride_unit & 0b10) >> 1)
        if self.name in ["vmaxs", "vmins", "vmuls", "vlrelu"]:
            args = [self.control_op.repeat_times, self.dst_tensor_op.blk_stride, self.src_tensor_op.blk_stride,
                    self.dst_tensor_op.rep_stride, self.src_tensor_op.rep_stride, self.control_op.stride_unit]
            args = [concat_params(args, VEC_SCALAR_OFFSET_LIST, VEC_SCALAR_SEGMENT_LIST)]
        if self.name == "vshr":
            args.append(self.round_en)

        if self.name == "vaxpy":
            scalar_tmp = dtype_convert(self.scalar, self.src_tensor_op.tensor_obj.dtype)
            dst_acc = self.dst_tensor_op.tensor_obj.access_ptr("rw")
        else:
            scalar_tmp = dtype_convert(self.scalar, self.dst_tensor_op.tensor_obj.dtype)
            dst_acc = self.dst_tensor_op.tensor_obj.access_ptr("w")

        if self.mask_mode == "counter" and (not TikSocManager.is_v300_610l_soc()):
            orig_ctrl = set_ctrl_counter_mask(self.tik_instance)
        else:
            orig_ctrl = ""
        self.emit_insn(dst_acc, scalar_tmp, args)

        if (self.mask_mode == "counter") and (not TikSocManager.is_v300_610l_soc()):
            reset_ctrl_value(self.tik_instance, orig_ctrl)

    def emit_insn(self, dst_acc, scalar_tmp, args):
        """
        code gen

        Returns
        -------
        None
        """
        with self.tik_instance.new_scope():
            config_args = args
            if TikSocManager.is_v300_610l_soc():
                self.tik_instance.add_source_id()
            # for regbased or int64 instr, pass all the args to backend
            if TikSocManager.is_v300_610l_soc() or TikSocManager.is_v210_vec_soc():
                config_args = [self.control_op.repeat_times, self.dst_tensor_op.blk_stride,
                               self.src_tensor_op.blk_stride, self.dst_tensor_op.rep_stride,
                               self.src_tensor_op.rep_stride, self.mask_mode,
                               self.control_op.stride_unit, self.round_en]
            elif self.dst_tensor_op.tensor_obj.dtype == "int64":
                mask_mode_list = gen_b64_mask_mode(self.control_op.mask, self.control_op.mask_mode)
                config_args = [self.control_op.repeat_times, self.dst_tensor_op.blk_stride,
                               self.src_tensor_op.blk_stride, self.dst_tensor_op.rep_stride,
                               self.src_tensor_op.rep_stride, mask_mode_list[0],
                               self.control_op.stride_unit, self.round_en]
                if len(mask_mode_list) > 1:  # len > 1 means "consecutive" mode, need append mask_call to config
                    config_args.append(mask_mode_list[1])
            instr = tvm.call_extern(self.dst_tensor_op.tensor_obj.dtype, self.name, dst_acc,
                                    self.src_tensor_op.tensor_obj.access_ptr("r"),
                                    scalar_tmp, *type_convert(config_args))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *self.mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        mask_o = self.scalar_multis_check_obj.all_check(self.tik_instance)
        if self.mask_o is None:
            self.mask_o = mask_o
        self.code_gen()


class NanoScalarMultisOps(ScalarMultisOps):
    """
    Single Vector Ops
    """

    def blklen_16_gen_code(self, params_list, api_name):
        """
        not compatible, code gen

        Parameter
        ----------
        params_list: list of params
        api_name : name of api

        Returns
        -------
        None
        """

        # vadds、vec_adds、vmuls、vec_muls、vmaxs、vmins、vshl、vshr、vrelu
        mask_o, dst, src, dst_blk_stide, src_blk_stride, dst_rep_stride, src_rep_stride = params_list
        if self.control_op.mask_mode == "counter":
            orig_ctrl = set_ctrl_counter_mask(self.tik_instance)
        args = [self.control_op.repeat_times, dst_blk_stide, src_blk_stride, dst_rep_stride, src_rep_stride]

        if api_name in ["vmaxs", "vmins", "vmuls", "vlrelu"]:
            args.append(self.control_op.stride_unit)
            args = [concat_params(args, VEC_SCALAR_OFFSET_LIST, VEC_SCALAR_SEGMENT_LIST)]
        if api_name == "vshr":
            sign = 0 if self.dst_tensor_op.tensor_obj.dtype == "uint16" else 1
            args.append(sign)
            args.append(self.round_en)

        if api_name in ("vshr", "vshl"):
            scalar_tmp = dtype_convert(self.scalar, "uint32")
        else:
            scalar_tmp = dtype_convert(self.scalar, self.dst_tensor_op.tensor_obj.dtype)

        with self.tik_instance.new_scope():
            instr = tvm.call_extern(dst.dtype, api_name,
                                    dst.access_ptr("w"), src.access_ptr("r"),
                                    scalar_tmp, *type_convert(args))
            self.tik_instance.emit(tvm.call_extern("int64", "set_vector_mask", *mask_o))
            self.tik_instance.scope_attr(tvm.thread_axis("cce"), "coproc_scope", PIPE_V)
            self.tik_instance.emit(instr)
        if self.control_op.mask_mode == "counter":
            reset_ctrl_value(self.tik_instance, orig_ctrl)

    def double_op_compatible_mode(self):
        """
        double op instr compatible mode code gen
        Returns
        -------
        """
        self.scalar_multis_check_obj.all_check(self.tik_instance)
        tensor_bit_len = get_bit_len(self.dst_tensor_op.tensor_obj.dtype)
        one_block_elements = get_block_size() // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
        # The compatibility mode consists of two incompatible instructions.
        # Each block is split into the first half and the second half.
        # Therefore, mask1 and mask2 are required for the two instructions.
        mask1, mask2 = reassign_mask(self.tik_instance, self.control_op.mask, one_block_elements)
        enable_mask1, enable_mask2 = check_mask1_mask2(mask1, mask2)

        if enable_mask1:
            mask_o = mask_concat(self.tik_instance, mask1, self.control_op.mask_mode, tensor_bit_len)
            self.blklen_16_gen_code((mask_o, self.dst_tensor_op.tensor_obj, self.src_tensor_op.tensor_obj,
                                     2 * self.dst_tensor_op.blk_stride, 2 * self.src_tensor_op.blk_stride,
                                     2 * self.dst_tensor_op.rep_stride, 2 * self.src_tensor_op.rep_stride),
                                    self.name)
        if enable_mask2:
            mask_o = mask_concat(self.tik_instance, mask2, self.control_op.mask_mode, tensor_bit_len)
            self.blklen_16_gen_code((mask_o, self.dst_tensor_op.tensor_obj[one_block_elements:],
                                     self.src_tensor_op.tensor_obj[one_block_elements:],
                                     2 * self.dst_tensor_op.blk_stride, 2 * self.src_tensor_op.blk_stride,
                                     2 * self.dst_tensor_op.rep_stride, 2 * self.src_tensor_op.rep_stride),
                                    self.name)

    def double_incompatible_mode(self):
        """
        double op compatible mode code gen
        Returns
        -------

        """
        mask_o = self.scalar_multis_check_obj.all_check(self.tik_instance)
        self.blklen_16_gen_code((mask_o, self.dst_tensor_op.tensor_obj,
                                 self.src_tensor_op.tensor_obj,
                                 self.dst_tensor_op.blk_stride, self.src_tensor_op.blk_stride,
                                 self.dst_tensor_op.rep_stride, self.src_tensor_op.rep_stride), self.name)

    def run_all_double(self):
        if is_compatible_mode():
            self.double_op_compatible_mode()
        else:
            self.double_incompatible_mode()

    def run_all_triple(self, dummy_dst):
        if is_compatible_mode():
            self.scalar_multis_check_obj.all_check(self.tik_instance)
            tensor_bit_len = get_bit_len(self.dst_tensor_op.tensor_obj.dtype)
            one_block_elements = get_block_size() // DTYPE_SIZE[self.dst_tensor_op.tensor_obj.dtype]
            mask1, mask2 = reassign_mask(self.tik_instance, self.control_op.mask, one_block_elements)
            enable_mask1, enable_mask2 = check_mask1_mask2(mask1, mask2)
            if enable_mask1:
                mask_o1 = mask_concat(self.tik_instance, mask1, self.control_op.mask_mode, tensor_bit_len)
                self.blklen_16_gen_code((mask_o1, dummy_dst,
                                         self.src_tensor_op.tensor_obj,
                                         2 * self.dst_tensor_op.blk_stride, 2 * self.src_tensor_op.blk_stride,
                                         2 * self.dst_tensor_op.rep_stride, 2 * self.src_tensor_op.rep_stride), "vmuls")
            if enable_mask2:
                one_block_elements = self.tik_instance.Scalar(name="one_block_elements", init_value=one_block_elements)
                mask_o2 = mask_concat(self.tik_instance, mask2, tensor_bit_len=tensor_bit_len)
                self.blklen_16_gen_code((mask_o2, dummy_dst[one_block_elements:],
                                         self.src_tensor_op.tensor_obj[one_block_elements:],
                                         2 * self.dst_tensor_op.blk_stride, 2 * self.src_tensor_op.blk_stride,
                                         2 * self.dst_tensor_op.rep_stride, 2 * self.src_tensor_op.rep_stride), "vmuls")
            self.tik_instance.vadd(self.control_op.mask, self.dst_tensor_op.tensor_obj,
                                   self.dst_tensor_op.tensor_obj,
                                   dummy_dst, self.control_op.repeat_times,
                                   self.dst_tensor_op.blk_stride, self.dst_tensor_op.blk_stride,
                                   self.dst_tensor_op.blk_stride, self.dst_tensor_op.rep_stride,
                                   self.dst_tensor_op.rep_stride, self.dst_tensor_op.rep_stride)
        else:
            mask_o = self.scalar_multis_check_obj.all_check(self.tik_instance)
            self.blklen_16_gen_code((mask_o, dummy_dst,
                                     self.src_tensor_op.tensor_obj,
                                     self.dst_tensor_op.blk_stride, self.src_tensor_op.blk_stride,
                                     self.dst_tensor_op.rep_stride, self.src_tensor_op.rep_stride), "vmuls")
            self.tik_instance.vadd(self.control_op.mask, self.dst_tensor_op.tensor_obj, self.dst_tensor_op.tensor_obj,
                                   dummy_dst, self.control_op.repeat_times, self.dst_tensor_op.blk_stride,
                                   self.dst_tensor_op.blk_stride, self.dst_tensor_op.blk_stride,
                                   self.dst_tensor_op.rep_stride, self.dst_tensor_op.rep_stride,
                                   self.dst_tensor_op.rep_stride)
        self.tik_instance.set_high_level_api_state()

    @source_info_decorator(depth=2)
    def run_all(self):
        """
        run all_check and code_gen

        Returns
        -------
        None
        """
        # block_size considered as 32
        if self.name in ("vaxpy", "vec_axpy"):
            dummy_dst = self.tik_instance.Tensor(self.dst_tensor_op.tensor_obj.dtype,
                                                 self.dst_tensor_op.tensor_obj.original_shape,
                                                 name="dummy_dst", scope=scope_ubuf)

            dummy_dst = dummy_dst[self.dst_tensor_op.tensor_obj.offset:]
            self.run_all_triple(dummy_dst)
        else:
            self.run_all_double()

    @source_info_decorator(depth=2)
    def run_all_incompatible(self):
        """
        run double op not compatible mode
        Returns
        -------

        """
        self.double_incompatible_mode()
