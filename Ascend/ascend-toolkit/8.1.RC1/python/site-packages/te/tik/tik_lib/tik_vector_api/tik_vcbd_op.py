#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vcbd_op.py
DESC:     provide params
CREATED:  2021-10-27 10:36:42
"""
from tbe import tvm
from tbe.common.platform import api_check_support
from tbe.tik.common.overflow_util import tensor_list_overflow_check
from tbe.tik.common.util import TikUtil
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import ceil_div
from tbe.tik.common.common_util import reduce_mul
from tbe.tik.common.tik_get_soc_name import get_compatible_blk_size
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_expr import is_basic_expr
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_params import VA0_INDEX
from tbe.tik.tik_lib.tik_params import MASK_LEN_128
from tbe.tik.tik_lib.tik_params import VNCHWCONV_LIST_LEN
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_7
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_vector_api.tik_tensor_op import TensorOp

_MIN_DST_BLK_STRIDE = 1
_BLK_LEN_16 = 16


class VcbdOp(TensorOp):
    """
    Tensor Ops
    """

    def __init__(self, tensor_obj, blk_stride, rep_stride, tensor_op_name):
        super().__init__(tensor_obj, blk_stride, rep_stride, tensor_op_name)
        self.tensor_op_name = tensor_op_name
        self.tensor_obj = tensor_obj
        self.context = None
        self.blk_stride = blk_stride
        self.rep_stride = rep_stride
        self.blk_stride_value = self.blk_stride
        self.src_mask_value = None
        self.rep_stride_value = self.rep_stride
        self.offset_value = None
        self.block_len = None
        self.nblock = None
        self.repeat_times_value = None
        self.mask_value = None
        self.stride_unit_value = 0

    def check_vcbd_overflow(self, control_op, tensor_bit_len_max):
        """
        check tensor_op overflow

        Parameters
        ----------
        control_op: control_op
        tensor_bit_len_max: tensor dtype bit length

        Returns
        -------
        None
        """
        self.set_stride_unit(control_op.stride_unit)
        if self.tensor_obj.offset is None:
            return
        self.set_repeat_times(control_op.repeat_times)
        repeat_times = self.repeat_times_value

        if not all(isinstance(value, int)
                   for value in [repeat_times, self.blk_stride_value, self.rep_stride_value, self.stride_unit_value]):
            return
        if is_basic_expr(TikUtil.to_list(control_op.mask)):
            return

        if repeat_times == 0:
            return

        # when stride_unit is 0/1, dst_blk_stride as 0 is considered as
        # dst_blk_stride as 1
        if self.blk_stride_value == 0 and self.stride_unit_value <= 1:
            self.set_blk_stride_value(1)

        ele_num_per_rep = self.one_rep_size // tensor_bit_len_max
        self.block_len = self.one_blk_size // DTYPE_SIZE.get(self.tensor_obj.dtype)
        self.nblock = ele_num_per_rep * DTYPE_SIZE.get(self.tensor_obj.dtype) // self.one_blk_size
        self.get_mask(control_op.mask)
        if control_op.mask_mode == "counter":
            mask = self.mask_value[0]
            repeat_times = ceil_div(mask, ele_num_per_rep)
            self.mask_value = mask % ele_num_per_rep

        self.check_expected_ele(control_op, repeat_times)

    def check_expected_ele(self, control_op, repeat_times):
        """
        check expected elements num
        Parameters
        ----------
        control_op: control op
        repeat_times: repeat times

        Returns
        -------

        """

        expected_ele = self.tensor_obj.offset + self.vector_max_offset_cal(self.mask_value, control_op, repeat_times)
        actual_ele = reduce_mul(self.tensor_obj.original_shape)

        expected_ele = Expr(expected_ele).eval_value()
        actual_ele = Expr(actual_ele).eval_value()
        if expected_ele is not None and actual_ele is not None:
            TikCheckUtil.check_ge(
                actual_ele, expected_ele,
                "%s tensor overflow, need %d elements but only %d elements" %
                (self.tensor_op_name, expected_ele, actual_ele))


class VbcbOp(TensorOp):
    """
    VbcbOp Ops
    """

    def __init__(self, tensor_obj, blk_stride, rep_stride, tensor_op_name):
        super().__init__(tensor_obj, blk_stride, rep_stride, tensor_op_name)
        self.tensor_op_name = tensor_op_name
        self.tensor_obj = tensor_obj
        self.context = None
        self.blk_stride = blk_stride
        self.rep_stride = rep_stride
        self.blk_stride_value = self.blk_stride
        self.rep_stride_value = self.rep_stride
        self.block_len = None
        self.nblock = None
        self.repeat_times_value = None

    def check_vbcb_overlapping(self, control_op, src_tensor_op, dst_offset, src_offset):
        """
        check vbcb overlapping

        Parameters
        ----------
        src_offset: src offset
        dst_offset: dst offset
        control_op: control_op
        src_tensor_op: src_tensor_op

        Returns
        -------
        None
        """
        if src_tensor_op.tensor_obj.buffer != self.tensor_obj.buffer:
            return

        if src_offset <= dst_offset:
            if src_offset + control_op.repeat_times * BLK_NUM_PER_REP >= dst_offset:
                TikCheckUtil.raise_error("vbcb src and dst address overlapping error.")

        else:
            one_blk_len = self.one_blk_size // DTYPE_SIZE.get(self.tensor_obj.dtype)
            one_repeat_len = (self.blk_stride_value * SHIFT_BIT_POS_7 + 1) * one_blk_len
            for time in range(control_op.repeat_times - 1):
                interval_src_min = src_tensor_op.tensor_obj.offset + BLK_NUM_PER_REP * time
                interval_src_max = src_tensor_op.tensor_obj.offset + BLK_NUM_PER_REP * (time + 1) - 1
                interval_dst_min = self.tensor_obj.offset + self.rep_stride_value * one_blk_len * time
                interval_dst_max = self.tensor_obj.offset + self.rep_stride_value * one_blk_len * time + one_repeat_len
                if max(interval_src_min, interval_dst_min) < min(interval_src_max, interval_dst_max):
                    TikCheckUtil.raise_error("vbcb src and dst address overlapping error.")

    def check_new_tensor_overflow_with_fixed_length(self, control_op, src_tensor_op):
        """
        check vbcb tensor overflow with fixed_length

        Parameters
        ----------
        control_op: control_op
        src_tensor_op: src_tensor_op

        Returns
        -------
        None
        """
        offset = src_tensor_op.tensor_obj.offset
        original_shape = src_tensor_op.tensor_obj.original_shape
        length = control_op.repeat_times * BLK_NUM_PER_REP
        if isinstance(offset, (tvm.tir.IntImm, tvm.tir.FloatImm, tvm.tir.StringImm)):
            offset = offset.value
        if isinstance(original_shape, (list, tuple)):
            total_size = reduce_mul(original_shape)
        else:
            total_size = original_shape
        need_offset = offset + length
        if Expr(need_offset).eval_value() is None or Expr(total_size).eval_value() is None:
            return
        TikCheckUtil.check_le(need_offset, total_size, "%s need %s but only %s"
                              % (self.tensor_op_name, need_offset, total_size))


class VnchwconvOp(TensorOp):
    """
    VnchwconvOp Ops
    """
    def __init__(self, tensor_obj, blk_stride, rep_stride, tensor_op_name):
        super().__init__(tensor_obj, blk_stride, rep_stride, tensor_op_name)
        self.tensor_op_name = tensor_op_name
        self.tensor_obj = tensor_obj
        self.rep_stride = rep_stride
        self.blk_stride = blk_stride
        self.rep_stride_value = self.rep_stride
        self.blk_stride_value = self.blk_stride
        self.context = None
        self.repeat_times_value = None

    @staticmethod
    def get_dtype_str(src_list, dst_list, name):
        """
        get_dtype_str of vnchwconv

        Parameters
        ----------
        src_list: src_list
        dst_list: dst_list
        name: the name of Intrinsic

        Returns
        -------
        dtype_str
        """
        dtype_str = ""
        for dst, src in zip(dst_list, src_list):
            dtype_str = DTYPE_MAP.get(dst.dtype) + DTYPE_MAP.get(src.dtype)
            TikCheckUtil.check_equality(dst.dtype, src.dtype,
                                        "Intrinsic {}'s src's dtype "
                                        "should be equal to dst's dtype".format(name))
            TikCheckUtil.check_equality(api_check_support("tik.%s" % name, dst.dtype), True,
                                        gen_api_check_statement(dst.dtype, name))
        return dtype_str

    @staticmethod
    def get_block_begin_end(tensor, valid_num_per_block, rep_stride, time, default_list):
        """
        get_block_begin_end

        Parameters
        ----------
        tensor: tensor
        valid_num_per_block: valid_num_per_block
        rep_stride: rep_stride
        time: the default is 1
        default_list: store_high_half, context

        Returns
        -------
        begin, end
        """
        store_high_half, context = default_list
        blk_len = get_compatible_blk_size() // DTYPE_SIZE.get(tensor.dtype)
        if blk_len != valid_num_per_block and store_high_half:
            if context is not None:
                begin = context.evaluate_expr(
                    tensor.offset + time * rep_stride * blk_len) * DTYPE_SIZE.get(tensor.dtype)
                end = context.evaluate_expr(begin + blk_len * DTYPE_SIZE.get(tensor.dtype))
            else:
                begin = Expr((tensor.offset + time * rep_stride *
                              blk_len) * DTYPE_SIZE[tensor.dtype]).eval_value()
                end = Expr(begin + blk_len * DTYPE_SIZE.get(tensor.dtype)).eval_value()
        else:
            if context is not None:
                begin = context.evaluate_expr(
                    tensor.offset + time * rep_stride * blk_len) * DTYPE_SIZE.get(tensor.dtype)
                end = context.evaluate_expr(
                    begin + valid_num_per_block * DTYPE_SIZE.get(tensor.dtype))
            else:
                begin = Expr((tensor.offset + time * rep_stride *
                              blk_len) * DTYPE_SIZE.get(tensor.dtype)).eval_value()
                end = Expr(begin + valid_num_per_block * DTYPE_SIZE.get(tensor.dtype)).eval_value()
        return begin, end

    def get_dst_buffer_dict(self, dst_dict, valid_num_per_block, default_list, time=1):
        """
        get dst_list buffer dict info

        Parameters
        ----------
        dst_dict: dst_dict
        valid_num_per_block: valid_num_per_block
        default_list: store_high_half, context
        time: the default is 1

        Returns
        -------
        dst_dict
        """
        store_high_half, context = default_list
        # elements of 1 blk
        for dst in self.tensor_obj:
            begin, end = self.get_block_begin_end(
                dst, valid_num_per_block, self.rep_stride_value, time, [store_high_half, context])
            if isinstance(end, int):
                if dst.buffer not in dst_dict.keys():
                    dst_dict[dst.buffer] = []
                    dst_dict[dst.buffer].append([begin, end])
                else:
                    dst_dict[dst.buffer].append([begin, end])
        return dst_dict

    def get_src_buffer_dict(self, src_dict, valid_num_per_block, default_list, time=1):
        """
        get src_list buffer dict info

        Parameters
        ----------
        src_dict: src_dict
        valid_num_per_block: valid_num_per_block
        default_list: high_half, context
        time: the default is 1

        Returns
        -------
        src_dict
        """
        high_half, context = default_list
        for src in self.tensor_obj:
            begin, end = self.get_block_begin_end(
                src, valid_num_per_block, self.rep_stride_value, time, [high_half, context])
            if isinstance(end, int):
                if src.buffer not in src_dict.keys():
                    src_dict[src.buffer] = []
                    src_dict[src.buffer].append([begin, end])
                else:
                    src_dict[src.buffer].append([begin, end])
        return src_dict

    def check_vnchwconv_tensor_overflow(self, name, control_op, dtype_str, store_high_half):
        """
        check vnchwconv tensor overflow

        Parameters
        ----------
        name: the name of Intrinsic
        control_op: control_op
        dtype_str: dtype_str
        store_high_half: store_high_half

        Returns
        -------
        None
        """
        rep_stride = self.rep_stride
        repeat_times = control_op.repeat_times
        if repeat_times is None:
            return
        if isinstance(repeat_times, int) and repeat_times == 1:
            rep_stride = 0
        if dtype_str == 'b8':
            tensor_list_overflow_check(
                self.tensor_obj, name, (MASK_LEN_128, repeat_times, rep_stride),
                _BLK_LEN_16, store_high_half)
        else:
            tensor_list_overflow_check(
                self.tensor_obj, name, (self.one_blk_size*VNCHWCONV_LIST_LEN //
                                        DTYPE_SIZE.get(self.tensor_obj[VA0_INDEX].dtype), repeat_times,
                                        rep_stride),
                self.one_blk_size // DTYPE_SIZE.get(self.tensor_obj[VA0_INDEX].dtype),
                store_high_half)
