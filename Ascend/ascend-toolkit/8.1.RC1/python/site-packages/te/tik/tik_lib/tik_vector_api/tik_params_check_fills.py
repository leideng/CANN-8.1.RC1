#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_params_check_fills.py
DESC:     provide params
CREATED:  2021-11-7 16:05:42
MODIFIED: 2021-11-7 19:17:00
"""
from tbe.common.platform import intrinsic_check_support
from tbe.common.platform.platform_info import api_check_support
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.common.common_util import check_address_align
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import check_scatter_dict_for_overlap
from tbe.tik.common.util import get_mask_len
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import get_bit_len
from tbe.tik.common.tik_get_soc_name import is_compatible_mode
from tbe.tik.common.tik_get_soc_name import get_compatible_blk_size
from tbe.tik.common.tik_get_soc_name import get_compatible_rep_size
from tbe.tik.debug.simd import eval_mask
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import VNCHWCONV_LIST_LEN
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_STRIDE_UNIT
from tbe.tik.tik_lib.tik_params import MIN_STRIDE_UNIT
from tbe.tik.tik_lib.tik_params import VA0_INDEX
from tbe.tik.tik_lib.tik_params import BIT_LEN_16
from tbe.tik.tik_lib.tik_params import ONE_BYTE_BIT_LEN
from tbe.tik.tik_lib.tik_params import BIT_LEN_8
from tbe.tik.tik_lib.tik_params import MAX_VNCHWTRANS_STRIDE
from tbe.tik.tik_lib.tik_params import MAX_VNCHWTRANS_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import PER_TRANSPOSE_DATA_SIZE
from tbe.tik.tik_lib.tik_params import MASK_VALUE_64
from tbe.tik.tik_lib.tik_params import MASK_VALUE_128
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import BLK_NUM_PER_REP
from tbe.tik.tik_lib.tik_params import ONE_REP_BYTE_SIZE
from tbe.tik.tik_lib.tik_params import MAX_BLK_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_SINGLE_BYTE
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_12_BITS
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_api_constants import VNCHWCONV_INSTR_APPENDIX_MAP
from tbe.tik.tik_lib.tik_api_constants import SCOPE_MAP
from tbe.tik.tik_lib.tik_api_util import check_repeat_times
from tbe.tik.tik_lib.tik_api_util import check_stride_unit
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.tik_lib.tik_vector_api.tik_params_check import SingleCheckParams
from tbe.tik.tik_lib.tik_vector_api.tik_vector_name_map import FILLS_NAME_DICT
from tbe.tik.tik_lib.tik_vector_api.vector_common_util import gen_block_list

_MIN_DST_BLK_STRIDE = 1
_MIN_PAD_MODE = 0
_MAX_PAD_MODE = 2


def check_list_vnchwconv(src_list, dst_list):
    """
    check tensor list number
    Parameters
    ----------
    src_list : list, the src operation list
    dst_list : list, the des operation list

    Returns
    -------
    Nones
    """
    TikCheckUtil.check_type_match(dst_list, (tuple, list),
                                  "dst_list should be tuple or list")
    TikCheckUtil.check_type_match(src_list, (tuple, list),
                                  "src_list should be tuple or list")
    TikCheckUtil.check_equality(len(dst_list), VNCHWCONV_LIST_LEN,
                                "there should be 16 addresses in dst_list")
    TikCheckUtil.check_equality(len(src_list), VNCHWCONV_LIST_LEN,
                                "there should be 16 addresses in src_list")
    for src_idx, src in enumerate(src_list):
        TikCheckUtil.check_type_match(
            src, Tensor, "src[%s] should be tensor" % src_idx)
        check_address_align([src], ["src[" + str(src_idx) + "]"])
    for dst_idx, dst in enumerate(dst_list):
        TikCheckUtil.check_type_match(
            dst, Tensor, "dst[%s] should be tensor" % dst_idx)
        check_address_align([dst], ["dst[" + str(dst_idx) + "]"])


class DupCheckParams(SingleCheckParams):
    """
    Dup Params Check
    """

    def __init__(self, print_name, params_list, name):
        super(DupCheckParams, self).__init__(print_name, params_list, name)
        self.print_name = print_name
        self.name = name
        self.params_list = params_list
        self.dst_tensor_op, self.scalar_op, self.control_op = params_list

    def check_dup_op_dtype_support(self):
        """
        check vector_dup_dtype support

        Returns
        -------
        None
        """
        if isinstance(self.scalar_op.scalar_obj, Scalar):
            dst_type = self.dst_tensor_op.tensor_obj.dtype
            # if dst_type is set to bfloat16, scalar's dtype can be set to bfloat16 or float32
            if dst_type == "bfloat16" and self.scalar_op.scalar_obj.dtype == "float32":
                dst_type = "float32"
            TikCheckUtil.check_equality(dst_type, self.scalar_op.scalar_obj.dtype,
                                        "Intrinsic {}'s scalar's dtype should be equal to dst's dtype".format
                                        (self.print_name))
        dtype = self.dst_tensor_op.tensor_obj.dtype
        # support int64, but api_check_support should be false
        if dtype == "int64":
            pass
        else:
            TikCheckUtil.check_equality(api_check_support("tik." + self.name, self.dst_tensor_op.tensor_obj.dtype),
                                        True,
                                        gen_api_check_statement(self.dst_tensor_op.tensor_obj.dtype, self.print_name))
        if "int" in self.dst_tensor_op.tensor_obj.dtype:
            TikCheckUtil.check_not_equality(type(self.scalar_op.scalar_obj), float,
                                            "{} should not be float when {}.dtype is {}".format
                                            (self.scalar_op.scalar_op_name, self.dst_tensor_op.tensor_op_name,
                                             self.dst_tensor_op.tensor_obj.dtype))

    def check_stride_unit(self):
        """
        check vector_dup stride_unit support

        Returns
        -------
        None
        """
        check_stride_unit(self.control_op.stride_unit)

    def check_all(self, tik_instance):
        """
        all check

        Returns
        -------
        None
        """
        # check tensor's tensor, scope, address, rep stride and blk stride
        self.dst_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.set_rep_stride_value()
        if TikSocManager.is_nano_soc():
            # on the nano soc, dst stride occupies 12-bit space, so tbe max_value is 4095
            self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_12_BITS)
        else:
            self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)
        self.scalar_op.check_scalar_type()
        self.check_repeat_times()
        self.control_op.check_mask_mode()
        self.check_stride_unit()
        self.check_dup_op_dtype_support()
        tensor_bit_len = get_bit_len(self.dst_tensor_op.tensor_obj.dtype)
        mask_o = self.control_op.check_and_gen_mask_o(tik_instance, tensor_bit_len)

        # check tensor overflow
        block_list_dst = gen_block_list(tensor_bit_len, self.dst_tensor_op.tensor_obj.dtype)
        self.dst_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_dst)

        return mask_o


class VciCheckParams(DupCheckParams):
    """
    Vci Params Check
    """

    def check_stride_unit(self):
        """
        check vci stride_unit support

        Returns
        -------
        None
        """
        stride_unit = self.control_op.stride_unit
        TikCheckUtil.check_type_match(
            stride_unit, int, "stride_unit should be int, input stride_unit: {}".format(type(stride_unit)))
        if TikSocManager.is_v100_soc() or TikSocManager.is_910b_soc() or TikSocManager.is_v210_vec_soc():
            TikCheckUtil.check_equality(
                stride_unit, MIN_STRIDE_UNIT,
                "{} only support stride_unit=0, input value is {}".format(get_soc_name(), stride_unit))
        TikCheckUtil.check_in_range_by_dtype(
            stride_unit, msg="stride_unit should be in the range of [%d, %d], "
            "input stride_unit: %s" % (MIN_STRIDE_UNIT, MAX_STRIDE_UNIT, stride_unit),
            var_range=[MIN_STRIDE_UNIT, MAX_STRIDE_UNIT])


class VpaddingCheckParams:
    """
    vpadding check params
    """

    def __init__(self, print_name, params_list, pad_mode, pad_side):
        self.print_name = print_name
        self.name = print_name
        self.params_list = params_list
        self.dst_tensor_op, self.src_tensor_op, self.control_op = params_list
        self.pad_mode = pad_mode
        self.pad_side = pad_side

    def check_pad_mode_and_side(self):
        """
        check pad_mode and pad_side

        Returns
        -------
        None
        """
        TikCheckUtil.check_type_match(self.pad_mode, int, "pad_mode should be int, "
                                                          "input pad_mode: {}".format(type(self.pad_mode)))
        TikCheckUtil.check_in_range_by_dtype(
            self.pad_mode, msg="pad_mode should be in the range of [{}, {}], input pad_mode: {}".format(
                _MIN_PAD_MODE, _MAX_PAD_MODE, self.pad_mode), var_range=[_MIN_PAD_MODE, _MAX_PAD_MODE])
        # check pad_side
        TikCheckUtil.check_var_in_list(self.pad_side, ["left", "right"], "pad_side should be 'left' or 'right', "
                                                                         "input pad_side: {}".format(self.pad_side))

    def check_dtype_support(self):
        """
        check single_dtype support

        Returns
        -------
        None
        """
        TikCheckUtil.check_equality(self.dst_tensor_op.tensor_obj.dtype, self.src_tensor_op.tensor_obj.dtype,
                                    "Instruction %s's src's "
                                    "dtype should be equal to dst's dtype" % self.print_name)
        TikCheckUtil.check_equality(
            intrinsic_check_support("Intrinsic_" + self.name, self.dst_tensor_op.tensor_obj.dtype), True,
            gen_api_check_statement(self.dst_tensor_op.tensor_obj.dtype, self.print_name))

    def check_debug_common_params(self, tensor_bit_len, context=None):
        """
        check debug and ca common params

        Parameters
        ----------
        tensor_bit_len: tensor_bit_len
        context: debug context

        Returns
        -------
        src_mask
        """
        # check repeat_times
        if context:
            repeat = context.evaluate_expr(self.control_op.repeat_times)
            TikCheckUtil.check_in_range_by_dtype(
                repeat, msg="repeat_times should be in the range of [%d, %d], input repeat_times: %s"
                            % (0, MAX_REPEAT_TIMES, repeat),
                var_range=[0, MAX_REPEAT_TIMES])
        else:
            check_repeat_times(self.control_op.repeat_times)

        # check strides
        self.dst_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.set_blk_stride_value()
        self.src_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.check_tensor_op_stride(MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)
        self.src_tensor_op.check_tensor_op_stride(MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)

        # gen src_mask
        if self.control_op.mask_mode == "normal":
            # all elements in src are read even their mask bits are invalid
            if get_bit_len(self.src_tensor_op.tensor_obj.dtype) == 32:
                src_mask = MASK_VALUE_64
            else:
                src_mask = MASK_VALUE_128
        else:
            if context:
                src_mask = eval_mask(self.control_op.mask, context)
            else:
                src_mask = self.control_op.mask
        # gen block_list
        block_list = [BLK_NUM_PER_REP, ONE_REP_BYTE_SIZE // tensor_bit_len]

        # check overlap
        if self.src_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
            value_range = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride,
                           self.src_tensor_op.blk_stride, self.dst_tensor_op.rep_stride, self.src_tensor_op.rep_stride)
            if all(isinstance(value, int) for value in value_range) or context:
                self.dst_tensor_op.set_src_mask(src_mask)
                self.dst_tensor_op.check_address_overlapping(self.print_name, self.control_op,
                                                             self.src_tensor_op, block_list)
        return src_mask

    def check_all(self, tik_instance):
        """
        check all

        Parameters
        ----------
        tik_instance: tik_instance

        Returns
        -------
        None
        """
        self.dst_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)
        self.src_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_DOUBLE_BYTE, MAX_REP_STRIDE_SINGLE_BYTE)
        self.check_pad_mode_and_side()
        self.control_op.check_mask_mode()
        self.check_dtype_support()
        check_stride_unit(self.control_op.stride_unit)

        tensor_bit_len = max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                             get_bit_len(self.src_tensor_op.tensor_obj.dtype), )
        mask_o = self.control_op.check_and_gen_mask_o(tik_instance, tensor_bit_len)

        # gen src_mask
        src_mask = self.check_debug_common_params(tensor_bit_len)

        block_list_dst = gen_block_list(tensor_bit_len, self.dst_tensor_op.tensor_obj.dtype)
        block_list_src = gen_block_list(tensor_bit_len, self.src_tensor_op.tensor_obj.dtype)
        mask = self.control_op.mask
        self.control_op.mask = src_mask
        self.src_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_src)
        self.control_op.mask = mask
        self.dst_tensor_op.check_tensor_op_overflow(self.print_name, self.control_op, block_list_dst)

        return mask_o


class VbcbCheckParams:
    """
    vbcb check params
    """

    def __init__(self, print_name, params_list):
        self.name = print_name
        self.print_name = FILLS_NAME_DICT.get(self.name)
        self.params_list = params_list
        self.dst_tensor_op, self.src_tensor_op, self.control_op = params_list

    def check_vbcb_dtype_support(self):
        """
        check vbcb_dtype support

        Returns
        -------
        None
        """
        if TikSocManager.is_nano_soc():
            TikCheckUtil.check_equality(self.dst_tensor_op.tensor_obj.dtype, self.src_tensor_op.tensor_obj.dtype,
                                        "Intrinsic %s's src's dtype should be equal to dst's dtype" % self.name)
            dtype_str = self.src_tensor_op.tensor_obj.dtype
        else:
            dtype_str = DTYPE_MAP.get(self.src_tensor_op.tensor_obj.dtype) + \
                        DTYPE_MAP.get(self.dst_tensor_op.tensor_obj.dtype)

        msg = "src: " + self.src_tensor_op.tensor_obj.dtype + ", dst: " + self.dst_tensor_op.tensor_obj.dtype
        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + self.print_name, dtype_str), True,
                                    gen_api_check_statement(msg, self.name))

    def check_vbcb_repeat_times(self):
        """
        check repeat_times

        Returns
        -------
        None
        """
        check_repeat_times(self.control_op.repeat_times)

    def check_all(self):
        """
        all check

        Returns
        -------
        None
        """
        tensor_bit_len = max(get_bit_len(self.dst_tensor_op.tensor_obj.dtype),
                             get_bit_len(self.src_tensor_op.tensor_obj.dtype))
        # check tensor's tensor, scope, address, rep stride and blk stride
        self.dst_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.set_blk_stride_value()
        self.dst_tensor_op.check_tensor_op_valid(self.name, MAX_BLK_STRIDE_SINGLE_BYTE, MAX_REP_STRIDE_12_BITS)
        # check src tensor and scope
        self.src_tensor_op.check_tensor_and_scope()
        # check src address_align
        self.src_tensor_op.check_tensor_op_address_align(self.name, align=None)
        # check repeat
        self.check_vbcb_repeat_times()
        # check dtype
        self.check_vbcb_dtype_support()
        mask = get_compatible_rep_size() // DTYPE_SIZE.get(self.dst_tensor_op.tensor_obj.dtype)
        self.control_op.mask = mask
        dst_block_list = gen_block_list(tensor_bit_len, self.dst_tensor_op.tensor_obj.dtype)

        # check the overlap
        overlap_value_range = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride_value,
                               self.dst_tensor_op.rep_stride_value, self.dst_tensor_op.tensor_obj.offset,
                               self.src_tensor_op.tensor_obj.offset)
        if all(isinstance(value, int) for value in overlap_value_range):
            self.dst_tensor_op.check_vbcb_overlapping(self.control_op, self.src_tensor_op,
                                                      self.dst_tensor_op.tensor_obj.offset,
                                                      self.src_tensor_op.tensor_obj.offset)

        # check tensor overflow(static)
        overflow_value_range = (self.control_op.repeat_times, self.dst_tensor_op.blk_stride_value,
                                self.dst_tensor_op.rep_stride_value)
        if all(isinstance(value, int) for value in overflow_value_range):
            self.src_tensor_op.check_new_tensor_overflow_with_fixed_length(self.control_op, self.src_tensor_op)
            self.dst_tensor_op.check_tensor_op_overflow(self.name, self.control_op, dst_block_list)


class VecTransCheckParams:
    """
    VecTrans check params
    """

    def __init__(self, name, params_list):
        self.params_list = params_list
        self.name = name
        self.dst_tensor_op, self.src_tensor_op, self.control_op = params_list

    @staticmethod
    def check_new_vec_trans_overflow(dst_tensor_op, src_tensor_op, control_op):
        """
        used to check the overflow of vec_trans

        Parameters
        ----------
        dst_tensor_op: dst_tensor_op
        src_tensor_op: src_tensor_op
        control_op: control_op
        Returns
        -------
        None
        """

        if is_immediate_number([control_op.repeat_times, dst_tensor_op.rep_stride_value,
                                src_tensor_op.rep_stride_value]):
            dst_offset = Expr(dst_tensor_op.tensor_obj.offset).eval_value()
            src_offset = Expr(src_tensor_op.tensor_obj.offset).eval_value()

            if dst_offset is not None:
                dst_elt_count = reduce_mul(dst_tensor_op.tensor_obj.original_shape)
                required_dst_elt_count = \
                    PER_TRANSPOSE_DATA_SIZE + dst_tensor_op.rep_stride_value * \
                    PER_TRANSPOSE_DATA_SIZE * (control_op.repeat_times - 1) + dst_offset

                TikCheckUtil.check_ge(dst_elt_count, required_dst_elt_count,
                                      "elements of dst should be more than %d" % required_dst_elt_count)
            if src_offset is not None:
                src_elt_count = reduce_mul(src_tensor_op.tensor_obj.original_shape)
                required_src_elt_count = \
                    PER_TRANSPOSE_DATA_SIZE + src_tensor_op.rep_stride_value * \
                    PER_TRANSPOSE_DATA_SIZE * (control_op.repeat_times - 1) + src_offset

                TikCheckUtil.check_ge(src_elt_count, required_src_elt_count,
                                      "elements of src should be more then %d" % required_src_elt_count)

    def check_vec_trasns_dtype_support(self):
        """
        used to check the dtype of vec_trans
        """

        # check data type, all supported data type
        if TikSocManager.is_nano_soc() and not is_compatible_mode():
            dst_src_map = ["u16u16", "s16s16", "f16f16", 'u8u8', 's8s8']
        else:
            dst_src_map = ["u16u16", "s16s16", "f16f16"]
        dtype_str = DTYPE_MAP.get(
            self.dst_tensor_op.tensor_obj.dtype) + DTYPE_MAP.get(self.src_tensor_op.tensor_obj.dtype)
        if dtype_str not in dst_src_map:
            TikCheckUtil.raise_error("instruction %s not support dst %s src %s" % (
                self.name, self.dst_tensor_op.tensor_obj.dtype, self.src_tensor_op.tensor_obj.dtype))

    def check_vec_trans_scope(self):
        """
        used to check the scope of vec_trans
        """

        # check tensor scope
        dst_scope = SCOPE_MAP.get(self.dst_tensor_op.tensor_obj.scope)
        TikCheckUtil.check_var_in_list(dst_scope, ['ubuf'],
                                       "dst tensor scope must be ubuf, "
                                       "but input scope: %s" % dst_scope)
        src_scope = SCOPE_MAP.get(self.src_tensor_op.tensor_obj.scope)
        TikCheckUtil.check_var_in_list(src_scope, ['ubuf'],
                                       "src tensor scope must be ubuf, "
                                       "but input scope: %s" % dst_scope)

    def _check_vtranspose_overlapping(self):
        required_elements_count = 256
        src_offset = Expr(self.src_tensor_op.tensor_obj.offset).eval_value()
        dst_offset = Expr(self.dst_tensor_op.tensor_obj.offset).eval_value()
        if all(isinstance(value, int) for value in (src_offset, dst_offset)):
            if self.src_tensor_op.tensor_obj.buffer == self.dst_tensor_op.tensor_obj.buffer:
                if not (src_offset == dst_offset or src_offset + required_elements_count <= dst_offset or
                        dst_offset + required_elements_count <= src_offset):
                    TikCheckUtil.raise_error("vtranspose not support partially address overlapping")

    def check_all(self):
        """
        all check

        Returns
        -------
        None
        """
        self.dst_tensor_op.check_tensor_and_scope()
        self.src_tensor_op.check_tensor_and_scope()
        self.check_vec_trans_scope()
        check_repeat_times(self.control_op.repeat_times, MAX_VNCHWTRANS_REPEAT_TIMES)
        self.dst_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.check_tensor_op_rep_stride(MAX_VNCHWTRANS_STRIDE)
        self.src_tensor_op.check_tensor_op_rep_stride(MAX_VNCHWTRANS_STRIDE)
        self.check_vec_trasns_dtype_support()
        self.dst_tensor_op.check_tensor_op_address_align(self.name, align=None)
        self.src_tensor_op.check_tensor_op_address_align(self.name, align=None)
        self.check_new_vec_trans_overflow(self.dst_tensor_op, self.src_tensor_op, self.control_op)
        self._check_vtranspose_overlapping()


class VnchwconvCheckParams:
    """
    Vnchwconv check params
    """

    def __init__(self, print_name, params_list):
        self.params_list = params_list
        self.print_name = print_name
        self.name = self.print_name
        self.dst_tensor_op, self.src_tensor_op, self.control_op, self.dst_high_half, self.src_high_half = params_list

    # check dst_high_half, src_high_half
    @staticmethod
    def check_high_half(dst_high_half, src_high_half):
        """
        check high_half

        Returns
        -------
        None
        """
        TikCheckUtil.check_type_match(
            dst_high_half, bool, "dst_high_half should be bool, input type: {}".format(type(dst_high_half)))
        TikCheckUtil.check_type_match(
            src_high_half, bool, "src_high_half should be bool, input type: {}".format(type(src_high_half)))

    @staticmethod
    def check_scatter_mask_len(bit_len, mask, mask_mode="normal", context=None):
        """
        check mask len for scatter address overlap
        """
        one_rep_byte_size = get_compatible_rep_size()
        if not isinstance(mask, (list, tuple)):
            mask = [mask]
        for value in mask:
            if Expr(value).eval_value() is None:
                return False
        if len(mask) == 1 or mask_mode == "counter":
            mask_len = mask[0]
        else:
            mask_len = get_mask_len(mask)
        if context is None:
            mask_len = Expr(mask_len).eval_value()
        else:
            mask_len = context.evaluate_expr(mask_len)
        if isinstance(mask_len, int):
            if mask_len % (ONE_BYTE_BIT_LEN * one_rep_byte_size // bit_len) == 0:
                return True
            if bit_len == BIT_LEN_8 and mask_len % MASK_VALUE_128 == 0:
                return True
        return False

    @staticmethod
    def check_tensor_offset(dst_list, src_list, context):
        """
        check Expr(offset) not None
        """
        for dst, src in zip(dst_list, src_list):
            if context is None:
                dst_offset = Expr(dst.offset).eval_value()
                src_offset = Expr(src.offset).eval_value()
            else:
                dst_offset = context.evaluate_expr(dst.offset)
                src_offset = context.evaluate_expr(src.offset)
            if dst_offset is None or src_offset is None:
                return False
        return True

    @staticmethod
    def get_same_buffer_num(dst_list, src_list, context=None):
        """
        get number of same buffer

        Parameters
        ----------
        dst_list: dst_list
        src_list: src_list
        context: context

        Returns
        -------
        same_buffer_num
        """
        same_buffer_num = 0
        for dst, src in zip(dst_list, src_list):
            if context is None:
                dst_offset = Expr(dst.offset).eval_value()
                src_offset = Expr(src.offset).eval_value()
            else:
                dst_offset = context.evaluate_expr(dst.offset)
                src_offset = context.evaluate_expr(src.offset)
            if dst_offset is not None and src_offset is not None:
                if src.buffer == dst.buffer and \
                        Expr(src.offset).eval_value() == \
                        Expr(dst.offset).eval_value():
                    same_buffer_num += 1
        return same_buffer_num

    def check_vnchwconv_repeat_times(self):
        """
        check repeat_times

        Returns
        -------
        None
        """
        check_repeat_times(self.control_op.repeat_times)

    def check_vnchwconv_dtype_support(self):
        """
        check dtype_support

        Returns
        -------
        None
        """
        for dst, src in zip(self.dst_tensor_op.tensor_obj, self.src_tensor_op.tensor_obj):
            TikCheckUtil.check_equality(dst.dtype, src.dtype,
                                        "Intrinsic {}'s src's dtype "
                                        "should be equal to dst's dtype".format(self.name))
            TikCheckUtil.check_equality(api_check_support("tik." + self.name, dst.dtype), True,
                                        gen_api_check_statement(dst.dtype, self.name))

    def check_vnchwconv_overlap(self, msg, store_high_half, src_store_high_half, context=None):
        """
        check vnchwconv_overlap
        Parameters
        ----------
        msg: the overflow msg source infor
        store_high_half: store_high_half
        src_store_high_half: src_store_high_half
        context: context

        Returns
        -------
        None
        """

        src_buf_list = (src.buffer for src in self.src_tensor_op.tensor_obj)
        dst_buf_list = (dst.buffer for dst in self.dst_tensor_op.tensor_obj)

        if all(isinstance(value, int)
               for value in (self.dst_tensor_op.rep_stride_value, self.src_tensor_op.rep_stride_value)) \
                and list(set(src_buf_list) & set(dst_buf_list)):
            self.check_new_scatter_address_overlap(msg, store_high_half, src_store_high_half, context)

    def check_new_scatter_address_overlap(self, msg, store_high_half, src_store_high_half, context=None):
        """
        check new scatter address overlap
        Parameters
        ----------
        msg: the overflow msg source infor
        store_high_half: store_high_half
        src_store_high_half: src_store_high_half
        context: context

        Returns
        -------
        None
        """
        one_blk_size = get_compatible_blk_size()
        one_rep_byte_size = get_compatible_rep_size()
        if context is None:
            repeat_times = Expr(self.control_op.repeat_times).eval_value()
        else:
            repeat_times = context.evaluate_expr(self.control_op.repeat_times)
        dtype_str = self.dst_tensor_op.get_dtype_str(self.src_tensor_op.tensor_obj, self.dst_tensor_op.tensor_obj,
                                                     self.name)
        if VNCHWCONV_INSTR_APPENDIX_MAP[dtype_str] == "b8":
            mask_len = MASK_VALUE_128
        else:
            mask_len = one_blk_size * VNCHWCONV_LIST_LEN // \
                       DTYPE_SIZE.get(self.dst_tensor_op.tensor_obj[VA0_INDEX].dtype)
        if not self.check_scatter_mask_len(max(get_bit_len(self.dst_tensor_op.tensor_obj[VA0_INDEX].dtype),
                                               get_bit_len(self.src_tensor_op.tensor_obj[VA0_INDEX].dtype)),
                                           mask_len, 'normal', context):
            return
        valid_num_per_block = min(one_rep_byte_size //
                                  get_bit_len(self.dst_tensor_op.tensor_obj[VA0_INDEX].dtype),
                                  one_rep_byte_size //
                                  get_bit_len(self.src_tensor_op.tensor_obj[VA0_INDEX].dtype))
        if self.dst_tensor_op.tensor_obj[VA0_INDEX].dtype == self.src_tensor_op.tensor_obj[VA0_INDEX].dtype and \
                self.dst_tensor_op.tensor_obj[VA0_INDEX].dtype in ("uint8", "int8"):
            valid_num_per_block = BIT_LEN_16

        if repeat_times is None or repeat_times <= 0:
            return
        # check offset
        if not self.check_tensor_offset(self.dst_tensor_op.tensor_obj, self.src_tensor_op.tensor_obj, context):
            return

        self.check_scatter_dict_overlap(msg, valid_num_per_block, [store_high_half, src_store_high_half], context)

    def check_scatter_dict_overlap(self, msg, valid_num_per_block, half_list, context=None):
        """
        check new scatter address overlap
        Parameters
        ----------
        msg: the overflow msg source infor
        valid_num_per_block: valid_num_per_block
        half_list: store_high_half, src_store_high_half
        context: context

        Returns
        -------
        None
        """
        if context is not None:
            repeat_times = self.dst_tensor_op.repeat_times_value
        else:
            repeat_times = self.control_op.repeat_times
        if repeat_times == 1:
            if self.dst_tensor_op.rep_stride_value == self.src_tensor_op.rep_stride_value:
                # check 100% same
                same_buffer_num = self.get_same_buffer_num(self.dst_tensor_op.tensor_obj,
                                                           self.src_tensor_op.tensor_obj, context)
                if same_buffer_num == len(self.src_tensor_op.tensor_obj):
                    # each VA block same, allow overlap, check end. return
                    return
            src_dict, dst_dict = self.get_src_dst_dict_for_overlap(valid_num_per_block, half_list, context)

            # check dict for overlap
            check_scatter_dict_for_overlap(src_dict, dst_dict, self.name, msg)
        else:
            for time in range(repeat_times - 1):
                src_dict, dst_dict = self.get_src_dst_dict_for_overlap(valid_num_per_block, half_list, context, time)
                # check dict for overlap
                check_scatter_dict_for_overlap(src_dict, dst_dict, self.name, msg)

    def get_src_dst_dict_for_overlap(self, valid_num_per_block, half_list, context=None, time=1):
        """
        check new scatter address overlap
        Parameters
        ----------
        valid_num_per_block : valid_num_per_block
        half_list : store_high_half, src_store_high_half
        context : context
        time : the time default is 1

        Returns
        -------
        src_dict, dst_dict
        """
        dst_dict = {}
        src_dict = {}

        store_high_half, src_store_high_half = half_list
        dst_dict = self.dst_tensor_op.get_dst_buffer_dict(dst_dict, valid_num_per_block,
                                                          [store_high_half, context], time)
        self.dst_tensor_op.context = context
        self.dst_tensor_op.set_repeat_times(self.control_op.repeat_times)
        if self.dst_tensor_op.repeat_times_value > 1:
            time = time + 1
        if src_store_high_half is None:
            src_dict = self.src_tensor_op.get_src_buffer_dict(src_dict, valid_num_per_block,
                                                              [store_high_half, context], time)
        else:
            src_dict = self.src_tensor_op.get_src_buffer_dict(src_dict, valid_num_per_block,
                                                              [src_store_high_half, context], time)
        return src_dict, dst_dict

    def check_all(self):
        """
        all check

        Returns
        -------
        None
        """
        self.check_high_half(self.dst_high_half, self.src_high_half)
        self.check_vnchwconv_repeat_times()
        self.check_vnchwconv_dtype_support()
        self.dst_tensor_op.set_rep_stride_value()
        self.src_tensor_op.set_rep_stride_value()
        self.dst_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_DOUBLE_BYTE)
        self.src_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_DOUBLE_BYTE)
        self.check_vnchwconv_overlap("dst_list and src_list", self.dst_high_half, self.src_high_half)
        dtype_str = self.dst_tensor_op.get_dtype_str(self.src_tensor_op.tensor_obj, self.dst_tensor_op.tensor_obj,
                                                     self.name)
        dtype_str = VNCHWCONV_INSTR_APPENDIX_MAP.get(dtype_str)
        self.dst_tensor_op.check_vnchwconv_tensor_overflow(self.name, self.control_op, dtype_str, self.dst_high_half)
        self.src_tensor_op.check_vnchwconv_tensor_overflow(self.name, self.control_op, dtype_str, self.src_high_half)
