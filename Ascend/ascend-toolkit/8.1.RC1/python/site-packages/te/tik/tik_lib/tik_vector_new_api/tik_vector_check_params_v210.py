#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_check_params_v210.py
DESC:     v210 all check
CREATED:  2021-11-22 14:15
MODIFIED: 2021-11-22 14:15
"""

from tbe.common.platform import intrinsic_check_support
from tbe.common.platform import scope_wreg
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_gm
from tbe.tik.tik_lib.tik_check_util import print_error_msg
from tbe.tik.common.util import TikCheckUtil
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.common_util import check_param_type_range
from tbe.tik.common.common_util import check_address_align
from tbe.tik.common.common_util import check_extent_overflow
from tbe.tik.common.common_check_func import check_mvf_data_move_overflow
from tbe.tik.common.tik_get_soc_name import get_soc_name
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.common.platform import scope_vreg
from tbe.tik.tik_lib.tik_params import WMUL_TYPE_MAP
from tbe.tik.tik_lib.tik_params import WMUL_TYPE_MAP_WITH_PART_MODE
from tbe.tik.tik_lib.tik_params import DTYPE_IMM_MIN
from tbe.tik.tik_lib.tik_params import DTYPE_IMM_MAX
from tbe.tik.tik_lib.tik_params import VSHR_TYPE_MAP
from tbe.tik.tik_lib.tik_params import VSHR_WIDE_TYPE_MAP
from tbe.tik.tik_lib.tik_params import VRND_TYPE_MAP
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_vector import Vector
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager


class VectorOpsCheck:
    """
    vector ops check
    """

    @staticmethod
    def check_reg_sig_ele_wise_params(name, dst, src):
        """
        check whether instruction params are legal

        Parameters
        ----------
        name : instruction name
        dst : destination tensor
        src : source tensor

        Returns
        -------
        None
        """
        # change cce instruction's name to tik API name
        api_name = name.replace("vectorized_", "vector_")

        # check vector
        TikCheckUtil.check_type_match(src, Vector, "Instruction %s's src should be Vector" % api_name)
        TikCheckUtil.check_type_match(dst, Vector, "Instruction %s's dst should be Vector" % api_name)

        # check src and dst dtype
        dtype_str = DTYPE_MAP[src.dtype] + DTYPE_MAP[dst.dtype]
        dtype_str_error = "src " + src.dtype + " dst " + dst.dtype
        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + name, dtype_str), True,
                                    gen_api_check_statement(dtype_str_error, api_name))

    @staticmethod
    def check_vector_one_src_four_dst_params(check_four_params):
        """
        check vector one src and four dst params
        """
        # current only for vector_vdhist, vector_vchist instructions
        api_name = check_four_params.name.replace("vectorized_", "vector_v")
        # check vector
        TikCheckUtil.check_type_match(check_four_params.src, Vector,
                                      "Instruction %s's src should be Vector" % api_name)
        TikCheckUtil.check_type_match(check_four_params.dst0, Vector,
                                      "Instruction %s's dst0 should be Vector" % api_name)
        TikCheckUtil.check_type_match(check_four_params.dst1, Vector,
                                      "Instruction %s's dst1 should be Vector" % api_name)
        TikCheckUtil.check_type_match(check_four_params.dst2, Vector,
                                      "Instruction %s's dst2 should be Vector" % api_name)
        TikCheckUtil.check_type_match(check_four_params.dst3, Vector,
                                      "Instruction %s's dst3 should be Vector" % api_name)

        # check data type
        TikCheckUtil.check_equality(check_four_params.dst1.dtype, check_four_params.dst0.dtype,
                                    "Instruction %s's dst1's dtype should be equal to dst0's dtype" % api_name)
        TikCheckUtil.check_equality(check_four_params.dst2.dtype, check_four_params.dst0.dtype,
                                    "Instruction %s's dst2's dtype should be equal to dst0's dtype" % api_name)
        TikCheckUtil.check_equality(check_four_params.dst3.dtype, check_four_params.dst0.dtype,
                                    "Instruction %s's dst3's dtype should be equal to dst0's dtype" % api_name)

        dtype_str = DTYPE_MAP[check_four_params.src.dtype] + DTYPE_MAP[check_four_params.dst0.dtype]
        dtype_str_error = "src " + check_four_params.src.dtype + " dst " + check_four_params.dst0.dtype
        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + check_four_params.name, dtype_str), True,
                                    gen_api_check_statement(dtype_str_error, api_name))

    @staticmethod
    def check_fifr_overflow(filter_params):
        """
        check fifr param and  tensor overflow
        """
        dst_offset = Expr(filter_params.dst.offset)
        src_offset = Expr(filter_params.src.offset)
        # check overflow
        if filter_params.window_size == 0:
            win_size = 3
        else:
            win_size = 5
        vl_static = 64
        # check src
        need_ele = (filter_params.line_num - 1) * (
                filter_params.src_stride * 32 // DTYPE_SIZE[filter_params.src.dtype]) + \
                   (vl_static + win_size - 1)
        src_extent_offset = Expr(src_offset + need_ele).eval_value()
        src_total_size = reduce_mul(filter_params.src.original_shape)
        if src_extent_offset is not None:
            TikCheckUtil.check_le(
                src_extent_offset, src_total_size,
                "%s's src tensor overflow, src need %s element, but only "
                "%s" % (filter_params.api_name, src_extent_offset, src_total_size))
        # check dst
        dst_need = \
            vl_static + (filter_params.line_num - win_size) * filter_params.dst_stride * \
            32 // DTYPE_SIZE[filter_params.dst.dtype]
        dst_extent_offset = Expr(dst_offset + dst_need).eval_value()
        dst_total_size = reduce_mul(filter_params.dst.original_shape)
        if dst_extent_offset is not None:
            TikCheckUtil.check_le(
                dst_extent_offset, dst_total_size,
                "%s's dst tensor overflow, dst need %s element, but only "
                "%s" % (filter_params.api_name, dst_extent_offset, dst_total_size))

    @staticmethod
    def check_vector_one_dst_two_src_params(name, tensor_params, dtype=None, api_name=None):
        """
        check dst, src0, src1
        """
        if api_name is None:
            api_name = name.replace("vectorized", "vector")
        # check vector
        TikCheckUtil.check_type_match(tensor_params.src0, Vector, "Instruction %s's src0 should be Vector" % api_name)
        TikCheckUtil.check_type_match(tensor_params.src1, Vector, "Instruction %s's src1 should be Vector" % api_name)
        TikCheckUtil.check_type_match(tensor_params.dst, Vector, "Instruction %s's dst should be Vector" % api_name)

        # check src1 dtype
        if name in ("vectorized_vshr", "vectorized_vshl"):
            if tensor_params.src0.scope == scope_vreg and tensor_params.dst.dtype in VSHR_TYPE_MAP.keys():
                TikCheckUtil.check_equality(
                    VSHR_TYPE_MAP[tensor_params.dst.dtype], tensor_params.src1.dtype,
                    "Instruction %s when dst's dtype is %s, src1's dtype should be %s"
                    % (api_name, tensor_params.dst.dtype, VSHR_TYPE_MAP[tensor_params.dst.dtype]))
            elif tensor_params.src0.scope == scope_wreg and tensor_params.dst.dtype in VSHR_WIDE_TYPE_MAP.keys():
                TikCheckUtil.check_equality(
                    VSHR_WIDE_TYPE_MAP[tensor_params.dst.dtype], tensor_params.src1.dtype,
                    "Instruction %s when dst's dtype is %s, src1's dtype should be %s"
                    % (api_name, tensor_params.dst.dtype, VSHR_WIDE_TYPE_MAP[tensor_params.dst.dtype]))
        elif name == "vectorized_vrnd":
            # when src0 is int16, src1 should be uint16
            # when src0 is int32, src1 should be uint32
            if tensor_params.dst.dtype in VRND_TYPE_MAP:
                TikCheckUtil.check_equality(
                    VRND_TYPE_MAP[tensor_params.dst.dtype], tensor_params.src1.dtype,
                    "Instruction %s when dst's dtype is %s, src1's dtype should be %s"
                    % (api_name, tensor_params.dst.dtype, VRND_TYPE_MAP[tensor_params.dst.dtype]))
        else:
            TikCheckUtil.check_equality(tensor_params.src0.dtype, tensor_params.src1.dtype,
                                        "Instruction %s's src1's dtype should be equal to src0's dtype" % api_name)

        if dtype is None:
            dtype_str = DTYPE_MAP[tensor_params.src0.dtype] + DTYPE_MAP[tensor_params.dst.dtype]
            dtype_str_error = "src0 " + tensor_params.src0.dtype + " dst " + tensor_params.dst.dtype
        else:
            # for vector pslide
            TikCheckUtil.check_equality(tensor_params.dst.dtype, tensor_params.src0.dtype,
                                        "Instruction %s's src0's dtype should be equal to dst's dtype" % api_name)
            dtype_str = DTYPE_MAP[dtype] + DTYPE_MAP[dtype]
            dtype_str_error = dtype
        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + name, dtype_str), True,
                                    gen_api_check_statement(dtype_str_error, api_name))

    @staticmethod
    def check_vector_one_dst_one_src_params(name, dst, src, src_name=None, api_name=None):
        """
        check vector one dst and one src params
        """
        if api_name is None:
            api_name = name.replace("vectorized", "vector")
        # check vector
        if src_name is None:
            src_name = "src"
        TikCheckUtil.check_type_match(src, Vector, "Instruction %s's %s should be Vector" % (api_name, src_name))
        TikCheckUtil.check_type_match(dst, Vector, "Instruction %s's dst should be Vector" % api_name)

        dtype_str = DTYPE_MAP[src.dtype] + DTYPE_MAP[dst.dtype]
        dtype_str_error = src_name + " " + src.dtype + " dst " + dst.dtype
        TikCheckUtil.check_equality(
            intrinsic_check_support("Intrinsic_" + name, dtype_str), True,
            gen_api_check_statement(dtype_str_error, api_name))

    @staticmethod
    def get_and_check_wmul_src1_dtype(src_dtype, src1, part_mode, api_name):
        """
        get src1 dtype
        :param src_dtype:
        :param src1:
        :param part_mode:
        :param api_name:
        :return:
        """
        TikCheckUtil.check_type_match(
            src1, (int, Expr, Scalar, Vector),
            "src1 should be int, Expr or Scalar, input type of src1: %s" % type(src1))

        if part_mode is None:
            src1_dtype_list = WMUL_TYPE_MAP[src_dtype]
        else:
            src1_dtype_list = WMUL_TYPE_MAP_WITH_PART_MODE[src_dtype]
        if isinstance(src1, (Expr, Scalar, Vector)):
            TikCheckUtil.check_var_in_list(
                src1.dtype, src1_dtype_list,
                "For intrinsic %s, when input src0's dtype is %s and part_mode is %s, src1's dtype should be in %s, "
                "but input dtype is %s" % (api_name, src_dtype, part_mode, src1_dtype_list, src1.dtype))
            src1_dtype = src1.dtype
        else:
            src1_dtype = ""
            for dtype in src1_dtype_list:
                dtype_range = [DTYPE_IMM_MIN[dtype], DTYPE_IMM_MAX[dtype]]
                if src1 < dtype_range[0] or src1 > dtype_range[1]:
                    continue
                src1_dtype = dtype
                break

            if src1_dtype == "":
                print_error_msg(
                    "Intrinsic %s's src1's out of range, when src0 dtype is %s and part_mode is %s, "
                    "src1 should be in range of dtype of %s" % (api_name, src_dtype, part_mode, src1_dtype_list))
        return src1_dtype

    @staticmethod
    def check_vector_two_dst_two_src_params(tensor_params, dtype=None):
        """
        check vector two dst and two src params
        """
        api_name = tensor_params.name.replace("vectorized_", "vector_")
        # check vector
        TikCheckUtil.check_type_match(tensor_params.src0, Vector, "Instruction %s's src0 should be Vector" % api_name)
        TikCheckUtil.check_type_match(tensor_params.src1, Vector, "Instruction %s's src1 should be Vector" % api_name)
        TikCheckUtil.check_type_match(tensor_params.dst0, Vector, "Instruction %s's dst should be Vector" % api_name)
        TikCheckUtil.check_type_match(tensor_params.dst1, Vector, "Instruction %s's dst should be Vector" % api_name)

        # check data type
        TikCheckUtil.check_equality(tensor_params.dst0.dtype, tensor_params.src0.dtype,
                                    "Instruction %s's src0's dtype should be equal to dst0's dtype" % api_name)
        TikCheckUtil.check_equality(tensor_params.dst0.dtype, tensor_params.src1.dtype,
                                    "Instruction %s's src1's dtype should be equal to dst0's dtype" % api_name)
        TikCheckUtil.check_equality(tensor_params.dst0.dtype, tensor_params.dst1.dtype,
                                    "Instruction %s's dst1's dtype should be equal to dst0's dtype" % api_name)

        # check same vector
        if id(tensor_params.dst0) == id(tensor_params.dst1):
            print_error_msg("Instruction %s's dst0 and dst1 cannot be the same Vector" % api_name)

        if dtype is None:
            dtype = tensor_params.dst0.dtype

        TikCheckUtil.check_equality(intrinsic_check_support("Intrinsic_" + tensor_params.name, dtype), True,
                                    gen_api_check_statement(dtype, api_name))

    @staticmethod
    def check_name_not_in_vshrs_vshls(tensor, vec_check_params, api_name, ):
        """
        check name not in vshrs vshls
        Parameters
        ----------
        tensor
        vec_check_params
        api_name

        Returns
        -------

        """
        TikCheckUtil.check_type_match(
            tensor, (int, float, Expr, Scalar),
            "src1 should be int, float, Expr or Scalar, input type of scalar: %s" % type(tensor))
        if isinstance(tensor, Expr) and vec_check_params.dst.dtype == "float16":
            TikCheckUtil.check_var_in_list(
                tensor.dtype, ["float16", "float32"],
                "Intrinsic %s's src1's dtype should be equal to dst's dtype, input dst's "
                "dtype: %s, input scalar's dtype: %s" % (api_name, vec_check_params.dst.dtype,
                                                         tensor.dtype))
        elif isinstance(tensor, (Expr, Scalar)):
            TikCheckUtil.check_equality(
                vec_check_params.dst.dtype, tensor.dtype,
                "Intrinsic %s's src1's dtype should be equal to dst's dtype, input dst's "
                "dtype: %s, input scalar's dtype: %s" % (api_name, vec_check_params.dst.dtype,
                                                         tensor.dtype))
        else:
            TikCheckUtil.check_in_range_by_dtype(
                tensor, vec_check_params.dst.dtype,
                "Variable out of range, input dtype of dst: %s" % vec_check_params.dst.dtype)
            if "int" in vec_check_params.dst.dtype:
                TikCheckUtil.check_equality(
                    type(tensor), int,
                    "Intrinsic %s's src1's dtype should be equal to dst's dtype, input dst's "
                    "dtype: %s, input scalar's dtype: %s" % (api_name, vec_check_params.dst.dtype, "float"))

    @staticmethod
    def check_filter1d(filter1d_params):
        """
        check coefficient check sid
        Parameters
        ----------
        filter1d_params

        Returns
        -------

        """
        # check coefficient
        check_param_type_range(
            [filter1d_params.r1c1, filter1d_params.r1c2, filter1d_params.r1c3, filter1d_params.r1c4,
             filter1d_params.r1c5, filter1d_params.r1t, filter1d_params.r2t, filter1d_params.r3t,
             filter1d_params.r4t, filter1d_params.r5t],
            [-16, -16, -16, -16, -16, -16, -16, -16, -16, -16], [15, 15, 15, 15, 15, 15, 15, 15, 15, 15],
            ["r1c1", "r1c2", "r1c3", "r1c4", "r1c5", "r1t", "r2t", "r3t", "r4t", "r5t"], "vector_filter1d")
        # check sid
        check_param_type_range([filter1d_params.rsah, filter1d_params.rsa], [0, 0], [7, 7],
                               ["rsah", "rsa"], "vector_filter1d")

    @staticmethod
    def check_wmul_v2(wmul_params):
        """
        check wmul_params
        Parameters
        ----------
        wmul_params : wmul's params

        Returns
        -------

        """
        # check st_mode
        if not TikSocManager.is_v300_610l_soc():
            print_error_msg("Instruction %s not support %s" % (wmul_params.api_name, get_soc_name()))
        if wmul_params.acc_mode is not None and wmul_params.acc_mode != "MULA":
            print_error_msg("acc_mode of instruction %s should be None or MULA" % wmul_params.api_name)
        TikCheckUtil.check_type_match(wmul_params.dst, Vector,
                                      "Instruction %s's dst should be Vector" % wmul_params.api_name)
        TikCheckUtil.check_type_match(wmul_params.src0, Vector,
                                      "Instruction %s's src0 should be Vector" % wmul_params.api_name)
        TikCheckUtil.check_type_match(wmul_params.src1, Vector,
                                      "Instruction %s's src1 should be Vector" % wmul_params.api_name)
        TikCheckUtil.check_type_match(wmul_params.src2, Vector,
                                      "Instruction %s's src2 should be Vector" % wmul_params.api_name)

        TikCheckUtil.check_equality(wmul_params.src1.dtype, wmul_params.src2.dtype,
                                    "Instruction {}'s src1's dtype should be equal "
                                    "to src2's dtype".format(wmul_params.api_name))
        dtype_str = wmul_params.dst.dtype + " " + wmul_params.src0.dtype + " " + wmul_params.src1.dtype

        wmul_v2_dtype_map = {"int24 int8 int16", "int24 uint8 int16", "int48 uint16 uint32",
                             "int48 int16 int32", "int48 uint16 int32", "int48 int16 uint32"}
        if dtype_str not in wmul_v2_dtype_map:
            print_error_msg("Instruction %s not support dst %s src0 %s src1 %s src2 %s" % (
                wmul_params.api_name, wmul_params.dst.dtype, wmul_params.src0.dtype,
                wmul_params.src1.dtype, wmul_params.src2.dtype))

    @staticmethod
    def check_wmuls_v2(wmuls_params):
        """
        check wmul_params
        Parameters
        ----------
        wmuls_params : wmuls params

        Returns
        -------

        """
        # check st_mode
        if not TikSocManager.is_v300_610l_soc():
            print_error_msg("Instruction %s not support %s" % (wmuls_params.api_name, get_soc_name()))
        if wmuls_params.acc_mode is not None and wmuls_params.acc_mode != "MULAS":
            print_error_msg("acc_mode of instruction %s should be None or MULAS" % (wmuls_params.api_name))
        TikCheckUtil.check_type_match(wmuls_params.dst, Vector,
                                      "Instruction %s's dst should be Vector" % wmuls_params.api_name)
        TikCheckUtil.check_type_match(wmuls_params.src0, Vector,
                                      "Instruction %s's src0 should be Vector" % wmuls_params.api_name)
        TikCheckUtil.check_type_match(wmuls_params.src1, (int, Expr, Scalar),
                                      "Instruction %s's src1 should be int, scalar or expr" % (wmuls_params.api_name))

        if isinstance(wmuls_params.src1, int):
            dtype_str = wmuls_params.dst.dtype + " " + wmuls_params.src0.dtype
            wmul_v2_dtype_map = {"int24 int8", "int24 uint8", "int48 uint16",
                                 "int48 int16", "int48 uint16", "int48 int16"}
            if dtype_str not in wmul_v2_dtype_map:
                print_error_msg("Instruction %s not support dst %s src0 %s when src1's type is int" % (
                    wmuls_params.api_name, wmuls_params.dst.dtype, wmuls_params.src0.dtype))
        else:
            dtype_str = wmuls_params.dst.dtype + " " + wmuls_params.src0.dtype + " " + wmuls_params.src1.dtype
            wmuls_v2_dtype_map = {"int24 int8 int16", "int24 uint8 int16", "int48 uint16 uint32",
                                  "int48 int16 int32", "int48 uint16 int32", "int48 int16 uint32"}
            if dtype_str not in wmuls_v2_dtype_map:
                print_error_msg("Instruction %s not support dst %s src0 %s src1 %s" % (
                    wmuls_params.api_name, wmuls_params.dst.dtype, wmuls_params.src0.dtype, wmuls_params.src1.dtype))

    @staticmethod
    def data_move_check(check_params):
        """
        check data move gather
        Parameters
        ----------
        check_params

        Returns
        -------

        """
        name = "data_move_gather"
        TikCheckUtil.check_type_match(check_params.dst, Tensor,
                                      "dst should be tensor, but input type: %s" % type(check_params.dst))
        TikCheckUtil.check_type_match(check_params.src, Tensor,
                                      "src should be tensor, but input type: %s" % type(check_params.src))
        TikCheckUtil.check_type_match(check_params.src_index, Tensor,
                                      "src_index should be tensor, but input type: %s" % type(check_params.src_index))
        TikCheckUtil.check_var_in_list(check_params.src_index.dtype, ["uint16", "uint32"],
                                       "src_index's dtype should be uint16 or uint32, "
                                       "but input dtype: %s" % check_params.src_index.dtype)
        TikCheckUtil.check_equality(check_params.dst.scope, scope_ubuf,
                                    "dst's scope should be %s, but get %s" %
                                    (scope_ubuf, check_params.dst.scope))
        TikCheckUtil.check_equality(check_params.src.scope, scope_gm,
                                    "src's scope should be %s, but get %s" %
                                    (scope_gm, check_params.src.scope))
        TikCheckUtil.check_equality(check_params.src_index.scope, scope_ubuf,
                                    "src_index's scope should be %s, but get %s" %
                                    (scope_ubuf, check_params.src_index.scope))
        TikCheckUtil.check_equality(check_params.dst.dtype, check_params.src.dtype,
                                    "Intrinsic %s's src's dtype should be equal to dst's dtype" % name)
        TikCheckUtil.check_equality(
            intrinsic_check_support("Intrinsic_" + name, check_params.dst.dtype), True,
            gen_api_check_statement(check_params.dst.dtype, name))
        # dst and src_index need 32 byte align
        check_address_align([check_params.dst, check_params.src_index], ["dst", "src_index"])
        # src need 512 byte align
        # check param: index_num, ele_size, dst_stride
        check_address_align([check_params.src], ["src"], 512)
        if TikSocManager.is_v300_610l_soc():
            ele_size_range, max_value_list = [0, 4], [65535, 6]
        else:
            ele_size_range, max_value_list = [0, 3], [65535, 5]
        check_param_type_range([check_params.index_num, check_params.dst_stride], [1, 0], max_value_list,
                               ["index_num", "dst_stride"], name)
        TikCheckUtil.check_type_match(check_params.ele_size, int,
                                      "input param ele_size should be type of int, input type %s" %
                                      type(check_params.ele_size))
        TikCheckUtil.check_in_range_by_dtype(
            check_params.ele_size, msg="ele_size should be in the range [%s, %s], input value: %s"
                                       % (ele_size_range[0], ele_size_range[1], check_params.ele_size),
            var_range=ele_size_range)

        # check tensor overflow
        check_mvf_data_move_overflow(check_params, check_params.index_num,
                                     check_params.dst_stride, Expr(check_params.dst.offset),
                                     Expr(check_params.src_index.offset))

    @staticmethod
    def check_ld_mode_and_get_vlen(ld_mode, src, ld):
        """
        check ld mode and get vlen
        Parameters
        ----------
        ld_mode
        src
        ld

        Returns
        -------

        """
        # check ld_mode

        if ld_mode is not None:
            TikCheckUtil.check_type_match(
                ld_mode, str, "ld_mode should be None or str, but input type:%s" % type(ld_mode))
            TikCheckUtil.check_var_in_list(
                ld_mode, ['NORM', 'BRC', 'US', 'DS', 'BDINTLV', 'DINTLV', 'UNPK', 'E2B'],
                "when ld_mode is not None, it should be in the range of ['NORM', 'BRC', 'US', 'DS', 'BDINTLV', "
                "'DINTLV', 'UNPK'], but input ld_mode: %s" % ld_mode)

        vlen = ld.get_vector_length(src.dtype)
        if ld_mode in ('NORM', 'DS', 'US', 'BDINTLV', 'DINTLV', 'UNPK'):
            check_address_align([src], ["src"])
        extent_map = {
            None: vlen * DTYPE_SIZE[src.dtype],
            'NORM': vlen * DTYPE_SIZE[src.dtype],
            'BRC': DTYPE_SIZE[src.dtype],
            'US': vlen * DTYPE_SIZE[src.dtype] // 2,
            'DS': vlen * DTYPE_SIZE[src.dtype] * 2,
            'BDINTLV': vlen * DTYPE_SIZE[src.dtype] * 2,
            'DINTLV': vlen * DTYPE_SIZE[src.dtype] * 2,
            'UNPK': vlen * DTYPE_SIZE[src.dtype] // 2,
        }
        check_extent_overflow(src, extent_map.get(ld_mode),
                              Expr(src.offset).eval_value(), 'src')
        # check instr support dtype
        vec_load_map = {
            None: ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'NORM': ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'BRC': ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'US': ('uint8', 'int8', 'uint16', 'int16', 'float16'),
            'DS': ('uint8', 'int8', 'uint16', 'int16', 'float16'),
            'BDINTLV': ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'DINTLV': ('uint8', 'int8', 'uint16', 'int16', 'float16'),
            'UNPK': ('uint8', 'int8', 'uint16', 'int16', 'float16'),
            'E2B': ('uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
        }
        TikCheckUtil.check_var_in_list(
            src.dtype, vec_load_map.get(ld_mode),
            "Instruction vector_load not support %s in ld_mode of %s" % (src.dtype, ld_mode))
        return vlen

    @staticmethod
    def check_store_mode(st_mode, dst, mask):
        """
        check st_mode
        Parameters
        ----------
        st_mode
        dst
        mask

        Returns
        -------

        """
        # check st_mode
        if st_mode is not None:
            TikCheckUtil.check_type_match(
                st_mode, str, "st_mode should be None or str, but input type: %s" % type(st_mode))
            TikCheckUtil.check_var_in_list(
                st_mode, ['NORM', 'ONEPT', 'PK', 'INTLV', 'SQZN'],
                "when st_mode is not None, it should be in the range of ['NORM', 'ONEPT', 'PK', 'INTLV', 'SQZN'], "
                "but input st_mode: %s" % st_mode)

        if st_mode in ('NORM', 'PK', 'INTLV'):
            check_address_align([dst], ["dst"])
        # check instr support dtype
        vec_store_map = {
            None: ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'NORM': ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'ONEPT': ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'INTLV': ('uint8', 'int8', 'uint16', 'int16', 'float16'),
            'PK': ('uint16', 'int16', 'float16', 'uint32', 'int32', 'float32'),
            'SQZN': ('uint8', 'int8', 'uint16', 'int16', 'float16', 'uint32', 'int32', 'float32')
        }
        TikCheckUtil.check_var_in_list(
            dst.dtype, vec_store_map.get(st_mode),
            "Instruction vector_store not support %s in st_mode of %s." % (dst.dtype, st_mode))

        # check mask
        if mask is not None:
            if st_mode in [None, 'ONEPT', 'INTLV']:
                print_error_msg("when st_mode is %s, mask should be None" % st_mode)
            if not isinstance(mask, (int, Scalar, Expr)):
                TikCheckUtil.check_type_match(
                    mask, Vector,
                    "mask should be None, int, Scalar, Expr or Vector, but input type: %s" % type(mask))
                TikCheckUtil.check_equality(
                    mask.dtype, "bool", "mask's dtype should be bool, but input dtype: %s" % mask.dtype)

    def check_elewise_scalar_func_params(self, name, vec_check_params, api_name=None):
        """
        dst = src <op> scalar
        Parameters
        ----------
        name: instr name
        dst: vector destination operator
        src: vector source operation
        scalar: scalar or imm
        api_name: if is not None, use it as api name for output

        Returns
        -------
        None
        :param name:
        :param api_name:
        :param vec_check_params:
        """
        if api_name is None:
            api_name = name.replace("vectorized_", "vector_")

        self.check_vector_one_dst_one_src_params(name, vec_check_params.dst, vec_check_params.src, api_name=api_name)

        if name in ("vectorized_vshrs", "vectorized_vshls"):
            TikCheckUtil.check_type_match(vec_check_params.scalar, (int, Expr, Scalar),
                                          "Intrinsic %s's src1 should be int, Expr or Scalar, "
                                          "input type of scalar: %s" % (api_name, type(vec_check_params.scalar)))
            scalar_dtype = "int16"
            if vec_check_params.src.scope == scope_wreg:
                scalar_dtype = "uint16"
            if isinstance(vec_check_params.scalar, (Expr, Scalar)):
                TikCheckUtil.check_equality(scalar_dtype, vec_check_params.scalar.dtype,
                                            "Intrinsic %s's src1's dtype should be %s, input scalar's "
                                            "dtype: %s" % (api_name, scalar_dtype, vec_check_params.scalar.dtype))
            else:
                TikCheckUtil.check_in_range_by_dtype(vec_check_params.scalar, scalar_dtype,
                                                     "src1 out of range, which should be %s" % scalar_dtype)
        else:
            self.check_name_not_in_vshrs_vshls(vec_check_params.scalar, vec_check_params, api_name)

    def check_elewise_tensor_func_params(self, name, vec_check_params, api_name=None):
        """
        dst = src <op> scalar
        Parameters
        ----------
        name: instr name
        dst: vector destination operator
        src: vector source operation
        scalar: scalar or imm
        api_name: if is not None, use it as api name for output

        Returns
        -------
        None
        :param name:
        :param api_name:
        :param vec_check_params:
        """
        if api_name is None:
            api_name = name.replace("vectorized_", "vector_")

        self.check_vector_one_dst_one_src_params(name, vec_check_params.dst, vec_check_params.src0, api_name=api_name)

        if name in ("vectorized_vshrs", "vectorized_vshls"):
            TikCheckUtil.check_type_match(vec_check_params.src1, (int, Expr, Scalar),
                                          "Intrinsic %s's src1 should be int, Expr or Scalar, "
                                          "input type of scalar: %s" % (api_name, type(vec_check_params.src1)))
            scalar_dtype = "int16"
            if vec_check_params.src0.scope == scope_wreg:
                scalar_dtype = "uint16"
            if isinstance(vec_check_params.src1, (Expr, Scalar)):
                TikCheckUtil.check_equality(scalar_dtype, vec_check_params.src1.dtype,
                                            "Intrinsic %s's src1's dtype should be %s, input scalar's "
                                            "dtype: %s" % (api_name, scalar_dtype, vec_check_params.src1.dtype))
            else:
                TikCheckUtil.check_in_range_by_dtype(vec_check_params.src1, scalar_dtype,
                                                     "src1 out of range, which should be %s" % scalar_dtype)
        else:
            self.check_name_not_in_vshrs_vshls(vec_check_params.src1, vec_check_params, api_name)
