#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_tensor.py
DESC:     tik tensor explanation
CREATED:  2019-04-18 18:53:42
MODIFIED: 2019-07-18 21:29:18
"""
from collections import namedtuple

from tbe import tvm
from tbe.tvm import decl_buffer
from tbe.tvm import tir
from tbe.tvm.ir import PrimType
from tbe.tvm.ir import PointerType
from tbe.common.platform import scope_gm
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_ca
from tbe.common.platform import scope_cb
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_smask
from tbe.common.platform.platform_info import scope_fb0
from tbe.common.platform.platform_info import scope_bt
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik import debug
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import flat_list
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.util import tvm_ceil_align
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.util import check_is_atomic_add_attr
from tbe.tik.common.tik_api_map import ASCEND_310
from tbe.tik.common.tik_api_map import ASCEND_910
from tbe.tik.tik_lib.tik_params import MAX_INT32_VALUE
from tbe.tik.tik_lib.tik_basic_data import BasicData
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_conf_ import set_context_global_variable_link
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_source_info import get_span
from tbe.tik.tik_lib.tik_tensor_ import TensorAddress
from tbe.tik.tik_lib.tik_tensor_ import normalize_shape
from tbe.tik.tik_lib.tik_tensor_ import tensor_type_convert
from tbe.tik.tik_lib.tik_tensor_ import has_scalar
from tbe.tik.tik_lib.tik_tensor_ import TensorInner
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_expr import Expr


def _check_tensor_scope(scope):
    """
    when tensor is applied in the scope ,tensor's scope should be checeked

    Parameters
    ----------
    scope:the tensor's parameter scope

    Returns
    -------
    No return
    """
    scope_list = [scope_gm, scope_cbuf, scope_ubuf, scope_ca, scope_cb, scope_cc, scope_smask, scope_fb0, scope_bt]
    TikCheckUtil.check_var_in_list(scope, scope_list, "scope out of Tensor Scope")


def _tvm_value_to_int(value):
    """
    According tvm value return value.

    Parameters
    ----------
    value : tvm value

    Returns
    -------
    value
    """
    if isinstance(value, (tvm.tir.IntImm, tvm.tir.FloatImm, tvm.tir.StringImm)):
        return value.value
    if isinstance(value, (tvm.ir.container.Array, list, tuple)):
        tmp = []
        for val in value:
            tmp.append(_tvm_value_to_int(val))
        return tmp
    if isinstance(value, int):
        return value
    return TikCheckUtil.raise_error("convert value is illegal")


@debug.tensor_register
class Tensor(TensorInner, BasicData, TensorAddress):
    """
    Defines a Tensor class.
    """
    VECTOR_MAC_BIT = 128 * 16
    BUFFER_STORAGE_COUNT = 0
    GLOBAL_VARIABLE_LINK = False
    set_new_tensor_api = namedtuple('SetNewTensorApi', ["scope", "ir_generator", "name", "shape_size",
                                                        "enable_buffer_reuse", "is_atomic_add",
                                                        "init_value", "start_addr", "is_tiling_tensor"])
    tensor_init_api = namedtuple('TensorInitApi', ["dims", "stride", "data", "buffer_", "original_shape",
                                                   "old_data", "last_tensor", "is_reshape",
                                                   "is_getitem", "max_mem_size"])

    def __init__(self, ir_generator, dtype=None, shape=None, scope=None, name=None, buffer_=None, dims=None,
                 enable_buffer_reuse=False, reuse_list=None, is_workspace=False, is_atomic_add=False, max_mem_size=None,
                 init_value=None, stride=None, data=None, original_shape=None, old_data=0, last_tensor=None,
                 is_reshape=False, is_getitem=False, is_global_tensor=False, start_addr=None, is_tiling_tensor=False):
        """
        tensor register initialization
        Parameters
        ----------
        ir_generator:Halide IR generator
        dtype:tensor register data type
        shape:tensor register shape
        scope:tensor register scope
        name:tensor register name
        buffer_:tensor register buffer
        enable_buffer_reuse : whether specify reuse relationship between tensors or not
        is_workspace: whether is workspace or not
        is_atomic_add: whether is atomic add or not
        is_global_tensor: whether is global tensor or not
        start_addr: start address in scope
        is_tiling_tensor: whether is tiling gm or ub

        Returns
        -------
        no return
        """
        TensorInner.__init__(self, ir_generator, scope=scope)
        BasicData.__init__(self, "Tensor")
        TensorAddress.__init__(self, ir_generator)

        self.__check_tensor_dtype_init_value(dtype, init_value, scope)
        # _available:The scalar state variable. when scalar variable
        # "_available" is True, this scalar can be accessed.when scalar variable
        # "_available" is False, this scalar can't be accessed and assert
        # "This Scalar is not defined in this scope."
        self.dtype = dtype
        self.buffer_storage_id = None
        self.reuse_list = reuse_list
        self.is_workspace = is_workspace
        self.is_global_tensor = is_global_tensor
        self.is_atomic_add = is_atomic_add
        self.buffer_shape_dtype = None
        self.max_mem_size = max_mem_size
        self.last_tensor = None
        self.start_addr = start_addr
        self.buffer = buffer_
        self.init_value = init_value
        self.is_tiling_tensor = is_tiling_tensor

        if (shape is not None) and (scope is not None) and (dtype is not None):

            self.__check_tensor_params(shape, scope, name)

            self.name = name
            self.original_name = name
            self.dimensions = normalize_shape(shape)
            self.strides = self.calc_strides(self.dimensions)
            # after "*" or "+" operator,
            # the result may exceed the range of dtype
            if has_scalar(self._get_shape()):
                self.data = tvm.const(0, "int64")
            else:
                self.data = 0
            self.is_static_shape = False
            shape_size = self.get_shape(max_mem_size, shape, self.dtype)
            # for gm tensor may have shape out of int32 range
            if isinstance(shape_size, int) and shape_size > MAX_INT32_VALUE:
                shape_size = tvm.const(shape_size, "int64")
            shape_size = self.convet_to_var(shape_size, "total_size")
            self.original_shape = self._get_shape()
            self.check_start_addr(scope, name, dtype, start_addr)
            new_tensor_obj = Tensor.set_new_tensor_api(scope, ir_generator, name, shape_size, enable_buffer_reuse,
                                                       self.is_atomic_add, init_value, start_addr, is_tiling_tensor)
            self._set_new_tensor(new_tensor_obj)
        elif shape is None:
            TikCheckUtil.check_not_is(self.buffer, None, "param buffer_ should not be None")
            tensor_init_obj = Tensor.tensor_init_api(dims, stride, data, self.buffer, original_shape,
                                                     old_data, last_tensor,
                                                     is_reshape, is_getitem, max_mem_size)
            self._init_with_shape_none(tensor_init_obj)
        else:
            TikCheckUtil.raise_error("Tensor Class must be initial by (dtype, shape, scope, name)")

    @source_info_decorator()
    def __getitem__(self, index_in):
        """
        Obtains partial tensor data to form a new tensor.
        Parameters
        ----------
        index_in : Tensor array index.

        Returns
        -------
        The new tensor
        """
        self.check_tensor_scope()
        items = self.get_items(index_in)

        if self.is_version1() and len(items) == 1 and len(self.dimensions) > 1:
            self.check_tensor_getitem_flatten()
            ori_shape_size = reduce_mul(self.original_shape)
            flatten_dims = [slice(0, tensor_type_convert(ori_shape_size), 1)]
            new_dims = self.calc_new_dims(items, flatten_dims)
            flatten_stride = [self.strides[-1]]
            new_strides = self.calc_new_strides(new_dims, flatten_stride)
            temp_data = self.data
            new_data = self.__calc_new_offset(new_dims, flatten_stride)

            if isinstance(new_data, (Scalar, Expr)):
                old_data = new_data - temp_data
            else:
                old_data = -1 * temp_data + new_data

            new_original_shape = [ori_shape_size]
            new_tensor_obj = Tensor.new_tensor_api(
                self.ir_generator, self.buffer, self.dtype, new_dims, new_strides, self.is_workspace,
                self.is_atomic_add, new_data, self.max_mem_size, new_original_shape, old_data, self,
                is_reshape=True, is_getitem=True, is_global_tensor=self.is_global_tensor, scope=self.tensor_scope,
                is_tiling_tensor=self.is_tiling_tensor)
            return self._new_tensor(new_tensor_obj)
        new_dims = self.calc_new_dims(items, self.dimensions)
        new_strides = self.calc_new_strides(new_dims, self.strides)
        temp_data = self.data
        new_data = self.calc_new_data(new_dims, self.strides, self.data)
        if isinstance(new_data, (Scalar, Expr)):
            old_data = new_data - temp_data
        else:
            old_data = -1 * temp_data + new_data

        new_tensor_obj = Tensor.new_tensor_api(
            self.ir_generator, self.buffer, self.dtype, new_dims, new_strides, self.is_workspace, self.is_atomic_add,
            new_data, self.max_mem_size, self.original_shape, old_data, self, is_reshape=False, is_getitem=True,
            is_global_tensor=self.is_global_tensor, scope=self.tensor_scope, is_tiling_tensor=self.is_tiling_tensor)
        return self._new_tensor(new_tensor_obj)

    @source_info_decorator()
    def __setitem__(self, index, value):
        """
        Changes a tensor.

        Parameters
        ----------
        index : Tensor array index.
        value : Specific value, which is related to the data type defined by the tensor.

        Returns
        -------
        no return
        """
        self.__getitem__(index).set_as(value)

    def __str__(self):
        """
        get tensor name
        Parameters
        ----------

        Returns
        -------
        get tensor name
        """
        return self.name

    @source_info_decorator()
    def __eq__(self, other):
        """
        judging whether two tensor are equal
        Parameters
        ----------
        other:other tensor register

        Returns
        -------
        judging whether two tensor are equal
        """
        reg = self.__to_reg()
        return reg == other

    @source_info_decorator()
    def __ne__(self, other):
        """
        judging whether two tensor aren't equal
        Parameters
        ----------
        other:other tensor register

        Returns
        -------
        """
        reg = self.__to_reg()
        return reg != other

    @source_info_decorator()
    def __lt__(self, other):
        """
        judging whether tensor is less than other
        Parameters
        ----------
        other:other tensor register

        Returns
        -------
        """
        reg = self.__to_reg()
        return reg < other

    @source_info_decorator()
    def __gt__(self, other):
        """
        judging whether tensor is greater than other
        Parameters
        ----------
        other:other tensor register

        Returns
        -------
        """
        reg = self.__to_reg()
        return reg > other

    @source_info_decorator()
    def __ge__(self, other):
        """
        judging whether tensor is greater than or equal to other
        Parameters
        ----------
        other:other tensor register

        Returns
        -------
        """
        reg = self.__to_reg()
        return reg >= other

    @source_info_decorator()
    def __le__(self, other):
        """
        judging whether tensor is less than or equal to other
        Parameters
        ----------
        other:other tensor register

        Returns
        -------
        """
        reg = self.__to_reg()
        return reg <= other

    def __hash__(self):
        """
        according buffer return hash value
        Parameters
        ----------

        Returns
        -------
        """
        return hash((self.buffer, self.data))

    @property
    @source_info_decorator()
    def shape(self):
        """
        return tensor shape
        Parameters
        ----------

        Returns
        -------
        """
        self.check_tensor_scope()
        return self._get_shape()

    @classmethod
    def _new_tensor(cls, obj):
        """
        tensor's constructive function

        Parameters
        ----------
        obj: is new_tensor_api
        """
        return cls(obj.ir_generator, buffer_=obj.buffer_,
                   dtype=obj.dtype, dims=obj.dims, stride=obj.stride,
                   is_workspace=obj.is_workspace, is_atomic_add=obj.is_atomic_add,
                   data=obj.data, max_mem_size=obj.max_mem_size, original_shape=obj.original_shape,
                   old_data=obj.old_data, last_tensor=obj.last_tensor, is_reshape=obj.is_reshape,
                   is_getitem=obj.is_getitem, is_global_tensor=obj.is_global_tensor, scope=obj.scope,
                   is_tiling_tensor=obj.is_tiling_tensor)

    @staticmethod
    def __check_tensor_dtype_init_value(dtype, init_value, scope):
        """
        check tensor dtype init_value

        Parameters
        ----------
        dtype:tensor register data type
        init_value: init_value
        scope:tensor register scope

        Returns
        -------
        None
        """
        # check dtype is str
        TikCheckUtil.check_type_match(dtype, str, "dtype should be str")
        TikCheckUtil.check_equality(init_value is None or scope == scope_gm, True,
                                    "init_value is only for gm Tensor.")

    @staticmethod
    def __check_tensor_params(shape, scope, name):
        """
        check_tensor_params

        Parameters
        ----------
        shape: tensor register shape
        scope: tensor register scope
        name: tensor register name

        Returns
        -------
        None
        """
        TikCheckUtil.check_type_match(shape, (list, tuple), "shape: %s must be list or tuple." % str(shape))
        for i in shape:
            TikCheckUtil.check_equality(
                isinstance(i, (int, float, Scalar, Expr, tvm.tir.IntImm,
                               tvm.tir.FloatImm, tvm.tir.StringImm)), True,
                "element in shape should be Imm, Scalar or Expr.")
            if isinstance(i, (Scalar, Expr)):
                TikCheckUtil.check_equality(i.dtype.startswith(("int", "uint")), True,
                                            "When the shape element is Scalar or Expr, it's dtype cannot be float")
            if is_immediate_number([i]) and i <= 0:
                TikCheckUtil.raise_error("invalid shape input, shape: %s" % list(shape))
        _check_tensor_scope(scope)
        # check name is valid
        TikCheckUtil.check_name_str_valid(name)

    @staticmethod
    def __calc_new_offset(new_dims, strides):
        """
        calc new data

        Parameters
        ----------
        new_dims: new dim
        strides: strides

        Returns
        -------
        return:new data
        """
        res = 0
        for dim, stride in zip(new_dims, strides):
            if isinstance(stride, (Scalar, Expr)):
                tmp = stride * dim.start
            else:
                tmp = dim.start * stride
            res = tmp
        return res

    @source_info_decorator()
    def reshape(self, new_shape):
        """
        Sets the tensor shape.
        Parameters
        ----------
        new_shape : New shape of a tensor object.

        Returns
        -------
        New tensor
        """
        return self.reshape_(new_shape)

    @source_info_decorator()
    def reinterpret_cast_to(self, dtype):
        """
        Casts the data type of a tensor.

        Parameters
        ----------
        dtype : Data type of the Tensor object.

        Returns
        -------
        The new tensor
        """
        # check dtype is str
        self.check_tensor_scope()
        TikCheckUtil.check_type_match(dtype, str, "dtype should be str")
        TikCheckUtil.check_var_in_list(dtype, DTYPE_SIZE, "Not a valid dtype to reinterpret_cast_to.")
        # last stride must be 1, then the dimension should be divided by dtype_size.
        # we will change the stride and dimension
        if Expr(self.strides[-1]).eval_value() is not None:
            TikCheckUtil.check_equality(Expr(self.strides[-1]).eval_value(), 1,
                                        "Last stride should be equal to 1 when do reinterpret_cast_to")
        # we will check last dimensions will be % by new dtype_size
        last_dim_offset = self.dimensions[-1].stop - self.dimensions[-1].start
        new_dim = []
        for i in self.dimensions:
            new_dim.append(slice(i.start, i.stop, i.step))
        if DTYPE_SIZE[dtype] >= DTYPE_SIZE[self.dtype]:
            # which means we should compress
            # we should check last can be divided by dtype_size
            dtype_factor = DTYPE_SIZE[dtype] // DTYPE_SIZE[self.dtype]
            if Expr(last_dim_offset).eval_value() is not None:
                TikCheckUtil.check_equality(
                    Expr(last_dim_offset).eval_value() % dtype_factor, 0, "Last dimension can't be divided")

            new_dim[-1] = slice(self.dimensions[-1].start // dtype_factor,
                                self.dimensions[-1].stop // dtype_factor, 1)
            new_stride = [stride // dtype_factor for stride in self.strides]
            new_stride[-1] = 1
            new_data = self.data // dtype_factor
            new_original_shape = list(self.original_shape)
            new_original_shape[-1] //= dtype_factor
        else:
            if self.is_version1():
                dtype_factor = DTYPE_SIZE[self.dtype] // DTYPE_SIZE[dtype]
                new_dim.append(slice(0, dtype_factor, 1))
                new_stride = [stride * dtype_factor for stride in self.strides]
                new_stride.append(1)
                new_data = self.data * dtype_factor
                new_original_shape = list(self.original_shape)
                new_original_shape.append(dtype_factor)
            else:
                dtype_factor = DTYPE_SIZE[self.dtype] // DTYPE_SIZE[dtype]
                new_dim[-1] = slice(self.dimensions[-1].start * dtype_factor,
                                    self.dimensions[-1].stop * dtype_factor, 1)
                new_stride = [stride * dtype_factor for stride in self.strides]
                new_stride[-1] = 1
                new_data = self.data * dtype_factor
                new_original_shape = list(self.original_shape)
                new_original_shape[-1] *= dtype_factor

        new_tensor_obj = Tensor.new_tensor_api(
            self.ir_generator, self.buffer, dtype, new_dim, new_stride, self.is_workspace, self.is_atomic_add,
            new_data, self.max_mem_size, new_original_shape, old_data=0, last_tensor=self, is_reshape=False,
            is_getitem=False, is_global_tensor=self.is_global_tensor, scope=self.tensor_scope,
            is_tiling_tensor=self.is_tiling_tensor)
        tmp_t = self._new_tensor(new_tensor_obj)
        return tmp_t

    @source_info_decorator()
    @debug.tensor_set_as_decorator
    def set_as(self, value, dst_offset=0, src_offset=None):
        """
        Sets a tensor.
        Parameters
        ----------
        value : Value to be assigned from:
        -       A tensor of one element
        -       A Scalar variable
        -       An Expr consisting of a Scalar variable and an immediate, which cannot be of type float
        dst_offset: An internal optional parameters
        src_offset: An internal optional parameters

        Returns
        -------
        no return
        """
        self.check_tensor_scope()
        if not isinstance(value, (Tensor, Scalar, Expr, int, float)):
            TikCheckUtil.raise_error("Tensor set_as only support Tensor, Scalar, Expr or Imm.")
        if isinstance(value, Tensor):
            if TikSocManager.is_v100_soc() and value.scope == scope_gm:
                TikCheckUtil.raise_error("set_as's value can't be GM Tensor when chip is %s,"
                                         "%s." % (ASCEND_310, ASCEND_910))
        if TikSocManager.is_910b_soc() or TikSocManager.is_310b_610l_soc():
            if self.scope not in [scope_ubuf, scope_gm]:
                TikCheckUtil.raise_error("Dst Tensor can only be UB/GM Tensor but get %s Tensor." % self.scope)
        elif self.scope != scope_ubuf:
            TikCheckUtil.raise_error("Dst Tensor can only be UB Tensor but get %s Tensor." % self.scope)
        self.ir_generator.assign(self, value, dst_offset, src_offset)

    def _init_with_shape_none(self, tensor_init_obj):
        """
        init tensor member var when shape is none
        Parameters
        ----------
        tensor_init_obj: tensor member var container

        Returns None
        -------

        """
        self.dimensions = tensor_init_obj.dims
        self.strides = tensor_init_obj.stride
        self.data = tensor_init_obj.data
        self.buffer = tensor_init_obj.buffer_
        self.buffer_shape_dtype = self.buffer.shape[0].dtype
        name = "%s_proxy_tensor_%s" % (self.buffer.name, str(Tensor.COUNT))
        self.name = name
        self.original_name = self.buffer.name
        self.original_shape = tensor_init_obj.original_shape
        self.old_data = tensor_init_obj.old_data
        # for proxy tensor, it is converted from another tensor,
        # which is called last_tensor
        self.last_tensor = tensor_init_obj.last_tensor
        self.is_reshape = tensor_init_obj.is_reshape
        self.is_getitem = tensor_init_obj.is_getitem
        self.max_mem_size = tensor_init_obj.max_mem_size

    def _set_new_tensor(self, new_tensor_api):
        if new_tensor_api.scope == scope_gm:
            if new_tensor_api.ir_generator.is_tensor_in_scope() and new_tensor_api.name != "_print_workspace":
                TikCheckUtil.raise_error("Tensor can't be defined in local")
            if new_tensor_api.init_value is None or new_tensor_api.is_atomic_add is True:
                self.data_var = tir.expr.Var(new_tensor_api.name,
                                             PointerType(PrimType(self.dtype), new_tensor_api.scope))
                self.buffer = decl_buffer(new_tensor_api.shape_size, self.dtype, new_tensor_api.name,
                                          data=self.data_var)
            else:
                size, init_value = self._check_new_tensor_params_init_value(new_tensor_api)
                if not Tensor.GLOBAL_VARIABLE_LINK:
                    set_context_global_variable_link()
                    Tensor.GLOBAL_VARIABLE_LINK = True
                self.buffer = new_tensor_api.ir_generator.apply_for_new_alloc(
                    (self.dtype, size, new_tensor_api.scope), new_tensor_api.name, init_value, self.buffer_storage_id)
            # attr for tik debug pass to add GM tensor var info
            shape_var = [self.convet_to_var(i) for i in self._get_shape()]
            if not self.ir_generator.debug_disabled_:
                new_tensor_api.ir_generator.scope_attr(
                    self.buffer.data, "gm_tensor_for_debug",
                    tvm.tir.call_extern("int32", "gm_tensor_info", self.name, self.dtype, *shape_var))
            if new_tensor_api.is_atomic_add and \
                    check_is_atomic_add_attr(self.original_shape) and self.dtype in ("float16", "float32"):
                shape_size = reduce_mul(self.original_shape)
                align_size = tvm_ceil_align(shape_size.get() * DTYPE_SIZE.get(self.dtype), 32) + 32
                init_value = self.init_value if self.init_value is not None else 0
                array = [self.access_ptr("r", extent=16), init_value, self.dtype, align_size]
                new_tensor_api.ir_generator.scope_attr(array, "pragma_atomic_info", 1)
            self.buffer_shape_dtype = self.buffer.shape[0].dtype
            self.source_loc = get_span()
        else:
            self.__new_tensor_scope_not_gm(new_tensor_api)

    def __new_tensor_scope_not_gm(self, new_tensor_api):
        """

        Parameters
        ----------
        new_tensor_api

        Returns
        -------
        None
        """
        if new_tensor_api.enable_buffer_reuse or new_tensor_api.start_addr:
            self.buffer_storage_id = Tensor.BUFFER_STORAGE_COUNT
            Tensor.BUFFER_STORAGE_COUNT += 1

        self.buffer = new_tensor_api.ir_generator.apply_for_new_alloc(
            (self.dtype, new_tensor_api.shape_size, new_tensor_api.scope),
            new_tensor_api.name, new_tensor_api.init_value, self.buffer_storage_id)
        self.buffer_shape_dtype = self.buffer.shape[0].dtype
        self.update_buffer_storage(new_tensor_api.ir_generator, new_tensor_api.enable_buffer_reuse,
                                   new_tensor_api.start_addr)
        new_tensor_api.ir_generator.code_buffer_manager.new_tensor(self)

    def _check_new_tensor_params_init_value(self, new_tensor_api):
        """
        check new_tensor init_value not None

        Parameters
        ----------
        new_tensor_api

        Returns
        -------
        size, init_value
        """
        TikCheckUtil.check_equality(
            self.is_workspace, False, "Tensor with init_value cannot be used as workspace")
        TikCheckUtil.check_equality(
            self.is_global_tensor, False, "Tensor with init_value cannot be used as global tensor")
        TikCheckUtil.check_not_equality(
            self.dtype, 'bool', "Tensor with init_value cannot be bool dtype")
        size = new_tensor_api.shape_size
        TikCheckUtil.check_equality(
            isinstance(size, int), True, "Tensor with init_value should be const size")
        init_value = new_tensor_api.init_value
        if not isinstance(init_value, (tuple, list)):
            init_value = [init_value] * size
        # flatten init_value
        init_value = list(flat_list(init_value))
        TikCheckUtil.check_le(
            size, 256 * 1024, "init_value at most support 256*1024 elements")
        TikCheckUtil.check_equality(
            len(init_value), size, "length of init_value(%d) should be equal to "
                                   "size of Tensor(%d)." % (len(init_value), size))
        if any(not isinstance(i, (int, float)) for i in init_value):
            TikCheckUtil.raise_error("init_value should be int/float.")

        return size, init_value

    def _get_shape(self):
        tmp_shape = []
        for i in self.dimensions:
            shape = self.calc_slice_length(i)
            tmp_shape.append(shape)
        return tmp_shape

    def __to_reg(self):
        """
        creat temp register
        Parameters
        ----------

        Returns
        -------
        return:the temp register
        """
        tmp_reg = self.ir_generator.scalar_(self.dtype)
        TikCheckUtil.check_is(self.is_single_point(), True, "single point sliced is false")
        tmp_reg.set_as(self)
        return tmp_reg

    def _get_last_tensor(self):
        """
        get the origin tensor
        Parameters
        ----------

        Returns
        -------
        return:the origin tensor
        """
        if self.last_tensor is not None:
            return self.last_tensor._get_last_tensor()
        else:
            return self


def get_addr_list(addr_list, tensor_object, mode, extent=None):
    """
    get tensor's address with mode, and put them in list
    Parameters
    ----------
    addr_list: the list store address we get
    tensor_object: tensor instance
    mode: w or r
    extent: extent of the addr_list

    Returns
    -------
    no return
    """
    if isinstance(tensor_object, Tensor):
        if extent is None:
            addr_list.append(tvm.tir.Cast("uint64", tensor_object.access_ptr(mode)) * tvm.const(1, "uint64"))
        else:
            addr_list.append(
                tvm.tir.Cast("uint64", tensor_object.access_ptr(mode, extent=extent)) * tvm.const(1, "uint64"))
    else:
        addr_list.append(tensor_object)
