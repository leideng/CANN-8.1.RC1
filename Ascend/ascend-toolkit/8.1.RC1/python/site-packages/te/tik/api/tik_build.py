#!/usr/bin/env python
# -*- coding:utf-8 -*-

"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_build.py
DESC:     tik build
CREATED:  2019-7-04 20:12:13
MODIFIED: 2019-8-06 15:04:45
"""
from collections import namedtuple
import os
import json
import numpy as np

from tbe import tvm
from tbe.tvm.runtime.cce_runtime import TIK_WORKSPACE_SIZE_LIST
from tbe.tvm.runtime.cce_runtime import TIK_ATOMIC_ADD_LIST
from tbe.tvm.runtime.cce_runtime import TIK_GLOBAL_TENSOR_LIST
from tbe.tvm.runtime.cce_runtime import TIK_MEM_STAMP_TENSOR_LIST

from tbe.common.context import get_context
from tbe.common.platform import scope_gm
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_ubuf
from tbe.common.platform import scope_preg
from tbe.common.platform import scope_vreg
from tbe.common.platform import scope_wreg
from tbe.common.buildcfg import get_current_build_config
from tbe.common.testing.run_model import IS_TENSOR
from tbe.common.testing.run_model import IS_INPUTSCALAR
from tbe.common.testing.run_model import IS_TENSOR_ADDR_LIST
from tbe.common.testing.run_model import IS_FT_TENSOR
from tbe.common.testing.run_model import IS_FT_SCALAR
from tbe.common.testing.run_model import run_model
from tbe.common.testing.run_model import IS_DESC_TENSOR

from tbe.tik.tik_lib import Expr
from tbe.tik.tik_lib import TikVectorApi
from tbe.tik.tik_lib import TikProposalApi
from tbe.tik.tik_lib import TikCubeApi
from tbe.tik.tik_lib import TikInner
from tbe.tik.tik_lib import TikSysControlApi
from tbe.tik.api.cube.tik_cube_api import TikCubeOpenApi
from tbe.tik.tik_lib.tik_build_ import build_cce
from tbe.tik.api.tik_vector_api import TikVectorApiv1
from tbe.tik.api.tik_data_operation_api import TikDataOpApiv1
from tbe.tik.api.tik_reduce_api import TikReduceApiv1
from tbe.tik.api.tik_cmp_api import TikCompareApiv1
from tbe.tik.api.tik_scalar_api import TikScalarApi
from tbe.tik.api.tik_scalar import InputScalar
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.api.tik_scalar_array import ScalarArray
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.api.tik_tensor_addr_list import TensorAddrList
from tbe.tik.api.tik_vector import Vector
from tbe.tik.api.tik_dprofile import Dprofile
from tbe.tik import debug
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.util import DTYPE_FOR_INPUT_SCALAR
from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.expr_bound_analyzer import NO_STAMP
from tbe.tik.common.expr_bound_analyzer import STAMP
from tbe.tik.common.expr_bound_analyzer import POSSIBLY_STAMP
from tbe.tik.common.tik_get_soc_name import TIK_SOC_INFO
from tbe.tik.common.tik_get_soc_name import get_block_size
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_check_util import ERROR_MSG_LEVEL
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_params import MAGIC_WORD_FOR_CORE_HEAD
from tbe.tik.tik_lib.tik_params import MAGIC_WORD_FOR_PRINT_HEAD
from tbe.tik.tik_lib.tik_params import PRINT_CORE_HEAD_BYTES
from tbe.tik.tik_lib.tik_params import EACH_PRINT_HEAD_BYTES
from tbe.tik.tik_lib.tik_params import MAX_PRINT_WORKSPACE_SIZE
from tbe.tik.tik_lib.tik_params import MIN_SINGLE_PRINT_LENGTH
from tbe.tik.tik_lib.tik_params import MAX_SINGLE_PRINT_LENGTH
from tbe.tik.tik_lib.tik_params import PRINT_WORKSPACE_DTYPE
from tbe.tik.tik_lib.tik_params import DEFAULT_PRINT_UB_BYTES
from tbe.tik.tik_lib.tik_params import MAX_PRINT_FORMAT_LENGTH
from tbe.tik.tik_lib.tik_params import MAX_PRINT_ARG_LENGTH
from tbe.tik.tik_lib.tik_params import FAKE_TENSOR_NAME
from tbe.tik.tik_lib.tik_params import MAX_INPUT_OUTPUT_NUM
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE_FOR_NANO
from tbe.tik.tik_lib.tik_lru_cache_clear_ import clear_lru_cache_info
from tbe.tik.tik_lib.tik_build_ import check_extend_params_feed_dict
from tbe.tik.tik_lib.tik_h_vec_api.tik_1_5_api_ import TikHVecApi
from tbe.tik.tik_lib.tik_data_move_api.tik_data_move_api_ import TikDataMoveOpApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_single_api import SingleVectorApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_multi_api import MultiOpsApi
from tbe.tik.tik_lib.tik_vector_api.tik_scalar_multis_api import ScalarMultisApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_fills_api import VectorFillsApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_compare_api import VcmpVectorApi
from tbe.tik.tik_lib.tik_vector_api.tik_scatter_gather_api import ScatterGatherOpsApi
from tbe.tik.tik_lib.tik_vector_api.tik_whole_reduce_api import ReduceOpsApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_compare_api import VreduceVectorApi
from tbe.tik.tik_lib.tik_vector_new_api.tik_v210_api import TikVectorNewApi
from tbe.tik.tik_lib.tik_vector_new_api.tik_preg_api import TikPregApi
from tbe.tik.tik_lib.tik_vector_new_api.tik_vector_special_api import TikSpecApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_identify_api import ProposalApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_vdp_api import VdpOpsApi
from tbe.tik.tik_lib.tik_mmad_convert_api.tik_mmad_convert_api_ import TikMMadConvertApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_spr_api import SPROpsApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_conv_api_ import VconvVectorApi
from tbe.tik.tik_lib.tik_build_check import check_build_cce_inputs_outpus
from tbe.tik.tik_lib.tik_build_check import check_start_profiling_feed_dict
from tbe.tik.tik_lib.tik_build_check import check_scalar_value_map
from tbe.tik.tik_lib.tik_build_check import check_input_tensor_type_match
from tbe.tik.tik_lib.tik_build_check import check_input_scalar_type_match
from tbe.tik.tik_lib.tik_build_check import check_input_tensor_addr_list_type_match

_BLOCK_SYNC_WAIT = "pragma_multicore_sync_wait_before"
_BLOCK_SYNC_SET = "pragma_multicore_sync_set_before"


class Tik(TikCubeOpenApi, TikVectorApiv1, TikDataOpApiv1, TikReduceApiv1, TikCompareApiv1, TikDataMoveOpApi,
          TikScalarApi, TikVectorApi, TikCubeApi, ProposalApi, TikProposalApi, TikSysControlApi, VreduceVectorApi,
          TikInner, TikMMadConvertApi, TikVectorNewApi, TikPregApi, TikSpecApi,
          TikHVecApi, SingleVectorApi, MultiOpsApi, ScalarMultisApi, VectorFillsApi, VcmpVectorApi, ReduceOpsApi,
          ScatterGatherOpsApi, VdpOpsApi, SPROpsApi, VconvVectorApi):
    """
    tik instance class
    """
    GLOBAL_SCALAR_COUNT = 0

    build_cce_api = namedtuple('BuildCce', ['kernel_name', 'tik_instance', 'inputs', 'outputs', 'global_scalar_list',
                                            'workspace_tensor_list', 'global_tensor_list', 'config', 'flowtable_tmp',
                                            'scalar_value_map', 'extend_params'])
    tensor_api = namedtuple('Tensor', ['dtype', 'shape', 'scope', 'name', 'enable_buffer_reuse', 'no_reuse_list',
                                       'reuse_list', 'is_workspace', 'is_atomic_add', 'max_mem_size', 'init_value',
                                       'is_global_tensor', 'start_addr'])

    def __init__(self, profiling=None, disable_debug=True, err_msg_level=0, block_size=ONE_BLK_SIZE):
        """
        Creates a TIK DSL container by passing a tik.Dprofile instance.

        Parameters
        ----------
        profiling:  Configuration information of the Ascend AI processor. Dprofile is supported.
        disable_debug:  An optional bool for disabling the debug function.
        err_msg_level:  An optional int for specifying the level of error messages to be printed.

        Returns
        -------
        Instance of class TIK
        """
        super(Tik, self).__init__()
        self._init_and_check_soc_info(block_size)
        tvm.tir.util.location_init()
        self.d_profiling = Dprofile()
        self.global_scalar_list = []
        # for storing global information
        self.global_dict = {}
        self.code_buffer_manager.inject_dprofile(self.d_profiling)

        # record for profiling
        self.last_output_path = None
        self.last_kernel_name = None
        self.last_outputs = []
        self.last_enable_l2 = True
        self.last_inputs = []
        self.build_done = False
        self.last_flowtable = []
        # BuildCCE default mode
        # if true then not check life cycle
        self._is_building_cce = False

        # debug mode for checking life cycle
        # True: in debug mode; False: not debug mode
        # if true then not check life cycle
        self._is_in_debug = False

        # debug status, if false, then can not debug
        self.debug_disabled_ = disable_debug

        self.tikdb = debug.Tikdb(self, disable_debug)
        self.context = self.tikdb.context

        # use this list to collect reuse and no_reuse relationship
        self.buffer_reuse_list = []
        self.buffer_no_reuse_list = []
        self._buffer_reuse_dict = {}
        self.buffer_buffer_id_dict = {}

        # use this list to collect Tensor start addr
        self.start_addr_dict = {}

        # use this list to collect workspace list.
        self._workspace_tensor_list = []
        self._workspace_tensor_name_set = set()

        # use this list to collect global tensor info
        self._global_tensor_list = []
        self._global_tensor_name_set = set()

        # use this set to collect init value gm tensor
        self._init_gm_tensor_set = set()

        self._print_workspace_size_in_bytes = 128 * 1024 * 1024
        self._print_workspace = None
        self._print_block_ub = None

        self._print_core_num = 1
        self._print_ub_1024 = None
        self._print_ub_512 = None
        self._print_ub_256 = None
        self._is_init_print_workspace = False
        self.discard_print_count = None
        self.first_print_offset = None
        self.last_print_offset = None
        self.print_count = None
        self.current_ub_offset = 0
        self._need_dump_print_wksp = False
        self._print_wksp_index = 0
        # use to save the valid index range of tensoraddrlist.
        self.tensor_addr_list_valid_idx = {}

        # for check whether the class is Tik class obj
        self.is_tik_instance = True
        self._sync_flag = False
        self._init_and_check(err_msg_level)
        # collecting all input scalar
        self.input_scalar_set = set()

    @staticmethod
    def _init_and_check_soc_info(block_size):
        TIK_SOC_INFO.is_compatible = (block_size == ONE_BLK_SIZE)
        if TikSocManager.is_nano_soc():
            TikCheckUtil.check_var_in_list(block_size, [ONE_BLK_SIZE, ONE_BLK_SIZE_FOR_NANO],
                                           "block size should be 16 or 32 for Ascend035")
        else:
            TikCheckUtil.check_equality(block_size, get_block_size(), "block size should be 32")

    @staticmethod
    def _init_and_check(err_msg_level):
        # here clear the lru_cache info for some exception exit
        clear_lru_cache_info()

        TIK_WORKSPACE_SIZE_LIST.local_list = []
        TIK_GLOBAL_TENSOR_LIST.local_list = []
        TIK_MEM_STAMP_TENSOR_LIST.local_list = []

        # use to save all atomic add list
        TIK_ATOMIC_ADD_LIST.local_list = []

        # set error message level
        if isinstance(err_msg_level, int):
            TikCheckUtil.check_var_in_list(
                err_msg_level, [0, 1], "err_msg_level only support 0 and 1, input value is %s." % err_msg_level)
        ERROR_MSG_LEVEL.err_msg_level = err_msg_level

    @staticmethod
    def _get_in_and_out_tmp(inputs, outputs, flowtable):
        """
        get inputs and outputs
        """
        if not isinstance(inputs, (list, tuple)):
            inputs_tmp = [inputs]
        else:
            inputs_tmp = list(inputs)
        if not isinstance(outputs, (list, tuple)):
            outputs_tmp = [outputs]
        else:
            outputs_tmp = list(outputs)
        if flowtable is not None:
            if not isinstance(flowtable, (list, tuple)):
                flowtable_tmp = [flowtable]
            else:
                flowtable_tmp = list(flowtable)
        else:
            flowtable_tmp = []
        return inputs_tmp, outputs_tmp, flowtable_tmp

    @staticmethod
    def _reset_local_list():
        TIK_ATOMIC_ADD_LIST.local_list = []
        TIK_WORKSPACE_SIZE_LIST.local_list = []
        TIK_GLOBAL_TENSOR_LIST.local_list = []
        TIK_MEM_STAMP_TENSOR_LIST.local_list = []

    @source_info_decorator()
    def BuildCCE(self, kernel_name, inputs, outputs, output_files_path=None, enable_l2=False, config=None,
                 flowtable=None, evaluates=None, extend_params=None):
        """
        Generates DSL defined on the target machine
        Parameters
        ----------
        kernel_name: A string; Specifies the names of the generated binary
        inputs: A list or tuple of Tensor or InputScalar or TensorAddrList whose scope is scope_gm.
        outputs: A list or tuple of Tensor or TensorAddrList whose scope is scope_gm.
        output_files_path: string specifying the path of the generated files after build.
        enable_l2: A bool specifying whether to enable L2 buffer enable. Defaults to False.
        config: A dictionary including a key string and its value, used to configure the operator build properties.
        flowtable: A list or tuple of InputScalars.
        evaluates: Debugging parameters,
        -          the user assigns values to the defined Scalar variables during the compilation phase.
        extend_params: A dictionary including a key string and its value,
        -              used to configure the operator build properties.

        Returns
        -------
        no return
        """
        # @cond
        self._is_building_cce = True
        # @endcond
        if config is None:
            config = {}
        TikCheckUtil.check_name_str_valid(kernel_name)
        if output_files_path is not None:
            output_files_path = os.path.realpath(output_files_path)
            if not os.path.exists(output_files_path):
                os.makedirs(output_files_path)
            config['kernel_meta_parent_dir'] = output_files_path
        else:
            output_files_path = get_current_build_config('kernel_meta_parent_dir')

        self.d_profiling.registe()
        inputs_tmp, outputs_tmp, flowtable_tmp = self._get_in_and_out_tmp(inputs, outputs, flowtable)
        with self.context.freeze():
            if not outputs_tmp:
                outputs_tmp = [Tensor(self, "float16", shape=(1,), scope=scope_gm, name=FAKE_TENSOR_NAME)]

        # before build cce, set TIK_WORKSPACE_SIZE_LIST
        self._set_workspace_list()
        # add global tensor to TIK_GLOBAL_TENSOR_LIST
        self._set_global_tensor_list()
        # add memory stamp tensor to TIK_MEM_STAMP_TENSOR_LIST
        self._set_mem_stamp_tensor_list(outputs_tmp)

        # check inputs and outputs num either less than 64
        self._check_in_and_out_num(inputs_tmp, outputs_tmp, flowtable_tmp)

        if self.tik_version != "1.0":
            self._check_left_size_of_ub()

        # check config is not none
        if config is not None:
            TikCheckUtil.check_type_match(config, dict, "config should be dict for BuildCCE")
            for i in config:
                TikCheckUtil.check_type_match(i, str, "config's key should be string")
        # check evaluates if is not none
        if evaluates is not None:
            check_scalar_value_map(evaluates)

        # check extend_params if is not none
        if extend_params is not None:
            TikCheckUtil.check_type_match(extend_params, dict, "extend_params should be dict for BuildCCE")
            check_extend_params_feed_dict(extend_params)

        # check if printf used out the multi-for range.
        if self._has_printf_out_multi_for and self.has_multi_block:
            TikCheckUtil.raise_error("can't use printf out of multi-core for_range!")
        # we do rewrite the core print workspace
        self._move_new_core_head_to_workspace()

        build_cce_obj = Tik.build_cce_api(kernel_name, self, inputs_tmp, outputs_tmp, self.global_scalar_list,
                                          self._workspace_tensor_list, self._global_tensor_list, config,
                                          flowtable_tmp, evaluates, extend_params)
        build_cce(build_cce_obj)

        # @cond
        self.last_outputs = outputs_tmp
        self.last_inputs = inputs_tmp
        self.last_output_path = os.path.join(output_files_path, "./kernel_meta/")
        self.last_kernel_name = kernel_name
        self.last_enable_l2 = enable_l2
        self.build_done = True
        self.last_flowtable = flowtable_tmp
        self.input_scalar_set.clear()
        self._reset_local_list()
        # reset class Tensor global value
        Tensor.BUFFER_STORAGE_COUNT = 0
        Tensor.GLOBAL_VARIABLE_LINK = False
        # clear lru cache info
        clear_lru_cache_info()
        # @endcond

    @source_info_decorator()
    def Tensor(self, dtype, shape, scope, name, enable_buffer_reuse=False, no_reuse_list=None, reuse_list=None,
               is_workspace=False, is_atomic_add=False, max_mem_size=None, init_value=None, is_global_tensor=False,
               start_addr=None):
        """
        Defines a Tensor variable.

        Parameters
        ----------
        dtype : Data type of the Tensor object.
        shape : A list or tuple of ints, specifying the shape of the Tensor object.
        scope : Buffer scope of the Tensor object, that is, buffer space where the Tensor object is located:
        -       scope_cbuf: L1 Buffer
        -       scope_cbuf_out: L1OUT Buffer
        -       scope_ubuf: Unified Buffer (UB)
        -       scope_gm: Global Memory (GM)
        name : A string specifying the name of the Tensor object.
        is_workspace : A bool. Defaults to False. If set to True, the current tensor is used for storing intermediate
        data only.
        is_atomic_add : A bool. Defaults to False. This argument does not take effect.
        enable_buffer_reuse: An internal optional parameters
        no_reuse_list: An internal optional parameters
        reuse_list : An internal optional parameters
        max_mem_size : An intImm. Max memory size for dynamic shape Tensor.
        init_value : A list of Imm. Init value for GM Tensor.
        is_global_tensor: Tensor as the global address space
        start_addr: tensor start addr
        Returns
        -------
        Tensor instance
        """
        tensor_obj = Tik.tensor_api(dtype, shape, scope, name, enable_buffer_reuse=enable_buffer_reuse,
                                    no_reuse_list=no_reuse_list, reuse_list=reuse_list,
                                    is_workspace=is_workspace, is_atomic_add=is_atomic_add,
                                    max_mem_size=max_mem_size, init_value=init_value,
                                    is_global_tensor=is_global_tensor, start_addr=start_addr)
        return self.tensor_(tensor_obj)

    @source_info_decorator()
    def TensorAddrList(self, size, scope, name):
        """
        Defines a TensorAddrList func.
        """
        return TensorAddrList(self, size, scope, name)

    @source_info_decorator()
    def block_barrier(self, sync_workspace):
        """
        Generate IR Attr for block synchronization.
        """
        self.check_barrier(sync_workspace, self.for_blk_num, self._block_id)
        iter_node = tvm.call_extern("uint64", "iter_var")
        load_node = tvm.tir.Load(sync_workspace.dtype, sync_workspace.buffer.data, 0)
        self.scope_attr(iter_node, _BLOCK_SYNC_SET, value=load_node)
        self.scope_attr(iter_node, _BLOCK_SYNC_WAIT, value=load_node)
        self._sync_flag = True
        self._block_barrier(self.for_blk_num)

    @source_info_decorator()
    def Scalar(self, dtype="int64", name="reg_buf", init_value=None):
        """
        Defines a Scalar variable.

        Parameters
        ----------
        dtype : Data type of the Scalar object.
        name : A string specifying the name of the Scalar object.d
        init_value : Initial value

        Returns
        -------
        Scalar instance
        """
        return self.scalar_(dtype=dtype, name=name, init_value=init_value)

    @source_info_decorator()
    def ScalarArray(self, dtype="int64", length=1, name="reg_buf", init_value=None):
        """
        Defines a Scalar Array variable.
        Parameters
        ----------
        dtype: Data type of the Scalar Array object.
        length: Length of Scalar Array. Must be int data type.
        name: A string specifying the name of the Scalar Array object.
        init_value: Initial value;

        Returns
        -------
        Scalar Array instance
        """
        return self.scalar_array_(dtype=dtype, length=length, name=name, init_value=init_value)

    @source_info_decorator()
    def InputScalar(self, dtype="int64", name="input_scalar"):
        """
        Defines an InputScalar variable.

        Parameters
        ----------
        dtype : Data type of the InputScalar object.
        name : A string specifying the name of the InputScalar object.
        """
        TikCheckUtil.check_name_str_valid(name)
        TikCheckUtil.check_type_match(
            dtype, str, "dtype should be str, but get " + str(type(dtype)))
        TikCheckUtil.check_var_in_list(
            dtype, DTYPE_FOR_INPUT_SCALAR,
            "dtype only support: " + " ".join(DTYPE_FOR_INPUT_SCALAR) + ", but get " + dtype)
        return InputScalar(self, dtype=dtype, name=name)

    def _set_mem_stamp_tensor_list(self, output_gm):
        """
        save memory stamp dst gm to TIK_MEM_STAMP_TENSOR_LIST
        Parameters
        ----------
        output_gm: output gm list

        Returns None
        -------

        """
        mem_stamp_dic = {}
        total_stamp = self.mem_stamp + self.h_data_move_mem_stamp
        for name, value in total_stamp:
            cur_exist_val = mem_stamp_dic.get(name)
            if cur_exist_val == POSSIBLY_STAMP and value == STAMP:
                mem_stamp_dic[name] = value
            elif cur_exist_val is None:
                mem_stamp_dic[name] = value

        for out in output_gm:
            if isinstance(out, Tensor) and out.original_name != FAKE_TENSOR_NAME:
                value = mem_stamp_dic.get(out.original_name, NO_STAMP)
                TIK_MEM_STAMP_TENSOR_LIST.local_list.append(value)

    def _update_buffer_use_list(self, tmp_tensor, name_list, reuse_type):
        """
        update buffer_reuse_list and buffer_no_reuse_list
        according to name_list

        Parameters
        ----------
        tmp_tensor : tensor defined
        name_list : list of reuse tensor or no_reuse tensor
        reuse_type : string, "reuse" or "no_reuse"

        Returns
        -------
        None
        """
        id_list = []
        for tensor_name in name_list:
            if tensor_name is None:
                if len(name_list) == 1 and reuse_type == "no_reuse":
                    break
                TikCheckUtil.raise_error(
                    "%s_list not support, input value is %s" % (reuse_type, name_list))
            if isinstance(tensor_name, str) and tensor_name in self._buffer_reuse_dict:
                id_list.append(self._buffer_reuse_dict.get(tensor_name))
            else:
                TikCheckUtil.raise_error(
                    "%s_list contains unspecified reuse tensor, please check enable_buffer_reuse param" % reuse_type)
        id_list.append(tmp_tensor.buffer_storage_id)
        if reuse_type == "reuse":
            if id_list not in self.buffer_reuse_list:
                self._merge_reuse_list(id_list)
        else:
            if id_list not in self.buffer_no_reuse_list:
                self._merge_no_reuse_list(id_list)

    def _set_workspace_list(self):
        """
        set workspace list
        """
        for index, tmp_tensor in enumerate(self._workspace_tensor_list):
            TIK_WORKSPACE_SIZE_LIST.local_list.append(tmp_tensor.buffer_size)
            if tmp_tensor.name == "_print_workspace":
                self._print_wksp_index = index
                self._need_dump_print_wksp = True

    def _set_global_tensor_list(self):
        """
        add global_tensor info to TIK_TENSOR_SIZE_LIST
        :self: Tensor instance
        :type self: Tensor
        :return: no return
        :rtype: None
        """
        for global_tensor in self._global_tensor_list:
            TIK_GLOBAL_TENSOR_LIST.local_list.append((global_tensor.name, global_tensor.buffer_size))

    def _check_in_and_out_num(self, inputs_tmp, outputs_tmp, flowtable_tmp):
        """
        check inputs and outputs nums
        """
        if len(inputs_tmp) + len(flowtable_tmp) + len(outputs_tmp) + \
                len(self._workspace_tensor_list) + len(self._global_tensor_list) > MAX_INPUT_OUTPUT_NUM:
            TikCheckUtil.raise_error(
                "Input and (output + workspace + global_tensor) num should <= " +
                str(MAX_INPUT_OUTPUT_NUM) + "!")
        input_output_name_set, non_flowtable_set =\
            check_build_cce_inputs_outpus(inputs_tmp, outputs_tmp, flowtable_tmp)

        self._check_non_inputs_outputs_tensor(input_output_name_set, non_flowtable_set)

    def _check_non_inputs_outputs_tensor(self, input_output_name_set, non_flowtable_set):
        """
        check whether workspace tensor global tensor and init value tensor are in input output flowtable
        The workspace tensor cannot be placed in the inputs, outputs, flowtable list.
        The global tensor cannot be placed in the inputs, outputs, flowtable list.
        The init value tensor cannot be placed in the inputs, outputs list.
        Parameters
        ----------
        input_output_name_set: the set contains inputs outputs flowtable
        non_flowtable_set: the set contains inputs outputs

        Returns None
        -------

        """
        TikCheckUtil.check_equality(len(input_output_name_set & self._workspace_tensor_name_set),
                                    0, "Workspace's name is used in input or output!")
        TikCheckUtil.check_equality(len(input_output_name_set & self._global_tensor_name_set),
                                    0, "global tensor's name is used in input or output!")
        TikCheckUtil.check_equality(len(non_flowtable_set & self._init_gm_tensor_set),
                                    0, "init value tensor's name is used in input or output!")

    def _check_left_size_of_ub(self):
        """
        check left size of ub, if used tik1.5 API, need reserve 16KB for pass
        Returns
        -------
        None

        """
        reserved_size = 16 * 1024  # if used tik1.5 API, need reserve 16KB for pass
        avaiable_ub_buffer = Expr(self.code_buffer_manager.buffer_aviable()[scope_ubuf]).eval_value()

        if avaiable_ub_buffer is not None and reserved_size > avaiable_ub_buffer:
            total_buf_size = self.code_buffer_manager.total_buffer[scope_ubuf]
            TikCheckUtil.raise_error(
                "If used tik1.5 API, available scope_ubuf size is: %sB (%s-%s), but used: %sB" % (
                    str(total_buf_size - reserved_size), str(total_buf_size), str(reserved_size),
                    str(total_buf_size - avaiable_ub_buffer)))

    def _start_model_run(self, config, simulatorlog_path=None, outputs_shape=None, desc_tensor=None):
        """
        Parameters
        ----------
        simulatorlog_path: where to put log
        outputs_shape: when the output tensor is dynamic shape, a static shape should be put into outputs_shape in order
        desc_tensor: A list is used to specify a special tensor's name for save the gm tensor info.

        Returns
        -------
        no return
        """
        feed_dict, _, is_profiling, is_generate_cfg = config
        # because input is list or tuple so could use len
        TikCheckUtil.check_type_match(feed_dict, dict, "feed_dict should be dict")
        if self.build_done is False:
            TikCheckUtil.raise_error("BuildCCE must be called before StartProfiling!")
        output_path = self.last_output_path
        if output_path is None:
            output_path = 'kernel_meta'
        if outputs_shape is not None:
            TikCheckUtil.check_type_match(outputs_shape, (list, tuple), "outputs_shape should be list")
        output_args = []
        output_need_init_list = []
        _gen_output_spec(self.last_outputs, self._need_dump_print_wksp, outputs_shape,
                         output_args, output_need_init_list)

        # once last_inputs not empty, we should check key is same
        check_start_profiling_feed_dict(self.last_inputs, feed_dict, self.last_flowtable)
        # check dtype and shape should be same
        input_args = []
        _gen_feed_data(self.last_inputs, feed_dict, input_args, desc_tensor)
        _gen_flowtable_data(self.last_flowtable, feed_dict, input_args)

        config_dict = {
            "kernel_path": output_path,
            "simulatorlog_path": simulatorlog_path,
            "need_dump_print_wksp": self._need_dump_print_wksp,
            "print_wksp_index": self._print_wksp_index,
            "is_printf_to_screen": self.is_printf_to_screen,
            "printf_file_path": self.printf_file_path,
            "output_need_init_list": output_need_init_list
        }
        if is_generate_cfg:
            _gen_cfg(self.last_kernel_name, input_args, output_args, config_dict)
            return []
        else:
            return run_model(self.last_kernel_name, is_profiling, input_args, output_args,
                             config=config_dict)

    def _move_new_core_head_to_workspace(self):
        """
        write the multi core head to workspace.

        Returns
        -------
        no return
        """
        if not self._has_done_printf:
            return
        if not self._print_block_ub.available:
            self._print_block_ub = self.Tensor(dtype="uint8", shape=(DEFAULT_PRINT_UB_BYTES,),
                                               scope=scope_ubuf, name="_print_block_ub")
        # move new head to workspace
        shape_of_each_block = self._print_workspace.size // self._print_core_num
        with self.for_range(0, self._print_core_num) as block_id:
            start_offset_of_each_block = shape_of_each_block * block_id
            print_ub_in_int64 = self._print_block_ub.reinterpret_cast_to("int64")
            self._print_block_ub.reinterpret_cast_to("uint16")[0].set_as(Expr(MAGIC_WORD_FOR_CORE_HEAD))
            self._print_block_ub.reinterpret_cast_to("uint16")[1].set_as(Expr(block_id))
            self._print_block_ub.reinterpret_cast_to("uint16")[2].set_as(Expr(self._print_core_num))
            # used to reserve to 8 bytes align
            self._print_block_ub.reinterpret_cast_to("uint16")[3].set_as(Expr(0))
            # Total_print_buffer_len, means bytes ot this length
            print_ub_in_int64[1].set_as(Expr(shape_of_each_block * DTYPE_SIZE.get(self._print_workspace.dtype),
                                             dtype="int64"))

            print_ub_in_int64[2].set_as(Expr(self.discard_print_count, dtype="int64"))
            print_ub_in_int64[3].set_as(Expr(self.single_print_length, dtype="int64"))
            print_ub_in_int64[4].set_as(Expr(self.first_print_offset, dtype="int64"))
            print_ub_in_int64[5].set_as(Expr(self.last_print_offset, dtype="int64"))
            print_ub_in_int64[6].set_as(Expr(self.print_count, dtype="int64"))
            print_ub_in_int64[7].set_as(Expr(0, dtype="uint64"))
            # 610l data move call data move pad, src/dst dtype of the data move pad should be equals
            if self._print_workspace.dtype != print_ub_in_int64.dtype:
                print_ub_in_int64 = print_ub_in_int64.reinterpret_cast_to(self._print_workspace.dtype)
            self.data_move(self._print_workspace[start_offset_of_each_block:], print_ub_in_int64, 0, 1, 2, 0, 0)

    def _modify_last_print_offset(self, total_bytes_for_each):
        """
        use to modify last_print_offset, first_print_offset
        Parameters
        ----------
        total_bytes_for_each:

        Returns
        -------
        no return
        """
        self.print_count.set_as(self.print_count + 1)
        # here means we will put scalar_last_print_offset to first.
        with self.if_scope(PRINT_CORE_HEAD_BYTES + self.last_print_offset + self.single_print_length >
                           total_bytes_for_each):
            # rewrite to first.
            self.last_print_offset.set_as(self.single_print_length)
            self.discard_print_count.set_as(self.discard_print_count + 1)
            # means we cloud only put one single_print_max_length
            with self.if_scope(self.single_print_length * 2 + PRINT_CORE_HEAD_BYTES > total_bytes_for_each):
                self.first_print_offset.set_as(Expr(0))
            with self.else_scope():
                # here first_print_offset is same as last_print_offset
                self.first_print_offset.set_as(self.single_print_length)
        with self.else_scope():
            #  1. already has discard means first already moves
            self.last_print_offset.set_as(self.last_print_offset + self.single_print_length)
            with self.if_scope(self.discard_print_count > 0):
                self.first_print_offset.set_as(self.first_print_offset + self.single_print_length)
                self.discard_print_count.set_as(self.discard_print_count + 1)

    def _flush_remain_ub_to_workspace(self, print_start_offset,
                                      single_print_end_offset):
        if isinstance(self.current_ub_offset, int) and isinstance(
                self._print_bytes_consumed, int):
            if self.current_ub_offset > 0:
                self.data_move(
                    self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                    self._print_block_ub, 0, 1,
                    ceil_div(self.current_ub_offset, 32), 0, 0)
            return
        with self.if_scope(self.current_ub_offset > 0):
            self.data_move(
                self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                self._print_block_ub, 0, 1,
                self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed.set_as(single_print_end_offset)
            self.current_ub_offset.set_as(0)

    def _gen_ub_for_printf(self, scope, dtype):
        """
        generate cache ub for store data from l0c/vector.

        Parameters
        ----------
        scope: scope of printf value, l0c/vector
        dtype: dtype of printf value

        Returns
        -------
        store_cache ub
        """
        # here 1024 bytes
        if self._print_ub_1024 is not None and self._print_ub_1024.available:
            return self._print_ub_1024
        # b32 for scope_cc
        if scope == scope_cc and DTYPE_SIZE[dtype] == 4:
            self._print_ub_1024 = self.Tensor(dtype="uint8", shape=(1024,),
                                              scope=scope_ubuf, name="_print_ub_1024")
            return self._print_ub_1024

        # here 512 bytes
        if self._print_ub_512 is not None and self._print_ub_512.available:
            return self._print_ub_512
        # b16 for scope_cc
        if scope == scope_cc and DTYPE_SIZE[dtype] == 2:
            self._print_ub_512 = self.Tensor(dtype="uint8", shape=(512,),
                                             scope=scope_ubuf, name="_print_ub_512")
            return self._print_ub_512

        # here 256 bytes
        if self._print_ub_256 is not None and self._print_ub_256.available:
            return self._print_ub_256
        # vreg, wreg
        if scope == scope_vreg or scope == scope_wreg or scope == scope_preg:
            self._print_ub_256 = self.Tensor(dtype="uint8", shape=(256,),
                                             scope=scope_ubuf, name="_print_ub_256")
            return self._print_ub_256

        # other case
        self._print_ub_1024 = self.Tensor(dtype="uint8", shape=(1024,),
                                            scope=scope_ubuf, name="_print_ub_1024")
        return self._print_ub_1024

    def _move_print_value_to_workspace(self, print_start_offset, value, dtype):
        dtype_size = DTYPE_SIZE[dtype]
        with self.if_scope(self.current_ub_offset + dtype_size > 256):
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1,
                           self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed.set_as(self._print_bytes_consumed + self.current_ub_offset)
            self.current_ub_offset.set_as(0)
        self._print_block_ub.reinterpret_cast_to(
            dtype)[self.current_ub_offset // dtype_size].set_as(value)
        self.current_ub_offset.set_as(self.current_ub_offset + dtype_size)

    def _move_print_value(self, print_start_offset, value, offset=0):
        dtype_size = DTYPE_SIZE[value.dtype]
        with self.if_scope(self.current_ub_offset + 32 > 256):
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed.set_as(self._print_bytes_consumed + 256)
            self.current_ub_offset.set_as(0)
        self.data_move(self._print_block_ub.reinterpret_cast_to(value.dtype)[self.current_ub_offset // dtype_size],
                       value[offset], 0, 1, 1, 0, 0)
        self.current_ub_offset.set_as(self.current_ub_offset + 32)

    def _move_print_ub_without_scalar(self, print_start_offset, value, dtype):
        dtype_size = DTYPE_SIZE[dtype]
        if self.current_ub_offset + dtype_size > 256:
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed += self.current_ub_offset
            self.current_ub_offset = 0
        self._print_block_ub.reinterpret_cast_to(dtype)[self.current_ub_offset // dtype_size].set_as(value)
        self.current_ub_offset += dtype_size

    def _move_print_value_to_workspace_in_imm(self, print_start_offset, value, offset=0, is_vector=False):
        dtype_size = DTYPE_SIZE[value.dtype]
        if self.current_ub_offset + 32 > 256:
            self.data_move(self._print_workspace[print_start_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1,
                           self.current_ub_offset // 32, 0, 0)
            self._print_bytes_consumed += self.current_ub_offset
            self.current_ub_offset = 0
        if not is_vector:
            self.data_move(self._print_block_ub.reinterpret_cast_to(value.dtype)[self.current_ub_offset // dtype_size:],
                           value[offset:], 0, 1, 1, 0, 0)
        else:
            tmp_vector = self.Vector(dtype="bool")
            self.vector_load(tmp_vector, value[offset:].reinterpret_cast_to("uint8"))
            self.vector_store(self._print_block_ub[self.current_ub_offset], tmp_vector)
        self.current_ub_offset += 32

    def _check_printf_args(self, format_string, args):
        # check formatstring is string
        TikCheckUtil.check_type_match(format_string, str, "format's type should be str.")
        max_string_length = min(self.single_print_length - EACH_PRINT_HEAD_BYTES, MAX_PRINT_FORMAT_LENGTH)
        TikCheckUtil.check_in_range_by_dtype(len(format_string), msg="length of format should be in [1, %d]"
                                             % max_string_length, var_range=[1, max_string_length])
        TikCheckUtil.check_le(len(args), MAX_PRINT_ARG_LENGTH, "arg num should be in [0, %d]" % MAX_PRINT_ARG_LENGTH)
        for i in args:
            TikCheckUtil.check_type_match(i, (Expr, Scalar, ScalarArray, Tensor, Vector),
                                          "arg can only be Expr, Scalar, ScalarArray, Tensor but get %s" % (type(i)))
            if isinstance(i, Tensor):
                TikCheckUtil.check_var_in_list(i.scope, [scope_ubuf, scope_gm, scope_cbuf, scope_cc],
                                               "Tensor in arg can only be UB, GM, L1, L1OUT, but get " + i.scope)

    def _init_workspace_in_printf(self):
        if not self._is_init_print_workspace:
            self.discard_print_count = self.global_scalar(dtype="int64", init_value=0)
            self.first_print_offset = self.global_scalar(dtype="int64", init_value=0)
            self.last_print_offset = self.global_scalar(dtype="int64", init_value=0)
            self.print_count = self.global_scalar(dtype="int64", init_value=0)
            # at least set the print_workspace.
            if self._print_workspace is None:
                shape_of_print_workspace = self._print_workspace_size_in_bytes // DTYPE_SIZE[PRINT_WORKSPACE_DTYPE]
                self._print_workspace = self.Tensor(dtype=PRINT_WORKSPACE_DTYPE, shape=(shape_of_print_workspace,),
                                                    scope=scope_gm, name="_print_workspace", is_workspace=True)
            if self._print_block_ub is None or not self._print_block_ub.available:
                self._print_block_ub = self.Tensor(dtype="uint8", shape=(DEFAULT_PRINT_UB_BYTES,),
                                                   scope=scope_ubuf, name="_print_block_ub")
            if self.for_blk_num > self._print_core_num:
                self._print_core_num = self.for_blk_num
            TikCheckUtil.check_equality(self._print_workspace_size_in_bytes % self._print_core_num, 0,
                                        "print_workspace_size is indivisible by block_dim")
            self._is_init_print_workspace = True
        if not self._print_block_ub.available:
            self._print_block_ub = self.Tensor(dtype="uint8", shape=(DEFAULT_PRINT_UB_BYTES,),
                                               scope=scope_ubuf, name="_print_block_ub")

    def _check_printf_size_args(self, print_workspace_size, single_print_length):
        # check print_workspace_size
        TikCheckUtil.check_type_match(print_workspace_size, int, "print_workspace_size can only be int")
        workspace_size_range = [1, MAX_PRINT_WORKSPACE_SIZE]
        TikCheckUtil.check_in_range_by_dtype(
            print_workspace_size, msg="print_workspace_size should be range in [%d, %d], but get %d"
            % (workspace_size_range[0], workspace_size_range[1], print_workspace_size),
            var_range=[workspace_size_range[0], workspace_size_range[1]])

        self._print_workspace_size_in_bytes = print_workspace_size * 1024 * 1024
        # check print_workspace_size_in_bytes can be divided by block_dim
        TikCheckUtil.check_equality(self._print_workspace_size_in_bytes % self._print_core_num, 0,
                                    "print_workspace_size is indivisible by block_dim")

        # check single_print_max_length
        TikCheckUtil.check_type_match(single_print_length, int, "single_print_length can only be int.")
        TikCheckUtil.check_in_range_by_dtype(
            single_print_length, msg="single_print_length should be range in [%d, %d], but get %d"
            % (MIN_SINGLE_PRINT_LENGTH, MAX_SINGLE_PRINT_LENGTH, single_print_length),
            var_range=[MIN_SINGLE_PRINT_LENGTH, MAX_SINGLE_PRINT_LENGTH])

        # we will check single_print_max_length should be 32 aligns.
        TikCheckUtil.check_equality(single_print_length % 32, 0, "single_print_length should be 32 align.")

        TikCheckUtil.check_ge(
            self._print_workspace_size_in_bytes, self._print_core_num * (PRINT_CORE_HEAD_BYTES + single_print_length),
            "print_workspace_size should be >= block_dim*(PRINT_CORE_HEAD_BYTES<64bytes> + single_print_length)")

    def _move_print_head(self, args):
        block_id, string_length, real_data_length, args_length, single_print_offset = args
        self._print_block_ub.reinterpret_cast_to("uint16")[0].set_as(Expr(MAGIC_WORD_FOR_PRINT_HEAD))
        self._print_block_ub.reinterpret_cast_to("uint16")[1].set_as(Expr(block_id))
        self._print_block_ub.reinterpret_cast_to("uint32")[1].set_as(Expr(0))
        self._print_block_ub.reinterpret_cast_to("int64")[1].set_as(Expr(string_length))
        self._print_block_ub.reinterpret_cast_to("int64")[2].set_as(Expr(args_length))
        self._print_block_ub.reinterpret_cast_to("int64")[3].set_as(Expr(real_data_length))
        # move this to print head to workspace
        self.data_move(self._print_workspace[single_print_offset], self._print_block_ub, 0, 1, 1, 0, 0)

    def _move_format_string(self, format_string, align_string_bytes, current_workspace_offset):
        times_for_ub_size = len(format_string) // 256
        for i in range(times_for_ub_size):
            for string_index in range(256):
                self._print_block_ub[string_index].set_as(Expr(ord(format_string[string_index + i * 256])))
            self.data_move(self._print_workspace[current_workspace_offset + self._print_bytes_consumed:],
                           self._print_block_ub, 0, 1, 8, 0, 0)
            self._print_bytes_consumed += 256
        remain_format_bytes = len(format_string) % 256
        for i in range(remain_format_bytes):
            self._print_block_ub[i].set_as(Expr(ord(format_string[times_for_ub_size * 256 + i])))
        self.current_ub_offset = remain_format_bytes + align_string_bytes

    def _merge_reuse_list(self, id_list):
        for lst in self.buffer_reuse_list:
            for i in lst:
                if i in id_list:
                    id_list.remove(i)
                    lst.extend(id_list)
                    return
        self.buffer_reuse_list.append(id_list)

    def _merge_no_reuse_list(self, id_list):
        for lst in self.buffer_no_reuse_list:
            for i in lst:
                if i in id_list:
                    id_list.remove(i)
                    lst.extend(id_list)
                    return
        self.buffer_no_reuse_list.append(id_list)


def _gen_output_spec(last_outputs, _need_dump_print_wksp,
                     outputs_shape, output_args, output_need_init_list):
    """
    generate output space
    Parameters
    ----------
    last_outputs: output params in BuildCCE
    _need_dump_print_wksp: need dump print workspace
    outputs_shape: shape of output params
    output_args: output args information
    output_need_init_list: whether each output need to init, for atomic_add

    Returns
    -------
    output space
    """
    outputs_index = 0
    for t in last_outputs:
        if isinstance(t, Tensor):
            if t.name == FAKE_TENSOR_NAME and _need_dump_print_wksp:
                # means we should make no output here.
                break
            t_shape = t.shape
            if any(not isinstance(s, int) for s in t_shape):
                TikCheckUtil.check_equality(
                    outputs_shape is not None, True,
                    "when the output tensor is dynamic shape, a static shape should be put into outputs_shape in order")
                TikCheckUtil.check_le(outputs_index, len(outputs_shape) - 1, "elements in outputs_shape are not enough")
                t_shape = outputs_shape[outputs_index]
                outputs_index += 1
            output_args.append([t_shape, t.dtype])
            output_need_init_list.append(1 if t.is_atomic_add else 0)


def _gen_feed_data(last_inputs, feed_dict, input_args, desc_tensor=None):
    """
    generate feed data
    Parameters
    ----------
    last_inputs: input params in BuildCCE
    feed_dict: input params dict
    desc_tensor: A list is used to specify a special tensor's name for save the gm tensor info.

    Returns
    -------
    classified ctored input data
    """
    for t in last_inputs:
        if isinstance(t, Tensor):
            check_input_tensor_type_match(t, feed_dict, desc_tensor)
            # for description Tensor, first element is GM Tensor Addr, others is Gm Tensor info,
            # for example, shape, dtype, dims
            if desc_tensor and t.name in desc_tensor:
                input_args.append([IS_DESC_TENSOR, feed_dict[t.name]])
            else:
                input_args.append([IS_TENSOR, feed_dict[t.name]])
        elif isinstance(t, TensorAddrList):
            check_input_tensor_addr_list_type_match(t, feed_dict)
            input_args.append([IS_TENSOR_ADDR_LIST, feed_dict[t.name]])

    for t in last_inputs:
        if isinstance(t, InputScalar):
            check_input_scalar_type_match(t, feed_dict)
            tmp_np = np.array((feed_dict[t.name],)).astype(dtype=t.dtype)
            input_args.append([IS_INPUTSCALAR, tmp_np])


def _gen_flowtable_data(last_flowtable, feed_dict, input_args):
    """
    generate flowtable data
    Parameters
    ----------
    last_flowtable: flowtable params in BuildCCE
    feed_dict: input params in BuildCCE

    Returns
    -------
    no returnclassified stored input flowtable data
    """
    for t in last_flowtable:
        if isinstance(t, InputScalar):
            check_input_scalar_type_match(t, feed_dict)
            tmp_np = np.array((feed_dict[t.name],)).astype(dtype=t.dtype)
            input_args.append([IS_FT_SCALAR, tmp_np])
        elif isinstance(t, Tensor):
            check_input_tensor_type_match(t, feed_dict)
            input_args.append([IS_FT_TENSOR, feed_dict[t.name]])


def _gen_cfg(kernel_name, input_args, output_args, config):
    """
    generate cfg file
    Parameters
    ----------
    kernel_name: kernel name
    input_args: input params
    output_args: output params
    config: printf config

    Returns
    -------
    None
    """
    parent_dir = get_current_build_config('kernel_meta_parent_dir')
    input_type, input_dtype, input_size, input_data = [], [], [], []

    for index, (data_type, data) in zip(range(len(input_args)), input_args):
        input_type.append(data_type)
        input_dtype.append(DTYPE_FOR_INPUT_SCALAR.get(str(data.dtype)))
        input_size.append(np.ascontiguousarray(data).nbytes)
        if data_type == IS_INPUTSCALAR or data_type == IS_FT_SCALAR:
            input_data.append(str(data[0]))
        else:
            file_name = os.path.join("./kernel_meta/", kernel_name + "_input_" + str(index) + ".bin")
            input_data.append(file_name)
            abs_path = os.path.join(parent_dir, file_name)
            data.tofile(abs_path)
    inputs = {
        "num": len(input_args),
        "type": input_type,
        "dtype": input_dtype,
        "size": input_size,
        "data": input_data
    }
    output_size = []
    output_data = []
    for index, (shape, dtype) in zip(range(len(output_args)), output_args):
        output_size.append(reduce_mul(shape) * DTYPE_SIZE[dtype])
        output_data.append("kernel_meta/" + kernel_name + "_output_" + str(index) + ".bin")
    outputs = {
        "num": len(output_args),
        "size": output_size,
        "data": output_data,
    }
    io_dict = {"inputs": inputs, "outputs": outputs}
    # for mdc and dc, runtime have 2 mode: aicore and vectorcore.
    # for vectorcore, run_kernel need rtSetTSDevice(1)
    config["is_vector_core"] = TikSocManager.is_vector_core() and \
        (TikSocManager.is_v200_soc() or TikSocManager.is_v210_soc())
    io_dict.update(config)
    json_path = os.path.join(parent_dir, "./kernel_meta/", kernel_name + ".cfg")
    with os.fdopen(os.open(json_path, os.O_RDWR | os.O_CREAT), "w") as file:
        json.dump(io_dict, file, indent=4)
