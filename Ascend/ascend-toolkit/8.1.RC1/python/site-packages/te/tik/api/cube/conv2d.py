#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     conv2d.py
DESC:     provide tiling method
CREATED:  2020-4-23 21:12:13
MODIFIED: 2020-12-7 19:17:00
"""

import math
import itertools
from collections import namedtuple
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_cc
from tbe.common.platform import get_soc_spec
from tbe.common.platform.platform_info import api_check_support
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.common.util import reduce_mul
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_api_constants import DTYPE_MAP
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_params import gen_api_check_statement
from tbe.tik.tik_lib.tik_params import PADDING_LEFT_IDX
from tbe.tik.tik_lib.tik_params import INT8_MIN
from tbe.tik.tik_lib.tik_params import INT8_MAX
from tbe.tik.tik_lib.tik_params import UINT_MIN
from tbe.tik.tik_lib.tik_params import UINT8_MAX
from tbe.tik.tik_lib.tik_params import MIN_DILATION
from tbe.tik.tik_lib.tik_params import MAX_DILATION
from tbe.tik.tik_lib.tik_params import MIN_STRIDE
from tbe.tik.tik_lib.tik_params import MAX_STRIDE
from tbe.tik.tik_lib.tik_params import MAX_PADDING
from tbe.tik.tik_lib.tik_params import MIN_C1_VALUE
from tbe.tik.tik_lib.tik_params import MAX_C1_VALUE
from tbe.tik.tik_lib.tik_params import BYTE_PER_C0
from tbe.tik.tik_lib.tik_params import COUT_B16
from tbe.tik.tik_lib.tik_params import MIN_KH_W_CHANNELS
from tbe.tik.tik_lib.tik_params import MAX_KH_W_CHANNELS
from tbe.tik.tik_lib.tik_params import MIN_CHANNELS
from tbe.tik.tik_lib.tik_params import MAX_CHANNELS
from tbe.tik.api.cube.reindex import ReIndexProxy
from tbe.tik.api.cube.cube_common import CubeTileCycles
from tbe.tik.api.cube.cube_common import get_iter_thread_num
from tbe.tik.api.cube.cube_common import CubeTilingMake
from tbe.tik.api.cube.cube_common import CubeModeTiling
from tbe.tik.api.cube.cube_common import TilingLoopInfo
from tbe.tik.common.common_util import get_l0c_align
from tbe.tik.common.common_util import check_address_align


TilingParams = namedtuple("TilingParams", ["fm_desc", "filter_desc", "m_block_num", "m_tile_block", "m_thread_num",
                                           "m_iter_num", "k_block_num", "k_tile_block", "k_thread_num", "k_iter_num",
                                           "n_block_num", "n_tile_block", "n_thread_num", "n_iter_num",
                                           "c_0", "loop_mode", "l0b_mode", "l0a_mode", "load_l0a_time_unit"])
TilingParams.__new__.__defaults__ = (None, None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)


# @cond
class ConvTileInfo(CubeTileCycles):
    """
    Conv Tiling Info
    """

    def __init__(self, conv_tile, cube_size):
        super(ConvTileInfo, self).__init__(cube_size)
        self.is_conv2d = True
        self.fm_desc = conv_tile.fm_desc
        self.filter_desc = conv_tile.filter_desc
        self.m_tile_block = conv_tile.m_tile_block
        self.m_thread_num = conv_tile.m_thread_num
        self.k_tile_block = conv_tile.k_tile_block
        self.k_thread_num = conv_tile.k_thread_num
        self.n_tile_block = conv_tile.n_tile_block
        self.n_thread_num = conv_tile.n_thread_num
        self.c_0 = conv_tile.c_0
        if conv_tile.fm_desc.output_dtype == "float16":
            self.is_output_fp16 = True
        self.total_howo = self.fm_desc.h_o * self.fm_desc.w_o
        # total block num of howo
        self.m_block_num = conv_tile.m_block_num
        self.m_tile_nums = self.m_tile_block * self.block_size
        # iter num of howo
        self.m_iter_num = conv_tile.m_iter_num
        # check whether have tail
        self.m_tail_block = 0
        if self.total_howo != self.m_iter_num*self.m_tile_block * self.block_size:
            self.m_has_tail = 1
        else:
            self.m_has_tail = 0

        self.m_tail_nums = 0
        if self.m_has_tail:
            self.m_tail_block = self.m_block_num - (self.m_iter_num - 1) * self.m_tile_block
            self.m_tail_nums = self.total_howo - (self.m_iter_num - 1) * self.m_tile_block * self.block_size

        self.k_block_num = conv_tile.k_block_num
        self.k_iter_num = conv_tile.k_iter_num
        self.k_has_tail = int(self.k_block_num < self.k_iter_num * self.k_tile_block)
        self.k_tail_block = 0
        if self.k_has_tail:
            self.k_tail_block = self.k_block_num - (self.k_iter_num - 1) * self.k_tile_block

        self.n_block_num = conv_tile.n_block_num
        self.n_iter_num = conv_tile.n_iter_num
        self.n_has_tail = int(self.n_block_num < self.n_iter_num * self.n_tile_block)
        self.n_tail_block = 0
        if self.n_has_tail:
            self.n_tail_block = self.n_block_num - (self.n_iter_num - 1) * self.n_tile_block
        self.l0a_mode = conv_tile.l0a_mode
        self.l0b_mode = conv_tile.l0b_mode
        self.loop_mode = conv_tile.loop_mode  # nk kn mn nm
        # create a fractal 512B data in L0A cost cycles
        self.load_l0a_time_unit = conv_tile.load_l0a_time_unit
        self.total_cycles = 0

    def __str__(self):
        """
        for print tiling info
        """
        return "HoWo: tile_block {} thread {} iter {} has tail {}\n" \
               "Cin: tile_block {} thread {} iter {} has tail {}\n" \
               "Cout tile_block {} thread {} iter {} has tail {}\n" \
               "loop mode {} l0a mode {} l0b mode {}\ntotal_cycles {}\n" \
            .format(
                    self.m_tile_block, self.m_thread_num, self.m_iter_num, self.m_has_tail,
                    self.k_tile_block, self.k_thread_num, self.k_iter_num, self.k_has_tail,
                    self.n_tile_block, self.n_thread_num, self.n_iter_num, self.n_has_tail,
                    self.loop_mode, self.l0a_mode, self.l0b_mode, self.total_cycles,)

    def __lt__(self, other):
        """
        sort by total_cycles with ascending order
        """
        return self.total_cycles < other.total_cycles

    @staticmethod
    def _cal_load3d_interval(k_tile_block, once_load_l0a_cycles):
        """
        cal two load3d interval cycles of mode 1
        """
        load_l0a_cycles = 0
        # if load mode 1, once load3d about 21 scalars,
        # it's cost 43 cycles, if one load3d cost cycles less than 43,
        # tow continuous load3d instructions will arise interval,
        # interval is about 43 - once_load3d_cycles
        # some scalar can be optimize by compile,
        # so interval is about (43 - once_load3d_cycles) * (K-1) / 2
        # if K > 2 and two continuous load3d instructions' cycles less then 43
        # two continuous load3d instructions and two continuous load3d
        # instructions will arise 25 cycles interval
        # interval sample:
        #     load3d ((43 - once_load3d_cycles) / 2) load3d (25) load3d
        #     ((43 - once_load3d_cycles) / 2) load3d (25) load3d ......
        if k_tile_block > 1 and once_load_l0a_cycles < 43:
            load_l0a_cycles += (k_tile_block - 1) * (43 - once_load_l0a_cycles) / 2
            if k_tile_block > 2 and 2 * once_load_l0a_cycles < 43:
                load_l0a_cycles += k_tile_block // 2 * 25
        return load_l0a_cycles

    def cal_kn_loop_cycles(self):
        """
        cal kn mode cycles

        Returns
        ----------
        kn_loop_cycles: total cycles of this mode
        """
        k_iter_num, k_thread_num = get_iter_thread_num(self, 'cin')
        # cal l0a load cycles, it's affected by l0b load cycles
        l0a_cycles_for = self.get_load_l0a_cycles(self.m_tile_block, self.k_tile_block,
                                                  self.get_load_l0b_cycles(self.k_tile_block, self.n_tile_block))
        # cal for loop n cycles
        # loop_for: _, n_loop_mte1_cycles_for, n_loop_cube_cycles_for, n_loop_head_for
        loop_for = self._do_n_loop(self.k_tile_block, k_thread_num)
        l0a_cycles_tail = 0
        n_loop_mte1_cycles_tail = 0
        n_loop_cube_cycles_tail = 0
        n_loop_head_tail = [0]
        if self.k_has_tail:
            l0b_cycles_tail = self.get_load_l0b_cycles(self.k_tail_block, self.n_tile_block)
            l0a_cycles_tail = self.get_load_l0a_cycles(self.m_tile_block, self.k_tail_block, l0b_cycles_tail)
            _, n_loop_mte1_cycles_tail, n_loop_cube_cycles_tail, n_loop_head_tail = \
                self._do_n_loop(self.k_tail_block, k_thread_num)

        # cannot delete
        # _do_loop only can deal with nm, mn mode inner iter num is 1
        head_cycles_for = [l0a_cycles_for, loop_for[3][0]]
        head_cycles_tail = [l0a_cycles_tail, n_loop_head_tail[0]]
        # if iter num is 1, head cycles need to add to one value
        if self.n_iter_num == 1:
            head_cycles_for = [l0a_cycles_for + loop_for[3][0]]
            head_cycles_tail = [l0a_cycles_tail + n_loop_head_tail[0]]
        if self.is_conv2d_c04 is True:
            inner_iter_nums = self.n_iter_num - 1 if self.n_has_tail else self.n_iter_num
            tiling_loop_info = TilingLoopInfo(
                k_iter_num, l0a_cycles_for + loop_for[1], l0a_cycles_tail + n_loop_mte1_cycles_tail,
                loop_for[2], n_loop_cube_cycles_tail, k_thread_num, self.k_thread_num, self.k_has_tail,
                inner_iter_nums)
        else:
            tiling_loop_info = TilingLoopInfo(
                k_iter_num, l0a_cycles_for + loop_for[1], l0a_cycles_tail + n_loop_mte1_cycles_tail,
                loop_for[2], n_loop_cube_cycles_tail, k_thread_num, self.k_thread_num, self.k_has_tail)
        kn_loop_cycles, _, _, _ = self.do_loop(
            tiling_loop_info, head_cycles_for=head_cycles_for, head_cycles_tail=head_cycles_tail)
        return kn_loop_cycles

    def cal_cycles(self):
        """
        calulate the cycles of different tiling
        Parameters
        ----------
        Returns
        ----------
        None
        """
        if self.loop_mode == "nk":
            self.total_cycles = self.cal_nk_loop_cycles()
        elif self.loop_mode == "kn":
            self.total_cycles = self.cal_kn_loop_cycles()
        elif self.loop_mode == "mn":
            self.total_cycles = self.cal_mn_loop_cycles()
        # mode nm
        else:
            self.total_cycles = self.cal_nm_loop_cycle()

    def get_load_l0a_cycles(self, m_tile_block, k_tile_block, load_l0b_cycles=0):
        """
        get load3d cycles
        Parameters
        ----------
        m_tile_block: M block nums
        k_tile_block: K block nums
        load_l0b_cycles: load2d cycles before load3d instruction

        Returns
        ----------
        load_l0a_cycles: load3d needed cycles
        """
        # DMA latency is 15 ~ 16
        head_cost = 15
        # camodel log show that: load3d can execute when load2d execute
        # so head cost is zero
        if self.l0a_mode == 0 and m_tile_block == 1 and self.m_iter_num == 1:
            head_cost = 0
        elif self.l0a_mode == 1 and self.n_iter_num == 1 and self.loop_mode != "nm":
            head_cost = 0

        if self.l0a_mode == 0:
            # call load3d times m_tile_block
            load_l0a_cycles = m_tile_block * (k_tile_block * self.load_l0a_time_unit + head_cost)
        else:
            # l0a_mode is 1
            load_l0a_cycles = self._get_mode1_l0a_cycles(m_tile_block, k_tile_block, load_l0b_cycles, head_cost)
        return load_l0a_cycles

    def cal_nk_loop_cycles(self):
        """
        cal nk mode cycles

        Returns
        ----------
        nk_loop_cycles: total cycles of this mode
        """

        def _do_k_loop(n_actual_blk):
            l0b_cycles_for = self.get_load_l0b_cycles(self.k_tile_block, n_actual_blk)
            l0a_cycles_for = self.get_load_l0a_cycles(self.m_tile_block, self.k_tile_block, l0b_cycles_for)
            cube_cycles_for = self.get_cube_cycles(self.m_tile_block, self.k_tile_block, n_actual_blk)
            l0a_cycles_tail = 0
            l0b_cycles_tail = 0
            cube_cycles_tail = 0
            if self.k_has_tail:
                l0b_cycles_tail = self.get_load_l0b_cycles(self.k_tail_block, n_actual_blk)
                l0a_cycles_tail = self.get_load_l0a_cycles(self.m_tile_block, self.k_tail_block, l0b_cycles_tail)
                cube_cycles_tail = self.get_cube_cycles(self.m_tile_block, self.k_tail_block, n_actual_blk)
            return self.do_loop(TilingLoopInfo(
                k_iter_num, l0a_cycles_for + l0b_cycles_for, l0a_cycles_tail + l0b_cycles_tail, cube_cycles_for,
                cube_cycles_tail, k_thread_num, max(self.n_thread_num, self.k_thread_num), self.k_has_tail))

        n_iter_num, n_thread_num = get_iter_thread_num(self, 'cout')
        k_iter_num, k_thread_num = get_iter_thread_num(self, 'cin', n_thread_num)
        _, k_loop_mte1_cycles_for, k_loop_cube_cycles_for, k_loop_head_for = _do_k_loop(self.n_tile_block)
        k_loop_mte1_cycles_tail = 0
        k_loop_cube_cycles_tail = 0
        k_loop_head_tail = [0]
        if self.n_has_tail:
            _, k_loop_mte1_cycles_tail, k_loop_cube_cycles_tail, k_loop_head_tail = _do_k_loop(self.n_tail_block)

        if self.is_conv2d_c04 is True:
            tiling_loop_info = TilingLoopInfo(
                n_iter_num, k_loop_mte1_cycles_for, k_loop_mte1_cycles_tail, k_loop_cube_cycles_for,
                k_loop_cube_cycles_tail, n_thread_num, self.n_thread_num, self.n_has_tail, k_iter_num)
        else:
            tiling_loop_info = TilingLoopInfo(
                n_iter_num, k_loop_mte1_cycles_for, k_loop_mte1_cycles_tail, k_loop_cube_cycles_for,
                k_loop_cube_cycles_tail, n_thread_num, self.n_thread_num, self.n_has_tail)

        # cal two for loop cycles
        nk_loop_cycles, _, _, _ = self.do_loop(
            tiling_loop_info, head_cycles_for=k_loop_head_for, head_cycles_tail=k_loop_head_tail)
        return nk_loop_cycles

    def _get_mode1_l0a_cycles_nk(self, k_tile_block, load_l0b_cycles, once_load_l0a_cycles):
        """
        cal load3dv1 cycles of l0a load mode is 1 and loop mode is nk
        """
        load_l0a_cycles = self._cal_load3d_interval(k_tile_block, once_load_l0a_cycles)
        # if l0b mode is 0, load2d need more scalar,
        # so if load2d cycles less then 43,
        # load3d and load2d interval is (43 - load_l0b_cycles),
        # read from camodel log min interval about 25
        # if l0b mode is 1, load2d cycles and load3d cycles both less then 43
        # interval is max value of 25, (43 - load_l0b_cycles)
        if self.l0b_mode == 0:
            if load_l0b_cycles < 43:
                load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        elif load_l0b_cycles < 43 and once_load_l0a_cycles < 43:
            load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        return load_l0a_cycles

    def _get_mode1_l0a_cycles_kn(self, k_tile_block, load_l0b_cycles, once_load_l0a_cycles):
        """
        cal load3dv1 cycles of l0a load mode is 1 and loop mode is kn
        """
        load_l0a_cycles = self._cal_load3d_interval(k_tile_block, once_load_l0a_cycles)
        # if l0b_mode is 0 and for n iter num is odd and
        # load2d cycles less then 43,
        # load3d and load2d interval is max(25, (43 - load_l0b_cycles))
        # if l0b mode is 1, load2d cycles and load3d cycles both less then 43
        # interval is max value of 25, (43 - load_l0b_cycles)
        if self.l0b_mode == 0 and self.n_iter_num & 1:
            if load_l0b_cycles < 43:
                load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        elif load_l0b_cycles < 43 and once_load_l0a_cycles < 43:
            load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        return load_l0a_cycles

    def _get_mode1_l0a_cycles_mn(self, k_tile_block, load_l0b_cycles, once_load_l0a_cycles):
        """
        cal load3dv1 cycles of l0a load mode is 1 and loop mode is mn
        """
        load_l0a_cycles = self._cal_load3d_interval(k_tile_block, once_load_l0a_cycles)
        # if l0b_mode is 0 and for n iter num is odd and
        # load2d cycles less then 43,
        # load3d and load2d interval is max(25, (43 - load_l0b_cycles))
        # if load2d cycles less then 43
        # interval is max value of 25, (43 - load_l0b_cycles)
        if self.l0b_mode == 0 and self.n_iter_num > 1 and self.n_iter_num & 1 and load_l0b_cycles < 43:
            load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        elif load_l0b_cycles < 43:
            load_l0a_cycles += max(25, (43 - load_l0b_cycles))

        if self.is_output_fp16 and k_tile_block == 1 and self.k_iter_num == 1 and self.m_thread_num > 1 \
            and self.n_thread_num > 1:
            load_l0a_cycles -= 25
        return load_l0a_cycles

    def _get_mode1_l0a_cycles_nm(self, k_tile_block, load_l0b_cycles, once_load_l0a_cycles):
        """
        cal load3dv1 cycles of l0a load mode is 1 and loop mode is nm
        """
        load_l0a_cycles = self._cal_load3d_interval(k_tile_block, once_load_l0a_cycles)
        # if for m iter num is 1 and for n iter num is odd and
        # l0b_mode is 0 and load2d cycles less then 43,
        # load3d and load2d interval is max(25, (43 - load_l0b_cycles))
        # if for m iter num is 1 and load2d cycles less then 43
        # interval is max value of 25, (43 - load_l0b_cycles)
        if self.m_iter_num == 1 and load_l0b_cycles < 43 and \
                self.n_iter_num > 1 and self.n_iter_num & 1 and self.l0b_mode == 0:
            load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        elif self.m_iter_num == 1 and load_l0b_cycles < 43:
            load_l0a_cycles += max(25, (43 - load_l0b_cycles))
        return load_l0a_cycles

    def _get_mode1_l0a_cycles(self, m_tile_block, k_tile_block, load_l0b_cycles, head_cost):
        """
        cal load3dv1 cycles of l0a load mode is 1
        """
        # load3d once load m_tile_block(512Byte), head_cost is DMA latency
        once_load_l0a_cycles = m_tile_block * self.load_l0a_time_unit + head_cost
        # load3d execute k_time_block times
        load_l0a_cycles = k_tile_block * once_load_l0a_cycles

        if self.loop_mode == "kn":
            load_l0a_cycles += self._get_mode1_l0a_cycles_kn(k_tile_block, load_l0b_cycles, once_load_l0a_cycles)
        elif self.loop_mode == "nk":
            load_l0a_cycles += self._get_mode1_l0a_cycles_nk(k_tile_block, load_l0b_cycles, once_load_l0a_cycles)
        elif self.loop_mode == "mn":
            load_l0a_cycles += self._get_mode1_l0a_cycles_mn(k_tile_block, load_l0b_cycles, once_load_l0a_cycles)
        # loop mode nm
        else:
            load_l0a_cycles += self._get_mode1_l0a_cycles_nm(k_tile_block, load_l0b_cycles, once_load_l0a_cycles)
        return load_l0a_cycles

    def _do_n_loop(self, k_actual_blk, k_thread_num):
        n_iter_num, n_thread_num = get_iter_thread_num(self, 'cout', k_thread_num)
        l0b_cycles_for = self.get_load_l0b_cycles(k_actual_blk, self.n_tile_block)
        cube_cycles_for = self.get_cube_cycles(self.m_tile_block, k_actual_blk, self.n_tile_block)
        l0b_cycles_tail = 0
        cube_cycles_tail = 0
        if self.n_has_tail:
            l0b_cycles_tail = self.get_load_l0b_cycles(k_actual_blk, self.n_tail_block)
            cube_cycles_tail = self.get_cube_cycles(self.m_tile_block, k_actual_blk, self.n_tail_block)
        return self.do_loop(TilingLoopInfo(
            n_iter_num, l0b_cycles_for, l0b_cycles_tail, cube_cycles_for, cube_cycles_tail, n_thread_num,
            max(self.k_thread_num, self.n_thread_num), self.n_has_tail))


class ValidTilingInfo(CubeTilingMake):
    """
    valid tiling info
    """

    def __init__(self, fm_desc, filter_desc, cube_size, tik_instance):
        super(ValidTilingInfo, self).__init__()
        self.fm_desc = fm_desc
        self.filter_desc = filter_desc
        self.tik_instance = tik_instance
        self.ho_wo = fm_desc.h_o * fm_desc.w_o
        self.block_size = 16
        self.m_block_num = math.ceil(self.ho_wo / self.block_size)
        self.fm_dtype_size = DTYPE_SIZE.get(fm_desc.input_dtype)
        self.c_0 = BYTE_PER_C0 // self.fm_dtype_size
        # K block num is kh*kw*C1
        self.k_block_num = math.ceil(filter_desc.cin * filter_desc.height * filter_desc.width / self.c_0)
        self.n_block_num = math.ceil(filter_desc.cout / self.block_size)
        self.filter_dtype_size = DTYPE_SIZE.get(filter_desc.dtype)
        self.valid_solution = []
        # default m_thread_num set to 1
        self.m_thread_num = 1
        self.load_l0a_time_unit = 0
        self.cube_size = cube_size

    @staticmethod
    def _get_loop_modes(outer_block_num, inner_block_num, modes):
        """
        according the block num and modes to create loop modes,
        this function for nk kn mode
        """
        # if block num > 1, thread num can be 1,2, else can only be 1
        if outer_block_num > 1:
            outer_thread_nums = [2, 1]
        else:
            outer_thread_nums = [1]

        if inner_block_num > 1:
            inner_thread_nums = [2, 1]
        else:
            inner_thread_nums = [1]

        loop_modes = []
        for outer_thread_num in outer_thread_nums:
            for inner_thread_num in inner_thread_nums:
                for mode in modes:
                    loop_modes.append((outer_thread_num,
                                       inner_thread_num, mode))
        return loop_modes

    @staticmethod
    def _load_output_count(total_cycle_count, current_wo, iter_num_for, i):
        """
        """
        total_cycle_count += 1
        if current_wo > 16:
            current_wo -= 16
            # if it's last line, needn't to add
            if i < iter_num_for - 1:
                total_cycle_count += 1
        elif current_wo == 16:
            current_wo = 0
        return current_wo, total_cycle_count

    def set_load_l0a_time(self):
        """
        according wo and stride_w to calculate
        create a 512B part unit in L0A cost cycles
        """
        # if w_o == w_i, l0a cost unit is stride_w
        # if kh == 1, l0a cost unit is stride_w
        # if w_o >= 64, l0a cost unit is stride_w
        # if w_o % 16 == 0, l0a cost unit is stride_w
        # others call _get_load_l0a_time_unit() to calculate
        stride_w = self.fm_desc.stride_w
        if self.fm_desc.w_o == self.fm_desc.w_i:
            one_instr_times = stride_w
        elif self.filter_desc.height == 1:
            one_instr_times = stride_w
        elif self.fm_desc.w_o >= 64:
            one_instr_times = stride_w
        elif self.fm_desc.w_o % 16 == 0:
            one_instr_times = stride_w
        else:
            one_instr_times = self._get_load_l0a_time_unit()
        self.load_l0a_time_unit = one_instr_times

    def create_tiling_info(self, tile_block_list, iter_num_list, thread_num_list, loop_mode):
        """
        create tiling info object and save to tiling list
        """
        m_tile_block, k_tile_block, n_tile_block = tile_block_list
        m_thread_num, k_thread_num, n_thread_num = thread_num_list
        m_iter_num, k_iter_num, n_iter_num = iter_num_list

        # cout block is 1, load by row, else load by column
        if n_tile_block == 1:
            l0b_mode = 1
        else:
            l0b_mode = 0
        # l0a mode can be 0, 1, 0 is load by row, 1 is load by column
        # cannot simplify judge 0 or 1 which is better
        # when row num <= column num, load by row
        # else try load by row or column

        if m_tile_block <= k_tile_block:
            l0a_modes = [0]
        else:
            l0a_modes = [0, 1]

        for l0a_mode in l0a_modes:
            conv_tile = TilingParams(self.fm_desc, self.filter_desc,
                                     self.m_block_num, m_tile_block, m_thread_num, m_iter_num,
                                     self.k_block_num, k_tile_block, k_thread_num, k_iter_num,
                                     self.n_block_num, n_tile_block, n_thread_num, n_iter_num,
                                     self.c_0, loop_mode, l0b_mode, l0a_mode, self.load_l0a_time_unit)

            tile_info = ConvTileInfo(conv_tile, self.cube_size)
            tile_info.cal_cycles()
            self.valid_solution.append(tile_info)

    def gen_mn_tiling(self):
        """
        generate mn tiling
        """
        # mn or nm mode, n_tile_block must be 1,
        # cannot load multi column to do mmad
        # detailed info please read detail design book
        n_tile_block = 1
        modes = ["mn", "nm"]
        loop_modes = self.get_loop_modes(self.m_block_num, self.n_block_num, self.k_block_num, modes)
        for nums, k_tile_block in itertools.product(loop_modes, range(1, self.k_block_num + 1)):
            m_thread_num, _, k_thread_num, loop_mode = nums
            k_iter_num = math.ceil(self.k_block_num / k_tile_block)
            if k_thread_num > k_iter_num:
                continue
            for m_tile_block in range(1, self.m_block_num + 1):
                m_iter_num = math.ceil(self.m_block_num / m_tile_block)
                if m_thread_num > m_iter_num:
                    continue
                # check needed tensor size of L0A, L0B
                if self.check_tensor_size_by_mode(nums, m_tile_block, k_tile_block, n_tile_block) is False:
                    continue

                n_thread_num = nums[1]
                # create valid tiling and add to tiling list
                self.create_tiling_info([m_tile_block, k_tile_block, n_tile_block],
                                        [m_iter_num, k_iter_num, self.n_block_num],
                                        [m_thread_num, k_thread_num, n_thread_num], loop_mode)

    def gen_nk_tiling(self, bias=None):
        """
        generate nk tiling
        """
        modes = ["kn", "nk"]
        # generate thread num and loop mode
        loop_modes = self._get_loop_modes(self.n_block_num, self.k_block_num, modes)
        for nums, k_tile_block in itertools.product(loop_modes, range(1, self.k_block_num + 1)):
            n_thread_num, k_thread_num, loop_mode = nums
            for n_tile_block in range(1, self.n_block_num + 1):
                # cal iter nums
                k_iter_num = math.ceil(self.k_block_num / k_tile_block)
                n_iter_num = math.ceil(self.n_block_num / n_tile_block)
                # thread num must more than iter num
                if k_thread_num > k_iter_num:
                    continue
                if n_thread_num > n_iter_num:
                    continue
                if self._check_is_skip(k_tile_block, n_tile_block, nums):
                    continue
                # bias is bt tensor, max size is 2048B, bias'size max is 256(16 block)
                # 310b, 610l, bt tensor max size is 1024B
                bias_max_blk = 8 if TikSocManager.is_310b_610l_soc() else 16
                bias_max_blk_db = bias_max_blk // 2
                if bias and (n_tile_block > bias_max_blk or (n_thread_num > 1 and n_tile_block > bias_max_blk_db)):
                    continue
                # n turn on double_buffer, bias'size max is 128(8 block)
                if bias and loop_mode == 'kn' and k_thread_num > 1 and n_tile_block > bias_max_blk_db:
                    continue

                # create valid tiling and add to tiling list
                self.create_tiling_info([self.m_block_num, k_tile_block, n_tile_block],
                                        [1, k_iter_num, n_iter_num],
                                        [1, k_thread_num, n_thread_num], loop_mode)

    def _check_is_skip(self, k_tile_block, n_tile_block, nums):
        n_thread_num, k_thread_num, loop_mode = nums
        # L0A size: (HoWo, Cin, Hk, Wk)
        if loop_mode == "kn":
            if self.check_tensor_size("l0a", self.m_block_num, k_tile_block, k_thread_num) is False:
                return True
        else:
            if self.check_tensor_size("l0a", self.m_block_num, k_tile_block, max(n_thread_num, k_thread_num)) is False:
                return True
        # check needed L0B size
        if self.check_tensor_size("l0b", k_tile_block, n_tile_block, max(n_thread_num, k_thread_num)) is False:
            return True
        return False

    def _get_load_l0a_time_unit(self):
        """
        calculate load 512B from L1 to L0A cost cycles
        """
        # through the iter_num, every time use 16 element of width,
        # and the number of total w_o in each 16 will be the l0a unit cost.
        # finally, return the average cost.
        # iter_num is 512B part num in L0A
        iter_num = self.m_block_num
        # iter_num_for is load lines num, it needs polishing if has tail
        iter_num_for = math.ceil(iter_num * 16 / self.fm_desc.w_o)
        # once load width is 16, but output is not 16, it's 16//stride_w
        if self.fm_desc.stride_w > 16:
            one_load_output_w = 1
        else:
            one_load_output_w = 16 // self.fm_desc.stride_w
        # calculate the load start index of w,
        # if can load 16 elements from one line, only cost one cycle;
        # if load 16 elements from multi lines, will cost multi cycles
        current_wo = 0
        total_cycle_count = 0
        for i in range(iter_num_for):
            # load in multi lines, multi lines build one fractal
            if one_load_output_w >= self.fm_desc.w_o:
                current_wo += self.fm_desc.w_o
                # load 512B cost one cycle
                current_wo, total_cycle_count = self._load_output_count(total_cycle_count, current_wo,
                                                                        iter_num_for, i)
            # load in one line, one line can build multi fractals
            else:
                start_wo = 0
                while start_wo < self.fm_desc.w_o:
                    current_wo += one_load_output_w
                    start_wo += one_load_output_w
                    # load once cost one cycle
                    current_wo, total_cycle_count = self._load_output_count(total_cycle_count, current_wo,
                                                                            iter_num_for, i)
        return total_cycle_count / iter_num


def gen_best_tiling(fm_desc, filter_desc, tik_instance):
    """
    get the best tiling for conv2d
    Parameters
    ----------
    fm_desc: FM info
    filter_desc: weight info
    tik_instance: Tik object

    Returns
    -------
    best tiling

    """
    cube_size = get_soc_spec("CUBE_SIZE")
    if fm_desc.input_dtype == "int8":
        cube_size[2] = cube_size[2] * 2
    valid_tiling = ValidTilingInfo(fm_desc, filter_desc, cube_size, tik_instance)
    valid_tiling.set_load_l0a_time()
    # create tiling of nk kn mode
    valid_tiling.gen_nk_tiling()
    # create tiling of nm mn mode
    valid_tiling.gen_mn_tiling()
    # if not find valid tiling, return the default tiling
    if not valid_tiling.valid_solution:
        tile_block = 1
        thread_num = 1
        c_0 = BYTE_PER_C0 // DTYPE_SIZE.get(fm_desc.input_dtype)
        # default mode cannot be nk kn, can only be mn or nm
        loop_mode = "mn"
        load_l0a_time_unit = 1
        l0a_mode = 0
        l0b_mode = 1
        block_size = 16
        ho_wo = fm_desc.h_o * fm_desc.w_o
        m_block_num = math.ceil(ho_wo / 16)

        k_block_num = math.ceil(filter_desc.cin * filter_desc.height * filter_desc.width / c_0)
        n_block_num = math.ceil(filter_desc.cout / block_size)

        conv_tile = TilingParams(fm_desc, filter_desc,
                                 m_block_num, tile_block, thread_num, m_block_num//tile_block,
                                 k_block_num, tile_block, thread_num, k_block_num//tile_block,
                                 n_block_num, tile_block, thread_num, n_block_num//tile_block,
                                 c_0, loop_mode, l0b_mode, l0a_mode, load_l0a_time_unit)
        tile_info = ConvTileInfo(conv_tile, cube_size)
        tile_info.cal_cycles()
        return tile_info
    valid_tiling.valid_solution.sort()
    return valid_tiling.valid_solution[0]


class Conv2dImpl(CubeModeTiling):
    """
    implement class of conv2d
    """

    def __init__(self, tik_instance):
        """
         init of conv2d.
        :param tik_instance: the instance of tik.
        """
        super(Conv2dImpl, self).__init__()
        self.tik_instance = tik_instance
        # M block size is always 16
        self.block_size = 16

    @staticmethod
    def _check_conv2d_tensor_overflow(need_shape, tensor, tensor_name):
        """
        check tensor overflow
        Parameters
        ----------
        need_shape: shape
        tensor: Tensor
        tensor_name: name of tensor

        Returns
        ----------
        """
        need_elements = Expr(reduce_mul(need_shape) + tensor.offset).eval_value()
        total_size = Expr(reduce_mul(tensor.original_shape)).eval_value()
        if need_elements is not None and total_size is not None:
            TikCheckUtil.check_ge(
                total_size, need_elements,
                "%s tensor overflow, instruction need %s but only %s" % (tensor_name, need_elements, total_size))

    @staticmethod
    def _check_conv2d_tensor_overlap(feature_map, weight, fm_shape, kernel_shape):
        """
        check tensor whether overlap
        """
        if feature_map.buffer == weight.buffer:
            feature_map_start = Expr(feature_map.offset).eval_value()
            feature_map_end = Expr(feature_map_start + reduce_mul(fm_shape)).eval_value()
            weight_start = Expr(weight.offset).eval_value()
            weight_end = Expr(weight_start + reduce_mul(kernel_shape)).eval_value()
            if all(value is not None for value in [feature_map_start, feature_map_end, weight_start, weight_end]):
                if max(feature_map_start, weight_start) < min(feature_map_end, weight_end):
                    TikCheckUtil.raise_error(
                        "feature_map and weight tensor address overlapping error.")

    @staticmethod
    def _check_conv2d_param_operator(dst, feature_map, weight, init_l1out):
        """
        check conv2d input params operator and type
        """
        # check operator
        TikCheckUtil.check_type_match(dst, Tensor,
                                      "dst should be tensor, input type: %s" % type(dst))
        TikCheckUtil.check_type_match(feature_map, Tensor,
                                      "feature_map should be tensor, input type: %s" % type(feature_map))
        TikCheckUtil.check_type_match(weight, Tensor,
                                      "weight should be tensor, input type: %s" % type(weight))
        # check operator scope, waiting for confirmation of naming !!!!
        TikCheckUtil.check_equality(dst.scope, scope_cc,
                                    "dst's scope must be L1_out, input scope is: %s" % dst.scope)
        dst_align = get_l0c_align(dst)
        check_address_align((dst,), ("dst",), dst_align)
        TikCheckUtil.check_equality(feature_map.scope, scope_cbuf,
                                    "feature_map's scope must be L1, input scope is: %s" % feature_map.scope)
        TikCheckUtil.check_equality(weight.scope, scope_cbuf,
                                    "weight's scope must be L1, input scope is: %s" % weight.scope)
        l1_align = 512
        check_address_align((feature_map, weight), ("feature_map", "weight"), l1_align)
        # check dtype
        dtype_str = DTYPE_MAP.get(feature_map.dtype) + DTYPE_MAP.get(weight.dtype) + DTYPE_MAP.get(dst.dtype)
        TikCheckUtil.check_equality(api_check_support("tik.conv2d", dtype_str), True,
                                    gen_api_check_statement(dtype_str, "conv2d"))
        # check init_l1out
        TikCheckUtil.check_type_match(init_l1out, bool, "init_l1out should be bool type.")

    @staticmethod
    def _check_c0_value(c0, dtype, input_param_name):
        valid_c0_value = BYTE_PER_C0 // DTYPE_SIZE.get(dtype)
        err_msg = "%s C0 dimension should be %d when feature_map is %s" % (input_param_name, valid_c0_value, dtype)
        TikCheckUtil.check_equality(c0, valid_c0_value, err_msg)

    @staticmethod
    def _check_channel_value(c, dtype, input_param_name):
        min_c_value = BYTE_PER_C0 // DTYPE_SIZE.get(dtype) * MIN_C1_VALUE
        err_msg = "%s channel(C1*C0) should be in range of [%s, %s]" % (input_param_name, min_c_value, MAX_CHANNELS)
        TikCheckUtil.check_in_range_by_dtype(c, msg=err_msg, var_range=[min_c_value, MAX_CHANNELS])

    @staticmethod
    def _check_conv2d_stride_pad_dilation(feature_map, stride, pad, dilation, pad_value):
        """
        check conv2d input params stride and pad and dilation
        """
        # check stride
        TikCheckUtil.check_type_match(stride, (list, tuple), "stride should be list or tuple")
        TikCheckUtil.check_equality(len(stride), 2, "stride should be two-dimensional")
        for num in stride:
            TikCheckUtil.check_type_match(num, int, "stride should be a list of int")
            TikCheckUtil.check_in_range_by_dtype(
                num, msg="stride_h/stride_w should be in range of [%s, %s]"
                         % (MIN_STRIDE, MAX_STRIDE), var_range=[MIN_STRIDE, MAX_STRIDE])
        # check pad
        TikCheckUtil.check_type_match(pad, (list, tuple), "pad should be list or tuple")
        TikCheckUtil.check_equality(len(pad), 4, "pad should be four-dimensional")
        for num in pad:
            TikCheckUtil.check_type_match(num, int,
                                          "pad should be a list of int")
            TikCheckUtil.check_in_range_by_dtype(
                num, msg="pad_left/pad_right/pad_top/pad_bottom should be in range of [%s, %s]"
                         % (PADDING_LEFT_IDX, MAX_PADDING), var_range=[PADDING_LEFT_IDX, MAX_PADDING])
        # check dilation
        TikCheckUtil.check_type_match(dilation, (list, tuple),
                                      "dilation should be list or tuple")
        TikCheckUtil.check_equality(len(dilation), 2,
                                    "dilation should be two-dimensional")
        for num in dilation:
            TikCheckUtil.check_type_match(num, int, "dilation should be a list of int")
            TikCheckUtil.check_in_range_by_dtype(
                num, msg="dilation_h/dilation_w should be in range of [%s, %s]"
                         % (MIN_DILATION, MAX_DILATION), var_range=[MIN_DILATION, MAX_DILATION])
        # check pad_value
        if feature_map.dtype in ("float16", "float32"):
            TikCheckUtil.check_type_match(
                pad_value, (int, float),
                "pad_value should be python int or float, input type is: %s" % type(pad_value))
        else:
            TikCheckUtil.check_type_match(
                pad_value, int, "pad_value should be python int, input type is: %s" % type(pad_value))
            if feature_map.dtype == "uint8":
                TikCheckUtil.check_in_range_by_dtype(
                    pad_value, "uint8", "pad_value should be in range of [%s, %s]" % (UINT_MIN, UINT8_MAX))
            elif feature_map.dtype == "int8":
                TikCheckUtil.check_in_range_by_dtype(
                    pad_value, "int8", "pad_value should be in range of [%s, %s]" % (INT8_MIN, INT8_MAX))

    @staticmethod
    def _get_filter_desc(kernel_shape, dtype):
        """
        Parameters
        ----------
        :param kernel_shape:
        kernel_shape:  the filter shape
        dtype: the filter tensor in L0B as dtype.

        Returns
        -------
        filte desc
        """
        filterdesc = namedtuple("FilterDesc", ["height", "width", "cin", "cout", "dtype"])
        c1, kh, kw, cout, c0 = kernel_shape
        return filterdesc(kh, kw, c1 * c0, cout, dtype)

    @staticmethod
    def get_fm_desc(conv2d_api):
        """
        get fm desc
        Parameters
        ----------
        conv2d_api: parameters of conv2d api
        """
        fm_desc_class = namedtuple("FMDesc", ["pad_list", "stride_h", "stride_w", "dilation_h", "dilation_w",
                                              "h_i", "w_i", "h_o", "w_o", "cin", "input_dtype", "output_dtype"])
        pad_list = conv2d_api.pad
        if pad_list is None:
            pad_list = [0, 0, 0, 0]
        fm_shape = conv2d_api.fm_shape
        dilation = conv2d_api.dilation
        kernel_shape = conv2d_api.kernel_shape
        stride_h = conv2d_api.stride[0]
        stride_w = conv2d_api.stride[1]
        h_o = math.floor((fm_shape[1] + pad_list[2] + pad_list[3] - dilation[0] *
                         (kernel_shape[1] - 1) - 1) / stride_h + 1)
        w_o = math.floor((fm_shape[2] + pad_list[0] + pad_list[1] - dilation[1] *
                         (kernel_shape[2] - 1) - 1) / stride_w + 1)

        fm_desc = fm_desc_class(pad_list, stride_h, stride_w, dilation[0], dilation[1],
                                fm_shape[1], fm_shape[2], h_o, w_o, fm_shape[0] * fm_shape[3],
                                conv2d_api.feature_map.dtype, conv2d_api.dst.dtype)
        return fm_desc

    def check_input_params(self, conv2d_api):
        """
        check conv2d input params
        Parameters
        ----------
        conv2d_api: input info

        Returns
        -------

        """
        self._check_conv2d_param_operator(conv2d_api.dst, conv2d_api.feature_map, conv2d_api.weight,
                                          conv2d_api.init_l1out)
        self._check_conv2d_params_fmshape(conv2d_api.feature_map, conv2d_api.fm_shape,
                                          conv2d_api.kernel_shape, conv2d_api.pad)
        self._check_conv2d_params_kernel_shape(conv2d_api.feature_map, conv2d_api.kernel_shape, conv2d_api.fm_shape)
        self._check_conv2d_stride_pad_dilation(conv2d_api.feature_map, conv2d_api.stride, conv2d_api.pad,
                                               conv2d_api.dilation, conv2d_api.pad_value)
        self._check_conv2d_params_over(conv2d_api.feature_map, conv2d_api.weight, conv2d_api.fm_shape,
                                       conv2d_api.kernel_shape)
        self.check_params_bias(conv2d_api.bias, conv2d_api.dst, conv2d_api.kernel_shape, conv2d_api.init_l1out,
                               "conv2d")

    def gen_code_by_tiling(self, conv2d_api, fm_desc, tiling):
        """
        gen code according the best tiling for conv2d
        Parameters
        ----------
        conv2d_api: conv2d input info
        fm_desc: fm info
        tiling: best tiling

        Returns
        -------

        """
        # check dst tensor overflow
        self._check_conv2d_tensor_overflow((tiling.m_block_num * self.block_size, conv2d_api.kernel_shape[3]),
                                           conv2d_api.dst, "dst")

        feature_map = ReIndexProxy(conv2d_api.feature_map, conv2d_api.fm_shape)
        weight = ReIndexProxy(conv2d_api.weight, conv2d_api.kernel_shape)

        # define a new tuple for save needed parameters
        # if used dont't use this API, no need install the third party software
        conv2d_param_class = namedtuple("Conv2dParam", ["feature_map", "weight", "k_w", "k_h", "dilation_w",
                                                        "dilation_h", "pad_value", "fm_desc", "init_l1out",
                                                        "dst", "tik_instance", "l1_bias"])

        param = conv2d_param_class(feature_map, weight, conv2d_api.kernel_shape[2], conv2d_api.kernel_shape[1],
                                   conv2d_api.dilation[1], conv2d_api.dilation[0], conv2d_api.pad_value, fm_desc,
                                   conv2d_api.init_l1out, conv2d_api.dst, self.tik_instance, conv2d_api.bias)

        # according loop mode to gen code
        if tiling.loop_mode == "nk":
            self.do_nk_tiling(tiling, param)
        elif tiling.loop_mode == "kn":
            self.do_kn_tiling(tiling, param)
        elif tiling.loop_mode == "mn":
            self.do_mn_tiling(tiling, param)
        elif tiling.loop_mode == "nm":
            self.do_nm_tiling(tiling, param)

    def execute(self, conv2d_api):
        """
        execute of conv2d to get best tiling and gen code
        Parameters
        ----------
        conv2d_api: contains var
        - dst:  the dst tensor in L0C
        - feature_map:  the Feature map tensor in L0A.
        - weight: the filter tensor in L0B.
        - fm_shape: the feature map shape
        - kernel_shape:  the filter shape
        - stride: conv2d stride data
        - pad: conv2d padding values
        - dilation: conv2d dilations
        - pad_value: padding value
        - init_l1out: whether init L0C result
        """
        self.check_input_params(conv2d_api)

        # fm_shape: fm_c1, fm_h, fm_w, fm_c0
        # kernel_shape:k_c1, k_h, k_w, k_cout, k_c0
        # create new object for save feature map info
        fm_desc = self.get_fm_desc(conv2d_api)

        # create new object for save filter info
        filter_desc = self._get_filter_desc(conv2d_api.kernel_shape, conv2d_api.weight.dtype)

        # find the best tiling
        tiling = gen_best_tiling(fm_desc, filter_desc, self.tik_instance)
        self.gen_code_by_tiling(conv2d_api, fm_desc, tiling)

    def _check_conv2d_params_fmshape(self, feature_map, fm_shape, kernel_shape, pad_list):
        """
        check conv2d input params fm_shape
        """
        # check fm_shape
        TikCheckUtil.check_type_match(fm_shape, (list, tuple),
                                      "fm_shape should be list or tuple")
        TikCheckUtil.check_equality(len(fm_shape), 4,
                                    "fm_shape should be four-dimensional")
        for num in fm_shape:
            TikCheckUtil.check_type_match(num, int, "fm_shape should be a list of int")
        TikCheckUtil.check_in_range_by_dtype(
            fm_shape[0], msg="fm_shape C1 dimension should be in range of [%s, %s]"
                             % (MIN_C1_VALUE, MAX_C1_VALUE), var_range=[MIN_C1_VALUE, MAX_C1_VALUE])

        self._check_c0_value(fm_shape[3], feature_map.dtype, "fm_shape")
        self._check_channel_value(fm_shape[0] * fm_shape[3], feature_map.dtype, "fm_shape")

        k_h = kernel_shape[1]
        k_w = kernel_shape[2]
        pad_h = pad_list[2] + pad_list[3]
        pad_w = pad_list[0] + pad_list[1]
        TikCheckUtil.check_in_range_by_dtype(
            fm_shape[1], msg="fm_shape H dimension should be in range of [%s, %s]"
                             % (MIN_CHANNELS, MAX_CHANNELS), var_range=[MIN_CHANNELS, MAX_CHANNELS])
        TikCheckUtil.check_ge(
            fm_shape[1] + pad_h, k_h, msg="fm_shape H + pad_top + pad_bottom (%s) should be ge kh (%s)"
                                          % (fm_shape[1] + pad_h, k_h))

        TikCheckUtil.check_in_range_by_dtype(
            fm_shape[2], msg="fm_shape W dimension should be in range of [%s, %s]" % (MIN_CHANNELS, MAX_CHANNELS),
            var_range=[MIN_CHANNELS, MAX_CHANNELS])

        TikCheckUtil.check_ge(
            fm_shape[2] + pad_w, k_w, msg="fm_shape W + pad_left + pad_right (%s) should be ge kw (%s)"
                                          % (fm_shape[2] + pad_w, k_w))

    def _check_conv2d_params_kernel_shape(self, feature_map, kernel_shape, fm_shape):
        """
        check conv2d input params kernel_shape
        """
        # check kernel_shape
        TikCheckUtil.check_type_match(kernel_shape, (list, tuple),
                                      "kernel_shape should be list or tuple")
        TikCheckUtil.check_equality(len(kernel_shape), 5,
                                    "kernel_shape should be five-dimensional")
        for num in kernel_shape:
            TikCheckUtil.check_type_match(
                num, int, "kernel_shape should be a list of int")
        TikCheckUtil.check_in_range_by_dtype(
            kernel_shape[0], msg="kernel_shape C1 dimension should be in range of [%s, %s]"
                                 % (MIN_C1_VALUE, MAX_C1_VALUE), var_range=[MIN_C1_VALUE, MAX_C1_VALUE])

        self._check_c0_value(kernel_shape[4], feature_map.dtype, "kernel_shape")
        self._check_channel_value(kernel_shape[0] * kernel_shape[4], feature_map.dtype, "kernel_shape")

        TikCheckUtil.check_in_range_by_dtype(
            kernel_shape[3], msg="kernel_shape Cout dimension should be in range of [%s, %s]"
                                 % (COUT_B16, MAX_CHANNELS), var_range=[COUT_B16, MAX_CHANNELS])
        TikCheckUtil.check_equality(
            kernel_shape[3] % 16, 0,
            "kernel_shape Cout dimension should be multiple of 16")
        TikCheckUtil.check_in_range_by_dtype(
            kernel_shape[1], msg="kernel_shape Kh dimension should be in range of [%s, %s]" % (
                MIN_KH_W_CHANNELS, MAX_KH_W_CHANNELS), var_range=[MIN_KH_W_CHANNELS, MAX_KH_W_CHANNELS])
        TikCheckUtil.check_in_range_by_dtype(
            kernel_shape[2], msg="kernel_shape Kw dimension should be in range of [%s, %s]" % (
                MIN_KH_W_CHANNELS, MAX_KH_W_CHANNELS), var_range=[MIN_KH_W_CHANNELS, MAX_KH_W_CHANNELS])
        TikCheckUtil.check_equality(
            kernel_shape[0] * kernel_shape[4], fm_shape[0] * fm_shape[3],
            "kernel_shape channel(C1*C0) should be equal with fm_shape channel(C1*C0)")

    def _check_conv2d_params_over(self, feature_map, weight, fm_shape, kernel_shape):
        """
        check conv2d input params
        """
        # check feature_map tensor overflow
        self._check_conv2d_tensor_overflow(fm_shape, feature_map, "feature_map")
        # check weight tensor overflow
        self._check_conv2d_tensor_overflow(kernel_shape, weight, "weight")
        # check feature_map and weight overlap
        self._check_conv2d_tensor_overlap(feature_map, weight, fm_shape, kernel_shape)
