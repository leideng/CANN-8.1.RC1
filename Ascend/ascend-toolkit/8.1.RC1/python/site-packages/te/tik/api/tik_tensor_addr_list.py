#!/usr/bin/env python
# -*- coding:utf-8 -*-

"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_tensor_addr_list.py
DESC:     def TensorAddrList
CREATED:  2021-7-20 20:12:13
MODIFIED: 2021-7-29 15:04:45
"""
from collections import namedtuple

from tbe import tvm
from tbe.tvm import decl_buffer
from tbe.tvm.ir import PointerType, PrimType
from tbe.common.platform import scope_gm
from tbe.common.platform import scope_ubuf
from tbe.tik import debug
from tbe.tik.common.tik_api_map import ASCEND_310
from tbe.tik.common.tik_api_map import ASCEND_910
from tbe.tik.common.util import is_immediate_number
from tbe.tik.common.util import reduce_mul
from tbe.tik.common.common_util import is_scalar
from tbe.tik.common.common_util import is_expr
from tbe.tik.common.common_util import is_tensor
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_basic_data import BasicData
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.api.tik_scalar import Scalar
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.api.tik_tensor import Tensor
from tbe.tik.tik_lib.tik_params import MAX_INT32_VALUE
from tbe.tik.tik_lib.tik_source_info import get_span
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_tensor_ import TensorBase
from tbe.tik.tik_lib.tik_tensor_ import has_scalar
from tbe.tik.tik_lib.tik_tensor_ import normalize_shape


def _check_tensor_addr_scope(scope):
    """
    when tensoraddrlist is applied in the scope ,tensoraddrlist's scope should be checeked

    Parameters
    ----------
    scope:the tensoraddrlist's parameter scope

    Returns
    ----------
    No return
    """
    scope_list = [scope_gm, scope_ubuf]
    TikCheckUtil.check_var_in_list(scope, scope_list, "scope out of TensorAddrList Scope")


@debug.tensor_addr_register
class TensorAddrList(BasicData, TensorBase):
    """
    Defines a TensorAddrList class.
    """
    new_tensor_addr_api = namedtuple('NewTensorAddrApi', ["ir_generator", "buffer", "new_dims",
                                                          "new_strides", "new_data", "last_tensor", "scope",
                                                          "addr_offset", "is_value"])

    def __init__(self, ir_generator, size=None, scope=None, name=None, buffer_=None, dims=None, stride=None,
                 last_tensor=None, data=None, addr_offset=0, is_value=False):
        """
        tensoraddrlist register initialization
        Parameters
        ----------
        ir_generator:Halide IR generator
        size:tensor_addr_list size
        scope:tensor_addr_list scope
        name:tensor_addr_list name
        Returns
        ----------
        no return
        """
        BasicData.__init__(self, "TensorAddrList")
        TensorBase.__init__(self, ir_generator, scope=scope)
        self._fixed_attrs()
        self.addr_offset = addr_offset
        self.last_tensor = last_tensor
        self.is_value = is_value

        if (size is not None) and (scope is not None):
            TikCheckUtil.check_equality(
                isinstance(size, (int, Scalar, Expr, tvm.tir.IntImm)),
                True, "element in size should be int, Scalar or Expr.")
            if isinstance(size, (Scalar, Expr)):
                TikCheckUtil.check_equality(size.dtype.startswith(("int", "uint")), True,
                                            "When the size element is Scalar or Expr, it's dtype cannot be float")
            if is_immediate_number([size]) and size <= 0:
                TikCheckUtil.raise_error("invalid size input, size: %s" % size)

            _check_tensor_addr_scope(scope)
            # check name is valid
            TikCheckUtil.check_name_str_valid(name)
            self.name = name
            self.dimensions = normalize_shape([size])
            self.strides = self.calc_strides(self.dimensions)

            # after "*" or "+" operator,
            # the result may exceed the range of dtype
            if has_scalar([size]):
                self.data = tvm.const(0, self.dtype)
            else:
                self.data = 0
            self.is_static_shape = False
            shape = self.get_shape(self.max_mem_size, [size], self.dtype)
            # for gm tensor may have shape out of int32 range
            if isinstance(shape, int) and shape > MAX_INT32_VALUE:
                shape = tvm.const(shape, "int64")
            shape = self.convet_to_var(shape, "total_size")
            self.original_shape = self.size

            self._set_new_tensor(ir_generator, name, shape)
        elif size is None:
            TikCheckUtil.check_not_is(buffer_, None, "param buffer_ should not be None")
            self.dimensions = dims
            self.strides = stride
            self.data = data
            self.original_shape = (self.size, )
            self.is_reshape = True
            self.last_tensor = last_tensor
            self.is_getitem = True
            self.buffer = buffer_
            self.buffer_shape_dtype = self.buffer.shape[0].dtype
            name = "%s_proxy_tensor_%s" % (self.buffer.name, str(TensorBase.COUNT))
            self.name = name
        else:
            TikCheckUtil.raise_error("TensorAddrList Class must be initial by (size, name)")

    @source_info_decorator()
    def __getitem__(self, index_in):
        """
        Obtains partial tensoraddrlist data to form a new tensoraddrlist.
        Parameters
        ----------
        index_in : TensorAddrList array index.

        Returns
        ----------
        The new tensoraddrlist
        """
        self.check_tensor_scope()
        items = self.get_items(index_in)

        new_dims = self.calc_new_dims(items, self.dimensions)
        new_strides = self.calc_new_strides(new_dims, self.strides)
        new_data = self.calc_new_data(new_dims, self.strides, self.data)

        new_tensor_addr_api = self.new_tensor_addr_api(
            self.ir_generator, self.buffer, new_dims, new_strides, new_data,
            self, self.tensor_scope, self.addr_offset, self.is_value)
        return self.new_tensor_addr(new_tensor_addr_api)

    def __add__(self, other):
        TikCheckUtil.check_equality(self.is_value, True, "Offset is supported only after value.")
        new_tensor_addr_api = self.new_tensor_addr_api(
            self.ir_generator, self.buffer, self.dimensions, self.strides, self.data,
            self, self.tensor_scope, self.addr_offset + other, self.is_value)
        return self.new_tensor_addr(new_tensor_addr_api)

    def __sub__(self, other):
        TikCheckUtil.check_equality(self.is_value, True, "Offset is supported only after value.")
        new_tensor_addr_api = self.new_tensor_addr_api(
            self.ir_generator, self.buffer, self.dimensions, self.strides, self.data,
            self, self.tensor_scope, self.addr_offset - other, self.is_value)
        return self.new_tensor_addr(new_tensor_addr_api)

    @property
    def shape(self):
        """
        return tensoraddrlist shape
        Parameters
        ----------

        Returns
        ----------
        tensoraddrlist shape
        """
        return [self.size]

    @property
    @source_info_decorator()
    def size(self):
        """
        return tensoraddrlist size
        Parameters
        ----------

        Returns
        ----------
        tensoraddrlist size
        """
        tmp_size = []
        for i in self.dimensions:
            size = self.calc_slice_length(i)
            tmp_size.append(size)
        return reduce_mul(tmp_size)

    @property
    def value(self):
        """
        Returns
        ----------
        gm addr start position
        """
        self.is_value = True
        return self

    @classmethod
    def new_tensor_addr(cls, new_tensor_addr_api):
        """
        tensoraddrlist's constructive function
        Parameters
        ----------
        new_tensor_addr_api: named_tuple of new_tensor_addr_api
        """
        return cls(new_tensor_addr_api.ir_generator, buffer_=new_tensor_addr_api.buffer,
                   dims=new_tensor_addr_api.new_dims, stride=new_tensor_addr_api.new_strides,
                   data=new_tensor_addr_api.new_data, last_tensor=new_tensor_addr_api.last_tensor,
                   scope=new_tensor_addr_api.scope, addr_offset=new_tensor_addr_api.addr_offset,
                   is_value=new_tensor_addr_api.is_value)

    @source_info_decorator()
    @debug.tensor_addr_set_as_decorator
    def set_as(self, value, dst_offset=0, src_offset=None):
        """
        Sets a tensor.
        Parameters
        ----------
        value : Value to be assigned from:
        -       A tensor of one element
        -       A Scalar variable
        -       An Expr consisting of a Scalar variable and an immediate, which cannot be of type float
        dst_offset: An internal optional parameters
        src_offset: An internal optional parameters

        Returns
        -------
        no return
        """

        self.check_tensor_scope()
        if not isinstance(value, (Tensor, Scalar, Expr, int, TensorAddrList)):
            TikCheckUtil.raise_error("TensorAddrList set_as only support Tensor, TensorAddrList, Scalar, Expr or Imm.")

        if is_scalar(value) and value.dtype != "int64":
            TikCheckUtil.raise_error("value's dtype should be int64, Instead of %s" % value.dtype)
        elif is_expr(value) and value.get().dtype != "int64":
            TikCheckUtil.raise_error("value's dtype should be int64, Instead of %s" % value.get().dtype)
        elif is_tensor(value) and value.dtype != "int64":
            TikCheckUtil.raise_error("value's dtype should be int64, Instead of %s" % value.dtype)
        if is_tensor(value) and value.scope not in (scope_ubuf, scope_gm):
            TikCheckUtil.raise_error("value's scope should int [scope_ubuf, scope_gm], Instead of %s" % value.scope)

        if isinstance(value, (Tensor, TensorAddrList)) and TikSocManager.is_v100_soc() and value.scope == scope_gm:
            TikCheckUtil.raise_error("set_as's value can't be GM TensorAddrList or Tensor when chip is %s,"
                                     "%s." % (ASCEND_310, ASCEND_910))
        if self.scope != scope_ubuf:
            TikCheckUtil.raise_error("Dst TensorAddrList can only be UB Tensor but get %s TensorAddrList." % self.scope)
        self.ir_generator.assign(self, value, dst_offset, src_offset)

    def _set_new_tensor(self, ir_generator, name, shape):
        if self.tensor_scope == scope_gm:
            self.var_name = tvm.tir.expr.Var(name, PointerType(PrimType(self.dtype), scope_gm))
            self.buffer = decl_buffer(shape, self.dtype, name, data=self.var_name, scope=scope_gm)

            self.buffer_shape_dtype = self.buffer.shape[0].dtype
            self.source_loc = get_span()
        else:
            self.buffer = ir_generator.apply_for_new_alloc((self.dtype, shape, self.tensor_scope), name)
            self.buffer_shape_dtype = self.buffer.shape[0].dtype
            ir_generator.code_buffer_manager.new_tensor(self)

    def _fixed_attrs(self):
        self.max_mem_size = None
        self.dtype = "int64"
        self.is_atomic_add = False
