#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_scalar.py
DESC:     scalar
CREATED:  2019-04-18 18:53:42
MODIFIED: 2019-07-18 21:29:18
"""
from tbe import tvm
from tbe.tvm.tir import Allocate
from tbe.common.platform import scope_reg
from tbe.common.platform import scope_ca
from tbe.common.platform import scope_cb
from tbe.common.platform import scope_cc
from tbe.common.platform import scope_cbuf
from tbe.common.platform import scope_gm
from tbe.tik.common.tik_api_map import ASCEND_910
from tbe.tik.common.tik_api_map import ASCEND_310
from tbe.tik.debug.decorators import scalar_register
from tbe.tik.debug.decorators import scalar_set_as_decorator
from tbe.tik.tik_lib.tik_expr import BasicExpr
from tbe.tik.tik_lib.tik_expr import Expr
from tbe.tik.tik_lib.tik_basic_data import BasicData
from tbe.tik.tik_lib.tik_util import need_check_out_of_scope
from tbe.tik.tik_lib.tik_buffervar import TikBufferVar
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_check_util import print_error_msg
from tbe.tik.tik_lib.tik_source_info import get_span
from tbe.tik.tik_lib.tik_source_info import source_info_decorator
from tbe.tik.tik_lib.tik_scalar_ import ScalarInner
from tbe.tik.tik_lib.tik_scalar_ import InputScalarInner
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.tik_lib.tik_params import scope_cbuf_out


_SCALAR_EXTENTS = (1,)


@scalar_register
class Scalar(ScalarInner):
    """
    hint:scalar expression
    """
    # @cond
    COUNT = 0

    def __init__(self, ir_generator, dtype="int64", name="reg_buf", init_value=None, if_global_scope=False,
                 buffer_=None, offset=None, debug_var=None, available=True):
        """
        scalar register initialization
        Parameters
        ----------
        ir_generator:Halide IR generator
        dtype:tensor register data type
        name:scalar_register name
        init_value:scalar_registerinit value
        if_global_scope:global_scope
        buffer_:reg buffer
        offset:offset
        debug_var:debug var

        Returns
        ----------
        return:no return
        """
        self.__check_scalar_params(dtype, name)

        ScalarInner.__init__(self, ir_generator)

        self.instance_func = None
        self.ir_generator = ir_generator
        self._if_global_scope = if_global_scope
        self.class_type = Expr
        self.offset = 0
        self.is_tiling_scalar = False
        self.const_value = None

        # _available:The scalar state variable. when scalar variable
        # "_available" is True, this scalar can be accessed.when scalar variable
        # "_available" is False, this scalar can't be accessed and assert
        # "This Scalar is not defined in this scope."
        self._available = available
        if if_global_scope:
            Scalar.COUNT += 1
            self._name = "global_%s%s" % (name, str(Scalar.COUNT))
            buffer_ptr_type = tvm.ir.PointerType(tvm.ir.PrimType(dtype), scope_reg)
            buffer_var = tvm.var(self._name, buffer_ptr_type)
            self.debug_var = [tvm.var('scalar_debug_var_%s' % self._name, dtype)]
            self.reg_buffer = TikBufferVar(self.ir_generator, buffer_var, None, dtype)

            _instance_func_loc = get_span()

            def _instance_func(pre_stmt):
                tmp_const = tvm.const(1, dtype="uint1")
                tmp_allocate = Allocate(buffer_var, dtype, _SCALAR_EXTENTS, tmp_const, pre_stmt)
                ir_generator.source_info.set_node_span(tmp_allocate, span=_instance_func_loc)
                ir_generator.source_info.set_node_span(tmp_allocate, span=_instance_func_loc)
                return tmp_allocate

            self.instance_func = _instance_func
        else:
            # if any of below is None, new buffer will be created
            if buffer_ is None or offset is None or debug_var is None:
                Scalar.COUNT += 1
                self._name = '%s%s' % (name, str(Scalar.COUNT))
                self.debug_var = [tvm.var('scalar_debug_var_%s' % self._name, dtype)]
                self._set_reg_buffer(init_value, dtype)
                ir_generator.code_scalar_manager.new_scalar(self)
            else:
                self._name = name
                self.debug_var = debug_var
                self.reg_buffer = buffer_
                self.offset = offset

    @source_info_decorator()
    def __add__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, addition are not supported")
        return self._add(other)

    @source_info_decorator()
    def __radd__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, addition are not supported")
        return self._add(other)

    @source_info_decorator()
    def __sub__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, subtraction are not supported")
        return self._sub(other)

    @source_info_decorator()
    def __rsub__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, subtraction are not supported")
        return self._rsub(other)

    @source_info_decorator()
    def __mul__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, multiplication are not supported")
        return self._mul(other)

    @source_info_decorator()
    def __rmul__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, multiplication are not supported")
        return self._mul(other)

    @source_info_decorator()
    def __div__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, division are not supported")
        return self._div(other)

    @source_info_decorator()
    def __rdiv__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, division are not supported")
        return self._rdiv(other)

    @source_info_decorator()
    def __truediv__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, division are not supported")
        return self._div(other)

    @source_info_decorator()
    def __rtruediv__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, division are not supported")
        return self._rdiv(other)

    @source_info_decorator()
    def __floordiv__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, division are not supported")
        return self._div(other)

    @source_info_decorator()
    def __rfloordiv__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, division are not supported")
        return self._rdiv(other)

    @source_info_decorator()
    def __mod__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, modulo are not supported")
        return self._mod(other)

    @source_info_decorator()
    def __rmod__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, modulo are not supported")
        return self._rmod(other)

    @source_info_decorator()
    def __lshift__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, left shifting are not supported")
        return self._lshift(other)

    @source_info_decorator()
    def __rshift__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, right shifting are not supported")
        return self._rshift(other)

    @source_info_decorator()
    def __lt__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, comparison calculation are not supported")
        return self._lt(other)

    @source_info_decorator()
    def __le__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, comparison calculation are not supported")
        return self._le(other)

    @source_info_decorator()
    def __ne__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, comparison calculation are not supported")
        return self._ne(other)

    @source_info_decorator()
    def __gt__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, comparison calculation are not supported")
        return self._gt(other)

    @source_info_decorator()
    def __ge__(self, other):
        if self.dtype == "bfloat16":
            print_error_msg("When the scalar type is bfloat16, comparison calculation are not supported")
        return self._ge(other)


    @staticmethod
    def __check_scalar_params(dtype, name):
        """
        check scalar params

        Parameters
        ----------
        dtype: tensor register data type
        name: scalar_register name

        Returns
        -------
        None

        """
        # check dtype is str
        TikCheckUtil.check_type_match(dtype, str, "dtype should be str")
        # check name is valid
        TikCheckUtil.check_name_str_valid(name)

    @source_info_decorator()
    @scalar_set_as_decorator
    def set_as(self, value, src_offset=None):
        """
        Sets the scalar value.

        Parameters
        ----------
        value : Value to be assigned from:
        -       An immediate of type int or float
        -       A Scalar variable
        -       A Tensor value
        -       An Expr (consisting of a Scalar variable and an immediate)
        src_offset: An internal optional parameter

        Returns
        -------
        no return
        """
        msg = "Scalar %s is not defined in this scope." % self._name
        if need_check_out_of_scope(self.ir_generator):
            TikCheckUtil.check_equality(self._available, True, msg)
        self._check_param_for_set(value)
        if isinstance(value, BasicData) and (value.is_tensor() or value.is_tensor_addr()):
            if value.scope in [scope_ca, scope_cb, scope_cc, scope_cbuf, scope_cbuf_out]:
                TikCheckUtil.raise_error("set_as's value's scope can't be %s" % value.scope)
            if TikSocManager.is_v100_soc() and value.scope == scope_gm:
                TikCheckUtil.raise_error("set_as's value can't be GM Tensor when chip is %s,"
                                         "%s." % (ASCEND_310, ASCEND_910))
            self.ir_generator.assign(self, value, src_offset=src_offset)
        else:
            if isinstance(value, BasicExpr):
                val = value.astype(self.dtype).get()
            elif isinstance(value, (float, int)):
                val = tvm.const(value, self.dtype)
            else:
                val = value
            if isinstance(self.offset, int):
                self.reg_buffer[self.offset] = val
            else:
                self.reg_buffer[Expr(self.offset).get()] = val


# @cond
class InputScalar(InputScalarInner):
    """
    only use to buildcce's input
    """
    COUNT = 0

    def __init__(self, ir_generator, dtype="int64", name="reg_buf"):
        """
        init inputscalar
        Parametets:
        ir_generator:
        dtype: dtype of var
        name: name of inputscalar
        """
        TikCheckUtil.check_name_str_valid(name)
        TikCheckUtil.check_type_match(dtype, str, "dtype should be str")

        InputScalarInner.__init__(self, ir_generator)
        InputScalar.COUNT += 1
        self.ir_generator = ir_generator
        self._name = name
        self._var = tvm.var(name=self._name, dtype=dtype)
        self.scope = scope_gm
        self.shape = (1,)
        self.offset = 0
        self.debug_var = [tvm.var('scalar_debug_var_%s' % self._name, dtype)]
        ir_generator.scope_attr(self._var, "tik_scalar", self._var)
        ir_generator.input_scalar_set.add(self._var)

    def __hash__(self):
        """
        for lru_cache to compare the InputScalar whether equal
        Returns
        -------
        the id of the tvm var

        """
        return id(self._var)
