#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vnchwconv_b32_debug_.py
DESC:     this file contains many data_transform debug
CREATED:  2021-12-15 10:50:08
MODIFIED: 2021-12-15 10:50:08
"""
from collections import namedtuple
from tbe.tik.debug.sim.util import TempEnv
from tbe.tik.debug.statement import STMT
from tbe.tik.debug.data_move_intrinsic import DataMove
from tbe.tik.debug.tik_vector_ops_debug.tik_vector_data_transform_debug_ import Vnchwconv
from tbe.tik.tik_lib.tik_params import ONE_BLK_SIZE
from tbe.tik.tik_lib.tik_params import MAX_REPEAT_TIMES
from tbe.tik.tik_lib.tik_params import MAX_REP_STRIDE_DOUBLE_BYTE
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil
from tbe.tik.tik_lib.tik_vector_api.tik_params_check_fills import VnchwconvCheckParams
from tbe.tik.tik_lib.tik_data_move_api.tik_data_move_operation_ import DataMoveApi
from tbe.tik.tik_lib.tik_vector_api.tik_vector_fills_api_ import VnchwconvApi
from tbe.tik.tik_lib.tik_params import BIT_LEN_16
from tbe.tik.tik_lib.tik_params import BIT_LEN_32
from tbe.tik.tik_lib.tik_params import BIT_LEN_8


DataMoveParams = namedtuple('DataMoveApi', ['dst', 'src', 'sid', 'nburst', 'burst', 'src_stride', 'dst_stride'])
VnchwconvParams = namedtuple('VnchwconvApi', ['dst_high_half', 'src_high_half', 'dst_list', 'src_list',
                                              'repeat_times', 'dst_rep_stride', 'src_rep_stride', 'name'])


class VnchwconvB32(STMT):
    def __init__(self, source_info, op_obj, dst_tensor_list, dst_fp16_tensor_list):
        super(VnchwconvB32, self).__init__(source_info, op_obj.tik_instance.context.tik_debugger)
        # self.parameter
        self.op_obj = op_obj
        self.name = op_obj.name
        if self.name is None:
            self.name = "vnchwconv"
        self.context = None
        self.dst_tensor_list = dst_tensor_list
        self.dst_fp16_tensor_list = dst_fp16_tensor_list
        self.dst_ub_fp16_high, self.dst_ub_fp16_lower = dst_fp16_tensor_list
        self.src_list_b16 = []
        ub = self.op_obj.src_tensor_op.tensor_obj[0].reinterpret_cast_to("float16")
        for i in range(16):
            src_offset = self.op_obj.src_tensor_op.tensor_obj[i].offset
            self.src_list_b16.append(ub[src_offset * 2])

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        # Parameter declaration
        self.op_obj.dst_tensor_op.context = context
        self.op_obj.src_tensor_op.context = context
        self.op_obj.dst_tensor_op.set_repeat_times(self.op_obj.control_op.repeat_times)
        self.op_obj.src_tensor_op.set_repeat_times(self.op_obj.control_op.repeat_times)
        self.op_obj.dst_tensor_op.set_rep_stride_value()
        self.op_obj.src_tensor_op.set_rep_stride_value()
        self._check_params(context)
        # Function Implementation
        for dst_fp16_tensor in self.dst_fp16_tensor_list:
            context.add_tensor(dst_fp16_tensor, None)
        for dst_tensor in self.dst_tensor_list:
            context.add_tensor(dst_tensor, None)
        self.vnchwconv_b32_build(context)

    def vnchwconv_b32_build(self, context):
        dst_ub_list = self.op_obj.dst_tensor_op.tensor_obj
        dst_ub_b32_type = dst_ub_list[0].dtype
        repeat_times_16 = 16
        repeat_times_8 = 8
        vnchwconv_b32_sid = 0
        vnchwconv_b32_nburst_1 = 1
        vnchwconv_b32_burst = 1
        vnchwconv_b32_stride = 0
        for repeat in range(0, self.op_obj.dst_tensor_op.repeat_times_value):
            for i in range(repeat_times_16):
                self.src_list_b16[i] = self.src_list_b16[i][
                                       (repeat * self.op_obj.src_tensor_op.rep_stride_value * BIT_LEN_16):]
                dst_ub_list[i] = dst_ub_list[i][(repeat * self.op_obj.dst_tensor_op.rep_stride_value * BIT_LEN_8):]
            for i in range(repeat_times_16):
                src1_list = [self.src_list_b16[i] for j in range(repeat_times_16)]
                dst1_list = [self.dst_tensor_list[i][BIT_LEN_16 * j] for j in range(repeat_times_16)]
                vnchwconv_params = VnchwconvParams(False, False, dst1_list, src1_list, 1, 0, 0, "vnchwconv")
                self._vnchwconv_b16(context, vnchwconv_params)
            for i in range(repeat_times_8):
                src2_list_high = [self.dst_tensor_list[j // 2][(j % 2) * BIT_LEN_16 + BIT_LEN_32 * i]
                                  for j in range(repeat_times_16)]
                dst2_list_high = [self.dst_ub_fp16_high[j * BIT_LEN_16 + i * BIT_LEN_16]
                                  for j in range(repeat_times_16)]
                vnchwconv_params = VnchwconvParams(False, False, dst2_list_high, src2_list_high, 1, 0, 0, "vnchwconv")
                self._vnchwconv_b16(context, vnchwconv_params)
                src2_list_lower = [self.dst_tensor_list[BIT_LEN_8 + j // 2][(j % 2) * BIT_LEN_16 + BIT_LEN_32 * i]
                                   for j in range(repeat_times_16)]
                dst2_list_lower = [self.dst_ub_fp16_lower[j * BIT_LEN_16 + BIT_LEN_16 * i]
                                   for j in range(repeat_times_16)]
                vnchwconv_params = VnchwconvParams(False, False, dst2_list_lower, src2_list_lower, 1, 0, 0, "vnchwconv")
                self._vnchwconv_b16(context, vnchwconv_params)
            for i in range(repeat_times_8):
                dst_ub_fp32_high = self.dst_ub_fp16_high.reinterpret_cast_to(dst_ub_b32_type)
                dst_ub_fp32_lower = self.dst_ub_fp16_lower.reinterpret_cast_to(dst_ub_b32_type)
                data_move_params = DataMoveParams(dst_ub_list[2*i],
                                                  dst_ub_fp32_high[BIT_LEN_8 * i], vnchwconv_b32_sid,
                                                  vnchwconv_b32_nburst_1, vnchwconv_b32_burst, vnchwconv_b32_stride,
                                                  vnchwconv_b32_stride)
                self._data_move_b32(context, data_move_params)
                data_move_params = DataMoveParams(dst_ub_list[2*i+1],
                                                  dst_ub_fp32_lower[BIT_LEN_8 * i], vnchwconv_b32_sid,
                                                  vnchwconv_b32_nburst_1, vnchwconv_b32_burst, vnchwconv_b32_stride,
                                                  vnchwconv_b32_stride)
                self._data_move_b32(context, data_move_params)

    def _check_params(self, context):
        # check repeat
        TikCheckUtil.check_in_range_by_dtype(
            self.op_obj.dst_tensor_op.repeat_times_value,
            msg="repeat_times should be in the range of [%s, %s], input value is %s"
                % (0, MAX_REPEAT_TIMES, str(self.op_obj.dst_tensor_op.repeat_times_value)),
            var_range=[0, MAX_REPEAT_TIMES])

        # check strides
        self.op_obj.dst_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_DOUBLE_BYTE)
        self.op_obj.src_tensor_op.check_tensor_op_rep_stride(MAX_REP_STRIDE_DOUBLE_BYTE)

        # check address overlapping
        msg = "dst_list and src_list"
        params_list = (self.op_obj.dst_tensor_op, self.op_obj.src_tensor_op, self.op_obj.control_op,
                       self.op_obj.dst_high_half, self.op_obj.src_high_half)
        VnchwconvCheckParams(self.name, params_list).check_vnchwconv_overlap(msg, self.op_obj.dst_high_half,
                                                                             self.op_obj.src_high_half, context)
        temp_env = TempEnv()
        src_read_offset = 0
        dst_write_offset = 0
        if self.op_obj.dst_tensor_op.repeat_times_value == 1:
            src_read_offset = self.op_obj.src_tensor_op.rep_stride_value * ONE_BLK_SIZE
            dst_write_offset = self.op_obj.dst_tensor_op.rep_stride_value * ONE_BLK_SIZE
        src_tensor_nums = len(self.op_obj.src_tensor_op.tensor_obj)
        dst_tensor_nums = len(self.op_obj.dst_tensor_op.tensor_obj)
        temp_env.check_mem_access_vnchwconv(context.model, tensor_nums_list=[src_tensor_nums, dst_tensor_nums],
                                            src_read_offset=src_read_offset, dst_store_offset=dst_write_offset)

    def _data_move_b32(self, context, data_move_params):
        data_move_all_obj = DataMoveApi(self.op_obj.tik_instance, data_move_params, None, None)
        data_move_part = DataMove(self.source_info, data_move_all_obj)
        data_move_part.eval_(context)

    def _vnchwconv_b16(self, context, vnchwconv_params):
        vnchwconv_b32_obj = VnchwconvApi(self.op_obj.tik_instance, vnchwconv_params)
        vnchwocov_b32_part = Vnchwconv(self.source_info, vnchwconv_b32_obj)
        vnchwocov_b32_part.eval_(context)
