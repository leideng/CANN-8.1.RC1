#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2019. Huawei Technologies Co., Ltd. All rights reserved.
FILE:     tik_vector_cube_debug_.py
DESC:     debug intrinsic
CREATED:  2021-12-07 9:53:13
MODIFIED: 2021-12-07 9:53:45
"""
import sys
import math

from tbe.tik.common.util import ceil_div
from tbe.tik.common.util import DTYPE_SIZE
from tbe.tik.common.common_util import check_depthwise_conv_l1_w
from tbe.tik.common.common_util import get_l0c_align
from tbe.tik.common.common_util import check_param_type_range
from tbe.tik.common.common_util import reduce_mul
from tbe.tik.tik_lib.tik_soc_manager import TikSocManager
from tbe.tik.common.common_check_func import check_depthwise_conv_params
from tbe.tik.debug.statement import STMT
from tbe.tik.debug.util import copy_tensor_to_model
from tbe.tik.debug.util import model_with_env
from tbe.tik.debug.util import cvt_float_to_uint
from tbe.tik.debug.sim.util import TempEnv
from tbe.tik.tik_lib.tik_params import ALIGN_TENSOR
from tbe.tik.tik_lib.tik_params import MIN_MATRIX
from tbe.tik.tik_lib.tik_params import MMAD_MATRIX_K_POS
from tbe.tik.tik_lib.tik_params import MMAD_MATRIX_N_POS
from tbe.tik.tik_lib.tik_params import MMAD_EN_WINOGRAD_A_POS
from tbe.tik.tik_lib.tik_params import MMAD_EN_WINOGRAD_B_POS
from tbe.tik.tik_lib.tik_params import MMAD_EN_WEIGHT_OFFSET_POS
from tbe.tik.tik_lib.tik_params import MMAD_EN_SSPARSE_POS
from tbe.tik.tik_lib.tik_params import MMAD_L0C_BIT_POS
from tbe.tik.tik_lib.tik_params import MAX_MATRIX
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_59
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_55
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_63
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_24
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_44
from tbe.tik.tik_lib.tik_params import SHIFT_BIT_POS_12
from tbe.tik.tik_lib.tik_params import ALIGN_DST
from tbe.tik.tik_lib.tik_check_util import TikCheckUtil

_WINOGRAD_TYPE_ID = {
    'int8': 1,
    "int16": 5
}

_MMAD_TYPE_BITS = {
    ('uint8', 'uint8', 'uint32'): 0b000,
    ('uint8', 'uint8', 'int32'): 0b000,
    ('int8', 'int8', 'int32'): 0b001,
    ('float16', 'float16', 'float16'): 0b010,
    ('float16', 'float16', 'float32'): 0b011,
    ('uint8', 'int8', 'int32'): 0b101,
    ('float32', 'float32', 'float32'): 0b010
}

_PAD_VALUE_SHIFT_BIT_POS = 8
_FEATURE_OFFSET_SHIFT_POS = 24
_DEPTH_L1_H_SHIFT_POS = 12
_PAD_MODE_SHIFT_POS = 62


class MMAD(STMT):
    """
    MMAD instruction
    """

    def __init__(self, source_info, mmad_api, tik_debugger=None):
        super(MMAD, self).__init__(source_info, tik_debugger)
        self.dst = mmad_api.dst_fm
        self.tensor_a = mmad_api.src_fm
        self.tensor_b = mmad_api.src_filter
        self.fm_offset = mmad_api.fm_offset
        self.en_weight_offset = int(mmad_api.en_weight_offset)
        self.smask = 0 if (mmad_api.smask is None) else mmad_api.smask
        self.en_small_channel = int(mmad_api.en_small_channel)
        self.en_small_k = int(mmad_api.en_small_k)
        self.en_ssparse = int(mmad_api.en_ssparse)
        self.en_winograd_a = int(mmad_api.en_winograd_a)
        self.en_winograd_b = int(mmad_api.en_winograd_b)

        self.matrix_m = mmad_api.matrix_m
        self.matrix_k = mmad_api.matrix_k
        self.matrix_n = mmad_api.matrix_n

        self.init_by_l0c = mmad_api.is_bias
        self.bias_tensor_bt = mmad_api.bias_tensor_bt

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        dst_type = self.dst.dtype
        src_type = self.tensor_a.dtype

        temp_env = TempEnv(is_aic_api=True)

        xn_idx, _, _, _ = copy_tensor_to_model(
            context, temp_env, self.tensor_a, ALIGN_TENSOR, access_mode='r')
        xm_idx, _, _, _ = copy_tensor_to_model(
            context, temp_env, self.tensor_b, ALIGN_TENSOR, access_mode='r')

        dst_align = get_l0c_align(self.dst)
        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.dst, dst_align, access_mode='rw')
        if self.bias_tensor_bt is not None:
            l0c_addr = model_with_env(context.model.read_gpr, temp_env, xd_idx)
            bias_tensor_align = 64  # dst is BT, bt align is 64Bytes
            xd_idx_bias, _, _, _ = copy_tensor_to_model(
                context, temp_env, self.bias_tensor_bt, bias_tensor_align, access_mode='r')
            bias_addr = model_with_env(context.model.read_gpr, temp_env, xd_idx_bias)
            l0c_addr |= bias_addr << 32
            bias_addr = model_with_env(context.model.write_gpr, temp_env, xd_idx, l0c_addr)

        param = context.encoder.new_param()

        param.type = _MMAD_TYPE_BITS.get((src_type, self.tensor_b.dtype, dst_type))
        # f32tof32, data_type uses 4 bits. Bits 2-0 are set to bits 24-22 of instr, and bit 3 is set to bit 0 of instr.
        if src_type == "float32" and dst_type == "float32":
            param.stVal0 = 0b1

        param.xd = xd_idx
        param.xn = xn_idx
        param.xm = xm_idx
        param.xt = self.create_gpr_x_t(context, temp_env)

        instr = context.encoder.gen_mmad(param)

        model_with_env(context.model.step, temp_env, instr)
        temp_env.check_mem_access(context.model)

        model_with_env(context.model.read_memory, temp_env, dst_addr, self.dst.scope, dst_ptr, dst_alloc_size)

    def create_gpr_x_t(self, context, temp_env):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context

        temp_env : the temp environment

        Returns
        -------
        xt_idx
        """
        matrix_m = context.evaluate_expr(self.matrix_m)
        matrix_k = context.evaluate_expr(self.matrix_k)
        matrix_n = context.evaluate_expr(self.matrix_n)
        # check matrix_* param
        TikCheckUtil.check_in_range_by_dtype(
            matrix_m, msg="matrix_m should be in range of [%d, %d], input value is %s"
                          % (MIN_MATRIX, MAX_MATRIX, str(matrix_m)), var_range=[MIN_MATRIX, MAX_MATRIX])
        TikCheckUtil.check_in_range_by_dtype(
            matrix_n, msg="matrix_n should be in range of [%d, %d], input value is %s"
                          % (MIN_MATRIX, MAX_MATRIX, str(matrix_n)), var_range=[MIN_MATRIX, MAX_MATRIX])
        TikCheckUtil.check_in_range_by_dtype(
            matrix_k, msg="matrix_k should be in range of [%d, %d], input value is %s"
                          % (MIN_MATRIX, MAX_MATRIX, str(matrix_k)), var_range=[MIN_MATRIX, MAX_MATRIX])
        TikCheckUtil.check_var_in_list(context.evaluate_expr(self.init_by_l0c), [0, 1], msg="is_bias should be 0 or 1")

        xt_idx = temp_env.alloc_register()

        if context.evaluate_expr(self.init_by_l0c) or self.bias_tensor_bt is not None:
            l0c_bit = 0
        else:
            l0c_bit = 1

        x_t = matrix_m
        x_t |= matrix_k << MMAD_MATRIX_K_POS
        x_t |= matrix_n << MMAD_MATRIX_N_POS

        # fm offset [43:36]
        bit_pos, bit_len = 36, 8
        _max_fm_offset = int(math.pow(2, bit_len) - 1)-1
        TikCheckUtil.check_in_range_by_dtype(
            self.fm_offset, var_range=[0, _max_fm_offset])
        x_t |= (self.fm_offset << bit_pos)

        # check smask is in [50:44]
        bit_pos, bit_len = 44, 7
        _max_smask = int(math.pow(2, bit_len) - 1)-1
        TikCheckUtil.check_in_range_by_dtype(
            self.smask, var_range=[0, _max_smask])
        if self.smask is not None:
            x_t |= (self.smask << bit_pos)

        if TikSocManager.is_910b_soc() or TikSocManager.is_310b_610l_soc():
            bias_table_enable_pos = 62
            if self.bias_tensor_bt is not None:  # bias bt enable
                x_t |= 1 << bias_table_enable_pos
        else:
            # en_winograd_a
            x_t |= (self.en_winograd_a << MMAD_EN_WINOGRAD_A_POS)
            # en_winograd_b
            x_t |= (self.en_winograd_b << MMAD_EN_WINOGRAD_B_POS)

        # en_ssparse
        x_t |= (self.en_ssparse << MMAD_EN_SSPARSE_POS)
        # en_weight_matrix_offset
        x_t |= (self.en_weight_offset << MMAD_EN_WEIGHT_OFFSET_POS)

        # C inital value control
        x_t |= l0c_bit << MMAD_L0C_BIT_POS
        if self.en_small_channel != 0 or self.en_small_k != 0:
            sys.stderr.write(
                "[INFO]: small-channel & small-k are not supported in debug flow yet!\n"
            )

        model_with_env(context.model.write_gpr, temp_env, xt_idx, x_t)

        return xt_idx


class DepthwiseConv(STMT):
    """
    DepthwiseConv instruction
    """

    def __init__(self, source_info, depthwise_conv_api, tik_debugger):

        super(DepthwiseConv, self).__init__(source_info, tik_debugger)
        self.dst = depthwise_conv_api.dst_fm
        self.feature_map = depthwise_conv_api.src_fm
        self.weight = depthwise_conv_api.src_filter
        self.pad_mode = depthwise_conv_api.pad_mode
        self.l1_h = depthwise_conv_api.l1_h
        self.l1_w = depthwise_conv_api.l1_w
        self.store_high_half = depthwise_conv_api.store_high_half
        self.feature_offset = depthwise_conv_api.feature_offset
        self.weight_offset = depthwise_conv_api.weight_offset
        self.pad_value = depthwise_conv_api.pad_value

    def eval_(self, context):
        """
        run the instruction

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        # check params
        check_depthwise_conv_params(
            self.feature_map, context.evaluate_expr(self.pad_mode),
            context.evaluate_expr(self.l1_h), context.evaluate_expr(self.l1_w),
            context.evaluate_expr(self.feature_offset))
        # check l1_w
        check_depthwise_conv_l1_w(context.evaluate_expr(self.pad_mode),
                                  context.evaluate_expr(self.l1_w))

        temp_env = TempEnv()

        dst_align = 1024

        if self.dst.dtype == 'float16':
            dst_align = 512

        xn_idx, _, _, _ = copy_tensor_to_model(
            context, temp_env, self.feature_map, 16, access_mode='r')
        xm_idx, _, _, _ = copy_tensor_to_model(
            context, temp_env, self.weight, 512, access_mode='r')

        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.dst, dst_align, access_mode='w')

        param = context.encoder.new_param()
        param.type = _MMAD_TYPE_BITS.get((self.feature_map.dtype, self.weight.dtype, self.dst.dtype))
        param.xd = xd_idx
        param.xn = xn_idx
        param.xm = xm_idx
        param.xt = self.create_gpr_x_t(context, temp_env)

        instr = context.encoder.gen_dp(param)
        # manually add #h param since encoder is not the latest ISA version

        instr |= self.store_high_half

        self.set_spr_padding(context)
        context.model.step(instr)

        dst_store_offset = 0
        if self.weight.dtype == "float16" and self.store_high_half is True:  # save data in high 8 channel
            dst_store_offset = 8 * DTYPE_SIZE.get(self.dst.dtype)
        temp_env.check_mem_access_depthwise_conv(context.model, dst_store_offset=dst_store_offset)
        context.model.read_memory(dst_addr, self.dst.scope, dst_ptr, dst_alloc_size)

    def set_spr_padding(self, context):
        """
        set spr PADDING

        Parameters
        ----------
        context : the stack context

        Returns
        -------
        None
        """
        if self.pad_value is not None:
            pad_value = context.evaluate_expr(self.pad_value)

            if context.evaluate_expr(self.pad_mode) != 0:
                if self.feature_map.dtype == 'float16':
                    spr_pad_value = cvt_float_to_uint('float16', pad_value)
                else:
                    spr_pad_value = pad_value
                    spr_pad_value |= pad_value << _PAD_VALUE_SHIFT_BIT_POS
                context.model.write_spr('PADDING', spr_pad_value)

    def create_gpr_x_t(self, context, temp_env):
        """
        create general purpose register x_t

        Parameters
        ----------
        context : the stack context
        temp_env : the temp environment

        Returns
        -------
        xt_idx
        """
        l1_h = context.evaluate_expr(self.l1_h)
        l1_w = context.evaluate_expr(self.l1_w)
        feature_offset = context.evaluate_expr(self.feature_offset)
        if feature_offset < 0:
            feature_offset = cvt_float_to_uint("int8", feature_offset)
        pad_mode = context.evaluate_expr(self.pad_mode)

        xt_idx = temp_env.alloc_register()
        x_t = l1_w
        x_t |= l1_h << _DEPTH_L1_H_SHIFT_POS
        x_t |= feature_offset << _FEATURE_OFFSET_SHIFT_POS
        x_t |= pad_mode << _PAD_MODE_SHIFT_POS

        context.model.write_gpr(xt_idx, x_t)

        return xt_idx


class WinogradConv(STMT):
    """
    winograd instruction
    """

    def __init__(self, source_info, winograd_conv_api, tik_debugger):
        super(WinogradConv, self).__init__(source_info, tik_debugger)
        self.dst = winograd_conv_api.dst
        self.src_fm = winograd_conv_api.src_fm
        self.src_filter = winograd_conv_api.src_filter
        self.matrix_m = winograd_conv_api.matrix_m
        self.matrix_k = winograd_conv_api.matrix_k
        self.matrix_n = winograd_conv_api.matrix_n
        self.is_bias = winograd_conv_api.is_bias

    def eval_(self, context):
        """
        eval the printf statement.
        Parameters
        ----------
        context: the stack context.

        Returns
        -------
        no return
        """
        temp_env = TempEnv()
        xn_idx, _, _, _ = copy_tensor_to_model(context, temp_env, self.src_fm, ALIGN_TENSOR, access_mode='r')
        xm_idx, _, _, _ = copy_tensor_to_model(context, temp_env, self.src_filter, ALIGN_TENSOR, access_mode='r')
        if self.is_bias == 0:
            dst_access_mode = "rw"
        else:
            dst_access_mode = "w"
        xd_idx, dst_addr, dst_alloc_size, dst_ptr = copy_tensor_to_model(
            context, temp_env, self.dst, ALIGN_DST, access_mode=dst_access_mode)

        param = context.encoder.new_param()
        param.xd = xd_idx
        param.xn = xn_idx
        param.xm = xm_idx
        param.xt = self.create_gpr_x_t(context, temp_env)
        param.type = _WINOGRAD_TYPE_ID.get(self.src_fm.dtype)

        instr = context.encoder.gen_dma_winograd_conv(param)

        context.model.step(instr)
        temp_env.check_mem_access(context.model)
        context.model.read_memory(dst_addr, self.dst.scope, dst_ptr, dst_alloc_size)

    def create_gpr_x_t(self, context, temp_env):
        """
        create register x_t

        Parameters
        ----------
        context : the stack context

        temp_env : the temp environment

        Returns
        -------
        xm_idx
        """
        matrix_m = context.evaluate_expr(self.matrix_m)
        matrix_k = context.evaluate_expr(self.matrix_k)
        matrix_n = context.evaluate_expr(self.matrix_n)
        check_param_type_range(
            [matrix_m, matrix_n],
            [0, 0],
            [4095, 4095],
            ["matrix_m", "matrix_n"], "winograd_conv")
        src_c0 = 32 // DTYPE_SIZE[self.src_fm.dtype]
        # k should be multiple of c0
        if matrix_k != src_c0:
            TikCheckUtil.raise_error(
                "matrix_k should be equal to C0, input "
                "matrix_k: {}, C0: {}"
                    .format(matrix_k, src_c0))
        if matrix_m != 0:
            dst_expected_ele = ceil_div(matrix_m, 16) * 16 * matrix_k * 16 + self.src_fm.offset
            dst_actual_ele = reduce_mul(self.src_fm.original_shape)
            TikCheckUtil.check_ge(
                dst_actual_ele, dst_expected_ele,
                "src_fm tensor overflow, expected src_fm shape: {}, actual src_fm shape: {}"
                    .format(dst_expected_ele, dst_actual_ele))
            if matrix_n != 0:
                dst_expected_ele = ceil_div(matrix_m, 16) * 16 * \
                                   ceil_div(matrix_n, 16) * 16 * 4 + self.dst.offset
                dst_actual_ele = reduce_mul(self.dst.original_shape)
                TikCheckUtil.check_ge(
                    dst_actual_ele, dst_expected_ele,
                    "dst tensor overflow, expected dst shape: {}, actual dst shape: {}"
                        .format(dst_expected_ele, dst_actual_ele))
        if matrix_n != 0:
            dst_expected_ele = matrix_k * ceil_div(matrix_n, 16) * 16 \
                               * 16 + self.src_filter.offset
            dst_actual_ele = reduce_mul(self.src_filter.original_shape)
            TikCheckUtil.check_ge(
                dst_actual_ele, dst_expected_ele,
                "src_filter tensor overflow, expected src_filter shape: {}, "
                "actual src_filter shape: {}"
                    .format(dst_expected_ele, dst_actual_ele))
        xt_idx = temp_env.alloc_register()
        x_t = matrix_m
        x_t |= matrix_k << SHIFT_BIT_POS_12
        x_t |= matrix_n << SHIFT_BIT_POS_24
        x_t |= 0 << SHIFT_BIT_POS_44
        x_t |= 0 << SHIFT_BIT_POS_55
        x_t |= 1 << SHIFT_BIT_POS_59
        x_t |= self.is_bias << SHIFT_BIT_POS_63
        context.model.write_gpr(xt_idx, x_t)

        return xt_idx
