#!/usr/bin/env python3
# -*- coding:utf-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Define the main function of generate schedule by cheque
"""
import re
from collections import namedtuple
from typing import Union
from typing import Tuple
from typing import List
from typing import Any
from functools import reduce

import tbe
from tbe import tvm
from tbe.dsl.base import operation
from tbe.dsl.instrinsic import cce_emitinsn_params
from tbe.dsl.base.operation import add_build_arg
from tbe.common.utils import log
from tbe.common.rl_bank.bank_cfg import INTRIN_MAP
from tbe.common.rl_bank.bank_cfg import SCOPE_DICT
from tbe.common.rl_bank.bank_cfg import MODE_RUNTIME
from tbe.common.rl_bank.bank_cfg import ScheduleTarget
from tbe.common.rl_bank.bank_cfg import Axis
from tbe.common.rl_bank.bank_cfg import PRIMITIVE_DICT
from tbe.common.rl_bank.bank_cfg import TBE_COMPILE_PARA_DICT
from tbe.common.rl_bank.bank_cfg import BUILD_CONFIG_DICT

REDUCE_OPT_MODE_MAP = {
    "entire_reduce": 0,
    "dichotomy_reduce": 1
}
TAIL_STRATEGY = {
    0: "round_up",
    1: "guard_with_if"
}
GenCodeLinesParam = namedtuple("GenCodeLinesParam",
                               ["args", "sch", "sch_targets", "primitive", "stage_index", "mode", "tvm_call_extern"])
VIRTUAL_LEAF_OUT_TAG = "elewise_empty_intrin"


def proc_cache_read(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_cache_read
    :param stage_index:
    :param primitive:
    :param args:
    :param sch_targets:
    :param sch:
    :param mode:
    :param code_lines:
    :return:
    """
    if input_param.primitive != 0:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    # cache Read args  is Scope and Consumers
    scope_id = input_param.args[0]
    scope = SCOPE_DICT[scope_id]
    consumers_indicies = input_param.args[1]
    consumers = [input_param.sch_targets[i].obj for i in consumers_indicies]
    consumer_names = ', '.join([input_param.sch_targets[i].name for i in consumers_indicies])

    readed_tensor = None
    if input_param.mode == MODE_RUNTIME:
        readed_tensor = input_param.sch.cache_read(sch_target.obj, scope, consumers)
    # orignal Tensor name x，cacheRead Tensor name：x_l_n
    read_index = 0
    readed_pattern = r'^%s_l_\d+$' % sch_target.name
    tmp_name = input_param.sch_targets[input_param.stage_index + 1].name
    if input_param.stage_index + 1 < len(input_param.sch_targets) and re.match(readed_pattern, tmp_name):
        read_index = int(tmp_name.split('_')[-1]) + 1
    readed_name = '%s_l_%03d' % (sch_target.name, read_index)
    # insert after original tensor
    input_param.sch_targets.insert(input_param.stage_index + 1, ScheduleTarget(readed_name, readed_tensor, []))
    code_line = "%s = sch.cache_read(%s, '%s', [%s])" % (readed_name, sch_target.name, scope,
                                                         consumer_names)
    code_lines.append(code_line)


def proc_cache_write(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_cache_write
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 1:
        return

    # cache write args is Scope
    scope_id = input_param.args[0]
    scope = SCOPE_DICT[scope_id]

    stage_index_used = input_param.stage_index
    if isinstance(input_param.stage_index, list):
        stage_index_used = input_param.stage_index[0]
    sch_target = input_param.sch_targets[stage_index_used]
    stage_name = sch_target.name

    if isinstance(input_param.stage_index, list):
        # cheque form is [[6, 2], 1, 1] when more than one tensors do cache_write
        written_tensors = [None]
        write_tensor_objs = []
        write_tensor_names = []
        written_tensor_names = []
        write_tensor_nums = input_param.stage_index[1]
        for idx in range(write_tensor_nums):
            write_tensor_objs.append(sch_target.obj.op.output(idx))
            write_tensor_names.append(stage_name + "_v%s" % idx)
            written_tensor_names.append(stage_name + "_v%s_l" % idx)
        if input_param.mode == MODE_RUNTIME:
            written_tensors = input_param.sch.cache_write(write_tensor_objs, scope)

        written_name = '%s_l' % stage_name
        # insert before original tensor
        input_param.sch_targets.insert(stage_index_used, ScheduleTarget(written_name, written_tensors[0], []))
        code_lines.append(
            "%s = sch.cache_write([%s], '%s')" %
            (', '.join(written_tensor_names), ', '.join(write_tensor_names), scope))
        code_lines.append('%s = %s' % (written_name, written_tensor_names[0]))

    else:
        written_tensor = None
        written_name = '%s_l' % stage_name
        if len(input_param.args) > 1:
            strategy =  TAIL_STRATEGY.get(input_param.args[1])
            written_tensor = input_param.sch.cache_write(sch_target.obj, scope, strategy)
            code_line = f"{written_name} = sch.cache_write({sch_target.name}, '{scope}', '{strategy}')"
        else:
            if input_param.mode == MODE_RUNTIME:
                written_tensor = input_param.sch.cache_write(sch_target.obj, scope)
            # Tensor name x, after x_l_n
            code_line = f"{written_name} = sch.cache_write({sch_target.name}, '{scope}')"
            if code_line in code_lines:
                written_name = f"{written_name}_000"
                code_line = f"{written_name} = sch.cache_write({sch_target.name}, '{scope}')"
        # insert before orignal tensor
        input_param.sch_targets.insert(input_param.stage_index, ScheduleTarget(written_name, written_tensor, []))
        code_lines.append(code_line)


def proc_preload(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_preload
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 20:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].preload()
    code_lines.append("sch[%s].preload()" % sch_target.name)


def proc_double_buffer(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_double_buffer
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 2:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].double_buffer()
    code_lines.append("sch[%s].double_buffer()" % sch_target.name)


def proc_compute_inline(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_compute_inline
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 3:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].compute_inline()
    code_lines.append("sch[%s].compute_inline()" % sch_target.name)


def proc_get_axis(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_get_axis
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 4:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    # When the axes of a stage are counted for multiple times, the axes need to be cleared.
    sch_target.axes = []
    axis_num = input_param.args[0]
    for i in range(axis_num):
        axis_obj = None
        if input_param.mode == MODE_RUNTIME:
            axis_obj = input_param.sch[sch_target.obj].op.axis[i]
        axis_name = '%s_axis_%d' % (sch_target.name, i)
        sch_target.axes.append(Axis(axis_name, axis_obj))
        code_lines.append("%s = sch[%s].op.axis[%d]" % (axis_name, sch_target.name, i))


def proc_get_reduce_axis(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_get_reduce_axis
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 5:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    axis_num = input_param.args[0]
    for i in range(axis_num):
        axis_obj = None
        if input_param.mode == MODE_RUNTIME:
            axis_obj = input_param.sch[sch_target.obj].op.reduce_axis[i]
        axis_name = '%s_reduce_axis_%d' % (sch_target.name, i)
        sch_target.axes.append(Axis(axis_name, axis_obj))
        code_lines.append("%s = sch[%s].op.reduce_axis[%d]" % (axis_name, sch_target.name, i))


def proc_split(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_split
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 6:
        return
    # Split by Factor
    sch_target = input_param.sch_targets[input_param.stage_index]
    # SplitByFactor args is axis_index and Factor
    axis_index = input_param.args[0]

    factor = None
    if isinstance(input_param.args[1], str):
        # generate var, and set to context
        factor = operation.var_inner(input_param.args[1], (1, None))
    elif isinstance(input_param.args[1], int):
        factor = input_param.args[1]

    # delete split axis
    proc_axis = sch_target.axes.pop(axis_index)
    axis_name = proc_axis.name
    axis_obj = proc_axis.obj
    outer, inner = None, None
    if input_param.mode == MODE_RUNTIME:
        if len(input_param.args) > 2:
            tail_strategy = TAIL_STRATEGY[input_param.args[2]]
            outer, inner = input_param.sch[sch_target.obj].split(axis_obj, factor=factor, tail_strategy=tail_strategy)
            code_lines.append("%s_o, %s_i = sch[%s].split(%s, factor=%s, tail_strategy=%s)" %
                              (axis_name, axis_name, sch_target.name, axis_name, factor, tail_strategy))
        else:
            outer, inner = input_param.sch[sch_target.obj].split(axis_obj, factor=factor)
            code_lines.append("%s_o, %s_i = sch[%s].split(%s, factor=%s)" %
                              (axis_name, axis_name, sch_target.name, axis_name, factor))
    # insert inner then outer
    sch_target.axes.insert(axis_index, Axis("%s_i" % axis_name, inner))
    sch_target.axes.insert(axis_index, Axis("%s_o" % axis_name, outer))


def proc_nparts(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_nparts
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 7:
        return
    # Split by Nparts
    sch_target = input_param.sch_targets[input_param.stage_index]
    # SplitByFactor args is axis index and nparts
    axis_index = input_param.args[0]
    nparts = input_param.args[1]
    # delete split axis
    proc_axis = sch_target.axes.pop(axis_index)
    axis_name = proc_axis.name
    axis_obj = proc_axis.obj
    outer, inner = None, None
    if input_param.mode == MODE_RUNTIME:
        if len(input_param.args) > 2:
            tail_strategy = TAIL_STRATEGY[input_param.args[2]]
            outer, inner = input_param.sch[sch_target.obj].split(axis_obj, nparts=nparts, tail_strategy=tail_strategy)
            code_lines.append("%s_o, %s_i = sch[%s].split(%s, nparts=%s, tail_strategy=%s)" %
                              (axis_name, axis_name, sch_target.name, axis_name, nparts, tail_strategy))
        else:
            outer, inner = input_param.sch[sch_target.obj].split(axis_obj, nparts=nparts)
            code_lines.append("%s_o, %s_i = sch[%s].split(%s, nparts=%s)" %
                              (axis_name, axis_name, sch_target.name, axis_name, nparts))
    # insert inner,then outer
    sch_target.axes.insert(axis_index, Axis("%s_i" % axis_name, inner))
    sch_target.axes.insert(axis_index, Axis("%s_o" % axis_name, outer))


def proc_reorder(input_param: namedtuple, code_lines: list, last_primitive: int) -> None:
    """
    proc_reorder
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :param last_primitive
    :return:
    """
    def _is_trs_reorder() -> bool:
        """
        flag_1: last proc is cache_rw
        flag-2: reorder_list len = all axes len
        :return:
        """
        trs_reorder_flag_1 = last_primitive in [0, 1]
        op_axis = input_param.sch[input_param.sch_targets[input_param.stage_index].obj].op.axis
        trs_reorder_flag_2 = len(input_param.args[0]) == len(op_axis)
        return trs_reorder_flag_1 and trs_reorder_flag_2

    if input_param.primitive != 8:
        return

    sch_target = input_param.sch_targets[input_param.stage_index]
    order = input_param.args[0]
    # trs Reorder
    if _is_trs_reorder():
        trs_reorder_axes = []
        trs_reorder_axes_name = []
        for i in range(len(input_param.sch[sch_target.obj].op.axis)):
            axis_obj = input_param.sch[sch_target.obj].op.axis[i]
            trs_reorder_axes.append(axis_obj)
            trs_reorder_axes_name.append('%s.op.axis[%d]' % (sch_target.name, i))

        axis_reorder_list = [trs_reorder_axes[i] for i in order]
        axis_reorder_name_list = [trs_reorder_axes_name[i] for i in order]

        if input_param.mode == MODE_RUNTIME:
            input_param.sch[sch_target.obj].reorder(*axis_reorder_list)
        new_order_str = ', '.join(axis_reorder_name_list)
        code_lines.append("sch[%s].reorder(%s,)" % (sch_target.name, new_order_str))

        return

    # normal Reorder
    left_axis_idx_list = list(range(len(sch_target.axes)))
    for axis_idx in order:
        left_axis_idx_list.remove(axis_idx)
    axis_reorder_list = [sch_target.axes[i] for i in order]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].reorder(*([axis.obj for axis in axis_reorder_list]))
    sch_target.axes = [sch_target.axes[i] for i in order + left_axis_idx_list]
    new_order_str = ', '.join([axis.name for axis in axis_reorder_list])
    code_lines.append("sch[%s].reorder(%s,)" % (sch_target.name, new_order_str))


def proc_allocate_at(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_allocate_at
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 21:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    at_stage_index = input_param.args[0]
    at_axis_index = input_param.args[1]
    run_once_axes_index_list = input_param.args[2]
    at_sch_target = input_param.sch_targets[at_stage_index]
    at_axis = at_sch_target.axes[at_axis_index]
    run_once_axes_list = [at_sch_target.axes[i] for i in run_once_axes_index_list]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].allocate_at(input_param.sch[at_sch_target.obj], at_axis.obj,
                                                    [axis.obj for axis in run_once_axes_list])
    run_once_axes_str = ""
    if run_once_axes_list:
        run_once_axes_str = ", [%s]" % ", ".join([axis.name for axis in run_once_axes_list])
    code_lines.append("sch[%s].allocate_at(sch[%s], %s%s)" %
                      (sch_target.name, at_sch_target.name, at_axis.name, run_once_axes_str))


def proc_mem_unique(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_mem_unique
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 22:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].mem_unique()
    code_lines.append("sch[%s].mem_unique()" % sch_target.name)


def proc_compute_at(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_compute_at
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 9:
        return
    # compute at
    sch_target = input_param.sch_targets[input_param.stage_index]
    at_stage_index = input_param.args[0]
    at_axis_index = input_param.args[1]
    at_sch_target = input_param.sch_targets[at_stage_index]
    at_axis = at_sch_target.axes[at_axis_index]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].compute_at(input_param.sch[at_sch_target.obj], at_axis.obj)
    code_lines.append("sch[%s].compute_at(sch[%s], %s)" %
                      (sch_target.name, at_sch_target.name, at_axis.name))


def proc_fuse(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_fuse
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 15:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    fuse_axis_idx_list = input_param.args[0]
    fuse_axis_obj_list = [sch_target.axes[i].obj for i in fuse_axis_idx_list]

    fuse_axis_name_list = [sch_target.axes[i].name for i in fuse_axis_idx_list]
    axis_type = "axis"
    if "reduce_axis" in fuse_axis_name_list[0]:
        axis_type = "reduce_axis"
    code_lines.append(
        "%s_%s_fused_0 = sch[%s].fuse(%s)" %
        (sch_target.name, axis_type, sch_target.name, ", ".join(fuse_axis_name_list)))

    if input_param.mode == MODE_RUNTIME:
        fused_axis_obj = input_param.sch[sch_target.obj].fuse(*fuse_axis_obj_list)
        fuse_start_idx = min(fuse_axis_idx_list)
        for _ in fuse_axis_idx_list:
            sch_target.axes.pop(fuse_start_idx)
        sch_target.axes.insert(
            fuse_start_idx, Axis("%s_%s_fused_0" % (sch_target.name, axis_type),
                                 fused_axis_obj))


def proc_rfactor(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_rfactor
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 17:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    rfactor_name = sch_target.name + "_rfactor"
    rfactor_axis = sch_target.axes[input_param.args[0]]
    factor_axis = input_param.args[1]
    if len(input_param.args) > 2:
        num2mode = {1: input_param.sch.RfactorModeOverlap, 2: input_param.sch.RfactorModeNormal }
        rfactor_mode = num2mode.get(input_param.args[2])
        code_lines.append("%s = sch.rfactor(%s, %s, factor_axis=%s, mode=%s)" %
                          (rfactor_name, sch_target.name, rfactor_axis.name, factor_axis, rfactor_mode))
        tensor_rfactor = input_param.sch.rfactor(sch_target.obj, rfactor_axis.obj, factor_axis,
                                                 mode=rfactor_mode)
    else:
        code_lines.append("%s = sch.rfactor(%s, %s, factor_axis=%s)" %
                          (rfactor_name, sch_target.name, rfactor_axis.name, factor_axis))
        tensor_rfactor = input_param.sch.rfactor(sch_target.obj, rfactor_axis.obj, factor_axis)

    if not isinstance(tensor_rfactor, tvm.Tensor):
        tensor_rfactor = tensor_rfactor[0]
    input_param.sch_targets.insert(input_param.stage_index, ScheduleTarget(rfactor_name, tensor_rfactor, []))


def proc_set_scope(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_set_scope
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 18:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    scope_id = int(input_param.args[0])
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].set_scope(SCOPE_DICT[scope_id])
    code_lines.append("sch[%s].set_scope('%s')" % (sch_target.name, SCOPE_DICT[scope_id]))


def proc_bind(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_bind
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 10:
        return

    sch_target = input_param.sch_targets[input_param.stage_index]
    bind_axis = sch_target.axes[0]
    if input_param.mode == MODE_RUNTIME:
        block = tvm.te.thread_axis('blockIdx.x')
        input_param.sch[sch_target.obj].bind(bind_axis.obj, block)
    code_lines.append("block = tvm.te.thread_axis('blockIdx.x')")
    code_lines.append("sch[%s].bind(%s, block)" % (sch_target.name, bind_axis.name))


def get_axis(args: list, sch_target: object, mode: str, sch: object) -> object:
    """
    get_axis
    :param args
    :param sch_target
    :param mode
    :param sch
    :return:
    """
    axis_index = args[0]
    if axis_index[0] == -1:
        axis = sch_target.axes[args[0][1]]
    else:
        axis_index = axis_index[0]
        if mode == MODE_RUNTIME:
            axis = Axis('sch[%s].op.axis[%d]' % (sch_target.name, axis_index),
                        sch[sch_target.obj].op.axis[axis_index])
        else:
            axis = Axis('sch[%s].op.axis[%d]' % (sch_target.name, axis_index), None)
    return axis


def proc_pragma(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_pragma
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 16:
        return
    # Pragma
    sch_target = input_param.sch_targets[input_param.stage_index]
    pragma_insn_name = INTRIN_MAP[input_param.args[1]]
    pragma_insn_offset = input_param.args[2]
    axis = get_axis(input_param.args, sch_target, input_param.mode, input_param.sch)

    if pragma_insn_name == "axis_group" and len(input_param.args) > 3:
        group = input_param.tvm_call_extern.get(input_param.args[3])
        input_param.sch[sch_target.obj].pragma(axis.obj, pragma_insn_name, group)
        group_name = f"group_{input_param.args[3]}"

        code_lines.append("sch[%s].pragma(%s, '%s', %s)" % (sch_target.name, axis.name, pragma_insn_name, group_name))
    else:
        if input_param.mode == MODE_RUNTIME:
            input_param.sch[sch_target.obj].pragma(axis.obj, pragma_insn_name, pragma_insn_offset)
        code_lines.append("sch[%s].pragma(%s, '%s', %s)" %
                          (sch_target.name, axis.name, pragma_insn_name, pragma_insn_offset))


def proc_emit_insn(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_emit_insn
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    def emit_insn_for_mad():
        """
        emit insn for intrinsic mad
        :return:
        """
        mad_pattern_value = int(input_param.args[2][0])
        init_bias_value = int(input_param.args[2][1])
        k_outer_axis_obj_list = [sch_target.axes[axis_idx].obj for axis_idx in input_param.args[2][2:]]
        k_outer_axis_name_list = [sch_target.axes[axis_idx].name for axis_idx in input_param.args[2][2:]]
        mad_dict = {"mad_pattern": mad_pattern_value, "k_outer": k_outer_axis_obj_list}
        if init_bias_value:
            mad_dict["init_bias"] = init_bias_value
        code_lines.append(
            'mad_dict = {"mad_pattern": %s, "k_outer": [%s]%s}' %
            (mad_pattern_value, ", ".join(k_outer_axis_name_list),
             ', "init_bias": %s' % init_bias_value if init_bias_value else ""))
        input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, mad_dict)
        # gen code
        code_lines.append(
            "sch[%s].emit_insn(%s, '%s'%s)" %
            (sch_target.name, axis.name, intrinsic, ", mad_dict"))
        return True

    def emit_insn_for_set_fmatrix():
        """
        emit insn for intrinsic set_fmatrix
        :return:
        """
        attr_dict = input_param.sch[sch_target.obj].op.attrs
        set_fmatrix_dict = {
            'conv_kernel_h': attr_dict['conv_kernel_h'].value,
            'conv_kernel_w': attr_dict['conv_kernel_w'].value,
            'conv_padding_top': attr_dict['conv_padding_top'].value,
            'conv_padding_bottom': attr_dict['conv_padding_bottom'].value,
            'conv_padding_left': attr_dict['conv_padding_left'].value,
            'conv_padding_right': attr_dict['conv_padding_right'].value,
            'conv_stride_h': attr_dict['conv_stride_h'].value,
            'conv_stride_w': attr_dict['conv_stride_w'].value,
            'conv_dilation_h': attr_dict['conv_dilation_h'].value,
            'conv_dilation_w': attr_dict['conv_dilation_w'].value,
            'conv_fm_c': attr_dict['conv_fm_c'].value,
            'conv_fm_h': attr_dict['conv_fm_h'].value,
            'conv_fm_w': attr_dict['conv_fm_w'].value
        }
        code_lines.append("setfmatrix_dict = {}".format(str(set_fmatrix_dict)))
        input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, set_fmatrix_dict)
        # gen code
        code_lines.append(
            "sch[%s].emit_insn(%s, '%s'%s)" %
            (sch_target.name, axis.name, intrinsic, ", setfmatrix_dict"))
        return True

    def emit_insn_for_dma_copy():
        """
        emit insn for intrinsic dma_copy
        :return:
        """
        if len(input_param.args) > 2:
            attrs = {}
            if input_param.args[2][0] != -1:
                no_overlap_value = int(input_param.args[2][0])
                attrs = {"no_overlap": no_overlap_value}
            if len(input_param.args[2]) > 1 and input_param.args[2][1] != -1:
                no_overlap_malloc_buf_for_tail_value = int(input_param.args[2][1])
                attrs.update({"no_overlap_malloc_buf_for_tail": no_overlap_malloc_buf_for_tail_value})
            if len(input_param.args[2]) > 2 and input_param.args[2][2] != -1:
                map_policy_key = int(input_param.args[2][2])
                police_map = {1: "2d"}
                map_policy_value = police_map.get(map_policy_key, "2d")
                attrs.update({"map_policy": map_policy_value})
            input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=attrs)
            code_lines.append("sch[%s].emit_insn(%s, '%s', attrs=%s)" % (sch_target.name, axis.name, intrinsic, attrs))
            return True
        return False

    def emit_insn_for_vector_transpose():
        """
        emit insn for intrinsic vector_transpose
        :return:
        """
        if len(input_param.args) > 2:
            src_in_dst_order_list = input_param.args[2][0]
            src_in_dst_order = tvm.call_cce_pure_intrin('handle', 'tir.tvm_tuple', *src_in_dst_order_list)
            code_lines.append(
                "src_in_dst_order = tvm.call_cce_pure_intrin('handle', 'tir.tvm_tuple', {})"
                .format(*src_in_dst_order_list))

            attrs = {"src_in_dst_order": src_in_dst_order}
            attrs_str = "{\'src_in_dst_order\': src_in_dst_order}"
            input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=attrs)
            code_lines.append("sch[%s].emit_insn(%s, '%s', attrs=%s)" %
                              (sch_target.name, axis.name, intrinsic, attrs_str))
            return True
        return False

    def emit_insn_for_vector_broadcast():
        """
        emit insn for intrinsic vector_broadcast
        :return:
        """
        if len(input_param.args) > 2:
            if not isinstance(input_param.args[2], list):
                vector_boradcast_attrs = {"last_src_valid_element": int(input_param.args[2])}
            elif int(input_param.args[2][0]) == 1:
                vector_boradcast_attrs = {"enable_vnchwconv": True}
            elif int(input_param.args[2][0]) == 2:
                vector_boradcast_attrs = {"enable_align_eight_dup": True}
            elif int(input_param.args[2][0]) == 3:
                vector_boradcast_attrs = {"enable_brc_block": True}
            else:
                vector_boradcast_attrs = {}

            input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=vector_boradcast_attrs)
            code_lines.append(
                "sch[%s].emit_insn(%s, '%s', attrs=%s)" %
                (sch_target.name, axis.name, intrinsic, vector_boradcast_attrs))
            return True
        return False

    def emit_insn_for_unknown_broadcast():
        """
        emit insn for intrinsic unknown_broadcast
        :return:
        """
        if len(input_param.args) > 2:
            dynamic_fuse = bool(input_param.args[2][0])
            dynamic_split = bool(input_param.args[2][1])
            attrs = {"dynamic_fuse": dynamic_fuse, "dynamic_split:": dynamic_split}
            input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=attrs)
            code_lines.append(
                "sch[%s].emit_insn(%s, '%s', attrs=%s)" %
                (sch_target.name, axis.name, intrinsic, attrs))
            return True
        return False

    def emit_insn_for_vector_reduce():
        """
        emit insn for intrinsic vector_reduce
        :return:
        """
        def _get_extra_info_(extra_infos, input_params):
            extra_bool_paras = [1, 4, 5, 6, 7]
            reduce_opt_mode_index = 2
            para_name = ["extra_space", "trans", "reduce_opt_mode", "storage_bound", "reuse_dst_tensor",
                         "enough_buffer",
                         "window", "reuse_src_tensor", "nlast_reduce_dichotomy"]
            for index, value in enumerate(input_params.args[2]):
                if not isinstance(value, int):
                    continue
                if index in extra_bool_paras and value != -1:
                    extra_infos[para_name[index]] = bool(value)
                elif index == reduce_opt_mode_index and value != -1:
                    extra_infos[para_name[index]] = \
                        {v: k for k, v in REDUCE_OPT_MODE_MAP.items()}.get(value)
                elif value != -1:
                    extra_infos[para_name[index]] = value

        if intrinsic.find('vector_reduce') != -1 and len(input_param.args) > 2:
            extra_info_dict = {}
            _get_extra_info_(extra_info_dict, input_param)

            input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=extra_info_dict)
            code_lines.append(
                "sch[%s].emit_insn(%s, '%s', attrs=%s)" %
                (sch_target.name, axis.name, intrinsic, extra_info_dict))
            return True
        return False

    def emit_insn_for_vector_mul_and_vector_add():
        """
        emit_insn_for_vector_mul_and_vector_add
        :return:
        """
        if len(input_param.args) > 2:
            use_ba_pattern_brc_flag = bool(input_param.args[2][0])
            use_ba_pattern_brc_dict = {"use_ba_pattern_brc": use_ba_pattern_brc_flag}
            input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=use_ba_pattern_brc_dict)
            code_lines.append(
                "sch[%s].emit_insn(%s, '%s', attrs=%s)" %
                (sch_target.name, axis.name, intrinsic, use_ba_pattern_brc_dict))
            return True
        return False

    def emit_insn_for_vnchwconv():
        """
        emit_insn_for_vnchwconv
        :return:
        """
        vnchwconv_default_dict = {"map_policy": "2d_dim2_size_no_overflow"}
        input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic, attrs=vnchwconv_default_dict)
        code_lines.append(
            "sch[%s].emit_insn(%s, '%s', attrs=%s)" %
            (sch_target.name, axis.name, intrinsic, vnchwconv_default_dict))
        return True

    if input_param.primitive != 11:
        return
    # EmitInsn
    sch_target = input_param.sch_targets[input_param.stage_index]
    intrinsic = INTRIN_MAP[input_param.args[1]]
    axis = get_axis(input_param.args, sch_target, input_param.mode, input_param.sch)
    if input_param.mode == MODE_RUNTIME:
        emit_insn_map = {
            "mad": emit_insn_for_mad,
            "set_fmatrix": emit_insn_for_set_fmatrix,
            "dma_copy": emit_insn_for_dma_copy,
            "vector_transpose": emit_insn_for_vector_transpose,
            "vector_broadcast": emit_insn_for_vector_broadcast,
            "unknown_broadcast": emit_insn_for_unknown_broadcast,
            'vector_mul': emit_insn_for_vector_mul_and_vector_add,
            'vector_add': emit_insn_for_vector_mul_and_vector_add,
            'vnchwconv': emit_insn_for_vnchwconv
        }
        match_ret = emit_insn_map.get(intrinsic)() if intrinsic in emit_insn_map.keys() else False
        if match_ret:
            return
        if emit_insn_for_vector_reduce():
            return
        input_param.sch[sch_target.obj].emit_insn(axis.obj, intrinsic)
        code_lines.append(
            "sch[%s].emit_insn(%s, '%s')" %
            (sch_target.name, axis.name, intrinsic))


def proc_sub_bind(input_param: namedtuple, code_lines: list):  # pylint: disable=too-many-locals, too-many-arguments
    '''
    proc_sub_bind
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    '''
    if input_param.primitive == 50:
        sch_target = input_param.sch_targets[input_param.stage_index]
        sub_bind_index = input_param.args[0]
        sub_bind_axis = sch_target.axes[sub_bind_index]

        if input_param.mode == MODE_RUNTIME:
            sub_block = tvm.te.thread_axis('subBlockIdx.x')
            input_param.sch[sch_target.obj].bind(sub_bind_axis.obj, sub_block)
        code_lines.append("sub_block = tvm.te.thread_axis('subBlockIdx.x')")
        code_lines.append("sch[%s].bind(%s, sub_block)" % (sch_target.name, sub_bind_axis.name))


def proc_reused_by(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_reused_by
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 19:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    reused_sch_target = input_param.sch_targets[input_param.args[0]]
    reuse_data = False
    if len(input_param.args) > 1:
        reuse_data = bool(input_param.args[1])
    if input_param.args[0] == -1:
        # gen code
        code_lines.append("sch[%s].reused_by(reuse_data=%s)" % (sch_target.name, reuse_data))
    else:
        # gen code
        code_lines.append("sch[%s].reused_by(%s)" % (sch_target.name, reused_sch_target.name))

    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].reused_by(reused_sch_target.obj, reuse_data=reuse_data)


def proc_insert_param(primitive: int, args: list, mode: str, code_lines: list) -> None:
    """
    proc_insert_param
    :param primitive:
    :param args:
    :param mode:
    :param code_lines:
    :return:
    """
    if primitive != 12:
        return

    # broadcast_axis_offset
    offset = args[0]
    if mode == MODE_RUNTIME:
        cce_emitinsn_params.cceEmitParamsIns.del_param('broadcast_axis_offset')
        cce_emitinsn_params.cceEmitParamsIns.insert_param('broadcast_axis_offset', offset)
    code_lines.append(
        "cce_emitinsn_params.cceEmitParamsIns.del_param('broadcast_axis_offset')")

    code_lines.append(
        "cce_emitinsn_params.cceEmitParamsIns.insert_param('broadcast_axis_offset', %d)" %
        offset)


def proc_storage_align(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_storage_align
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 13:
        return
    if isinstance(input_param.args[0], list):
        # proc for get_storage_align_cheque_enhance
        sch_target = input_param.sch_targets[input_param.stage_index]
        block_num = input_param.args[1]
        axis = get_axis(input_param.args, sch_target, input_param.mode, input_param.sch)
        if input_param.mode == MODE_RUNTIME:
            input_param.sch[sch_target.obj].storage_align(axis.obj, block_num, 0)
        code_lines.append("sch[%s].storage_align(%s, %s, 0)" %
                          (sch_target.name, axis.name, block_num))
    else:
        # storage_align args : axis_index and block_num
        sch_target = input_param.sch_targets[input_param.stage_index]
        axis_index = input_param.args[0]
        block_num = input_param.args[1]
        if input_param.mode == MODE_RUNTIME:
            axis = Axis('sch[%s].op.axis[%d]' % (sch_target.name, axis_index),
                        input_param.sch[sch_target.obj].op.axis[axis_index])
        else:
            axis = Axis('sch[%s].op.axis[%d]' % (sch_target.name, axis_index), None)

        if input_param.mode == MODE_RUNTIME:
            input_param.sch[sch_target.obj].storage_align(axis.obj, block_num, 0)
        code_lines.append("sch[%s].storage_align(%s, %s, 0)" %
                          (sch_target.name, axis.name, block_num))


def proc_compute_align(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_compute_align
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 53:
        return
    # storage_align args : axis_index and block_num
    sch_target = input_param.sch_targets[input_param.stage_index]
    axis_index = input_param.args[0]
    block_num = input_param.args[1]
    axis_type = input_param.args[2]
    if input_param.mode == MODE_RUNTIME:
        if int(axis_type) == 0:
            axis = Axis('sch[{}].op.axis[{}]'.format(sch_target.name, axis_index),
                        input_param.sch[sch_target.obj].op.axis[axis_index])
        else:
            axis = Axis('sch[{}].op.reduce_axis[{}]'.format(sch_target.name, axis_index),
                        input_param.sch[sch_target.obj].op.reduce_axis[axis_index])
    else:
        if int(axis_type) == 0:
            axis = Axis('sch[{}].op.axis[{}]'.format(sch_target.name, axis_index), None)
        else:
            axis = Axis('sch[{}].op.reduce_axis[{}]'.format(sch_target.name, axis_index), None)

    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].compute_align(axis.obj, block_num)
    code_lines.append("sch[{}].compute_align({}, {})".format(sch_target.name, axis.name, block_num))


def proc_set_buffer_size(*args: Any) -> None:
    """
    proc_set_buffer_size
    :param args:
    :return:
    """
    stage_index, primitive, sch_targets, sch, mode, code_lines, args = args

    if primitive == 39:
        # storage_bound args : bound_size
        sch_target = sch_targets[stage_index]
        bound_size = args[0]
        if mode == MODE_RUNTIME:
            sch[sch_target.obj].set_buffer_size(bound_size)
        code_lines.append("sch[%s].set_buffer_size(%s)" % (sch_target.name, bound_size))


def proc_set_store_predicate(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_set_set_store_predicate
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 40:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    if not input_param.args and input_param.mode == MODE_RUNTIME:
        stage = input_param.sch[sch_target.obj]
        stage.set_store_predicate(stage.op.body[0].condition)
        code_lines.append("sch[%s].set_store_predicate(sch[%s].op.body[0].condition)" %
                          (sch_target.name, sch_target.name))
        return

    # set_set_store_predicate args : target_stage_index, axis_index, compare_num
    target_stage_index = input_param.args[0]
    axis_index = input_param.args[1]
    compare_num = input_param.args[2]
    ub_split_axis = input_param.args[3]
    remove_pad = input_param.args[4]
    at_sch_target = input_param.sch_targets[target_stage_index]
    at_axis = at_sch_target.axes[axis_index]
    if input_param.mode == MODE_RUNTIME:
        if len(input_param.args) > 5 and input_param.args[5]:
            brc_info_list = input_param.args[5]
            bind_stage_index = brc_info_list[0]
            bind_axis_index = brc_info_list[1]
            compute_at_index = brc_info_list[2]
            compute_at_axis_index = brc_info_list[3]
            block_factor = brc_info_list[4]
            ub_out_shape = brc_info_list[5]
            ub_split_axis = brc_info_list[6]
            db_tag = brc_info_list[7]
            bind_target = input_param.sch_targets[bind_stage_index]
            bind_axis = bind_target.axes[bind_axis_index]
            at_target = input_param.sch_targets[compute_at_index]
            at_axis = at_target.axes[compute_at_axis_index]
            if sch_target.obj.op.tag in ['unknown_broadcast', 'unified_broadcast']:
                cond = tvm.any(
                    (bind_axis.obj * block_factor + at_axis.obj) % ub_out_shape < db_tag,
                    at_axis.obj < db_tag,
                    sch_target.obj.op.input_tensors[0].shape[ub_split_axis] != 1)
                input_param.sch[sch_target.obj].set_store_predicate(cond)
                cond_str = "tvm.any((%s * %s + %s) %% %s < %s, %s < %s, %s.op.input_tensors[0].shape[%s] != 1)" % \
                           (bind_axis.name, block_factor, at_axis.name,
                            ub_out_shape, db_tag, at_axis.name, db_tag, sch_target.name, ub_split_axis)
                code_lines.append("sch[%s].set_store_predicate(%s)" %
                                  (sch_target.name, cond_str))
            else:
                cond = tvm.any(
                    (bind_axis.obj * block_factor + at_axis.obj) % ub_out_shape < db_tag,
                    at_axis.obj < db_tag,
                    sch_target.obj.shape[ub_split_axis] != 1)
                input_param.sch[sch_target.obj].set_store_predicate(cond)
                cond_str = "tvm.any((%s * %s + %s) %% %s < %s, %s < %s, %s.shape[%s] != 1)" % \
                           (bind_axis.name, block_factor, at_axis.name,
                            ub_out_shape, db_tag, at_axis.name, db_tag, sch_target.name, ub_split_axis)
                code_lines.append("sch[%s].set_store_predicate(%s)" %
                                  (sch_target.name, cond_str))
            return
        if remove_pad == 1:
            cond = tvm.te.any(at_axis.obj < compare_num,
                           sch_target.obj.op.input_tensors[0].op.input_tensors[0].shape[ub_split_axis] != 1)
            input_param.sch[sch_target.obj].set_store_predicate(cond)
            cond_str = "tvm.te.any(%s < %s, %s.op.input_tensors[0].op.input_tensors[0].shape[%s] != 1)" % \
                       (at_axis.name, compare_num, sch_target.name, ub_split_axis)
            code_lines.append("sch[%s].set_store_predicate(%s)" %
                              (sch_target.name, cond_str))
        elif sch_target.obj.op.tag in ['unknown_broadcast', 'unified_broadcast']:
            cond = tvm.te.any(at_axis.obj < compare_num, sch_target.obj.op.input_tensors[0].shape[ub_split_axis] != 1)
            input_param.sch[sch_target.obj].set_store_predicate(cond)
            cond_str = "tvm.te.any(%s < %s, %s.op.input_tensors[0].shape[%s] != 1)" % \
                       (at_axis.name, compare_num, sch_target.name, ub_split_axis)
            code_lines.append("sch[%s].set_store_predicate(%s)" %
                              (sch_target.name, cond_str))
        else:
            cond = tvm.te.any(at_axis.obj < compare_num, sch_target.obj.shape[ub_split_axis] != 1)
            input_param.sch[sch_target.obj].set_store_predicate(cond)
            cond_str = "tvm.te.any(%s < %s, %s.shape[%s] != 1)" % \
                       (at_axis.name, compare_num, sch_target.name, ub_split_axis)
            code_lines.append("sch[%s].set_store_predicate(%s)" %
                              (sch_target.name, cond_str))


def proc_set_constraint(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_set_constraint
    [var_name_list, 50, compare_symbol, compare_num]
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 51:
        return
    op_context = operation.get_context()
    if op_context is None:
        return

    var_list = op_context.get_vars()
    for i, cpt in enumerate(op_context.get_computes()):
        var_list = var_list + cpt.get_vars()
        for sch_context in cpt.get_schedules():
            var_list = var_list + sch_context.get_vars()
    cond_vars = [var.get_tvm_var() for var in var_list if var.get_tvm_var().name in input_param.stage_index]
    multi_size = reduce(lambda x, y: x * y, cond_vars)
    if input_param.args[0] == 0:
        if input_param.mode == MODE_RUNTIME:
            input_param.sch.set_constraint(multi_size <= input_param.args[1])
        cond_str = ''
        for var_name in input_param.stage_index:
            cond_str = cond_str + var_name
        cond_str = cond_str + " <= " + str(input_param.args[1])
        code_lines.append("sch.set_constraint(%s)" % cond_str)


def proc_buffer_align(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_buffer_align
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 25:
        return
    # storage_align args : axis_index and block_num
    sch_target = input_param.sch_targets[input_param.stage_index]
    align_args = input_param.args[0]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].buffer_align(*align_args)
    code_lines.append("sch[%s].buffer_align(*%s)" % (sch_target.name, align_args))


def proc_tbe_compile_para(primitive: int, args: list, sch: object, mode: str, code_lines: list) -> None:
    """
    proc_tbe_compile_para
    :param args:
    :param code_lines:
    :return:
    """
    if primitive != 49:
        return
    num2bool = {0: False, 1: True}
    tbe_para = {}
    for ele in args:
        key = TBE_COMPILE_PARA_DICT.get(ele[0])
        value = num2bool.get(ele[1])
        if key is not None and value is not None:
            tbe_para[key] = value

    if mode == MODE_RUNTIME:
        sch.tbe_compile_para = tbe_para
    code_lines.append("sch.tbe_compile_para = {}".format(tbe_para))


def get_tensor_list_info(sch_targets: list, tensor_list_index: list) -> (list, list):
    """
    get_tensor_list_info
    :param sch_targets:
    :param tensor_list_index:
    :return:
    """
    tensor_list_objs = []
    tensor_list_names = []
    for stage_index in tensor_list_index:
        if isinstance(stage_index, list):
            tensor_nums = stage_index[1]
            stage_index = stage_index[0]
            sch_target = sch_targets[stage_index]
            stage_name = sch_target.name
            tensor_name_postfix = ""
            if stage_name.endswith('_l'):
                tensor_name_postfix = "_l"
            for idx in range(tensor_nums):
                tensor_list_objs.append(sch_target.obj.op.output(idx))
                tensor_name = "%s_v%s" % (stage_name.split('_l')[0], idx)
                tensor_name += tensor_name_postfix
                tensor_list_names.append(tensor_name)
        else:
            tensor_list_objs.append(sch_targets[stage_index].obj.op.output(0))
            tensor_list_names.append(sch_targets[stage_index].name)
    return tensor_list_objs, tensor_list_names


def proc_cce_special(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_cce_special
    :param args_param:
    :param code_lines:
    :return:
    """
    if input_param.primitive != 14:
        return
    # cce_special
    tensor_list_objs = []
    tensor_list_names = []
    orign_out_tensor_list_objs = []
    orign_out_tensor_list_names = []
    real_out_tensor_list_objs = []
    real_out_tensor_list_names = []
    # general cce_special cheque form is [-1, 14, [], [8], [7]]
    # tuple_reduce cce_special cheque form is [-1, 14, [], [[8, 2]], [[7, 2]]]
    for arg_index, tmp_tensor_list_index in enumerate(input_param.args):
        tmp_tensor_list_objs, tmp_tensor_list_names = \
            get_tensor_list_info(input_param.sch_targets, tmp_tensor_list_index)
        if arg_index == 0:
            tensor_list_objs = tmp_tensor_list_objs
            tensor_list_names = tmp_tensor_list_names
        elif arg_index == 1:
            orign_out_tensor_list_objs = tmp_tensor_list_objs
            orign_out_tensor_list_names = tmp_tensor_list_names
        else:
            real_out_tensor_list_objs = tmp_tensor_list_objs
            real_out_tensor_list_names = tmp_tensor_list_names

    if input_param.mode == MODE_RUNTIME:
        input_param.sch.cce_special = {}
        input_param.sch.cce_special["tensor_list"] = tensor_list_objs
        input_param.sch.cce_special["orign_out_tensor"] = orign_out_tensor_list_objs
        input_param.sch.cce_special["real_out_tensor"] = real_out_tensor_list_objs

    code_lines.append("sch.cce_special = dict()")
    code_lines.append('sch.cce_special["tensor_list"] = [%s]' % ", ".join(tensor_list_names))
    code_lines.append('sch.cce_special["orign_out_tensor"] = [%s]' % ", ".join(orign_out_tensor_list_names))
    code_lines.append('sch.cce_special["real_out_tensor"] = [%s]' % ", ".join(real_out_tensor_list_names))


def proc_build_config(input_param: namedtuple, code_lines: list) -> None:
    if input_param.primitive != 54:
        return
    attr_index = input_param.args[0]
    attr_value = input_param.args[1]

    if attr_value:
        add_build_arg(BUILD_CONFIG_DICT.get(attr_index), True)
        code_lines.append('set_current_build_config("%s", True)' % BUILD_CONFIG_DICT.get(attr_index))
    else:
        add_build_arg(BUILD_CONFIG_DICT.get(attr_index), False)
        code_lines.append('set_current_build_config("%s", False)' % BUILD_CONFIG_DICT.get(attr_index))


def proc_set_block_sync(input_param: namedtuple, code_lines: list, sync_tensor: List) -> None:
    """
    proc_set_block_sync
    :param input_param:
    :param code_lines:
    :param sync_tensor:
    :return:
    """
    # set_block_sync args : axis_index, _, bottom
    if input_param.primitive != 38:
        return
    axis_index, _, bottom = input_param.args
    sch_target = input_param.sch_targets[input_param.stage_index]
    if sync_tensor is None:
        sync_tensor_arg = input_param.sch.get_block_sync_size() - 1
        sync_tensor_str = "sync"
    else:
        sync_tensor_arg = sync_tensor[0]
        sync_tensor_str = sync_tensor.name + "[0]"

    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].set_block_sync(sch_target.axes[axis_index].obj,
                                                       sync_tensor_arg,
                                                       bottom=bool(bottom))
    code_lines.append("sch[%s].set_block_sync(%s, %s, bottom=%s)" %
                      (sch_target.name, sch_target.axes[axis_index].name,
                       sync_tensor_str, bool(bottom)))


def proc_wait_block_sync(input_param: namedtuple, code_lines: list, sync_tensor: List) -> None:
    """
    proc_wait_block_sync
    :param input_param:
    :param code_lines:
    :param sync_tensor:
    :return:
    """
    # wait_block_sync args : axis_index, sync_stage_index, bottom
    if input_param.primitive != 48:
        return
    axis_index, _, bottom = input_param.args
    sch_target = input_param.sch_targets[input_param.stage_index]
    if sync_tensor is None:
        sync_tensor_arg = input_param.sch.create_block_sync()
        code_lines.append("sync = sch.create_block_sync()")
        sync_tensor_str = "sync"
    else:
        sync_tensor_arg = sync_tensor[0]
        sync_tensor_str = sync_tensor.name + "[0]"

    if input_param.mode == MODE_RUNTIME:
        input_param.sch[sch_target.obj].wait_block_sync(sch_target.axes[axis_index].obj,
                                                        sync_tensor_arg,
                                                        bottom=bool(bottom))
    code_lines.append("sch[%s].wait_block_sync(%s, %s, bottom=%s)" %
                      (sch_target.name, sch_target.axes[axis_index].name,
                       sync_tensor_str, bool(bottom)))


def proc_cache_clone(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_cache_clone
    :param stage_index:
    :param primitive:
    :param args:
    :param sch_targets:
    :param sch:
    :param mode:
    :param code_lines:
    :return:
    """
    if input_param.primitive != 52:
        return
    sch_target = input_param.sch_targets[input_param.stage_index]
    # cache Read args  is Scope and Consumers
    scope_id = input_param.args[0]
    scope = SCOPE_DICT[scope_id]
    consumers_indicies = input_param.args[1]
    consumers = [input_param.sch_targets[i].obj for i in consumers_indicies]
    consumer_names = ', '.join([input_param.sch_targets[i].name for i in consumers_indicies])

    readed_tensor = None
    if input_param.mode == MODE_RUNTIME:
        readed_tensor = input_param.sch.cache_clone(sch_target.obj, scope, consumers)

    clone_index = 10
    readed_pattern = r'^%s_l_\d+$' % sch_target.name
    tmp_name = input_param.sch_targets[input_param.stage_index + 1].name
    if input_param.stage_index + 1 < len(input_param.sch_targets) and re.match(readed_pattern, tmp_name):
        clone_index = int(tmp_name.split('_')[-1]) + 1
    readed_name = '%s_l_%03d' % (sch_target.name, clone_index)
    # insert after original tensor
    input_param.sch_targets.insert(input_param.stage_index + 1, ScheduleTarget(readed_name, readed_tensor, []))
    code_line = "%s = sch.cache_clone(%s, '%s', [%s])" % (readed_name, sch_target.name, scope,
                                                         consumer_names)
    code_lines.append(code_line)


def proc_compute_with(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_compute_with
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 55:
        return

    sch_targets = []
    sch_target_names = []
    for i in input_param.stage_index:
        sch_targets.append(input_param.sch_targets[i].obj)
        sch_target_names.append(input_param.sch_targets[i].name)

    loop_level = input_param.args[0]
    if input_param.mode == MODE_RUNTIME:
        input_param.sch.compute_with(sch_targets, loop_level)
    code_lines.append("sch.compute_with([%s], %s)" % (", ".join(sch_target_names), loop_level))


def proc_call_extern(input_param: namedtuple, code_lines: list) -> None:
    """
    proc_compute_with
    :param input_param: namedtuple("args", "sch", "sch_targets", "primitive", "stage_index", "mode")
    :param code_lines
    :return:
    """
    if input_param.primitive != 56:
        return
    group_idx = input_param.stage_index

    tvm_call_extern = input_param.tvm_call_extern
    pragma_insn_name = INTRIN_MAP[input_param.args[0]]
    group_type_dict = {0: "append", 1: "overwrite"}
    pragma_insn_offset = input_param.args[1]
    group_type = group_type_dict.get(input_param.args[2])
    group_name = f"group_{group_idx}"

    group = tvm.call_extern("int32", pragma_insn_name, pragma_insn_offset, group_type)
    tvm_call_extern[group_idx] = group
    code_lines.append("""%s = tvm.call_extern("int32", "axis_group", %s, %s)""" %
                      (group_name, pragma_insn_offset, group_type))


def get_leaves(res_list: list) -> list:
    """
    get leaf outs
    :param res_list:
    :return:
    """
    tensors = [tensor for tensor in res_list]
    visited_tensor = set()
    non_leaf_nodes = set()
    while tensors:
        current_tensor = tensors.pop(0)
        current_op = current_tensor.op
        current_name = current_op.name
        if current_name in visited_tensor or isinstance(current_op, tvm.PlaceholderOp):
            continue
        visited_tensor.add(current_name)
        for input_tensor in current_op.input_tensors:
            input_name = input_tensor.op.name
            if input_name not in non_leaf_nodes:
                non_leaf_nodes.add(input_name)
            tensors.append(input_tensor)
    leaf_outs = []
    for tensor in res_list:
        if tensor.name not in non_leaf_nodes:
            leaf_outs.append(tensor)
    return leaf_outs


def _gen_virtual_leaf_out(leaf_outs):
    """
    :param leaf_outs:
    :return:
    """
    virtual_out_shape = _get_virtual_out_shape(leaf_outs)
    virtual_out = _creat_virtual_leaf_out(leaf_outs, virtual_out_shape)
    return virtual_out


def _get_virtual_out_shape(leaf_outs):
    """
    if origin leaf output shape are all the same, virtual out shape is equal any out's shape.
    if origin leaf output shape are not same, virtual out shape is equal max input shape
    :param leaf_outs:
    :return:
    """
    # if origin leaf output shape are all the same, virtual out shape is equal any out's shape
    shape_set = set()
    for leaf_out in leaf_outs:
        shape_set.add(str(leaf_out.shape))
    if len(shape_set) == 1:
        return [x.value for x in leaf_outs[0].shape]

    # get all inputs shape
    input_shapes = []
    tensors = leaf_outs[:]
    all_tensors = leaf_outs[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if isinstance(tensor.op, tvm.PlaceholderOp):
                input_shapes.append([int(axis) for axis in tensor.shape])
                continue
            new_tensors.extend(tensor.op.input_tensors)
        tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(tensors)

    # get final virtual_out shape: contain all input axis
    virtual_out_shape = input_shapes[0]
    for input_shape in input_shapes[1:]:
        if len(input_shape) > len(virtual_out_shape):
            virtual_out_shape = input_shape
        elif len(input_shape) == len(virtual_out_shape):
            # get max value for each dim
            for i, curr_value in enumerate(input_shape):
                virtual_out_shape[i] = max(curr_value, virtual_out_shape[i])
    return virtual_out_shape


def _gen_virtual_leaf_out_by_broadcast(leaf_outs, virtual_out_shape):
    # 针对不keepdim
    def _get_broadcast_leaf_out(virtual_out_shape, index_list, leaf_out):
        broadcast_leaf_out = tvm.te.compute(
            virtual_out_shape,
            lambda *index: leaf_out(
                *[x for i, x in enumerate(index) if i in index_list]),
            name='%s_broadcast' % leaf_out.name,
            tag='broadcast_for_tensor')
        return broadcast_leaf_out

    # 将leaf_outs broadcast到virtual_out的shap
    broadcast_leaf_outs = []
    for leaf_out in leaf_outs:
        leaf_out_shape = [int(axis_len) for axis_len in leaf_out.shape]
        if leaf_out_shape == virtual_out_shape:
            broadcast_leaf_outs.append(leaf_out)
            continue
        if len(leaf_out_shape) == len(virtual_out_shape):
            # keepdim
            index_list = list(range(len(leaf_out_shape)))
        else:
            # 不keepdim
            index_list = []
            index_dict = {}
            for axis_len in leaf_out_shape:
                index_dict.setdefault(axis_len, -1)
                index = virtual_out_shape.index(axis_len,
                                                index_dict.get(axis_len) + 1)
                index_dict[axis_len] = index
                index_list.append(index)
        broadcast_leaf_out = _get_broadcast_leaf_out(
            virtual_out_shape, index_list, leaf_out)
        broadcast_leaf_outs.append(broadcast_leaf_out)

    # broadcast后的leaf_outs相加得到virtual_out
    virtual_out = broadcast_leaf_outs[0]
    for broadcast_leaf_out in broadcast_leaf_outs[1:]:
        virtual_out = tbe.dsl.vadd(virtual_out, broadcast_leaf_out)
    return virtual_out


def _creat_virtual_leaf_out(leaf_outs, virtual_out_shape):
    """
    :param leaf_outs:
    :param virtual_out_shape:
    :return:
    """
    # 不支持不keepdim
    tensors = leaf_outs[:]
    all_tensors = leaf_outs[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if len(tensor.shape) != len(virtual_out_shape):
                log.info("gen virtual_leaf_out by broadcast.")
                # 尝试一下broadcast + add
                return _gen_virtual_leaf_out_by_broadcast(
                    leaf_outs, virtual_out_shape)
            if not isinstance(tensor.op, tvm.PlaceholderOp):
                new_tensors.extend(tensor.op.input_tensors)
        tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(tensors)
    log.debug("gen virtual_leaf_out not by broadcast.")

    def phony_insn_fuse(*indice):
        """
        :param indice:
        :return:
        """
        dtype = leaf_outs[0].dtype
        virtual_out = tvm.const(1, dtype)
        for leaf_out in leaf_outs:
            cur_index = []
            for i, virtual_out_shape_i in enumerate(virtual_out_shape):
                if leaf_out.shape[i].value == virtual_out_shape_i:
                    cur_index.append(indice[i])
                else:
                    cur_index.append(indice[i] % leaf_out.shape[i].value)
            virtual_out *= tvm.tir.Cast(dtype, leaf_out(*cur_index))
        return virtual_out

    def phony_insn_split(*indices):
        axis = 1
        func = None
        concat_axis_size = sum(t.shape[axis] for t in leaf_outs)
        for tensor_i in reversed(leaf_outs):
            index = []
            for i, _ in enumerate(virtual_out_shape):
                if i == axis:
                    index.append(indices[i] - (concat_axis_size - tensor_i.shape[axis]))
                else:
                    index.append(indices[i])
            if func is None:
                func = tensor_i(*index)
            else:
                func = tvm.select(indices[axis] < concat_axis_size, tensor_i(*index), func)
            concat_axis_size -= tensor_i.shape[axis]
        return func

    func = phony_insn_fuse
    for tensor in leaf_outs:
        if 'split' in tensor.op.tag:
            func = phony_insn_split
            break

    with tvm.tag_scope(VIRTUAL_LEAF_OUT_TAG):
        virtual_out = tvm.te.compute(virtual_out_shape, func, name="virtual_leaf_out")
    return virtual_out


def withdraw(res_list: List, cheque: List, mode: str = "runtime", sync_tensor: List = None,
             dynamic: bool = False) -> Tuple[Any, List]:
    """
    withdraw
    :param res_list: output tensor
    :param cheque: cheque list
    :param mode: runtime or offline
    :param sync_tensor: None
    :param dynamic: True for dynamic op ,False for static op
    :return:schedule, code lines
    """
    if not isinstance(res_list, list):
        res_list = [res_list]

    # create schedule only need leaf-node.
    # Multi leaf op should add proc, except tuple_reduce_sum
    leaves = get_leaves(res_list)
    is_tuple_reduce_sum = False
    for leaf_out in leaves:
        if 'tuple_reduce_sum' in leaf_out.op.tag:
            is_tuple_reduce_sum = True

    if len(leaves) > 1 and not is_tuple_reduce_sum:
        virtual_leaf_out = _gen_virtual_leaf_out(leaves)
        leaves = [virtual_leaf_out]

    sch = tvm.create_schedule([res.op for res in leaves])
    # element in sch_targets List is [Tensor name， Tensor obj， comm axis list， reduce axis list]
    sch_targets = []
    last_primitive = None
    for stage in sch.stages:
        sch_targets.append(ScheduleTarget(stage.op.name, stage.op.output(0), []))

    code_lines = []
    tvm_call_extern = {}
    for action in cheque:
        stage_index, primitive, *args = action
        if primitive not in PRIMITIVE_DICT:
            raise RuntimeError('Invalid primitive: [%s]' % primitive)
        gen_code_lines_param = GenCodeLinesParam(args, sch, sch_targets, primitive, stage_index, mode, tvm_call_extern)
        proc_cache_clone(gen_code_lines_param, code_lines)
        proc_cache_read(gen_code_lines_param, code_lines)
        proc_cache_write(gen_code_lines_param, code_lines)
        proc_preload(gen_code_lines_param, code_lines)
        proc_double_buffer(gen_code_lines_param, code_lines)
        proc_compute_inline(gen_code_lines_param, code_lines)
        proc_get_axis(gen_code_lines_param, code_lines)
        proc_get_reduce_axis(gen_code_lines_param, code_lines)
        proc_split(gen_code_lines_param, code_lines)
        proc_nparts(gen_code_lines_param, code_lines)
        proc_reorder(gen_code_lines_param, code_lines, last_primitive)
        proc_allocate_at(gen_code_lines_param, code_lines)
        proc_mem_unique(gen_code_lines_param, code_lines)
        proc_compute_at(gen_code_lines_param, code_lines)
        proc_fuse(gen_code_lines_param, code_lines)
        proc_rfactor(gen_code_lines_param, code_lines)
        proc_set_scope(gen_code_lines_param, code_lines)
        proc_bind(gen_code_lines_param, code_lines)
        proc_sub_bind(gen_code_lines_param, code_lines)
        proc_wait_block_sync(gen_code_lines_param, code_lines, sync_tensor)
        proc_set_block_sync(gen_code_lines_param, code_lines, sync_tensor)
        proc_pragma(gen_code_lines_param, code_lines)
        proc_emit_insn(gen_code_lines_param, code_lines)
        proc_reused_by(gen_code_lines_param, code_lines)
        proc_insert_param(primitive, args, mode, code_lines)
        proc_storage_align(gen_code_lines_param, code_lines)
        proc_compute_align(gen_code_lines_param, code_lines)
        proc_set_buffer_size(stage_index, primitive, sch_targets, sch, mode, code_lines, args)
        proc_set_store_predicate(gen_code_lines_param, code_lines)
        proc_set_constraint(gen_code_lines_param, code_lines)
        proc_buffer_align(gen_code_lines_param, code_lines)
        proc_tbe_compile_para(primitive, args, sch, mode, code_lines)
        proc_cce_special(gen_code_lines_param, code_lines)
        proc_build_config(gen_code_lines_param, code_lines)
        proc_compute_with(gen_code_lines_param, code_lines)
        proc_call_extern(gen_code_lines_param, code_lines)
        last_primitive = primitive

    return sch, code_lines


def gen_sch_by_cheque(out_tensors: List, action_list: List, sync_tensor: List = None,
                      dynamic: bool = False) -> Union[Tuple[bool, Any], Tuple[bool, None]]:
    """
    gen_sch_by_cheque
    :param out_tensors: output tensor
    :param action_list: cheque list
    :param sync_tensor: None is default value
    :param dynamic: True for dynamic op ,False for static op
    :return:tuple (result, schedule)
    """
    try:
        sch, _ = withdraw(out_tensors, action_list, MODE_RUNTIME, sync_tensor, dynamic)
        return True, sch
    except (ValueError, TypeError, KeyError) as e:
        log.error("withdraw gen_sch_by_cheque failed, msg: %s", repr(e))
        return False, None
    except RuntimeError:
        return False, None
    finally:
        pass
