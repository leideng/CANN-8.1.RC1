#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.

Define the tuning utils
"""
import os

from tbe.common.utils import log


OUTPUT_DTYPE_WHITE_LIST = {"float16", "float32", "bfloat16"}
RL_OP_WHITE_LIST = {
    "Abs",
    "AccumulateNV2",
    "Add",
    "AddN",
    "Adds",
    "BatchNorm3D",
    "BiasAdd",
    "BiasAddGrad",
    "BNInfer",
    "BNTrainingReduce",
    "BNTrainingUpdate",
    "BNTrainingUpdateGrad",
    "BNTrainingUpdateV2",
    "BNTrainingUpdateV3",
    "BroadcastToD",
    "ClipByNormNoDivSum",
    "ClipByValue",
    "DiagPartD",
    "Div",
    "DivNoNan",
    "DynamicRNN",
    "DynamicLSTMV2",
    "Elu",
    "EluGrad",
    "Equal",
    "Exp",
    "ExpandD",
    "Floor",
    "FloorDiv",
    "Gelu",
    "Greater",
    "GreaterEqual",
    "InstanceNorm",
    "L2Loss",
    "LambUpdateWithLrV2",
    "LayerNormBetaGammaBackpropV2",
    "LeakyRelu",
    "Less",
    "LessEqual",
    "Log",
    "Log1p",
    "LogicalAnd",
    "LogicalNot",
    "LogicalOr",
    "LpNorm",
    "MatrixDiagD",
    "Maximum",
    "Minimum",
    "Mul",
    "Muls",
    "Neg",
    "NotEqual",
    "Pow",
    "PRelu",
    "RealDiv",
    "ReduceAllD",
    "ReduceMaxD",
    "ReduceMeanD",
    "ReduceMinD",
    "ReduceSumD",
    "Relu",
    "ReluV2",
    "Round",
    "Rsqrt",
    "RsqrtGrad",
    "Sigmoid",
    "Sign",
    "SoftmaxCrossEntropyWithLogits",
    "Softplus",
    "SplitVD",
    "SqrtGrad",
    "Square",
    "SquaredDifference",
    "SquareSumV1",
    "SquareSumV2",
    "StridedSliceD",
    "Sub",
    "Tanh",
    "TileD",
    "Unpack",
}
RL_OP_BLACK_LIST = {
    "antiquant",
    "ascenddequant",
    "ascendquant",
    "batchmatmul",
    "batchmatmulv2",
    "batchnorm",
    "batchnormext2",
    "batchnormgrad",
    "bninferconv2dbackpropinputd",
    "bninference",
    "bninferenced",
    "bninfergrad",
    "concat",
    "concatd",
    "conv2d",
    "conv2dbackpropfilterd",
    "convolution",
    "crop",
    "deconvolution",
    "depthwiseconv2d",
    "depthwiseconv2dbackpropfilterd",
    "depthwiseconv2dbackpropinputd",
    "dequant",
    "dropout",
    "dynamicgru",
    "dynamicgruv2",
    "dynamicgruv2hidden",
    "flatten",
    "fsrdetectionoutput",
    "fullyconnection",
    "gather",
    "gathernd",
    "gatherv2",
    "gatherv2d",
    "gaussian",
    "innerproduct",
    "LogSoftmaxGrad",
    "LogSoftmaxV2",
    "lrn",
    "matmul",
    "matmulv2",
    "maxpool",
    "msra",
    "pooling",
    "priorboxdv2",
    "proposal",
    "proposald",
    "psroipooling",
    "quant",
    "reshape",
    "roipooling",
    "select",
    "shufflechannel",
    "slice",
    "SoftmaxGrad",
    "SoftmaxV2",
    "ssddecodebbox",
    "ssddetectionoutput",
    "transdata",
    "transposed",
    "upsample",
    "xavier",
    "yolo",
    "yolov2detectionoutputd",
    "yolov3detectionoutputv2d",
}
FUSED_OP_BLACK_LIST = [
    ["mul", "tanhgrad", "sigmoidgrad"],
    ["mul", "log", "reciprocal"]
]
OP_CONFIG_INFO = {
    "all_op_switch": "off",
    "tune_op_list": [
    ]
}


class RLOpFilter:
    """
    class for filter rl op by blacklist and whitelist
    """
    def __init__(self) -> None:
        """
        load config json
        """
        self.op_config_info = OP_CONFIG_INFO
        self.config_op_list = self._get_config_op_list()
        self.is_tune_all_op = self._is_tune_all_op()

        self.single_op_black_list = {op_type.lower() for op_type in RL_OP_BLACK_LIST}
        self.fused_op_black_list = ["_".join(
            sorted((op_type.lower() for op_type in fused_op_rule))) for fused_op_rule in FUSED_OP_BLACK_LIST]
        self.single_op_white_list = {op_type.lower() for op_type in RL_OP_WHITE_LIST}

    @staticmethod
    def is_conv2d_l1fusion(op_info: dict) -> bool:
        """
        check such op whether is multi conv2d L1Fusion
        :param op_info: op"s info loaded from op json(fusion_op_te_xxx.json)
        :return: true: multi conv2d L1Fusion, false: not
        """
        scope_id = int(op_info.get("scope_id", 0))
        if scope_id <= 0:
            return False

        kernel_name = op_info.get("fusion_op_name")
        node_list = op_info.get("op_list")
        op_type_list = []
        for node_info in node_list:
            if node_info.get("type") == "Data":
                continue
            op_type_list.append(node_info.get("type").lower())

        if op_type_list.count("conv2d") > 1:
            log.info("RL tune info: fused op %s is conv2d L1Fusion, scope_id: %s.", kernel_name, scope_id)
            return True

        return False

    @staticmethod
    def _check_output_dtype(all_output_info: list, input_name_list: list, kernel_name: str) -> bool:
        """
        check op's output dtype, now only support "float16" and "float32"
        :return: true: support; false: unsupport
        """
        for output_info in all_output_info:
            # inter output, not real op output
            if output_info.get("name") in input_name_list:
                continue
            if output_info.get("data_type") not in OUTPUT_DTYPE_WHITE_LIST:
                log.warn("RL filter info: op %s output_dtype is %s, not in %s, rl tune not support!",
                         kernel_name, output_info.get("data_type"), str(OUTPUT_DTYPE_WHITE_LIST))
                return False
        return True

    def optype_check_support(self, op_type_list: list, kernel_name: str) -> bool:
        """
        check whether op is supported by rl tune, called in the rl_bank.py
        :param op_type_list: op's type get from context
        :param kernel_name: op's kernel_name get from context
        :return: true:supported, false: not supported
        """
        if not isinstance(op_type_list[0], str):
            log.warn("RL filter info: op_type format is not str, only support str type.")
            return False

        if len(op_type_list) == 1:
            is_supported = self._is_single_optype_supported(op_type_list[0])
        else:
            is_supported = self._is_fusion_optype_supported(op_type_list, kernel_name)
        return is_supported

    def rl_check_support(self, op_info: dict) -> bool:
        """
        check whether op is supported by rl tune, called in the tuning_utils.py
        :param op_info: op's type loaded from op json(fusion_op_te_xxx.json)
        :return: true:supported, false: not supported
        """
        scope_id = int(op_info.get("scope_id"))
        if scope_id < 0:
            is_supported = self._is_single_op_supported(op_info)
        else:
            is_supported = self._is_fusion_op_supported(op_info)
        log.debug("RL filter info: op %s whether is supported or not by rl tune: %s",
                op_info.get("fusion_op_name"), str(is_supported))
        return is_supported

    def _get_config_op_list(self) -> set:
        """
        init op list from config file
        :return: tune op list from config file
        """
        tune_op_list = set()
        if self.op_config_info and self.op_config_info.get("tune_op_list"):
            tune_op_list = {str(op_type).lower() for op_type in self.op_config_info.get("tune_op_list")}
            log.info("RL filter info: get tune op list from op_config_info, op list:%s", str(tune_op_list))
        return tune_op_list

    def _is_tune_all_op(self) -> bool:
        """
        parse all_op_switch in config file, all_op_switch only support "on" and "off"
        :return: true: all_op_switch="on", false: all_op_switch="off"
        """
        tune_all_op = False
        if self.op_config_info and self.op_config_info.get("all_op_switch"):
            all_op_switch = str(self.op_config_info.get("all_op_switch")).lower()
            if all_op_switch not in {"on", "off"}:
                log.warn("RL filter info: all_op_switch:%s is not supported, only support \"on\" and \"off\"",
                    str(all_op_switch))
            tune_all_op = (all_op_switch == "on")
        log.debug("RL filter info: is_tune_all_op: %s", str(tune_all_op))
        return tune_all_op

    def _is_single_optype_supported(self, op_type: str) -> bool:
        """
        check by whitelist and blacklist
        :paramï¼š op's type loaded from op json(fusion_op_te_xxx.json) or get from context
        :return: true: supported, false: not supported
        """
        op_type_lower = op_type.lower()
        if op_type_lower in self.single_op_black_list:
            log.warn("RL filter info: op type [%s] in op blacklist, rl tune not support!", op_type)
            return False

        if self.is_tune_all_op:
            # if all_op_switch is on, then tune all op
            log.info("RL filter info: op type [%s] is enabled by all_op_switch=on in op_config_info.", op_type)
            return True

        # if all_op_switch is off, then filter single op by white list
        if op_type_lower in self.single_op_white_list:
            log.debug("RL filter info: op type [%s] in op whitelist, rl tune support!", op_type)
        elif op_type_lower in self.config_op_list:
            log.info("RL filter info: op type [%s] is enabled by tune_op_list in .", op_type)
        else:
            log.warn("RL filter info: op type [%s] not in op whitelist, rl tune not support!", op_type)
            return False
        return True

    def _is_fusion_optype_supported(self, op_type_list: list, kernel_name: str) -> bool:
        """
        check by blacklist
        :param op_type_list: op's type loaded from op json(fusion_op_te_xxx.json) or get from context
        :param kernel_name: op's kernel_name loaded from op json(fusion_op_te_xxx.json) or get from context
        :return: true: supported, false: not supported
        """
        op_type_lower = []
        for op_type in op_type_list:
            # if one of fused ops in black_list, will not supported by rl tune
            op_type_lower.append(op_type.lower())
            if op_type.lower() in self.single_op_black_list:
                log.warn("RL filter info: fused op %s is not supported.", kernel_name)
                return False

        # if fused ops in fused_op_black_list, will not supported by rl tune
        op_type_join = "_".join(sorted(op_type_lower))
        for fused_op_black_rule in self.fused_op_black_list:
            if fused_op_black_rule in op_type_join:
                log.warn("RL filter info: fused op %s is not supported.", kernel_name)
                return False
        return True

    def _is_single_op_supported(self, op_info: dict) -> bool:
        """
        check by whitelist and blacklist
        :param op_info: op's info loaded from op json(fusion_op_te_xxx.json)
        :return: true: supported, false: not supported
        """
        kernel_name = op_info.get("fusion_op_name")
        node_list = op_info.get("op_list")
        input_name_list = []
        all_output_info = []
        op_type = ""
        for node_info in node_list:
            if node_info.get("type") == "Data":
                continue
            all_output_info.extend(node_info.get("output_desc"))
            input_name_list.extend((x.get("name") for x in node_info.get("input_desc")))
            op_type = node_info.get("type")

        optype_is_supported = self._is_single_optype_supported(op_type)
        dtype_is_supported = self._check_output_dtype(all_output_info, input_name_list, kernel_name)
        return optype_is_supported and dtype_is_supported

    def _is_fusion_op_supported(self, op_info: dict) -> bool:
        """
        check by blacklist
        :param op_info: op's info loaded from op json(fusion_op_te_xxx.json)
        :return: true: supported, false: not supported
        """
        kernel_name = op_info.get("fusion_op_name")
        node_list = op_info.get("op_list")
        input_name_list = []
        all_output_info = []
        op_type_list = []
        for node_info in node_list:
            if node_info.get("type") == "Data":
                continue
            op_type_list.append(node_info.get("type"))
            all_output_info.extend(node_info.get("output_desc"))
            input_name_list.extend((x.get("name") for x in node_info.get("input_desc")))

        optype_is_supported = self._is_fusion_optype_supported(op_type_list, kernel_name)
        dtype_is_supported = self._check_output_dtype(all_output_info, input_name_list, kernel_name)
        return optype_is_supported and dtype_is_supported


rl_op_filter = RLOpFilter()
