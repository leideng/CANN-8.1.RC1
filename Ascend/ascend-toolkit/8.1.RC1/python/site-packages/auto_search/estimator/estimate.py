#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
estimate
"""
from typing import Union
from typing import Tuple
from typing import List
from typing import Any
import os
import time
import json
import math
from io import StringIO
from contextlib import redirect_stdout
from contextlib import redirect_stderr
from functools import reduce as functools_reduce

from tbe import tvm
from tbe.common.rl_bank.withdraw import withdraw
from tbe.common.rl_bank.static_rl_bank import RL_STATIC_KERNEL_KEY
import tbe
from tbe.dsl.base import operation
from auto_search.utils import logger
from auto_search.utils import util as comm_util
from auto_search.config import soc_cfg
from auto_search.utils.util import ErrorCode
from auto_search.utils.util import store_tmp_proc
from auto_search.estimator.om_runner import OMRunner
from auto_search.utils.util import run_cmd_comm
from auto_search.utils.util import SchInfo
from auto_search.utils.util import KernelInfo
from tbe.common.platform import set_current_compile_soc_info
from tbe.common.platform.platform_info import get_soc_spec


class OomError:
    """
    OomError
    """
    L1_BUFF = "l1_oom"
    UB_BUFF = "ub_oom"
    L0AB = "l0ab_oom"
    L0C = "l0c_oom"
    SEGMENT = "segment_oom"
    OVERFLOW = "overflow_oom"
    ALL = [L1_BUFF, UB_BUFF, L0AB, L0C, SEGMENT, OVERFLOW]

    def useless_func1(self):
        """
        useless_func1
        :return:
        """
        return self.UB_BUFF

    def useless_func2(self):
        """
        useless_func2
        :return:
        """
        return self.ALL


class KernelRunArgsInfo:
    """
    KernelRunArgsInfo
    """

    def __init__(self,  # pylint: disable=R0913
                 op_name,
                 output_cnt,
                 kernel_bin_path,
                 unique_id,
                 gen_golden=0):
        self.kernel_bin_path = kernel_bin_path
        self.unique_id = unique_id
        self.gen_golden = gen_golden

        kernel_json_path = kernel_bin_path.replace(".o", ".json")
        if not os.path.exists(kernel_bin_path) or not os.path.exists(kernel_json_path):
            raise RuntimeError("%s or %s does not exists." % (kernel_bin_path, kernel_json_path))

        with open(kernel_json_path, 'r') as file_handler:
            kernel_dict = json.load(file_handler)
            self.block_dim = kernel_dict.get("blockDim", 1)
            self.kernel_name = kernel_dict.get("binFileName", "unknown")
            self.workspace_sizes = kernel_dict.get("workspace", {}).get('size', [])

        self.inplace_list = [-1] * output_cnt

    def useless_func1(self):
        """
        useless_func1
        :return:
        """
        return self.gen_golden

    def useless_func2(self):
        """
        useless_func2
        :return:
        """
        return self.kernel_name


def _get_oom_type(
        fail_str,
        default_type=ErrorCode.BUILD_FAIL):
    """

    :param fail_str:
    :param default_type:
    :return:
    """
    if 'exceed bound of memory' in fail_str:
        if 'local.UB' in fail_str:
            return OomError.UB_BUFF
        if 'local.L1' in fail_str:
            return OomError.L1_BUFF
        if 'local.L0C' in fail_str:
            return OomError.L0C
        if 'local.L0A' in fail_str or 'local.L0B' in fail_str:
            return OomError.L0AB

    for curr_fail_str in [
        'Allocation exceed bound of memory tag:local.L0',
        'm < Arch: :Matrix::MATRIX_MAX_M',
        'm < Arch::Matrix::MATRIX_MAX_M',
        'n < Arch: :Matrix::MATRIX_MAX_N',
        'n < Arch::Matrix::MATRIX_MAX_N',
        'k < Arch: :Matrix::MATRIX_MAX_K', 'k < Arch::Matrix::MATRIX_MAX_K'
    ]:
        if curr_fail_str in fail_str:
            return OomError.L0AB

    if 'Segmentation fault (core dumped)' in fail_str:
        return OomError.SEGMENT

    if ' overflow' in fail_str:
        return OomError.OVERFLOW

    if 'the start address of operands must be 32 byte aligned' in fail_str or \
            'vector shape is not 32B align' in fail_str:
        return ErrorCode.BUILD_NOALIGN
    return default_type


def get_tensors_by_tensors_str(tensor_list_str, all_tensors, all_tensor_names, input_info_list=None):
    """
    get real tensor from sch by tensor_list_str
    """
    tensor_list = []

    tensor_list_str = tensor_list_str.lstrip('[').rstrip(']').replace(' ', '')
    if not tensor_list_str:
        return tensor_list
    tensor_name_list = tensor_list_str.split(',')
    for tensor_name in tensor_name_list:
        if tensor_name not in set(all_tensor_names):
            if input_info_list is None:
                raise RuntimeError('cannot find tensor name %s in sch, %s' %
                                   (tensor_name, all_tensor_names))
            else:
                input_names = [input_info.name for input_info in input_info_list]
                if tensor_name not in input_names:
                    raise RuntimeError('cannot find tensor name %s in sch, %s' %
                                       (tensor_name, all_tensor_names))
                input_index = input_names.index(tensor_name)
                new_tensor = tvm.placeholder(input_info_list[input_index].shape, name="data_input_axes",
                                             dtype=input_info_list[input_index].dtype)
                tensor_list.append(new_tensor)
                continue
        tensor_list.append(all_tensors[all_tensor_names.index(tensor_name)])
    logger.debug("%s --> %s", tensor_name_list, tensor_list)
    return tensor_list


def get_all_tensors(sch, special_tensor_dict):
    """
    """
    # Because multioutput will add a fake node except tuple_reduce_sum, so get sch.outputs[0]
    all_tensors = []
    for idx in range(sch.outputs[0].num_outputs):
        all_tensors.append(sch.outputs[0].output(idx))
    tensors = all_tensors[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if isinstance(tensor.op, tvm.PlaceholderOp):
                continue
            new_tensors.extend(tensor.op.input_tensors)
        new_tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(new_tensors)
        tensors = new_tensors
    all_tensors = list(set(all_tensors))
    all_tensor_names = []
    for tensor in all_tensors:
        if tensor.name.startswith("%s.v" % tensor.op.name):
            tensor_idx = tensor.name.split('.v')[-1]
            all_tensor_names.append('%s_v%s' % (tensor.op.name, tensor_idx))
            continue
        all_tensor_names.append(tensor.op.name)
    for tensor_name, tensor in special_tensor_dict.items():
        all_tensor_names.append(tensor_name)
        all_tensors.append(tensor)
    logger.debug("all_tensors: %s, all_tensor_names: %s", all_tensors,
                 all_tensor_names)
    return all_tensors, all_tensor_names


def _gen_kernel_name(op_name, unique_id=None):
    """
    :param op_name:
    :param unique_id:
    :return:
    """
    # cut out 30 char from op name
    op_name = op_name[:30]
    if unique_id is None:
        curr_pid = os.getpid()
        curr_time = int(time.time() * 1000)
        unique_id = str(curr_pid) + "_" + str(curr_time)
        kernel_name = "rltune__%s_%s" % (op_name, unique_id)
    elif unique_id == "":
        kernel_name = op_name
    else:
        kernel_name = "rltune__%s_%s" % (op_name, unique_id)
    return kernel_name, unique_id


def _get_build_config(sch,
                      op_schedule_info,
                      print_output=False,
                      kernel_name=""):
    """
    """
    all_tensors, all_tensor_names = get_all_tensors(
        sch, op_schedule_info.special_tensor_dict)
    tensor_list_str = op_schedule_info.tensor_list_str

    # config
    config = {"print_ir": print_output, "need_build": True}
    if not kernel_name:
        config["name"], config["unique_id"] = _gen_kernel_name(op_schedule_info.op_name)
    else:
        config["name"] = kernel_name
    config["tensor_list"] = get_tensors_by_tensors_str(tensor_list_str,
                                                       all_tensors,
                                                       all_tensor_names, op_schedule_info.input_info_list)
    config["bool_storage_as_1bit"] = False
    config["kernel_meta_parent_dir"] = soc_cfg.kernel_meta_parent_dir()
    logger.debug("tensor_list_str: %s, config: %s", tensor_list_str, config)

    return config


def _gen_sch_by_cheque(out_tensors: List, action_list: List, sync_tensor: List = None) \
        -> Union[Tuple[bool, Any, List], Tuple[bool, None, None]]:
    """
    gen_sch_by_cheque
    :param out_tensors: output tensor
    :param action_list: cheque list
    :param sync_tensor: None is default value
    :param dynamic: True for dynamic op ,False for static op
    :return:tuple (result, schedule, code_lines)
    """
    try:
        sch, code_lines = withdraw(out_tensors, action_list, "runtime", sync_tensor)
    except RuntimeError:
        return False, None, None
    return True, sch, code_lines


def _static_build(sch_info, op_schedule_info, config):
    """
    static build
    :param sch_info:
    :param op_schedule_info:
    :param config:
    :return:
    """
    with tbe.common.context.op_context.OpContext("static"):
        op_context = tbe.common.context.op_context.get_context()
        operation.add_build_arg("double_buffer_non_reuse", True)
        operation.get_context().add("hit_static_rl_bank", True)
        with tbe.dsl.compute() as cpt_context:
            op_pattern = op_schedule_info.op_pattern
            cpt_context.set_pattern(op_pattern)

            with operation.schedule() as context:
                context.add("_sch_idx", 0)
                out_tensors = []
                for tensor in config["tensor_list"]:
                    if isinstance(tensor.op, tvm.ComputeOp):
                        out_tensors.append(tensor)
                ret, sch, code_lines = _gen_sch_by_cheque(out_tensors, sch_info.cheque_list)
                sch.tiling_key = RL_STATIC_KERNEL_KEY
                op_schedule_info.tiling_key = str(RL_STATIC_KERNEL_KEY)
                sch.addition = {"original_outs": sch.cce_special.get("orign_out_tensor"),
                                "real_outs": sch.cce_special.get("real_out_tensor"), "context": context}
                # calc workspace info
                workspace_tensor = sch.cce_special.get("tensor_list")
                workspace_num = len(workspace_tensor)
                workspace_size = []
                workspace_type = []
                if workspace_num > 0:
                    for tensor in workspace_tensor:
                        tensor_shape = comm_util.shape_to_list(tensor.shape)
                        block_size = comm_util.get_block_num(tensor.dtype)
                        align_tensor_shape = tensor_shape[:]
                        align_tensor_shape[-1] = math.ceil(align_tensor_shape[-1] / block_size) * block_size
                        shape_size = functools_reduce(lambda x, y: x * y, align_tensor_shape)
                        workspace_size.append(comm_util.get_dtype_size(tensor.dtype) * shape_size)
                        workspace_type.append(0)

                    workspace_dict_in_json = {
                        "num": workspace_num,
                        "size": workspace_size,
                        "type": workspace_type
                    }
                    op_context.add_build_json_result("workspace", workspace_dict_in_json)
                del sch.cce_special

                if ret:
                    for i, _ in enumerate(code_lines):
                        code_lines[i] = '    %s' % code_lines[i]
                    schedule_code = "\n".join(code_lines)
                    py_file = store_tmp_proc(op_schedule_info, schedule_code,
                                             'cheque_sch', config.get('name'))
                    logger.debug("cheque_sch py file: %s", py_file)

            tbe.dsl.build([sch], config)
            logger.debug("static cce_build_code finished.")


def _cce_build_code(sch_info, op_schedule_info, config):
    """
    compile for .o.
    Note Maybe stuck in Lower, so need add timeout for protecting
    """
    f_out = StringIO()
    f_err = StringIO()
    with redirect_stderr(f_err), redirect_stdout(f_out):
        if op_schedule_info.option.get('op_config').get('op_mode', '') in ['static']:
            _static_build(sch_info, op_schedule_info, config)
            return
        logger.error("op mode is unknown: %s", op_schedule_info.option.get('op_config').get('op_mode', ''))


def _get_schedule_code(op_schedule_info, sch_info):
    """

    :param sch_info:
    :return:
    """
    if not isinstance(sch_info, list):
        return sch_info.code

    schedule_code = ""
    cheque_lists = []
    for i, single_sch_info in enumerate(sch_info):
        schedule_code += "\ndef dsl_func_{}():".format(i)
        schedule_code += "    {}\n".format(op_schedule_info[i].compute_code)
        schedule_code += "\n    # cheque_list: {}\n".format(
            single_sch_info.cheque_list)
        schedule_code += "    {}\n    return sch\n".format(
            single_sch_info.code)
        cheque_lists.append(single_sch_info.cheque_list)
    schedule_code += "\nCHEQUE_LISTS = {}\n\n".format(cheque_lists)
    return schedule_code


def _build_fail_proc(err_str, kernel_bin_path, op_schedule_info, sch_info,
                     kernel_name):
    """
    """
    error_code = _get_oom_type(err_str)
    if "timeout" in err_str:
        error_code = ErrorCode.BUILD_TIMEOUT
        error_dir = ErrorCode.BUILD_TIMEOUT
    elif error_code in OomError.ALL:
        error_dir = ErrorCode.BUILD_OOM
    elif error_code == ErrorCode.BUILD_NOALIGN:
        error_dir = ErrorCode.BUILD_NOALIGN
    else:
        error_dir = ErrorCode.BUILD_FAIL
    schedule_code = _get_schedule_code(op_schedule_info, sch_info)
    if isinstance(op_schedule_info, list):
        single_op_schedule = op_schedule_info[0]
    else:
        single_op_schedule = op_schedule_info
    py_file = store_tmp_proc(single_op_schedule, schedule_code, error_dir,
                             kernel_name)
    if error_dir == ErrorCode.BUILD_FAIL:
        logger.debug("%s", py_file)
    if os.path.exists(kernel_bin_path):
        remove_dir, remove_file = os.path.split(
            os.path.splitext(kernel_bin_path)[0])
        run_cmd_comm("rm -rf %s" %
                     os.path.join(remove_dir, "*%s*" % remove_file))
    return error_code


def _build_kernel(  # pylint: disable=R0912,R0914
        sch_info,
        config,
        op_schedule_info):
    """
    compile schedule into .o, and catch error info
    """
    kernel_name = config["name"]
    kernel_bin_path = os.path.join(soc_cfg.kernel_meta_dir(),
                                   "%s.o" % kernel_name)
    unique_id = config["unique_id"]
    err_print = os.getenv('CONTEXT_MODELCOMPILING')
    os.environ['CONTEXT_MODELCOMPILING'] = 'TRUE'

    op_name = op_schedule_info.op_name
    output_cnt = len(op_schedule_info.output_info_list)
    try:
        _cce_build_code(sch_info, op_schedule_info, config)
    except Exception as exception:
        logger.warn("build exception occurred, exception is %s.", str(exception))
        error_code = _build_fail_proc(str(exception), kernel_bin_path,
                                      op_schedule_info, sch_info, kernel_name)
        if err_print:
            os.environ['CONTEXT_MODELCOMPILING'] = err_print
        return error_code, None

    soc_version = get_soc_spec("FULL_SOC_VERSION")
    if soc_version in ['Ascend310P3']:
        set_current_compile_soc_info("Ascend310P3", "VectorCore")
        try:
            _cce_build_code(sch_info, op_schedule_info, config)
        except Exception as exception:
            logger.warn("build exception occurred, exception is %s.", str(exception))
            error_code = _build_fail_proc(str(exception), kernel_bin_path,
                                          op_schedule_info, sch_info, kernel_name)
            if err_print:
                os.environ['CONTEXT_MODELCOMPILING'] = err_print
            return error_code, None

    kernel_run_obj = KernelRunArgsInfo(op_name, output_cnt,
                                       kernel_bin_path, unique_id)
    if err_print:
        os.environ['CONTEXT_MODELCOMPILING'] = err_print
    return ErrorCode.BUILD_SUCC_, kernel_run_obj


def actions_to_kernel_bin(action_tensors,
                          op_schedule_info,
                          sample_actions=None,
                          print_output=False):
    """
    """
    code_lines = op_schedule_info.code_lines
    # add four character indent for all
    for i, _ in enumerate(code_lines):
        code_lines[i] = '    %s' % code_lines[i]
    code_lines.extend([''])
    schedule_code = '\n'.join(code_lines)
    sch_info = SchInfo(op_schedule_info.schedule_obj, schedule_code, action_tensors,
                       None, op_schedule_info.cheque_list)

    config = _get_build_config(sch_info.sch, op_schedule_info, print_output)
    logger.debug("get build_config finished.")

    error_code, kernel_run_obj = _build_kernel(sch_info, config,
                                               op_schedule_info)
    logger.debug("build_kernel finished.")

    return KernelInfo(error_code, kernel_run_obj, sch_info)


def proc(op_schedule_info, kernel_info):
    """
    :param op_schedule_info:
    :param kernel_info:
    :return:
    """
    if isinstance(op_schedule_info, list):
        single_op_schedule = op_schedule_info[0]
    else:
        single_op_schedule = op_schedule_info

    if isinstance(kernel_info, list):
        kernel_run_obj = kernel_info[0].kernel_run_obj
    else:
        kernel_run_obj = kernel_info.kernel_run_obj

    runner = OMRunner(single_op_schedule, kernel_info, run_base=False)
    ret, tick, err_code = runner.run()
    if not ret:
        return err_code, tick

    logger.debug("replay.do, run_kernel success, unique_id: %s! tick: %d",
                 kernel_run_obj.kernel_name, tick)
    return err_code, tick
