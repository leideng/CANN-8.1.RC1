#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.

rl schedule search, tss
"""
import os
import json
import copy
import hashlib
from collections import namedtuple
from functools import reduce as functools_reduce
from typing import Optional
from typing import List

from tbe import tvm
from tbe.common.rl_bank.search_bank import get_rl_bank_key
from auto_search.utils import logger
from auto_search.utils import util
from auto_search.solution_space import depend
from auto_search.estimator.estimate import get_all_tensors
from auto_search.utils.dynamic_util import BlockSplitInfo
from auto_search.utils.dynamic_util import UbSplitInfo
from auto_search.config.cce_intrin_map import OP_INTRIN_KEY_INDEX
from auto_search.compute_analysis.reduce_compute_graph import ReduceComputeGraphInfo
from auto_search.compute_analysis.compute_pattern import ComputePattern
from auto_search.solution_space.t2c_util import SplitInfo
from auto_search.solution_space.t2c_util import MODE_RUNTIME
from auto_search.compute_analysis import ReduceInfoForRfactor
from auto_search.solution_space import tvm_compute as tvm_compute_to_tensor
from auto_search.solution_space.tensor_cfg import MAX_ORIGIN_STAGE_CNT
from auto_search.solution_space.tensor_cfg import TensorInfo
from auto_search.config.config import WORKSPACE
from auto_search.compute_analysis.pattern_parser import get_pattern
from auto_search.solution_space.tensor_cfg import AXIS_CNT
from auto_search.compute_analysis import get_stage_ordered_axes_obj
from auto_search.estimator.om_runner import OMRunner
from auto_search.config import soc_cfg

FILE_PATH = os.path.dirname(os.path.realpath(__file__))

RETRY_TIMES = 3
StageParams = namedtuple("StageParams",
                         ["all_stage_list", "origin_outputs", "input_def_args_list", "input_run_args_list",
                          "data_type"])

# 数据比对的模式
CHECK_OUTPUT_TYPE_NONE = 0
CHECK_OUTPUT_TYPE_MEMCMP = 1
CHECK_OUTPUT_TYPE_ELEMENT_CHECK = 2
CHECK_OUTPUT_TYPE_SUM_CHECK = 3

NEED_UPDATE_INPUT_INFO_OP_NAME = ['reduce_sum', 'reduce_all', 'reduce_any', 'reduce_mean', 'reduce_min', 'broadcast_to']


class OpScheduleInfo:
    """
    OpScheduleInfo
    """

    def __init__(
            self,
            op_name,
            feature_tensor,
            schedule_obj,
            input_info_list,
            output_info_list,
            op_md5,
            compute_code,
            check_output,
            option,
            reduce_axis_dict,
            stages_info,
            res=None):
        self.output_tensors = res
        self.op_name = op_name
        self.feature_tensor = feature_tensor
        self.schedule_obj = schedule_obj
        self.compute_code = compute_code
        self.check_output = check_output
        self.input_info_list = input_info_list
        self.output_info_list = output_info_list
        self.reduce_axis_dict = reduce_axis_dict
        self.op_md5 = op_md5
        self.option = option

        self.shape_list_str = _gen_shape_tag(input_info_list, option, op_md5)
        self.tensor_list_str = ""
        self.orign_out_tensor_str = ""
        self.real_out_tensor_str = ""
        self.api_def_args = ""
        self.api_run_args = ""
        self.input_file_list = []
        self.input_size_list = []
        self.base_tick = 0
        self.base_totaltime = 0
        self.base_run_time = 0
        self.axis_info_list = []
        self.stages_info = stages_info
        self.ori_op_name = self.get_ori_op_name()
        # variables for at
        self.ori_at_dict = {}
        self.at_choices = []
        self.at_dict = {}
        self.at_targets = []

        self.workspace = self.option.get("WORKSPACE", "")
        self.replay_dir = os.path.join(self.workspace, "replay_dir")
        self.lock_dir = os.path.join(self.workspace, "local_lock")
        self.stage_index_map = {}
        self.workspace_list_str = ""
        self.workspace_size = []
        # in out dict
        self.fanin_dict = {}
        self.fanout_dict = {}
        self.real_fanin_dict = {}
        self.real_fanout_dict = {}
        self.all_fanin_dict = {}
        self.use_ba_pattern_brc_stage_list = []

        self.store_dir = ''

        all_stage_list, origin_outputs, input_def_args_list, input_run_args_list, data_type = \
            self._get_stage_list(input_info_list)
        self.tensor_list_str = "[" + ", ".join(all_stage_list) + "]"
        self.orign_out_tensor_str = "[" + ", ".join(origin_outputs) + "]"
        self.real_out_tensor_str = "[" + ", ".join(origin_outputs) + "]"

        self.api_def_args = ", ".join(input_def_args_list) + ", dtype"
        self.api_run_args = ", ".join(input_run_args_list) \
                            + ", '%s'" % data_type

        self.code_lines = []
        self.cheque_list = None
        self.inlined_stages = []
        self.reduce_atomic_dict = {}
        self.axis_info_list = []
        self.special_tensor_dict = {}

        # variables for dynamic vector op to infer range
        self.block_split_info: Optional[BlockSplitInfo] = None
        self.ub_split_info_list: Optional[List[UbSplitInfo]] = None
        self.is_need_block_tiling: bool = True
        self.ub_is_full_cut: bool = False
        self.ub_factor: Optional[int] = None
        self.ub_split_outer_name = ""
        self.cheque_list_var: List = []
        self.infer_range: List = []
        # variables for reduce feature
        self.is_atomic: bool = False
        self.enable_atomic_ra_bind = False
        self.reduce_info_for_rfactor: ReduceInfoForRfactor = None
        self.last_reduce_and_ub_split_last_reduce_axis: bool = False
        self.last_reduce_and_ub_split_nlast_reduce_axis: bool = False
        self.is_ub_transpose: bool = False
        self.last_reduce: bool = False
        self.atomic_last_reduce_and_ub_split_last_reduce_axis: bool = False
        self.atomic_last_reduce_and_ub_split_nlast_reduce_axis: bool = False
        self.atomic_last_reduce: bool = False
        self.atomic_nlast_reduce: bool = False
        self.atomic_ub_split_type: str = ''
        self.atomic_block_split_info: Optional[SplitInfo] = None
        self.atomic_ub_split_info: Optional[SplitInfo] = None
        # variables for pad
        self.is_align_pad: bool = False
        self.is_remove_pad: bool = False
        # variables for db
        self.is_open_db: bool = False
        # compute pattern
        self.op_pattern = get_pattern(self.output_tensors)
        # compute graph
        self.compute_graph_info = _get_compute_graph_info(self.op_pattern, schedule_obj)
        # cheque_list
        self.cheque_list = []
        # tiling info
        self.cut_axis_index = {}
        self.last_cut_axis_index = {}
        self.bind_stages = []
        self.tiling_record = None
        # mode
        self.mode = MODE_RUNTIME
        # variables for reorder
        self.dma_reorder_list = []
        self.leveled_axis_for_reorder = []
        # variables for record have been processed or not
        self.proc_flag_dict = {}
        # variables for reused_by
        self.reuse_stages = []
        self.reused_input_list = []
        # use for avoid op_scehdule_info deepcopy bug
        self.set_buffer_size = False
        self.is_reused_by = False
        self.is_mem_unique = False
        # record storage_align para for compute align
        self.storage_align_para = {}
        # op_intrin_key_index for emit insn
        self.op_intrin_key_index = OP_INTRIN_KEY_INDEX
        # whether need compute_align
        self.compute_align_index = []

    def __str__(self):
        print_str = ["op_name: %s\noption:%s\ncheck_output:%s\nop_md5:%s\n" %
                     (self.op_name, self.option, self.check_output, self.op_md5)]
        for input_info in self.input_info_list:
            print_str.append("input name: %s, input shape: %s, input dtype: %s\n"
                             % (input_info.name, input_info.shape,
                                input_info.dtype))
        for output_info in self.output_info_list:
            print_str.append("output name: %s, output shape: %s, output "
                             "dtype: %s" % (output_info.name,
                                            output_info.shape,
                                            output_info.dtype))
        return ''.join(print_str)

    @staticmethod
    def _gen_at_candidate(consumer_info, dict_subtree_ats, depended_stage_index, spec_nodes, workspace_nodes):
        if depended_stage_index in workspace_nodes:
            consumer_info.add_at_candidate(depended_stage_index)
            return
        for candidate in workspace_nodes:
            if depended_stage_index == candidate or candidate in dict_subtree_ats.get(depended_stage_index):
                consumer_info.add_at_candidate(candidate)
        new_candidate_list = []
        for candidate in spec_nodes:
            if depended_stage_index == candidate or candidate in dict_subtree_ats.get(depended_stage_index):
                new_candidate_list.append(candidate)

        for candidate in new_candidate_list:
            consumer_info.add_at_candidate(candidate)

    def at_analysis(self):
        stages_info = self.stages_info
        schedule_obj = self.schedule_obj

        _, dict_depended = util.get_depends(schedule_obj)

        for i in range(len(stages_info) - 1, -1, -1):
            stage_info = stages_info[i]
            if stage_info.get("at_info"):
                logger.debug("skip handled forked node, index:%s", i)
                continue

            at_info = AtInfo(i)
            stage_info["at_info"] = at_info
            # set last stage's at_target
            if i == len(stages_info) - 1:
                consumer_info = ConsumerInfo(i)
                consumer_info.set_at_candidates([i])
                consumer_info.set_sampled_target(i)
                at_info.add_consumer(consumer_info)
                continue
            self._at_analysis_for_one_stage(at_info, dict_depended, i, stages_info)

            if at_info.need_sample():
                logger.debug("Is node:%s workspace need to be decided by sampler", i)
                return True

        return False

    def update_worksapce_list(self):
        """
        update_worksapce_list
        """
        workspace_list = []
        workspace_size = []
        for stage_info in self.stages_info:
            if 'workspace' in stage_info.get('type', []):
                workspace_list.append(stage_info['name'])
        self.workspace_list_str = "[" + ", ".join(workspace_list) + "]"

        if workspace_list:
            all_tensors, _ = get_all_tensors(self.schedule_obj,
                                             self.special_tensor_dict)
            for name in workspace_list:
                for tensor in all_tensors:
                    if name == tensor.op.name:
                        tensor_shape = util.shape_to_list(tensor.shape)
                        shape_size = functools_reduce(lambda x, y: x * y, tensor_shape)
                        workspace_size.append(util.get_dtype_size(tensor.dtype) * shape_size)
        self.workspace_size = workspace_size

    def update_at_dict(self):
        """
        update_at_dict
        """
        for i, stage_info in enumerate(self.stages_info):
            at_info = stage_info.get('at_info')
            # 没有inline的stage, 非最后一个stage
            if at_info and i not in self.inlined_stages and i != len(self.stages_info) - 1:
                at_target = at_info.get_at_target()
                # 如果为-1，表示最后一个stage, 进行更新
                self.at_dict[i] = at_target if at_target != -1 else len(self.stages_info) - 1
            else:
                self.at_dict[i] = None
            if {'CacheRead', 'CacheWrite'} & set(stage_info.get('type', [])):
                continue
            self.ori_at_dict[at_info.ori_index] = at_info.get_sampled_target()

    def update_moves(self, moves):
        """

        :param moves:
        """
        self.compute_code += "\n    # moves: %s\n" % str(moves)

    def update_stages_info(self):
        """
        1, 更新stage info中的workspace属性；2, 更新comsumer的at target
        """
        stage_index_map = self.stage_index_map
        for stage_index, stage_info in enumerate(self.stages_info):
            at_info = stage_info['at_info']
            for consumer in at_info.consumers:
                self._update_at_sampled_target(consumer, stage_index, stage_index_map)

    def update_dependency_dict(self):
        """
        update_dependency_dict
        """
        stage_num = len(self.feature_tensor)
        for i in range(stage_num):
            self.fanin_dict[i], self.real_fanin_dict[i] = \
                depend.get_real_fanin_fanout_stages(
                    i, self.feature_tensor, self.inlined_stages, 'fanin')
            self.fanout_dict[i], self.real_fanout_dict[i] = \
                depend.get_real_fanin_fanout_stages(
                    i, self.feature_tensor, self.inlined_stages, 'fanout')

        self.all_fanin_dict = depend.get_fanin_sub_tree_indices(
            self.feature_tensor)

    def get_ori_op_name(self):
        """

        :param self
        :return:
        """
        ori_op_name = self.option.get('op_config').get('ori_op_name', None)
        if ori_op_name:
            return ori_op_name

        kernel_name = self.option.get('op_config').get('kernel_name',
                                                       self.shape_list_str)

        return kernel_name

    def get_compute_op_list(self):
        """

        :param self
        :return:
        """
        compute_op_list = []
        for i, stage in enumerate(self.schedule_obj.stages):
            if isinstance(stage.op, tvm.PlaceholderOp):
                continue
            compute_op_list.append((stage.op, i))
        return compute_op_list

    def stage_num(self):
        return len(self.schedule_obj.stages)

    def get_nonzero_axes(self, stage_index):
        stages_info = self.stages_info
        stage_types = stages_info[stage_index].get('type', [])
        if 'reduce' in stage_types or 'reduce_atomic' in stage_types:
            ordered_axes_obj = get_stage_ordered_axes_obj(self, stage_index)
            nonzero_axes = ordered_axes_obj.get_origin_axis_dim()
        else:
            nonzero_axes = self._get_nonzero_axes_normal(stage_index)
        return nonzero_axes

    def _at_analysis_for_one_stage(self, at_info, dict_depended, curr_stage_index, stages_info):
        spec_nodes = []
        subtree = []
        util.get_sub_tree(curr_stage_index, dict_depended, subtree)
        dict_subtree_ats = {}
        workspace_nodes = []
        for k, sub_stage_info in enumerate(stages_info[curr_stage_index + 1:]):
            stage_index = k + curr_stage_index + 1
            if is_spec_node(sub_stage_info):
                spec_nodes.append(stage_index)

            if sub_stage_info["at_info"].is_fork():
                workspace_nodes.append(stage_index)

            if stage_index in subtree:
                dict_subtree_ats.setdefault(stage_index, [])
                dict_subtree_ats[stage_index] \
                    = sub_stage_info["at_info"].get_all_at_targets()
        dict_paths = {}
        spec_nodes = list(set(spec_nodes))
        spec_stage_names = (stages_info[x]["name"] for x in spec_nodes)
        logger.debug("[%d]spec_nodes:%s, %s: %s", curr_stage_index, spec_nodes,
                     stages_info[curr_stage_index]["name"], spec_stage_names)
        logger.debug("[%d]dict_paths:%s", curr_stage_index, dict_paths)
        spec_node_dict = {"workspace_nodes": workspace_nodes, "spec_nodes": spec_nodes}
        self._gen_consumer_info(dict_depended[curr_stage_index], at_info, spec_node_dict, dict_subtree_ats)

    def _gen_consumer_info(self, dict_depended, at_info, spec_node_dict, dict_subtree_ats):
        spec_nodes = spec_node_dict.get("spec_nodes")
        workspace_nodes = spec_node_dict.get("workspace_nodes")
        for j in dict_depended:
            consumer_info = ConsumerInfo(j)
            at_info.add_consumer(consumer_info)

            self._gen_at_candidate(consumer_info, dict_subtree_ats, j, spec_nodes, workspace_nodes)

            if len(consumer_info.at_candidates) == 1:
                consumer_info.set_sampled_target(
                    consumer_info.at_candidates[0])

            logger.debug(
                "consumer index:%s at_candidates:%s dict_subtree_ats: %s", consumer_info.index,
                consumer_info.at_candidates, dict_subtree_ats.get(j))

    def _update_at_sampled_target(self, consumer, stage_index, stage_index_map):
        """
        :param consumer:
        :param stage_index:
        :param stage_index_map:
        :return:
        """
        if consumer.sampled_target is None:
            consumer.set_updated_sampled_target(None)
            return
        new_sampled_target = stage_index_map[consumer.sampled_target]
        stage_types = self.stages_info[new_sampled_target].get('type', [])
        current_stage_types = self.stages_info[stage_index].get('type', [])
        if 'reduce_gm' in stage_types and 'reduce_atomic' not in stage_types and \
                'CacheWrite' not in stage_types and \
                'reduce_window_max' != self.stages_info[new_sampled_target].get('tag'):
            # if have done remove_pad, reduce_gm will do cache_write one more time
            if self.is_remove_pad:
                # remove pad stage need at reduce_gm stage
                if 'remove_pad' in current_stage_types:
                    consumer.set_updated_sampled_target(new_sampled_target)
                    return
                # reduce cache write stage need at reduce_gm stage
                if 'CacheWrite' in current_stage_types and 'reduce' in current_stage_types:
                    consumer.set_updated_sampled_target(new_sampled_target)
                    return
                # other stages need at reduce cache_write stage
                previous_stage_types = self.stages_info[new_sampled_target - 1].get('type', [])
                if 'remove_pad' in previous_stage_types:
                    new_sampled_target = new_sampled_target - 2
                else:
                    new_sampled_target = new_sampled_target - 1
            else:
                # if current stage is reduce cache write stage, at reduce_gm stage
                if 'CacheWrite' in current_stage_types and 'reduce' in current_stage_types:
                    consumer.set_updated_sampled_target(new_sampled_target)
                    return
                # other stages need at reduce cache_write stage
                new_sampled_target = new_sampled_target - 1
        consumer.set_updated_sampled_target(new_sampled_target)

    def _get_nonzero_axes_normal(self, stage_index):
        start_pos = 0
        if self.feature_tensor[stage_index][AXIS_CNT] > 0:
            start_pos = AXIS_CNT
        stage_fea = self.feature_tensor[stage_index]
        axis_list = stage_fea[start_pos:start_pos + AXIS_CNT]
        nonzero_axes = [dim for dim in axis_list if dim > 0]

        return nonzero_axes

    def _get_stage_list(self, input_info_list):
        """

        :param input_info_list:
        :return:
        """
        all_stage_list = []
        origin_outputs = []
        input_def_args_list = []
        input_run_args_list = []

        data_type = "float16"
        for i, input_info in enumerate(input_info_list):
            data_type = input_info.dtype
            all_stage_list.append(input_info.name)
            input_def_args_list.append("input%d_shape" % i)
            input_run_args_list.append(str(input_info.shape).replace(" ", ""))

        for output_info in self.output_info_list:
            all_stage_list.append(output_info.name)
            origin_outputs.append(output_info.name)

        return StageParams(all_stage_list, origin_outputs, input_def_args_list, input_run_args_list, data_type)


class AtInfo:
    """
    AtInfo
    """

    def __init__(self, index):
        # 初始化之后就不能变的成员
        # 初始索引
        self.ori_index = index

        # 最终的索引
        self.index = index
        self.consumers = []
        self.at_target = None

    def __repr__(self):
        repr_str = ["[%d]:\n" % self.ori_index]
        for consumer in self.consumers:
            repr_str.append("    [%d] --> %s (%s)\n" % (consumer.ori_index,
                                                        consumer.at_candidates,
                                                        consumer.sampled_target))

        return ''.join(repr_str)

    def is_fork(self):
        """

        :return:
        """
        # 遍历所有的consumer, 如果所有的consumer at target都一样
        at_targets = set()
        for consumer in self.consumers:
            if consumer.sampled_target:
                at_targets.add(consumer.sampled_target)

        # 1:叶子节点没有at target,2:只有一个at target
        if len(at_targets) <= 1:
            return False

        # 如果consumer的at target不同，则返回True
        return True

    def get_all_at_targets(self):
        """

        :return:
        """
        at_targets = []
        for consumer in self.consumers:
            at_targets.append(consumer.sampled_target)

        return list(set(at_targets))

    def consumers_in_group(self):
        """

        :return:
        """
        groups = []
        target_groups = []
        for consumer in self.consumers:
            groups_len = len(groups)
            new_group = True
            for i in range(groups_len):
                if consumer.sampled_target \
                        and consumer.sampled_target in target_groups[i]:
                    new_group = False
                    groups[i].append(consumer)
                    break

            if new_group:
                groups.append([consumer])
                target_groups.append([consumer.sampled_target])

        return groups

    def need_sample(self):
        """
        是否需要通过采样来确定at的target
        """
        for consumer in self.consumers:
            if len(consumer.at_candidates) > 1:
                return True

        return False

    def add_consumer(self, consumer):
        """

        :param consumer:
        """
        self.consumers.append(consumer)

    def get_at_target(self):
        """

        :return:
        """
        # workspace at到最后一个stage
        if self.is_fork():
            return -1

        at_target = self.consumers[0].updated_sampled_target
        if at_target:
            return at_target

        return None

    def get_sampled_target(self):
        """

        :return:
        """
        at_targets = []
        for consumer in self.consumers:
            at_targets.append(consumer.sampled_target)

        return at_targets


class ConsumerInfo:
    """
    ConsumerInfo
    """

    def __init__(self, index):
        # 初始化之后就不能变的成员
        self.ori_index = index
        self.at_candidates = []
        self.sampled_target = None

        # 最终的index
        self.index = index
        # 最终的at target
        self.updated_sampled_target = None

    def __str__(self):
        return "at_candidates:%s, sampled_target:%s, " \
               "updated_sampled_target:%s" % \
               (str(self.at_candidates), str(self.sampled_target),
                str(self.updated_sampled_target))

    def add_at_candidate(self, target_index):
        """

        :param target_index:
        """
        if target_index not in self.at_candidates:
            self.at_candidates.append(target_index)
            self.at_candidates.sort()

    def set_sampled_target(self, at_target):
        """

        :param at_target:
        """
        self.sampled_target = at_target

    def set_updated_sampled_target(self, at_target):
        """

        :param at_target:
        """
        self.updated_sampled_target = at_target

    def set_at_candidates(self, candidates):
        """

        :param candidates:
        """
        self.at_candidates = copy.deepcopy(candidates)


def get_op_schedule_info(output_tensors, option):
    """
    :param output_tensors:
    :param option:
    :return:
    """
    op_schedule_info = _gen_op_schedule_info(output_tensors, option, add_virtual_leaf_out=True)

    base_op_schedule_info = op_schedule_info

    if "virtual_leaf_out" in op_schedule_info.stages_info[-1].get('type', []):
        base_op_schedule_info = _gen_op_schedule_info(output_tensors, option,
                                                      add_virtual_leaf_out=False,
                                                      op_md5=op_schedule_info.op_md5)

    runner = OMRunner(base_op_schedule_info, None, run_base=True)
    op_schedule_info.base_tick = runner.run()[1]

    return op_schedule_info


def _gen_op_schedule_info(output_tensors, option, add_virtual_leaf_out=True, op_md5=None):
    """
    :param output_tensors:
    :param option:
    :param add_virtual_leaf_out:
    :param op_md5:
    :return:
    """
    op_name = option.get('op_config').get('op_name')
    if option.get('op_config').get('bank_key', None) is None:
        option['op_config']['bank_key'] = get_rl_bank_key(output_tensors)

    # compute_code only contain default schedule code
    sch, compute_code, stages_info = tvm_compute_to_tensor.gen_compute_code(output_tensors,
                                                                            add_virtual_leaf_out=add_virtual_leaf_out)
    if len(stages_info) > MAX_ORIGIN_STAGE_CNT:
        raise RuntimeError('stage_num[{}] > max_stages[{}].'.format(len(stages_info), MAX_ORIGIN_STAGE_CNT))

    option["error_tolerance"], option["accuracy_tolerance"] = (0, 0.001)
    # get feature tensor and reduce axis dict
    feature_tensor, reduce_axis_dict = tvm_compute_to_tensor.proc(sch, stages_info, op_name)

    input_info_list, output_info_list = get_input_output_info(sch, option, output_tensors)

    if not op_md5:
        op_md5 = _get_op_md5_by_features(feature_tensor, input_info_list, output_info_list, reduce_axis_dict, sch)

    rl_schedule_key = util.gen_rl_schedule_key(input_info_list,
                                               output_info_list, sch)
    op_yaml_config = json.dumps(option.get("op_config", []))
    # compute_code add shape dtype reduce axis info
    compute_code += "\n\n    #rl_schedule_key: " + rl_schedule_key + \
                    "\n    #reduce_axis: " + \
                    str([json.dumps(reduce_axis_dict.get(x), sort_keys=True)
                         for x in sorted(reduce_axis_dict.keys())]) + \
                    "\n    #op_config: " + op_yaml_config

    check_output = CHECK_OUTPUT_TYPE_ELEMENT_CHECK

    ret = check_data_require(option, sch)
    if ret is False:
        raise Exception('"golden_input" must be configured. %s' % op_name)

    op_schedule_info = OpScheduleInfo(op_name,
                                      feature_tensor,
                                      sch,
                                      input_info_list,
                                      output_info_list,
                                      op_md5,
                                      compute_code,
                                      check_output,
                                      option,
                                      reduce_axis_dict,
                                      stages_info,
                                      res=output_tensors)

    return op_schedule_info


def check_data_require(option, sch):
    """
    :param option:
    :param sch:
    :return:
    """
    # 检查是否有对输入有特殊依赖的IR
    special_tag_list = [
        "elewise_single_sqrt", "elewise_single_rec", "elewise_single_rsqrt",
        "elewise_binary_div", "elewise_single_log"
    ]

    tag_list = []
    for stage in sch.stages:
        tag_list.append(stage.op.tag.split("|")[0])
    if set(special_tag_list) & set(tag_list):
        # rl tune时，如果没有开启auto_schedule_golden， 就打开data_require
        if option.get("rl_tune", False):
            if not option.get("auto_schedule_golden", False):
                option["data_require"] = "positive"
        # 如果已经配置了positive则可以跳过校验
        if not (option.get("data_require", "") == "positive"
                or option.get("auto_schedule_golden", False)
                or option.get("golden_input", [])):
            logger.debug(
                'Special IR is existed, "golden_input" must be configured.')
            return False

    return True


def get_dir_by_option(option, op_schedule_info):
    """
    :param option:
    :param op_schedule_info:
    :return:
    """
    op_name = option.get('op_config').get("op_name", "default")
    workspace = option.get("WORKSPACE", WORKSPACE)
    replay_dir = os.path.join(workspace, "replay_dir")
    store_tmp_dir = os.path.join(option.get('full_tune_workspace'), "store_tmp")
    option["output_dir"] = os.path.join(store_tmp_dir, op_name)
    util.ensure_dir_exists(replay_dir, reset=False)
    util.ensure_dir_exists(store_tmp_dir, reset=False)
    store_dir = os.path.join(store_tmp_dir, op_name, op_schedule_info.shape_list_str)
    util.ensure_dir_exists(store_dir, reset=False)
    op_schedule_info.store_dir = store_dir
    op_schedule_info.replay_dir = replay_dir


def is_spec_node(sub_stage_info):
    """
    :param sub_stage_info:
    :return:
    """
    sub_stage_tag = sub_stage_info.get('tag')
    sub_stage_types = sub_stage_info.get('type', [])

    spec_node_flag = False

    # 其它的只有leaf和reduce stage
    if 'leaf' in sub_stage_types:
        spec_node_flag = True
        return spec_node_flag

    if not sub_stage_tag:
        logger.debug('sub_stage_tag is blank!')
        return spec_node_flag

    spec_patterns = ['reduce']
    for pattern in spec_patterns:
        if pattern in sub_stage_tag:
            logger.debug('spec node: %s', sub_stage_tag)
            spec_node_flag = True
            break

    return spec_node_flag


def _get_op_md5_by_features(feature_tensor, input_info_list, output_info_list, reduce_axis_dict, sch):
    """
    op_md5 is generated by feature_tensor, reduce_axis_dict, input_info_list, output_info_list and placeholder op body
    :param feature_tensor:
    :param input_info_list:
    :param output_info_list:
    :param reduce_axis_dict:
    :param sch:
    :return:
    """
    op_md5_str = feature_tensor.tobytes()
    op_md5_str += str([
        json.dumps(reduce_axis_dict[x], sort_keys=True)
        for x in sorted(reduce_axis_dict.keys())
    ]).encode()
    for stage in sch.stages:
        if str(stage.op).startswith('placeholder'):
            continue
        op_md5_str += str(stage.op.body).encode()
    op_md5_str += ";".join([str(x) for x in input_info_list]).encode()
    op_md5_str += ";".join([str(x) for x in output_info_list]).encode()
    op_md5 = hashlib.sha256(op_md5_str).hexdigest()[:util.HASH_LEN]
    return op_md5


def _get_compute_graph_info(op_pattern, schedule_obj):
    """
    :param op_pattern:
    :param schedule_obj:
    :return:
    """
    compute_graph_info = None
    if op_pattern in [ComputePattern.REDUCE, ComputePattern.TUPLE_REDUCE]:
        compute_graph_info = ReduceComputeGraphInfo(list(schedule_obj.stages))
    return compute_graph_info


def _gen_shape_tag(input_info_list, option, op_md5=None):
    """
    :param input_info_list:
    :param option:
    :param op_md5:
    :return:
    """
    shape_str_list = []
    # Only get two shape, or the name is too long
    for input_info in input_info_list[:2]:
        shape_item_list = []
        for shape_vaule in input_info.shape:
            shape_item_list.append(str(shape_vaule))
        shape_str_list.append("_".join(shape_item_list))
        shape_str_list.append(input_info.dtype)
    shape_str = "@".join(shape_str_list)
    # add ori shape when shape has changed in compute
    if option.get("shape", None) and option.get("dtype", None):
        shape_list = option["shape"]
        if not isinstance(shape_list[0], list):
            shape_list = [shape_list]
        dtype = option["dtype"]
        shape_dtype_list = []
        for shape in shape_list[:2]:
            shape_dtype_list.extend(["_".join([str(x) for x in shape]), dtype])
        option_shape_str = "@".join(shape_dtype_list)
        if not shape_str.startswith(option_shape_str):
            shape_str = option_shape_str + "-" + shape_str
    if op_md5:
        shape_str += "@" + op_md5
    shape_str += "@" + soc_cfg.get_soc_version()
    return shape_str


def _get_input_info(option, input_info_list, output_tensors=None):
    """
    :param option:
    :param input_info_list:
    :param output_tensors:
    :return:
    """
    tensor_list = option.get('op_config').get("tensor_list", [])
    if not tensor_list:
        return _update_input_seq(option, input_info_list)

    input_update_list = []
    for tensor_dict in tensor_list:
        if tensor_dict.get("shape", "NULL") == "NULL":
            continue
        if option.get('op_config').get('op_name') not in NEED_UPDATE_INPUT_INFO_OP_NAME:
            if tensor_dict.get("name", None) in [input_info.name for input_info in input_info_list]:
                input_update_list.append(TensorInfo(**tensor_dict))
            if len(input_update_list) == len(input_info_list):
                break
        else:
            output_tensor_names = [output_tensor.op.name for output_tensor in output_tensors]
            if tensor_dict.get("name", None) not in output_tensor_names:
                input_update_list.append(TensorInfo(**tensor_dict))

    return input_update_list


def _update_input_seq(option, input_info_list):
    """
    :param option:
    :param input_info_list:
    :return:
    """
    op_args = option.get('op_config').get("inputs", [])
    if not op_args:
        return input_info_list

    input_update_list = []
    input_temp_list = copy.deepcopy(input_info_list)
    input_cnt = len(input_info_list)
    dtype_align = {"int8": ["bool"], "bool": ["int8"]}
    curr_cnt = 0
    for arg_dict in op_args:
        if not isinstance(arg_dict, dict):
            return input_info_list
        if arg_dict.get("shape", "NULL") == "NULL":
            continue
        input_shape = arg_dict["shape"]
        input_dtype = arg_dict["dtype"]
        dtype_align_list = [input_dtype] + dtype_align.get(input_dtype, [])
        for input_info in input_temp_list:
            input_size = _get_tensor_size(input_shape, input_dtype)
            tensor_size = _get_tensor_size(input_info.shape, input_dtype)
            if input_info.dtype in dtype_align_list and \
                    tensor_size == input_size:
                input_temp_list.remove(input_info)
                input_update_list.append(input_info)
                break
        curr_cnt += 1
        if curr_cnt == input_cnt:
            break
    if not input_temp_list:
        return input_update_list

    logger.warn("input tensor can not be parsed, %s.", input_temp_list[0])
    return input_info_list


def get_input_output_info(schedule_obj, option, output_tensors):
    """
    :param option:
    :param schedule_obj:
    :param output_tensors:
    :return:
    """
    input_info_list = []
    output_info_list = []
    input_size = 0
    for stage in schedule_obj.stages:
        if str(stage.op).startswith("placeholder"):
            tensor_info = TensorInfo(stage.op.name,
                                     util.tvm_shape_trans(stage.op.shape),
                                     stage.op.dtype)
            input_info_list.append(tensor_info)
            input_size += _get_tensor_size(tensor_info.shape, tensor_info.dtype)

    input_info_list = _get_input_info(option, input_info_list, output_tensors)

    output_size = 0
    if not isinstance(output_tensors, list):
        output_tensors = [output_tensors]
    for output_tensor in output_tensors:
        if output_tensor.op.tag == "conv_virtual_res":
            continue
        tensor_name = output_tensor.name
        if output_tensor.name.startswith("%s.v" % output_tensor.op.name):
            tensor_idx = output_tensor.name.split('.v')[-1]
            tensor_name = '%s_v%s' % (output_tensor.op.name, tensor_idx)
        tensor_info = TensorInfo(tensor_name,
                                 util.tvm_shape_trans(output_tensor.shape),
                                 output_tensor.dtype)
        output_info_list.append(tensor_info)
        output_size += _get_tensor_size(tensor_info.shape, tensor_info.dtype)

    return input_info_list, output_info_list


def _get_tensor_size(tensor_shape, data_type):
    """
    :param tensor_shape:
    :param data_type:
    :return:
    """
    element_cnt = functools_reduce(lambda x, y: x * y, tensor_shape)
    data_size = util.get_dtype_size(data_type) * element_cnt
    return data_size


