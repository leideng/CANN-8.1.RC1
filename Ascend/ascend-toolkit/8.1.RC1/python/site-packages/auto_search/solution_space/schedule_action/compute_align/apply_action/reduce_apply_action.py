#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
reduce apply action
"""
from tbe.tvm import Stage
from tbe.dsl.unify_schedule import util
from tbe.common.platform import ASCEND_910B
from tbe.common.platform.platform_info import get_soc_spec
from tbe.tvm.tir.expr import Reduce
from tbe.tvm import PlaceholderOp
from tbe.tvm import Tensor

from auto_search.utils import logger
from auto_search.bank.cheque_generator import get_compute_align_cheque
from auto_search.utils import util as comm_util
from auto_search.solution_space.action import apply_action_register
from auto_search.solution_space.action import ScheduleActionType
from auto_search.compute_analysis import ComputePattern
from auto_search.solution_space.op_schedule_info import OpScheduleInfo


@apply_action_register([ComputePattern.REDUCE, ComputePattern.TUPLE_REDUCE], ScheduleActionType.COMPUTE_ALIGN)
def apply_for_reduce(progress):
    """
    apply for compute align
    :param progress:
    :return:
    """
    # compute_align only support ASCEND_910B now
    version = get_soc_spec("SHORT_SOC_VERSION")
    if version not in [ASCEND_910B]:
        return
    op_schedule_info = progress.op_schedule_info
    if op_schedule_info.is_ub_transpose:
        return
    # if no stage do storage_align, need not do compute align
    if not op_schedule_info.storage_align_para:
        return

    atomic_flag = op_schedule_info.is_atomic
    if atomic_flag:
        _do_compute_align_for_atomic(op_schedule_info)
    else:
        _do_compute_align_for_normal(op_schedule_info)

    logger.debug("apply compute align done.")


def _do_compute_align_for_atomic(op_schedule_info):
    """
    :param op_schedule_info:
    :return:
    """
    code_lines = []
    code_lines.extend(['\n', '# compute_align code begin'])
    for align_stage_index in op_schedule_info.storage_align_para:
        stage_type = op_schedule_info.stages_info[align_stage_index].get('type', [])
        stage = op_schedule_info.schedule_obj.stages[align_stage_index]
        # get_dsl_insn get tag of tensor, '' emit insn 'dma_copy'
        if "reduce_atomic_rfactor" in stage_type or util.get_dsl_insn(stage.op.output(0)) not in [""]:
            _do_compute_align_for_a_stage(op_schedule_info, align_stage_index,
                                          op_schedule_info.storage_align_para.get(align_stage_index), code_lines)

    reduce_compute_info = op_schedule_info.compute_graph_info
    # in last reduce scene, 'reduce_atomic_rfactor' is not do storage_align, need do it alone
    if reduce_compute_info.is_last_reduce:
        # compute_align is not appoint pad value, the pad value get from origin value,
        # do not affect reduce_min and reduce_max calculate
        if util.get_dsl_insn(reduce_compute_info.reduce_tensors[0]) in ["reduce_max", "reduce_min"]:
            atomic_stage_index, align_factor = _get_reduce_atomic_rfactor_index_and_factor(op_schedule_info)
            _do_compute_align_for_a_stage(op_schedule_info, atomic_stage_index, align_factor, code_lines)

    op_schedule_info.code_lines.extend(code_lines)


def _do_compute_align_for_normal(op_schedule_info):
    """
    :param op_schedule_info:
    :return:
    """
    code_lines = []
    code_lines.extend(['\n', '# compute_align code begin'])
    for align_stage_index in op_schedule_info.storage_align_para:
        stage = op_schedule_info.schedule_obj.stages[align_stage_index]
        # get_dsl_insn get tag of tensor, '' emit insn 'dma_copy'
        if util.get_dsl_insn(stage.op.output(0)) not in [""]:
            _do_compute_align_for_a_stage(op_schedule_info, align_stage_index,
                                          op_schedule_info.storage_align_para.get(align_stage_index), code_lines)

    reduce_compute_info = op_schedule_info.compute_graph_info
    # in last reduce scene, 'reduce_cachewrite' stage is not do storage_align, need do it alone
    if reduce_compute_info.is_last_reduce:
        # compute_align is not appoint pad value, the pad value get from origin value,
        # do not affect reduce_min and reduce_max calculate
        if util.get_dsl_insn(reduce_compute_info.reduce_tensors[0]) in ["reduce_max", "reduce_min"]:
            reduce_stage_index, align_factor = _get_reduce_stage_index_and_factor(op_schedule_info)
            _do_compute_align_for_a_stage(op_schedule_info, reduce_stage_index, align_factor, code_lines)

    op_schedule_info.code_lines.extend(code_lines)


def _get_reduce_stage_index_and_factor(op_schedule_info):
    """
    :param op_schedule_info:
    :return:
    """
    reduce_stage_index = None
    for stage_index, _ in enumerate(op_schedule_info.schedule_obj.stages):
        stage_type = op_schedule_info.stages_info[stage_index].get('type', [])
        if "CacheWrite" in stage_type and "reduce" in stage_type:
            reduce_stage_index = stage_index
            break
    if reduce_stage_index is None:
        raise RuntimeError("can not find reduce_atomic_rfactor stage!")
    align_factor = \
        comm_util.get_block_num(op_schedule_info.schedule_obj.stages[reduce_stage_index].op.output(0).dtype)
    return reduce_stage_index, align_factor


def _get_reduce_atomic_rfactor_index_and_factor(op_schedule_info):
    """
    :param op_schedule_info:
    :return:
    """
    atomic_stage_index = None
    for stage_index, _ in enumerate(op_schedule_info.schedule_obj.stages):
        stage_type = op_schedule_info.stages_info[stage_index].get('type', [])
        if "reduce_atomic_rfactor" in stage_type:
            atomic_stage_index = stage_index
            break
    if atomic_stage_index is None:
        raise RuntimeError("can not find reduce_atomic_rfactor stage!")
    align_factor = \
        comm_util.get_block_num(op_schedule_info.schedule_obj.stages[atomic_stage_index].op.output(0).dtype)
    return atomic_stage_index, align_factor


def _do_compute_align_for_a_stage(op_schedule_info, align_stage_index, align_factor, code_lines):
    """
    :param op_schedule_info:
    :param align_stage_index:
    :param align_factor:
    :param code_lines:
    :return:
    """
    stage = op_schedule_info.schedule_obj.stages[align_stage_index]
    stage_name = op_schedule_info.stages_info[align_stage_index]["name"]
    align_axis_name, align_axis_index, align_axis = _get_align_axis(op_schedule_info, stage, align_stage_index)
    align_code = 'sch[{}].compute_align({}, {})'.format(stage_name, align_axis_name, align_factor)
    code_lines.append(align_code)

    stage.compute_align(align_axis, align_factor)
    if align_axis_name.find('reduce_axis') != -1:
        # the last num mean axis type, 1 is reduce_axis, 0 is axis
        cheque = get_compute_align_cheque(align_stage_index, align_axis_index, align_factor, 1)
    else:
        cheque = get_compute_align_cheque(align_stage_index, align_axis_index, align_factor, 0)
    op_schedule_info.cheque_list.append(cheque)


def _get_align_axis(op_schedule_info: OpScheduleInfo, stage: Stage, align_stage_index: int) -> tuple:
    """
    :param op_schedule_info:
    :param stage:
    :param align_stage_index:
    :return:
    """
    reduce_compute_info = op_schedule_info.compute_graph_info
    stage_name = op_schedule_info.stages_info[align_stage_index]["name"]
    if not (_is_reduce_tensor(stage.op.output(0)) and reduce_compute_info.is_last_reduce):
        for align_axis_index in range(len(stage.leaf_iter_vars)-1, -1, -1):
            align_axis = stage.leaf_iter_vars[align_axis_index]
            align_axis_index, align_axis_name = \
                _find_leaf_iter_vars_last_axis(align_axis, stage, stage_name)
            if align_axis_name.find('reduce_axis') == -1:
                return align_axis_name, align_axis_index, align_axis
    if op_schedule_info.is_atomic:
        if len(stage.op.reduce_axis) == 1:
            align_axis = stage.op.reduce_axis[-1]
            align_axis_index = len(stage.op.reduce_axis) - 1
            align_axis_name = f'sch[{stage_name}].op.reduce_axis[{align_axis_index}]'
            return align_axis_name, align_axis_index, align_axis
        align_axis = stage.op.reduce_axis[-2]
        align_axis_index = len(stage.op.reduce_axis) - 2
        align_axis_name = f'sch[{stage_name}].op.reduce_axis[{align_axis_index}]'
    else:
        align_axis = stage.op.reduce_axis[-1]
        align_axis_index = len(stage.op.reduce_axis) - 1
        align_axis_name = f'sch[{stage_name}].op.reduce_axis[{align_axis_index}]'

    return align_axis_name, align_axis_index, align_axis


def _find_leaf_iter_vars_last_axis(align_axis, stage, stage_name):
    """
    :param align_axis:
    :param stage:
    :param stage_name:
    :return:
    """
    # if reduce stage is not be cut, align axis is op.axis[0]
    for axis_index, op_axis in enumerate(stage.op.reduce_axis):
        if op_axis == align_axis:
            align_axis_index = axis_index
            align_axis_name = f'sch[{stage_name}].op.reduce_axis[{align_axis_index}]'
            return align_axis_index, align_axis_name
    for axis_index, op_axis in enumerate(stage.op.axis):
        if op_axis == align_axis:
            align_axis_index = axis_index
            align_axis_name = f'sch[{stage_name}].op.axis[{align_axis_index}]'
            return align_axis_index, align_axis_name

    raise RuntimeError("current stage cant find align axis!")


def _is_reduce_tensor(tensor: Tensor) -> bool:
    """
    Check if tensor contains reduce body
    """
    if isinstance(tensor.op, PlaceholderOp):
        return False
    if isinstance(tensor.op.body[0], Reduce):
        return True
    return False
