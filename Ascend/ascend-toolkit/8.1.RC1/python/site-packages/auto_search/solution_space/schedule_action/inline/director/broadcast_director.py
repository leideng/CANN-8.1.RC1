#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
common director
"""
from typing import NoReturn

from tbe.common.platform import ASCEND_910B
from tbe.common.platform import SHORT_SOC_VERSION
from tbe.common.platform.platform_info import get_soc_spec

from auto_search.utils import logger
from auto_search.utils import util
from auto_search.solution_space.action import director_register
from auto_search.solution_space.action import ScheduleActionType
from auto_search.compute_analysis import ComputePattern
from auto_search.solution_space.schedule_action.split.split_action import SchSplitAction
from auto_search.compute_analysis.broadcast_compute_graph import BroadcastComputeGraphInfo
from auto_search.solution_space.schedule_action.cache_read.cache_read_action import CacheReadAction
from auto_search.solution_space.progress import Progress


@director_register([ComputePattern.BROADCAST], ScheduleActionType.INLINE)
def broadcast_direct(progress: Progress) -> NoReturn:
    """
    broadcast direct
    :param progress:
    :return:
    """
    pattern = progress.op_schedule_info.op_pattern

    # only ASCEND_910B can support mov_align and remove_pad
    if get_soc_spec(SHORT_SOC_VERSION) not in [ASCEND_910B]:
        logger.debug("SHORT_SOC_VERSION is: %s, not support mov_align!", get_soc_spec(SHORT_SOC_VERSION))
        next_action = SchSplitAction(pattern)
        progress.todo = next_action
        return

    remove_pad_for_broadcast(progress)


def remove_pad_for_broadcast(progress):
    """
    :param progress:
    :return:
    """
    stages_info = progress.op_schedule_info.stages_info
    pattern = progress.op_schedule_info.op_pattern

    # only ASCEND_910B can support mov_align and remove_pad
    if get_soc_spec(SHORT_SOC_VERSION) not in [ASCEND_910B]:
        logger.debug("SHORT_SOC_VERSION is: %s, not support mov_align!", get_soc_spec(SHORT_SOC_VERSION))
        progress.todo = SchSplitAction(pattern)
        return

    output_tensors = progress.op_schedule_info.output_tensors
    broadcast_compute_info = BroadcastComputeGraphInfo(output_tensors, 0)
    broadcast_compute_info.construct_compute_graph()
    remove_tensors = broadcast_compute_info.broadcast_tensors - broadcast_compute_info.absorbable_broadcast_tensors
    remove_tensor_name_list = [tensor.op.name for tensor in remove_tensors]

    for stage_index, stage_info in enumerate(stages_info):
        if _last_axis_is_align(progress, stage_index):
            # id broadcast tensor last axis is align, need not do storage_align, so need not do remove pad
            continue
        ori_stage_name = stage_info.get('ori_name', '')
        if ori_stage_name in remove_tensor_name_list \
                and ori_stage_name not in progress.op_schedule_info.inlined_stages_ori_name \
                and 'CacheWrite' in stage_info.get('type', []) \
                and not {'src_cache_read', 'src_remove_pad'} & set(stage_info.get('type', [])):
            next_action = CacheReadAction(pattern, stage_index)
            progress.todo = next_action
            logger.debug("%s do cache_read for remove pad.", ori_stage_name)
            return

    progress.todo = SchSplitAction(pattern)


def _last_axis_is_align(progress, stage_index):
    """
    :param progress:
    :param stage_index:
    :param stage_info:
    :return:
    """
    stage = progress.op_schedule_info.schedule_obj.stages[stage_index]
    curr_tensor = stage.origin_op.output(0)
    stage_shape = [dim.value for dim in curr_tensor.shape]
    block_size = util.get_block_num(curr_tensor.dtype)
    last_axis_shape = stage_shape[-1]
    if last_axis_shape % block_size == 0:
        return True
    return False
