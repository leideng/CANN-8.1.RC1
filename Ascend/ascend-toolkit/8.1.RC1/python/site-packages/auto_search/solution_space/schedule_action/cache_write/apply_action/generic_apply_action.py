#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
common apply action
"""
import copy
from tbe.common.platform import platform_info

from auto_search.utils import logger
from auto_search.solution_space.action import apply_action_register
from auto_search.solution_space.action import ScheduleActionType
from auto_search.compute_analysis import ComputePattern
from auto_search.solution_space.op_schedule_info import AtInfo
from auto_search.solution_space.op_schedule_info import ConsumerInfo
from auto_search.bank.cheque_generator import get_cache_write_cheque
from auto_search.bank.cheque_generator import get_cache_write_cheque_spec


def get_cache_rw_name(ori_tensor_name, scope, index=None, ub2l1=False):
    """

    :param ori_tensor_name:
    :param scope:
    :param index:
    :return:
    """
    head = ''
    tail = ''
    if ub2l1:
        head = 'sub'
    elif scope in [platform_info.scope_cbuf, platform_info.scope_ubuf]:
        tail = '_l'
    elif scope in [platform_info.scope_cc, platform_info.scope_ca,
                   platform_info.scope_cb]:
        head = 'sub'

    if index is not None:
        index_str = "_%03d" % index
    else:
        index_str = ""

    return head + ori_tensor_name + tail + index_str


def gen_cache_write_info(cur_tensor, scope, stage_info):
    """

    :param cur_tensor:
    :param scope:
    :param stage_info:
    :return:
    """
    tensor_name = cur_tensor.op.name
    base_cache_rw_info = {
        "type": "CacheWrite",
        "scope": scope,
        "name": stage_info["name"],
    }

    # 初始compute中的tensor name
    ori_tensor_name = stage_info.get("ori_name", tensor_name)
    cache_rw_info = base_cache_rw_info
    cache_rw_info["tensor"] = cur_tensor
    cache_rw_info['rw_name'] = get_cache_rw_name(ori_tensor_name, scope)
    if "src_cache_write" in stage_info.get('type', []):
        cache_rw_info['remove_pad'] = True
        cache_rw_info['rw_name'] = f"{cache_rw_info.get('rw_name')}_pad"

    return cache_rw_info


def update_stage_info(progress, cache_rw_info, rw_tensor):
    """
    update stage info
    :param progress:
    :param cache_rw_info:
    :param rw_tensor:
    :return:
    """
    sch = progress.op_schedule_info.schedule_obj
    stages_info = progress.op_schedule_info.stages_info
    tensor_ori_index = cache_rw_info["tensor_ori_index"]
    buf_str = str(cache_rw_info['scope'])

    src_stage_info = stages_info[tensor_ori_index]
    src_at_info = src_stage_info['at_info']
    if "src_cache_write" not in src_stage_info['type']:
        src_stage_info['type'].append('src_cache_write')

    rw_at_info = AtInfo(src_at_info.index)
    rw_stage_types = ["CacheWrite"]
    # 1 cache write
    if src_at_info.is_fork():
        # src is workspace
        consumer = ConsumerInfo(src_at_info.index)
        consumer.set_sampled_target(src_at_info.index)
        rw_at_info.add_consumer(consumer)
    else:
        rw_at_info = copy.deepcopy(src_at_info)

    reduce_type = ''
    if 'reduce_gm' in src_stage_info.get('type', []):
        rw_stage_types.append('reduce')
        reduce_type = src_stage_info.get('reduce_type', '')

    if cache_rw_info.get('remove_pad', False):
        rw_stage_types.append('remove_pad')
        src_stage_info['type'].append('src_remove_pad')

    rw_stage_index = list(sch.stages).index(sch[rw_tensor])
    rw_stage_info = {
        'name': cache_rw_info['rw_name'],
        'type': rw_stage_types,
        'scope': buf_str,
        'at_info': rw_at_info,
        'ori_name': src_stage_info.get('ori_name'),
        'tag': src_stage_info.get('tag'),
        'rw_tensor': rw_tensor,
        'reduce_type': reduce_type
    }

    stages_info.insert(rw_stage_index, rw_stage_info)


def update_stage_info_no_at_info(progress, cache_rw_info, rw_tensor):
    """
    update stage info
    :param progress:
    :param cache_rw_info:
    :param rw_tensor:
    :return:
    """
    sch = progress.op_schedule_info.schedule_obj
    stages_info = progress.op_schedule_info.stages_info
    tensor_ori_index = cache_rw_info["tensor_ori_index"]
    buf_str = str(cache_rw_info['scope'])

    src_stage_info = stages_info[tensor_ori_index]
    if "src_cache_write" not in src_stage_info['type']:
        src_stage_info['type'].append('src_cache_write')

    rw_stage_types = ["CacheWrite"]

    reduce_type = ''
    if 'reduce_gm' in src_stage_info.get('type', []):
        rw_stage_types.append('reduce')
        reduce_type = src_stage_info.get('reduce_type', '')

    rw_stage_index = list(sch.stages).index(sch[rw_tensor])
    rw_stage_info = {
        'name': cache_rw_info['rw_name'],
        'type': rw_stage_types,
        'scope': buf_str,
        'ori_name': src_stage_info.get('ori_name'),
        'tag': src_stage_info.get('tag'),
        'rw_tensor': rw_tensor,
        'reduce_type': reduce_type
    }

    stages_info.insert(rw_stage_index, rw_stage_info)


def _real_do_cache_write(progress, cache_rw_info, tensor):
    """
    real do cache wirte
    :param progress:
    :param tensor:
    :return:
    """
    tensor_name = cache_rw_info['name']
    rw_tensor_name = cache_rw_info['rw_name']
    sch = progress.op_schedule_info.schedule_obj
    buf = cache_rw_info['scope']

    code_lines = []
    if tensor.op.num_outputs > 1:
        tensor_names = []
        written_tensor_names = []
        for idx in range(tensor.op.num_outputs):
            tensor_names.append(tensor_name + "_v%s" % idx)
            written_tensor_names.append(tensor_name + "_v%s_l" % idx)
        code_line = "%s = sch.cache_write([%s], '%s')" % (', '.join(
            written_tensor_names), ', '.join(tensor_names), str(buf))
        code_lines.append(code_line)
        code_lines.append('%s = %s' %
                          (rw_tensor_name, written_tensor_names[0]))
        rw_tensors = sch.cache_write(cache_rw_info['tensors'], buf)
        rw_tensor = rw_tensors[0]
        cheque = get_cache_write_cheque_spec(cache_rw_info["tensor_ori_index"], buf, tensor.op.num_outputs)
        progress.op_schedule_info.cheque_list.append(cheque)
    else:
        code_line = "%s = sch.cache_write(%s, '%s')" % (
            rw_tensor_name, tensor_name, str(buf))
        code_lines.append(code_line)
        rw_tensor = sch.cache_write(tensor, buf)
        cheque = get_cache_write_cheque(cache_rw_info["tensor_ori_index"], buf)
        progress.op_schedule_info.cheque_list.append(cheque)

    return code_lines, rw_tensor


def do_cache_write(progress, cache_rw_info):
    """
    do cache write
    :param progress:
    :param cache_rw_info:
    :return:
    """
    if cache_rw_info["type"] != "CacheWrite":
        logger.warn("Unknown action:%s", cache_rw_info["type"])
        return [""]
    tensor = cache_rw_info['tensor']
    tensor_name = cache_rw_info['name']
    stage_index_map = progress.op_schedule_info.stage_index_map


    stage_names = (stage_info["name"] for stage_info in progress.op_schedule_info.stages_info)
    tensor_ori_index = list(stage_names).index(tensor_name)
    cache_rw_info["tensor_ori_index"] = tensor_ori_index

    code_lines, rw_tensor = _real_do_cache_write(progress, cache_rw_info, tensor)

    for ori_index, cur_index in stage_index_map.items():
        if cur_index >= tensor_ori_index:
            stage_index_map[ori_index] = cur_index + 1

    progress.op_schedule_info.code_lines.extend(code_lines)
    if 'at_info' not in progress.op_schedule_info.stages_info[tensor_ori_index]:
        update_stage_info_no_at_info(progress, cache_rw_info, rw_tensor)
    else:
        update_stage_info(progress, cache_rw_info, rw_tensor)

    return code_lines


@apply_action_register([ComputePattern.ELEMENTWISE, ComputePattern.BROADCAST, ComputePattern.POOLING,
                        ComputePattern.REDUCE, ComputePattern.NORM, ComputePattern.TRANSPOSE,
                        ComputePattern.TUPLE_REDUCE],
                       ScheduleActionType.CACHE_WRITE)
def apply(progress):
    """
    :param progress:
    :return:
    """
    stage_index = progress.todo.stage_index
    stage = progress.op_schedule_info.schedule_obj.stages[stage_index]
    stage_info = progress.op_schedule_info.stages_info[stage_index]

    if 'reduce_atomic' in stage_info.get('type', []):
        return

    curr_tensor = stage.origin_op.output(0)
    tensors = []
    for idx in range(stage.op.num_outputs):
        tensors.append(stage.origin_op.output(idx))
    scope = 'local.UB'

    cache_write_info = gen_cache_write_info(curr_tensor, scope, stage_info)
    cache_write_info["tensors"] = tensors
    do_cache_write(progress, cache_write_info)

    logger.debug("apply cache write done.")
