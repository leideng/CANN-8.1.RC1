#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
set store predicate for broadcast
"""
from dataclasses import make_dataclass
from collections import namedtuple
from typing import List

from tbe import tvm

from auto_search.utils import logger
from auto_search.solution_space.progress import Progress
from auto_search.solution_space.op_schedule_info import OpScheduleInfo
from auto_search.bank.cheque_generator import get_set_store_predicate_cheque
from auto_search.bank.cheque_generator import get_mem_uniq_cheque
from auto_search.compute_analysis.broadcast_compute_graph import get_set_store_predicate_tensors
from auto_search.compute_analysis.broadcast_analysis import can_use_optimize_schedule_for_bn_operators

RW_TYPE = ['CacheRead', 'CacheWrite']
TensorNames = make_dataclass(
    'TensorNames',
    ['broadcast_tensors_name', 'all_need_tensors_name', 'common_tensors_name'])

SetStorePredicteChequeParams = namedtuple(
    'SetStorePredicteChequeParams',
    ['target_stage_index', 'axis_index', 'cond_limit', 'ub_split_axis', 'remove_pad', 'broadcast_predicate_list'])

CalcPredicateConditionParams = namedtuple(
    'CalcPredicateConditionParams',
    ['op_schedule_info', 'ub_split_src', 'ub_split_src_str', 'ub_split_axis', 'stage_ind', 'cond_limit'])


def proc(progress: Progress) -> bool:
    """
    set_store_predicate for broadcast
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    # if do compute root not need do set_store_predicate
    if can_use_optimize_schedule_for_bn_operators(progress):
        return False
    # if after split ub, outer axis participate bind multi core directly,
    # can't enable set_store_predicate
    if not op_schedule_info.is_need_block_tiling:
        return False

    code_lines = []
    broadcast_store_predicate, all_pre_node_broadcast, store_predicate_common_tensors = \
        get_set_store_predicate_tensors(progress)
    if len(broadcast_store_predicate) == 0:
        return False

    all_need_set_predict_tensors = broadcast_store_predicate | all_pre_node_broadcast
    broadcast_store_predicate_tensor_name_list = {tensor.op.name: tensor for tensor in broadcast_store_predicate}
    all_need_set_predict_tensors_name_list = {tensor.op.name: tensor for tensor in all_need_set_predict_tensors}
    store_predicate_common_tensor_name_list = {tensor.op.name: tensor for tensor in store_predicate_common_tensors}
    tensor_name_list: TensorNames = TensorNames(broadcast_store_predicate_tensor_name_list,
                                                all_need_set_predict_tensors_name_list,
                                                store_predicate_common_tensor_name_list)
    # do set_store_predicate
    for stage_ind in range(len(op_schedule_info.schedule_obj.stages)):
        _do_set_store_predicate_for_a_stage(code_lines, op_schedule_info, stage_ind, tensor_name_list)

    op_schedule_info.code_lines.extend(code_lines)
    logger.debug('code_lines: %s', ("\n".join(op_schedule_info.code_lines)))

    return True


def calc_predicate_condition(predicate_params):
    op_schedule_info = predicate_params.op_schedule_info
    ub_split_src = predicate_params.ub_split_src
    ub_split_src_str = predicate_params.ub_split_src_str
    ub_split_axis = predicate_params.ub_split_axis
    stage_ind = predicate_params.stage_ind
    cond_limit = predicate_params.cond_limit

    ub_factor = op_schedule_info.ub_factor

    ub_out_shape = tvm.floordiv(op_schedule_info.output_info_list[0].shape[ub_split_axis] - 1, ub_factor) + 1

    #get bind axis info
    _, bind_stage_index = op_schedule_info.bind_stages[0]
    bind_axis_index = 0
    block_factor = op_schedule_info.stages_info[bind_stage_index]["block_factor"]
    bind_axis = op_schedule_info.schedule_obj.stages[bind_stage_index].leaf_iter_vars[bind_axis_index]
    bind_axis_name = op_schedule_info.axis_info_list[bind_stage_index][bind_axis_index].name

    #get compute axis info
    compute_at_index = op_schedule_info.at_dict[stage_ind]
    compute_at_axis_name = op_schedule_info.at_targets[stage_ind].body.var.name
    compute_at_axis_list = [i.var.name for i in op_schedule_info.schedule_obj.stages[bind_stage_index].leaf_iter_vars]
    compute_at_axis_index = compute_at_axis_list.index(compute_at_axis_name)
    compute_at_axis = op_schedule_info.at_targets[stage_ind]

    broadcast_predicate_list = [bind_stage_index, bind_axis_index, compute_at_index,
                                compute_at_axis_index, block_factor, ub_out_shape, ub_split_axis, cond_limit]

    cond_str = "tvm.any((%s * %s + %s) %% %s < %s, %s < %s, %s != 1)" % \
               (bind_axis_name, block_factor, compute_at_axis.name,
                ub_out_shape, cond_limit, compute_at_axis.name, cond_limit, ub_split_src_str)
    return tvm.any(
        (bind_axis * block_factor + compute_at_axis.body) % ub_out_shape < cond_limit,
        compute_at_axis.body < cond_limit,
        ub_split_src != 1), cond_str, broadcast_predicate_list


def _do_set_store_predicate_for_a_stage(code_lines: List, op_schedule_info: OpScheduleInfo, stage_ind: int,
                                        tensor_name_list: TensorNames) -> NameError:
    """
    :param code_lines:
    :param op_schedule_info:
    :param stage_ind:
    :param tensor_name_list:
    :return:
    """
    # if is_open_db is open, cond_limit = 2, else cond_limit = 1
    cond_limit = 1
    if op_schedule_info.is_open_db:
        cond_limit = 2
    current_stage = op_schedule_info.schedule_obj.stages[stage_ind]
    ub_split_axis = op_schedule_info.cut_axis_index[len(op_schedule_info.schedule_obj.stages) - 1]
    stage_name = op_schedule_info.stages_info[stage_ind].get('name', '')
    stage_ori_name = op_schedule_info.stages_info[stage_ind].get('ori_name', '')
    target_axis = op_schedule_info.at_targets[stage_ind]
    target_stage_index = op_schedule_info.at_dict.get(stage_ind, None)
    if stage_ori_name in tensor_name_list.all_need_tensors_name.keys() \
            and (set(op_schedule_info.stages_info[stage_ind].get('type', [])) & set(RW_TYPE)):
        if target_axis is not None and target_stage_index is not None:
            remove_pad = 0
            broadcast_predicate_list = []
            if 'broadcast_remove_pad' in op_schedule_info.stages_info[stage_ind].get('type', []):
                cond_str = "tvm.any(%s < %s, %s.op.input_tensors[0].op.input_tensors[0].shape[%s] != 1)" % \
                           (target_axis.name, cond_limit, stage_name, ub_split_axis)
                cond = tvm.any(target_axis.body < cond_limit,
                               current_stage.op.output(0).op.input_tensors[0].shape[ub_split_axis] != 1)
                current_stage.set_store_predicate(cond)
                remove_pad = 1
            elif op_schedule_info.stages_info[stage_ind].get('tag') in ['unknown_broadcast', 'unified_broadcast']:
                tensor_i = tensor_name_list.all_need_tensors_name.get(stage_ori_name)
                ub_split_src = tensor_i.op.input_tensors[0].shape[ub_split_axis]
                ub_split_src_str = "%s.op.input_tensors[0].shape[%s]" % (tensor_i.name, ub_split_axis)
                predicate_params = CalcPredicateConditionParams(op_schedule_info, ub_split_src, ub_split_src_str
                                                                , ub_split_axis, stage_ind, cond_limit)
                cond, cond_str, broadcast_predicate_list = calc_predicate_condition(predicate_params)
                current_stage.set_store_predicate(cond)
            else:
                tensor_i = tensor_name_list.all_need_tensors_name.get(stage_ori_name)
                ub_split_src = tensor_i.shape[ub_split_axis]
                ub_split_src_str = "%s.shape[%s]" % (tensor_i.name, ub_split_axis)
                predicate_params = CalcPredicateConditionParams(op_schedule_info, ub_split_src, ub_split_src_str
                                                                , ub_split_axis, stage_ind, cond_limit)
                cond, cond_str, broadcast_predicate_list = calc_predicate_condition(predicate_params)
                current_stage.set_store_predicate(cond)

            code_lines.append('sch[%s].set_store_predicate(%s)' % (stage_name, cond_str))

            # get axis_index
            axis_index = _get_axis_index(op_schedule_info, target_axis, target_stage_index)
            cheque_param = SetStorePredicteChequeParams(target_stage_index, axis_index, cond_limit, ub_split_axis,
                                                        remove_pad, broadcast_predicate_list)
            cheque = get_set_store_predicate_cheque(stage_ind, cheque_param)
            op_schedule_info.cheque_list.append(cheque)

            # need do mem_unique's tensor:
            # 1.broadcast tensor
            # 2.multi outputs tensor, and have tensor not in all_need_set_predict_tensors
            is_need_mem_unique = (stage_ori_name in tensor_name_list.broadcast_tensors_name or
                                  stage_ori_name in tensor_name_list.common_tensors_name) \
                                 and 'local.UB' in current_stage.scope \
                                 and 'src_remove_pad' not in op_schedule_info.stages_info[stage_ind].get('type', [])
            if is_need_mem_unique:
                code_lines.append('sch[%s].mem_unique()' % stage_name)
                current_stage.mem_unique()
                cheque = get_mem_uniq_cheque(stage_ind)
                op_schedule_info.cheque_list.append(cheque)
                op_schedule_info.is_mem_unique = True


def _get_axis_index(op_schedule_info: OpScheduleInfo, target_axis: int, target_stage_index: int) -> int:
    """
    get at target stage axis index in axis_info_list which will be set store predicate
    :param op_schedule_info:
    :param target_axis:
    :param target_stage_index:
    :return:
    """
    axis_index = None
    for i, axis_info in enumerate(op_schedule_info.axis_info_list[target_stage_index]):
        if axis_info.name == target_axis.name:
            axis_index = i
            break
    return axis_index
