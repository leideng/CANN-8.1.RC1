#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
norm rfactor
"""
from auto_search.solution_space.tensor_cfg import FeatureTensorCfg
from auto_search.bank.cheque_generator import get_emit_insn_cheque
from auto_search.utils.util import DTYPE_BYTE_MAPPING
from auto_search.utils.util import BLOCK_SIZE
from auto_search.utils.util import DTYPE_REPEAT_MAPPING
from auto_search.solution_space.schedule_action.emit_insn.apply_action.rules.comm import \
    get_emit_insn_axis

REDUCE_TRANS = "trans"
STORAGE_BOUND = "storage_bound"
REDUCE_OPT_MODE = "reduce_opt_mode"


def _is_last_axis_align(stage):
    """
    :param stage:
    :return:
    """
    input_shape = stage.op.input_tensors[0].shape
    out_dtype = stage.op.output(0).dtype
    dtype_byte = DTYPE_BYTE_MAPPING.get(out_dtype, None)
    block_size = BLOCK_SIZE // dtype_byte
    if int(input_shape[-1]) % block_size == 0:
        return True
    return False


def _do_emit_insn_for_reduce_rfactor(op_schedule_info, stage_index):
    """
    :param op_schedule_info:
    :return:
    """
    stage_name = op_schedule_info.stage_name
    op_intrin_key_index = op_schedule_info.op_intrin_key_index
    axis_info_list = op_schedule_info.axis_info_list
    intrinsic_func_name = op_intrin_key_index[op_schedule_info.feature_tensor[stage_index][
        FeatureTensorCfg.compute_s]].intrin

    # get emit_insn axis
    curr_axis_info_list = axis_info_list[stage_index]
    for i, axis_info in enumerate(curr_axis_info_list):
        axis_name = axis_info.name
        if 'reduce_axis' not in axis_name:
            continue
        emit_insn_axis = axis_name
        emit_insn_axis_obj = axis_info.body
        axis_num = i
        break

    op_schedule_info.stage.emit_insn(emit_insn_axis_obj, intrinsic_func_name)
    op_schedule_info.code_lines.append(
        f"sch[{stage_name}].emit_insn({emit_insn_axis}, '{intrinsic_func_name}')")

    cheque = get_emit_insn_cheque(stage_index, intrinsic_func_name, (emit_insn_axis, axis_num))
    op_schedule_info.cheque_list.append(cheque)

    op_schedule_info.proc_flag_dict[stage_index] = True


def _do_emit_insn_for_reduce(op_schedule_info, stage_index):
    """
    :param op_schedule_info:
    :param stage_index:
    :return:
    """
    stage_name = op_schedule_info.stage_name
    norm_graph_info = op_schedule_info.compute_graph_info
    op_intrin_key_index = op_schedule_info.op_intrin_key_index
    axis_info_list = op_schedule_info.axis_info_list
    intrinsic_func_name = op_intrin_key_index[op_schedule_info.feature_tensor[stage_index][
        FeatureTensorCfg.compute_s]].intrin
    axis_num, emit_insn_axis, emit_insn_axis_obj = get_emit_insn_axis(
        op_schedule_info.stage, stage_index, axis_info_list,
        op_schedule_info.stages_info[stage_index])

    at_stage_index = op_schedule_info.at_dict.get(stage_index)
    at_target = op_schedule_info.at_targets[stage_index]
    at_stage_axis_info_list = op_schedule_info.axis_info_list[at_stage_index][:]
    at_stage_axis_info_list.reverse()
    storage_bound = 1
    for axis_info in at_stage_axis_info_list:
        if axis_info.name == at_target.name:
            break
        if axis_info.attr in ['o', 'i']:
            storage_bound *= axis_info.len
        else:
            axis_index = axis_info.index
            storage_bound *= norm_graph_info.shape_after_reduce[axis_index]
    if axis_info_list[stage_index]:
        storage_bound = 1

    attrs = {STORAGE_BOUND: int(storage_bound)}
    if op_schedule_info.is_ub_transpose:
        if intrinsic_func_name in ("vector_reduce_max", "vector_reduce_min", "vector_reduce_sum") \
                and not _is_last_axis_align(op_schedule_info.stage):
            attrs.update({REDUCE_TRANS: True})
    elif intrinsic_func_name in ("vector_reduce_max", "vector_reduce_min", "vector_reduce_sum") and \
            len(norm_graph_info.shape_before_reduce) == 2 and \
            norm_graph_info.reduce_axis_index == [1] and \
            norm_graph_info.shape_before_reduce[-1] > 128 and \
            op_schedule_info.axis_info_list[stage_index] is None:
        if intrinsic_func_name in ("vector_reduce_max", "vector_reduce_min"):
            attrs.update({REDUCE_OPT_MODE: 'entire_reduce', "reuse_dst_tensor": True, "enough_buffer": True})
        else:
            attrs.update({REDUCE_OPT_MODE: 'entire_reduce', "reuse_dst_tensor": True})
        reduce_dtype = op_schedule_info.schedule_obj.stages[stage_index].op.output(0).dtype
        storage_bound = DTYPE_REPEAT_MAPPING.get(reduce_dtype) * attrs.get(STORAGE_BOUND)
        attrs.update({STORAGE_BOUND: storage_bound})

    op_schedule_info.stages_info[stage_index][STORAGE_BOUND] = storage_bound

    op_schedule_info.stage.emit_insn(emit_insn_axis_obj, intrinsic_func_name, attrs=attrs)
    op_schedule_info.code_lines.append(
        f"sch[{stage_name}].emit_insn({emit_insn_axis}, '{intrinsic_func_name}', {str(attrs)})")

    cheque = get_emit_insn_cheque(stage_index, intrinsic_func_name, (emit_insn_axis, axis_num), extra_info=attrs)
    op_schedule_info.cheque_list.append(cheque)

    op_schedule_info.proc_flag_dict[stage_index] = True


def proc(progress):
    """
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    stage_index = op_schedule_info.stage_index

    if op_schedule_info.proc_flag_dict.get(stage_index, False):
        return True

    if {'align_pad', 'remove_pad'} & set(op_schedule_info.stages_info[stage_index].get('type', [])):
        return True

    stage_type = op_schedule_info.stages_info[stage_index].get('type')

    if 'reduce_rfactor' in stage_type:
        _do_emit_insn_for_reduce_rfactor(op_schedule_info, stage_index)
    elif "CacheWrite" in stage_type and "reduce" in stage_type:
        _do_emit_insn_for_reduce(op_schedule_info, stage_index)

    return True
