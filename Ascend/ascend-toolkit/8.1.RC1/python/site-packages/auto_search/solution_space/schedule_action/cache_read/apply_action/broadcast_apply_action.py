#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
common apply action
"""
import copy
import numpy

from auto_search.solution_space import tvm_compute as tvm_compute_to_tensor
from auto_search.solution_space.tensor_cfg import get_init_action_tensor_zero
from auto_search.utils import logger
from auto_search.solution_space.action import apply_action_register
from auto_search.solution_space.action import ScheduleActionType
from auto_search.compute_analysis import ComputePattern
from auto_search.solution_space.op_schedule_info import AtInfo
from auto_search.bank.cheque_generator import get_cache_read_cheque
from .generic_apply_action import get_cache_read_name


@apply_action_register([ComputePattern.BROADCAST], ScheduleActionType.CACHE_READ)
def apply(progress):
    """
    cache read apply
    :param progress:
    :return:
    """
    stage_index = progress.todo.stage_index

    sch = progress.op_schedule_info.schedule_obj
    stages_info = progress.op_schedule_info.stages_info
    progress.op_schedule_info.stage_index_map = {}
    stage_index_map = progress.op_schedule_info.stage_index_map

    for i in range(len(sch.stages)):
        index = stages_info[i]['at_info'].index
        if index not in stage_index_map \
                and 'local.UB' not in stages_info[i].get('scope', []):
            stage_index_map[index] = i

    all_tensors = []
    for stage in sch.stages:
        tmp_tensor_list = []
        for idx in range(stage.op.num_outputs):
            tmp_tensor_list.append(stage.origin_op.output(idx))
            all_tensors.append(stage.origin_op.output(idx))

    curr_tensor = sch.stages[stage_index].origin_op.output(0)

    cache_read_infos = gen_cache_read_info(curr_tensor, all_tensors, stages_info[stage_index], stage_index_map)
    for cache_read_info in cache_read_infos:
        do_cache_read(progress, cache_read_info)
    if cache_read_infos[0].get('broadcast_remove_pad', False):
        if len(cache_read_infos) > 1:
            raise RuntimeError("remove_pad only do one time for each not align broadcast tensor!")
        _update_stages_relevant_info(progress)
    logger.debug("apply cache read done.")


def _update_stages_relevant_info(progress):
    """
    after doing rfactor add a stageï¼Œneed update stage relevant info
    :param progress:
    :param reduce_stage_index:
    :param rfactor_stage_name:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    sch = op_schedule_info.schedule_obj
    stages_info = op_schedule_info.stages_info
    cache_read_index = progress.todo.stage_index + 1
    _update_inlined_stages(progress, cache_read_index)
    new_feature_tensor, new_reduce_axis_dict \
        = tvm_compute_to_tensor.proc(sch, stages_info,
                                     op_schedule_info.op_name)

    op_schedule_info.feature_tensor = new_feature_tensor
    op_schedule_info.reduce_axis_dict = new_reduce_axis_dict
    op_schedule_info.update_stages_info()

    op_schedule_info.update_worksapce_list()
    op_schedule_info.update_at_dict()
    op_schedule_info.update_dependency_dict()
    _update_actions(progress, cache_read_index)


def _update_inlined_stages(progress, cache_read_index):
    """
    :param progress:
    :param rfactor_stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    inlined_stages = op_schedule_info.inlined_stages
    for idx, stage_index in enumerate(inlined_stages):
        if stage_index >= cache_read_index:
            inlined_stages[idx] = stage_index + 1

    op_schedule_info.inlined_stages = list(inlined_stages)


def _update_actions(progress, cache_read_index):
    """
    :param progress:
    :return:
    """
    action = get_init_action_tensor_zero(1)
    action = action.tolist()[0]
    progress.action_tensor = numpy.insert(progress.action_tensor, cache_read_index, action, 0)


def gen_cache_read_info(cur_tensor, all_tensors, stage_info, stage_index_map):
    """
    gen cache read info
    :param cur_tensor:
    :param all_tensors:
    :param stage_info:
    :param stage_index_map:
    :return:
    """
    tensor_name = cur_tensor.op.name
    scope = 'local.UB'
    base_cache_rw_info = {"type": "CacheRead", "scope": scope, "name": stage_info["name"], }
    idx = 0

    if 'CacheWrite' in stage_info.get('type', []) and stage_info.get("tag", []).find('broadcast') != -1:
        base_cache_rw_info["broadcast_remove_pad"] = True
        idx = 100

    ori_tensor_name = stage_info.get("ori_name", tensor_name)
    cache_rw_infos = []
    worksapce_info = stage_info["at_info"]
    consumers_in_group = worksapce_info.consumers_in_group()
    for i, group in enumerate(consumers_in_group):
        cache_rw_info = copy.deepcopy(base_cache_rw_info)
        cache_rw_info['consumers'] = [consumer.index for consumer in group]
        cache_rw_info['fanout_tensors'] = [all_tensors[stage_index_map[consumer.index]] for consumer in group]
        cache_rw_info['rw_name'] = get_cache_read_name(ori_tensor_name, scope, index=i + idx)
        cache_rw_info["tensor"] = cur_tensor
        cache_rw_info["ub2l1"] = False
        cache_rw_infos.append(cache_rw_info)

    return cache_rw_infos


def update_stage_info(progress, cache_rw_info, rw_tensor):
    """
    update_stage_info
    :param progress:
    :param cache_rw_info:
    :return:
    """
    stages_info = progress.op_schedule_info.stages_info
    sch = progress.op_schedule_info.schedule_obj
    buf_str = str(cache_rw_info['scope'])

    src_stage_info = stages_info[cache_rw_info["tensor_ori_index"]]
    src_stage_info['type'].append('src_cache_read')
    src_at_info = src_stage_info['at_info']
    rw_stage_types = ["CacheRead"]
    rw_at_info = AtInfo(src_at_info.index)
    for consumer in src_at_info.consumers:
        if consumer.index in cache_rw_info['consumers']:
            consumer_info = copy.deepcopy(consumer)
            rw_at_info.add_consumer(consumer_info)

    # remove_pad for broadcast
    if cache_rw_info.get('broadcast_remove_pad', False):
        rw_stage_types.append('broadcast_remove_pad')
        src_stage_info['type'].append('src_remove_pad')

    rw_stage_index = list(sch.stages).index(sch[rw_tensor])
    rw_stage_info = {
        'name': cache_rw_info['rw_name'],
        'type': rw_stage_types,
        'scope': buf_str,
        'at_info': rw_at_info,
        'ori_name': src_stage_info.get('ori_name'),
        'tag': src_stage_info.get('tag'),
        "rw_tensor": rw_tensor
    }

    stages_info.insert(rw_stage_index, rw_stage_info)


def _update_broadcast_remove_pad_fanout_tensors(progress, cache_rw_info):
    """
    :param progress:
    :param cache_rw_info:
    :return:
    """
    if not cache_rw_info.get('broadcast_remove_pad', False):
        return
    curr_tensor = cache_rw_info['tensor']
    sch = progress.op_schedule_info.schedule_obj
    fanout_tensors = []
    for stage in sch.stages:
        if curr_tensor not in list(stage.op.input_tensors):
            continue
        for idx in range(stage.op.num_outputs):
            fanout_tensors.append(stage.origin_op.output(idx))
    cache_rw_info['fanout_tensors'] = fanout_tensors


def do_cache_read(progress, cache_rw_info):
    """
    do cache read and generate the stage info
    :param progress:
    :param cache_rw_info:
    :return:
    """
    if cache_rw_info["type"] != 'CacheRead':
        logger.warn("Unknown action:%s", cache_rw_info["type"])
        return

    sch = progress.op_schedule_info.schedule_obj
    stage_index_map = progress.op_schedule_info.stage_index_map

    stage_names = [stage_info["name"] for stage_info in progress.op_schedule_info.stages_info]
    tensor_ori_index = stage_names.index(cache_rw_info['name'])
    cache_rw_info["tensor_ori_index"] = tensor_ori_index

    consumer_stage_idx_list = []
    real_fanouts = []
    out_name_list = []
    _update_broadcast_remove_pad_fanout_tensors(progress, cache_rw_info)
    for fanout_tensor in cache_rw_info['fanout_tensors']:
        fanout_index = list(sch.stages).index(sch[fanout_tensor])
        real_fanouts.append(fanout_tensor)
        out_name_list.append(progress.op_schedule_info.stages_info[fanout_index]["name"])
        consumer_stage_idx_list.append(fanout_index)

    code_line = "%s = sch.cache_read(%s, '%s', [%s])" % (
        cache_rw_info['rw_name'], cache_rw_info['name'], str(cache_rw_info['scope']), ",".join(out_name_list))

    rw_tensor = sch.cache_read(cache_rw_info['tensor'], cache_rw_info['scope'], real_fanouts)

    # Generate the cheque
    cheque = get_cache_read_cheque(tensor_ori_index, cache_rw_info['scope'], consumer_stage_idx_list)
    progress.op_schedule_info.cheque_list.append(cheque)

    # update stage_index_map
    for ori_index, cur_index in stage_index_map.items():
        if cur_index > tensor_ori_index:
            stage_index_map[ori_index] = cur_index + 1

    update_stage_info(progress, cache_rw_info, rw_tensor)

    progress.op_schedule_info.code_lines.append(code_line)

    return
