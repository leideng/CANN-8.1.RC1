#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
reduce_r05_block_tiling_rfactor_for_atomic
"""
from typing import List
from typing import NoReturn
from functools import reduce
import copy
import numpy

from tbe import tvm
from tbe.common.platform import platform_info

from auto_search.utils import logger
from auto_search.config import soc_cfg
from auto_search.solution_space.tensor_cfg import AxisInfo
from auto_search.solution_space.tensor_cfg import TensorInfo
from auto_search.solution_space.tensor_cfg import ActionTensorCfg
from auto_search.solution_space.tensor_cfg import get_init_action_tensor_zero
from auto_search.solution_space import t2c_util
from auto_search.solution_space.t2c_util import SplitInfo
from auto_search.compute_analysis import StageOrderedAxes
from auto_search.solution_space.schedule_action.split.apply_action import reduce_comm
from auto_search.solution_space import tvm_compute as tvm_compute_to_tensor
from auto_search.solution_space.progress import Progress
from auto_search.bank.cheque_generator import get_split_cheque
from auto_search.bank.cheque_generator import get_axis_cheque
from auto_search.bank.cheque_generator import get_fuse_cheque
from auto_search.bank.cheque_generator import get_rfactor_cheque
from auto_search.bank.cheque_generator import get_set_scope_cheque
from auto_search.bank.cheque_generator import get_cache_write_cheque
from auto_search.bank.cheque_generator import get_cache_write_cheque_spec


def get_ub_tiling(progress: Progress, code_lines: List, reduce_gm_stage_index: int) -> SplitInfo:
    """
    get ub tiling info
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    reduce_stage_info = op_schedule_info.stages_info[reduce_gm_stage_index]
    reduce_stage_name = reduce_stage_info.get('name')
    ub_split_axis_type = reduce_stage_info.get('split_axis_type', None)
    if ub_split_axis_type is None:
        raise RuntimeError("atomic ub_split_axis_type must be set as 'axis' or 'reduce_axis'!")
    split_vector = progress.action_tensor[reduce_gm_stage_index][
                   ActionTensorCfg.split_factor_s:ActionTensorCfg.split_factor_e + 1]
    split_axis_obj = op_schedule_info.schedule_obj.stages[reduce_gm_stage_index].op.reduce_axis
    if ub_split_axis_type == 'axis':
        split_axis_obj = op_schedule_info.schedule_obj.stages[reduce_gm_stage_index].op.axis
    ub_split_axis_index, ub_split_factor = \
        _get_ub_split_axis_index_and_factor(op_schedule_info, reduce_gm_stage_index,
                                            reduce_stage_info, split_axis_obj,
                                            split_vector)

    # stage's reduce axis will changed，record ub_split_axis
    prefix = "reduce_axis"
    if ub_split_axis_type == 'axis':
        prefix = "axis"
    ub_split_axis_name = f'ub_split_{prefix}'
    code_lines.append(f'{ub_split_axis_name} = sch[{reduce_stage_name}].op.{prefix}[{ub_split_axis_index}]')

    ub_split_info = reduce_comm.gen_split_info(
        factor=ub_split_factor,
        axis_index=ub_split_axis_index,
        axis_name=ub_split_axis_name,
        axis_obj=split_axis_obj[ub_split_axis_index])
    return ub_split_info


def _get_ub_split_axis_index_and_factor(op_schedule_info, reduce_gm_stage_index, reduce_stage_info, split_axis_obj,
                                        split_vector):
    """
    :param op_schedule_info:
    :param reduce_gm_stage_index:
    :param reduce_stage_info:
    :param split_axis_obj:
    :param split_vector:
    :return:
    """
    ub_split_axis_index = len(split_axis_obj) - 1
    ub_split_factor = 1
    if 'stage_ordered_axes_obj' not in reduce_stage_info:
        reduce_stage_info['stage_ordered_axes_obj'] = \
            StageOrderedAxes(op_schedule_info.schedule_obj.stages, reduce_gm_stage_index)
    ordered_axes_obj: StageOrderedAxes = reduce_stage_info.get('stage_ordered_axes_obj', None)
    for axis_index, factor in enumerate(split_vector):
        # find first not zero factor is split axis
        if factor != 0:
            # axis_index is the index in reduce stage's reduce_before_shape,
            # here need get index in reduce_axis or axis
            ub_split_axis_index = ordered_axes_obj.get_index_within_type(axis_index)
            ub_split_factor = factor
            break
    return ub_split_axis_index, ub_split_factor


def get_block_dim(axis_len: int) -> int:
    """
    :param axis_len:
    :return:
    """
    core_num = soc_cfg.get_core_num()

    if axis_len <= core_num:
        block_dim = axis_len
    else:
        block_dim = core_num
    return block_dim


def get_block_tiling(progress: Progress, reduce_gm_stage_index: int, ub_split_info: SplitInfo) -> SplitInfo:
    """
    gen block tiling info
    :param progress:
    :param reduce_gm_stage_index:
    :param ub_split_info:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    reduce_stage = op_schedule_info.schedule_obj.stages[reduce_gm_stage_index]
    reduce_stage_info = op_schedule_info.stages_info[reduce_gm_stage_index]
    stage_reduce_axis_objs = reduce_stage.op.reduce_axis
    ub_split_axis_type = reduce_stage_info.get('split_axis_type', None)
    if ub_split_axis_type is None:
        raise RuntimeError("atomic ub_split_axis_type must be set as 'axis' or 'reduce_axis'!")
    ub_split_axis_index = ub_split_info.axis_index
    ub_split_factor = ub_split_info.factor

    block_split_axis_index, block_split_outer_size = \
        _get_block_split_axis_index_and_outer_size(stage_reduce_axis_objs,
                                                   ub_split_axis_index,
                                                   ub_split_axis_type,
                                                   ub_split_factor)

    # get split factor
    block_split_axis_len = stage_reduce_axis_objs[block_split_axis_index].dom.extent.value
    block_split_factor = (block_split_axis_len + block_split_outer_size - 1) // block_split_outer_size
    # if ub and block split axis is same and type is reduce axis,
    # block_split_factor need bigger than ub_split_factor,
    # because the inner axis after block tiling will be used to split ub
    if ub_split_axis_type != 'axis' and block_split_axis_index == ub_split_axis_index:
        block_split_factor = max(block_split_factor, ub_split_factor)

    block_split_info = reduce_comm.gen_split_info(
        factor=block_split_factor,
        axis_index=block_split_axis_index,
        axis_name=f"{reduce_stage_info.get('name')}_reduce_axis_{block_split_axis_index}",
        axis_obj=stage_reduce_axis_objs[block_split_axis_index])
    return block_split_info


def _get_block_split_axis_index_and_outer_size(stage_reduce_axis_objs, ub_split_axis_index, ub_split_axis_type,
                                               ub_split_factor):
    """
    :param stage_reduce_axis_objs:
    :param ub_split_axis_index:
    :param ub_split_axis_type:
    :param ub_split_factor:
    :return:
    """
    # if ub split axis is normal axis, all reduce axis can be bind core
    if ub_split_axis_type == 'axis':
        block_split_shape = [
            stage_reduce_axis_obj.dom.extent.value
            for stage_reduce_axis_obj in stage_reduce_axis_objs
        ]
    # if ub split axis is reduce axis,
    # only reduce axis before split axis and current split axis can be bind core
    else:
        block_split_shape = [
            stage_reduce_axis_objs[axis_index].dom.extent.value
            for axis_index in range(ub_split_axis_index)
        ]
        ub_split_axis_len = stage_reduce_axis_objs[ub_split_axis_index].dom.extent.value
        block_split_shape.append((ub_split_axis_len + ub_split_factor - 1) // ub_split_factor)
    block_dim = get_block_dim(reduce(lambda x, y: x * y, block_split_shape))
    block_split_axis_index, block_split_outer_size \
        = reduce_comm.find_split_axis(block_split_shape, 0, len(block_split_shape) - 1, block_dim)
    return block_split_axis_index, block_split_outer_size


def get_tiling(progress: Progress, code_lines: List, reduce_gm_stage_index: int) -> tuple:
    """
    get split and split axis
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :return:
    """
    # 1. get ub_split_axis、ub_split_factor, get from search
    ub_split_info = get_ub_tiling(progress, code_lines, reduce_gm_stage_index)

    # 2. get block_split_axis、block_split_factor, get from ub split info and core_num
    block_split_info = get_block_tiling(progress, reduce_gm_stage_index, ub_split_info)

    return ub_split_info, block_split_info


def do_block_tiling(progress: Progress, code_lines: List[str], reduce_gm_stage_index: int,
                    block_split_info: SplitInfo) -> SplitInfo:
    """
    atomic do block tiling
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :param block_split_info:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    stage = op_schedule_info.schedule_obj.stages[reduce_gm_stage_index]
    stage_name = op_schedule_info.stages_info[reduce_gm_stage_index].get('name')
    outer_axis_name = '%s_o' % block_split_info.axis_name
    inner_axis_name = '%s_i' % block_split_info.axis_name
    code_lines.append(
        '%s, %s = sch[%s].split(sch[%s].op.reduce_axis[%d], factor=%d)' %
        (outer_axis_name,
         inner_axis_name,
         stage_name,
         stage_name,
         block_split_info.axis_index,
         block_split_info.factor))

    # get block_split_info.axis_obj in all axis index
    all_axes = list(stage.leaf_iter_vars)
    axis_index_in_all_axes = 0
    for axis_idx, axis in enumerate(all_axes):
        if axis == block_split_info.axis_obj:
            axis_index_in_all_axes = axis_idx
            break

    block_split_axis_obj_o, block_split_axis_obj_i = \
        stage.split(block_split_info.axis_obj, factor=block_split_info.factor)

    # gen block_split_info
    block_split_info = reduce_comm.gen_split_info(block_split_info,
                                                  outer_axis_name=outer_axis_name,
                                                  outer_axis_obj=block_split_axis_obj_o,
                                                  inner_axis_name=inner_axis_name,
                                                  inner_axis_obj=block_split_axis_obj_i)

    # gen cheque
    cheque = get_split_cheque(reduce_gm_stage_index, axis_index_in_all_axes, block_split_info.factor)
    op_schedule_info.cheque_list.append(cheque)

    return block_split_info


def fuse_proc(progress: Progress, code_lines: List[str], reduce_gm_stage_index: int,
              block_split_info: SplitInfo) -> tuple:
    """
    fuse all reduce axes before block_split_axis
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :param block_split_info:
    :return:
    """
    reduce_stage = progress.op_schedule_info.schedule_obj.stages[reduce_gm_stage_index]
    reduce_stage_name = progress.op_schedule_info.stages_info[reduce_gm_stage_index].get('name')
    stage_reduce_axis_objs = reduce_stage.op.reduce_axis
    fused_axis_name = f'{reduce_stage_name}_reduce_axis_fused'
    fuse_code = [f'{fused_axis_name} = sch[{reduce_stage_name}].fuse(']
    fused_list = []

    for axis_index in range(block_split_info.axis_index):
        axis_name = f'{reduce_stage_name}_reduce_axis_{axis_index}'
        code_lines.append(f'{axis_name} = sch[{reduce_stage_name}].op.reduce_axis[{axis_index}]')
        fuse_code.append(f'{axis_name}, ')
        fused_list.append(stage_reduce_axis_objs[axis_index])
    fuse_code.append(f'{block_split_info.outer_axis_name})')
    fused_list.append(block_split_info.outer_axis_obj)

    # get fused_list in all axis index
    fuse_end_index, fuse_start_index = _get_fused_axis_index_in_all_axes(fused_list, reduce_stage)

    fused_axis_obj = reduce_stage.fuse(*fused_list)
    code_lines.append(''.join(fuse_code))

    cheque = get_fuse_cheque(reduce_gm_stage_index, list(range(fuse_start_index, fuse_end_index + 1)))
    progress.op_schedule_info.cheque_list.append(cheque)

    return fused_axis_name, fused_axis_obj


def _get_fused_axis_index_in_all_axes(fused_list, reduce_stage):
    """
    :param fused_list:
    :param reduce_stage:
    :return:
    """
    all_axes = list(reduce_stage.leaf_iter_vars)
    fuse_start_index = 0
    fuse_end_index = 0
    for axis_idx, axis in enumerate(all_axes):
        if axis == fused_list[0]:
            fuse_start_index = axis_idx
        if axis == fused_list[-1]:
            fuse_end_index = axis_idx
    return fuse_end_index, fuse_start_index


def get_reduce_tag(reduce_stage):
    """
    get reduce info, get reduce tag
    :param reduce_stage:
    :return:
    """
    # get reduce's type
    reduce_type = t2c_util.REDUCE_NIST_KEYWORD
    source_axis_vars = reduce_stage.op.body[0].source[0].indices
    for reduce_axis in reduce_stage.op.reduce_axis:
        if reduce_axis.var.same_as(source_axis_vars[-1]):
            reduce_type = t2c_util.REDUCE_LAST_KEYWORD
            break
    # get reduce tag
    reduce_op_tag = reduce_stage.origin_op.tag + reduce_type
    return reduce_type, reduce_op_tag


def rfactor_proc(progress: Progress, code_lines: List[str], reduce_gm_stage_index: int, reduce_tensors: List,
                 fused_axis_info: List) -> NoReturn:
    """
    do rfactor after fused
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :param reduce_tensors:
    :param fused_axis_info:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    # get reduce tag
    reduce_type, reduce_op_tag = get_reduce_tag(op_schedule_info.schedule_obj.stages[reduce_gm_stage_index])

    # rfactor code_lines
    fused_axis_name, fused_axis_obj = fused_axis_info
    reduce_stage_name = op_schedule_info.stages_info[reduce_gm_stage_index].get('name')
    rfactor_stage_name = f'{reduce_stage_name}_rfactor'
    if len(reduce_tensors) > 1:
        rfactor_codes = f'sch.rfactor({reduce_stage_name}, {fused_axis_name}, factor_axis=-1)[0]'
    else:
        rfactor_codes = f'sch.rfactor({reduce_stage_name}, {fused_axis_name}, factor_axis=-1)'
    rfactor_codes = f'{rfactor_stage_name} = {rfactor_codes}'
    code_lines.append(rfactor_codes)
    code_lines.append(f"sch[{rfactor_stage_name}].set_scope('{platform_info.scope_ubuf}')")

    # get fused_axis_obj in all axis index
    all_axes = list(op_schedule_info.schedule_obj.stages[reduce_gm_stage_index].leaf_iter_vars)
    axis_index_in_all_axes = 0
    for axis_idx, axis in enumerate(all_axes):
        if axis == fused_axis_obj:
            axis_index_in_all_axes = axis_idx
            break

    # notice: rfactor object is the op in original compute, not stage.op, is stage.origin_op
    if len(reduce_tensors) > 1:
        rfactor_tensor = op_schedule_info.schedule_obj.rfactor(reduce_tensors[0], fused_axis_obj, factor_axis=-1)[0]
    else:
        rfactor_tensor = op_schedule_info.schedule_obj.rfactor(reduce_tensors[0], fused_axis_obj, factor_axis=-1)
    if not isinstance(rfactor_tensor, tvm.Tensor):
        logger.error("rfactor tensor type should be Tensor!")
    op_schedule_info.schedule_obj[rfactor_tensor].set_scope(platform_info.scope_ubuf)

    # after do rfactor, will gen a new stage, add a new stage_info to stages_info
    at_info = copy.deepcopy(op_schedule_info.stages_info[reduce_gm_stage_index].get('at_info'))
    op_schedule_info.stages_info.insert(
        reduce_gm_stage_index,
        {
            'name': rfactor_stage_name,
            'type': ['reduce_atomic_rfactor'],
            'scope': platform_info.scope_ubuf,
            'op_tag': reduce_op_tag,
            'reduce_type': reduce_type,
            'at_info': at_info,
            'split_axis_type': op_schedule_info.stages_info[reduce_gm_stage_index].get('split_axis_type', None)
        }
    )
    # gen rfactor cheque
    cheque = get_rfactor_cheque(reduce_gm_stage_index, axis_index_in_all_axes, -1)
    op_schedule_info.cheque_list.append(cheque)


def do_rfactor(progress: Progress, code_lines: List[str], reduce_gm_stage_index: int, block_split_info: SplitInfo,
               reduce_tensors: List) -> NoReturn:
    """
    do atomic rfactor
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :param block_split_info:
    :param reduce_tensors:
    :return:
    """
    fused_axis_name, fused_axis_obj = fuse_proc(progress, code_lines, reduce_gm_stage_index, block_split_info)

    rfactor_proc(progress, code_lines, reduce_gm_stage_index,
                 reduce_tensors, [fused_axis_name, fused_axis_obj])

    cheque = get_set_scope_cheque(reduce_gm_stage_index, platform_info.scope_ubuf)
    progress.op_schedule_info.cheque_list.append(cheque)

    # after rfactor and before split, gen current stage all axis cheque
    axis_cheque_list = \
        get_axis_cheque(progress.op_schedule_info.schedule_obj.stages[reduce_gm_stage_index], reduce_gm_stage_index)
    progress.op_schedule_info.cheque_list.extend(axis_cheque_list)


def gen_a_stage_axis_info(progress: Progress, stage_index: int) -> List:
    """
    gen reduce stage axis_info
    :param progress:
    :param stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    code_lines = op_schedule_info.code_lines
    stage_name = op_schedule_info.stages_info[stage_index].get('name')
    stage = op_schedule_info.schedule_obj.stages[stage_index]

    stage_axis_info = []
    for axis_index, axis in enumerate(stage.op.axis):
        axis_name = f'{stage_name}_axis_{axis_index}'
        code_lines.append(f'{axis_name} = sch[{stage_name}].op.axis[{axis_index}]')
        stage_axis_info.append(AxisInfo(axis_name,
                                        stage.op.axis[axis_index].dom.extent.value,
                                        'axis', axis_index, 'axis', axis))

    for axis_index, axis in enumerate(stage.op.reduce_axis):
        axis_name = f'{stage_name}_reduce_axis_{axis_index}'
        code_lines.append(f'{axis_name} = sch[{stage_name}].op.reduce_axis[{axis_index}]')
        stage_axis_info.append(AxisInfo(axis_name,
                                        stage.op.reduce_axis[axis_index].dom.extent.value,
                                        'reduce_axis', axis_index, 'reduce_axis', axis))

    axis_cheque_list = get_axis_cheque(op_schedule_info.schedule_obj.stages[stage_index], stage_index)
    op_schedule_info.cheque_list.extend(axis_cheque_list)

    return stage_axis_info


def do_gm_cache_write(progress: Progress, code_lines: List[str], reduce_gm_stage_index: int,
                      reduce_tensors: List) -> tuple:
    """
    reduce stage do cache_write
    :param progress:
    :param code_lines:
    :param reduce_gm_stage_index:
    :param reduce_tensors:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    reduce_stage_name = op_schedule_info.stages_info[reduce_gm_stage_index].get('name')
    write_stage_name = f"{reduce_stage_name}_l"
    # tuple reduce sum
    if len(reduce_tensors) > 1:
        reduce_tensor_names = []
        write_tensor_names = []
        for index in range(len(reduce_tensors)):
            reduce_tensor_names.append('%s_v%d' % (reduce_stage_name, index))
            write_tensor_names.append('%s_v%d_l' % (reduce_stage_name, index))
        code_lines.append("{} = sch.cache_write({}, '')".format(", ".join(write_tensor_names),
                                                                "[" + ", ".join(reduce_tensor_names) + "]"))
        code_lines.append('{} = {}'.format(write_stage_name, write_tensor_names[0]))
        write_tensors = op_schedule_info.schedule_obj.cache_write(reduce_tensors, "")
        cheque = get_cache_write_cheque_spec(reduce_gm_stage_index, "", len(reduce_tensors))
        progress.op_schedule_info.cheque_list.append(cheque)
    # single output
    else:
        code_lines.append(f"{write_stage_name} = sch.cache_write({reduce_stage_name}, '')")
        # reduce_tensors and write_tensors type are all list
        write_tensors = op_schedule_info.schedule_obj.cache_write(reduce_tensors, "")
        write_tensor_names = [write_stage_name]

        # gen cache_write cheque
        cheque = get_cache_write_cheque(reduce_gm_stage_index, "")
        op_schedule_info.cheque_list.append(cheque)

    # after cache write, stage index will change
    write_stage_index = reduce_gm_stage_index

    # add reduce_write stage_info to stages_info
    at_info = copy.deepcopy(op_schedule_info.stages_info[reduce_gm_stage_index].get('at_info'))
    op_schedule_info.stages_info.insert(
        write_stage_index,
        {
            'name': write_stage_name,
            'type': ['CacheWrite', 'reduce_atomic_write'],
            'scope': "",
            'at_info': at_info
        }
    )

    return write_tensor_names, write_tensors


def update_stage_index_map(progress: Progress, reduce_stage_index: int) -> bool:
    """
    reduce_stage_index add 2，need update stage_index_map
    :param progress:
    :param reduce_stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    for key, value in op_schedule_info.stage_index_map.items():
        if value == reduce_stage_index - 2:
            del op_schedule_info.stage_index_map[key]
            op_schedule_info.stage_index_map[value] = reduce_stage_index
            return True
    return False


def update_feature(progress: Progress, block_split_axis_index: int, rfactor_stage_index: int) -> bool:
    """
    stage num is changed，need update feature
    :param progress:
    :param block_split_axis_index:
    :param rfactor_stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    feature_tensor, reduce_axis_dict = \
        tvm_compute_to_tensor.proc(op_schedule_info.schedule_obj, op_schedule_info.stages_info,
                                   op_schedule_info.op_name)
    if block_split_axis_index \
            not in reduce_axis_dict.get(rfactor_stage_index).get("axis"):
        reduce_axis_dict.get(rfactor_stage_index)["axis"].append(
            block_split_axis_index)
    op_schedule_info.feature_tensor = feature_tensor
    op_schedule_info.reduce_axis_dict = reduce_axis_dict
    return True


def update_actions(progress: Progress) -> NoReturn:
    """
    after rfactor, the ub split stage is rfactor stage,
    so need set the rfactor_stage's action as reduce_gm_stage's action,
    reduce_write_stage and reduce_gm_stage need not be split,
    so insert two init action is meet requirements.
    :param progress:
    :return:
    """
    action = get_init_action_tensor_zero(1)
    progress.action_tensor = numpy.append(progress.action_tensor, action, 0)
    progress.action_tensor = numpy.append(progress.action_tensor, action, 0)


def update_output_info_list(progress: Progress,
                            reduce_tensors: List,
                            reduce_write_tensor_names: List,
                            reduce_write_tensors: List) -> NoReturn:
    """
    output tensor change to cache_write tensor
    :param progress:
    :param reduce_tensors:
    :param reduce_write_tensor_names:
    :param reduce_write_tensors:
    :return:
    """
    op_schedule_info = progress.op_schedule_info

    for reduce_index, reduce_tensor in enumerate(reduce_tensors):
        reduce_tensor_name = reduce_tensor.op.name
        if len(reduce_tensors) > 1:
            reduce_tensor_name += "_v%d" % reduce_index

        reduce_output_index = -1
        for index, output_info in enumerate(op_schedule_info.output_info_list):
            if output_info.name == reduce_tensor_name:
                reduce_output_index = index
                break

        # output_info_list
        shape_list = reduce_write_tensors[reduce_index].shape
        if isinstance(reduce_write_tensors[reduce_index].shape, tvm.container.Array):
            shape_tmp_list = []
            for shape in reduce_write_tensors[reduce_index].shape:
                shape_tmp_list.append(shape.value)
            shape_list = shape_tmp_list

        tensor_info = TensorInfo(reduce_write_tensor_names[reduce_index],
                                 shape_list,
                                 reduce_write_tensors[reduce_index].dtype)
        op_schedule_info.output_info_list[reduce_output_index] \
            = tensor_info


def update_real_out_tensor_str(progress: Progress,
                               reduce_tensors: List,
                               reduce_write_tensor_names: List) -> NoReturn:
    """
    output tensor change to cache_write tensor
    :param progress:
    :param reduce_tensors:
    :param reduce_write_tensor_names:
    :return:
    """
    op_schedule_info = progress.op_schedule_info

    for reduce_index, reduce_tensor in enumerate(reduce_tensors):
        reduce_tensor_name = reduce_tensor.op.name
        if len(reduce_tensors) > 1:
            reduce_tensor_name += "_v%d" % reduce_index
        # real_out_tensor_str
        if reduce_write_tensor_names[reduce_index] not in \
                op_schedule_info.real_out_tensor_str:
            op_schedule_info.real_out_tensor_str = \
                op_schedule_info.real_out_tensor_str.replace(
                    reduce_tensor_name,
                    reduce_write_tensor_names[reduce_index])


def update_special_tensor_dict(progress: Progress,
                               reduce_write_tensor_names: List,
                               reduce_write_tensors: List) -> NoReturn:
    """
    output tensor change to cache_write tensor
    :param progress:
    :param reduce_write_tensor_names:
    :param reduce_write_tensors:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    for reduce_write_tensor_name, reduce_write_tensor in zip(
            reduce_write_tensor_names, reduce_write_tensors):
        op_schedule_info.special_tensor_dict[
            reduce_write_tensor_name] = reduce_write_tensor


def update_output(progress: Progress,
                  reduce_tensors: List,
                  reduce_write_tensor_names: List,
                  reduce_write_tensors: List) -> NoReturn:
    """
    output tensor change to cache_write tensor
    :param progress:
    :param reduce_tensors:
    :param reduce_write_tensor_names:
    :param reduce_write_tensors:
    :return:
    """
    update_output_info_list(progress,
                            reduce_tensors,
                            reduce_write_tensor_names,
                            reduce_write_tensors)

    update_real_out_tensor_str(progress,
                               reduce_tensors,
                               reduce_write_tensor_names)

    update_special_tensor_dict(progress,
                               reduce_write_tensor_names,
                               reduce_write_tensors)


def update_reduce_atomic_dict(progress: Progress, reduce_stage_index: int) -> NoReturn:
    """
    :param progress:
    :param reduce_stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    code_lines = op_schedule_info.code_lines
    reduce_compute_graph = op_schedule_info.compute_graph_info
    block_split_info = op_schedule_info.atomic_block_split_info
    ub_split_info = op_schedule_info.atomic_ub_split_info

    reduce_atomic_dict = {
        "code_lines": code_lines,
        "cheque_list": op_schedule_info.cheque_list,
        "rfactor_stage_index": reduce_stage_index - 2,
        "reduce_write_stage_index": reduce_stage_index - 1,
        "stage_axis_infos": None,
        "reduce_op_tag": op_schedule_info.stages_info[reduce_stage_index - 2].get(
            'op_tag'),
        "reduce_dtype": op_schedule_info.schedule_obj.outputs[0].output(0).dtype,
        "shape_before_reduce": reduce_compute_graph.shape_before_reduce,
        "reduce_axis_indexs": reduce_compute_graph.reduce_axes_index,
        "block_split_axis_index": block_split_info.axis_index,
        "block_split_factor": block_split_info.factor,
        "ub_split_axis_type": op_schedule_info.stages_info[reduce_stage_index].get(
            'split_axis_type', 'reduce_axis'),
        "ub_split_axis_index": ub_split_info.axis_index,
        "ub_split_factor": ub_split_info.factor,
        "ub_outer_axis_info": None,
        "ub_inner_axis_info": None
    }

    op_schedule_info.reduce_atomic_dict.update(reduce_atomic_dict)


def update_at_dict(progress: Progress, rfactor_stage_index: int, reduce_write_stage_index: int) -> NoReturn:
    """
    :param progress:
    :param rfactor_stage_index:
    :param reduce_write_stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    for _, stage_info in enumerate(op_schedule_info.stages_info):
        if 'at_info' not in stage_info:
            continue
        at_info = stage_info['at_info']
        for consumer in at_info.consumers:
            stage_types = stage_info.get('type', [])
            if 'reduce_atomic_rfactor' in stage_types:
                # if reduce_atomic_rfactor, compute at reduce_atomic_write
                consumer.set_updated_sampled_target(reduce_write_stage_index)
                continue
            if 'reduce_atomic_write' in stage_types:
                # reduce_atomic_write need not compute at
                consumer.set_updated_sampled_target(None)
                continue
            if 'reduce_atomic' in stage_types:
                # reduce_atomic need not compute at
                consumer.set_updated_sampled_target(None)
                continue
            # before reduce_atomic_rfactor stage compute at reduce_atomic_rfactor stage
            consumer.set_updated_sampled_target(rfactor_stage_index)

    op_schedule_info.update_at_dict()


def update_stages_relevant_info(progress: Progress,
                                reduce_gm_stage_index: int,
                                reduce_write_tensor_names: List,
                                reduce_write_tensors: List) -> NoReturn:
    """
    add two stage，need update something
    :param progress:
    :param reduce_gm_stage_index:
    :param reduce_write_tensor_names:
    :param reduce_write_tensors:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    reduce_compute_graph = op_schedule_info.compute_graph_info
    block_split_info = op_schedule_info.atomic_block_split_info

    reduce_write_stage_index = reduce_gm_stage_index - 1
    rfactor_stage_index = reduce_gm_stage_index - 2

    update_stage_index_map(progress, reduce_gm_stage_index)

    update_at_dict(progress, rfactor_stage_index, reduce_write_stage_index)

    update_feature(progress, block_split_info.axis_index, rfactor_stage_index)

    update_actions(progress)

    update_output(progress,
                  reduce_compute_graph.reduce_tensors,
                  reduce_write_tensor_names,
                  reduce_write_tensors)

    update_reduce_atomic_dict(progress,
                              reduce_gm_stage_index)

    op_schedule_info.update_dependency_dict()


def proc(progress: Progress):
    """
    reduce atomic rfactor
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    if not op_schedule_info.is_atomic:
        return

    stages = list(op_schedule_info.schedule_obj.stages)
    # atomic last stage need be reduce stage
    reduce_gm_stage_index = len(stages) - 1
    if 'reduce_atomic' not in op_schedule_info.stages_info[reduce_gm_stage_index].get('type', []):
        logger.error("last stage is not reduce_atomic stage!")
        return

    # gen split stage all axes cheque
    axis_cheque_list = \
        get_axis_cheque(op_schedule_info.schedule_obj.stages[reduce_gm_stage_index], reduce_gm_stage_index)
    op_schedule_info.cheque_list.extend(axis_cheque_list)

    code_lines = op_schedule_info.code_lines
    reduce_compute_graph = op_schedule_info.compute_graph_info

    # also can write as stages[reduce_gm_stage_index].origin_op.output(0)
    reduce_compute_graph.reduce_tensors = [
        op_schedule_info.schedule_obj.outputs[0].output(idx)
        for idx in range(op_schedule_info.schedule_obj.outputs[0].num_outputs)
    ]
    reduce_compute_graph.reduce_stage = stages[reduce_gm_stage_index]

    # get split info
    ub_split_info, block_split_info = get_tiling(progress, code_lines, reduce_gm_stage_index)

    # do block tiling
    block_split_info = do_block_tiling(progress, code_lines, reduce_gm_stage_index, block_split_info)
    # save tiling info
    op_schedule_info.atomic_block_split_info = block_split_info
    op_schedule_info.atomic_ub_split_info = ub_split_info

    # do atomic rfactor
    do_rfactor(progress, code_lines, reduce_gm_stage_index, block_split_info, reduce_compute_graph.reduce_tensors)

    # rfactor_stage and reduce_gm_stage will change after do rfactor
    rfactor_stage_index = reduce_gm_stage_index
    reduce_gm_stage_index = rfactor_stage_index + 1

    # reduce gm stage do cache_write
    reduce_write_tensor_names, reduce_write_tensors = \
        do_gm_cache_write(progress, code_lines, reduce_gm_stage_index, reduce_compute_graph.reduce_tensors)
    # after do cache_write, add a cache_write stage，stage_index need update
    reduce_write_stage_index = reduce_gm_stage_index
    reduce_gm_stage_index = reduce_write_stage_index + 1

    # get cache_write stage axis_info
    reduce_write_stage_axis_info = gen_a_stage_axis_info(progress, reduce_write_stage_index)

    # axis_info_list is [] now，need creat
    stage_num = len(op_schedule_info.schedule_obj.stages)
    op_schedule_info.axis_info_list = [[]] * stage_num
    op_schedule_info.axis_info_list[reduce_write_stage_index] = reduce_write_stage_axis_info

    # add two stage，need update something
    update_stages_relevant_info(progress,
                                reduce_gm_stage_index,
                                reduce_write_tensor_names,
                                reduce_write_tensors)
    return
