#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
common apply action
"""
import copy

from tbe.common.platform import platform_info

from auto_search.utils import logger
from auto_search.solution_space.action import apply_action_register
from auto_search.solution_space.action import ScheduleActionType
from auto_search.compute_analysis import ComputePattern
from auto_search.solution_space.op_schedule_info import AtInfo
from auto_search.bank.cheque_generator import get_cache_read_cheque


def get_cache_read_name(ori_tensor_name, scope, index=None, ub2l1=False):
    """
    get cache read name
    :param ori_tensor_name:
    :param scope:
    :param index:
    :return:
    """
    head = ''
    tail = ''
    if ub2l1:
        head = 'sub'
    elif scope in [platform_info.scope_cbuf, platform_info.scope_ubuf]:
        tail = '_l'
    elif scope in [platform_info.scope_cc, platform_info.scope_ca,
                   platform_info.scope_cb]:
        head = 'sub'

    if index is not None:
        index_str = "_%03d" % index
    else:
        index_str = ""

    return head + ori_tensor_name + tail + index_str


def gen_cache_read_info(cur_tensor, all_tensors, stage_info, stage_index_map):
    """
    gen cache read info
    :param cur_tensor:
    :param all_tensors:
    :param stage_info:
    :param stage_index_map:
    :return:
    """
    tensor_name = cur_tensor.op.name
    scope = 'local.UB'
    base_cache_rw_info = {
        "type": "CacheRead",
        "scope": scope,
        "name": stage_info["name"],
    }
    idx = 0
    if "CacheRead" in stage_info.get("type", []):
        base_cache_rw_info["align_pad"] = True
        idx = int(stage_info["name"].split('_')[-1]) + 100

    ori_tensor_name = stage_info.get("ori_name", tensor_name)
    cache_rw_infos = []
    worksapce_info = stage_info["at_info"]
    consumers_in_group = worksapce_info.consumers_in_group()
    for i, group in enumerate(consumers_in_group):
        cache_rw_info = copy.deepcopy(base_cache_rw_info)
        cache_rw_info['consumers'] = [consumer.index for consumer in group]
        cache_rw_info['fanout_tensors'] = [
            all_tensors[stage_index_map[consumer.index]]
            for consumer in group
        ]

        cache_rw_info['rw_name'] = get_cache_read_name(ori_tensor_name,
                                                     scope,
                                                     index=i+idx)
        cache_rw_info["tensor"] = cur_tensor
        cache_rw_info["ub2l1"] = False
        cache_rw_infos.append(cache_rw_info)

    return cache_rw_infos


def update_stage_info(progress, cache_rw_info, rw_tensor):
    """
    update_stage_info
    :param progress:
    :param cache_rw_info:
    :return:
    """
    stages_info = progress.op_schedule_info.stages_info
    sch = progress.op_schedule_info.schedule_obj
    buf_str = str(cache_rw_info['scope'])

    src_stage_info = stages_info[cache_rw_info["tensor_ori_index"]]
    src_at_info = src_stage_info['at_info']
    src_stage_info['type'].append('src_cache_read')

    rw_at_info = AtInfo(src_at_info.index)
    rw_stage_types = ["CacheRead"]
    for consumer in src_at_info.consumers:
        if consumer.index in cache_rw_info['consumers']:
            consumer_info = copy.deepcopy(consumer)
            rw_at_info.add_consumer(consumer_info)

    if cache_rw_info.get('align_pad', False):
        rw_stage_types.append('align_pad')
        src_stage_info['type'].append('src_align_pad')

    rw_stage_index = list(sch.stages).index(sch[rw_tensor])
    rw_stage_info = {
        'name': cache_rw_info['rw_name'],
        'type': rw_stage_types,
        'scope': buf_str,
        'at_info': rw_at_info,
        'ori_name': src_stage_info.get('ori_name'),
        'tag': src_stage_info.get('tag'),
        "rw_tensor": rw_tensor
    }

    stages_info.insert(rw_stage_index, rw_stage_info)


def do_cache_read(progress, cache_rw_info):
    """
    do cache read and generate the stage info
    :param progress:
    :param cache_rw_info:
    :return:
    """
    if cache_rw_info["type"] != 'CacheRead':
        logger.warn("Unknown action:%s", cache_rw_info["type"])
        return [""]

    sch = progress.op_schedule_info.schedule_obj
    stage_index_map = progress.op_schedule_info.stage_index_map
    rw_tensor = None

    stage_names = (stage_info["name"] for stage_info in progress.op_schedule_info.stages_info)
    tensor_ori_index = list(stage_names).index(cache_rw_info['name'])
    cache_rw_info["tensor_ori_index"] = tensor_ori_index

    out_name_list = []
    consumer_stage_idx_list = []
    real_fanouts = []
    for fanout_tensor in cache_rw_info['fanout_tensors']:
        fanout_index = list(sch.stages).index(sch[fanout_tensor])
        real_fanouts.append(fanout_tensor)
        out_name_list.append(progress.op_schedule_info.stages_info[fanout_index]["name"])
        consumer_stage_idx_list.append(fanout_index)

    code_line = "%s = sch.cache_read(%s, '%s', [%s])" % (
        cache_rw_info['rw_name'], cache_rw_info['name'], str(cache_rw_info['scope']), ",".join(out_name_list))

    rw_tensor = sch.cache_read(cache_rw_info['tensor'], cache_rw_info['scope'], real_fanouts)

    # Generate the cheque
    cheque = get_cache_read_cheque(tensor_ori_index, cache_rw_info['scope'], consumer_stage_idx_list)
    progress.op_schedule_info.cheque_list.append(cheque)

    # update stage_index_map
    for ori_index, cur_index in stage_index_map.items():
        if cur_index > tensor_ori_index:
            stage_index_map[ori_index] = cur_index + 1

    update_stage_info(progress, cache_rw_info, rw_tensor)

    progress.op_schedule_info.code_lines.append(code_line)

    return [code_line]


@apply_action_register([ComputePattern.ELEMENTWISE,
                        ComputePattern.REDUCE,
                        ComputePattern.TUPLE_REDUCE,
                        ComputePattern.NORM,
                        ComputePattern.TRANSPOSE,
                        ComputePattern.POOLING],
                       ScheduleActionType.CACHE_READ)
def apply(progress):
    """
    cache read apply
    :param progress:
    :return:
    """
    stage_index = progress.todo.stage_index

    sch = progress.op_schedule_info.schedule_obj
    stages_info = progress.op_schedule_info.stages_info
    stage_index_map = progress.op_schedule_info.stage_index_map

    if not stage_index_map:
        for i in range(len(sch.stages)):
            stage_index_map[i] = i

    tensor_list = []
    all_tensors = []
    for stage in sch.stages:
        tmp_tensor_list = []
        for idx in range(stage.op.num_outputs):
            tmp_tensor_list.append(stage.origin_op.output(idx))
            all_tensors.append(stage.origin_op.output(idx))
        tensor_list.append(tmp_tensor_list)

    curr_tensor = sch.stages[stage_index].origin_op.output(0)

    cache_read_infos = gen_cache_read_info(curr_tensor, all_tensors, stages_info[stage_index], stage_index_map)
    for cache_read_info in cache_read_infos:
        do_cache_read(progress, cache_read_info)

    logger.debug("apply cache read done.")
