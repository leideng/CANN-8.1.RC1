#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
common apply action
"""
import math
import copy
import importlib
from typing import NoReturn
from functools import reduce as functools_reduce

from auto_search.utils import util
from auto_search.utils import logger
from auto_search.solution_space.features import sch_to_stmt
from auto_search.solution_space.features import FeatureExtraction
from auto_search.solution_space.tensor_cfg import FeatureTensorCfg
from auto_search.bank.cheque_generator import get_set_buffer_size_cheque
from auto_search.solution_space.t2c_util import MODE_RUNTIME
from auto_search.solution_space.action import apply_action_register
from auto_search.solution_space.action import ScheduleActionType
from auto_search.compute_analysis import ComputePattern
from auto_search.solution_space.op_schedule_info import OpScheduleInfo
from auto_search.utils.util import DTYPE_BYTE_MAPPING
from auto_search.solution_space.progress import Progress

DTYPE_PAD_BUFF_LIMIT_SIZE_MAPPING = {
    'int8': 1024,
    'float16': 256,
    'float32': 128
}
V_TRANSPOSE_MIN_CALCULATE_DATA = 512


def _do_set_buffer_size(op_schedule_info: OpScheduleInfo, need_set_tensors_dict: dict, tensor_name: str,
                        code_lines: list, buffer_size: int) -> NoReturn:
    """
    do set_buffer_size
    :param op_schedule_info:
    :param need_set_tensors_dict:
    :param tensor_name:
    :param code_lines:
    :param buffer_size:
    :return:
    """
    for stage_index, need_set_stage in need_set_tensors_dict.items():
        stage_op = op_schedule_info.schedule_obj.stages[stage_index].op
        stage_name = stage_op.name
        # tuple_reduce_sum stage output num is 2, one stage have 2 tensor ,tensor name is different use _v0 or _v1
        # they allocate are same
        if stage_op.num_outputs > 1:
            stage_name = stage_name + '.v0'
        if tensor_name == stage_name:
            buffer_size = _update_buffer_size_for_pad(buffer_size, op_schedule_info, stage_index, need_set_tensors_dict)
            buffer_size = max(buffer_size, op_schedule_info.stages_info[stage_index].get('storage_bound', 0))

            # 1.generate code
            code_lines.append('sch[%s].set_buffer_size(%s)' % (need_set_stage, buffer_size))

            # 2.do set_buffer_size
            if op_schedule_info.mode == MODE_RUNTIME:
                stage = op_schedule_info.schedule_obj.stages[stage_index]
                stage.set_buffer_size(buffer_size)

            # 3.generate cheque
            cheque = get_set_buffer_size_cheque(stage_index, buffer_size)
            op_schedule_info.cheque_list.append(cheque)
        elif f"{tensor_name}.repl" == stage_name:
            # 1.generate code
            code_lines.append('sch[%s].set_buffer_size(%s)' % (need_set_stage, buffer_size))

            # 2.do set_buffer_size
            if op_schedule_info.mode == MODE_RUNTIME:
                stage = op_schedule_info.schedule_obj.stages[stage_index]
                stage.set_buffer_size(buffer_size)

            # 3.generate cheque
            cheque = get_set_buffer_size_cheque(stage_index, buffer_size)
            op_schedule_info.cheque_list.append(cheque)


def _update_buffer_size_for_pad(buffer_size: int, op_schedule_info: OpScheduleInfo, stage_index: int,
                                need_set_tensors_dict: dict) -> int:
    """
    :param buffer_size:
    :param op_schedule_info:
    :param stage_index:
    :return:
    """
    op_intrin_key_index = op_schedule_info.op_intrin_key_index
    intrinsic_func_name = op_intrin_key_index[op_schedule_info.feature_tensor[stage_index][
        FeatureTensorCfg.compute_s]].intrin
    # Whether to adopt ub_transpose reduce
    trans_reduce_tag = op_schedule_info.is_ub_transpose and \
                       intrinsic_func_name in ("vector_reduce_max", "vector_reduce_min", "vector_reduce_sum")
    # Whether to adopt vnchwconv broadcast
    vnch_broadcast_tag = op_schedule_info.is_ub_transpose and intrinsic_func_name == 'vector_broadcast'
    if 'align_pad' in op_schedule_info.stages_info[stage_index].get('type', []) or \
            'remove_pad' in op_schedule_info.stages_info[stage_index].get('type', []) or \
            trans_reduce_tag or vnch_broadcast_tag:
        # if ub_transpose reduce, as least ask for 256 * before_reduce_shape[-1] space
        if trans_reduce_tag:
            tensor = op_schedule_info.schedule_obj.stages[stage_index].op.input_tensors[0]
        else:
            tensor = op_schedule_info.schedule_obj.stages[stage_index].op.output(0)
        tensor_shape = util.shape_to_list(tensor.shape)
        align_byte = util.BLOCK_SIZE // DTYPE_BYTE_MAPPING.get(tensor.dtype)
        limit_buffer_size = math.ceil(tensor_shape[-1] / align_byte) * align_byte * \
                            DTYPE_PAD_BUFF_LIMIT_SIZE_MAPPING.get(tensor.dtype)
        if buffer_size < limit_buffer_size:
            buffer_size = limit_buffer_size
    # Whether ub_transpose broadcast input
    if stage_index + 1 in need_set_tensors_dict:
        next_intrinsic_func_name = op_intrin_key_index[op_schedule_info.feature_tensor[stage_index + 1][
            FeatureTensorCfg.compute_s]].intrin
        if next_intrinsic_func_name == 'vector_broadcast':
            #if ub_transpose broadcast, broadcast input need 512 byte align
            tensor = op_schedule_info.schedule_obj.stages[stage_index].op.output(0)
            align_byte = V_TRANSPOSE_MIN_CALCULATE_DATA // DTYPE_BYTE_MAPPING.get(tensor.dtype)
            buffer_size = math.ceil(buffer_size / align_byte) * align_byte
    return buffer_size


def _gen_code_and_do_set_buffer_size(op_schedule_info: OpScheduleInfo, code_lines: list,
                                     need_set_tensors_dict: dict) -> NoReturn:
    """
    generate code and do set_buffer_size
    :param op_schedule_info:
    :param code_lines:
    :param need_set_tensors_dict:
    :return:
    """
    # lower to get IR
    sch = op_schedule_info.schedule_obj
    stmt = sch_to_stmt(copy.deepcopy(sch))
    worker = FeatureExtraction()
    worker.analyse_stmt(stmt, -1, 1)

    # get allocate info
    for (tensor_name, allocate_info) in worker.info.get('allocate').items():
        extents = allocate_info.get('extents')
        buffer_size = functools_reduce(lambda x, y: x * y, extents)

        _do_set_buffer_size(op_schedule_info, need_set_tensors_dict, tensor_name,
                            code_lines, buffer_size)


def _get_need_set_tensors_dict(op_schedule_info: OpScheduleInfo) -> dict:
    """
    :param op_schedule_info:
    :return:
    """
    # no need set_buffer_size tensors
    need_set_tensors_dict = {}
    schedule_obj = op_schedule_info.schedule_obj
    stages_info = op_schedule_info.stages_info
    for stage_index, stage in enumerate(schedule_obj.stages):
        # inlined stages need not set buffer size
        if stage_index in op_schedule_info.inlined_stages:
            continue
        # inlined stages need not set buffer size
        if (str(stage.op).startswith("placeholder")) or \
                'placeholder' in stages_info[stage_index].get('type', []):
            continue
        # gm stages need not set buffer size
        gm_type_set = {'leaf', 'inter_out', 'origin_leaf_out', 'workspace'}
        if gm_type_set.intersection(set(op_schedule_info.stages_info[stage_index].get('type', []))):
            continue

        need_set_tensors_dict[stage_index] = stages_info[stage_index].get('name', [])
    return need_set_tensors_dict


@apply_action_register(
    [ComputePattern.ELEMENTWISE, ComputePattern.BROADCAST, ComputePattern.REDUCE,
     ComputePattern.TRANSPOSE, ComputePattern.POOLING, ComputePattern.TUPLE_REDUCE],
    ScheduleActionType.SET_BUFFER_SIZE)
def apply(progress: Progress) -> NoReturn:
    """
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    dynamic_flag = op_schedule_info.option.get('op_config').get('op_mode', '') in ['dynamic']

    code_lines = ['\n', '# set_buffer_size code']
    op_schedule_info.proc_flag_dict = {}

    need_set_tensors_dict = _get_need_set_tensors_dict(op_schedule_info)

    # static
    if not dynamic_flag:
        # reduce ub_transpose need not set buffer size
        if op_schedule_info.is_ub_transpose:
            return
        _gen_code_and_do_set_buffer_size(op_schedule_info, code_lines, need_set_tensors_dict)
        op_schedule_info.code_lines.extend(code_lines)
        op_schedule_info.set_buffer_size = True
        return

    op_schedule_info.code_lines.extend(code_lines)
    logger.debug("apply set buffer size done.")
    return


@apply_action_register([ComputePattern.NORM], ScheduleActionType.SET_BUFFER_SIZE)
def apply(progress: Progress) -> NoReturn:
    """
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    dynamic_flag = op_schedule_info.option.get('op_config').get('op_mode', '') in ['dynamic']

    code_lines = ['\n', '# set_buffer_size code']
    op_schedule_info.proc_flag_dict = {}

    need_set_tensors_dict = _get_need_set_tensors_dict(op_schedule_info)

    # static
    if not dynamic_flag:
        _gen_code_and_do_set_buffer_size(op_schedule_info, code_lines, need_set_tensors_dict)
        op_schedule_info.code_lines.extend(code_lines)
        op_schedule_info.set_buffer_size = True
        return

    op_schedule_info.code_lines.extend(code_lines)
    logger.debug("apply set buffer size done.")
    return
