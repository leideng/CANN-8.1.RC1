#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
rl schedule search, tss
"""
from itertools import combinations
from collections import defaultdict
from io import StringIO
from contextlib import redirect_stdout
from contextlib import redirect_stderr
from typing import NoReturn
from operator import floordiv
from operator import mod
import numpy as np

from tbe.common.buildcfg import build_config
from tbe import tvm
from tbe.tvm import tir as ir_pass
from tbe.tvm.tir import Stmt
from tbe.common.platform import platform_info

from auto_search.utils import logger
from auto_search.config import soc_cfg
from auto_search.solution_space.tvm_compute import DTYPE_INDEX
from auto_search.solution_space.tensor_cfg import AXIS_CNT
from auto_search.config.cce_intrin_map import OP_INTRIN_KEY_TAG


UB_BUFFER_SIZE = soc_cfg.get_ub_size() // 2  # divide 2 for double buffer
SEARCH_N = AXIS_CNT

ITERVAR_NUM = 250  # max num of iter_var

ITERVAR_TODO_LEN = 5  # len of iter_var to do

ITERVAR_SHAPE_LEN = AXIS_CNT * 6  # len of iter_var shape, six inputs are supported by default

FEATURE_CHANNELS = 32  # channel num of feature bitwise conversion

FEATURE_MAX_STAGE_NUM = ITERVAR_NUM  # upper bound of stage. It should less than max iter_var, now they are same

# column for Data/Stage. One column one Data
FEATURE_DATA_COLUMN_NUM = (FEATURE_MAX_STAGE_NUM - 1) // FEATURE_CHANNELS + 1

# len of feature's part
FEATURE_LOOP_INFO_LEN = 6
FEATURE_COMPUTE_INFO_LEN = 6
FEATURE_AXIS_INFO_LEN = AXIS_CNT * 3  # 3 for Extents, Axis, Reduce_Axis
FEATURE_DATA_INFO_LEN = FEATURE_DATA_COLUMN_NUM * 4  # 4 for write, read, allocate, pragma

# len of iter_var feature.
# Shape/Todo feature should after iter_var feature, followed by other feature
ITERVAR_ATTR_FEATURE_LEN = FEATURE_LOOP_INFO_LEN
# len of one iter_var feature
ITERVAR_FEATURE_LEN = \
    FEATURE_LOOP_INFO_LEN + \
    FEATURE_COMPUTE_INFO_LEN + \
    FEATURE_AXIS_INFO_LEN + \
    FEATURE_DATA_INFO_LEN

FEATURE_LEN = ITERVAR_FEATURE_LEN + ITERVAR_TODO_LEN + ITERVAR_SHAPE_LEN


def extract_read_name(expr):  # pylint: disable=R0911,R0912
    """
    get var name from tvm.expr(expression)
    check for info of assign
    """
    expr_type = str(type(expr))[21:-2]
    binaryop = {
        "Add", "And", "Div", "EQ", "EqualOp", "FloorDiv", "FloorMod", "GE",
        "GT", "LE", "LT", "Max", "Min", "Mod", "Mul", "NE", "NotEqualOp", "Or",
        "Sub"
    }
    ignore = {"FloatImm", "IntImm", "StringImm", "UIntImm", "Var"}
    if expr_type in ignore:
        return set()

    ret = set()
    if expr_type == "Load":
        ret1 = {expr.buffer_var.name}
        ret2 = extract_read_name(expr.index)
        ret = ret1 | ret2
    elif expr_type == "Call":
        ret1 = set()
        for arg in expr.args:
            ret2 = extract_read_name(arg)
            ret1 = ret1 | ret2
        ret = ret1
    elif expr_type == "Cast":
        ret = extract_read_name(expr.value)
    elif expr_type in binaryop:
        ret1 = extract_read_name(expr.a)
        ret2 = extract_read_name(expr.b)
        ret = ret1 | ret2
    elif expr_type == "Not":
        ret = extract_read_name(expr.a)
    elif expr_type == "Select":
        ret0 = extract_read_name(expr.condition)
        ret1 = extract_read_name(expr.true_value)
        ret2 = extract_read_name(expr.false_value)
        ret = ret0 | ret1 | ret2
    else:
        logger.warn("Error extract_read_name Unknown tvm.expr type: %s",
                 expr_type)
    return ret


def get_extent_value(extent):
    """
    """
    # int
    if isinstance(extent, tvm.expr.IntImm):
        return extent.value
    # like (max(((i1.outer*1024) + 1023), 36547) + 1)，
    # hard to calculate，return 0
    return 0


def else_case_empty_check(stmt: Stmt) -> NoReturn:
    """
    else_case empty check
    :param stmt:
    :return:
    """
    if stmt.else_case is not None:
        # Now check index in Tensor with If clause
        # namely Else case is None
        logger.debug("Error FEC.analyse_stmt tvm.stmt.IfThenElse "
                "Else case not empty")


class FeatureExtraction:  # pylint: disable=R0902
    """
    get info from tvm.stmt, extract features.
    usage:
    0. new a class
    1. call analyse_sch_stmt, extract info from schedule and stmt
    2. return features with build_feas
    """
    def __init__(self):
        # iter info, iter_id :EmptyStructure
        self.for_stmt = {}
        self.for_var_name_list = []
        self.for_var_name_with_dtype_list = []
        # extract temp info from analyse_stmt
        self.storage_scope = {}  # key:iter var name  val:store position
        self.double_buffer = {}  # key:iter var name  val:1 for double_buffer
        # key:iter var name（not find corresponding For）  val: to be assined attr
        self.waiting_attr = {}
        # var name set: not find corresponding nextFor of Allocate
        self.waiting_allocate = set()
        # extract temp info from extract_sch_placeholder
        self.stage_name = []  # list：the order of stage
        # assign clause info, innermost For layer id: EmptyStructure
        # data info
        # key:data naem  val:EmptyStructure，include axis and reduce_axis
        # key:data naem  val:EmptyStructure，include extents, dtype and nextFor
        # key:placeholder name val:EmptyStructure，include axis,dtype
        self.info = {'assign': {},
                     'data': {},
                     'shape': {},
                     'allocate': {},
                     'placeholder': {}}

        # for calculating expression
        self.analyzer = tvm.arith.Analyzer()

    def _analyse_non_thread_ext_stmt(self,  # pylint: disable=R0912,R0915,R0914
                                     stmt):
        '''
        _analyse_non_thread_ext_stmt
        :param stmt:
        :return:
        '''
        # storage_scope store position
        if stmt.attr_key == "storage_scope":
            self.storage_scope[stmt.node.name] = stmt.value.value
        # double_buffer_scope store double_buffer
        elif stmt.attr_key == "double_buffer_scope":
            self.double_buffer[stmt.node.name] = 1
        # put known instruction into waiting_attr(key:iter var name)
        elif stmt.attr_key == "pragma_emit_insn" or \
                stmt.attr_key == "pragma_k_outer" or \
                stmt.attr_key == "pragma_mad_pattern":
            self.waiting_attr.setdefault(stmt.node.var.name, [])
            if isinstance(stmt.value,
                          (tvm.expr.StringImm, tvm.expr.IntImm)):
                val = stmt.value.value
                self.waiting_attr[stmt.node.var.name].append(
                    [stmt.attr_key, val])
            elif isinstance(stmt.value, tvm.Var):
                val = stmt.value.name
                self.waiting_attr[stmt.node.var.name].append(
                    [stmt.attr_key, val])
            else:
                logger.debug(
                    "Error FEC.analyse_stmt tvm.stmt.AttrStmt "
                    "Unknown value type: %s", str(type(stmt.value)))
        else:
            logger.debug(
                "Error FEC.analyse_stmt tvm.stmt.AttrStmt "
                "Unknown attr_key: %s", str(stmt.attr_key))

    def _analyse_thread_ext_stmt(self, stmt, outer_for, depth):
        '''
        _analyse_thread_ext_stmt
        :param stmt:
        :return:
        '''
        empty_stru = defaultdict(lambda: None)
        if isinstance(stmt, tvm.stmt.For):
            empty_stru['var'] = stmt.loop_var.name
            empty_stru['extent'] = get_extent_value(stmt.extent)
            empty_stru['dtype'] = stmt.loop_var.dtype

            min_val = stmt.min.value
            self.analyzer.update(stmt.loop_var, tvm.arith.ConstIntBound(min_val, empty_stru['extent']))
        else:
            empty_stru['var'] = stmt.node.var.name
            empty_stru['extent'] = stmt.value.value
            empty_stru['dtype'] = stmt.node.var.dtype
            self.analyzer.update(stmt.node.var, tvm.arith.ConstIntBound(0, empty_stru['extent']))
        # idx, outer_for idx, For depth
        idx = len(self.for_stmt)
        empty_stru['idx'] = idx
        empty_stru['outer_for'] = outer_for
        empty_stru['depth'] = depth

        if empty_stru['var'] in self.waiting_attr:
            empty_stru['attr'] = self.waiting_attr[empty_stru['var']]
            del self.waiting_attr[empty_stru['var']]
        else:
            empty_stru['attr'] = []

        self.for_stmt[idx] = empty_stru
        self.for_var_name_with_dtype_list.append("{}: {}".format(empty_stru['var'], empty_stru['dtype']))
        self.for_var_name_list.append("{}".format(empty_stru['var']))

        for name in self.waiting_allocate:
            self.info['allocate'][name]['nextFor'] = idx
        self.waiting_allocate = set()
        return idx

    def _analyse_prod_consm_stmt(self, stmt):
        """
        _analyse_prod_consm_stmt
        :param stmt:
        :return:
        """
        if isinstance(stmt.node, tvm.ComputeOp):
            name = stmt.node.name
            shape = defaultdict(lambda: None)
            shape['axis'] = []
            for axis in stmt.node.axis:
                shape['axis'].append(axis.dom.extent.value)
            shape['reduce_axis'] = []
            for axis in stmt.node.reduce_axis:
                shape['reduce_axis'].append(axis.dom.extent.value)
            if name in self.info['shape']:
                logger.debug(
                    "Error FEC.analyse_stmt tvm.stmt.ProducerConsumer "
                    "shape of %s has existed", name)
            else:
                self.info['shape'][name] = shape

    def _analyse_store_stmt(self, stmt, outer_for):
        '''
        _analyse_store_stmt
        :param stmt:
        :param outer_for:
        :return:
        '''
        store = defaultdict(lambda: None)
        write_name = stmt.buffer_var.name
        store['write_name'] = write_name
        store['read_name'] = extract_read_name(stmt.value)
        store['outer_for'] = outer_for
        if outer_for in self.info['assign']:
            # Now only one assign clause in innermost layer for
            # The key of self.assign include iter_id of innermost layer
            # If there are more than two assign clauses in innermost layer, then covered successively
            logger.debug("Error FEC.analyse_stmt tvm.stmt.Store "
                    "Multi Store after a For")
            return
        self.info['assign'][outer_for] = store

    def _get_extent_value(self, extent):
        """
        fetch the value of extent
        """
        # int
        if isinstance(extent, tvm.expr.IntImm):
            return extent.value
        # expression evaluation, such as(max(((i1.outer*1024) + 1023), 36547) + 1)
        try:
            calc_extent = str(extent)
            self.for_var_name_list.append("blockIdx.x")
            self.for_var_name_with_dtype_list.append("blockIdx.x: int32")
            var_name = list(set(self.for_var_name_list))
            var_name_dtype = list(set(self.for_var_name_with_dtype_list))
            var_name.sort(key=len, reverse=True)
            var_name_dtype.sort(key=len, reverse=True)
            for value in var_name_dtype:
                calc_extent = calc_extent.replace(value, "0")
            for value in var_name:
                calc_extent = calc_extent.replace(value, "0")
            calc_extent = calc_extent.replace("floormod", "mod")
            max_value = eval(calc_extent)
        except Exception as excep:
            logger.warn(str(excep))
            bd1 = self.analyzer.const_int_bound(extent)
            max_value = bd1.max_value
        return max_value

    def _analyse_allocate_stmt(self, stmt, outer_for):
        '''
        _analyse_allocate_stmt
        :param stmt:
        :param outer_for:
        :return:
        '''
        allocate = defaultdict(lambda: None)
        name = stmt.buffer_var.name
        allocate['outer_for'] = outer_for
        allocate['dtype'] = stmt.dtype
        allocate['extents'] = []
        for ex in stmt.extents:
            allocate['extents'].append(self._get_extent_value(ex))
        if name in self.info['allocate']:
            from functools import reduce as functools_reduce
            extents_new = allocate['extents']
            buffer_size_new = functools_reduce(lambda x, y: x * y, extents_new)
            extents = self.info.get('allocate').get(name).get('extents')
            buffer_size = functools_reduce(lambda x, y: x * y, extents)
            if buffer_size_new > buffer_size:
                self.info['allocate'][name] = allocate
        else:
            self.info['allocate'][name] = allocate
            self.waiting_allocate.add(name)

    def analyse_stmt(self, stmt, outer_for, depth):  # pylint: disable=R0912
        """
        outer_for: outer For id
        depth: For depth
        """
        is_attr_stmt = isinstance(stmt, tvm.stmt.AttrStmt)
        is_thread_ext = (getattr(stmt, 'attr_key', '') == "thread_extent")
        # tvm.stmt.AttrStmt assign some attr to node
        # (except thread_extent, thread_extent belong to For clause）
        non_thread_ext_stmt_flag = is_attr_stmt and not is_thread_ext
        thread_ext_stmt_flag = isinstance(stmt, tvm.stmt.For) or (is_attr_stmt and is_thread_ext)
        if non_thread_ext_stmt_flag:
            self._analyse_non_thread_ext_stmt(stmt)
            self.analyse_stmt(stmt.body, outer_for, depth)
        # tvm.stmt.For clause (include thread_extent)
        elif thread_ext_stmt_flag:
            idx = self._analyse_thread_ext_stmt(stmt, outer_for, depth)
            # outer_for,depth can changed only here
            self.analyse_stmt(stmt.body, idx, depth + 1)
        # tvm.stmt.ProducerConsumer no input stage
        # except output Stage，other stage first Producer, then Consumer
        # extract shape（axis,reduce_axis）info, first save into self.shape
        # then save into self.data in process_data function
        elif isinstance(stmt, tvm.stmt.AttrStmt) and getattr(stmt, 'attr_key', '') == "is_producer":
            self._analyse_prod_consm_stmt(stmt)
            self.analyse_stmt(stmt.body, outer_for, depth)
        # tvm.stmt.Block: Around Block all have one Stmt, recurse separately
        elif isinstance(stmt, tvm.stmt.Block):
            self.analyse_stmt(stmt.first, outer_for, depth)
            self.analyse_stmt(stmt.rest, outer_for, depth)
        elif isinstance(stmt, tvm.stmt.SeqStmt):
            for i in range(len(stmt.seq)):
                self.analyse_stmt(stmt.seq[i], outer_for, depth)
        elif isinstance(stmt, tvm.stmt.Store):
            self._analyse_store_stmt(stmt, outer_for)
        # tvm.stmt.Allocate allocate memory, get dtype, extents
        # first save into self.allocate，then save int self.data in process_data function
        elif isinstance(stmt, tvm.stmt.Allocate):
            self._analyse_allocate_stmt(stmt, outer_for)
            self.analyse_stmt(stmt.body, outer_for, depth)
        elif isinstance(stmt, tvm.stmt.IfThenElse):
            else_case_empty_check(stmt)
            self.analyse_stmt(stmt.then_case, outer_for, depth)
        elif isinstance(stmt, tvm.stmt.SeqStmt):
            for stmt_tmp in stmt.seq:
                self.analyse_stmt(stmt_tmp, outer_for, depth)

    def extract_sch_placeholder(self, sch):
        """
        extract name, axis, dtype info and order of each stage from placeholder with tvm.schedule
        """
        for stage in sch.stages:
            if not isinstance(stage.origin_op, tvm.PlaceholderOp):
                continue
            empty_stru = defaultdict(lambda: None)
            name = stage.origin_op.name
            empty_stru['name'] = name
            empty_stru['axis'] = []
            for shape in stage.origin_op.shape:
                empty_stru['axis'].append(shape.value)
            empty_stru['dtype'] = stage.origin_op.dtype
            self.info['placeholder'][name] = empty_stru

        for stage in sch.stages:
            self.stage_name.append(stage.origin_op.name)

    def _process_data_get_name(self):
        '''
        extract Data info
        :return:
        '''
        data = self.info['data']
        for store in self.info['assign'].values():
            names = {store['write_name']}
            for read_name in store['read_name']:
                names.add(read_name)
            for name in names:
                if name not in data:
                    empty_stru = defaultdict(lambda: None)
                    empty_stru['name'] = name
                    empty_stru['write'] = -1
                    data[name] = empty_stru
            # save id of outer For
            outer_for = store['outer_for']
            data[store['write_name']]['write'] = outer_for

    def _parse_info_from_allocate(self: object, name: str, data_obj: defaultdict) -> defaultdict:
        """
        parse memory info from allocate
        """
        if name in self.info.get('allocate'):
            buff = self.info.get('allocate').get(name)
            data_obj['allocate_outer_for'] = buff['outer_for']
            data_obj['allocate_nextFor'] = buff['nextFor']
            data_obj['dtype'] = buff['dtype']
            data_obj['extents'] = buff['extents']
        else:
            # some data has not allocated memory(mainly input data，placeholder)
            data_obj['allocate_outer_for'] = -1
            data_obj['allocate_nextFor'] = -1
            data_obj['dtype'] = "unknown"
            data_obj['extents'] = [-1]
        return data_obj

    def _process_data_fill_info(self):  # pylint: disable=R0912
        """
        _process_data_fill_info
        :return:
        """
        data = self.info['data']
        for name, data_obj in data.items():
            data_obj = self._parse_info_from_allocate(name, data_obj)
            # shape info
            if name in self.info['shape']:
                shape_obj = self.info['shape'][name]
                data_obj['axis'] = shape_obj['axis']
                data_obj['reduce_axis'] = shape_obj['reduce_axis']
            else:
                data_obj['axis'] = [-1]
                data_obj['reduce_axis'] = [-1]
            # double_buffer and storage_scope info
            if name in self.double_buffer:
                data_obj['double_buffer'] = 1
            else:
                data_obj['double_buffer'] = 0
            if name in self.storage_scope:
                data_obj['storage_scope'] = self.storage_scope[name]
            else:
                data_obj['storage_scope'] = "global"

        for name, plhd in self.info['placeholder'].items():
            if (name in data) and (data[name]['dtype'] == "unknown"):
                data[name]['axis'] = plhd['axis']
                data[name]['dtype'] = plhd['dtype']
                data[name]['extents'] = plhd['axis']
                data[name]['reduce_axis'] = []

    def _process_data_index_data(self):
        """
        :return:
        """
        data = self.info['data']
        for name, data_obj in data.items():
            data_obj['idx'] = -1
        data_id = 0
        for i in range(len(self.stage_name)):
            name = self.stage_name[i]
            if name in data:
                data[name]['idx'] = data_id
                data_id += 1
        # number in order in rest of data, but it should not has reset data
        for name, data_obj in data.items():
            if data_obj['idx'] == -1:
                logger.debug(
                    "Error FEC.process_dataAssign: data: %s "
                    "not exist in stage", name)
                data[name]['idx'] = data_id
                data_id += 1

    def process_data(self):  # pylint: disable=R0912
        """
        process data:
        1、extract data name from assign clause
        2、pefect info from allocate, shape, double_buffer, storage_scope, placeholder
        3、number data according to stage_name
        """
        self._process_data_get_name()

        self._process_data_fill_info()

        self._process_data_index_data()

    def _get_assign_info(self: object, idx: int) -> (int, list, list):
        """
        :param: cur_id
        """
        ids = []  # iter_id
        cur_id = idx
        cur_attr = []
        # search insn from the inside out
        while cur_id != -1:
            ids.append(cur_id)
            found = False
            # has insn
            for attr in self.for_stmt.get(cur_id).get('attr'):
                if attr[0] == "pragma_emit_insn":
                    cur_attr = attr
                    found = True
                    break
            if found:
                break
            # search outer for
            cur_id = self.for_stmt.get(cur_id).get('outer_for')
        return cur_id, cur_attr, ids

    def process_assign(self):  # pylint: disable=R0912
        """
        process assign clause 
        confirm insn
        """
        for idx, assign_obj in self.info['assign'].items():
            cur_id, cur_attr, ids = self._get_assign_info(idx)
            # if not found insn, set no_pragma_assign，continue
            if cur_id == -1:
                assign_obj['p_id'] = -1
                assign_obj['p_ids'] = []
                assign_obj['p_fist'] = 'no_pragma_assign'
                assign_obj['p_rest'] = 'no_pragma_assign'
                continue

            assign_obj['p_id'] = cur_id
            assign_obj['p_ids'] = ids  # iter_id
            if cur_attr[1] == 'dma_copy':
                # if it is dma_copy, get input/output storage scope from data
                assign_obj['p_fist'] = 'dma_copy'
                wbuffer = self.info.get('data').get(assign_obj.get('write_name')).get('storage_scope')
                rset = set(assign_obj.get('read_name'))
                read_name = rset.pop()
                rbuffer = self.info.get('data').get(read_name).get('storage_scope')
                tps = (rbuffer, wbuffer)
                assign_obj['p_rest'] = tps
            else:
                # save insn
                assign_obj['p_fist'] = 'pragma_emit_insn'
                assign_obj['p_rest'] = cur_attr[1]

    def _build_feas_init(self):
        """
        check IterVar whether it's num exceed the upper limit.
        If exceed, log warning, save it with larger array, then cut out 
        """
        if len(self.for_stmt) > ITERVAR_NUM:
            itervar_num_too_large = True
            logger.debug("Number of IterVar ( %s ) > ITERVAR_NUM ( %s )",
                    len(self.for_stmt), ITERVAR_NUM)
            feas = np.zeros([len(self.for_stmt), ITERVAR_FEATURE_LEN],
                            dtype="int32")
        else:
            itervar_num_too_large = False
            feas = np.zeros([ITERVAR_NUM, ITERVAR_FEATURE_LEN],
                            dtype="int32")
        return feas, itervar_num_too_large

    def _build_feas_loop_info(self, feas, part_begin):
        # Loop Info
        for idx, for_obj in self.for_stmt.items():
            # length
            feas[idx, part_begin + 0] = for_obj['extent']
            # nest_level
            feas[idx, part_begin + 1] = for_obj['depth']
            # isBlockIdx
            if for_obj['var'] == "blockIdx.x":
                feas[idx, part_begin + 2] = 1
            # isFinalFor & topdown
            feas[idx, part_begin + 3] = 1
            topdown = for_obj['extent']
            outer_id = for_obj['outer_for']
            while outer_id != -1:
                feas[outer_id, part_begin + 3] = 0
                topdown *= self.for_stmt[outer_id]['extent']
                outer_id = self.for_stmt[outer_id]['outer_for']
            bits = pow(2, FEATURE_CHANNELS)
            feas[idx, part_begin + 4] = topdown % bits
            feas[idx, part_begin + 5] = topdown // bits
        return feas

    def _build_feas_axis_info(self, feas, part_begin):  # pylint: disable=R0912
        '''
        Axis features: Extents, Axis, Reduce_Axis
        '''
        for idx, assign_obj in self.info['assign'].items():
            extents = self.info['data'][assign_obj['write_name']]['extents']
            axis = self.info['data'][assign_obj['write_name']]['axis']
            reduce_axis \
                = self.info['data'][assign_obj['write_name']]['reduce_axis']
            for i, extent in enumerate(extents):
                if extent < 0:
                    continue
                feas[idx, part_begin + i] = extent
            for i, axis_len in enumerate(axis):
                if axis_len < 0:
                    continue
                feas[idx, part_begin + AXIS_CNT + i] = axis_len
            for i, aixs_len in enumerate(reduce_axis):
                if aixs_len < 0:
                    continue
                feas[idx, part_begin + 2 * AXIS_CNT + i] = aixs_len
        return feas

    def _build_feas_data_info(self, feas, part_begin):  # pylint: disable=R0914
        '''
        data features Data Access Info (write, read, allocate, pragma, )
        '''
        da_bits = np.zeros([len(self.for_stmt), FEATURE_DATA_INFO_LEN],
                           np.uint32)
        for assign_f_id, assign_obj in self.info['assign'].items():
            # write
            write_assign_obj = self.info['data'][assign_obj['write_name']]
            w_c = write_assign_obj['idx'] // FEATURE_CHANNELS
            w_b = write_assign_obj['idx'] % FEATURE_CHANNELS
            da_bits[assign_f_id][w_c] |= pow(2, w_b)
            # read
            for read_name in assign_obj['read_name']:
                r_c = self.info['data'][read_name]['idx'] // FEATURE_CHANNELS
                r_b = self.info['data'][read_name]['idx'] % FEATURE_CHANNELS
                da_bits[assign_f_id][FEATURE_DATA_COLUMN_NUM + r_c] \
                    |= pow(2, r_b)

            allocate_f_id = write_assign_obj['allocate_nextFor']
            da_bits[allocate_f_id][2 * FEATURE_DATA_COLUMN_NUM + w_c] \
                |= pow(2, w_b)
            for pragma_f_id in assign_obj['p_ids']:
                da_bits[pragma_f_id][3 * FEATURE_DATA_COLUMN_NUM + w_c] \
                    |= pow(2, w_b)
        for j in range(len(self.for_stmt)):
            for i in range(FEATURE_DATA_INFO_LEN):
                feas[j, part_begin + i] = da_bits[j][i]
        return feas


def ana_lower(sch,  # pylint: disable=no-member
              args,
              binds=None):
    """
    Do lower while keeping all axes in IR
    i.e. Do not eliminate loop with extent of 1,
    do not vectorize, unroll or inject virtual threads
    """
    # keep it silent because tvm may spit out strange log
    f_out = StringIO()
    f_err = StringIO()
    with redirect_stderr(f_err), redirect_stdout(f_out):
        binds, out_args = tvm.driver.build_module.get_binds(args, binds=binds)
        sch = sch.normalize()
        # Phase 0
        bounds = tvm.te.schedule.InferBound(sch)  # pylint: disable=E1101
        stmt = tvm.te.schedule.ScheduleOps(sch, bounds, True)
        func = tvm.te.schedule.SchedulePostProcToPrimFunc(out_args, stmt, binds)
        module = tvm.IRModule.from_expr(func)
        module = tvm.tir.transform.StorageFlatten(64)(module)
        module = tvm.tir.transform.CanonicalSimplify()(module)
        stmt = module["main"].body
    return stmt


def sch_to_stmt(sch):
    """
    convert schedule into stmt
    :param sch: schedule
    :return: stmt
    """
    # get input output tensor
    args_tensors = []
    for stage in sch.stages:
        # only has leaf out no mid out
        if str(stage.op).startswith('placeholder') or stage.is_output:
            for idx in range(stage.op.num_outputs):
                args_tensors.append(stage.op.output(idx))
    # get ana_lower
    try:
        with build_config():
            stmt = ana_lower(sch, args_tensors)
    except BaseException as exception:
        logger.debug("can not get stmt, %s",  str(exception))
    return stmt
