#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
rl schedule search, tss
"""
import pickle
import numpy as np

import tbe
from tbe import tvm

from auto_search.utils import util
from auto_search.utils import logger
from auto_search.solution_space.tensor_cfg import FeatureTensorCfg
from auto_search.solution_space.tensor_cfg import AXIS_CNT
from auto_search.config.cce_intrin_map import OP_INTRIN_KEY_TAG


VIRTUAL_LEAF_OUT_TAG = "elewise_empty_intrin"
INPUT_TENSOR_GEN = "input_tensor_gen.py"
SCHEDULE_DUMP = "schedule.dump"
REAL_AXIS_DUMP = "real_axis.dump"
DTYPE_INDEX = {'int8': 1,
               'uint8': 2,
               'int16': 3,
               'uint16': 4,
               'int32': 5,
               'uint32': 6,
               'int64': 7,
               'uint64': 8,
               'float16': 9,
               'bfloat16': 10,
               'float32': 11,
               'float64': 12,
               'bool': 13,
               'uint1': 14,
               'unknown': 0}


def _gen_depends_tensor(schedule_obj, input_tensor):
    """
    :param schedule_obj:
    :param input_tensor:
    :return:
    """
    dict_depend, dict_depended = util.get_depends(schedule_obj)
    # 8 bits per depend，16 depend ,so at most 16*8=128 stages
    # the num of no-zero depend is divide 8 to stage plus 1
    stage_len = len(schedule_obj.stages)
    inner_iter_num = stage_len // 8 + 1

    for i in range(stage_len):
        dict_depend[i].sort()
        if i in dict_depended:
            dict_depended[i].sort()
        for j in range(inner_iter_num):
            depend_value = 0
            depended_value = 0
            for depend in dict_depend[i]:
                if j * 8 <= depend < (j + 1) * 8:
                    depend_value = 1 << (depend % 8) | depend_value

            if i in dict_depended:
                for depended in dict_depended[i]:
                    if j * 8 <= depended < (j + 1) * 8:
                        depended_value = 1 << (depended % 8) | depended_value

            deps_int = depend_value | depended_value
            input_tensor[i][FeatureTensorCfg.deps_e - j] = deps_int


def _gen_axis_tensor(schedule_obj, input_tensor):
    """
    :param schedule_obj:
    :param input_tensor:
    :return:
    """
    for stage_idx, stage in enumerate(schedule_obj.stages):
        if isinstance(stage.op, tvm.PlaceholderOp):
            for i, axis in enumerate(stage.op.shape):
                input_tensor[stage_idx][i] = axis
        else:
            for i, axis in enumerate(stage.op.axis):
                input_tensor[stage_idx][i] = axis.dom.extent.value

            for i, reduce_axis in enumerate(stage.op.reduce_axis):
                input_tensor[stage_idx][AXIS_CNT + i] = reduce_axis.dom.extent.value


def _get_empty_tag_idx(stages_info, stage_index):
    """
    :param stages_info:
    :param stage:
    :param stage_index:
    :param c_op:
    :return:
    """
    key = stages_info[stage_index].get("op_tag", "mem_copy")
    op_index = OP_INTRIN_KEY_TAG.get(key).op_index
    return op_index


def _get_local_stage_idx(stage, reduce_type):
    """
    :param stage:
    :param reduce_type:
    :return:
    """
    key = stage.op.tag.split("|")[0]
    if reduce_type and \
            not key.endswith(reduce_type) and \
            key not in OP_INTRIN_KEY_TAG:
        key = '%s_%s' % (key, reduce_type)
    op_index = OP_INTRIN_KEY_TAG.get(key).op_index
    return op_index


def _get_stage_op_index(stage_index, stages_info, sch, op_name):
    """
    :param stage_index:
    :param stages_info:
    :param sch:
    :param c_op:
    :param op_name:
    :return:
    """
    stage = sch.stages[stage_index]
    reduce_type = stages_info[stage_index].get('reduce_type', '')
    op_index = 0
    # The element in cond_func_list is cond, func, parameter
    cond_func_list = [
        [
            isinstance(stage.op, tvm.PlaceholderOp),  # placeholder
            int,
            (0,)
        ],
        [
            stage.op.tag == '',  # tag is ''
            _get_empty_tag_idx,
            (stages_info, stage_index)
        ],
        [
            stage_index == len(sch.stages) - 1,  # last stage is mem_copy
            int,
            (0,)
        ],
        [
            '.local.' in stage.op.name,  # .local stage
            _get_local_stage_idx,
            (stage, reduce_type)
        ]
    ]

    for cond, func, params in cond_func_list:
        if cond:
            op_index = func(*params)
            break

    return op_index


def _get_op_tensor(sch, stage_index, stages_info, op_name, input_tensor):
    """
    :param sch:
    :param stage_index:
    :param stages_info:
    :param op_name:
    :param c_op:
    :param input_tensor:
    :return:
    """
    # tag of matmul at last stage，need remove to reduce stage
    # record tag into stages_info for special processing,
    # the value of tag is assigned to mem_copy when there is not has tag attr
    stage = sch.stages[stage_index]
    if not stages_info[stage_index].get('tag'):
        stages_info[stage_index]['tag'] = \
            stage.op.tag if stage.op.tag else 'mem_copy'
    op_index = _get_stage_op_index(stage_index, stages_info, sch, op_name)
    input_tensor[stage_index][FeatureTensorCfg.compute_s] = op_index


def _get_reduce_axis_dict(stage, stage_index, stages_info, reduce_axis_dict):
    """
    :param stage:
    :param stage_index:
    :param stages_info:
    :param reduce_axis_dict:
    :return:
    """
    if isinstance(stage.op, tvm.PlaceholderOp) or not stage.op.reduce_axis:
        return

    reduce_axis_list = util.get_reduce_axis_index(stage.op)
    keep_dim = len(stage.op.input_tensors[0].shape) == len(
        stage.op.output(0).shape)
    reduce_axis_dict[stage_index] = {
        "axis": reduce_axis_list,
        "keep_dim": keep_dim,
        "stage_name": stage.op.name,
        "bc_followed": {}, }

    stage_type = stages_info[stage_index].setdefault('type', [])

    if 'reduce_atomic' not in stage_type:
        stages_info[stage_index]['type'].append('reduce_gm')

    if not stages_info[stage_index].get('reduce_type', ''):
        if len(stage.op.input_tensors[0].shape) - 1 in reduce_axis_list:
            reduce_type = 'last'
        else:
            reduce_type = 'nist'
        stages_info[stage_index]['reduce_type'] = reduce_type
        reduce_axis_dict[stage_index]["type"] = reduce_type


def _gen_compute_tensor(sch, stages_info, op_name, input_tensor):
    """
    get stage compute info
    :param sch:
    :param stages_info:
    :param op_name:
    :param input_tensor:
    :return:
    """
    reduce_axis_dict = {}
    for stage_index, stage in enumerate(sch.stages):
        _get_reduce_axis_dict(stage, stage_index, stages_info, reduce_axis_dict)
        _get_op_tensor(sch, stage_index, stages_info, op_name, input_tensor)
    return reduce_axis_dict


def _parse_stages(sch, inter_out_names):
    """
    parse feature with sch
    """
    stages_info = [{
        'name': stage.op.name,
        'tag': stage.op.tag,
        'ori_name': stage.op.name,
        'type': []

    } for stage in sch.stages]

    # Stage dependency
    dict_fanin, dict_fanout = util.get_depends(sch)

    # mid output Stage and leaf Stage
    inter_out_stage_indices = []
    leaf_stage_indices = []
    for i, stage in enumerate(sch.stages):
        if stage.op.name in inter_out_names:
            inter_out_stage_indices.append(i)
        if not dict_fanout[i]:
            leaf_stage_indices.append(i)

    # all leaf recurse sub tree
    leaf_subtree = {}
    for leaf_stage_index in leaf_stage_indices:
        stages_info[leaf_stage_index].setdefault('type', []).append('leaf')
        leaf_subtree[leaf_stage_index] = []
        util.get_sub_tree(leaf_stage_index, dict_fanin,
                          leaf_subtree.get(leaf_stage_index))
        # Sort the data in reverse order to match the topology.
        leaf_subtree.get(leaf_stage_index).sort(reverse=True)

    for index in inter_out_stage_indices:
        stages_info[index].setdefault('type', []).append('inter_out')

    tensors = [stage.op.output(0) for stage in sch.stages]
    for index, tensor in enumerate(tensors):
        if str(tensor.op).startswith('placeholder'):
            stages_info[index].setdefault('type', []).append('placeholder')

    return stages_info


def _gen_virtual_leaf_out(leaf_outs):
    """
    :param leaf_outs:
    :return:
    """
    virtual_out_shape = _get_virtual_out_shape(leaf_outs)
    virtual_out = _creat_virtual_leaf_out(leaf_outs, virtual_out_shape)
    return virtual_out


def _get_virtual_out_shape(leaf_outs):
    """
    if origin leaf output shape are all the same, virtual out shape is equal any out's shape.
    if origin leaf output shape are not same, virtual out shape is equal max input shape
    :param leaf_outs:
    :return:
    """
    # if origin leaf output shape are all the same, virtual out shape is equal any out's shape
    shape_set = set()
    for leaf_out in leaf_outs:
        shape_set.add(str(leaf_out.shape))
    if len(shape_set) == 1:
        return [x.value for x in leaf_outs[0].shape]

    # get all inputs shape
    input_shapes = []
    tensors = leaf_outs[:]
    all_tensors = leaf_outs[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if isinstance(tensor.op, tvm.PlaceholderOp):
                input_shapes.append([int(axis) for axis in tensor.shape])
                continue
            new_tensors.extend(tensor.op.input_tensors)
        tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(tensors)

    # get final virtual_out shape: contain all input axis
    virtual_out_shape = input_shapes[0]
    for input_shape in input_shapes[1:]:
        if len(input_shape) > len(virtual_out_shape):
            virtual_out_shape = input_shape
        elif len(input_shape) == len(virtual_out_shape):
            # get max value for each dim
            for i, curr_value in enumerate(input_shape):
                virtual_out_shape[i] = max(curr_value, virtual_out_shape[i])
    return virtual_out_shape


def _gen_virtual_leaf_out_by_broadcast(leaf_outs, virtual_out_shape):
    # fro keepdim is True
    def _get_broadcast_leaf_out(virtual_out_shape, index_list, leaf_out):
        broadcast_leaf_out = tvm.compute(
            virtual_out_shape,
            lambda *index: leaf_out(
                *[x for i, x in enumerate(index) if i in index_list]),
            name='%s_broadcast' % leaf_out.name,
            tag='broadcast_for_tensor')
        return broadcast_leaf_out

    # broadcast leaf_outs to virtual_out
    broadcast_leaf_outs = []
    for leaf_out in leaf_outs:
        leaf_out_shape = [int(axis_len) for axis_len in leaf_out.shape]
        if leaf_out_shape == virtual_out_shape:
            broadcast_leaf_outs.append(leaf_out)
            continue
        if len(leaf_out_shape) == len(virtual_out_shape):
            # keepdim is true
            index_list = list(range(len(leaf_out_shape)))
        else:
            # keepdim is false
            index_list = []
            index_dict = {}
            for axis_len in leaf_out_shape:
                index_dict.setdefault(axis_len, -1)
                index = virtual_out_shape.index(axis_len,
                                                index_dict.get(axis_len) + 1)
                index_dict[axis_len] = index
                index_list.append(index)
        broadcast_leaf_out = _get_broadcast_leaf_out(
            virtual_out_shape, index_list, leaf_out)
        broadcast_leaf_outs.append(broadcast_leaf_out)

    # add broadcasted leaf_outs together for virtual_out
    virtual_out = broadcast_leaf_outs[0]
    for broadcast_leaf_out in broadcast_leaf_outs[1:]:
        virtual_out = tbe.dsl.vadd(virtual_out, broadcast_leaf_out)
    return virtual_out


def _creat_virtual_leaf_out(leaf_outs, virtual_out_shape):
    """
    :param leaf_outs:
    :param virtual_out_shape:
    :return:
    """
    # keepdim when is false is not support
    tensors = leaf_outs[:]
    all_tensors = leaf_outs[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if len(tensor.shape) != len(virtual_out_shape):
                logger.info("gen virtual_leaf_out by broadcast.")
                # try broadcast + add
                return _gen_virtual_leaf_out_by_broadcast(
                    leaf_outs, virtual_out_shape)
            if not isinstance(tensor.op, tvm.PlaceholderOp):
                new_tensors.extend(tensor.op.input_tensors)
        tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(tensors)
    logger.debug("gen virtual_leaf_out not by broadcast.")

    def phony_insn_fuse(*indice):
        """
        :param indice:
        :return:
        """
        dtype = leaf_outs[0].dtype
        virtual_out = tvm.const(1, dtype)
        for leaf_out in leaf_outs:
            cur_index = []
            for i, virtual_out_shape_i in enumerate(virtual_out_shape):
                if leaf_out.shape[i].value == virtual_out_shape_i:
                    cur_index.append(indice[i])
                else:
                    cur_index.append(indice[i] % leaf_out.shape[i].value)
            virtual_out *= tvm.expr.Cast(dtype, leaf_out(*cur_index))
        return virtual_out

    with tvm.tag_scope(VIRTUAL_LEAF_OUT_TAG):
        virtual_out = tvm.compute(virtual_out_shape,
                                  phony_insn_fuse,
                                  name="virtual_leaf_out")
    return virtual_out


def _get_tensors_code(op_outputs, indent, output_op_names, op_list, output_tensor_names):
    """
    :param op_outputs:
    :param indent:
    :param output_op_names:
    :param op_list:
    :param output_tensor_names:
    :return:
    """
    sch = tvm.create_schedule(op_list)
    sch_dump = pickle.dumps(sch, protocol=2)
    base_compute_code = """
    #op_outputs:%s
    import pickle
    sch = pickle.loads(%s)
    """ % (",".join(op_outputs), sch_dump)
    # get output from sch
    code_lines = [
        base_compute_code,
        "%s%s, = sch.outputs" % (indent, ", ".join(output_op_names))
    ]
    output_tensor_name_idx = 0
    for op_idx, stage_op in enumerate(op_list):
        op_name = output_op_names[op_idx]
        for output_idx in range(stage_op.num_outputs):
            code_lines.append(
                '%s%s = %s.output(%d)' %
                (indent, output_tensor_names[output_tensor_name_idx], op_name,
                 output_idx))
            output_tensor_name_idx += 1
        # tuple_reduce_sum add not contain '_v'
        if stage_op.num_outputs > 1:
            code_lines.append('%s%s = %s' %
                              (indent, stage_op.name,
                               output_tensor_names[output_tensor_name_idx - stage_op.num_outputs]))
    return code_lines


def _mark_stage_type(has_virtual_leaf_out, ori_leaf_outs, leaf_outs, sch, stages_info):
    """
    if has virtual_leaf_out,
    origin out stage type append “origin_leaf_out”,
    virtual_out stage type append “virtual_leaf_out”,
    :param has_virtual_leaf_out:
    :param ori_leaf_outs:
    :param leaf_outs:
    :param sch:
    :param stages_info:
    :return:
    """
    if has_virtual_leaf_out:
        ori_leaf_out_names = [x.name for x in ori_leaf_outs]
        leaf_out_names = [x.name for x in leaf_outs]
        for stage_index, stage in enumerate(sch.stages):
            if stage.op.output(0).name in ori_leaf_out_names:
                stages_info[stage_index].setdefault('type', []). \
                    append('origin_leaf_out')
            if stage.op.output(0).name in leaf_out_names:
                stages_info[stage_index].setdefault('type', []). \
                    append('virtual_leaf_out')


def gen_compute_code(output_tensors, add_virtual_leaf_out=True):
    """
    :param output_tensors:
    :param add_virtual_leaf_out:
    :return:
    """
    # output_tensors type need be a list
    if not isinstance(output_tensors, list):
        output_tensors = [output_tensors]

    op_outputs = [stage.op.name for stage in output_tensors]

    # the indentation of each line of code
    indent = '    '
    logger.debug("output_tensors: %s", output_tensors)
    leaf_outs, inter_outs = util.classify_outs(output_tensors)
    ori_leaf_outs = leaf_outs[:]

    is_tuple_reduce_sum = False
    for leaf_out in leaf_outs:
        if 'tuple_reduce_sum' in leaf_out.op.tag:
            is_tuple_reduce_sum = True
    has_virtual_leaf_out = False
    if add_virtual_leaf_out and len(leaf_outs) > 1 and not is_tuple_reduce_sum:
        has_virtual_leaf_out = True
        virtual_leaf_out = _gen_virtual_leaf_out(leaf_outs)
        output_tensors = list(set(output_tensors) - set(leaf_outs))
        output_tensors.append(virtual_leaf_out)
        leaf_outs = [virtual_leaf_out]

    op_list = []
    for leaf_out in leaf_outs:
        if leaf_out.op not in op_list:
            op_list.append(leaf_out.op)
    output_op_names = [op.name + '_op' for op in op_list]
    output_tensor_names = []
    for stage_op in op_list:
        if stage_op.num_outputs > 1:
            for idx in range(stage_op.num_outputs):
                output_tensor_names.append(stage_op.name + "_v%s" % idx)
        else:
            output_tensor_names.append(stage_op.name)

    code_lines = _get_tensors_code(op_outputs, indent, output_op_names, op_list, output_tensor_names)

    # recursive traversal from output tensor, restore compute code
    def rec_def_tensor(tensors, code_lines, visited_tensors):
        """
        :param tensors:
        :param code_lines:
        :param visited_tensors:
        :return:
        """
        new_tensors = []
        for tensor in tensors:
            for i, input_tensor in enumerate(tensor.op.input_tensors):
                input_name = input_tensor.op.name
                code_line = "%s%s = %s.op.input_tensors[%s]" % (
                    indent, input_name, tensor.op.name, i)
                if input_name in visited_tensors:
                    continue
                code_lines.append(code_line)
                visited_tensors.append(input_name)
                new_tensors.append(input_tensor)
        if new_tensors:
            rec_def_tensor(new_tensors, code_lines, visited_tensors)

    visited_tensors = []
    rec_def_tensor(leaf_outs, code_lines, visited_tensors)
    # add create_schedule code
    sch_code_line = indent + "sch = tvm.create_schedule([%s])" % (
        ', '.join(output_op_names))
    code_lines.append(sch_code_line)
    compute_code = "\n".join(code_lines)
    # create_schedule
    sch = tvm.create_schedule(op_list)
    inter_out_names = [tensor.op.name for tensor in inter_outs]
    stages_info = _parse_stages(sch, inter_out_names)
    # add stage type for virtual_leaf_out
    _mark_stage_type(has_virtual_leaf_out, ori_leaf_outs, leaf_outs, sch, stages_info)
    logger.debug("compute_code: %s", compute_code)
    return sch, compute_code, stages_info


def proc(schedule_obj, stages_info, op_name):
    """
    :param schedule_obj:
    :param stages_info:
    :param op_name:
    :return:
    """
    stage_num = len(schedule_obj.stages)
    # get init feature_tensor
    feature_tensor = np.zeros([stage_num, FeatureTensorCfg.featurn_len], dtype=np.int32)
    _gen_axis_tensor(schedule_obj, feature_tensor)
    reduce_axis_dict = _gen_compute_tensor(
        schedule_obj, stages_info, op_name, feature_tensor)
    _gen_depends_tensor(schedule_obj, feature_tensor)
    return feature_tensor, reduce_axis_dict
