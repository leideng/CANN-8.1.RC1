#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
util
"""
import copy
import datetime
import errno
import os
import stat
import shutil
import subprocess
import sys
from collections import namedtuple
from collections import defaultdict

from auto_search.config import config
from auto_search.config import soc_cfg
from auto_search.utils import logger
from tbe import tvm

BLOCK_SIZE = 32
HASH_LEN = 32
FAIL_SAFE = 500000
OPEN_FILE_FLAGS = os.O_WRONLY | os.O_CREAT
WRITE_FILE_FLAGS = os.O_WRONLY | os.O_CREAT | os.O_TRUNC
OPEN_FILE_MODES_640 = stat.S_IWUSR | stat.S_IRUSR | stat.S_IRGRP
MAX_TIMEOUT = 2000000

DTYPE_BYTE_MAPPING = {
    "uint1": 0.125,
    "bool": 1,
    "int8": 1,
    "uint8": 1,
    "float16": 2,
    "bfloat16": 2,
    "int16": 2,
    "uint16": 2,
    "float32": 4,
    "int32": 4,
    "uint32": 4,
    "int64": 8,
    "uint64": 8,
}

DTYPE_REPEAT_MAPPING = {
    "float16": 128,
    "float32": 64
}

DYNC_COMPUTE_INFO = None

LOG_PARAMS = namedtuple("LogPrint", ["cmd", "quiet", "ret", "ret_str", "output", "print_output"])
SchInfo = namedtuple(
        'SchInfo',
        ['sch', 'code', 'cleaned_actions', 'retry_t2c_rules', 'cheque_list'])

KernelInfo = namedtuple('KernelInfo', [
        'error_code', 'kernel_run_obj', 'sch_info'])

OP_IMPLEMENT_IMPORT_HEADER = '''\
# -*- coding: UTF-8 -*-
import copy
import tbe
from tbe import tvm
from tbe.common.platform.platform_info import set_current_compile_soc_info
from tbe.common.buildcfg import build_config, set_current_build_config, dynamic_build_config_dict

dynamic_build_config = copy.deepcopy(dynamic_build_config_dict)
{set_product}
'''
OP_IMPLEMENT_FUNC_HEADER = '''\
def op_cce({api_def_args}, kernel_name="{kernel_name}", 
           need_build=False, need_print=False):
'''

OP_IMPLEMENT_TAIL = '''
    config = dict()
    config["print_ir"] = need_print
    config["need_build"] = need_build,
    config["name"] = kernel_name
    config["tensor_list"] = {tensor_list}
    config["bool_storage_as_1bit"] = False
    with build_config(kernel_meta_parent_dir=kmp_dir):
        tbe.dsl.build(sch, config)
'''

OP_DYNAMIC_COMPILE_TAIL = '''
    dynamic_build_config["double_buffer_non_reuse"] = True
    dynamic_build_config["save_temp_cce_file"] = True
    with build_config(**dynamic_build_config):
        tvm.build(sch, 
                  {tensor_list}, 
                  target="cce", 
                  name=kernel_name, )
'''

OP_FUNC_CALL = '''
op_cce({api_run_args}, kernel_name="{kernel_name}", 
       need_build={need_build}, need_print={need_print})
'''


class ErrorCode:
    """
    ErrorCode
    """
    BUILD_TIMEOUT = "build_timeout"
    LOWER_TIMEOUT = "lower_timeout"
    BUILD_OOM = "build_oom"
    BUILD_NOALIGN = "build_noalign"
    BUILD_FAIL = "build_fail"
    BUILD_SUCC_ = "build_succ"
    RUN_SUCC = 'run_succ'
    PROFILING_FAIL = "profiling_fail"
    AICORE_ERROR = "aicore_error"
    RUN_CRASH = 'run_crash'
    QUEUE_TIMEOUT = "queue_timeout"
    RUN_FAIL = 'run_fail'
    ENV_EXCEPTION = "env_exception"
    RUN_TIMEOUT = "run_timeout"
    RUN_EXCEPTION = "run_exception"
    LOCK_TIMEOUT = "lock_timeout"

    # RESULT_FAIL has sequence
    EVB_RUN_ERROR_LIST_ = \
        [("AICORE ERROR occur, ", AICORE_ERROR),
         ('exec timeout', QUEUE_TIMEOUT),
         ('memcmp output fail', RUN_FAIL),
         ('host log not found!', ENV_EXCEPTION),
         (' Terminated', RUN_TIMEOUT),
         ('memcmp output succ', RUN_EXCEPTION),
         ('get lock timeout', LOCK_TIMEOUT)
         ]

    def useless_func1(self):
        """
        useless_func1
        :return:
        """
        return self.BUILD_OOM

    def useless_func2(self):
        """
        useless_func2
        :return:
        """
        return self.RUN_EXCEPTION


def _get_print_log(cmd: str, ret_str: str) -> str:
    """
    get_print_log for run_cmd_comm
    """
    if "sshpass -p " in cmd:
        print_log = "run cmd: sshpass -p ... %s!" % ret_str
    else:
        print_log = "run cmd: %s %s!" % (cmd, ret_str)
    return print_log


def _print_log_proc(log_params: object) -> None:
    """
    print_log_proc for run_cmd_comm
    """
    if log_params.quiet:
        return
    print_log = ""
    if not log_params.ret or log_params.print_output:
        print_log = _get_print_log(log_params.cmd, log_params.ret_str)
    if log_params.print_output:
        print_log += "output: " + log_params.output
    if print_log:
        if log_params.ret:
            logger.debug("%s", print_log)
        else:
            logger.info("%s", print_log)


def get_depends(sch):
    """
    :param sch:
    :return:
    """
    tensors = {}
    for i, stage in enumerate(sch.stages):
        for idx in range(stage.op.num_outputs):
            tensors[stage.op.output(idx)] = i
    dict_inputs = defaultdict(list)
    dict_outputs = defaultdict(list)
    for i, stage in enumerate(sch.stages):
        for input_tensor in stage.op.input_tensors:
            input_index = tensors.get(input_tensor, 0)
            dict_inputs[i].append(input_index)
            dict_outputs[input_index].append(i)

    return dict_inputs, dict_outputs


def get_sub_tree(stage_index, next_dict, sub_tree_indices):
    """
    :param stage_index:
    :param next_dict:
    :param sub_tree_indices:
    """
    for fwd_index in next_dict[stage_index]:
        if fwd_index not in sub_tree_indices:
            sub_tree_indices.append(fwd_index)
            get_sub_tree(fwd_index, next_dict, sub_tree_indices)


def run_cmd_comm(cmd: str, timeout: int = 1800, print_output: bool = False,
                 shell: bool = True, quiet: bool = False) -> tuple:
    """
    when shell is True, execution time may timeout, use caution.
    when timeout will raise TimeoutExpired
    :param cmd:
    :param timeout:
    :param print_output:
    :param shell:
    :param quiet:
    :return:
    """
    print_log = _get_print_log(cmd, "")
    logger.debug("begin to %s", print_log)

    if not shell:
        cmd = cmd.split(" ")

    try:
        timeout = float(timeout)
    except (ValueError, TypeError):
        timeout = None
    finally:
        pass
    if sys.version_info >= (3, 5):
        try:
            out_bytes = subprocess.check_output(cmd, stderr=subprocess.PIPE, shell=shell, timeout=timeout)
        except subprocess.CalledProcessError as proc_err:
            stderr_str = proc_err.stderr.decode('utf-8', errors="ignore")
            output_str = proc_err.output.decode('utf-8', errors="ignore")
            output_str += stderr_str
            log_params = LOG_PARAMS(cmd, quiet, False, "fail", output_str, print_output)
            _print_log_proc(log_params)
            return False, output_str
        except subprocess.TimeoutExpired:
            log_params = LOG_PARAMS(cmd, quiet, False, "timeout", "timeout:%d" % timeout, print_output)
            _print_log_proc(log_params)
            return False, 'timeout'
        finally:
            pass
    else:
        return False, 'unsupported python version'
    log_params = LOG_PARAMS(cmd, quiet, True, "succ", out_bytes.decode('utf-8', errors="ignore"), print_output)
    _print_log_proc(log_params)

    return True, out_bytes.decode('utf-8', errors="ignore")


def create_dir(dir_path):
    """
    :param dir_path:
    :return:
    """
    dir_path = os.path.realpath(dir_path)
    is_exists = os.path.exists(dir_path)
    if not is_exists:
        try:
            os.makedirs(dir_path)
        except (OSError, TypeError) as exception:
            logger.warn('Can not create %s, exception: %s', dir_path, str(exception))
        finally:
            pass


def write_to_file(file_path, content=""):
    """
    :param file_path:
    :param content:
    :return:
    """
    file_path = os.path.realpath(file_path)
    dir_name = os.path.dirname(file_path)
    create_dir(dir_name)
    if not os.path.isdir(dir_name):
        logger.warn('can not create dir: %s.', dir_name)
        return False

    with os.fdopen(os.open(file_path, WRITE_FILE_FLAGS, OPEN_FILE_MODES_640), "w") as file_handler:
        file_handler.write(content)

    return True


def cp_src_to_dst(src_path, dst_path, overwriting=True, mode=None):
    """
    :param src_path:
    :param dst_path:
    :param overwriting:
    :param mode:
    :return:
    """
    if overwriting is False:
        if os.path.exists(dst_path):
            return True

    if not os.path.exists(src_path):
        logger.warn('src: %s not exist!', src_path)
        return False

    dst_dir = os.path.dirname(dst_path)
    create_dir(dst_dir)
    if not os.path.isdir(dst_dir):
        logger.warn('can not create %s.', dst_dir)
        return False

    shutil.copy(src_path, dst_path)
    if mode is not None:
        os.chmod(dst_path, mode)
    return True


def _validate_option_value(option_value: object, option_yaml_dict: dict, option_key: str, value_type: object) -> object:
    """
    :param option_value:
    :param option_yaml_dict:
    :param option_key:
    :param value_type:
    :return:
    """
    # if the option_value's type is not equal to config_value's type, need convert
    if not isinstance(option_yaml_dict[option_key], value_type):
        old_option_value = option_value
        try:
            option_value = value_type(old_option_value)
        except (ValueError, TypeError) as exception:
            logger.warn("format is not supported, type of %s must be %s.", option_key, value_type)
            raise RuntimeError("format fail! please check it!") from exception
        finally:
            pass
    return option_value


def init_option(option):
    """
    option priority:
    1、user input parameters;
    2、config.py OPTION_CONFIG
    """
    option_yaml_dict = {}
    option_yaml_dict.update(option)

    # if key is not in option from input parameters, use config.OPTION_CONFIG default value
    config_option = copy.deepcopy(config.OPTION_CONFIG)

    for option_key in option_yaml_dict:
        option_value = option_yaml_dict[option_key]
        if option_key in config_option:
            value_type = type(config_option[option_key])
            option_value = _validate_option_value(option_value, option_yaml_dict, option_key, value_type)
        config_option[option_key] = option_value

    # check error_tolerance
    if "error_tolerance" in config_option:
        error_tolerance = config_option["error_tolerance"]
        if error_tolerance < 0 or error_tolerance > 1:
            raise RuntimeError("error_tolerance must be in the range (0,1)!")

    option = copy.deepcopy(config_option)
    # set option timeout upper limit
    option['timeout'] = min(MAX_TIMEOUT, option.get('timeout'))
    workspace = option["WORKSPACE"]
    # if workspace is not exists, creat
    create_dir(workspace)
    return option


def ensure_dir_exists(directory, reset=True):
    """
    Creates local directories if they don't exist.
    """

    if not os.path.isdir(directory):
        os.makedirs(directory, exist_ok=True)
    elif reset:
        shutil.rmtree(directory)
        os.makedirs(directory, exist_ok=True)


def get_reduce_axis_index(stage_op):
    """
    :param stage_op:
    :return:
    """
    reduce_axis_var = []
    for i in stage_op.reduce_axis:
        reduce_axis_var.append(i.var)
    source_len = len(stage_op.body[0].source)

    try:
        data_axis_var = stage_op.body[0].source[source_len - 1].indices
    except AttributeError:
        logger.debug('stage: %s, body: %s, reduce_axis_var: %s', stage_op, stage_op.body, reduce_axis_var)
        data_axis_var = []
    reduce_axis_list = []
    for axis in reduce_axis_var:
        axis_num = 0
        for i in data_axis_var:
            if isinstance(i, tvm.expr.Add) and i.b.same_as(axis):
                reduce_axis_list.append(axis_num)
            elif i.same_as(axis):
                reduce_axis_list.append(axis_num)
            axis_num += 1
    reduce_axis_list.sort()
    return reduce_axis_list


def gen_rl_schedule_key(input_info_list, output_info_list, sch_obj):
    """
    :param input_info_list:
    :param output_info_list:
    :param sch_obj:
    :return:
    """
    shape_dtype_list = []
    for tensor_info in input_info_list + output_info_list:
        shape_dtype_list.extend(
            [str(list(tensor_info.shape)), tensor_info.dtype])

    real_stages = list(sch_obj.stages)
    reduce_axis_str_list = []
    for stage in real_stages:
        if isinstance(stage.op, tvm.PlaceholderOp) or not stage.op.reduce_axis:
            continue
        reduce_axis_list = get_reduce_axis_index(stage.op)
        reduce_axis_str = "_".join([str(x) for x in reduce_axis_list])
        reduce_axis_str_list.append(reduce_axis_str)

    rl_schedule_key = "_".join(shape_dtype_list).replace(" ", "")
    rl_schedule_key = rl_schedule_key.replace(",", "_").replace("[", "").replace("]", "")
    if reduce_axis_str_list:
        rl_schedule_key += "_" + "_r_".join(reduce_axis_str_list)
    return rl_schedule_key


def update_workspace(option):
    """
    :param option:
    :return:
    """
    workspace = option.get("WORKSPACE", config.WORKSPACE)
    time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    full_tune_workspace = os.path.join(workspace, option.get("tune_workspace", "tune_workspace_" + time_stamp))
    option['full_tune_workspace'] = full_tune_workspace
    ensure_dir_exists(full_tune_workspace, reset=False)
    if not option.get("tss_workspace", None):
        option["tss_workspace"] = os.path.basename(option.get('full_tune_workspace'))


def get_dtype_size(dtype):
    """
    :param dtype:
    :return:
    """
    if dtype in ["bool", "uint1"]:
        bit_size = 8
    else:
        digit_list = list(filter(str.isdigit, dtype))
        bit_size = int("".join(digit_list))
    return bit_size // 8


def get_block_num(dtype):
    """
    :param dtype:
    :return:
    """
    # 32Byte for DMA of D
    return 32 // get_dtype_size(dtype)


def shape_to_list(tvm_shape):
    """
    :param tvm_shape:
    :return:
    """
    tmp = []
    for i in tvm_shape:
        tmp.append(i.value)
    return tmp


def pid_exists(pid):
    """
    Check whether pid exists in the current process table.
    UNIX only.
    Possible error values are EINVAL, EPERM, ESRCH
    EINVAL:invalid argument
    ESRCH:No such process
    EPERM:operation not permitted
    """
    pid = int(pid)
    if pid <= 0:
        return False
    try:
        os.kill(pid, 0)
    except OSError as exc:
        if exc.errno == errno.EPERM:
            return True
        if exc.errno == errno.ESRCH:
            return False
        raise
    else:
        return True


def get_best_sch_path(op_schedule_info: object,
                      dst_dir: str = None,
                      silent: bool = False) -> str:
    """
    :param op_schedule_info: op tune info 
    :param dst_dir: target dir
    :param silent: T/F
    :return: best_src_path
    """
    if not op_schedule_info:
        return ""

    op_name = op_schedule_info.op_name
    shape_list_str = op_schedule_info.shape_list_str
    store_dir = op_schedule_info.store_dir
    run_succ_path = os.path.join(store_dir, "run_succ")

    best_sch_name = ""
    best_src_path = ""
    if os.path.exists(run_succ_path):
        sch_list = os.listdir(run_succ_path)
        if not sch_list:
            logger.warn('can not get sch_list.')
            return ""

        sch_list.sort(key=lambda file_name: int(file_name.split("_")[0]))
        try:
            best_sch_name = sch_list[0]
        except Exception as e:
            logger.warn('can not get best sch name: %s' % e)
            return ""

    if best_sch_name:
        best_src_path = os.path.join(run_succ_path, best_sch_name)

    if dst_dir and best_src_path:
        dst_path = os.path.join(dst_dir, "best_" + best_sch_name)
        cp_src_to_dst(best_src_path, dst_path)
        if not silent:
            logger.info("%s@%s search result is %s!", op_name, shape_list_str, best_src_path)

    return best_src_path


def gen_schedule_py(op_schedule_info, schedule_code, kernel_name, op_file):
    """
    :param op_schedule_info:
    :param schedule_code:
    :param kernel_name:
    :param op_file:
    :return:
    """
    import_str = OP_IMPLEMENT_IMPORT_HEADER.format(
        set_product=soc_cfg.set_product_code())
    head_str = OP_IMPLEMENT_FUNC_HEADER.format(
        api_def_args=op_schedule_info.api_def_args, kernel_name=kernel_name)
    if op_schedule_info.option.get('op_config').get('op_mode', '') in ['static', 'dynamic']:
        tail_str = OP_DYNAMIC_COMPILE_TAIL.format(
            tensor_list=op_schedule_info.tensor_list_str)
    else:
        tail_str = OP_IMPLEMENT_TAIL.format(
            tensor_list=op_schedule_info.tensor_list_str)
    func_call_str = OP_FUNC_CALL.format(
        api_run_args=op_schedule_info.api_run_args,
        kernel_name=kernel_name,
        need_build=True,
        need_print=False)

    op_code_str = "%s%s%s%s%s%s" % (import_str, head_str,
                                    op_schedule_info.compute_code,
                                    schedule_code, tail_str, func_call_str)

    fd = os.open(op_file, WRITE_FILE_FLAGS, OPEN_FILE_MODES_640)
    with os.fdopen(fd, 'w') as file_handler:
        file_handler.write(op_code_str)

    return True


def store_tmp_proc(
        op_schedule_info,
        schedule_code,
        err_code,
        kernel_name,
        tick=None):
    """
    :param op_schedule_info:
    :param schedule_code:
    :param err_code:
    :param kernel_name:
    :param tick:
    :return:
    """
    if not schedule_code:
        return None

    store_dir = os.path.join(op_schedule_info.store_dir, err_code)
    create_dir(store_dir)
    if not tick:
        dst_file_name = "%s.py" % kernel_name
    else:
        if op_schedule_info.base_tick is None:
            base_tick = 0
        else:
            base_tick = op_schedule_info.base_tick
        dst_file_name = "%d_%d_%s.py" % (tick, base_tick, kernel_name)
        if err_code != ErrorCode.RUN_SUCC:
            dst_file_name = "%s_%s" % (err_code, dst_file_name)
    dst_file = os.path.join(store_dir, dst_file_name)
    gen_schedule_py(op_schedule_info, schedule_code, kernel_name, dst_file)
    return dst_file


def tvm_shape_trans(tvm_shape):
    """
    :param tvm_shape:
    :return:
    """
    shape_list = []
    for value in tvm_shape:
        shape_list.append(int(value))
    return shape_list


def get_tensor_list(ori_tensor_list):
    """
    :param ori_tensor_list:
    :return:list
    """
    tensor_list = []
    for ori_tensor in ori_tensor_list:
        shape_list = tvm_shape_trans(ori_tensor.shape)
        tensor_list.append({
            "shape": shape_list,
            "name": ori_tensor.name,
            "dtype": ori_tensor.dtype
        })
    return tensor_list


def get_timeout(high_perf: bool = False) -> int:
    """
    get_timeout
    :return:
    """
    # if env not set, use default_timeout
    default_timeout = config.TIMEOUT
    if high_perf:
        default_timeout = config.TIMEOUT_HIGH_PERF
    try:
        return int(os.getenv("TUNE_TIMEOUT", str(default_timeout)))
    except ValueError:
        logger.error("TUNE_TIMEOUT error, must be a number. Now use default timeout:%d", default_timeout)
        return default_timeout
    finally:
        pass


def classify_outs(outs):
    """
    classify outs into inter and leaf
    """
    operation_list = copy.deepcopy(outs)
    visited_tensor = set()
    inter_out = set()
    while operation_list:
        tmp_operation_list = []
        for current_tensor in operation_list:
            current_op = current_tensor.op
            current_name = current_op.name
            if current_name in visited_tensor:
                logger.debug("tensor:%s has been visited", current_tensor.op.name)
                continue
            visited_tensor.add(current_name)
            if isinstance(current_op, tvm.PlaceholderOp):
                continue
            for i in range(len(current_op.input_tensors)):
                input_tensor = current_op.input_tensors[i]
                input_name = input_tensor.op.name
                if input_name not in inter_out:
                    inter_out.add(input_name)
                tmp_operation_list.append(input_tensor)
        operation_list = list(set(tmp_operation_list))
    leaf_outs = []
    inter_outs = []
    for tensor in outs:
        if tensor.name in inter_out:
            inter_outs.append(tensor)
            logger.debug("inter_out: %s", tensor.name)
        else:
            leaf_outs.append(tensor)
            logger.debug("leaf_out:%s", tensor.name)
    return leaf_outs, inter_outs

