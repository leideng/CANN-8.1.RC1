#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
reduce compute graph
"""
from math import sqrt
from math import ceil
from typing import List
from typing import NoReturn
from typing import Optional
from dataclasses import make_dataclass

from tbe import tvm
from tbe.tvm import Stage
from tbe.common.platform import ASCEND_910
from tbe.common.platform import ASCEND_910B
from tbe.common.platform import ASCEND_310P
from tbe.common.platform import ASCEND_310B
from tbe.common.platform import AS31XM1
from tbe.common.platform import BS9SX1A
from tbe.common.platform.platform_info import get_soc_spec

from auto_search.config import soc_cfg
from auto_search.utils import util
from auto_search.utils import logger
from auto_search.utils.util import DTYPE_BYTE_MAPPING
from auto_search.utils.util import BLOCK_SIZE


OrderedAxisInfo = make_dataclass('OrderedAxisInfo', ['origin_idx', 'idx_by_type', 'prefix', 'dim'])
ReduceInfoForRfactor = make_dataclass('ReduceInfoForRfactor', ['is_atomic', 'is_last_reduce', 'ub_split_reduce_axis',
                                                               'ub_split_last_reduce_axis', 'split_stage_num'])
REDUCE_CASE_LENGTH_TWO = 2
REDUCE_CASE_LENGTH_THREE = 3
REDUCE_CASE_LENGTH_FOUR = 4

AXIS_CNT = 8  # at most 8 aixes per stage
SPLIT_FACTOR_S = 0
SPLIT_FACTOR_E = AXIS_CNT * 2 - 1

REDUCE_COMPUTE = {
    "reduce_min", "reduce_max", "reduce_sum",
    "reduce_prod", "tuple_reduce_sum",
}

REDUCE_EMITSN_SUPPORT_TRANSPOSE = {
    "reduce_sum", "reduce_min", "reduce_max"
}

REDUCE_SUPPORT_TRANSPOSE = {
    # AR
    2: {"0_0"},
}

REDUCE_SUPPORT_PAD = {
    # AR
    2: {"0_0": False},

    # ARA
    3: {"0_0": True, "0_1": True, "1_1": False, "1_0": False}
}

ATOMIC_SUPPORT_MAP_910 = {"support_dtype": ["float32", ],
                          "support_insn": ["reduce_sum", "tuple_reduce_sum"], }

ATOMIC_SUPPORT_MAP_910B = {"support_dtype": ["float32", "float16", "int32", "int16", "int8", "bfloat16"],
                           "support_insn": ["reduce_sum", "reduce_max", "reduce_min", "tuple_reduce_sum"]}

ATOMIC_SUPPORT_MAP_310P = {"support_dtype": ["float32", "float16"],
                           "support_insn": ["reduce_sum", "tuple_reduce_sum"]}

ATOMIC_SUPPORT_MAP_310B = {"support_dtype": ["float32", "float16"],
                           "support_insn": ["reduce_sum"]}

ATOMIC_SUPPORT_MAP_AS31XM1 = {"support_dtype": ["float32", "float16"],
                           "support_insn": ["reduce_sum"]}

ATOMIC_SUPPORT_MAP_BS9SX1A = {"support_dtype": ["float32", "float16"],
                           "support_insn": ["reduce_sum", "tuple_reduce_sum"]}

SUPPORT_ATOMIC_DICT = {ASCEND_910: ATOMIC_SUPPORT_MAP_910,
                       ASCEND_910B: ATOMIC_SUPPORT_MAP_910B,
                       ASCEND_310P: ATOMIC_SUPPORT_MAP_310P,
                       ASCEND_310B: ATOMIC_SUPPORT_MAP_310B,
                       AS31XM1: ATOMIC_SUPPORT_MAP_AS31XM1,
                       BS9SX1A: ATOMIC_SUPPORT_MAP_BS9SX1A}


def get_stage_ordered_axes_obj(op_schedule_info: object, stage_index: int) -> OrderedAxisInfo:
    """
    :param op_schedule_info:
    :param stage_index:
    :return:
    """
    stages = op_schedule_info.schedule_obj.stages
    stage_info = op_schedule_info.stages_info[stage_index]
    if 'stage_ordered_axes_obj' not in stage_info:
        stage_info['stage_ordered_axes_obj'] = StageOrderedAxes(stages, stage_index)
    return stage_info['stage_ordered_axes_obj']


def _get_reduce_info(stage: Stage) -> tuple:
    """
    get keep_dim, last_reduce, and reduce axis info
    :param stage:
    :return:
    """
    is_reduce_stage = False
    if isinstance(stage.op, tvm.ComputeOp):
        is_reduce_stage = len(stage.op.reduce_axis) > 0
    keep_dim = False
    reduce_axis_index = None
    reduce_last = False
    if is_reduce_stage:
        shape_before = stage.op.input_tensors[0].shape
        shape_after = stage.op.output(0).shape
        keep_dim = len(shape_after) == len(shape_before)
        reduce_axis_index = util.get_reduce_axis_index(stage.op)
        reduce_last = reduce_axis_index[-1] == (len(shape_before) - 1)
    return keep_dim, reduce_last, reduce_axis_index


class StageOrderedAxes:
    """
    StageOrderedAxes
    """
    def __init__(self, stages, stage_index):
        """
        :param stages:
        :param stage_index:
        """
        stage = stages[stage_index]
        self._shape_before = stage.op.input_tensors[0].shape
        self._shape_after = stage.op.output(0).shape
        self._is_reduce_stage = len(stage.op.reduce_axis) > 0
        self._gather_reduce_info(stages, stage_index)
        self._calc_ordered_axes()

    def get_ordered_axis_dim(self) -> List:
        """
        :return:
        """
        return [ordered_axis_info.dim for ordered_axis_info in self.ordered_axis_info_wrapper]

    def get_ordered_axis_origin_idx(self) -> List:
        """
        :return:
        """
        return [ordered_axis_info.origin_idx for ordered_axis_info in self.ordered_axis_info_wrapper]

    def get_origin_axis_dim(self) -> List:
        """
        :return:
        """
        return [dim.value for i, dim in enumerate(self._shape_before)]

    def get_total_size_after_split_axis(self, cut_axis: int) -> int:
        """
        :param cut_axis:
        :return: return the total size of after the split_axis_index in the ordered axes
        """
        ordered_axis_indice = self.get_ordered_axis_origin_idx()
        total_size = 1

        ordered_cut_idx = 0
        for idx, origin_idx in enumerate(ordered_axis_indice):
            if idx == cut_axis:
                ordered_cut_idx = origin_idx

        for idx in ordered_axis_indice[ordered_cut_idx + 1:]:
            total_size *= self._shape_before[idx].value

        return total_size

    def get_cut_axis_info_wrapper(self, cut_axis: int) -> int:
        """
        :param cut_axis:
        :return:
        """
        for item in self.ordered_axis_info_wrapper:
            if item.origin_idx == cut_axis:
                return item

        raise RuntimeError("can't find cut_axis!")

    def get_normal_axis_info_wrapper(self) -> List:
        """
        :return:
        """
        result = []
        for item in self.ordered_axis_info_wrapper:
            if item.prefix == '':
                result.append(item)

        if self._is_reduce_stage and self._shape_has_keep_dim:
            for i in self._reduce_axis_index:
                result.insert(i, OrderedAxisInfo(i, i, '', 1))
            # reset the idx_by_type
            for i, item in enumerate(result):
                item.idx_by_type = i

        return result

    def get_reduce_axis_info_wrapper(self) -> List:
        """
        :return:
        """
        result = []
        for item in self.ordered_axis_info_wrapper:
            if item.prefix == 'reduce_':
                result.append(item)
        return result

    def get_axis_info_wrapper_with_cut_axis(self, cut_axis: int) -> tuple:
        """
        :param cut_axis:
        :return:
        """
        cut_aiw = self.get_cut_axis_info_wrapper(cut_axis)
        a_aiw = self.get_normal_axis_info_wrapper()
        r_aiw = self.get_reduce_axis_info_wrapper()
        if cut_aiw.prefix == 'reduce_':
            r_aiw.remove(cut_aiw)
        else:
            a_aiw.remove(cut_aiw)

        return cut_aiw, a_aiw, r_aiw

    def is_reduce_stage(self) -> bool:
        """
        :return:
        """
        return self._is_reduce_stage

    def is_reduce_last(self) -> bool:
        """
        :return:
        """
        return self._is_reduce_last

    def get_reduce_axes(self) -> List:
        """
        :return:
        """
        return self._reduce_axis_index

    def get_index_within_type(self, cut_axis: int) -> int:
        """
        :param cut_axis:
        :return:
        """
        if self._is_reduce_stage and cut_axis in self._reduce_axis_index:
            return self._reduce_axis_index.index(cut_axis)
        for item in self.get_normal_axis_info_wrapper():
            if item.origin_idx == cut_axis:
                return item.idx_by_type

        raise RuntimeError("can't find cut_axis!")

    def _gather_reduce_info(self, stages: List[Stage], stage_index: int) -> NoReturn:
        """
        :param stages:
        :param stage_index:
        :return:
        """
        keep_dim = False
        reduce_axis_index = None
        reduce_last = False
        if self._is_reduce_stage:
            keep_dim, reduce_last, reduce_axis_index = _get_reduce_info(stages[stage_index])
        else:
            for stage in stages:
                keep_dim, reduce_last, reduce_axis_index = _get_reduce_info(stage)
                if keep_dim:
                    break
        self._shape_has_keep_dim = keep_dim
        self._reduce_axis_index = reduce_axis_index
        self._is_reduce_last = reduce_last

    def _calc_ordered_axes(self) -> NoReturn:
        """
        :return:
        """
        if self._is_reduce_stage:
            a_axes_dim, r_axes_dim = [], []
            a_idx, r_idx = 0, 0
            for i, d in enumerate(self._shape_before):
                if i not in self._reduce_axis_index:
                    a_axes_dim.append(OrderedAxisInfo(i, a_idx, '', d.value))
                    a_idx += 1
                else:
                    r_axes_dim.append(OrderedAxisInfo(i, r_idx, 'reduce_', d.value))
                    r_idx += 1

            if self._is_reduce_last:
                self.ordered_axis_info_wrapper = a_axes_dim + r_axes_dim
            else:
                self.ordered_axis_info_wrapper = a_axes_dim[:-1] + r_axes_dim + a_axes_dim[-1:]
        else:
            a_axes_dim = [OrderedAxisInfo(i, i, '', d.value) for i, d in enumerate(self._shape_before)]
            self.ordered_axis_info_wrapper = a_axes_dim


class Dim:
    """
    do actions for dim
    """

    BLOCK_OUTER = "block_outer"
    BLOCK_INNER = "block_inner"
    UB_OUTER = "ub_outer"
    UB_INNER = "ub_inner"
    A = "A"
    R = "R"

    def __init__(self, axis_type, idx, var_type=None):
        self.axis_type = axis_type
        self.var_type = var_type
        self.idx = idx

    @staticmethod
    def split(in_shape: List, split_idx: int, model: str = None) -> NoReturn:
        """
        do dim split
        """
        if model == "UBSplit":
            outer = Dim.UB_OUTER
            inner = Dim.UB_INNER
        else:
            outer = Dim.BLOCK_OUTER
            inner = Dim.BLOCK_INNER

        # update index
        for item in in_shape[split_idx + 1:]:
            item.idx += 1

        # insert split
        in_shape[split_idx].var_type = outer
        in_shape.insert(split_idx + 1, Dim(in_shape[split_idx].axis_type, split_idx + 1, inner))

    @staticmethod
    def group(nums: List) -> List:
        """
        group nums
        """
        nums = sorted(set(nums))
        gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s + 1 < e]
        edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])
        return list(zip(edges, edges))


class ReduceComputeGraphInfo:
    """
    ReduceComputeGraphInfo
    """
    def __init__(self, stages: List) -> NoReturn:
        self.stages = stages
        self.reduce_stage: Optional[Stage] = None
        self.reduce_stage_tag: Optional[str] = ''
        self.reduce_stage_index: Optional[int] = None
        self.reduce_tensors: List = []
        self.reduce_stage_dtype: Optional[str] = None
        self.is_last_reduce: Optional[bool] = None
        self.shape_before_reduce: Optional[List[int]] = None
        self.shape_after_reduce: Optional[List[int]] = None
        self.keep_dim: Optional[bool] = None
        self.reduce_axes_index: Optional[List[int]] = None
        self.normal_axes_index: Optional[List[int]] = None
        self.last_axis_is_align: Optional[bool] = None
        self.analyse_graph()

    def analyse_graph(self) -> NoReturn:
        """
        :return:
        """
        for stage_index, stage in enumerate(self.stages):
            if isinstance(stage.op, tvm.ComputeOp):
                if len(stage.op.reduce_axis) > 0:
                    self.reduce_stage_index = stage_index
                    self.reduce_stage = stage
                    self.reduce_stage_tag = stage.op.tag
                    self.reduce_tensors.append(stage.op.output(0))
                    break

        if self.reduce_stage is None:
            raise RuntimeError("reduce graph need have reduce node!")

        self.shape_before_reduce = [int(dim_value) for dim_value in self.reduce_stage.op.input_tensors[0].shape]
        self.shape_after_reduce = [int(dim_value) for dim_value in self.reduce_stage.op.output(0).shape]
        self.keep_dim = len(self.shape_after_reduce) == len(self.shape_before_reduce)
        self.normal_axes_index, self.reduce_axes_index = self.get_reduce_and_normal_axes_index()
        self.is_last_reduce = self.reduce_axes_index[-1] == (len(self.shape_before_reduce) - 1)

        self.reduce_stage_dtype = self.reduce_stage.op.output(0).dtype
        dtype_byte = DTYPE_BYTE_MAPPING.get(self.reduce_stage_dtype, None)
        block_size = BLOCK_SIZE // dtype_byte
        self.last_axis_is_align = self.shape_before_reduce[-1] % block_size == 0

    def get_reduce_and_normal_axes_index(self) -> List[int]:
        """
        get all reduce axes index
        :return:
        """
        stage_op = self.reduce_stage.op
        reduce_axis_var = []
        for i in stage_op.reduce_axis:
            reduce_axis_var.append(i.var)
        source_len = len(stage_op.body[0].source)
        try:
            data_axis_var = stage_op.body[0].source[source_len - 1].indices
        except AttributeError:
            logger.debug('stage: %s, body: %s, reduce_axis_var: %s', stage_op, stage_op.body, reduce_axis_var)
            data_axis_var = []
        reduce_axis_list = []
        for axis in reduce_axis_var:
            for axis_index, axis_var in enumerate(data_axis_var):
                if axis_var.same_as(axis):
                    reduce_axis_list.append(axis_index)
        reduce_axis_list.sort()
        normal_axis_list = \
            [axis_index for axis_index in range(len(data_axis_var)) if axis_index not in reduce_axis_list]
        normal_axis_list.sort()
        return normal_axis_list, reduce_axis_list


def support_atomic_search(output_tensors: List) -> bool:
    """
    judge current soc version and instruction is support atomic or not
    """
    # Common Regulation
    version = get_soc_spec("SHORT_SOC_VERSION")
    if version not in [ASCEND_910B, ASCEND_910, ASCEND_310P, ASCEND_310B, AS31XM1, BS9SX1A]:
        return False

    leaf_outs, _ = util.classify_outs(output_tensors)
    reduce_tensor = leaf_outs[0]

    # Special Regulation
    _map = SUPPORT_ATOMIC_DICT.get(version, {})
    if reduce_tensor.dtype not in _map.get("support_dtype"):
        return False
    if reduce_tensor.op.tag not in _map.get("support_insn"):
        return False

    return True


def support_atomic_ra_bind_search(op_schedule_info: object) -> bool:
    """
    judge current case is support atomic r0a0 split and bind multicore
    1.tuple_reduce pattern
    2.atomic
    3.r0a0r1a1
    4.a0 and core_num has common factor
    """
    reduce_compute_graph = op_schedule_info.compute_graph_info
    # only atomic support r0a0 bind
    if not op_schedule_info.is_atomic:
        return False
    # r0a0r1a1
    if len(reduce_compute_graph.shape_before_reduce) != REDUCE_CASE_LENGTH_FOUR:
        return False
    if list(reduce_compute_graph.reduce_axes_index) != [0, 2]:
        return False
    # try find r0_factor and a0_factor pairs which can split full core. if not, then not support
    r0_shape = reduce_compute_graph.shape_before_reduce[0]
    a0_shape = reduce_compute_graph.shape_before_reduce[1]
    candidate_factor = _calc_candidate_factor(a0_shape, r0_shape)
    if not candidate_factor:
        return False

    logger.debug("current case support atomic_ra_bind search for tuple_reduce.")
    return True


def support_ub_transpose_search(op_schedule_info: object) -> bool:
    """
    judge current case is support ub transpose or not
    :param op_schedule_info:
    :return:
    """
    reduce_compute_graph: ReduceComputeGraphInfo = op_schedule_info.compute_graph_info
    # atomic is not support ub transpose
    if op_schedule_info.is_atomic:
        return False
    # reduce axis must be one dim
    if len(reduce_compute_graph.reduce_axes_index) != 1:
        return False
    # reduce before shape dims must be 2
    if len(reduce_compute_graph.shape_before_reduce) != REDUCE_CASE_LENGTH_TWO:
        return False
    # reduce before shape[0] must bigger than 1
    if reduce_compute_graph.shape_before_reduce[0] == 1:
        return False
    # last axis must not align
    if reduce_compute_graph.last_axis_is_align:
        return False
    # reduce_prod is not support ub transpose
    if reduce_compute_graph.reduce_tensors[0].op.tag not in REDUCE_EMITSN_SUPPORT_TRANSPOSE:
        return False
    # block_cut_axis and ub_cut_axis should be an axis
    # because block_cut_axis must be an in normal template, so only need judge ub_cut_axis
    ub_cut_axis = op_schedule_info.cut_axis_index.get(reduce_compute_graph.reduce_stage_index, None)
    if ub_cut_axis != 0:
        return False
    # split stage must be cut, otherwise can not do ub transpose
    if not _is_split_reduce_stage(op_schedule_info):
        return False

    return True


def _is_split_reduce_stage(op_schedule_info):
    """
    # judge whether it's just split res stage
    :param op_schedule_info:
    :return:
    """
    stages_info_list = op_schedule_info.stages_info
    at_target_set = set()
    for stage_index, stage_info in enumerate(stages_info_list):
        at_target_set.add(stage_info['at_info'].consumers[0].sampled_target)
    if len(at_target_set) == 1:
        return False
    return True


def support_align_pad_search(progress: object) -> bool:
    """
    :param progress:
    :return:
    """
    reduce_compute_graph = progress.op_schedule_info.compute_graph_info
    # for pure move case don't need do pad
    first_dim_is_one = reduce_compute_graph.shape_before_reduce[0] == 1
    if first_dim_is_one and len(reduce_compute_graph.shape_before_reduce) == REDUCE_CASE_LENGTH_TWO:
        return False
    # only support shape is 2 dims or 3 dims
    if len(reduce_compute_graph.shape_before_reduce) < REDUCE_CASE_LENGTH_TWO \
            or len(reduce_compute_graph.shape_before_reduce) > REDUCE_CASE_LENGTH_THREE:
        return False
    #  only have one reduce axis
    if len(reduce_compute_graph.reduce_axes_index) != 1:
        return False
    # last axis must not align
    if reduce_compute_graph.last_axis_is_align:
        return False
    # atomic can not last reduce
    if progress.op_schedule_info.is_atomic and reduce_compute_graph.is_last_reduce:
        return False
    # norm reduce can not only split res
    if not progress.op_schedule_info.is_atomic and not _is_split_reduce_stage(progress.op_schedule_info):
        return False

    return True


def support_remove_pad_search(progress: object, reduce_compute_graph: ReduceComputeGraphInfo) -> bool:
    """
    :param progress:
    :param reduce_compute_graph:
    :return:
    """
    # only ARA support
    if len(reduce_compute_graph.shape_before_reduce) != REDUCE_CASE_LENGTH_THREE:
        return False
    if reduce_compute_graph.is_last_reduce:
        return False
    # atomic need not do remove pad
    if progress.op_schedule_info.is_atomic:
        return False
    return True


def get_reduce_info_for_rfactor(progress: object) -> ReduceInfoForRfactor:
    """
    :param progress:
    :return:
    """
    # split_stage_num == 1 only split res
    # split_stage_num == 2 only split res and reduce
    split_stage_num = 0

    op_schedule_info = progress.op_schedule_info
    stages = list(op_schedule_info.schedule_obj.stages)
    is_atomic = progress.op_schedule_info.is_atomic

    ub_split_stage_index = None
    if is_atomic:
        ub_split_stage_index = len(stages) - 1
        split_stage_num = 1
    else:
        for stage_index, _ in enumerate(stages):
            split_vector = progress.action_tensor[stage_index][SPLIT_FACTOR_S:SPLIT_FACTOR_E + 1]
            if sum(split_vector) != 0:
                split_stage_num = split_stage_num + 1

            if 'CacheWrite' in op_schedule_info.stages_info[stage_index].get('type', []) \
                    and 'reduce' in op_schedule_info.stages_info[stage_index].get('type', []):
                ub_split_stage_index = stage_index
        if split_stage_num == 1:
            return ReduceInfoForRfactor(False, False, False, False, 1)
    if ub_split_stage_index is None:
        raise RuntimeError("can not find reduce ub stage!")

    # find reduce ub split axis index
    ub_cut_axis = find_cut_axis_index(progress, ub_split_stage_index)

    is_last_reduce = progress.op_schedule_info.compute_graph_info.is_last_reduce
    ub_split_reduce_axis = ub_cut_axis in progress.op_schedule_info.compute_graph_info.reduce_axes_index
    ub_split_last_reduce_axis = ub_cut_axis == progress.op_schedule_info.compute_graph_info.reduce_axes_index[-1]

    return ReduceInfoForRfactor(is_atomic, is_last_reduce, ub_split_reduce_axis, ub_split_last_reduce_axis,
                                split_stage_num)


def find_cut_axis_index(progress: object, ub_split_stage_index: int) -> int:
    """
    :param progress:
    :param ub_split_stage_index:
    :return:
    """
    # find reduce ub split axis index
    ub_cut_axis = 0
    split_vector = progress.action_tensor[ub_split_stage_index][SPLIT_FACTOR_S:SPLIT_FACTOR_E + 1]
    for i, f in enumerate(split_vector):
        if f > 0:
            ub_cut_axis = i
            break

    return ub_cut_axis


def _get_factor(num):
    result = []
    for i in range(1, int(sqrt(num)) + 1):
        if num % i == 0:
            result.append((i, num // i))
    return result


def _calculate_r0_a0_factor(reduce_compute_graph, r0_split_axis_index, a0_split_axis_index):
    """
    calculate r0 factor and a0 factor
    :param progress:
    :return:
    """
    r0_shape = reduce_compute_graph.shape_before_reduce[r0_split_axis_index]
    a0_shape = reduce_compute_graph.shape_before_reduce[a0_split_axis_index]
    candidate_factor = _calc_candidate_factor(a0_shape, r0_shape)

    candidate_factor.sort(reverse=True)
    r0_factor, a0_factor = candidate_factor[0]

    return r0_factor, a0_factor


def _calc_candidate_factor(a0_shape, r0_shape):
    core_num = soc_cfg.get_core_num()
    core_num_factor = _get_factor(core_num)
    candidate_factor = []
    for item in core_num_factor:
        r0_remain, a0_remain = item
        if a0_remain == ceil(a0_shape / ceil(a0_shape / a0_remain)):
            if r0_remain == ceil(r0_shape / ceil(r0_shape / r0_remain)):
                candidate_factor.append((ceil(r0_shape / r0_remain), ceil(a0_shape / a0_remain)))

        r0_remain, a0_remain = a0_remain, r0_remain
        if a0_remain == ceil(a0_shape / ceil(a0_shape / a0_remain)):
            if r0_remain == ceil(r0_shape / ceil(r0_shape / r0_remain)):
                candidate_factor.append((ceil(r0_shape / r0_remain), ceil(a0_shape / a0_remain)))
    return candidate_factor
