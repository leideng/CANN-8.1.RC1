#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
broadcast analysis
"""
from functools import reduce as functools_reduce

from tbe import tvm

from auto_search.solution_space.comm import BLOCK_SIZE_BYTE
from auto_search.utils import util
from auto_search.utils import logger


def _is_broadcast_nist(tensor):
    """

    :param tensor:
    :return:
    """
    if tensor.op.tag.find('broadcast') != -1:
        # broadcast not last axis
        if list(tensor.op.input_tensors):
            original_tensor = tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            broadcast_shape = util.shape_to_list(tensor.shape)
            if original_shape[-1] == broadcast_shape[-1]:
                # not include (1,1,1,1,1,1,1)->(10, 10, 5, 2, 3, 9,1)
                if sum(original_shape[:]) != len(original_shape):
                    return True
    return False


def _is_broadcast_last(tensor):
    """
    support like (1,1,1,1,1,1,1)->(10, 10, 5, 2, 3, 9,1)
                 (3,1,1) -> (3,2,1)
    :param tensor:
    :return:
    """
    if tensor.op.tag.find('broadcast') != -1:
        # broadcast not last axis
        if list(tensor.op.input_tensors):
            original_tensor = tensor.op.input_tensors[0]
            original_shape = util.shape_to_list(original_tensor.shape)
            broadcast_shape = util.shape_to_list(tensor.shape)
            if original_shape[-1] == 1 and broadcast_shape[-1] != 1:
                return True

            for i in reversed(range(len(original_shape))):
                if original_shape[i] != 1 and broadcast_shape[i] != 1:
                    return False
                if original_shape[i] == 1 and broadcast_shape[i] != 1:
                    return True
    return False


def get_broadcast_tensor(schedule):
    """

    :param schedule:
    :return:
    """
    # get all broadcast tensor.
    # If it found reduce tensor, then return empty list
    broadcast_last_tensors = []
    broadcast_nist_tensors = []
    tensors = [
        schedule.outputs[0].output(idx)
        for idx in range(schedule.outputs[0].num_outputs)
    ]
    all_tensors = tensors[:]
    while tensors:
        new_tensors = []
        for tensor in tensors:
            if tensor.op.tag.find('reduce') >= 0:
                return None, None
            if tensor.op.tag.find('broadcast') >= 0:
                if _is_broadcast_last(tensor):
                    broadcast_last_tensors.append(tensor)
                elif _is_broadcast_nist(tensor):
                    broadcast_nist_tensors.append(tensor)
            if not isinstance(tensor.op, tvm.PlaceholderOp):
                new_tensors.extend(tensor.op.input_tensors)
        tensors = list(set(new_tensors) - set(all_tensors))
        all_tensors.extend(tensors)
    return broadcast_last_tensors, broadcast_nist_tensors


def can_use_optimize_schedule_for_bn_operators(progress):
    """
    optimize schedule for bn_training_reduce_grad:
    1. all small stage not compute at res
    2. broadcast stage inline, and it's consumer emit insn add attr  {"use_ba_pattern_brc": True}
    can optimize condition: last axis size is 2 or 3 or 4 block(current usr is align to judge)
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    stage_index = len(op_schedule_info.schedule_obj.stages) - 1
    stage = op_schedule_info.schedule_obj.stages[stage_index]
    op_name = op_schedule_info.option.get('op_config').get('op_name', '')
    support_op_list = ['bn_training_reduce_grad', 'bn_training_update', 'bn_training_update_grad']
    if not any(True if name in op_name else False for name in support_op_list):
        return False
    logger.debug('op_name: %s', op_name)
    curr_stage_nonzero_axes = progress.get_nonzero_axes(stage_index)
    stage_dtype = stage.op.output(0).dtype
    elemt_in_block = BLOCK_SIZE_BYTE // util.get_dtype_size(stage_dtype)
    last_axis = curr_stage_nonzero_axes[-1]
    if elemt_in_block == 0:
        raise RuntimeError('elemt_in_block is 0!')
    if last_axis % elemt_in_block != 0:
        return False

    return True


def is_small_stage_in_bn_operator(progress, stage_index):
    """
    bn_training_reduce_grad have two types of shape, big and small
    :param progress:
    :param stage_index:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    if not can_use_optimize_schedule_for_bn_operators(progress):
        return False
    all_shape = []
    for each_stage_index, stage in enumerate(op_schedule_info.schedule_obj.stages):
        if not {'reduce_atomic_rfactor', 'reduce_atomic_write'} & \
               set(op_schedule_info.stages_info[each_stage_index].get('type', [])):
            all_shape.append(progress.get_nonzero_axes(each_stage_index))
    all_shape_after_remove_repetition = []
    for each_shape in all_shape:
        shape_size = functools_reduce(lambda x, y: x * y, each_shape)
        if shape_size not in all_shape_after_remove_repetition:
            all_shape_after_remove_repetition.append(shape_size)
    all_shape_after_remove_repetition.sort()
    current_shape_size = functools_reduce(lambda x, y: x * y, progress.get_nonzero_axes(stage_index))
    base_line_shape_size = all_shape_after_remove_repetition[-1]
    return current_shape_size < base_line_shape_size
