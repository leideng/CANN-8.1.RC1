#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.

rl schedule search, tss
"""

from functools import reduce
from operator import mul

from tbe import tvm
from tbe.dsl.unify_schedule.constants import SUPPORT_SCALAR_INSNS
from tbe.dsl.unify_schedule.constants import BROADCAST_INSNS
from tbe.dsl.unify_schedule.constants import TERNARY_INSNS
from tbe.dsl.base.expr_compare import expr_equal
from auto_search.utils import util
from auto_search.compute_analysis.broadcast_analysis import get_broadcast_tensor


class BroadcastComputeGraphInfo:
    """
    broadcast compute graph info
    """
    MAX_EXTEND_NODE_NUM = 2

    def __init__(self, ouput_tensors, ub_split_axis):
        self.outs = ouput_tensors
        self.ub_split_axis = ub_split_axis
        self.input_tensors = set()
        self.middle_tensors = set()
        self.pure_middle_tensors = set()
        self.middle_out_tensors = set()
        self.out_tensors = set()

        self.broadcast_tensors = set()
        self.absorbable_broadcast_tensors = set()
        self.broadcast_axis_num = {}

        self.broadcast_store_predicate = set()
        self.store_predicate_common_tensors = set()
        self.all_pre_node_broadcast = set()

        self._dtypes = set()
        self._outs_dtypes = set()

        # input -> outputs mapping relations
        self.in_out_map = {}

        self.all_tensors = []

    @staticmethod
    def get_dsl_insn(tensor: tvm.Tensor):
        """
        :param tensor:
        :return:
        """
        tag = tensor.op.tag
        if tensor.op.tag.find("|") != -1:
            insn = tag.split("|")[0]
        else:
            insn = tag
        return insn

    @staticmethod
    def is_placeholder(tensor: tvm.Tensor):
        """
        :param tensor:
        :return:
        """
        return isinstance(tensor.op, tvm.PlaceholderOp)

    @staticmethod
    def _merge_value(map_: dict, key, value):
        """
        :param map_:
        :param key:
        :param value: value container is set
        :return:
        """
        if key not in map_:
            map_[key] = set()
        if isinstance(value, list):
            map_[key].update(value)
        else:
            map_[key].add(value)

    @staticmethod
    def _get_tensor_size(tensor: tvm.Tensor):
        """
        :param tensor:
        :return:
        """
        def shape_to_list(shape):
            """
            :param shape:
            :return:
            """
            shape0 = []
            for i in shape:
                if isinstance(i, tvm.expr.ConstExpr):
                    shape0.append(i.value)
                else:
                    shape0.append(i)
            return shape0
        shape = shape_to_list(tensor.shape)
        if all(isinstance(i, int) for i in shape):
            return reduce(mul, shape_to_list(tensor.shape), 1)
        return -1

    def calc_store_predicate(self):
        """
        calculate need set_store_predicate tensor
        :return:
        """
        def _dfs_cur_tensor(tensor_i):
            for _tensor in tensor_i.op.input_tensors:
                all_pre_node.add(_tensor)
                _dfs_cur_tensor(_tensor)

        # ternary insn can't do set_store_predicate
        if self._has_ternary_insns():
            return

        for tensor_i in self.broadcast_tensors:
            if len(tensor_i.op.input_tensors) != 1:
                continue
            src_shape = tensor_i.op.input_tensors[0].shape
            dst_shape = tensor_i.shape
            if expr_equal(src_shape[self.ub_split_axis], dst_shape[self.ub_split_axis]):
                continue
            cur_tensor = tensor_i
            pre_tensor = tensor_i
            all_pre_node = set()
            if tensor_i in self.absorbable_broadcast_tensors:
                cur_tensor = tensor_i.op.input_tensors[0]
                pre_tensor = cur_tensor
            self.broadcast_store_predicate.add(cur_tensor)
            _dfs_cur_tensor(pre_tensor)
            self.all_pre_node_broadcast.update(all_pre_node)

        disable_store_predicate = self._jude_is_disable_store_predicate()
        if disable_store_predicate:
            self.broadcast_store_predicate.clear()
            self.all_pre_node_broadcast.clear()
            self.store_predicate_common_tensors.clear()

    def construct_compute_graph(self):
        """
        :return:
        """
        self.out_tensors = set(self.outs)
        visited_tensors = set()
        for out in self.out_tensors:
            self.all_tensors.append(out)
            if self._is_broadcast(out):
                self.broadcast_tensors.add(out)
            self.__dfs_sub_graph(out, visited_tensors)
            self._dtypes.add(out.dtype)
            self._outs_dtypes.add(out.dtype)
        self.pure_middle_tensors = self.middle_tensors - self.out_tensors

        for tensor_i in self.broadcast_tensors:
            if self._match_scalar_scene(tensor_i):
                self.absorbable_broadcast_tensors.add(tensor_i)

    def _jude_is_disable_store_predicate(self):
        """
        :return:
        """
        disable_store_predicate = False
        for tensor_i in self.all_pre_node_broadcast:
            common_tensor = self.in_out_map.get(tensor_i) - \
                            (self.all_pre_node_broadcast | self.broadcast_store_predicate)
            if len(common_tensor) > 0:
                # common in multi output
                if tensor_i in self.out_tensors:
                    disable_store_predicate = True
                    break
                self.store_predicate_common_tensors.add(tensor_i)
        extend_node_num = len(self.broadcast_store_predicate) + len(self.store_predicate_common_tensors)
        # keep same with handle template, avoid too much tensors occupy too much memory
        disable_store_predicate = disable_store_predicate or \
                                  extend_node_num > BroadcastComputeGraphInfo.MAX_EXTEND_NODE_NUM

        return disable_store_predicate

    def _support_scalar(self, tensor: tvm.Tensor):
        """
        :param tensor:
        :return:
        """
        return self.get_dsl_insn(tensor) in SUPPORT_SCALAR_INSNS

    def _match_scalar_scene(self, tensor_):
        # condition:
        # 1. tensor --> tensor
        # 2. broadcast tensor is output
        # 3. next compute support scalar
        if len(tensor_.op.input_tensors) != 0 and self._get_tensor_size(tensor_.op.input_tensors[0]) != 1:
            return False
        if tensor_ in self.out_tensors:
            return False
        if all(self._support_scalar(tensor_o) for tensor_o in self.in_out_map.get(tensor_)):
            return True
        return False

    def _is_broadcast(self, tensor: tvm.Tensor):
        """
        :param tensor:
        :return:
        """
        return self.get_dsl_insn(tensor) in BROADCAST_INSNS

    def _has_ternary_insns(self):
        """
        ternary insn must  assign input memory reused by output,
        can't do set_store_predicate
        :return:
        """
        for tensor_i in self.out_tensors | self.pure_middle_tensors:
            insn = self.get_dsl_insn(tensor_i)
            if insn in TERNARY_INSNS:
                return True
        return False

    def __dfs_sub_graph(self, out, visited_tensors: set):
        for tensor_i in out.op.input_tensors:
            self._merge_value(self.in_out_map, tensor_i, out)
            self._dtypes.add(tensor_i.dtype)

            if self.is_placeholder(tensor_i):
                self.input_tensors.add(tensor_i)
                self.all_tensors.append(tensor_i)
            else:
                self.middle_tensors.add(tensor_i)
                self.all_tensors.append(tensor_i)
                if self._is_broadcast(tensor_i):
                    self.broadcast_tensors.add(tensor_i)

            if tensor_i in visited_tensors:
                continue

            visited_tensors.add(tensor_i)

            self.__dfs_sub_graph(tensor_i, visited_tensors)


def get_broadcast_tensors_tag(progress):
    """
    :param progress:
    :return:
    """
    broadcast_last_tensors, broadcast_nist_tensors = get_broadcast_tensor(progress.op_schedule_info.schedule_obj)
    broadcast_tensors = broadcast_last_tensors + broadcast_nist_tensors

    broadcast_tensors_tag = [broadcast_tensor.op.tag for broadcast_tensor in broadcast_tensors]

    return broadcast_tensors_tag


def is_unknown_broadcast(progress):
    """
    :param progress:
    :return:
    """
    broadcast_tensors_tag = get_broadcast_tensors_tag(progress)
    for broadcast_tensor_tag in broadcast_tensors_tag:
        if broadcast_tensor_tag == 'unknown_broadcast':
            return True

    return False


def _get_dynamic_static_compute_tensor_map(static_broadcast_compute_info, dynamic_broadcast_compute_info):
    """
    dynamic_static_compute_tensor_map
    :param static_broadcast_compute_info:
    :param dynamic_broadcast_compute_info:
    :return:
    """
    all_dynamic_compute_tensors = dynamic_broadcast_compute_info.all_tensors
    all_static_compute_tensors = static_broadcast_compute_info.all_tensors

    dynamic_static_compute_tensor_map = {}

    if len(all_dynamic_compute_tensors) != len(all_static_compute_tensors):
        raise RuntimeError("static and dynamic compute is not equal !")

    for index, dynamic_tensor in enumerate(all_dynamic_compute_tensors):
        dynamic_static_compute_tensor_map[dynamic_tensor] = all_static_compute_tensors[index]

    return dynamic_static_compute_tensor_map


def get_set_store_predicate_tensors(progress):
    """
    get need set_store_predicate tensors, contain three part:
    1. broadcast_store_predicate are broadcast tensors
    2. all_pre_node_broadcast are tensors before broadcast tensors
    3. store_predicate_common_tensors are all_pre_node_broadcast's consumers
        and not in broadcast_store_predicate + all_pre_node_broadcast
    :param progress:
    :return:
    """
    op_schedule_info = progress.op_schedule_info
    # get ub split axis
    last_stage_index = len(op_schedule_info.schedule_obj.stages) - 1
    cut_axis_index = op_schedule_info.cut_axis_index[last_stage_index]

    # if none unknown broadcast tensor, static and dynamic compute graph calculate method is same;
    if not is_unknown_broadcast(progress):
        output_tensors = op_schedule_info.output_tensors
        broadcast_compute_info = BroadcastComputeGraphInfo(output_tensors, cut_axis_index)
        broadcast_compute_info.construct_compute_graph()
        broadcast_compute_info.calc_store_predicate()

        return broadcast_compute_info.broadcast_store_predicate, \
               broadcast_compute_info.all_pre_node_broadcast, \
               broadcast_compute_info.store_predicate_common_tensors
    # if have unknown broadcast tensor:
    # can't get unknown broadcast axis by static compute graph(broadcast tensor's shape is const),
    # need get by dynamic compute graph, and need save a map {dynamic_tensor: static_tensor}
    dynamic_compute_info = util.DYNC_COMPUTE_INFO
    dynamic_output_tensors = dynamic_compute_info.res
    static_output_tensors = op_schedule_info.output_tensors

    static_broadcast_compute_info = BroadcastComputeGraphInfo(static_output_tensors, cut_axis_index)
    static_broadcast_compute_info.construct_compute_graph()

    dynamic_broadcast_compute_info = BroadcastComputeGraphInfo(dynamic_output_tensors, cut_axis_index)
    dynamic_broadcast_compute_info.construct_compute_graph()
    dynamic_broadcast_compute_info.calc_store_predicate()

    dynamic_static_compute_tensor_map = \
        _get_dynamic_static_compute_tensor_map(static_broadcast_compute_info, dynamic_broadcast_compute_info)

    static_broadcast_store_predicate = {dynamic_static_compute_tensor_map.get(tensor, None)
                                        for tensor in dynamic_broadcast_compute_info.broadcast_store_predicate}
    static_all_pre_node_broadcast = {dynamic_static_compute_tensor_map.get(tensor, None)
                                        for tensor in dynamic_broadcast_compute_info.all_pre_node_broadcast}
    static_store_predicate_common_tensors = {dynamic_static_compute_tensor_map.get(tensor, None)
                                        for tensor in dynamic_broadcast_compute_info.store_predicate_common_tensors}

    return static_broadcast_store_predicate, static_all_pre_node_broadcast, static_store_predicate_common_tensors
