#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2022-2022. All rights reserved.
"""
norm compute graph
"""

from typing import List

from tbe import tvm
from tbe.dsl.base.expr_compare import expr_equal

from auto_search.utils import logger
from auto_search.utils.util import DTYPE_BYTE_MAPPING
from auto_search.utils.util import BLOCK_SIZE

# upper limit of last asix that enable align pad
ALIGN_PAD_LIMIT = 256
# upper limit of last axis that enable reduce transpose
REDUCE_TRANSPOSE_UPPER_THRESHOLD = 128


class NormComputeGraphInfo:
    """
    Operator Compute Graph Info collector and container
    """
    def __init__(self, op_schedule_info):
        self.stages: List = list(op_schedule_info.schedule_obj.stages)
        self.tensor_producers_map = op_schedule_info.fanin_dict
        self.reduce_tensor_index_list: List = []
        self.after_reduce_tensor_indxe_list: List = []
        self.before_reduce_tensor_index_list: List = []
        self.reduce_axis_index: List = []
        self.shape_after_reduce: List = []
        self.shape_before_reduce: List = []
        self.is_last_reduce: bool = True
        # reduce fork tensor
        self.reduce_fork_tensor_index_list: List = []
        # broadcast fork tensor
        self.broadcast_fork_tensor_index_list: List = []
        self.broadcast_tensor_index_list: List = []
        self.other_tensor_index_list: List = []
        self.out_tensor_index_list: List = []
        self.mid_out_tensor_list: List = []
        self.align_pad_tensor_list: List = []
        self.remove_pad_tensor_list: List = []

        self.get_out_tensor_index(op_schedule_info)
        self.analysis()

    @staticmethod
    def _judge_tvm_shape_equal(shape_a, shape_b):
        """
        compare two tvm shape
        """
        length_a = len(shape_a)
        length_b = len(shape_b)
        if length_a != length_b:
            return False
        for idx in range(length_a):
            if not expr_equal(shape_a[idx], shape_b[idx]):
                return False

        return True

    def get_tensors_before_and_after_reduce(self):
        """
        get before reduce tensors and after reduce tensors
        """
        # Assume all reduce node have the same shape, axis and keepdims
        reduce_stage = self.stages[self.reduce_tensor_index_list[0]]
        shape_after_reduce = list(reduce_stage.op.output(0).shape)
        shape_before_reduce = list(reduce_stage.op.input_tensors[0].shape)
        self.is_last_reduce = len(shape_before_reduce) - 1 in self.reduce_axis_index
        self.shape_after_reduce = [int(dim_value) for dim_value in shape_after_reduce]
        self.shape_before_reduce = [int(dim_value) for dim_value in shape_before_reduce]

        for stage_index, stage in enumerate(self.stages):
            tensor = stage.op.output(0)
            if self._judge_tvm_shape_equal(list(tensor.shape), shape_before_reduce):
                self.before_reduce_tensor_index_list.append(stage_index)
            elif self._judge_tvm_shape_equal(list(tensor.shape), shape_after_reduce):
                self.after_reduce_tensor_indxe_list.append(stage_index)
            else:
                self.other_tensor_index_list.append(stage_index)

    def get_tensors_in_fork(self):
        """
        get tensors in fork
        """
        # broadcast fork tensor
        for broadcast_tensor_index in self.broadcast_tensor_index_list:
            visited_list = []
            _traverse_tensor_list(broadcast_tensor_index, visited_list, self.tensor_producers_map)
            # visited tensor cannot include reduce tensor
            if set(visited_list) & set(self.reduce_tensor_index_list):
                continue

            for single_tensor in visited_list:
                self.broadcast_fork_tensor_index_list.append(single_tensor)
        # output reduce fork tensor
        for out_tensor in set(self.out_tensor_index_list) & set(self.after_reduce_tensor_indxe_list):
            visited_list = []
            _traverse_tensor_list(out_tensor, visited_list, self.tensor_producers_map, self.reduce_tensor_index_list)
            for single_tensor in visited_list:
                self.reduce_fork_tensor_index_list.append(single_tensor)

    def get_out_tensor_index(self, op_schedule_info):
        """
        Parameters
        ----------
        op_schedule_info

        Returns
        -------

        """
        for stage_index, stage in enumerate(self.stages):
            tensor_name = stage.op.name
            if tensor_name in op_schedule_info.real_out_tensor_str:
                self.out_tensor_index_list.append(stage_index)

    def get_support_align_pad_tensor(self):
        """
        :return:
        """
        for stage_index, stage in enumerate(self.stages):
            if stage_index in self.broadcast_fork_tensor_index_list:
                continue
            if not isinstance(stage.op, tvm.PlaceholderOp):
                continue
            input_shape = stage.op.output(0).shape
            input_dtype = stage.op.output(0).dtype
            dtype_byte = DTYPE_BYTE_MAPPING.get(input_dtype, None)
            block_size = BLOCK_SIZE // dtype_byte
            if int(input_shape[-1]) % block_size != 0 and int(input_shape[-1]) < ALIGN_PAD_LIMIT:
                self.align_pad_tensor_list.append(stage_index)

    def get_support_remove_pad_tensor(self):
        """

        :return:
        """
        for stage_index in self.out_tensor_index_list:
            if stage_index in self.reduce_fork_tensor_index_list:
                continue
            out_shape = self.stages[stage_index].op.output(0).shape
            out_dtype = self.stages[stage_index].op.output(0).shape
            dtype_byte = DTYPE_BYTE_MAPPING.get(out_dtype, None)
            block_size = BLOCK_SIZE // dtype_byte
            if int(out_shape[-1]) % block_size != 0:
                self.remove_pad_tensor_list.append(stage_index)


    def analysis(self):
        """
        :return:
        """
        for stage_index, stage in enumerate(self.stages):
            if isinstance(stage.op, tvm.ComputeOp):
                if stage.op.tag.find("reduce") != -1:
                    self.reduce_tensor_index_list.append(stage_index)
                elif stage.op.tag.find("broadcast") != -1:
                    self.broadcast_tensor_index_list.append(stage_index)

        self.reduce_axis_index = get_reduce_axes_index(self.stages[self.reduce_tensor_index_list[0]])

        self.get_tensors_before_and_after_reduce()
        self.get_tensors_in_fork()
        self.get_support_align_pad_tensor()


def _traverse_tensor_list(root_tensor, visited_list, traverse_map, stop_tensor_set=None):
    if root_tensor not in visited_list:
        visited_list.append(root_tensor)
    if stop_tensor_set is not None and root_tensor in stop_tensor_set:
        return
    if root_tensor in traverse_map:
        for input_tensor in traverse_map.get(root_tensor):
            _traverse_tensor_list(input_tensor, visited_list, traverse_map, stop_tensor_set)
    else:
        return


def get_reduce_axes_index(reduce_stage) -> List[int]:
    """
    get all reduce axes index
    :return:
    """
    stage_op = reduce_stage.op
    reduce_axis_var = []
    for i in stage_op.reduce_axis:
        reduce_axis_var.append(i.var)
    source_len = len(stage_op.body[0].source)
    try:
        data_axis_var = stage_op.body[0].source[source_len - 1].indices
    except AttributeError:
        logger.debug('stage: %s, body: %s, reduce_axis_var: %s', stage_op, stage_op.body, reduce_axis_var)
        data_axis_var = []
    reduce_axis_list = []
    for axis in reduce_axis_var:
        for axis_index, axis_var in enumerate(data_axis_var):
            if axis_var.same_as(axis):
                reduce_axis_list.append(axis_index)
    reduce_axis_list.sort()
    return reduce_axis_list


def is_support_ub_transpose_search(op_schedule_info: object) -> bool:
    """
    :param op_schedule_info:
    :return:
    """
    norm_graph_info = op_schedule_info.compute_graph_info
    if len(norm_graph_info.reduce_axis_index) != 1:
        return False
    # reduce before shape dims must be 2
    if len(norm_graph_info.shape_before_reduce) != 2:
        return False
    # reduce before shape[0]/shape[1] must bigger than 1
    if norm_graph_info.shape_before_reduce[0] == 1 or norm_graph_info.shape_before_reduce[1] == 1:
        return False
    # last reduce
    if not norm_graph_info.is_last_reduce:
        return False
    # last axis must be less than 128
    if norm_graph_info.shape_before_reduce[-1] > REDUCE_TRANSPOSE_UPPER_THRESHOLD:
        return False
    # not cut reduce axis
    if len(set(op_schedule_info.at_dict.values())) > 2:
        return False

    return True
