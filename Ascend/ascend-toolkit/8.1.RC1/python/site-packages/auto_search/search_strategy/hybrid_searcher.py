#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) Huawei Technologgeries Co., Ltd. 2022-2022. All rights reserved.
"""
This search strategy will traverse the whole search space by mcts alg and dfs alg with multi process
"""
import time
import multiprocessing

from auto_search.utils import logger
from auto_search.search_strategy.searcher import strategy_register
from auto_search.search_strategy.traversal_searcher import search as traversal_search
from auto_search.search_strategy.mcts_searcher import search as mcts_search


CPU_OCCUPY_RATE = 0.5
MAX_PROCESS_COUNT = 8
SINGLE_PROCESS_MAX_TIME = 1800


class HybridSearchPool:
    """
    Class which supports an async version of applying functions to arguments.
    """
    def __init__(self, processes, time_out):
        """
        :param processes: process_num
        :param time_out
        """
        # process nums
        self._processes_num = processes
        # process pool to save current running process
        self._pool = []
        # process pool longest run time, user can configure it
        self._pool_time_out = time_out
        # single process longest run time
        self._single_process_max_time = time_out + SINGLE_PROCESS_MAX_TIME
        self.dfs_process = None

    @staticmethod
    def _stop_processes(process_info_list):
        """
        stop all processes in process_list
        :param process_info_list:
        """
        process_to_remove = []
        for process_info in process_info_list:
            process_handler, _, _ = process_info
            if process_handler and process_handler.is_alive():
                process_pid = process_handler.pid
                process_handler.kill()
                process_handler.join()
                process_handler.close()
                process_to_remove.append(process_info)
                logger.info("auto search info: process %s is stopped.", process_pid)
        return process_to_remove

    def apply_all_works(self, func_name, func_args, dfs_res):
        """
        apply_sync
        :param func_name
        :param func_args
        :param dfs_res
        """
        index = 1
        pool_start_time = time.time()
        early_stop_flag = False

        # multiple mcts trees
        while True:
            if self.dfs_process and \
                    dfs_res.value and \
                    (self.dfs_process not in self._pool or not self.dfs_process[0].is_alive()):
                logger.event("auto search info: dfs search process end, early stop!")
                early_stop_flag = True

            if time.time() - pool_start_time > self._pool_time_out or early_stop_flag:
                process_to_remove = self._stop_processes(self._pool)
                self._remove_process(process_to_remove)
                break
            self._clean_pool()
            # if resources are available, create more processes
            while self._processes_num > len(self._pool):
                func_args_tuple = (func_args, )
                self._repopulate_pool(func_name, func_args_tuple)
                index += 1

    def apply_dfs_work(self, dfs_func_name, op_schedule_info, dfs_res):
        """
        apply one dfs search process before multiple mcts search processes
        :param dfs_func_name:
        :param op_schedule_info:
        :param dfs_res:
        """
        # one dfs tree
        func_args_tuple = (op_schedule_info, dfs_res)
        self._repopulate_pool(dfs_func_name, func_args_tuple)
        self.dfs_process = self._pool[0]
        logger.info("auto search info: dfs process %s is added to pool.", self.dfs_process[0].pid)

    def _repopulate_pool(self, func_name, func_args_tuple):
        """
        create a new process and add to the pool
        :param func_name
        :param func_args_tuple
        """
        ctx = multiprocessing.get_context('fork')
        sample_process = ctx.Process(target=func_name, args=func_args_tuple, daemon=True)
        sample_process.start()
        self._pool.append([sample_process, time.time(), self._single_process_max_time])
        logger.info("auto search info: new process %s add to pool, single_process_max_time %s(s).",
                    sample_process.pid, self._single_process_max_time)

    def _clean_pool(self):
        """
        delete process handler when process is not alive
        """
        # stop some process when it run out of its time
        process_to_remove = []
        for process_info in self._pool:
            process_handler, start_time, process_time_out = process_info
            if time.time() - start_time > process_time_out:
                process_to_remove.extend(self._stop_processes([process_info]))
        self._remove_process(process_to_remove)

        # clean one process when it is not alive
        process_dead = []
        for process_info in self._pool:
            process_handler, _, _ = process_info
            if process_handler and not process_handler.is_alive():
                logger.info("auto search info: dead process %s", process_handler.pid)
                process_handler.close()
                process_dead.append(process_info)
        self._remove_process(process_dead)

    def _remove_process(self, process_to_remove):
        """
        remove process in self._pool
        :param process_to_remove:
        """
        if process_to_remove:
            for process in process_to_remove:
                self._pool.remove(process)
                del process


def _mcts_worker(op_schedule_info):
    try:
        mcts_search(op_schedule_info)
    except Exception as e:
        logger.warn(e)


def _traversal_worker(op_schedule_info, dfs_res):
    try:
        if traversal_search(op_schedule_info):
            dfs_res.value = True
    except Exception as e:
        logger.warn(e)


@strategy_register('hybrid')
def search(op_schedule_info):
    """
    hybrid search is traversal search + mcts search
    :param op_schedule_info:
    :return:
    """
    option = op_schedule_info.option
    process_num = min(int(multiprocessing.cpu_count() * CPU_OCCUPY_RATE), MAX_PROCESS_COUNT)
    concurrent_num = option.get('concurrent_num', process_num)
    logger.event("auto search info: cpu total count: %s, used count: %s.", multiprocessing.cpu_count(), concurrent_num)

    timeout_secs = option.get('timeout')
    logger.info("auto search info: timeout is: %s seconds.", timeout_secs)

    pool = HybridSearchPool(processes=concurrent_num, time_out=timeout_secs)
    dfs_res = multiprocessing.Value("b", False)
    pool.apply_dfs_work(_traversal_worker, op_schedule_info, dfs_res)
    pool.apply_all_works(_mcts_worker, op_schedule_info, dfs_res)
