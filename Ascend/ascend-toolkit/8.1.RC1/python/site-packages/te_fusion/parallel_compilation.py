#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
support parallel compilation
"""
# 'pylint: disable=too-many-lines
import multiprocessing as mp
import queue
import time
import os
import stat
import importlib
import signal
import datetime
import traceback
import shutil
import zlib
import pickle
import sys
import subprocess
import json
import threading
import glob
import faulthandler
import re
from typing import Optional
from contextlib import contextmanager
from pathlib import Path
from configparser import ConfigParser

import tbe.common.utils.log as logger
import te_fusion.log_util as te_log

from tbe.common.buildcfg import build_config
from tbe.common.buildcfg import get_current_build_config
from tbe.common.buildcfg import get_L1_info
from tbe.common.buildcfg import set_L1_info
from tbe.common.platform.platform_info import get_soc_spec
from tbe.common.platform.platform_info import set_soc_spec
from tbe.common.platform.platform_info import te_update_version
from tbe.common.platform.platform_info import set_current_compile_soc_info
from tbe.common.platform.platform_info import set_platform_info_res
from tbe.common.platform.platform_info import set_core_num_by_core_type
from te_fusion.fusion_manager import build_single_op
from te_fusion.fusion_manager import op_build_cfg_en
from te_fusion.fusion_util import fusion_op
from te_fusion.fusion_util import get_kwargs
from te_fusion.fusion_util import OpImplPolicy
from te_fusion.fusion_util import set_l1_l2_fusion_enabled
from te_fusion.fusion_util import check_valid_core_type
from te_fusion.fusion_util import set_kernel_meta_parent_dir
from te_fusion.fusion_util import compile_kernel_fusion, compile_super_kernel
from te_fusion.fusion_util import trans_bool_to_int8_op_list
from te_fusion.fusion_util import get_tbe_debug_level


FILE_MODE_440 = stat.S_IRUSR | stat.S_IRGRP
FLAG = os.O_WRONLY | os.O_CREAT
MAXINT32 = 2**32


# 'pylint: disable=too-few-public-methods
class Counter:
    """
    Atomic counter
    """
    counter = MAXINT32
    locker = threading.Lock()

    @staticmethod
    def next():
        """
        get next counter
        :return: next counter
        """
        with Counter.locker:
            Counter.counter += 1
        return Counter.counter


def mygetattr(obj, name):
    """
    get object attr recursively
    :param obj: python object
    :param name: attr name
    :return: attr
    """
    if not name:
        return obj
    name_list = name.split(".")
    while name_list:
        obj = getattr(obj, name_list[0])
        name_list = name_list[1:]
    return obj


def set_soc_info_before_compile(options):
    """
    Set core type for this compilation process.
    There is a L1 fusion problem on lhisi platform when invoking
    set_current_compile_soc_info. So only do this operation for
    specific soc.
    :param options: soc info which will be passed to tbe compiler
    :return: void
    """
    soc_version = options["socVersion"]
    logger.info("start to set_soc_info_before_compile, socVersion is [%s]", soc_version)
    if soc_version in ('OPTG', 'TsnsC'):
        return
    core_type = options.get("coreType", "")

    if core_type == "":
        ori_core_type = get_soc_spec("AICORE_TYPE")
        core_type = ori_core_type
    set_soc_spec(str(core_type))
    device_id = options.get('device_id', None)
    try:
        if device_id is not None and device_id != "":
            platform_info = init_platform_info(options)
            set_platform_info_res(int(device_id), platform_info)
    except Exception:
        logger.info("device id is not integer.")
    logger.info("set core num by core type[%s]", core_type)
    set_core_num_by_core_type(core_type)


def mysetattr(obj, name, value):
    """
    get object attr recursively
    :param obj:  python object
    :param name: attr name
    :param value: attr value
    :return: None
    """
    name_list = name.split(".")
    target_obj = mygetattr(obj, ".".join(name_list[:-1]))
    target_attr = name_list[-1]
    setattr(target_obj, target_attr, value)


def excepthook_silent(etype, value, tback):  # pylint: disable=unused-argument
    """
    excepthook to print nothing
    """


def worker_sigint_handler(signum, frame):  # pylint: disable=unused-argument
    """
    worker process just quit when Ctrl-C pressed
    """
    # logging module uses reentrant threading.Rlock,
    # can be safely used in signal handler
    logger.warn('recv Ctrl-C pressed.signum[%s]', signum)
    del_tmp_files_by_pid(OpCompiler.master_pid)


def worker_sigint_handler_no_print(signum, frame):  # pylint: disable=unused-argument
    """
    worker process terminal when recv signal
    """
    # logging module uses reentrant threading.Rlock,
    # can be safely used in signal handler
    logger.warn('signum: [%d]. worker terminate, worker process quiting...', signum)
    del_tmp_files_by_pid(OpCompiler.master_pid)
    os._exit(1)  # pylint: disable=protected-access


def check_dict_paras(dict_ops):
    if dict_ops['op_debug_dir'] == None or dict_ops['op_debug_dir'] == '':
        dict_ops['op_debug_dir'] = '.'
    if dict_ops['vector_fp_ceiling'] == None or dict_ops['vector_fp_ceiling'] == "":
        dict_ops['vector_fp_ceiling'] = get_current_build_config('vector_fp_ceiling')
    if dict_ops['enable_L1_fusion'] == None or dict_ops['enable_L1_fusion'] == "":
        dict_ops['enable_L1_fusion'] = get_current_build_config('enable_L1_fusion')
    if dict_ops['enable_L2_fusion'] == None or dict_ops['enable_L2_fusion'] == "":
        dict_ops['enable_L2_fusion'] = get_current_build_config('enable_L2_fusion')
    if 'op_bank_path' not in dict_ops:
        dict_ops['op_bank_path'] = ""
    if 'mdl_bank_path' not in dict_ops:
        dict_ops['mdl_bank_path'] = ""


def init_sighandler(dict_ops, master_pid):
    # just quit, avoid KeyboardInterrupt traceback printing mess
    sig_list = [signal.SIGINT, signal.SIGHUP]
    for sig in sig_list:
        signal.signal(sig, worker_sigint_handler)
    signal.signal(signal.SIGTERM, worker_sigint_handler_no_print)

    # init faulthandler when running task
    kernel_meta_temp_dir = dict_ops.get("kernel_meta_temp_dir", None)
    if kernel_meta_temp_dir is None:
        logger.warn("kernel_meta_temp_dir is None.")
        faulthandler.enable()
        return None

    if not os.path.exists(kernel_meta_temp_dir):
        logger.warn("Invalid kernel_meta_temp dir: %s", kernel_meta_temp_dir)
        faulthandler.enable()
        return None

    cur_pid = os.getpid()
    try:
        bt_handler = open(kernel_meta_temp_dir + '/' + str(master_pid) + '_' + str(cur_pid) + '_bt.log', 'a+')
        faulthandler.enable(bt_handler, False)
        return bt_handler
    except Exception:
        logger.warn("Init faulthandler fail.")
        return None


def init_platform_info(dict_ops):
    platform_info = {}
    info_key = ['cube_core_cnt', 'ai_core_cnt', 'vector_core_cnt', 'memory_size', 'l2_size', 'l0_a_size',
                'l0_b_size', 'l0_c_size', 'l1_size', 'ddr_rate', 'l2_rate']
    for key in info_key:
        value = dict_ops.get(key, None)
        if value is not None and value != "":
            platform_info[key] = value
    return platform_info


def exec_compilation_task(worker_env, task_env):
    """
    compilation task worker entry
    :param soc_info: soc_version, core_type, core_num, l1size
    :param task_env: tuple of task queue, pipe, etc...
    :return: None
    """

    soc_info, platform_info, option_info, dispatcher, pid, slog_level, slog_event, reset_op_info = worker_env

    dict_ops = get_kwargs(*soc_info, kwargs=option_info)
    check_dict_paras(dict_ops)
    bt_handler = init_sighandler(option_info, pid)
    try:
        # 'pylint: disable=no-name-in-module, import-error, import-outside-toplevel
        from te.utils.AscendLog import AscendLog
        slog = AscendLog()
        if slog_level is not None:
            slog.set_level(-1, slog_level, slog_event)
    except Exception:       # 'pylint: disable=broad-except
        pass

    import te.platform.cce_conf as cce_conf
    logger.info("soc_info:[%s], option:[%s]", soc_info, option_info)
    cce_conf.te_set_version(*soc_info, option_info)

    device_id = dict_ops.get('device_id', None)
    try:
        if device_id is not None and device_id != "":
            set_platform_info_res(int(device_id), platform_info)
    except Exception:
        logger.info("device id is not integer.")
    set_core_num_by_core_type(soc_info[1])

    OpCompiler.task_dispatcher = dispatcher
    OpCompiler.master_pid = pid
    OpCompiler.reset_op_info = reset_op_info

    logger.info("Default compiler:%s", OpCompiler.compiler)
    logger.info("compile_options when executing compile_task:%s", dict_ops)
    worker = TaskWorker(task_env, pid)
    worker.add_dict(dict_ops)
    worker.loop()

    if bt_handler is not None:
        bt_handler.close()


def get_multi_process_count():
    """
    get compilation worker number from ini conf file
    :return: compilation worker number
    """

    try:
        if 'TE_PARALLEL_COMPILER' in os.environ:
            count = int(os.getenv('TE_PARALLEL_COMPILER'))
            logger.info("TE_PARALLEL_COMPILER=%s", count)
        else:
            home_path = os.getenv('HOME')
            config_file_path = os.path.join(home_path, ".tbe_build.ini")
            config_file = ConfigParser()
            config_file.read(config_file_path)
            count = config_file.getint('compilation', 'max_parallel_jobs')
        count = max(0, min(count, len(os.sched_getaffinity(0))))
        # parallel processes num range [1,32] default is 8
        if count < 1 or count > 32:
            count = 8
        return count
    except Exception:       # 'pylint: disable=broad-except
        return 8


def set_main_info():
    """
    set __file__ and name of main to None
    :return: (orignal main module name, path)
    """
    main_module = sys.modules['__main__']
    main_mod_name = getattr(main_module.__spec__, "name", None)
    main_path = getattr(main_module, '__file__', None)
    if main_mod_name is not None:
        setattr(main_module.__spec__, "name", None)

    if main_path is not None:
        setattr(main_module, '__file__', None)
    return (main_mod_name, main_path)


def restore_main_info(name, path):
    """
    restor main module name and path
    """
    main_module = sys.modules['__main__']
    if name is not None:
        setattr(main_module.__spec__, "name", name)
    if path is not None:
        setattr(main_module, '__file__', path)


def guess_pyexe_path(mp_ctx):
    """
    search for a suitable python exe, should be called before any multiprocessing calls
    :param mp_ctx: multiprocessing module
    """
    pylibver = sys.version_info
    pyver = subprocess.run([sys.executable, '-V'], stderr=subprocess.DEVNULL,
                           check=False,
                           stdout=subprocess.PIPE).stdout.decode().split()[1].split('.')
    if pyver[0] == str(pylibver.major) and pyver[1] == str(pylibver.minor):
        return

    targetpy = "python" + str(pylibver.major) + "." + str(pylibver.minor)
    path_list = os.environ['PATH'].split(os.pathsep) + ['/usr/bin', '/usr/local/bin']
    binpath = [os.path.join(path, targetpy) for path in path_list]

    for path in binpath:
        if os.path.isfile(path):
            mp_ctx.set_executable(path)
            logger.info("guessed python path:%s", path)
            return


class CompilerEnv:
    """
    CompilerEnv
    """
    embedding = None
    soc_info = None
    platform_info = None
    option_info = None
    slog_level = None
    slog_event = None
    pid_timestamp = None
    locker = threading.Lock()

    task_counter = 0
    task_running = 0
    restart_step = -1
    restart_counter = 0

    @classmethod
    def task_counter_modify(cls, i):
        """task counter modify"""
        if cls.restart_step == -1:
            cls.restart_step = max(0, int(os.getenv('TE_AUTO_RESTART_COUNTER', '0')))
        if cls.restart_step == 0:
            return False

        if i > 0:
            cls.task_counter += i
        cls.task_running += i
        if cls.task_running == 0 and cls.task_counter > cls.restart_counter:
            # need restart parallel process
            logger.info("@@@ auto restart step: %s",
                        cls.restart_step)
            while cls.restart_counter - cls.task_counter < cls.restart_step:
                logger.info("@@@ need restart: %s, %s",
                            cls.restart_counter, cls.task_counter)
                cls.restart_counter += cls.restart_step

            return True
        return False

    @classmethod
    @contextmanager
    def env_guard(cls):
        """env guard"""
        if cls.restart_step == -1:
            cls.restart_step = max(0, int(os.getenv('TE_AUTO_RESTART_COUNTER', '0')))
        if cls.restart_step > 0:
            while True:
                res = cls.locker.acquire()
                if res:
                    break
        try:
            yield None
        finally:
            if cls.restart_step > 0:
                cls.locker.release()


class OpCompiler:# 'pylint: disable=too-many-instance-attributes
    """
    OpCompiler
    """
    SUB_PROCESS_OK = 0
    SUB_PROCESS_ERROR = 1
    compiler = None
    worker_checker_count = 0
    master_pid = 0
    task_dispatcher = None
    atc_time_stamp = None
    task_start_status = 0
    status = SUB_PROCESS_OK
    reset_op_info = None

    # 'pylint: disable=too-many-arguments
    def __init__(self, embedding, worker_num, soc_info, option_info,
                 time_stamp=None, slog_level=None, slog_event=1):
        """
        init
        :param task_env:
        :param worker_list:
        """
        self.task_dispatcher = None
        # '{graphid: {taskid: desc}}
        self._task_running = {}

        # '{graphid: {taskid: result}}
        self._task_finished = {}
        self._worker_num = worker_num
        self._soc_info = soc_info
        self._platform_info = init_platform_info(option_info)
        self._option_info = option_info
        self._embedding = embedding
        self.finished_task_queue = None
        self.live_checker = None
        self.extra_res_queue = {}
        self._worker_list = []
        self.data_queue = []
        self.task_queue = []
        self._reset_op_info = None
        self._time_stamp = time_stamp
        self._slog_level = slog_level
        self._slog_event = slog_event

        OpCompiler.compiler = self
        OpCompiler.master_pid = os.getpid()

    @classmethod
    def is_sub_proc_ok(cls):
        """
        check if sub processes are ok
        """
        return cls.status == cls.SUB_PROCESS_OK

    def init(self):
        """
        init task queue, data queue and result queue
        """
        if self.task_dispatcher is not None:
            return

        # set soc info and platform info
        import te.platform.cce_conf as cce_conf
        logger.info("soc_info:[%s], option:[%s]", self._soc_info, self._option_info)
        cce_conf.te_set_version(*self._soc_info, self._option_info)
        try:
            device_id = self._option_info.get('device_id', None)
            if device_id is not None and device_id != "":
                set_platform_info_res(int(device_id), self._platform_info)
        except Exception:
            pass
        set_core_num_by_core_type(self._soc_info[1])

        self._build_random_op()

        # multiprocessing will access sys.argv, if sys.argv not exist
        # exception raised and can not be caught here
        if not hasattr(sys, "argv"):
            sys.argv = ['']

        ctx = mp.get_context("forkserver")
        ctx.set_forkserver_preload([])
        """
        clear preload messages before generate child process, which is risky
        """
        self.task_queue = ctx.Queue()
        self.task_queue.cancel_join_thread()
        self.finished_task_queue = ctx.Queue()
        self.finished_task_queue.cancel_join_thread()
        self.live_checker = ctx.Pipe()
        self.data_queue = [ctx.Queue() for _ in range(0, self._worker_num)]
        for dqueue in self.data_queue:
            dqueue.cancel_join_thread()
        self.task_dispatcher = TaskDispatcher((self.task_queue, self.finished_task_queue, self.data_queue))

    def start(self):
        """
        start worker compiler process
        """
        if self._worker_list:
            return self._worker_num, self.finished_task_queue, \
                self.live_checker

        if self._embedding:
            guess_pyexe_path(mp)

        # Child process of py multiprocessing will import all modules imported
        # by parent, which is unnecessary and problematic, here is a hack to
        # bypass it.
        main_mod_name, main_path = set_main_info()

        deamon_status = mp.current_process().daemon
        mp.current_process().daemon = False
        for idx in range(0, self._worker_num):
            data_queue = self.data_queue[idx]
            task_dispatcher = TaskDispatcher((OpCompiler.compiler.task_dispatcher._task_queue,
                                              OpCompiler.compiler.task_dispatcher.fin_task_queue,
                                              data_queue))
            ctx = mp.get_context("forkserver")
            worker = \
                ctx.Process(target=exec_compilation_task,
                            args=(
                                (self._soc_info, self._platform_info, self._option_info, task_dispatcher,
                                 OpCompiler.master_pid, self._slog_level, self._slog_event, self._reset_op_info),
                                (self.task_queue, self.finished_task_queue, data_queue,
                                 self.live_checker[0], self._time_stamp)
                            ),
                            daemon=True)
            worker.start()
            self._worker_list.append(worker)

        mp.current_process().daemon = deamon_status
        restore_main_info(main_mod_name, main_path)

        self.task_start_status = 1
        return self._worker_num

    def _build_random_op(self):
        need_build_random_op = True
        resource_ctrl = os.getenv("MIN_COMPILE_RESOURCE_USAGE_CTRL")
        if resource_ctrl is not None:
            resource_ctrl.replace(" ", "")
            options = resource_ctrl.split(",")
            if "op_compile" in options:
                need_build_random_op = False
                python_path = os.getenv("PYTHONPATH")
                dirs = python_path.split(":")
                fake_lib_path = str()
                for dir in dirs:
                    index = dir.find("open_source/tvm/python")
                    if index != -1:
                        fake_lib_path = dir + "/" + "fake_load_lib_tvm"
                        break
                import tbe.common.utils.log as logger
                if len(fake_lib_path) == 0:
                    logger.warn("PYTHONPATH doesn't contain tvm path")
                else:
                    new_path = fake_lib_path + ":" + python_path
                    logger.info("new PYTHONPATH is %s", new_path)
                    os.environ["PYTHONPATH"] = new_path

        if need_build_random_op:
            # get op debug dir
            op_debug_dir_str = "."
            if isinstance(self._option_info, dict):
                tmp_debug_dir_str = self._option_info.get("op_debug_dir", "")
                if len(tmp_debug_dir_str) > 0:
                    op_debug_dir_str = tmp_debug_dir_str
            # call vector_random_buff and cube_random_buff
            random_buffer_start_time = int(time.time() * 1000000)
            import tbe.common.context.op_context as op_context
            with op_context.OpContext("static"):
                context = op_context.get_context()
                with build_config(kernel_meta_parent_dir = op_debug_dir_str, compatible=True):
                    importlib.import_module("te.platform.vector_random_buff")
                    importlib.import_module("te.platform.cube_random_buff")
                    import te.platform.vector_random_buff as vector_random_buff
                    import te.platform.cube_random_buff as cube_random_buff

                    vector_random_buff.vector_random_buff()
                    cube_random_buff.cube_random_buff()

                    self._reset_op_info = context.get_addition("reset_op_info")
                    OpCompiler.reset_op_info = self._reset_op_info

            random_buffer_end_time = int(time.time() * 1000000)
            import tbe.common.utils.log as logger
            logger.info("The time cost of random buffer compile is [%d] micro second.",
                        random_buffer_end_time - random_buffer_start_time)
        else:
            import tbe.common.utils.log as logger
            logger.info("Op compile is turned off, will not build random op.")

    def destory(self):
        """
        deinit multi compilation process
        :return: None
        """
        dispatcher = self.task_dispatcher
        if dispatcher is None:
            return

        try:
            for worker in self._worker_list:
                if worker.is_alive():
                    worker.terminate()
        except Exception:       # 'pylint: disable=broad-except
            # Sub processes may already quit when being killed
            logger.warn("Exception: %s", traceback.format_exc())
        self.task_dispatcher = None
        self._task_running = {}
        OpCompiler.compiler = None
        OpCompiler.master_pid = 0

    def is_worker_alive(self):
        """
        check wether all worker processes are alive
        :return:
        """
        if self.task_dispatcher is None:
            return False
        all_alive = True
        died_processes = []
        for worker in self._worker_list:
            if not worker.is_alive():
                died_processes.append(worker.pid)
                logger.warn("worker process %s died. exitcode %s",
                               worker.pid, worker.exitcode)
                all_alive = False
        return all_alive, died_processes

    def get_traceback_msgs(self, task_pid):
        kernel_temp = self.get_kernel_meta_temp_dir()
        if kernel_temp is None:
            logger.warn("kernel_meta_temp is not exist!")
            return None

        master_pid = os.getpid()
        bt_log_file = kernel_temp + '/' + str(master_pid) + "_" + str(task_pid) + '_bt.log'
        with open(bt_log_file, "r") as bt_handler:
            if bt_handler is None:
                logger.warn("Cur child pid %s has no bt file", worker.pid)
                return None
            stack_msgs = bt_handler.read()
            bt_handler.close()
            return stack_msgs

    def get_kernel_meta_temp_dir(self):
        kernel_temp = "."
        if isinstance(self._option_info, dict):
            kernel_temp = self._option_info.get("kernel_meta_temp_dir", None)
        return kernel_temp

    def save_died_process_err_msgs(self, died_pids):
        kernel_temp = self.get_kernel_meta_temp_dir()
        if kernel_temp is None:
            logger.warn("kernel_meta_temp is not exist!")
            return

        master_pid = os.getpid()
        for pid in died_pids:
            died_pid_task_file = kernel_temp + "/" + "task_pid_" + str(master_pid) + "_" + str(pid) + ".txt"
            if not os.path.exists(died_pid_task_file):
                logger.warn("Task-pid file[%s] is not exist!", died_pid_task_file)
                continue
            with open(died_pid_task_file, "r", encoding='utf-8') as f:
                for line in f.readlines():
                    line = line.strip()
                    task_info = line.split(".")
                    logger.debug("Line content of file[%s] is[%s]", died_pid_task_file, line)
                    if task_info:
                        gid = int(task_info[0])
                        tid = int(task_info[1])
                        if self._task_running.get(gid, None) is None:
                            logger.debug("Did not find gid[%d] in _task_running", gid)
                            continue
                        task_desc = self._task_running[gid].get(tid, None)
                        if task_desc is None:
                            logger.debug("Did not find task_desc[%d:%d]", gid, tid)
                            continue
                        task_desc_info = json.loads(task_desc)
                        task_res = gen_task_res(0, gid, tid, 3, 'FatalError',
                                                "compiler process died, caused by current task.",
                                                except_msg=self.get_traceback_msgs(pid),
                                                err_args=task_desc_info["task_infos"])
                        self.save_finished_task(gid, tid, task_res)

    def check_worker_status(self):
        """
        check if worker process are alive, if not, set all running task as fail
        """
        # if any worker process dead, all task will be markded as failed
        OpCompiler.worker_checker_count += 1
        if OpCompiler.worker_checker_count % 3000 != 0:
            return

        died_pids = []
        died_tune_pids = []
        worker_alive, died_pids = self.is_worker_alive()
        if len(died_pids) != 0:
            self.save_died_process_err_msgs(died_pids)

        if not worker_alive:
            for gid, tasks in list(self._task_running.items()):
                for tid, task in list(tasks.items()):
                    errmsg = "compiler process died, caused by other process."
                    task_res = gen_task_res(0, gid, tid, 1, 'FatalError', errmsg)
                    self.save_finished_task(gid, tid, task_res)
            self._task_running.clear()

    def get_finished_task(self, graphid=None, taskids=None):
        """
        return finished compilation task
        :return:
        """
        if self.task_dispatcher is None:
            return []
        try:
            while True:
                task_res = self.task_dispatcher.get_result(False)
                gid = task_res['graph_id']
                tid = task_res['task_id']
                self.save_finished_task(gid, tid, task_res)
        except queue.Empty:
            pass

        self.check_worker_status()

        res = []
        if graphid is not None:
            task_res = self._task_finished.get(graphid)
            if task_res is None:
                return res

            if taskids is None:
                res = list(task_res.values())
                del self._task_finished[graphid]
            else:
                res = [task_res.pop(tid, None) for tid in taskids]
                if task_res == {}:
                    del self._task_finished[graphid]
        else:
            for gid, tasks in self._task_finished.items():
                res.extend(list(tasks.values()))
            self._task_finished.clear()

        return res

    def update_running_task(self, task):
        """
        update task to _task_running
        :param task:
        """
        if self.task_dispatcher is None:
            return
        runnings = self._task_running.setdefault(task.graph_id, {})
        running = runnings.get(task.task_id)
        if running is not None:
            logger.warn("task already exist, dispatch failed. %d:%d",
                           task.graph_id, task.task_id)
            return
        runnings[task.task_id] = task.desc()

    def dispatch_task(self, task):
        """
        dispatch task to workers
        :param task:
        """
        if self.task_dispatcher is None:
            return
        self.task_dispatcher.dispatch(task)
        self.update_running_task(task)

    def sync_data(self, data):
        """
        sync data to all workers
        :param data:
        """
        if self.task_dispatcher is None:
            return
        self.task_dispatcher.sync_data(data)

    def clear_running_task(self, gid, tid):
        """
        clear running task
        :param gid: task graphid
        :param tid: task taskid
        """
        if self.task_dispatcher is None:
            return True
        tasks_in_gid = self._task_running.get(gid)
        if tasks_in_gid is None:
            logger.info("task finished, but graphid not found. %d:%d",
                        gid, tid)
            return False

        running = tasks_in_gid.get(tid)
        if running is None:
            logger.info("task finished, but taskid not found. %d:%d",
                        gid, tid)
            return False

        del tasks_in_gid[tid]
        return True

    def save_finished_task(self, gid, tid, res):
        """
        save finished task
        :param gid: task graphid
        :param tid: task taskid
        :param res: task result
        """
        if self.task_dispatcher is None:
            return
        self.clear_running_task(gid, tid)

        finished_task_in_gid = self._task_finished.setdefault(gid, {})
        finished_task_in_gid[tid] = res

    def get_reset_op_info(self):
        return self._reset_op_info


# 'pylint: disable=too-few-public-methods
class DeferredOpRes:
    """
    DeferredOpRes
    """
    _task_finished = {}

    def __init__(self, gid, tid, res=None, tag=None, from_worker=-1):# 'pylint: disable = too-many-arguments
        """
        init DeferredOpRes
        :param gid
        :param tid
        """
        self._gid = gid
        self._tid = tid
        self._res = res
        self._tag = tag
        self._from_worker = from_worker

    def get(self):
        """get impl"""
        with CompilerEnv.env_guard():
            return self.get_impl()

    def get_impl(self):
        """
        get Op compilation result
        :return: None if still runing, others if finished
        """
        if self._res is not None:
            return self._res

        compiler = OpCompiler.compiler
        res = compiler.get_finished_task(self._gid, [self._tid])
        if len(res) == 0:
            return None
        res = res[0]
        if res is not None:
            if res.get("status_code") != 0:
                except_msg = res.get("except_msg", "")
                except_msg_escape = re.sub(r'%', '%%', except_msg)
                logger.warn("RL tune task compile fail, task_id [%d, %d], which may cause process exit, reason is %s.",
                            self._gid, self._tid, except_msg_escape)
            self._res = res
            if CompilerEnv.task_counter_modify(-1):
                restart_paralle_compiler()

        return self._res


def check_rlimit():
    """
    check rlimit
    """
    try:
        import resource as res
        rlimit = res.getrlimit(res.RLIMIT_NOFILE)
        te_nofile = 10240
        if rlimit[0] < te_nofile:
            nofile = te_nofile if te_nofile < rlimit[1] else rlimit[1]
            res.setrlimit(res.RLIMIT_NOFILE, (nofile, rlimit[1]))
    except Exception:
        logger.warn("check rlimit failed. traceback message: %s", traceback.format_exc())


def check_restart_env(need_unset_restart_env):
    """
    unset TE_AUTO_RESTART_COUNTER in non-AOE mode
    """
    if need_unset_restart_env:
        os.environ['TE_AUTO_RESTART_COUNTER'] = '0'
        logger.info("Env var after unset TE_AUTO_RESTART_COUNTER is: %s", os.getenv('TE_AUTO_RESTART_COUNTER', '0'))


# 'pylint: disable = too-many-arguments
def init_multi_process_env(embedding, soc_info, option_info, slog_level=None, slog_event=1, pid_timestamp="",
                           need_unset_restart_env=True):
    """
    init multi compilation process
    :param embedding: if is embedding python
    :param soc_info:
    :param tune_mode: useless, need remove
    :param l2mode:
    :return: compilation worker number
    """
    check_restart_env(need_unset_restart_env)
    check_rlimit()
    logger.info("pid_timestamp: %s", pid_timestamp)
    atc_time_stamp = pid_timestamp
    OpCompiler.atc_time_stamp = atc_time_stamp
    process_count = get_multi_process_count()
    logger.info("process_count is [%s]", process_count)
    if process_count <= 0:
        return 0, None, None, None

    compiler = OpCompiler(embedding, process_count, soc_info, option_info, slog_level=slog_level, slog_event=slog_event)

    # when process_count is large(e.g., 16,32), it will cause performance problem
    compiler.init()
    res = compiler.start()

    CompilerEnv.embedding = embedding
    CompilerEnv.soc_info = soc_info
    CompilerEnv.option_info = option_info
    CompilerEnv.slog_level = slog_level
    CompilerEnv.slog_event = slog_event
    CompilerEnv.pid_timestamp = pid_timestamp
    return res


def restart_paralle_compiler():
    """
    restart sub process
    """

    logger.info("@@@ restart Compiler")
    deinit_multi_process_env()
    try:
        init_multi_process_env(CompilerEnv.embedding,
                               CompilerEnv.soc_info,
                               CompilerEnv.option_info,
                               CompilerEnv.slog_level,
                               CompilerEnv.slog_event,
                               CompilerEnv.pid_timestamp)
    except Exception:       # 'pylint: disable=broad-except
        OpCompiler.status = OpCompiler.SUB_PROCESS_ERROR
        logger.error("@@@ restart failed")


def del_tmp_files_by_pid(pid):
    """
    deleate tmp files by atc pid
    :param: atc pid
    :return: None
    """
    cur_path = os.getcwd()
    tmp_list = []
    file_lock_path = os.path.join(cur_path, "file.lock")
    tmp_list.append(file_lock_path)
    file_connect_info = os.path.join(cur_path, ".temp_connect.log")
    tmp_list.append(file_connect_info)
    for path_item in Path(cur_path).glob('*pid{}*'.format(pid)):
        list_item = os.fspath(path_item)
        if "tune_result" not in list_item and "aoe_result" not in list_item:
            tmp_list.append(list_item)
    try:
        for item in tmp_list:
            if os.path.isfile(item):
                os.remove(item)
            if os.path.isdir(item):
                shutil.rmtree(item)
    except Exception:
        logger.warn("remove temp file failed. traceback message: %s", traceback.format_exc())
        pass


def del_tmp_files(atc_time_stamp):
    """
    deleate tmp files in the atc process
    :param: atc_time_stamp, the atc pid and time stamp str
    :return: None
    """
    cur_path = os.getcwd()
    tune_show_dir_name = "tune_show_{}".format(atc_time_stamp)
    lock_file_name = "file.lock"
    tune_show_dir = os.path.join(cur_path, tune_show_dir_name)
    lock_file = os.path.join(cur_path, lock_file_name)
    try:
        if os.path.isfile(lock_file):
            os.remove(lock_file)
        if os.path.isdir(tune_show_dir):
            ga_in_use = os.path.join(tune_show_dir, "GA_in_use.flag")
            if os.path.isfile(ga_in_use):
                os.remove(ga_in_use)
            in_use_flag = glob.glob(os.path.join(tune_show_dir, "*_in_use.flag"))
            if not in_use_flag:
                shutil.rmtree(tune_show_dir)
        file_connect_info = os.path.join(cur_path, ".temp_connect.log")
        if os.path.isfile(file_connect_info):
            os.remove(file_connect_info)
    except Exception:
        logger.warn("remove temp file failed. traceback message: %s", traceback.format_exc())


def deinit_multi_process_env():
    """
    deinit multi compilation process
    :return: None
    """
    del_tmp_files(OpCompiler.atc_time_stamp)
    if OpCompiler.compiler is not None:
        logger.info("destory compiler %s", OpCompiler.compiler)
        OpCompiler.compiler.destory()

    logger.info("all compiler destoryed.")


def get_finished_compilation_task(graph_id):
    """
    return finished compilation task
    :return:
    """
    with CompilerEnv.env_guard():
        compiler = OpCompiler.compiler
        if compiler is None:
            return []
        res = compiler.get_finished_task(graph_id)
        if res and CompilerEnv.task_counter_modify(-len(res)):
            restart_paralle_compiler()

        return res


# 'pylint: disable=too-many-arguments
def gen_task_res(ttype, gid, tid, status_code, result, msg, **kwds):
    """
    gen_task_res
    :return: task result
    """
    res = {
        'type': ttype,
        'graph_id': gid,
        'task_id': tid,
        'status_code': status_code,
        'result': result,
        'info_msg': msg,
    }

    for key, value in kwds.items():
        res[key] = value

    return res


def check_task_succ(task_res: dict) -> bool:
    """
    check if task is successful by task result
    """
    if task_res.get("status_code", -1) == 0:
        return True
    return False


# 'pylint: disable=too-many-instance-attributes
class TaskDispatcher:
    """
    Task Dispatcher
    """

    def __init__(self, task_env):
        """
        init
        :param task_env:
        :param worker_list:
        """
        self._task_queue, \
            self.fin_task_queue, \
            self._data_queue, \
            = task_env
        self._data_sync_count = {}
        self._concurrent = 0

    def get_result(self, block=True, queue_idx=-1):
        """
        get result form finished task queue
        :param block:
        :return:
        """
        fin_queue = self.fin_task_queue
        if queue_idx >= 0:
            fin_queue = fin_queue[queue_idx]
        task = fin_queue.get(block)
        self._concurrent -= 1
        return task

    def dispatch(self, task):
        """
        dispatch task to compilation worker
        :param task:
        :return:
        """
        tqueue = self._task_queue
        task.set_data_sync_count(self._data_sync_count)
        tqueue.put(task, True)
        self._concurrent += 1

    def sync_data(self, data_task, pidfrom=0):
        """
        sync data to compilation worker
        :param data_task:
        :return:
        """
        data_task.pidfrom = pidfrom
        for dqueue in self._data_queue:
            dqueue.put(data_task, True)
        count = self._data_sync_count.setdefault(pidfrom, 0)
        self._data_sync_count[pidfrom] = count + 1


# 'pylint: disable=too-many-instance-attributes
class TaskWorker:
    """
    Task runner
    """

    def __init__(self, task_env, pid):
        """
        init
        :param task_env:
        """
        self._task_queue, \
            self.fin_task_queue, \
            self._data_queue, \
            self._live_checker,\
            self._time_stamp = task_env
        self._block_timeout = 2
        self._data_synced = {}
        self._start = None
        self._end = None
        self._delta = datetime.timedelta()
        self._count = 0
        self._dict = {}
        self._master_pid = pid

    def add_dict(self, kwargs):
        if kwargs:
            self._dict = kwargs

    def get_dict(self):
        return self._dict

    def do_sync_data(self, block=False, timeout=2):
        """
        load synced data from dispatcher process
        :param block:
        :param timeout:
        :return:
        """
        data_task = self._data_queue.get(block, timeout)
        data_task.run()
        pidfrom = data_task.pidfrom
        count = self._data_synced.setdefault(pidfrom, 0)
        self._data_synced[pidfrom] = count + 1

    def try_sync_data(self):
        """
        try sync data non-blocking
        :return:
        """
        # sync as much as possible
        try:
            while True:
                self.do_sync_data()
        except queue.Empty:
            return

    def mandatory_sync_data(self, need_sync):
        """
        sync exactly count data
        :param count:
        :return:
        """
        # sync exactly 'count' data
        # if there's no enough data, raise exception
        try:
            for pidfrom, count in need_sync.items():
                while count - self._data_synced.get(pidfrom, 0) > 0:
                    self.do_sync_data(True, 60)
        except queue.Empty:
            logger.warn("syncing mandatory data failed. count: %d/%d",
                           count, self._data_synced)

    def loop(self):# 'pylint: disable = too-many-statements
        """
        main loop
        :return:
        """
        def _update_options_value(options_key, options):# 'pylint: disable = inconsistent-return-statements
            if options_key in options:
                return options[options_key]

        def _update_options(task, kwargs):# 'pylint: disable = too-many-locals, too-many-branches
            l1_fusion = (get_L1_info("L1_fusion_enabled") == 1)
            l2_fusion = (get_L1_info("L2_fusion_enabled") == 1)
            l2_mode = get_current_build_config("l2_mode")
            op_debug_dir = get_current_build_config("kernel_meta_parent_dir")
            vector_fp_ceiling = get_current_build_config("vector_fp_ceiling")

            kwargs['op_debug_dir'] = op_debug_dir
            kwargs['vector_fp_ceiling'] = vector_fp_ceiling
            kwargs['l2_mode'] = l2_mode
            if hasattr(task, 'options') and task.options is not None and 'socVersion' in task.options:
                soc_version = task.options["socVersion"]
                core_type = "AiCore"
                core_num = None
                if 'coreType' in task.options:
                    core_type = task.options["coreType"]
                if 'coreNum' in task.options:
                    core_num = task.options["coreNum"]
                if 'l1Fusion' in task.options:
                    l1_fusion = task.options["l1Fusion"]
                if 'l2Fusion' in task.options:
                    l2_fusion = task.options["l2Fusion"]
                if 'l2Mode' in task.options:
                    l2_mode = task.options["l2Mode"]
                if 'op_debug_dir' in task.options:
                    if task.options["op_debug_dir"] is not None:
                        op_debug_dir = task.options["op_debug_dir"]
                if 'vector_fp_ceiling' in task.options:
                    vector_fp_ceiling = task.options["vector_fp_ceiling"]
                kwargs["mdl_bank_path"] = _update_options_value("mdl_bank_path", task.options)
                kwargs["op_bank_path"] = _update_options_value("op_bank_path", task.options)
                kwargs['op_debug_dir'] = op_debug_dir
                kwargs['vector_fp_ceiling'] = vector_fp_ceiling
                kwargs['l2_mode'] = l2_mode
                check_valid_core_type(task.build_type, core_type)
                set_kernel_meta_parent_dir(kwargs)
                # 'pylint: disable=logging-too-many-args
                logger.info("new options{[soc_version:%s], [core_type:%s]}, [core_num:%s], [l1_fusion:%s]}, "
                            "[l2_fusion:%s], [l2_mode:%s]}",
                            soc_version, core_type, core_num, l1_fusion, l2_fusion, l2_mode)
                te_update_version(soc_version, core_type, core_num, l1_fusion,
                                  l2_mode, l2_fusion, kwargs)

            set_l1_l2_fusion_enabled(l1_fusion, l2_fusion, kwargs)

        def _save_task_pid_infos(task):
            kernel_temp = self._dict.get("kernel_meta_temp_dir", None)
            if kernel_temp is None:
                logger.warn("kernel_meta_temp is not set!")
                return
            cur_pid = os.getpid()
            task_pid_json = kernel_temp + "/" + "task_pid_" + str(self._master_pid) + "_" + str(cur_pid) + ".txt"
            if not os.path.exists(task_pid_json):
                if not os.path.exists(kernel_temp):
                    logger.warn("kernel_meta_temp[%s] is not exist, cur task will has no trace info.", kernel_temp)
                    return
                f = open(task_pid_json, "w+", encoding='utf-8')
                f.close()

            with open(task_pid_json, "r+", encoding='utf-8') as f:
                f.seek(0, 2)
                task_pid_info = str(task.graph_id) + "." + str(task.task_id) + "\n"
                f.write(task_pid_info)

        def _prerun(task, kwargs):
            """
            set l1 size and python env parameter before run
            """
            _save_task_pid_infos(task)
            _update_options(task, kwargs)
            # set l1 size
            set_L1_info("op_L1_space", task.l1size)
            logger.info("set l1 size %s, %s", task.l1size, task)
            # set Compile Strategy
            if hasattr(task, "pre_task") and task.pre_task is not None and "module_name" in task.pre_task:
                pymodule = importlib.import_module(task.pre_task["module_name"])
                task.post_task["object_value"] = mygetattr(pymodule, task.pre_task["object_name"])
                kernel_name = ""
                if hasattr(task, "get_kernel"):
                    kernel_name = task.get_kernel()
                logger.info("get Compile Strategy %s, kernel name [%s]", task.post_task["object_value"], kernel_name)
                mysetattr(pymodule, task.pre_task["object_name"], task.pre_task["object_value"])
                logger.info("set Compile Strategy %s, kernel name [%s]", task.pre_task["object_value"], kernel_name)

        def _postrun(task):
            """
            restore l1 size after run
            """
            if task.l1size >= 0:
                set_L1_info("op_L1_space", -1)  # reset l1 space
            logger.info("restore l1 size. %s, %s", task.l1size, task)
            # reset Compile Strategy
            if hasattr(task, "post_task") and task.post_task is not None and "module_name" in task.post_task:
                pymodule = importlib.import_module(task.post_task["module_name"])
                mysetattr(pymodule, task.post_task["object_name"], task.post_task["object_value"])
                logger.info("reset Compile Strategy %s", task.post_task["object_value"])

        optask_dispatcher = OpCompiler.task_dispatcher
        while True:
            try:
                # check dispatcher process is alive
                if self._live_checker.poll():
                    self._live_checker.recv()

            except EOFError:
                logger.warn("Master process dead. worker process quiting..")
                # Avoid 'Broken PIPE' exception msg of multiprocessing module,
                # we are quiting anyway.
                sys.excepthook = excepthook_silent
                break

            # CAUTION: task dispatcher MUST dispatch data_sync task first
            try:
                task = self._task_queue.get(True, self._block_timeout)
            except queue.Empty:
                task = None

            if task is None:
                self.try_sync_data()
                continue

            if self._start is None:
                self._start = datetime.datetime.now()

            count = task.check_need_sync()
            self.mandatory_sync_data(count)

            kwargs = self.get_dict()
            _prerun(task, kwargs)
            OpImplPolicy.bank_path_kwargs = kwargs
            logger.info("compile_options when running task:%s", kwargs)
            cce_dump_key = kwargs.get('op_debug_level', '0') == '4'
            debug_config = ""
            if task.options:
                debug_config = task.options.get("op_debug_config", "")
            with build_config(tbe_debug_level = get_tbe_debug_level(kwargs.get('op_debug_level', '0')),
                              l2_mode = int(kwargs['l2_mode']),
                              kernel_meta_parent_dir = kwargs['op_debug_dir'],
                              op_debug_config = debug_config,
                              vector_fp_ceiling = int(kwargs['vector_fp_ceiling']),
                              enable_L1_fusion = kwargs["enable_L1_fusion"],
                              enable_L2_fusion = kwargs["enable_L2_fusion"],
                              save_temp_cce_file = cce_dump_key,
                              compatible = True):

                if self._time_stamp:
                    logger.info("start task.run,%s", task)
                    res = task.run(self._time_stamp)
                    logger.info("end task.run,%s", task)
                else:
                    logger.info("start task.run,%s", task)
                    res = task.run()
                    logger.info("end task.run,%s", task)
                _postrun(task)

            self._count += 1
            task_tag = getattr(task, "tag", None)
            if res is None:
                continue

            if task_tag is None:
                self.fin_task_queue.put(res)
                logger.info("%s has been added to fin_task_queue,finished", task)
            elif task_tag == "autotune":
                logger.info("autotune task %s", task.desc())
                fin_queue = optask_dispatcher.fin_task_queue
                fin_queue.put(res)
                logger.info("%s has been added to fin_task_queue,finished", task)

            self._end = datetime.datetime.now()


class OpTask:
    """
    Base class of various parallel task
    """

    def __init__(self, timeout_ms=2000):
        self._timeout_ms = timeout_ms
        self._data_sync_count = {}
        self.l1size = -1
        self.res = []
        self.pidfrom = 0

    def check_need_sync(self, pidfrom=None):
        """
        check if need to sync data before do this op task
        :return:
        """
        if pidfrom is None:
            return self._data_sync_count

        return self._data_sync_count.get(pidfrom, 0)

    def set_data_sync_count(self, count):
        """
        set the exactly number of data need to sync
        :param count:
        :return:
        """
        self._data_sync_count = count

    def set_l1size(self, l1size):
        """
        set l1 size when compile op
        :param count:
        :return:
        """
        self.l1size = l1size

    def run(self):
        """
        should overide in sub class
        """


class ObjSyncTask(OpTask):
    """
    Task to sync module objects from parent to child process.
    """

    def __init__(self, module_name, obj_name, obj_value):
        """
        init
        :param module_name:
        :param obj_name:
        :param obj_value:
        """
        super().__init__()
        self._module_name = module_name
        self._obj_name = obj_name
        self._obj_value = obj_value

    def run(self):
        """
        do the data sync
        :return:
        """
        pymodule = importlib.import_module(self._module_name)
        obj = pickle.loads(zlib.decompress(self._obj_value))
        mysetattr(pymodule, self._obj_name, obj)


class SingleOpTask(OpTask):
    """
    Task to compile single tbe op
    """

    # 'pylint: disable=too-many-arguments, too-many-locals
    def __init__(self, graph_id, task_id, op_module, op_type, op_func, kernel_name, *op_args,
                 inputs=None, outputs=None, attrs=None, private_attrs=None, unknown_shape=False, int64_mode=False,
                 options=None, pre_task=None, post_task=None, is_dynamic_impl=False, op_pattern=None,
                 fuzz_build_info=None, reset_op_info=None, context_param=None, pass_opt_list=None,
                 tune_param=None, master_pid=None, op_name=None, extra_params=None, relation_param=None,
                 op_impl_switch=None, op_impl_mode=None, optional_input_mode=None, enable_superkernel_plus=None,
                 superkenel_sub_info="", dynamic_param_mode=None, binary_compile_mode=False):
        """
        init
        :param graph_id:
        :param task_id:
        :param op_module:
        :param op_func:
        :param kernel_name:
        :param op_args:
        """
        super().__init__()
        self.graph_id = graph_id
        self.task_id = task_id
        self._op_module = op_module
        self._op_type = op_type
        self._op_func = op_func
        self._op_type = op_type
        self._kernel_name = kernel_name
        self._op_args = op_args
        self._op_inputs = inputs
        self._op_outputs = outputs
        self._op_attrs = attrs
        self._op_private_attrs = private_attrs
        self.build_type = 1
        self._unknown_shape = unknown_shape
        self._int64mode = int64_mode
        self.options = options
        self.pre_task = pre_task
        self.post_task = post_task
        self._is_dynamic_impl = is_dynamic_impl
        self._op_pattern = op_pattern
        self._device_id = self.options.get("device_id") if self.options is not None else None
        self._fuzz_build_info = fuzz_build_info
        self._tune_param = tune_param
        self._reset_op_info = reset_op_info
        self._context_param = context_param
        self._pass_opt_list = pass_opt_list
        self._master_pid = master_pid
        self._op_name = op_name
        self._extra_params = extra_params
        self._relation_param = relation_param
        self._op_impl_switch = op_impl_switch
        self._op_impl_mode = op_impl_mode
        self._optional_input_mode = optional_input_mode
        self._enable_superkernel_plus = enable_superkernel_plus
        self._superkenel_sub_info = superkenel_sub_info
        self._dynamic_param_mode = dynamic_param_mode
        self._binary_compile_mode = binary_compile_mode

    def __str__(self):
        """
        string representation
        :return:
        """
        # 'pylint: disable=too-many-locals
        return "taskID[{}.{}]".format(self.graph_id, self.task_id)

    def get_kernel(self):
        """
        get task kernel name
        """
        return self._kernel_name

    def run(self):
        """
        do single op compilation
        :return:
        """
        try:
            set_soc_info_before_compile(self.options)
            start = datetime.datetime.now()
            build_res = build_single_op(self._op_module, self._op_func,
                                        self._op_type, "build",
                                        *self._op_args,
                                        inputs=self._op_inputs,
                                        outputs=self._op_outputs,
                                        attrs=self._op_attrs,
                                        private_attrs=self._op_private_attrs,
                                        options=self.options,
                                        unknown_shape=self._unknown_shape,
                                        int64_mode=self._int64mode,
                                        is_dynamic_impl=self._is_dynamic_impl,
                                        op_pattern=self._op_pattern,
                                        device_id=self._device_id,
                                        fuzz_build_info=self._fuzz_build_info,
                                        reset_op_info=self._reset_op_info,
                                        context_param=self._context_param,
                                        pass_opt_list=self._pass_opt_list,
                                        tune_param=self._tune_param,
                                        master_pid=self._master_pid,
                                        op_name=self._op_name,
                                        extra_params=self._extra_params,
                                        relation_param=self._relation_param,
                                        op_impl_switch=self._op_impl_switch,
                                        op_impl_mode=self._op_impl_mode,
                                        optional_input_mode=self._optional_input_mode,
                                        enable_superkernel_plus=self._enable_superkernel_plus,
                                        superkenel_sub_info=self._superkenel_sub_info,
                                        dynamic_param_mode=self._dynamic_param_mode,
                                        binary_compile_mode=self._binary_compile_mode)
            end = datetime.datetime.now()
            infomsg = "single op compile success. kernel[{}] "\
                "module[{}] func[{}], time:{}/{}"\
                .format(self._kernel_name, self._op_module,
                        self._op_func, start, end-start)

            if build_res is not None:
                op_res_dict = build_res
            else:
                op_res_dict = {}
            return gen_task_res(self.build_type, self.graph_id,
                                self.task_id, 0, "success", infomsg, op_res=json.dumps(op_res_dict))
        except Exception:       # 'pylint: disable=broad-except
            except_msg, except_list_msg = te_log.except_msg()
            errmsg = "do single op compile failed. kernel[{}] "\
                "module[{}] func[{}]"\
                .format(self._kernel_name, self._op_module,
                        self._op_func)
            te_log.tefusion_log_full(te_log.LogLevel.ERROR, "%s. args: [%s].\n", errmsg, self._op_args)
            return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                1, "exception", errmsg,
                                err_args="args:{}, input:{}, outputs:{}, attrs:{}"
                                .format(self._op_args,
                                        self._op_inputs,
                                        self._op_outputs,
                                        self._op_attrs),
                                except_msg=except_msg,
                                except_tuple_msg=(except_list_msg,
                                                  self._op_func))

    def desc(self):
        """
        task description in json format
        """
        op_desc = {
            "type:": "single build",
            "module": self._op_module,
            "kernel_name": self._kernel_name,
            "args": self._op_args,
            "task_infos": "args:{}, input:{}, outputs:{}, attrs:{}"
            .format(self._op_args,
                    self._op_inputs,
                    self._op_outputs,
                    self._op_attrs)
        }
        return json.dumps(op_desc)


class FusionOpTask(OpTask):
    """
    Task to compile fusion op
    """

    def __init__(self, graph_id, task_id, json_str, kernel_name, pre_task=None, post_task=None, options=None,
                 reset_op_info=None, context_param=None, pass_opt_list=None, master_pid=None, soc_options=None,
                 op_name=None, reused_relation="", fixpipe_ub_cfg="", optional_input_mode=None,
                 dynamic_param_mode=None, binary_compile_mode=False):
        """
        init
        :param graph_id:
        :param task_id:
        :param json_str:
        :param kernel_name:
        :param pre_task:
        :param post_task:
        :param options:
        :param reset_op_info:
        :param context_param:
        :param pass_opt_list:
        :param master_pid:
        :param soc_options:
        :param op_name:
        :return:
        """
        super().__init__(self)
        self.graph_id = graph_id
        self.task_id = task_id
        self._json_str = json_str
        self._kernel_name = kernel_name
        self.build_type = 2
        self.tag = None
        self.op_env_cfg = None
        self.from_worker = -1
        self.pre_task = pre_task
        self.post_task = post_task
        self._reset_op_info = reset_op_info
        self.options = options
        self._context_param = context_param
        self._pass_opt_liost = pass_opt_list
        self._master_pid = master_pid
        self._soc_options = soc_options
        self._op_name = op_name
        self._reused_relation = reused_relation
        self._fixpipe_ub_cfg = fixpipe_ub_cfg
        self._optional_input_mode = optional_input_mode
        self._dynamic_param_mode = dynamic_param_mode
        self._binary_compile_mode = binary_compile_mode

    def __str__(self):
        """
        string representation
        :return:
        """
        return "taskID[{}.{}]".format(self.graph_id, self.task_id)

    def run(self):
        """
        do fusion op compilation
        :return:
        """
        try:
            if self._soc_options is None:
                set_soc_info_before_compile(self.options)

            else:
                set_soc_info_before_compile(self._soc_options)
            if self.options is None:
                op_desc = json.loads(self._json_str)
                status_check = op_desc["SocInfo"].get("status_check", "").lower() != "false"
            else:
                status_check = self.options.get('status_check', '').lower() != "false"
            start = datetime.datetime.now()
            op_build_cfg_en()

            relation_list = self._reused_relation.split()
            relation_list = [int(i) for i in relation_list]

            json_data = json.loads(self._json_str)
            op_list = json_data["op_list"]
            ddr_base_prop_vec = []
            all_input_desc_name = []
            for op_node in op_list:
                if op_node["type"] == "Data" and op_node.get("output_desc") is not None:
                    for output_desc in op_node["output_desc"]:
                        if output_desc.get("ddr_base_prop") is not None:
                            ddr_base_prop_vec.append(output_desc.get("ddr_base_prop"))
                            output_desc.pop("ddr_base_prop")
                if op_node.get("input_desc") is not None:
                    for input_desc in op_node["input_desc"]:
                        all_input_desc_name.append(input_desc["name"])

            for op_node in op_list:
                if op_node["type"] != "Data" and op_node.get("output_desc") is not None:
                    for output_desc in op_node["output_desc"]:
                        if output_desc.get("name") not in all_input_desc_name \
                            and output_desc.get("ddr_base_prop") is not None:
                            ddr_base_prop_vec.append(output_desc.get("ddr_base_prop"))
                            output_desc.pop("ddr_base_prop")
            if not self._binary_compile_mode:
                trans_bool_to_int8_op_list(json_data["op_list"])
            deterministic = True if (self.options and self.options.get('deterministic') == 'true') else False
            enable_superkernel_plus = json_data.get("enable_superkernel_plus", "")
            is_enable_model_fusion = enable_superkernel_plus == "true"
            is_enable_vector_core = json_data["SocInfo"].get("enable_vector_core")
            with build_config(enable_op_prebuild=False, compatible=True, status_check=status_check,
                              dummy_placeholder=False, inplace_output_tensor_index = relation_list,
                              ddr_base_property=ddr_base_prop_vec, enable_deterministic_mode=deterministic,
                              enable_model_fusion=is_enable_model_fusion, enable_vector_core=is_enable_vector_core):

                op_run_env_func(self.op_env_cfg, "prerun")
                logger.info("kernel[{}] start fusion_op".format(self._kernel_name))
                build_res = fusion_op(self._json_str, False, self._reset_op_info,
                                      self._context_param, self._pass_opt_liost,
                                      self._master_pid, status_check, self._op_name, self._fixpipe_ub_cfg,
                                      self._optional_input_mode, self._dynamic_param_mode)
                post_res = op_run_env_func(self.op_env_cfg, "postrun")
                end = datetime.datetime.now()
                infomsg = "fusion op compile success. "\
                    "kernel[{}], time:{}/{}"\
                    .format(self._kernel_name, start, end-start)

                if build_res is not None:
                    op_res_dict = build_res
                else:
                    op_res_dict = {}
                return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                    0, "success", infomsg, op_res=json.dumps(op_res_dict),
                                    post_res=post_res)
        except Exception:       # 'pylint: disable=broad-except
            except_msg, except_list_msg = te_log.except_msg()
            errmsg = "do fusion op compile fail. kernel_name[{}]"\
                .format(self._kernel_name)
            te_log.tefusion_log_full(te_log.LogLevel.WARN, "%s. json: [%s].\n", errmsg, self._json_str)
            op_run_env_func(self.op_env_cfg, "postrun")
            return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                1, self._kernel_name, errmsg,
                                err_args="json_str:{}".format(self._json_str),
                                except_msg=except_msg,
                                except_tuple_msg=(except_list_msg,
                                                  self._kernel_name))

    def desc(self):
        """
        task description in json format
        """
        op_desc = {
            "type:": "fusion build",
            "kernel_name": self._kernel_name,
            "task_infos": self._json_str
        }
        return json.dumps(op_desc)

    def get_kernel(self):
        """
        get task kernel name
        """
        return self._kernel_name


class TaskFusionTask(OpTask):
    """
    Task to support task fusion tasks
    """

    def __init__(self, graph_id, task_id, json_str, options=None, soc_options=None):
        super().__init__(self)
        self.graph_id = graph_id
        self.task_id = task_id
        self._json_str = json_str
        self.build_type = 3
        self.options = options
        self._soc_options = soc_options
        self.op_env_cfg = None

    def __str__(self):
        """
        string representation
        :return:
        """
        return "taskID[{}.{}]".format(self.graph_id, self.task_id)

    def run(self):
        """
        do task fusion
        :return:
        """

        try:
            set_soc_info_before_compile(self.options)

            start = datetime.datetime.now()
            op_run_env_func(self.op_env_cfg, "prerun")
            build_res = compile_kernel_fusion(self._json_str)
            post_res = op_run_env_func(self.op_env_cfg, "postrun")
            end = datetime.datetime.now()
            infomsg = "run task fusion success. "\
                "task_id[{}], time:{}/{}"\
                .format(self.task_id, start, end-start)

            if build_res is not None:
                op_res_dict = build_res
            else:
                op_res_dict = {}
            return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                0, "success", infomsg, op_res=json.dumps(op_res_dict),
                                post_res=post_res)

        except Exception:       # 'pylint: disable=broad-except
            except_msg, except_list_msg = te_log.except_msg()
            errmsg = "run task fusion fail. task_id:[{}], graph_id[{}]".format(self.task_id, self.graph_id)
            te_log.tefusion_log_full(te_log.LogLevel.WARN, "%s. json: [%s].\n", errmsg, self._json_str)
            op_run_env_func(self.op_env_cfg, "postrun")
            return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                1, "", errmsg,
                                err_args="json_str:{}".format(self._json_str),
                                except_msg=except_msg,
                                except_tuple_msg=(except_list_msg, ""))


    def desc(self):
        """
        task description in json format
        """
        op_desc = {
            "type:": "task fusion",
            "task id": self.task_id,
            "json_info": self._json_str
        }
        return json.dumps(op_desc)


def sync_py_object(module_name, obj_name, has_value=False, obj_value=None):
    """
    sync python object to worker process
    :param module_name:
    :param obj_name:
    """
    if OpCompiler.master_pid == 0:
        return
    if has_value:
        obj_value = zlib.compress(pickle.dumps(obj_value))
        task = ObjSyncTask(module_name, obj_name, obj_value)
    else:
        opm = importlib.import_module(module_name)
        obj = mygetattr(opm, obj_name)
        obj = zlib.compress(pickle.dumps(obj))
        task = ObjSyncTask(module_name, obj_name, obj)

    if OpCompiler.master_pid == os.getpid():
        OpCompiler.compiler.sync_data(task)
    else:
        # This is autotune sub process
        pid = os.getpid()
        OpCompiler.task_dispatcher.sync_data(task, pidfrom=pid)


def sync_py_object_serial(module_name, obj_name, obj_value):
    """
    sync python object
    :param module_name:
    :param obj_name:
    :param obj_value:
    """
    pymodule = importlib.import_module(module_name)
    mysetattr(pymodule, obj_name, obj_value)


def op_run_env_func(op_env_cfg, func_name):# 'pylint: disable=inconsistent-return-statements
    """
    run op environment setting function
    """
    if op_env_cfg is None:
        return
    prerun = op_env_cfg.get(func_name)
    if prerun is None:
        return
    try:
        args = []
        kwargs = {}
        if len(prerun) > 1:
            args = prerun[1]
        if len(prerun) > 2:
            kwargs = prerun[2]

        return prerun[0](*args, **kwargs)
    except Exception:   # 'pylint: disable=broad-except
        except_msg, _ = te_log.except_msg()
        logger.warn("%s", except_msg)
    return


def compile_op_at_curr_thrd(json_str: str, op_env_cfg: Optional[dict]) -> DeferredOpRes:
    """
    compile op at current thread synchronously
    """
    op_desc = json.loads(json_str)
    set_soc_info_before_compile(op_desc["SocInfo"])
    kernel_name = op_desc["fusion_op_name"]
    if op_desc.get('l1Fusion', 'false') == 'false':
        l1_fusion_flag = False
    else:
        l1_fusion_flag = True
    if op_desc.get('l2Fusion', 'false') == 'false':
        l2_fusion_flag = False
    else:
        l2_fusion_flag = True
    l2_mode_flag = int(op_desc.get('l2Mode', '0'))
    op_build_cfg_en()
    gid = os.getpid()
    tid = Counter.next()

    with build_config(
        enable_L1_fusion=l1_fusion_flag,
        l2_mode=l2_mode_flag,
        enable_L2_fusion=l2_fusion_flag,
        enable_op_prebuild=False,
        compatible=True
    ):
        try:
            op_run_env_func(op_env_cfg, "prerun")
            build_res = fusion_op(json_str, reset_op_info=OpCompiler.reset_op_info, master_pid=gid)
            post_res = op_run_env_func(op_env_cfg, "postrun")

            if build_res is not None:
                op_res_dict = build_res
            else:
                op_res_dict = {}
            res = gen_task_res(2, gid, tid, 0, "success", "syncbuild succ", \
                op_res=json.dumps(op_res_dict), post_res=post_res)
        except Exception:   # 'pylint: disable=broad-except
            except_msg, _ = te_log.except_msg()
            errmsg = "compile op fail. kernel[{}]".format(kernel_name)
            logger.warn("%s. json:%s\n%s", errmsg, json_str, except_msg)
            op_run_env_func(op_env_cfg, "postrun")
            res = gen_task_res(2, gid, tid, 1, "exception", "syncbuild faild")
        finally:
            pass
        return DeferredOpRes(gid, tid, res)


def compile_op_async(json_str: str, op_env_cfg: Optional[dict]) -> DeferredOpRes:
    """
    compile op by dispatch a compilation task
    """
    op_desc = json.loads(json_str)
    soc_options = op_desc["SocInfo"]
    gid = os.getpid()
    tid = Counter.next()
    kernel_name = op_desc['fusion_op_name']
    reset_op_info = OpCompiler.reset_op_info
    if OpCompiler.master_pid == gid:
        # invoke from parent process
        task = FusionOpTask(1, tid, json_str, kernel_name, reset_op_info=reset_op_info,
                            soc_options=soc_options, master_pid=OpCompiler.master_pid)
        task.op_env_cfg = op_env_cfg
        OpCompiler.compiler.dispatch_task(task)
        res = DeferredOpRes(1, tid)
        CompilerEnv.task_counter_modify(1)
        return res


def compile_op(json_str: str, op_env_cfg: Optional[dict] = None) -> DeferredOpRes:
    """
    compile op parallelly
    """
    with CompilerEnv.env_guard():
        if OpCompiler.master_pid == 0:
            # parallel compiler not active
            return compile_op_at_curr_thrd(json_str, op_env_cfg)

        # call compile op by dispatch a compilation task
        return compile_op_async(json_str, op_env_cfg)


def update_running_task(task):
    """
    update runing task status
    """
    if OpCompiler.compiler is None:
        return
    OpCompiler.compiler.update_running_task(task)


def start_ga_multi_process(tune_mode):
    """
    start ga multi process need remove
    """
    pass


class SuperKernelTask(OpTask):
    """
    Task to support task fusion tasks
    """

    def __init__(self, graph_id, task_id, json_str, options=None):
        super().__init__(self)
        self.graph_id = graph_id
        self.task_id = task_id
        self._json_str = json_str
        self.build_type = 4
        self.op_env_cfg = None
        self.options = options

    def __str__(self):
        """
        string representation
        :return:
        """
        return "taskID[{}.{}]".format(self.graph_id, self.task_id)

    def run(self):
        """
        do task fusion
        :return:
        """

        try:
            start = datetime.datetime.now()
            op_run_env_func(self.op_env_cfg, "prerun")
            build_res = compile_super_kernel(self._json_str)
            post_res = op_run_env_func(self.op_env_cfg, "postrun")
            end = datetime.datetime.now()
            infomsg = "run super kernel success. "\
                "task_id[{}], time:{}/{}"\
                .format(self.task_id, start, end-start)

            if build_res is not None:
                op_res_dict = build_res
            else:
                op_res_dict = {}
            return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                0, "success", infomsg, op_res=json.dumps(op_res_dict),
                                post_res=post_res)

        except Exception:       # 'pylint: disable=broad-except
            except_msg, except_list_msg = te_log.except_msg()
            errmsg = "run super kernel fail. task_id:[{}], graph_id[{}]".format(self.task_id, self.graph_id)
            te_log.tefusion_log_full(te_log.LogLevel.WARN, "%s. json: [%s].\n", errmsg, self._json_str)
            op_run_env_func(self.op_env_cfg, "postrun")
            return gen_task_res(self.build_type, self.graph_id, self.task_id,
                                1, "", errmsg,
                                err_args="json_str:{}".format(self._json_str),
                                except_msg=except_msg,
                                except_tuple_msg=(except_list_msg, ""))


    def desc(self):
        """
        task description in json format
        """
        op_desc = {
            "type:": "task fusion",
            "task id": self.task_id,
            "json_info": self._json_str
        }
        return json.dumps(op_desc)