#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Copyright 2019-2020 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
fusion template
"""
# 'pylint: disable=too-many-lines,import-error, ungrouped-imports
import os
import sys
import stat
import inspect
import json
import traceback
import importlib
import copy

from te_fusion.fusion_manager import fusion_manager as fusion_manager_obj
from te_fusion.fusion_manager import op_build_cfg_en
from te_fusion.fusion_manager import reset_fusion_build_cfg
from te_fusion.fusion_manager import get_fusion_build_cfg
from te_fusion.fusion_manager import set_lic_pass_opt_list
from te_fusion.fusion_manager import update_compile_info

from te_fusion.fusion_pass_handler import handle_fusion_pass

from te_fusion.reduce_classify_fusion import ReduceClassifyFusion
from te_fusion.common_classify_fusion import CommonClassifyFusion
from te_fusion.norm_classify_fusion import NormClassifyFusion
from te_fusion.cube_classify_fusion import CubeClassifyFusion
from te_fusion import log_util as te_log

from tbe.dsl.unify_schedule.constants import Pattern
from tbe.dsl import auto_schedule
from tbe.tvm import save_json as tvm_save_json
from tbe.common.utils import shape_util
from tbe.common import context
from tbe.common.utils.op_util import op_util_conv2d
from tbe.common.utils.const import WEIGHT_SPARSE_4_2
from tbe.common.platform import platform_info
import tbe.common.utils.log as logger

bool_storage_as_1bit_oplist = \
            ["Asinh", "Atanh", "Acosh", "Asin", "Atan2", "Acos", "Pow", "Xlogy",
             "ApproximateEqual", "DataFormatDimMap", "Elu", "Select", "SelectV2",
             "BNLL", "ClipByNormNoDivSum", "BesselI1e", "Expm1", "Log1p"]
enable_branch_eliminator_oplist = ("Conv2DBackpropInput",)

MAX_OP_NAME_LEN = 512


class OpImplPolicy:
    bank_path_kwargs = {}
    op_impl_mode_dict = {}


def get_op_impl_mode_args(op_func, op_impl_mode):
    if op_impl_mode is not None and len(op_impl_mode) > 0:
        impl_mode_arg = inspect.signature(op_func).parameters.get('impl_mode', None)
        if impl_mode_arg is not None and \
                impl_mode_arg.kind in (inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
            return {'impl_mode': op_impl_mode}
    return {}


def get_tbe_debug_level(op_debug_level):
    tbe_debug_level = 0
    if op_debug_level is None or op_debug_level == "":
        return tbe_debug_level
    tbe_debug_level = int(op_debug_level)
    if tbe_debug_level == 3 or tbe_debug_level == 4:
        return 0
    else:
        return tbe_debug_level


def add_context_param(context, param_dict):
    """
    add conetxt param
    """
    if context is None or param_dict is None:
        return
    for key in param_dict.keys():
        context.add_addition(key, param_dict[key])


def get_extra_params(extra_params):
    extra_params_dict = {}
    try:
        extra_params_dict = json.loads(extra_params)
    except Exception:       # pylint: disable=bare-except,broad-except
        pass

    if isinstance(extra_params_dict, dict):
        return extra_params_dict
    return {}


def aipp_format_change(op_node, op_list):
    """
    specific case for conv2d_compress placeholder
    :param data_node:
    :param op_list:
    """
    for op_operator in op_list:
        try:
            if op_operator["type"] != "Aipp":
                continue

            aipp_input = op_operator["input_desc"]
            if op_node['name'] != aipp_input[0]['name']:
                continue

            desc = op_node["output_desc"][0]

            for op_f, op_s in (('format', 'shape'),
                               ('ori_format', 'ori_shape')):
                if desc.get(op_f) == "NHWC":
                    desc[op_f] = "NCHW"
                    desc[op_s] = [desc[op_s][0], desc[op_s][3],
                                  desc[op_s][1], desc[op_s][2]]
        except Exception:       # 'pylint: disable=broad-except
            continue


def compress_node(op_node, op_list):
    """
    specific case for conv2d_compress placeholder
    :param data_node:
    :param op_list:
    """
    alg_conv2d_compress = 6 # alg_conv2d_compress index in attrs
    alg_matmul_compress = 3
    for op_operator in op_list:
        # check wether op_operator['attr_desc'][alg_conv2d_compress] exist or not
        if op_operator["type"] == "Conv2DCompress" and len(op_operator['attr_desc']) >= alg_conv2d_compress + 1:
            if op_operator['attr_desc'][alg_conv2d_compress] == op_util_conv2d.WEIGHT_SPARSE_4_2:
                continue
        if  op_operator["type"] == "MatMulV2Compress" and len(op_operator['attr_desc']) >= alg_matmul_compress + 1:
            if op_operator['attr_desc'][alg_matmul_compress] == WEIGHT_SPARSE_4_2:
                continue
        if op_operator["type"] != "Conv2DCompress" and \
                op_operator["type"] != "FullyConnectionCompress" and \
                op_operator["type"] != "MatMulV2Compress":
            continue
        compress_index_input = op_operator["input_desc"]
        try:
            if op_node["name"] != compress_index_input[2]["name"]:
                continue
        except Exception:       # 'pylint: disable=broad-except
            logger.warn("Caught exception from the option_node: %s", op_node["name"])
            continue

        from tbe import tvm
        compress_index_shape = tvm.var("compress_index_shape", dtype="int32")
        compress_index = tvm.placeholder((compress_index_shape,),
                                         name='compress_index', dtype="int8")
        return compress_index

    return None


def _fresh_shape_and_attr(desc, attr):
    """
    when dw transdata fusion node, fuse the h and w dim of trans_data_input
    """
    if desc.get("input_pattern") == "dw_transdata":
        attr["shape"] = desc["shape"]
        desc["shape"] = [desc["shape"][0], desc["shape"][1], desc["shape"][2] * desc["shape"][3]]
    return desc, attr


def update_const_value_by_null_desc(desc):
    '''
    "const_value": [
        1.0,
        null,
        null,
        null
    ],
    "const_value_null_desc": [
        null,
        "nan",
        "inf",
        "-inf"
    ],
    '''
    const_value_null_desc = desc.get("const_value_null_desc", None)
    #update null value if necessary
    if const_value_null_desc:
        logger.debug("const_value_null_desc = %s.", json.dumps(const_value_null_desc))
        for index, item in enumerate(const_value_null_desc):
            # nan/inf/-inf
            if item and item in ["nan", "inf", "-inf"]:
                desc["const_value"][index] = float(item)
        desc.pop("const_value_null_desc")


def update_attr_desc_by_null_desc(desc):
    '''
    "attr_desc": [
        false,
        1,
        null,
        [
            1.0,
            null,
            null
        ]
    ],
    "attr_desc_null_desc": [
        null,
        null,
        "inf",
        [
            null,
            "inf",
            "-inf"
        ]
    ],
    '''
    attr_desc_null_desc = desc.get("attr_desc_null_desc", None)
    #update null value if necessary
    if attr_desc_null_desc:
        logger.debug("attr_desc_null_desc = %s.", json.dumps(attr_desc_null_desc))
        for index, item in enumerate(attr_desc_null_desc):
            # nan/inf/-inf
            if isinstance(item, (list, tuple)):
                for sub_index, sub_item in enumerate(item):
                    if sub_item and sub_item in ["nan", "inf", "-inf"]:
                        desc["attr_desc"][index][sub_index] = float(sub_item)
            elif item and item in ["nan", "inf", "-inf"]:
                desc["attr_desc"][index] = float(item)
        desc.pop("attr_desc_null_desc")


def update_attr_value_by_null_desc(desc):
    value_null_desc = desc.get("value_null_desc", None)
    #update null value if necessary
    if value_null_desc:
        logger.debug("value_null_desc = %s.", json.dumps(value_null_desc))
        if isinstance(value_null_desc, (list, tuple)):
            for index, item in enumerate(value_null_desc):
                # nan/inf/-inf
                if item and item in ["nan", "inf", "-inf"]:
                    desc["value"][index] = float(item)
        elif value_null_desc in ["nan", "inf", "-inf"]:
            desc["value"] = float(value_null_desc)
        desc.pop("value_null_desc")


# 'pylint: disable=too-many-locals
def create_placeholder_tensor(op_node, tensor_list, input_list, op_list, params_count):
    """
    create placeholder tensor, get input tensor list

    Parameters
    ----------
    op_node : input fusion op node

    tensor_list : tensor list

    input_list : input tensor list

    op_list : op list

    params_count : param loop count

    Returns
    -------
    None
    """

    desc = op_node["output_desc"][0]
    aipp_format_change(op_node, op_list)

    if desc["shape"] == "NULL":
        tensor_list[desc["name"]] = None
    else:
        sformat = desc.get("format", "")
        sub_format = desc.get("sub_format", "")
        ori_shape = desc.get("ori_shape", [])
        ori_format = desc.get("ori_format", "")
        addr_type = desc.get("addr_type", 0)
        valid_shape = desc.get("valid_shape", [])
        slice_offset = desc.get("slice_offset", [])
        l1_fusion_type = desc.get("L1_fusion_type", -1)
        l1_addr_flag = desc.get("L1_addr_flag", -1)
        l1_addr_offset = desc.get("L1_addr_offset", -1)
        l1_valid_size = desc.get("L1_valid_size", -1)
        range_value = desc.get("range", [])
        update_const_value_by_null_desc(desc)

        disable_fuse_axes = desc.get("disable_fuse_axes", [])

        for idx, range_val in enumerate(range_value):
            if isinstance(range_val, tuple):
                range_value[idx] = list(range_val)
            if len(range_val) == 2 and range_val[1] is None:
                range_value[idx][1] = -1

        para_name = "params_%s" % str(params_count[0])

        out = compress_node(op_node, op_list)
        if out is not None:
            tensor_list[desc["name"]] = out
        else:
            attr = {
                "format": sformat,
                "sub_format": sub_format,
                "ori_shape": ori_shape,
                "ori_format": ori_format,
                "addr_type": addr_type,
                "valid_shape": valid_shape,
                "slice_offset": slice_offset,
                "L1_fusion_type": l1_fusion_type,
                "L1_addr_flag": l1_addr_flag,
                "L1_addr_offset": l1_addr_offset,
                "L1_valid_size": l1_valid_size,
                "range": range_value,
                "disable_fuse_axes": disable_fuse_axes}
            if "const_value" in desc:
                attr["const_value"] = desc["const_value"]
            desc, attr = _fresh_shape_and_attr(desc, attr)
            from tbe import tvm
            # 'pylint: disable=unexpected-keyword-arg
            tensor_list[desc["name"]] = tvm.placeholder(desc["shape"], desc["data_type"],
                                                        para_name, attrs=attr)
        params_count[0] = params_count[0] + 1

    input_list.append(tensor_list[desc["name"]])


# 'pylint: disable=too-many-locals
def create_placeholder_tensor_v2(op_node, tensor_list, input_list, op_list, params_count):
    """
    create placeholder tensor, get input tensor list

    Parameters
    ----------
    op_node : input fusion op node
    tensor_list : tensor list
    input_list : input tensor list
    op_list : op list
    params_count : param loop count
    Returns
    -------
    None
    """

    desc = op_node["output_desc"][0]

    if desc["shape"] == "NULL":
        tensor_list[desc["name"]] = None
    else:
        sformat = desc.get("format", "")
        sub_format = desc.get("sub_format", "")
        ori_shape = desc.get("ori_shape", [])
        ori_format = desc.get("ori_format", "")
        addr_type = desc.get("addr_type", 0)
        valid_shape = desc.get("valid_shape", [])
        slice_offset = desc.get("slice_offset", [])
        l1_fusion_type = desc.get("L1_fusion_type", -1)
        l1_addr_flag = desc.get("L1_addr_flag", -1)
        l1_addr_offset = desc.get("L1_addr_offset", -1)
        l1_valid_size = desc.get("L1_valid_size", -1)
        range_value = desc.get("range", [])
        update_const_value_by_null_desc(desc)

        for idx, range_val in enumerate(range_value):
            if isinstance(range_val, tuple):
                range_value[idx] = list(range_val)
            if len(range_val) == 2 and range_val[1] is None:
                range_value[idx][1] = -1

        para_name = "params_%s" % str(params_count[0])

        out = compress_node(op_node, op_list)
        if out is not None:
            tensor_list[desc["name"]] = out
        else:
            attr = {
                "format": sformat,
                "sub_format": sub_format,
                "ori_shape": ori_shape,
                "ori_format": ori_format,
                "addr_type": addr_type,
                "valid_shape": valid_shape,
                "slice_offset": slice_offset,
                "L1_fusion_type": l1_fusion_type,
                "L1_addr_flag": l1_addr_flag,
                "L1_addr_offset": l1_addr_offset,
                "L1_valid_size": l1_valid_size,
                "range": range_value}
            if "const_value" in desc:
                attr["const_value"] = desc["const_value"]
            desc["dtype"] = desc.get("data_type", "")

            from tbe.dsl import placeholder as tbe_dsl_placeholder
            # 'pylint: disable=unexpected-keyword-arg
            tensor_list[desc["name"]] = tbe_dsl_placeholder(desc, para_name, attrs=attr)
        params_count[0] = params_count[0] + 1

    input_list.append(tensor_list[desc["name"]])


def get_fusion_op_placeholder_input_c(op_list):
    caxis_values = []
    for op_node in op_list:
        if op_node["type"] == "Data" and op_node.get("output_desc") is not None:
            output_desc = op_node["output_desc"][0]
            if op_node["output_desc"][0].get("shape", "NULL") != "NULL":
                caxis_value = op_node["output_desc"][0].get("caxis_values", -1)
                caxis_values.append(caxis_value)
    return caxis_values


# 'pylint: disable=too-many-arguments
def add_input_tensor(op_node, tensor_list, op_input_list, is_used_tensor_list,
                     input_tensor_cnt, dyn_input_dict, input_desc_names,
                     auto_fusion_pattern=False):
    """
    add input tensor

    Parameters
    ----------
    op_node : input fusion op node

    tensor_list : tensor list

    op_input_list : input tensor list

    is_used_tensor_list : used tensor list

    input_tensor_cnt : input tensor used count

    dyn_input_dict : dynamic input dict

    Returns
    -------
    None
    """

    for input_desc in op_node["input_desc"]:
        input_desc_names.append(input_desc["name"])
        check_input_desc_not_in_tensor(input_desc, tensor_list)
        tensor_info = tensor_list[input_desc["name"]]
        if auto_fusion_pattern is False and tensor_info is not None:
            tensor_info.op.attrs["current_shape"] = input_desc["shape"]

        if "dyn_index" in input_desc:
            if "dyn_index" not in dyn_input_dict:
                dyn_input_dict["dyn_index"] = []
            dyn_input_dict["dyn_index"].append(tensor_info)
        else:
            op_input_list.append(tensor_info)

        is_used_tensor_list.add(tensor_info)
        # count input tensor called by other tensor
        if tensor_info in input_tensor_cnt:
            input_tensor_cnt[tensor_info] += 1
        else:
            input_tensor_cnt[tensor_info] = 1


def replace_attr_for_op_input_list(op_input_list, input_to_attr_list):
    if not input_to_attr_list:
        return

    for key in input_to_attr_list:
        op_input_list[key] = input_to_attr_list[key]


def get_fusion_op_kernel_name(func_name, kernel_name):
    """
    get kernel_name kwds of the op
    """
    try:
        opfunc = fusion_manager_obj.get_op_compute_func(func_name)
        if inspect.signature(opfunc).parameters['kernel_name'].kind in \
           (inspect.Parameter.KEYWORD_ONLY,
            inspect.Parameter.POSITIONAL_OR_KEYWORD):
            return {'kernel_name': kernel_name}
        return {}
    except Exception:           # 'pylint: disable=broad-except
        return {}


def get_attrs_from_cls_info(op_node, ins_attrs_options):
    """
    get op outputs and attrs
    """
    outputs_attrs = []
    if "output_data_desc" in op_node:
        for compute_output in op_node["output_data_desc"]:
            outputs_attrs.append(compute_output)

    if len(ins_attrs_options) < 2:
        return outputs_attrs
    op_name = op_node.get("name")
    ins_attrs = ins_attrs_options[1]
    for attr in ins_attrs:
        if attr["name"] == op_name:
            for item in attr["val"]:
                outputs_attrs.append(item)
            break

    return outputs_attrs


def get_classify_options(op_node, ins_attrs_options):
    if len(ins_attrs_options) < 3:
        return {}
    options = ins_attrs_options[2]
    op_name = op_node.get("name")

    for option in options:
        if option["name"] == op_name:
            return {"options": option["options"]}
    return {}


def import_op_module(op_node):
    """
    imort op py module if necessary.
    return op func
    """
    py_module_path = op_node.get('py_module_path', '')
    if py_module_path != '' and py_module_path not in sys.path:
        sys.path.append(py_module_path)

    module_name = None
    try:
        module_name = op_node.get('module_name', '')
        if module_name != '':
            opm = importlib.import_module(module_name)
            opfunc = getattr(opm, op_node["func_name"])
            return opfunc
        return None
    except Exception:       # pylint: disable=bare-except,broad-except
        msg = traceback.format_exception_only(sys.exc_info()[0],
                                              sys.exc_info()[1])
        logger.warn("import_op_module module=%s failed, reason: %s", module_name, msg)
        return None


def import_dyn_op_module(op_node):
    """
    imort op py module if necessary.
    return op func
    """
    import tbe.common.register as tbe_register
    py_module_path = op_node.get('py_module_path', '')
    if py_module_path != '' and py_module_path not in sys.path:
        sys.path.append(py_module_path)

    module_name = None
    try:
        module_name = op_node.get('module_name', '')
        if module_name != '':
            dyn_op_module = module_name
            if 'dynamic' not in dyn_op_module:
                dyn_op_module = module_name.split('.')
                dyn_op_module[-1] = 'dynamic'
                dyn_op_module = '.'.join(dyn_op_module)
            importlib.import_module(dyn_op_module)
            op_type = op_node['type']
            return tbe_register.get_op_compute(op_type)
        return None
    except Exception:       # pylint: disable=bare-except,broad-except
        msg = traceback.format_exception_only(sys.exc_info()[0],
                                              sys.exc_info()[1])
        logger.warn("import_dyn_op_module module=%s failed, reason: %s", module_name, msg)
        return None


def get_compute_options(op_func, options):
    """
    get options kwargs
    """
    if options:
        options_arg = \
            inspect.signature(op_func).parameters.get('options', None)
        if options_arg is not None and \
                options_arg.kind in \
                (inspect.Parameter.KEYWORD_ONLY,
                 inspect.Parameter.POSITIONAL_OR_KEYWORD):
            return {'options': options}
    return {}


def get_compute_func(op_node):
    import tbe.common.register as tbe_register
    is_dyn_impl = False
    if "is_dynamic_impl" in op_node.keys():
        is_dyn_impl = op_node["is_dynamic_impl"]
    if is_dyn_impl:
        op_type = op_node["type"]
        op_compute = tbe_register.get_op_compute(op_type)
        if op_compute is None:
            op_compute = import_dyn_op_module(op_node)
        if op_compute is None:
            return None
        else:
            return op_compute.get_func()
    else:
        func_name = op_node["func_name"]
        op_compute_func = fusion_manager_obj.get_op_compute_func(func_name)
        if op_compute_func is None:
            import_op_module(op_node)
            op_compute_func = fusion_manager_obj.get_op_compute_func(func_name)
        return op_compute_func


def get_output_attrs_for_all_args(all_args, op_node, ins_list_none, ins_attrs_options):
    if op_node["type"] != "conv2d_data_rm":
        if ins_list_none:
            all_outputs_attrs = get_attrs_from_cls_info(op_node, ins_attrs_options)
            all_args.extend(all_outputs_attrs)
        else:
            outputs = op_node["output_data_desc"] if "output_data_desc" in op_node else None
            update_attr_desc_by_null_desc(op_node)

            attrs = op_node["attr_desc"] if "attr_desc" in op_node else None
            if outputs:
                all_args.extend(outputs)
            if attrs:
                all_args.extend(attrs)


def get_options_kwds(op_node, op_compute_func, ins_list_none, ins_attrs_options):
    options_kwds = {}
    if ins_list_none:
        options_kwds = get_classify_options(op_node, ins_attrs_options)
    elif 'options' in op_node:
        options_kwds = get_compute_options(op_compute_func, op_node["options"])
    return options_kwds


def call_op_compute(op_node, ins_attrs_options, op_input_list, dyn_input_dict,
                    ins_list_none=False, kernel_name=None, auto_fusion_pattern=False):# 'pylint: disable=too-many-locals
    """
    call op compute
    Parameters
    ----------
    op_node : input fusion op node
    ins_attrs_options: return from registered classify function
    op_input_list : input tensor list
    dyn_input_dict : dynamic input dict
    Returns
    -------
    op_output_list: output op list tensor
    """
    # call op's compute
    from tbe.tvm.target import cce as _cce
    with _cce():
        op_compute_func = get_compute_func(op_node)
        if op_compute_func is None:
            raise RuntimeError("Compute function of node[%s, %s] is null.", op_node.get("name"), op_node.get("type"))

        all_args = op_input_list
        # add dyn input para to commpute args
        for dyn_input in dyn_input_dict:
            all_args.append(dyn_input_dict[dyn_input])

        get_output_attrs_for_all_args(all_args, op_node, ins_list_none, ins_attrs_options)
        kernel_kwds = get_fusion_op_kernel_name(op_node["func_name"], kernel_name)
        op_impl_kwds = get_op_impl_mode_args(op_compute_func, op_node.get("op_impl_mode"))
        options_kwds = get_options_kwds(op_node, op_compute_func, ins_list_none, ins_attrs_options)
        all_args_kwds = {}
        logger.info("call op_compute_func compile node[%s, %s]", op_node.get("name"), op_node.get("type"))
        op_output_list = op_compute_func(*all_args, **kernel_kwds,
                                         **all_args_kwds,
                                         **op_impl_kwds,
                                         **options_kwds)

        logger.info("call compute func of node[%s, %s] return, op_output_list: %s",
                    op_node.get("name"), op_node.get("type"), str(op_output_list))

    if auto_fusion_pattern:
        op_output_list = [op_output_list] if not hasattr(op_output_list, "index") else op_output_list
    else:
        from tbe.tvm import Tensor
        op_output_list = [op_output_list] if isinstance(op_output_list, Tensor) else op_output_list

    return op_output_list


def check_input_desc_not_in_op(op_node):
    """
    check input description in op or not

    Parameters
    ----------
    op_node : input fusion op node

    Returns
    -------
    None
    """
    if "input_desc" not in op_node:
        raise RuntimeError(
            "Lack of input_desc for op name: %s" % op_node["name"])


def check_output_desc_not_in_op(op_node):
    """
    check output description in op or not

    Parameters
    ----------
    op_node : output fusion op node

    Returns
    -------
    None
    """
    if "output_desc" not in op_node:
        raise RuntimeError(
            "Lack of output_desc for op name: %s" % op_node["name"])


def check_input_desc_not_in_tensor(input_desc, tensor_list):
    """
    check input description in tensor or not

    Parameters
    ----------
    input_desc : input tensor

    tensor_list : tensor list

    Returns
    -------
    None
    """
    if input_desc["name"] not in tensor_list:
        raise RuntimeError(
            'Can not find input tensor: %s' % input_desc["name"])


def check_output_desc_in_tensor(output_desc, tensor_list):
    """
    check output description in tensor or not

    Parameters
    ----------
    output_desc : output tensor

    tensor_list : tensor list

    Returns
    -------
    None
    """
    if output_desc["name"] in tensor_list:
        raise RuntimeError(
            "Output tensor already exists %s" % output_desc["name"])


# 'pylint: disable=too-many-locals, too-many-branches, too-many-statements
def fusion_op_compute(json_str):
    """
    get the output tensor

    Parameters
    ----------
    json_str : input json data

    Returns
    -------
    output tensor
    """
    op_desc = json.loads(json_str)
    if op_desc.get('SocInfo').get('l1Fusion', 'false') == 'false':
        l1_fusion_flag = False
    else:
        l1_fusion_flag = True
    if op_desc.get('SocInfo').get('l2Fusion', 'false') == 'false':
        l2_fusion_flag = False
    else:
        l2_fusion_flag = True
    l2_mode_flag = int(op_desc.get('SocInfo').get('l2Mode', '0'))
    op_build_cfg_en()
    import tbe.common.context.op_context as op_context
    context = op_context.get_context()
    master_pid = context.get_addition("master_pid")
    from tbe.common.buildcfg import build_config
    with build_config(enable_L1_fusion=l1_fusion_flag,
                    l2_mode=l2_mode_flag,
                    enable_L2_fusion=l2_fusion_flag,
                    enable_op_prebuild=False,
                    compatible=True):
        return fusion_op(json_str, True, master_pid=master_pid)


def check_fusion_op_type(op_list):
    """
    check fusion op type
    ----------
    op_list: all ops info

    Returns
    -------
    None
    """
    matmul_elmt_fuse_type = ["Elu", "LeakyRelu", "Gelu", "Softsign", "Relu6",
                             "Relu", "Softplus", "Sigmoid", "Tanh", "Selu",
                             "GeluGrad", "Add", 'AddN', 'FastGelu', "FastGeluV2",
                             'FastGeluGrad', "Relu6D", "Eltwise", "PRelu",
                             "Mul", "Power", "TanhGrad", "Swish"]

    matmul_deq_elwt_fuse_type = ["Elu", "LeakyRelu", "Gelu", "Softsign",
                                 "Relu6", "Relu", "Softplus", "Sigmoid",
                                 "Tanh", "Selu", "GeluGrad", "Add", 'AddN',
                                 'FastGelu', "FastGeluV2", 'FastGeluGrad', "Eltwise",
                                 "PRelu", "Mul", "Power", "Relu6D"]

    matmul_fusion = False
    dequant_fusion = False
    for op_node in op_list:
        if "pattern" in op_node:
            if op_node["pattern"] in [Pattern.MAT_MUL, "GEMM"]:
                matmul_fusion = True
            if op_node["pattern"] == "dequant":
                dequant_fusion = True

    if not matmul_fusion:
        return

    for op_node in op_list:
        if "pattern" in op_node:
            if not dequant_fusion:
                if (op_node["pattern"] in [Pattern.ELEMWISE, Pattern.BROADCAST]) \
                        and (op_node["type"] not in matmul_elmt_fuse_type):
                    raise RuntimeError(
                        "Matmul elementwise fusion only support ('Elu', "
                        "'Relu6D', 'LeakyRelu','Gelu','Softsign','Relu6', "
                        "'Relu','Selu', 'Sigmoid','Tanh','Softplus', "
                        "'GeluGrad', 'Add', 'AddN', 'FastGelu', 'FastGeluV2', "
                        "'Eltwise', 'PRelu', 'Mul', 'Power', 'TanhGrad', "
                        "'FastGeluGrad', 'Swish'), not support fusion with '%s'"
                        % op_node["type"])
            else:
                if (op_node["pattern"] in [Pattern.ELEMWISE, Pattern.BROADCAST]) \
                        and (op_node["type"] not in matmul_deq_elwt_fuse_type):
                    raise RuntimeError(
                        "Matmul elementwise fusion only support ('Elu', "
                        "'Relu6', 'LeakyRelu','Gelu','Softsign','Relu6D', "
                        "'Relu','Selu', 'Sigmoid','Tanh','Softplus', "
                        "'GeluGrad', 'Add', 'AddN', 'FastGelu', 'FastGeluV2', "
                        "'FastGeluGrad', 'Eltwise', 'PRelu', 'Mul', 'Power'), "
                        "not support fusion with '%s'" % op_node["type"])


def get_real_output(sch, output_list):
    """
    get_real_output
    """
    # some schedule will modify out tensor, need update real out tensor
    try:
        if sch.cce_special["real_out_tensor"]:
            real_output = sch.cce_special["real_out_tensor"]
        else:
            real_output = output_list
        return real_output
    except Exception:           # 'pylint: disable=broad-except
        return output_list


def check_single_op(op_json):
    """Check if the json string contains only one op

    Parameters
    ----------
    op_json : json description of fusion op

    Returns
    -------
    succ_flag : boolean
    """
    count = len([operator
                 for operator in op_json['op_list']
                 if operator['type'] != 'Data'])
    return count == 1


def check_lx_stride_info(op_list):
    def _check_desc(op_desc):
        for desc in op_desc:
            if desc["shape"] == "NULL":
                continue
            if desc["addr_type"] != 0 \
                    or (desc["addr_type"] == 2 and desc["L1_addr_offset"] != 0) \
                    or any(desc["valid_shape"]):
                return True
        return False

    for op_node in op_list:
        if op_node["type"] == "Data":
            continue
        if "input_desc" in op_node:
            if _check_desc(op_node["input_desc"]):
                return True
        if "output_desc" in op_node:
            if _check_desc(op_node["output_desc"]):
                return True
    return False


def set_l1_fusion_type(l1_enable, op_node):
    if l1_enable is False:
        return
    from tbe.tvm.buffer_manager import get_buffer_manager
    buffer_manager = get_buffer_manager()
    l1_fusion_type = op_node["output_desc"][0].get("L1_fusion_type", -1)
    buffer_manager.set_l1_fusion_type(l1_fusion_type)


def set_ub_space_size(op_node):
    from tbe.common.buildcfg import set_UB_info
    ub_space_size = op_node.get("UBSpaceSize", None)
    if ub_space_size is not None:
        set_UB_info("op_UB_space", ub_space_size)
        logger.info("op %s type %s set ub space size [%d]", op_node["name"], op_node["type"], ub_space_size)


def get_lx_enable(json_data):

    lx_enable = True
    op_list = json_data["op_list"]
    if not check_lx_stride_info(op_list):
        lx_enable = False
    return lx_enable


def add_stride_info(stride_info, desc, index):
    buffer_scope = "global"
    addr_type = desc.get("addr_type", 0)
    if addr_type == 1:
        buffer_scope = "local.L1_Fusion"
    if addr_type == 3:
        buffer_scope = "local.UB_Fusion"
    input_shape = desc.get("total_shape", [])
    valid_shape = desc.get("valid_shape", [])
    l1_addr_offset = None
    if addr_type == 2:
        l1_addr_offset = desc.get("L1_addr_offset", None)

    tmp_attr = {}
    tmp_attr["L1_addr_flag"] = desc.get("L1_addr_flag", "nothing")
    tmp_attr["L1_valid_size"] = desc.get("L1_valid_size", -1)
    tmp_attr["slice_offset"] = desc.get("slice_offset", (0, 0, 0, 0, 0))

    stride_info.append((index, buffer_scope, tuple(input_shape),
                        tuple(valid_shape), l1_addr_offset, tmp_attr))


def add_out_stride_info(stride_info, desc, index):
    buffer_scope = "global"
    addr_type = desc.get("addr_type", 0)
    if addr_type == 1:
        buffer_scope = "local.L1_Fusion"
    if addr_type == 3:
        buffer_scope = "local.UB_Fusion"
    output_shape = desc.get("total_shape", [])
    valid_shape = desc.get("shape", [])
    l1_addr_offset = None
    if addr_type == 2:
        l1_addr_offset = desc.get("L1_addr_offset", None)
    stride_info.append((index, buffer_scope, tuple(output_shape),
                        tuple(valid_shape), l1_addr_offset))


def set_single_op_stride_info(l1_enable, lx_enable, json_data):
    op_list = json_data["op_list"]
    for op_node in op_list:
        if op_node["type"] != "Data":
            set_l1_fusion_type(l1_enable, op_node)
    if lx_enable == False:
        return
    from tbe.tvm.buffer_manager import get_buffer_manager
    buffer_manager = get_buffer_manager()
    buffer_manager.clear_remapped_buffers()
    for op_node in op_list:
        stride_info = []
        stride_index = [0]
        if op_node["type"] == "Data":
            continue
        for input_desc in op_node["input_desc"]:
            if input_desc["shape"] != "NULL":
                add_stride_info(stride_info, input_desc, stride_index[0])
                stride_index[0] += 1
        for output_desc in op_node["output_desc"]:
            if output_desc["shape"] != "NULL":
                add_out_stride_info(stride_info, output_desc, stride_index[0])
                stride_index[0] += 1
        l1_fusion_type = op_node["output_desc"][0].get("L1_fusion_type", -1)
        buffer_manager.set_l1_fusion_type(l1_fusion_type)

        rbs = []
        from tbe.tvm.buffer_manager import RemappedBuffer
        for i in stride_info:
            rbs.append(RemappedBuffer(*i))
        buffer_manager.set_remapped_buffers(rbs)


def get_op_args_json(json_str, attrs):
    json_data = json.loads(json_str)
    single_op = [operator
                 for operator in json_data["op_list"]
                 if operator["type"] != "Data"][0]

    kernel_name = str(json_data["fusion_op_name"])
    context_extra_params = {}
    opfunc = import_op_module(single_op)
    kwargs = get_op_impl_mode_args(opfunc, single_op.get("op_impl_mode"))
    context_extra_params.update(kwargs)
    extra_params_str = single_op["extra_params"] if "extra_params" in single_op else None
    context_extra_params.update(get_extra_params(extra_params_str))

    kernel_kwds = get_fusion_op_kernel_name(single_op["func_name"], kernel_name)
    if "kernel_name" in kernel_kwds:
        kernel_name = str(kernel_kwds["kernel_name"])

    op_args_dict = {
        "op_type": single_op["type"],
        "op_params": {
            "input_desc": single_op["input_desc"],
            "output_desc": single_op["output_data_desc"] if "output_data_desc" in single_op else None,
            "attr_desc": attrs,
            "kernel_name": kernel_name,
            "extra_param": context_extra_params
        }
    }
    return json.dumps(op_args_dict)


def single_op_build(json_data):
    """call op's entry function directly if there's only one single op

    Parameters
    ----------
    json_data : dict
        json description of op

    """
    single_op = [operator
                 for operator in json_data["op_list"]
                 if operator["type"] != "Data"][0]
    opfunc = import_op_module(single_op)
    if opfunc is None:
        raise RuntimeError("Compute function of node[%s, %s] is null." % (single_op["name"], single_op["type"]))
    op_info = set_single_op_info_to_context(json_data["op_list"], single_op,
                                            json_data["fusion_op_name"], opfunc)
    l1_enable = json_data["SocInfo"]["l1Fusion"] == "true"
    lx_enable = get_lx_enable(json_data)
    set_single_op_stride_info(l1_enable, lx_enable, json_data)

    attrs = []
    update_attr_desc_by_null_desc(single_op)
    if "attr_desc" in single_op:
        attr_desc = single_op["attr_desc"]
        for attr_desc in attr_desc:
            attrs.append(attr_desc)

    op_build_cfg_en()
    from tbe.common.buildcfg import build_config
    with build_config(enable_op_prebuild = False, compatible = True):
        if has_dynshape(json_data["op_list"]):
            logger.info("start dynamic compile, single_op_type [%s]", single_op["type"])
        else:
            logger.info("start static compile, single_op_type [%s]", single_op["type"])
        kwargs = {"impl_mode": op_info.extra_params["impl_mode"]} if "impl_mode" in op_info.extra_params else {}
        opfunc(*op_info.inputs, *op_info.outputs, *attrs, op_info.kernel_name, **kwargs)

        import tbe.common.context.op_context as op_context
        json_file_path = op_context.get_context().get_build_res("json_file_path")
        logger.debug("compile json_file_path = %s.", json_file_path)

        if not json_file_path:
            from tbe.common.buildcfg import get_current_build_config
            json_file_path = os.path.join(get_current_build_config("kernel_meta_parent_dir"), "kernel_meta",
                op_info.kernel_name + ".json")
            logger.warn("invalid json_file_path=%s.", json_file_path)

    build_res = None
    if has_dynshape(json_data["op_list"]):
        update_compile_info(json_file_path, op_context.get_context().get_compile_info(None))

        if os.path.exists(json_file_path):
            build_res = {"json_file_path": json_file_path}
        else:
            build_res = {"json_file_path": json_file_path, \
                "compile_info": op_context.get_context().get_compile_info(None)}
    else:
        build_res = {"json_file_path": json_file_path}

    return build_res


def dump_fusion_json(json_str, dump_path, dup_check=False):
    """
    dump fusion json to kernel_meta directory
    """
    # get json data
    json_data = json.loads(json_str)
    json_file_name = json_data["fusion_op_name"]
    scope_id = json_data.get("scope_id", "")

    # remove sequence in file name
    if dup_check:
        json_file_name = "_".join(json_file_name.split('_')[:-1])

    json_str = json.dumps(json_data)

    dump_path = create_dir(dump_path)
    if len(dump_path) == 0:
        return dump_path
    file_name = "fusion_op_{}_{}.json".format(json_file_name, scope_id)
    file_path = os.path.join(dump_path, file_name)
    return write_file_and_set_auth(file_path, json_str)


def dump_compute_json(output_list, input_list, fusion_op_name):
    """
    dump compute json to kernel_meta directory
    """
    # to avoid circular referrence, copy output/input list to a new one
    output_list_new = copy.deepcopy(output_list)
    input_list_new = copy.deepcopy(input_list)
    output_list[0].op.attrs["output_list"] = output_list_new
    output_list[0].op.attrs["input_list"] = input_list_new
    json_str = tvm_save_json(output_list[0])

    # get json data
    from tbe.common.buildcfg import get_current_build_config
    dump_path = "{}/kernel_meta/".format(get_current_build_config('kernel_meta_parent_dir'))
    dump_path = create_dir(dump_path)
    if len(dump_path) == 0:
        return
    file_name = "{}_compute.json".format(fusion_op_name)
    file_path = os.path.join(dump_path, file_name)
    write_file_and_set_auth(file_path, json_str)


def create_dir(dir_path):
    dir_path = os.path.realpath(dir_path)
    try:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path, stat.S_IRWXU + stat.S_IRGRP + stat.S_IXGRP, exist_ok=True)
            logger.info("The dir %s has been created successfully.", dir_path)
    except OSError:
        logger.warn("Something went wrong when trying to create dir %s.", dir_path)
        return ""
    else:
        return dir_path


def write_file_and_set_auth(file_path, context_str):
    try:
        with open(file_path, "w") as jsonf:
            jsonf.write(context_str)
        os.chmod(file_path, stat.S_IWUSR + stat.S_IRUSR + stat.S_IRGRP + stat.S_IROTH)
        return ""
    except Exception:           # 'pylint: disable=broad-except
        msg = traceback.format_exception_only(sys.exc_info()[0],
                                              sys.exc_info()[1])
        logger.warn("dump compute json for %s failed, reason: %s", file_path, msg)
        return "".join(msg)


def init_op_cfg(json_data):
    """
    init l1 size, etc...
    """
    from tbe.common.buildcfg import set_L1_info
    if 'l1_size' in json_data:
        set_L1_info("op_L1_space", json_data['l1_size'])
    else:
        set_L1_info("op_L1_space", -1)


def has_dynshape(op_list):
    """
    check if dynamic shape
    """
    for node in op_list:
        if node['type'] == 'Data':
            for data in node['output_desc']:
                if data['shape'] == 'NULL':
                    continue
                if [ele for ele in data['shape'] if ele < 0]:
                    return True
    return False


def modify_duplicated_inputs(json_data):
    """
    rename names of duplicated inputs
    """
    dup_data_names = {}
    for operator in json_data['op_list']:
        if operator['type'] != 'Data':
            continue
        count = dup_data_names.setdefault(operator['name'], [])
        count.append(operator)

    for key, value in dup_data_names.items():
        if len(value) > 1:
            dup_data_names[key] = value

    dup_indesc_names = {}
    for operator in json_data['op_list']:
        if operator['type'] == 'Data':
            continue
        for indesc in operator['input_desc']:
            if indesc['name'] in dup_data_names.keys():
                count = dup_indesc_names.setdefault(indesc['name'], [])
                count.append(indesc)
    if len(dup_data_names) != len(dup_indesc_names):
        raise RuntimeError('Duplicated names not match')

    for name, ops in dup_data_names.items():
        indesc_names = dup_indesc_names[name]
        if len(ops) != len(indesc_names):
            raise RuntimeError('Duplicated names not match')
        for idx, opdesc in enumerate(zip(ops, indesc_names)):
            new_name = "{}___{}".format(name, str(idx))
            opdesc[0]['name'] = new_name
            opdesc[0]['output_desc'][0]['name'] = new_name
            opdesc[1]['name'] = new_name


def update_atomic_build_cfg(op_list, op_name):
    from tbe.common.buildcfg import set_current_build_config
    from tbe.common.buildcfg import get_current_build_config
    atomic_type = []
    op_input_names = []
    for op_node in op_list:
        for input_desc in op_node.get("input_desc", ""):
            op_input_names.append(input_desc.get('name', ''))
    for op_node in op_list:
        if op_node.get("output_desc") is not None:
            output_desc = op_node["output_desc"][0]
            if op_node["type"] == "Data" and output_desc["shape"] != "NULL":
                atomic_type.append("")
    for op_node in op_list:
        if op_node["type"] != "Data" and op_node.get("output_desc") is not None:
            for output_desc in op_node["output_desc"]:
                output_desc_name = output_desc.get('name', '')
                if output_desc_name not in op_input_names:
                    atomic_type.append(output_desc.get("atomic_type", ""))

    set_current_build_config("enable_auto_atomic", atomic_type)


def process_nan_inf(json_data):
    if "op_list" not in json_data:
        return json_data

    op_list_data = json_data["op_list"]
    for op_data in op_list_data:
        if "type" not in op_data:
            continue

        if op_data["type"] == "Data":
            for output_data in op_data["output_desc"]:
                update_const_value_by_null_desc(output_data)
        else:
            update_attr_desc_by_null_desc(op_data)

            if "attr_info_desc" in op_data:
                attr_list_data = op_data["attr_info_desc"]
                if attr_list_data:
                    for attr_data in attr_list_data:
                        update_attr_value_by_null_desc(attr_data)

            if "input_desc" in op_data:
                input_list_data = op_data["input_desc"]
                if input_list_data:
                    for input_data in input_list_data:
                        update_const_value_by_null_desc(input_data)

    return json_data


# 'pylint: disable=too-many-locals, too-many-branches, too-many-statements
def fusion_op(json_str, compute_only=False, reset_op_info=None, context_param=None, pass_opt_list=None,
              master_pid=None, status_check=False, op_name=None, fixpipe_ub_cfg=None, optional_input_mode=None,
              dynamic_param_mode=None):
    """
    fusion template
    Parameters
    ----------
    json_str : string
        The input json data.

    Returns
    -------
    succ_flag : boolean
        end of execution
    """
    # get json data
    json_data = json.loads(json_str)
    # restore nan inf
    process_nan_inf(json_data)

    if reset_op_info is not None:
        json_data["reset_op_info"] = reset_op_info
    use_int64_mode = False
    is_dyn_impl = True
    has_dyn_imp = False
    for op_node in json_data["op_list"]:
        if op_node['type'] == 'Data':
            continue
        if "int64mode" in op_node.keys():
            if op_node["int64mode"]:
                use_int64_mode = True
        if "is_dynamic_impl" in op_node.keys():
            if op_node["is_dynamic_impl"] is False:
                is_dyn_impl = False
            else:
                has_dyn_imp = True
        else:
            is_dyn_impl = False

    update_atomic_build_cfg(json_data["op_list"], op_name)
    dyn_flag = has_dynshape(json_data["op_list"])

    from tbe.tvm._api_config import api_config as _api_config
    if use_int64_mode and not has_dyn_imp:
        with _api_config.bit_width_64():# 'pylint:disable=not-context-manager
            return fusion_op_impl(json_data, compute_only, dyn_flag, is_dyn_impl, context_param, pass_opt_list,
                                  master_pid, status_check, op_name, fixpipe_ub_cfg, optional_input_mode,
                                  dynamic_param_mode)
    else:
        with _api_config.bit_width_32():# 'pylint:disable=not-context-manager
            return fusion_op_impl(json_data, compute_only, dyn_flag, is_dyn_impl, context_param, pass_opt_list,
                                  master_pid, status_check, op_name, fixpipe_ub_cfg, optional_input_mode,
                                  dynamic_param_mode)


def set_stride_info(desc, stride_info, stride_index):
    """
    :param op_node
    :return: None
    """
    if desc["shape"] != "NULL":
        add_stride_info(stride_info, desc, stride_index[0])
        stride_index[0] += 1


def set_out_stride_info(desc, stride_info, stride_index):
    """
    :param op_node
    :return: None
    """
    if desc["shape"] != "NULL":
        add_out_stride_info(stride_info, desc, stride_index[0])
        stride_index[0] += 1


def set_input_buffer_manager(lx_enable, op_node, op_list, op_input_list, dyn_input_dict, stride_index,
                             output_desc_names, input_tensor_list_name, output_tensor_list_name, tensor_index):
    if lx_enable is False:
        return
    from tbe.tvm.buffer_manager import get_buffer_manager
    buffer_manager = get_buffer_manager()
    stride_info = []
    for input_desc in op_node["input_desc"]:
        if input_desc["name"] not in output_desc_names:
            desc = input_desc
            set_stride_info(desc, stride_info, stride_index)
    rbs = []
    from tbe.tvm.buffer_manager import RemappedBuffer
    for i in stride_info:
        rbs.append(RemappedBuffer(*i))
    buffer_manager.set_remapped_buffers(rbs)

    # set l1 fusion tensor index
    tensor_list_index = {}
    for tensor in op_input_list:
        if tensor is None:
            continue
        input_tensor_list_name.append(tensor.op.name)
        if tensor.op.name not in output_tensor_list_name:
            tensor_list_index.update({tensor: tensor_index[0]})
            tensor_index[0] += 1
    if "dyn_index" in dyn_input_dict:
        for tensor in dyn_input_dict["dyn_index"]:
            input_tensor_list_name.append(tensor.op.name)
            if tensor is not None and tensor.op.name not in output_tensor_list_name:
                tensor_list_index.update({tensor: tensor_index[0]})
                tensor_index[0] += 1
    buffer_manager.set_tensor_index(tensor_list_index)

    l1_fusion_type = op_node["output_desc"][0].get("L1_fusion_type", -1)
    buffer_manager.set_l1_fusion_type(l1_fusion_type)


def set_output_buffer_manager(lx_enable, op_list, input_desc_names,
                              stride_index, output_list, tensor_index):
    if lx_enable is False:
        return
    stride_info = []
    for op_node in op_list:
        if op_node["type"] == "Data":
            continue
        if op_node.get("output_desc") is None:
            continue
        for output_desc in op_node["output_desc"]:
            if output_desc["name"] not in input_desc_names:
                set_out_stride_info(output_desc, stride_info, stride_index)

    from tbe.tvm.buffer_manager import get_buffer_manager
    buffer_manager = get_buffer_manager()
    rbs = []
    from tbe.tvm.buffer_manager import RemappedBuffer
    for i in stride_info:
        rbs.append(RemappedBuffer(*i))
    buffer_manager.set_remapped_buffers(rbs)
    tensor_list_index = {}
    for tensor in output_list:
        tensor_list_index.update({tensor: tensor_index[0]})
        tensor_index[0] += 1
    buffer_manager.set_tensor_index(tensor_list_index)


def collect_output_desc_names(op_list, output_desc_names):
    for op_node in op_list:
        if op_node["type"] == "Data":
            continue
        if op_node.get("output_desc") is None:
            continue
        for output_desc in op_node["output_desc"]:
            output_desc_names.append(output_desc["name"])

def collect_out_tensor_list_name(lx_enable, op_output_list, all_op_output,
                                 output_tensor_list_name):
    if lx_enable is True:
        for output in op_output_list:
            all_op_output.append(output)
        for output in op_output_list:
            if output is not None:
                output_tensor_list_name.append(output.op.name)


def call_all_registered_functions(op_list):
    """call all registered functions

    Parameters
    ----------
    op_list : all op info.
    """
    import tbe.common.register
    registered_functions = tbe.common.register.get_all_fusion_pass()
    import tbe.common.utils.log as logger
    if len(registered_functions) == 0:
        return
    for func_info in registered_functions:
        if func_info.get_stage() == tbe.common.register.InvokeStage.STAGE_BUILD:
            try:
                logger.debug("Start to call fusion pass[%s].", func_info.get_func_name())
                func = func_info.get_func()
                func(op_list)
                logger.debug("Finish to call fusion pass[%s].", func_info.get_func_name())
            except Exception:
                logger.warn("Exception happened while call fusion pass[%s].", func_info.get_func_name())
                pass


def set_context_parameter(context, json_data):
    soc_info = json_data.get("SocInfo")
    if soc_info is None:
        return

    device_id = soc_info.get("device_id")
    if device_id is not None and len(device_id) > 0:
        context.add_addition("device_id", device_id)
        context.add_compile_info("device_id", device_id)
    core_type = soc_info.get("coreType", "AiCore")
    context.add_compile_info("_cube_vector_core_type", core_type)

    reset_op_info = json_data.get("reset_op_info")
    if reset_op_info is not None:
        context.add_addition("reset_op_info", reset_op_info)

    op_build_options = json_data.get("op_build_options")
    if op_build_options is not None:
        context.add_addition("build_options", op_build_options)


def fusion_op_impl_with_context(context_name, json_data, compute_only, context_param, master_pid,
                                bank_path_dict, status_check, op_name, fixpipe_ub_cfg, optional_input_mode,
                                dynamic_param_mode):
    import tbe.common.context.op_context as op_context
    with op_context.OpContext(context_name):
        context = op_context.get_context()
        context.add_addition("op_bank_path", bank_path_dict.get("op_bank_path", None))
        context.add_addition("mdl_bank_path", bank_path_dict.get("mdl_bank_path", None))
        context.add_addition("master_pid", master_pid)
        context.add_addition("status_check", status_check)
        context.add_addition("op_name", op_name)
        context.add_addition("build_options", fixpipe_ub_cfg)
        context.add_addition("optional_input_mode", optional_input_mode)
        context.add_addition("dynamic_param_mode", dynamic_param_mode)
        add_context_param(context, context_param)
        set_context_parameter(context, json_data)
        if json_data.get("auto_fusion_pattern"):
            return fusion_op_dynamic_v2(json_data, compute_only)

        return fusion_op_dynamic(json_data, compute_only)


def fusion_op_impl(json_data, compute_only=False, dyn_flag = False, is_dyn_impl=False, context_param=None,
                   pass_opt_list=None, master_pid=None, status_check=False, op_name=None, fixpipe_ub_cfg=None,
                   optional_input_mode=None, dynamic_param_mode=None):
    """
    fusion op impl
    """
    reset_fusion_build_cfg()
    set_lic_pass_opt_list("fusion_op", pass_opt_list)
    bank_path_dict = OpImplPolicy.bank_path_kwargs

    from tbe.tvm.buffer_manager import get_buffer_manager
    buffer_manager = get_buffer_manager()
    buffer_manager.clear_remapped_buffers()
    init_op_cfg(json_data)
    logger.warn("Clear the saved information in buffer_manager, and reset operation is completed.")
    if dyn_flag:
        return fusion_op_impl_with_context("dynamic", json_data, compute_only, context_param,
                                           master_pid, bank_path_dict, status_check, op_name,
                                           fixpipe_ub_cfg, optional_input_mode, dynamic_param_mode)
    else:
        if is_dyn_impl:
            return fusion_op_impl_with_context("static", json_data, compute_only, context_param,
                                               master_pid, bank_path_dict, status_check, op_name,
                                               fixpipe_ub_cfg, optional_input_mode, dynamic_param_mode)
        else:
            return fusion_op_static(json_data, compute_only, context_param, master_pid, status_check,
                                    op_name, fixpipe_ub_cfg)


def collect_out_tensor(op_node, tensor_collect_params, auto_fusion_pattern=False):
    """
    collect out tensor
    """
    tensor_list = tensor_collect_params.tensor_list
    op_output_list = tensor_collect_params.op_output_list
    compute_output_tensor_list = tensor_collect_params.compute_output_tensor_list
    output_tensor_cnt = tensor_collect_params.output_tensor_cnt
    for output_desc in op_node["output_desc"]:
        if not output_desc or not isinstance(output_desc, dict) or "name" not in output_desc:
            continue
        if output_desc.get("has_data_anchor") is not None:
            continue
        if output_desc["name"] not in tensor_list:
            output_tensor = op_output_list[output_desc["output_index"]]
            if not auto_fusion_pattern:
                output_tensor.op.attrs["addr_type"] = \
                    output_desc.get("addr_type", 0)
            tensor_list[output_desc["name"]] = output_tensor
            compute_output_tensor_list.append(output_tensor)

            # record output tensor called by other tensor
            if output_tensor not in output_tensor_cnt:
                output_tensor_cnt[output_tensor] = 0

            tmp_cnt = output_tensor_cnt[output_tensor]
            output_tensor_cnt[output_tensor] = tmp_cnt + 1
        else:
            raise RuntimeError(
                "Output tensor already exists %s" % output_desc["name"])


def update_out_attr(op_node, op_output_list):
    for output_desc in op_node["output_desc"]:
        if not output_desc:
            continue
        output_tensor = op_output_list[output_desc["output_index"]]
        output_tensor.op.attrs["format_"] = output_desc.get("format", "")


def get_sub_graph_output(compute_output_tensor_list, is_used_tensor_list, output_list,
                         output_tensor_cnt, input_tensor_cnt):
    """
    After Compute, find sub-graph output
    """

    for tensor in compute_output_tensor_list:
        if tensor not in is_used_tensor_list:
            output_list.append(tensor)
            is_used_tensor_list.add(tensor)
            input_tensor_cnt[tensor] = output_tensor_cnt[tensor]
        # expose the tensor while input cnt < output cnt
        elif output_tensor_cnt[tensor] > input_tensor_cnt[tensor]:
            output_list.append(tensor)
            input_tensor_cnt[tensor] = output_tensor_cnt[tensor]


def is_dump_compute_json(soc_info):
    if soc_info is None:
        return False
    op_debug_level = int(soc_info.get("op_debug_level", "0"))
    if op_debug_level == 4:
        return True
    op_debug_config = soc_info.get("op_debug_config", "")
    if len(op_debug_config) == 0:
        return False
    return "dump_UBfusion" in op_debug_config.split(",")


def fusion_op_static(json_data, compute_only, context_param=None, master_pid=None,
                     status_check=False, op_name=None, fixpipe_ub_cfg=None):
    import tbe.common.context.op_context as op_context
    bank_path_dict = OpImplPolicy.bank_path_kwargs
    if check_single_op(json_data) and not compute_only:
        with op_context.OpContext("pre_static"):
            context = op_context.get_context()
            context.add_addition("op_bank_path", bank_path_dict.get("op_bank_path", None))
            context.add_addition("mdl_bank_path", bank_path_dict.get("mdl_bank_path", None))
            context.add_addition("master_pid", master_pid)
            context.add_addition("status_check", status_check)
            context.add_addition("op_name", op_name)
            context.add_addition("build_options", fixpipe_ub_cfg)
            context.add_addition("input_c_values", get_fusion_op_placeholder_input_c(json_data["op_list"]))
            add_context_param(context, context_param)
            set_context_parameter(context, json_data)
            return single_op_build(json_data)

    modify_duplicated_inputs(json_data)

    # get params from json_data
    fusion_op_name = str(json_data["fusion_op_name"])
    op_list = json_data["op_list"]

    # check input args
    import tbe.common.utils.para_check as para_check
    para_check.check_kernel_name(fusion_op_name)

    # check fusion op type
    check_fusion_op_type(op_list)

    call_all_registered_functions(op_list)

    # handle fusion pass for op list
    handle_fusion_pass(op_list)

    # init
    tensor_list = {}  # collect all tensors in fusion template
    input_list = []  # record all input tensors for AI Core codegen
    input_tensor_cnt = {}  # record input tensor called count
    output_tensor_cnt = {}  # record output tensor called count
    output_list = []  # record output tensors' name for AI Core codegen
    compute_output_tensor_list = []  # record all compute output tensor
    stride_index = [0]
    input_desc_names = []
    output_desc_names = []
    input_tensor_list_name = []
    output_tensor_list_name = []
    all_op_output = []
    tensor_index = [0]
    l1_enable = json_data["SocInfo"]["l1Fusion"] == "true"
    lx_enable = get_lx_enable(json_data)
    # record tensor used in fusion_op
    # a tensor which is not used is a output tensor
    is_used_tensor_list = set()
    # combine computes
    params_count = [0]
    cmp_bool_storage_as_1bit = True
    enable_group_inplace = False
    enable_vector_2x = True
    op_caxis_valus = get_fusion_op_placeholder_input_c(op_list)
    collect_output_desc_names(op_list, output_desc_names)
    with op_context.OpContext("pre_static"):
        context = op_context.get_context()
        context.add_addition("op_bank_path", bank_path_dict.get("op_bank_path", None))
        context.add_addition("mdl_bank_path", bank_path_dict.get("mdl_bank_path", None))
        context.add_addition("op_name", op_name)
        context.add_addition("build_options", fixpipe_ub_cfg)
        context.add_addition("master_pid", master_pid)
        context.add_addition("status_check", status_check)
        context.add_addition("input_c_values", op_caxis_valus)
        add_context_param(context, context_param)
        set_context_parameter(context, json_data)
        set_op_info_to_context(op_list, fusion_op_name)
        op_type_list = []
        for op_node in op_list:
            # op with 'bool_storage_as_1bit' needs to add this config in fusion_op
            if op_node["type"] in bool_storage_as_1bit_oplist:
                cmp_bool_storage_as_1bit = False

            op_type_list.append(op_node["type"])
            change_type_dict = {"Conv3DBackpropInputD": True,
                                "FullyConnection": True,
                                "FullyConnectionCompress": True,
                                "MatMul": True,
                                "MatMulCompress": True,
                                "MatMulV2": True,
                                "MatMulV2Compress": True,
                                "BatchMatMul": True,
                                "BatchMatMulV2": True}
            tmp_enable_group_inplace = change_type_dict.get(op_node["type"], False)
            enable_group_inplace = enable_group_inplace or tmp_enable_group_inplace
            if "BatchMatMulV2" in op_type_list:
                enable_vector_2x = False

            if op_node["type"] == "Data":
                # create placeholder
                create_placeholder_tensor(op_node, tensor_list, input_list, op_list, params_count)
                continue
            # collect input tensors for this op
            op_input_list = []
            # Assem dynamic input parameter
            dyn_input_dict = {}
            check_input_desc_not_in_op(op_node)
            add_input_tensor(op_node, tensor_list, op_input_list,
                             is_used_tensor_list, input_tensor_cnt, dyn_input_dict,
                             input_desc_names)
            # set input buffer manager info for l1fusion
            set_input_buffer_manager(lx_enable, op_node, op_list, op_input_list, dyn_input_dict, stride_index,
                                     output_desc_names, input_tensor_list_name, output_tensor_list_name, tensor_index)
            set_l1_fusion_type(l1_enable, op_node)
            set_ub_space_size(op_node)
            # call op's compute
            op_output_list = call_op_compute(op_node, None, op_input_list, dyn_input_dict, False, fusion_op_name)
            collect_out_tensor_list_name(lx_enable, op_output_list, all_op_output, output_tensor_list_name)
            check_output_desc_not_in_op(op_node)
            update_out_attr(op_node, op_output_list)
            tensor_collect_params = TensorCollectParams(tensor_list, op_output_list,
                                                        compute_output_tensor_list, output_tensor_cnt)
            collect_out_tensor(op_node, tensor_collect_params)
        # find sub-graph output compute
        get_sub_graph_output(compute_output_tensor_list, is_used_tensor_list, output_list,
                             output_tensor_cnt, input_tensor_cnt)
        # set output buffer manager info for l1fusion
        set_output_buffer_manager(lx_enable, op_list, input_desc_names, stride_index, output_list, tensor_index)
        if compute_only:
            return output_list

        is_rl_conv2d_l1fusion = json_data.get("is_rl_conv2d_l1fusion", False)
        if is_rl_conv2d_l1fusion:
            context.add_addition("is_rl_conv2d_l1fusion", is_rl_conv2d_l1fusion)

        # generate schedule
        from tbe.tvm.target import cce as _cce
        with _cce():
            logger.info("[%s] start auto_schedule", fusion_op_name)
            # call auto_schedule
            sch = auto_schedule(output_list)
            logger.info("[%s] end auto_schedule", fusion_op_name)

        input_list = [ele for ele in input_list if ele is not None]
        real_output = get_real_output(sch, output_list)
        if is_dump_compute_json(json_data.get("SocInfo", None)):
            dump_compute_json(output_list, input_list, fusion_op_name)

        if is_rl_conv2d_l1fusion:
            return ""

        # codegen
        config = {"name": fusion_op_name,
                  "tensor_list": input_list + real_output,
                  "fusion_build_config": get_fusion_build_cfg(),
                  "bool_storage_as_1bit": cmp_bool_storage_as_1bit,
                  "enable_group_inplace": enable_group_inplace,
                  "enable_vector_2x": enable_vector_2x}

        from tbe.dsl import build as tbe_dsl_build
        logger.info("call tbe_dsl_build fusion_op_name = [%s].", fusion_op_name)
        tbe_dsl_build(sch, config)

        json_file_path = context.get_build_res("json_file_path")
        logger.info("fusion_op_name [%s] end, json_file_path=%s.", fusion_op_name, json_file_path)

        if not json_file_path:
            from tbe.common.buildcfg import get_current_build_config
            json_file_path = os.path.join(get_current_build_config("kernel_meta_parent_dir"), "kernel_meta",
                fusion_op_name + ".json")
            logger.warn("json_file_path=%s.", json_file_path)

        build_res = {"json_file_path": json_file_path}

        return build_res


def get_op_inputs_args(op_list, op_node):
    """
    get op inputs
    """
    inputs_args = []
    dyn_inputs = []
    dyn_idx = -1
    desc = op_node["input_desc"]
    for op_input in desc:
        if op_input["shape"] == "NULL":
            inputs_args.append(None)
        else:
            data_type = op_input.get("data_type")
            if data_type is not None:
                op_input["dtype"] = data_type
                del op_input["data_type"]
            idx = op_input.get("dyn_index")
            if idx is not None:
                if dyn_idx == -1:
                    dyn_idx = idx
                elif dyn_idx != idx:
                    logger.warn("get multiple dyn_index: %d, %d", dyn_idx, idx)
                dyn_inputs.append(op_input)
            else:
                inputs_args.append(op_input)
    if len(dyn_inputs) > 0:
        inputs_args.append(dyn_inputs)
    return inputs_args


def set_single_op_info_to_context(op_list, op_node, kernel_name, op_compute_func):
    """
    set single op_info to context
    """
    import tbe.common.context.op_context as op_context
    import tbe.common.context.op_info as operator_info
    context = op_context.get_context()
    update_attr_desc_by_null_desc(op_node)
    op_name = str(op_node["name"])
    op_type = str(op_node["type"])
    attrs = op_node["attr_desc"] if "attr_desc" in op_node else None
    private_attrs = op_node["private_attr_desc"] if "private_attr_desc" in op_node else None
    outputs = op_node["output_data_desc"] if "output_data_desc" in op_node else None
    context_extra_params = {}
    extra_params_str = op_node["extra_params"] if "extra_params" in op_node else None
    context_extra_params.update(get_extra_params(extra_params_str))
    op_impl_kwds = get_op_impl_mode_args(op_compute_func, op_node.get("op_impl_mode"))
    context_extra_params.update(op_impl_kwds)
    options_kwds = get_options_kwds(op_node, op_compute_func, False, None)
    context_extra_params.update(options_kwds)

    op_info = operator_info.OpInfo(str(op_node["name"]), str(op_node["type"]))
    op_info.pattern = str(op_node["pattern"])
    op_info.inputs = get_op_inputs_args(op_list, op_node)
    op_info.outputs = outputs
    op_info.attrs = attrs
    if private_attrs:
        op_info.attrs = attrs + private_attrs
    logger.debug("Op[%s, %s], op_info attrs: %s, private_attr: %s", op_name, op_type,
                str(op_info.attrs), str(private_attrs))
    kernel_kwds = get_fusion_op_kernel_name(op_node["func_name"], kernel_name)
    if "kernel_name" in kernel_kwds:
        op_info.kernel_name = str(kernel_kwds["kernel_name"])
    else:
        op_info.kernel_name = kernel_name
    op_info.extra_params = context_extra_params
    op_info.precision_mode = op_node.get("op_impl_mode")
    context.add_op_info(op_info)

    # before op use attrs which construct by themself(need name, type, value)
    # now use attrs in context directly
    graph_op_info = operator_info.OpInfo(str(op_node["name"]), str(op_node["type"]))
    graph_op_info.inputs = op_info.inputs
    graph_op_info.outputs = op_info.outputs
    graph_op_info.attrs = op_node["attr_info_desc"] if "attr_info_desc" in op_node else None
    if "private_attr_info_desc" in op_node:
        graph_op_info.attrs = graph_op_info.attrs + op_node["private_attr_info_desc"]
    if graph_op_info.attrs:
        for attr in graph_op_info.attrs:
            update_attr_value_by_null_desc(attr)
    context.set_graph_op_info(graph_op_info)
    return op_info


def set_op_info_to_context(op_list, kernel_name):
    """
    set opInfo to op context
    """
    for op_node in op_list:
        if op_node["type"] == "Data":
            continue

        op_compute_func = get_compute_func(op_node)
        if op_compute_func is None:
            raise RuntimeError("Compute function of node[%s, %s] is null." % (op_node["name"], op_node["type"]))
        set_single_op_info_to_context(op_list, op_node, kernel_name, op_compute_func)


OP_PATTERN_NODE_DICT = {
    Pattern.QUANT_CONV2D: "quant_conv2d",
    Pattern.CONV2D : "cube",
    Pattern.CONV2D_BACKPROP_INPUT : "conv2d_backprop_input",
    Pattern.CONV2D_BACKPROP_FILTER : "conv2d_backprop_filter",
    Pattern.REDUCE : "reduce",
    Pattern.NORM : "norm",
    Pattern.BROADCAST : "broadcast",
    Pattern.ELEMWISE : "elewise"
}


def get_op_mode_by_pattern(op_pattern):
    return OP_PATTERN_NODE_DICT.get(op_pattern) if op_pattern in OP_PATTERN_NODE_DICT.keys() else op_pattern


OP_PATTERN_PRIORITY_DICT = {
    Pattern.QUANT_CONV2D : 4,
    Pattern.CONV2D : 4,
    Pattern.CONV2D_BACKPROP_INPUT : 4,
    Pattern.CONV2D_BACKPROP_FILTER : 4,
    Pattern.MAT_MUL : 4,
    Pattern.BATCH_MATMUL : 4,
    Pattern.CONV3D : 4,
    Pattern.REDUCE : 3,
    Pattern.NORM : 3,
    Pattern.ASCEND_ANTI_QUANT : 2,
    Pattern.ASCEND_QUANT : 2,
    Pattern.BROADCAST : 1
}


def get_fusion_pattern(op_list):
    # FUSION_PATTERN: ELEWISE  ELEWISE_WITH_BROADCAST  REDUCE
    fusion_pattern = Pattern.ELEMWISE
    pattern_priority = 0
    for op_node in op_list:
        if "pattern" not in op_node:
            continue
        if op_node["pattern"] not in OP_PATTERN_PRIORITY_DICT.keys():
            continue
        priority = OP_PATTERN_PRIORITY_DICT.get(op_node["pattern"])
        if priority > pattern_priority:
            pattern_priority = priority
            fusion_pattern = op_node["pattern"]

    return fusion_pattern


CLASSIFY_TYPE_FUNC = {
        Pattern.REDUCE : ReduceClassifyFusion, Pattern.BROADCAST : CommonClassifyFusion,
        Pattern.ELEMWISE : CommonClassifyFusion, Pattern.NORM : NormClassifyFusion,
        Pattern.ASCEND_ANTI_QUANT : CommonClassifyFusion, Pattern.CONV2D : CubeClassifyFusion,
        Pattern.CONV2D_BACKPROP_FILTER : CubeClassifyFusion, Pattern.ASCEND_QUANT : CommonClassifyFusion,
        Pattern.QUANT_CONV2D: CubeClassifyFusion
    }


def get_classify_info(fusion_pattern, fusion_mode, op_list):
    global CLASSIFY_TYPE_FUNC

    def correct_d_type(op_desc_list):
        for op_desc in op_desc_list:
            data_type = op_desc.get("data_type")
            if data_type is None:
                continue
            op_desc["dtype"] = data_type

    for op in op_list:
        if "input_desc" in op:
            correct_d_type(op["input_desc"])
        if "output_desc" in op:
            correct_d_type(op["output_desc"])

    if fusion_pattern in CLASSIFY_TYPE_FUNC.keys():
        classify_func = CLASSIFY_TYPE_FUNC.get(fusion_pattern)
        return classify_func(op_list, fusion_pattern, fusion_mode)
    else:
        return CubeClassifyFusion(op_list, fusion_pattern, fusion_mode)


def modify_input_shape_range(inputs):
    for i, input in enumerate(inputs):
        in_ranges = input.get("range")
        if in_ranges is not None:
            tmp_ranges = []
            is_change = False
            for in_range in in_ranges:
                if len(in_range) > 1 and in_range[1] == -1:
                    tmp_range = (in_range[0], None)
                    tmp_ranges.append(tmp_range)
                    is_change = True
                else:
                    tmp_ranges.append(in_range)
            if is_change:
                input["range"] = tmp_ranges


def replace_dw_tvm_shapes(inputs, vector_info, op_list):
    """
    get input shape of dw transdata fusion
    """
    tvm_shapes_dict = {}
    dw_tvm_shapes = shape_util.variable_shape(list(inputs), op_mode="conv2d_backprop_filter")
    for i, input_member in enumerate(inputs):
        tvm_shapes_dict[input_member["name"]] = dw_tvm_shapes[i]
    for index in vector_info.placeholder_op.idx:
        op_list[index]["output_desc"][0]["shape"] = tvm_shapes_dict.get(op_list[index]["output_desc"][0]["name"])
        if op_list[index]["output_desc"][0].get("input_pattern") is None:
            op_list[index]["output_desc"][0]["input_pattern"] = "dw_transdata"


def replace_gemm_tvm_shapes(inputs, vector_info, op_list):
    """
    get input shape of dw transdata fusion
    """
    tvm_shapes_dict = {}
    gemm_tvm_shapes = shape_util.variable_shape(list(inputs), op_mode="gemm")
    for i, input_member in enumerate(inputs):
        tvm_shapes_dict[input_member["name"]] = gemm_tvm_shapes[i]
    for index in vector_info.placeholder_op.idx:
        op_list[index]["output_desc"][0]["shape"] = tvm_shapes_dict.get(op_list[index]["output_desc"][0]["name"])


def replace_dx_tvm_shapes(inputs, vector_info, op_list):
    """
    get input shape of dx transdata fusion
    """
    cube_inputs = []
    vector_inputs = []

    if platform_info.intrinsic_check_support("Intrinsic_conv_ub_to_ub"):
        tvm_shapes_dict = {}
        dx_tvm_shapes = shape_util.variable_shape(list(inputs), op_mode="conv2d_backprop_input")
        for i, input_desc in enumerate(inputs):
            tvm_shapes_dict[input_desc["name"]] = dx_tvm_shapes[i]
 
        for index in vector_info.placeholder_op.idx:
            op_list[index]["output_desc"][0]["shape"] = tvm_shapes_dict.get(op_list[index]["output_desc"][0]["name"])
        return

    for input_desc in inputs:
        if input_desc.get("input_pattern") == "cube":
            cube_inputs.append(input_desc)
        else:
            vector_inputs.append(input_desc)

    tvm_shapes_dict = {}
    cube_tvm_shapes = shape_util.variable_shape(list(cube_inputs), op_mode="conv2d_backprop_input")
    for i, input_desc in enumerate(cube_inputs):
        tvm_shapes_dict[input_desc["name"]] = cube_tvm_shapes[i]
    vector_tvm_shapes = shape_util.variable_shape(list(vector_inputs))
    for i, input_desc in enumerate(vector_inputs):
        tvm_shapes_dict[input_desc["name"]] = vector_tvm_shapes[i]

    for index in vector_info.placeholder_op.idx:
        op_list[index]["output_desc"][0]["shape"] = tvm_shapes_dict.get(op_list[index]["output_desc"][0]["name"])


def _inner_replace_cube_tvm_shapes(transdata_input_name_vec, inputs, cube_inputs, vector_inputs):
    # this situation is [conv2d + transdata]
    if len(transdata_input_name_vec) == 1:
        for input_op in inputs:
            if input_op.get("input_pattern") == "cube":
                if input_op.get("format") == "NC1HWC0":
                    input_op["input_pattern"] = "cube_transdata"
                cube_inputs.append(input_op)
            else:
                vector_inputs.append(input_op)
    # [transdata + conv2d + transdata] or other situation.
    else:
        for input_op in inputs:
            if input_op.get("input_pattern") == "cube":
                cube_inputs.append(input_op)
            elif input_op.get("name") in transdata_input_name_vec:
                input_op["input_pattern"] = "cube_transdata"
                cube_inputs.append(input_op)
            else:
                vector_inputs.append(input_op)


def replace_cube_tvm_shapes(inputs, vector_info, op_list, ins_attrs_options):
    cube_inputs = []
    vector_inputs = []

    transdata_op_type = "TransData"
    transdata_input_name_vec = []
    for op_node in op_list:
        if op_node.get("type") == transdata_op_type:
            transdata_input_name_vec.append(op_node["input_desc"][0]["name"])

    _inner_replace_cube_tvm_shapes(transdata_input_name_vec, inputs, cube_inputs, vector_inputs)

    tvm_shapes_dict = {}
    cube_tvm_shapes = shape_util.variable_shape(list(cube_inputs), op_mode="cube")
    for i, input in enumerate(cube_inputs):
        tvm_shapes_dict[input["name"]] = cube_tvm_shapes[i]

    if len(vector_inputs) > 0:
        if op_util_conv2d.is_conv2d_binary():
            vector_tvm_shapes = op_util_conv2d.replace_conv2d_vector_tvm_shapes(list(vector_inputs), ins_attrs_options)
        else:
            vector_tvm_shapes = shape_util.variable_shape(list(vector_inputs))
        for i, input in enumerate(vector_inputs):
            tvm_shapes_dict[input["name"]] = vector_tvm_shapes[i]

    for index in vector_info.placeholder_op.idx:
        op_list[index]["output_desc"][0]["shape"] = tvm_shapes_dict.get(op_list[index]["output_desc"][0]["name"])
        if op_list[index]["output_desc"][0].get("name") in transdata_input_name_vec:
            op_list[index]["output_desc"][0]["input_pattern"] = "cube_transdata"


def replace_tvm_shapes(fusion_pattern, fusion_mode, vector_info, op_list, ins, input_to_attr_list, ins_attrs_options):
    modify_input_shape_range(ins)
    if fusion_pattern in ["QuantConvolution"] and context.get_context().get_op_mode() == "static":
        logger.debug("[%s] op static fusion case do not do change var.", fusion_pattern)
        return
    if fusion_pattern in [Pattern.REDUCE, Pattern.BROADCAST, Pattern.NORM, Pattern.ELEMWISE,
                          Pattern.ASCEND_ANTI_QUANT, Pattern.ASCEND_QUANT]:
        if fusion_pattern in [Pattern.REDUCE, ]:
            if vector_info.label in ["D", ]:
                tvm_shapes = shape_util.variable_shape(list(ins), op_mode=fusion_mode)[0: len(ins) - 1]
                # update reduce func
                for index in vector_info.reduce_op.idx:
                    op_list[index]["attr_desc"][0] = ins[-1].get("value")
            else:
                if vector_info.axis_idx:
                    # axis is one of placeholders
                    axis_attr = ins[vector_info.axis_idx].get("value")
                    input_to_attr_list[vector_info.axis_idx] = axis_attr
                    tvm_shapes = shape_util.variable_shape(list(ins), op_mode=fusion_mode)
                else:
                    # axis not belong to placeholder
                    axis_attr = ins[-1].get("value")
                    input_to_attr_list[-1] = axis_attr
                    tvm_shapes = shape_util.variable_shape(list(ins), op_mode=fusion_mode)[0: len(ins) - 1]
        elif fusion_mode == "broadcast":
            # variable_shape does not support broadcast, using the default value which is elewise
            tvm_shapes = shape_util.variable_shape(list(ins))
        else:
            tvm_shapes = shape_util.variable_shape(list(ins), fusion_mode)

        # if we delete the null input before calling dsl function, need to add null to tvm_shapes.
        tvm_shapes = add_null_shape(vector_info, tvm_shapes)

        # update placeholder
        idx = 0
        for index in vector_info.placeholder_op.idx:
            op_list[index]["output_desc"][0]["shape"] = tvm_shapes[idx]
            idx += 1
    elif fusion_pattern == Pattern.CONV2D_BACKPROP_FILTER:
        replace_dw_tvm_shapes(ins, vector_info, op_list)
    elif fusion_pattern == Pattern.CONV2D_BACKPROP_INPUT:
        replace_dx_tvm_shapes(ins, vector_info, op_list)
    elif fusion_pattern in [Pattern.MAT_MUL, Pattern.GEMM, Pattern.BATCH_MATMUL]:
        replace_gemm_tvm_shapes(ins, vector_info, op_list)
    else:
        replace_cube_tvm_shapes(ins, vector_info, op_list, ins_attrs_options)


def add_null_shape(vector_info, tvm_shapes):
    if vector_info.null_shape_dict is not None:
        for i in vector_info.null_shape_dict.keys():
            tvm_shapes.insert(i, "NULL")
    return tvm_shapes


def check_vector_info(vector_info):
    ins_list_none = False
    if vector_info.ins_list is not None:
        return ins_list_none, vector_info.ins_list
    else:
        ins_list_none = True
        return ins_list_none, vector_info.ins_with_attr_list


def get_and_update_ins_by_flag(ins_list_none, fusion_pattern, ins_attrs_options, vector_info):
    if ins_list_none:
        return ins_attrs_options[0]
    else:
        if fusion_pattern == Pattern.NORM and len(ins_attrs_options) > 0:
            axes_attr = ins_attrs_options[-1]
            if vector_info.norm_output_desc is not None:
                vector_info.norm_output_desc["disable_fuse_axes"] = axes_attr
            if len(vector_info.norm_op.get("attr_desc")) > 0:
                vector_info.norm_op.get("attr_desc")[0] = axes_attr
            return ins_attrs_options[0:len(ins_attrs_options) - 1]
        else:
            return ins_attrs_options


def update_range_common(tensor_desc):
    if not isinstance(tensor_desc, dict):
        return
    shape = tensor_desc.get('shape', [])
    if not isinstance(shape, (list, tuple)):
        return
    shape_range = tensor_desc.get('range', [])
    if len(shape) != len(shape_range):
        tmp_range = []
        for dim in shape:
            dim1 = dim if dim > 0 else 1
            dim2 = dim if dim > 0 else None
            tmp_range.append([dim1, dim2])
        tensor_desc['range'] = tmp_range


def dynamic_range_padding(op_list):
    import tbe.common.context.op_context as op_context
    context = op_context.get_context()
    op_mode = context.get_op_mode()
    if op_mode == 'static':
        return
    for op_node in op_list:
        if "input_desc" in op_node:
            for input_desc in op_node["input_desc"]:
                update_range_common(input_desc)
        if "output_desc" in op_node:
            for output_desc in op_node["output_desc"]:
                update_range_common(output_desc)


def assemble_graph_op_name(op_list):
    first_node_name = ""
    graph_op_name = ""
    op_name_list = []
    for op_node in op_list:
        if op_node["type"] == "Data":
            continue

        if len(first_node_name) == 0:
            first_node_name = op_node["name"]

        graph_op_name += op_node["name"]

    if len(graph_op_name) > MAX_OP_NAME_LEN:
        graph_op_name = first_node_name + "_ub_fusion"

    return graph_op_name


def fusion_op_dynamic(json_data, compute_only):
    """
    fusion op for dynamic shape
    """
    # single_op_build support dynamic shape
    is_build_single_op = check_single_op(json_data) and not compute_only
    if is_build_single_op:
        return single_op_build(json_data)

    modify_duplicated_inputs(json_data)

    # get params from json_data
    fusion_op_name = str(json_data["fusion_op_name"])
    op_list = json_data["op_list"]

    import tbe.common.utils.para_check as para_check
    para_check.check_kernel_name(fusion_op_name)
    set_op_info_to_context(op_list, fusion_op_name)
    graph_op_name = assemble_graph_op_name(op_list)
    logger.debug("graph_op_name is [%s].", graph_op_name)
    set_graph_op_info_to_context(op_list, graph_op_name)

    # check fusion op type
    check_fusion_op_type(op_list)

    # for conv2d usage
    call_all_registered_functions(op_list)

    # handle fusion pass for op list
    handle_fusion_pass(op_list)

    op_caxis_valus = get_fusion_op_placeholder_input_c(op_list)
    import tbe.common.context
    conetxt = tbe.common.context.get_context()
    conetxt.add_addition("input_c_values", op_caxis_valus)
    """
    Create classify, need fusion_pattern just like Pattern.BROADCAST (sub.py)
    ins_list: new inputs'shapes from create_classify
    update_op_list: update new shapes in op_list
    """
    fusion_pattern = get_fusion_pattern(op_list)
    fusion_mode = get_op_mode_by_pattern(fusion_pattern)
    logger.debug("current fusion pattern and mode is [%s, %s]", fusion_pattern, fusion_mode)
    dynamic_range_padding(op_list)
    vector_info = get_classify_info(fusion_pattern, fusion_mode, op_list)

    # ins_list classify shape
    schedules_list, tensors_list = [], []
    ori_op_list = op_list
    real_ins_list = []
    ins_list_none = False
    ins_list_none, real_ins_list = check_vector_info(vector_info)
    enable_branch_eliminator = True
    cmp_bool_storage_as_1bit = True
    caxis_valus = []
    for ins_attrs_options in real_ins_list:
        #############################
        #####Init for each sch#######
        #############################
        ins = get_and_update_ins_by_flag(ins_list_none, fusion_pattern, ins_attrs_options, vector_info)

        op_list = copy.deepcopy(ori_op_list)
        tensor_list = {}  # collect all tensors in fusion template
        input_list = []  # record all input tensors for AI Core codegen
        input_tensor_cnt = {}  # record input tensor called count
        input_desc_names = []
        output_tensor_cnt = {}  # record output tensor called count
        output_list = []  # record output tensors' name for AI Core codegen
        compute_output_tensor_list = []  # record all compute output tensor
        input_to_attr_list = {}  # record input tensor which needs to be replaced by attr
        # record tensor used in fusion_op
        # a tensor which is not used is a output tensor
        is_used_tensor_list = set()
        # combine computes
        params_count = [0]
        enable_branch_eliminator = True
        cmp_bool_storage_as_1bit = True

        from tbe.dsl import compute as tbe_dsl_compute
        with tbe_dsl_compute():
            #############################
            #####Create New Shapes#######
            #############################
            replace_tvm_shapes(fusion_pattern, fusion_mode, vector_info, op_list,
                               ins, input_to_attr_list, ins_attrs_options)

            # Placeholder + Compute
            for op_index, op_node in enumerate(op_list):
                # op with 'bool_storage_as_1bit' needs to add this config in fusion_op
                if op_node["type"] in bool_storage_as_1bit_oplist:
                    cmp_bool_storage_as_1bit = False
                if op_node["type"] in enable_branch_eliminator_oplist:
                    enable_branch_eliminator = False

                if op_node["type"] == "Data":
                    # create placeholder
                    create_placeholder_tensor(op_node, tensor_list, input_list, op_list, params_count)
                    continue

                # collect input tensors for this op
                op_input_list = []
                # Assem dynamic input parameter
                dyn_input_dict = {}

                check_input_desc_not_in_op(op_node)
                add_input_tensor(op_node, tensor_list, op_input_list,
                                is_used_tensor_list, input_tensor_cnt, dyn_input_dict, input_desc_names)
                is_replace_attr = fusion_pattern in [Pattern.REDUCE, ] and op_index in vector_info.reduce_op.idx
                if is_replace_attr:
                    replace_attr_for_op_input_list(op_input_list, input_to_attr_list)
                # call op's compute
                op_output_list = call_op_compute(op_node, ins_attrs_options, op_input_list,
                                                 dyn_input_dict, ins_list_none, fusion_op_name)
                check_output_desc_not_in_op(op_node)
                update_out_attr(op_node, op_output_list)
                tensor_collect_params = TensorCollectParams(tensor_list, op_output_list,
                                                            compute_output_tensor_list, output_tensor_cnt)
                collect_out_tensor(op_node, tensor_collect_params)
            # After Compute, find sub-graph output
            get_sub_graph_output(compute_output_tensor_list, is_used_tensor_list, output_list,
                                 output_tensor_cnt, input_tensor_cnt)

        # generate schedule
        from tbe.tvm.target import cce as _cce
        with _cce():
            logger.info("[%s] start call auto_schedule.", fusion_op_name)
            # call auto_schedule
            sch = auto_schedule(output_list)
            logger.info("[%s] end auto_schedule.", fusion_op_name)
        real_output = get_real_output(sch, output_list)
        input_list = [ele for ele in input_list if ele is not None]
        input_list += real_output
        # each sch: each input_list
        tensors_list.append(input_list)
        if is_dump_compute_json(json_data.get("SocInfo", None)):
            dump_compute_json(output_list, input_list, fusion_op_name)
        schedules_list.append(sch)

    is_add_axis_contesxt = fusion_pattern in [Pattern.REDUCE, ] and \
                           vector_info.label not in ["D", ] and hasattr(vector_info, 'axis_idx')

    import tbe.common.context.op_context as op_context

    context1 = op_context.get_context()
    if is_add_axis_contesxt:
        context1.add_compile_info("axes_idx", vector_info.axis_idx)
    # codegen
    config = {"name": fusion_op_name,
              "tensor_list": tensors_list,
              "fusion_build_config": get_fusion_build_cfg(),
              "bool_storage_as_1bit": cmp_bool_storage_as_1bit,
              "enable_branch_eliminator_else_case ": enable_branch_eliminator}

    from tbe.dsl import build as tbe_dsl_build
    logger.info("call tbe_dsl_build fusion_op_name = [%s].", fusion_op_name)
    tbe_dsl_build(schedules_list, config)
    logger.info("fusion_op_name tbe_dsl_build [%s] end.", fusion_op_name)

    build_res = assemble_build_res(context1, fusion_op_name, json_data)
    return build_res


def fusion_op_dynamic_v2(json_data, compute_only):
    """
    fusion op for dynamic shape without classify
    """
    logger.info("Start to excute fusion_op_dynamic_v2")
    # single_op_build support dynamic shape
    is_build_single_op = check_single_op(json_data) and not compute_only
    if is_build_single_op:
        return single_op_build(json_data)

    # get params from json_data
    modify_duplicated_inputs(json_data)

    # get params from json_data
    fusion_op_name = str(json_data["fusion_op_name"])
    op_list = json_data["op_list"]

    import tbe.common.utils.para_check as para_check
    para_check.check_kernel_name(fusion_op_name)
    set_op_info_to_context(op_list, fusion_op_name)
    set_graph_op_info_to_context(op_list, fusion_op_name)

    op_caxis_valus = get_fusion_op_placeholder_input_c(op_list)
    import tbe.common.context
    conetxt = tbe.common.context.get_context()
    conetxt.add_addition("input_c_values", op_caxis_valus)
    only_fusion_check = json_data.get("only_fusion_check")
    if only_fusion_check:
        logger.info("only_fusion_check is true")
        conetxt.add_addition("_only_fusion_check", True)

    dynamic_range_padding(op_list)

    schedules_list, tensors_list = [], []
    ori_op_list = op_list
    enable_branch_eliminator = True
    cmp_bool_storage_as_1bit = True
    caxis_valus = []

    #############################
    #####Init for each sch#######
    #############################
    op_list = copy.deepcopy(ori_op_list)
    tensor_list = {}  # collect all tensors in fusion template
    input_list = []  # record all input tensors for AI Core codegen
    input_tensor_cnt = {}  # record input tensor called count
    input_desc_names = []
    output_tensor_cnt = {}  # record output tensor called count
    output_list = []  # record output tensors' name for AI Core codegen
    compute_output_tensor_list = []  # record all compute output tensor
    input_to_attr_list = {}  # record input tensor which needs to be replaced by attr
    # record tensor used in fusion_op
    # a tensor which is not used is a output tensor
    is_used_tensor_list = set()
    # combine computes
    params_count = [0]
    enable_branch_eliminator = True
    cmp_bool_storage_as_1bit = True

    from tbe.tvm.target import cce as _cce
    import tbe.common.context.op_context as op_context
    from tbe.dsl import build as tbe_dsl_build
    context1 = op_context.get_context()

    #############################
    #####Create New Shapes#######
    #############################
    # Placeholder + Compute
    for op_index, op_node in enumerate(op_list):
        # op with 'bool_storage_as_1bit' needs to add this config in fusion_op
        if op_node["type"] in bool_storage_as_1bit_oplist:
            cmp_bool_storage_as_1bit = False
        if op_node["type"] in enable_branch_eliminator_oplist:
            enable_branch_eliminator = False
        if op_node["type"] == "Data":
            # create placeholder
            create_placeholder_tensor_v2(op_node, tensor_list, input_list, op_list, params_count)
            continue
        # collect input tensors for this op
        op_input_list = []
        # Assem dynamic input parameter
        dyn_input_dict = {}
        check_input_desc_not_in_op(op_node)
        add_input_tensor(op_node, tensor_list, op_input_list,
                        is_used_tensor_list, input_tensor_cnt, dyn_input_dict, input_desc_names, True)
        # call op's compute
        op_output_list = call_op_compute(op_node, None, op_input_list, dyn_input_dict, False, fusion_op_name, True)
        check_output_desc_not_in_op(op_node)
        tensor_collect_params = TensorCollectParams(tensor_list, op_output_list,
                                                    compute_output_tensor_list, output_tensor_cnt)
        collect_out_tensor(op_node, tensor_collect_params, True)
    # After Compute, find sub-graph output
    get_sub_graph_output(compute_output_tensor_list, is_used_tensor_list, output_list,
                         output_tensor_cnt, input_tensor_cnt)
    # generate schedule
    with _cce():
        logger.info("[%s] start auto_schedule.", fusion_op_name)
        # call auto_schedule
        sch = auto_schedule(output_list)
        logger.info("[%s] end auto_schedule.", fusion_op_name)

    real_output = get_real_output(sch, output_list)
    input_list = [ele for ele in input_list if ele is not None]
    input_list += real_output
    # each sch: each input_list
    tensors_list.extend(input_list)

    # codegen
    config = {"name": fusion_op_name,
              "tensor_list": tensors_list,
              "fusion_build_config": get_fusion_build_cfg(),
              "bool_storage_as_1bit": cmp_bool_storage_as_1bit,
              "enable_branch_eliminator_else_case ": enable_branch_eliminator}

    logger.info("call tbe_dsl_build fusion_op_name=%s.", fusion_op_name)
    tbe_dsl_build(sch, config)
    logger.info("fusion_op_name tbe_dsl_build [%s] end.", fusion_op_name)

    build_res = assemble_build_res(context1, fusion_op_name, json_data)
    if only_fusion_check:
        add_fusion_check_res(build_res, context1)

    logger.info("build_res=%s.", build_res)
    return build_res


def add_fusion_check_res(build_res, context1):
    # res_code, 0: success; -1: fail
    build_res["fusion_check_result"] = -1
    fusion_check_result = context1.get_addition("_fusion_check_result")
    logger.info("fusion_check_result is %s", fusion_check_result)
    if fusion_check_result is not None:
        build_res["fusion_check_result"] = fusion_check_result


def assemble_build_res(context1, fusion_op_name, json_data):
    json_file_path = context1.get_build_res("json_file_path")
    logger.debug("json_file_path=%s.", json_file_path)

    if not json_file_path:
        from tbe.common.buildcfg import get_current_build_config
        json_file_path = os.path.join(get_current_build_config("kernel_meta_parent_dir"),
            "kernel_meta", fusion_op_name + ".json")
        logger.warn("json_file_path=%s.", json_file_path)

    # check if it is in dynamic shape scenario, it may be static shape too.
    has_dyn_shape = has_dynshape(json_data["op_list"])
    if (has_dyn_shape):
        update_compile_info(json_file_path, context1.get_compile_info(None))

    build_res = {"json_file_path": json_file_path}
    if not os.path.exists(json_file_path) and has_dyn_shape:
        build_res.update({"compile_info": context1.get_compile_info(None)})

    return build_res


def set_graph_op_info_to_context(op_list, graph_op_name):
    """
    set graph opInfo to op context
    find output, input and attrs of subgraph from op_list
    and then set them to graph_op_info
    """
    input_name_list = []
    output_name_list = []
    input_dict = {}
    output_dict = {}
    attr_list = []
    for op_node in op_list:
        if op_node["type"] == "Data":
            continue

        is_attrinfo_valid = "attr_info_desc" in op_node and op_node["attr_info_desc"] is not None
        attr_list.extend(op_node["attr_info_desc"] if is_attrinfo_valid else [])
        for attr in attr_list:
            update_attr_value_by_null_desc(attr)

        outputs = op_node["output_desc"] if "output_desc" in op_node else None
        for output in outputs:
            data_type = output.get("data_type")
            if data_type is not None:
                output["dtype"] = data_type
                del output["data_type"]
            output_name = output["name"]
            output_name_list.append(output_name)
            output_dict[output_name] = output

        inputs = op_node["input_desc"] if "input_desc" in op_node else None
        for op_input in inputs:
            input_name = op_input["name"]
            input_name_list.append(input_name)

            if op_input["shape"] == "NULL":
                op_input = None
            else:
                data_type = op_input.get("data_type")
                if data_type is not None:
                    op_input["dtype"] = data_type
                    del op_input["data_type"]
            input_dict[input_name] = op_input

    graph_output_name = [name for name in output_name_list if name not in input_name_list]
    graph_input_name = [name for name in input_name_list if name not in output_name_list]
    graph_outputs = []
    for name in graph_output_name:
        graph_outputs.append(output_dict[name])
    graph_inputs = []
    for name in graph_input_name:
        graph_inputs.append(input_dict[name])

    import tbe.common.context.op_context as op_context
    import tbe.common.context.op_info as operator_info
    local_context = op_context.get_context()
    graph_op_info = operator_info.OpInfo(graph_op_name, "")
    graph_op_info.inputs = graph_inputs
    graph_op_info.outputs = graph_outputs
    graph_op_info.attrs = attr_list
    local_context.set_graph_op_info(graph_op_info)


def set_kernel_meta_parent_dir(kwargs):
    """set kernel_meta parent dir

    """
    if not isinstance(kwargs, dict):
        return

    op_debug_dir_str = kwargs.get('op_debug_dir', '.')
    if op_debug_dir_str is (None or ""):
        return

    kernel_meta_dir = op_debug_dir_str
    if not os.path.exists(kernel_meta_dir):
        try:
            os.makedirs(kernel_meta_dir, stat.S_IRWXU + stat.S_IRGRP + stat.S_IXGRP)
        except FileExistsError:
            pass
    kwargs['op_debug_dir'] = kernel_meta_dir
    return


def set_vector_fp_ceiling(kwargs):
    """
    set set_vector_fp_ceiling:

    the op compile args : -cce-aicore-fp-ceiling {0, 1, 2}
    """
    if not isinstance(kwargs, dict):
        return
    vector_fp_ceiling_str = kwargs.get('vector_fp_ceiling', '2')
    if vector_fp_ceiling_str is (None or ""):
        return
    vector_fp_ceiling_strs = ('0', '1', '2')
    if vector_fp_ceiling_str not in vector_fp_ceiling_strs:
        raise RuntimeError("Unsupported vector_fp_ceiling, it must be "
                           "one of ('0', '1', '2') and the data type "
                           "must be string ")
    vector_fp_ceiling_int = int(vector_fp_ceiling_str)
    kwargs['vector_fp_ceiling'] = vector_fp_ceiling_int
    return


def check_and_get_l1_fusion(l1_fusion):
    # check l1_fusion
    if l1_fusion is None:
        l1_fusion = ""
    elif l1_fusion is True:
        l1_fusion = "true"
    elif l1_fusion is False:
        l1_fusion = "false"
    elif l1_fusion in ("True", "False", "TRUE", "FALSE", "true", "false", ""):
        l1_fusion = l1_fusion.lower()
    else:
        raise RuntimeError("Unsupported l1_fusion: %s" % l1_fusion)

    return l1_fusion


def set_l1_l2_fusion_enabled(l1_fusion, l2_fusion, kwargs):
    if not isinstance(kwargs, dict):
        return
    l1_fusion = check_and_get_l1_fusion(l1_fusion)
    if l1_fusion in ("true", True):
        kwargs['enable_L1_fusion'] = True
    else:
        kwargs['enable_L1_fusion'] = False
    l2_fusion = check_and_get_l1_fusion(l2_fusion)
    if l2_fusion in ("true", True):
        kwargs['enable_L2_fusion'] = True
    else:
        kwargs['enable_L2_fusion'] = False


def check_valid_core_type(build_type, core_type):
    logger.debug("core type: %s, build_type: %s ", core_type, str(build_type))
    if core_type in ("AiCore", "VectorCore", "MIX_VECTOR_CORE", "MIX_AICORE", None):
        if core_type is None and build_type != 0:
            logger.error("Unsupported core type: %s, build_type: %s ", core_type, str(build_type))
            raise RuntimeError("core type can't be None")
    elif (core_type == "Default") and build_type == 0:
        logger.warn("prebuild core type: %s.", core_type)
    else:
        logger.error("Unsupported core type: %s", core_type)
        raise RuntimeError("Unsupported core type: %s" % core_type)


def set_soc_info(soc_version, core_type="AiCore",
                 aicore_num=None, l1_fusion=None,
                 l2_mode="0", l2_fusion=None, kwargs=None):
    from tbe.common.platform.platform_info import set_soc_spec
    set_soc_spec(str(soc_version))
    set_soc_spec(str(core_type))
    set_soc_spec(str(aicore_num))
    set_soc_spec(str(l1_fusion))


def get_kwargs(soc_version, core_type="AiCore",
               aicore_num=None, l1_fusion=None,
               l2_mode="0", l2_fusion=None, kwargs=None):
    if not kwargs:
        kwargs = {}
    kwargs['l2_mode'] = int(l2_mode)
    set_vector_fp_ceiling(kwargs)
    set_l1_l2_fusion_enabled(l1_fusion, l2_fusion, kwargs)
    return kwargs


def check_child_process():
    """
    check whether child process is alive

    :return: bool
    """
    import multiprocessing.process as mpr
    try:
        child_list = mpr.active_children()
        for child in child_list:
            child.is_alive()
    except AssertionError:
        return False
    finally:
        pass
    return True


def fix_tracker():
    """
    check whether tracker is usable

    :return: bool
    """
    if sys.version_info[1] == 7:
        import multiprocessing.semaphore_tracker as tr
    else:
        import multiprocessing.resource_tracker as tr

    try:
        tr.ensure_running()
    except ChildProcessError:
        return False
    finally:
        pass
    return True


def fix_forkserver():
    """
    check whether forkserver is usable

    :return: bool
    """
    import multiprocessing.forkserver as fs
    try:
        fs.ensure_running()
    except ChildProcessError:
        return False
    finally:
        pass
    return True


def multi_process_check():
    """
    forkserver is unusable in forked process if forkserver is used in parent process before forking,
    as forkserver objects are inherited from parent process and can't be used by child process.

    :return: bool
    """
    if sys.version_info[1] < 7:
        raise RuntimeError("Wrong python version: {}, should >= 3.7".format(sys.version))

    if not fix_tracker():
        logger.error("multiprocessing tracker check failed.")
        return False

    if not fix_forkserver():
        logger.error("multiprocessing forkserver check failed.")
        return False

    if not check_child_process():
        logger.error("multiprocessing child process check failed.")
        return False

    return True


tensor_attr_list = ["shape", "ori_shape", "format", "ori_format", "dtype", "range", "ori_range", "addr_type",
                    "use_l1_workspace", "l1_addr_flag", "l1_fusion_type", "split_index", "l1_workspace_size",
                    "l1_addr_offset", "l1_valid_size", "is_first_layer", "slice_offset", "valid_shape", "total_shape"]


class TensorData:
    """
    class for input or output tensor, def tensor data and process
    """
    def __init__(self, attr_dict):
        for attr in tensor_attr_list:
            if attr in attr_dict:
                setattr(self, attr, attr_dict[attr])
            else:
                setattr(self, attr, None)

        if self.ori_format is None:
            logger.debug("Ori_format is none, set as format.")
            self.ori_format = self.format

        if self.shape == []:
            logger.debug("Shape is [], set as [1].")
            self.shape = [1]

        if self.ori_shape is None:
            logger.debug("Ori_shape is none, set as shape.")
            self.ori_shape = self.shape

    @staticmethod
    def replace_negative_one_with_none(cur_range):
        """
        resolve op info range -1 to None, and range is a two dimensional array list
        """
        logger.debug("Resolve range enter: %s.", str(cur_range))
        # if cur_range is None or len(cur_range) == 0:
        if cur_range is None:
            return

        for row_index, row in enumerate(cur_range):
            for col_index, value in enumerate(row):
                if value == -1:
                    cur_range[row_index][col_index] = None

        logger.debug("Resolve range exit: %s.", str(cur_range))
        return

    def is_tensor_null(self):
        """
        check tensor data is all none
        """
        is_null = True
        for attr in tensor_attr_list:
            if getattr(self, attr, None) is not None:
                is_null = False
                break;
        if is_null:
            return True
        elif getattr(self, "format", None) == "FORMAT_RESERVED":
            return True
        elif getattr(self, "dtype", None) == "undefined":
            return True
        else:
            return False


    def adjust_range_value(self):
        """
        adjust tensor range value
        """
        self.replace_negative_one_with_none(self.range)
        self.replace_negative_one_with_none(self.ori_range)

    @staticmethod
    def set_tensor_dict(tensor_dict, key, value):
        """
        generate tensor dict not none value
        """
        if value is not None:
            tensor_dict[key] = value

    def create_tensor_dict(self):
        """
        generate dict from tensor data
        """
        tensor_dict = {}
        for attr in tensor_attr_list:
            self.set_tensor_dict(tensor_dict, attr, getattr(self, attr, None))
        logger.debug("tensor_dict is %s.", str(tensor_dict))
        return tensor_dict


def get_input_or_output_from_json(json_desc, input_or_output):
    """
    resolve op info input or output from json op
    """
    def _resolve_tensor(tensor):
        # reslove field
        attr_dict = {}
        for attr in tensor_attr_list:
            attr_dict[attr] = tensor.get(attr)
        tensor_data = TensorData(attr_dict)

        if tensor_data.is_tensor_null():
            logger.debug("Tensor is null.")
            return None

        tensor_data.adjust_range_value()
        return tensor_data.create_tensor_dict()

    logger.debug("Start to resolve %s, json is %s", input_or_output, str(json_desc[input_or_output]))
    try:
        input_output_tuple = ()
        for tensors in json_desc.get(input_or_output):
            if tensors is None:
                logger.debug("Tensor is null.")
                input_output_tuple += (None, ) # null tensor set None
                continue
            if isinstance(tensors, list):
                tensor_list = ()
                for item in tensors:
                    tensor_list += (_resolve_tensor(item), )
                input_output_tuple += (tensor_list, )
            else:
                input_output_tuple += (_resolve_tensor(tensors), )

        return input_output_tuple
    except Exception as e:
        raise RuntimeError("Exception: Failed to resolve %s, exception is %s." % (input_or_output, str(e))) from e
    finally:
        pass


def create_attr_dict(name, dtype, value):
    """
    create attr dict
    """
    attr_dict = {}
    attr_dict["name"] = name
    attr_dict["dtype"] = dtype
    attr_dict["value"] = value
    return attr_dict


def get_attr_from_json(json_desc):
    """
    resolve op info attr from json op
    """
    try:
        attrs = json_desc.get("attrs")
        attrs_list = []
        if attrs is not None:
            for attr in attrs:
                if attr is None:
                    attrs_list.append(None)
                    continue
                # reslove field
                name = attr.get("name")
                if name is None:
                    raise RuntimeError("Attr key:[name] is not exist in json.")
                dtype = attr.get("dtype")
                if dtype is None:
                    raise RuntimeError("Attr key:[dtype] is not exist in json.")
                value = attr.get("value")

                # create attr dict
                attr_dict = create_attr_dict(name, dtype, value)

                # create attr list
                attrs_list.append(attr_dict)

        logger.debug("attrs_list: %s.", str(attrs_list))
        return attrs_list
    except Exception as e:
        raise RuntimeError("Exception: Failed to resolve attrs, exception is %s." % str(e)) from e
    finally:
        pass


def get_graph_inputs_outputs(graph_desc, graph_inputs, graph_outputs):
    input_name_list = []
    output_name_list = []
    input_dict = {}
    output_dict = {}
    attr_list = []
    for op_node in graph_desc["op_list"]:
        if op_node["type"] == "Data":
            continue

        outputs = op_node["output_desc"] if "output_desc" in op_node else None
        if outputs is None:
            continue
        for output in outputs:
            output_name = output["name"]
            output_name_list.append(output_name)
            output_dict[output_name] = output

        inputs = op_node["input_desc"] if "input_desc" in op_node else None
        if inputs is None:
            continue
        for op_input in inputs:
            if op_input["shape"] != "NULL":
                input_name = op_input["name"]
                input_name_list.append(input_name)
                input_dict[input_name] = op_input

    graph_output_name = [name for name in output_name_list if name not in input_name_list]
    graph_input_name = [name for name in input_name_list if name not in output_name_list]
    logger.debug("input_name: %s, output_name: %s", str(graph_input_name), str(graph_output_name))
    for name in graph_output_name:
        graph_outputs.append(output_dict[name])
    for name in graph_input_name:
        graph_inputs.append(input_dict[name])


def update_graph_desc_with_generalized_info(graph_desc, generalized_desc):
    graph_inputs = []
    graph_outputs = []
    get_graph_inputs_outputs(graph_desc, graph_inputs, graph_outputs)

    generalized_inputs = generalized_desc["inputs"]
    if len(graph_inputs) != len(generalized_inputs):
        logger.error("lenth of graph_inputs[%d] is not equal to generalized_inputs[%d].",
                     len(graph_inputs), len(generalized_inputs))
        return ""

    for index, op_input in enumerate(graph_inputs):
        for attr, value in generalized_inputs[index].items():
            if attr != "name":
                op_input[attr] = value
        logger.debug("op_input: %s", str(op_input))

    generalized_attrs_list = generalized_desc["graphOpParams"]
    if generalized_attrs_list is not None and len(generalized_attrs_list) > 0:
        if len(graph_inputs) != len(generalized_attrs_list):
            logger.warn("length of graph_inputs[%d] is not equal to generalized_attrs[%d].",
                        len(graph_inputs), len(generalized_attrs_list))
        for index, generalized_attrs in enumerate(generalized_attrs_list):
            attrs_value = []
            if len(generalized_attrs) > 0:
                for attr in generalized_attrs["attrs"]:
                    attrs_value.append(attr["value"])
                graph_inputs[index]["attr_desc"] = attrs_value

    return str(graph_desc).replace("'", '"').replace("True", "true").replace("False", "false").replace("None", "null")


def compile_kernel_fusion(json_str):
    json_data = json.loads(json_str)
    task_info = json_data.get("task_info")

    from tbe.tvm.driver.cce_build_module import task_fusion
    import tbe.common.context.op_context as op_context
    with op_context.OpContext(""):
        te_log.tefusion_log_full(te_log.LogLevel.INFO, "Start to call task_fusion, task_info is %s", str(task_info))
        task_fusion(task_info)
        json_file_path = op_context.get_context().get_build_res("json_file_path")
        logger.info("End to call task_fusion, json_file_path is %s", json_file_path)
    build_res = {"json_file_path": json_file_path}
    return build_res


def compile_super_kernel(json_str):
    json_data = json.loads(json_str)
    kernel_name = json_data["kernel_name"]
    ascendc_super_kernel_impl = "tbe.tikcpp.ascendc_super_kernel"
    opm = importlib.import_module(ascendc_super_kernel_impl)
    op_func = getattr(opm, "ascendc_super_kernel_plus")
    import tbe.common.context.op_context as op_context
    with op_context.OpContext("super_kernel"):
        op_func(json_data, kernel_name)
        json_file_path = op_context.get_context().get_build_res("json_file_path")
        if not json_file_path:
            from tbe.common.buildcfg import get_current_build_config
            json_file_path = os.path.join(get_current_build_config("kernel_meta_parent_dir"),
                "kernel_meta", json_data["kernel_name"] + ".json")
            logger.debug("json_file_path= %s.", json_file_path)
        build_res = {"json_file_path": json_file_path, \
                     "compile_info":op_context.get_context().get_compile_info(None)}
    return build_res


class SingleOpParams:
    """
    class for input or output tensor, def tensor data and process
    """
    def __init__(self, graph_id, task_id, op_module, op_type, op_func, kernel_name, inputs, outputs, attrs,
                 unknown_shape, int64_mode, options, pre_task_json, post_task_json, is_dynamic_impl, op_pattern,
                 context_param, pass_opt_list, op_name, extra_params, relation_param, op_impl_switch,
                 op_impl_mode, l1_size, optional_input_mode, dynamic_param_mode):
        self.graph_id = graph_id
        self.task_id = task_id
        self.op_module = op_module
        self.op_type = op_type
        self.op_func = op_func
        self.kernel_name = kernel_name
        self.inputs = inputs
        self.outputs = outputs
        self.attrs = attrs
        self.unknown_shape = unknown_shape
        self.int64_mode = int64_mode
        self.options = options
        self.pre_task_json = pre_task_json
        self.post_task_json = post_task_json
        self.is_dynamic_impl = is_dynamic_impl
        self.op_pattern = op_pattern
        self.context_param = context_param
        self.pass_opt_list = pass_opt_list
        self.op_name = op_name
        self.extra_params = extra_params
        self.relation_param = relation_param
        self.op_impl_switch = op_impl_switch
        self.op_impl_mode = op_impl_mode
        self.l1_size = l1_size
        self.optional_input_mode = optional_input_mode
        self.dynamic_param_mode = dynamic_param_mode


def get_params_from_task_desc(task_desc_str):
    task_desc = json.loads(task_desc_str)
    graph_id = task_desc["graph_id"]
    task_id = task_desc["task_id"]

    op_module = task_desc["op_module"]
    op_type = task_desc["op_type"]
    op_func = task_desc["op_func"]
    kernel_name = task_desc["kernel_name"]
    inputs = get_input_or_output_from_json(task_desc, "inputs")
    outputs = get_input_or_output_from_json(task_desc, "outputs")
    attrs = get_attr_from_json(task_desc)
    attrs.append(kernel_name)
    unknown_shape = task_desc["unknown_shape"]
    int64_mode = task_desc["int64_mode"]
    options = task_desc["options"]
    optional_input_mode = None
    if "optional_input_mode" in task_desc:
        optional_input_mode = task_desc["optional_input_mode"]
    dynamic_param_mode = None
    if "dynamic_param_mode" in task_desc:
        dynamic_param_mode = task_desc["dynamic_param_mode"]
    pre_task_json = None
    if "pre_task" in task_desc:
        pre_task = task_desc["pre_task"]
        if pre_task is not None:
            pre_task_json = json.loads(pre_task)
    post_task_json = None
    if "post_task" in task_desc:
        post_task = task_desc["post_task"]
        if post_task is not None:
            post_task_json = json.loads(post_task)

    is_dynamic_impl = task_desc["is_dynamic_impl"]
    op_pattern = task_desc["op_pattern"]
    context_param = task_desc["context_param"]
    pass_opt_list = task_desc["pass_opt_list"]
    op_name = task_desc["op_name"]
    extra_params = task_desc["extra_params"]
    relation_param = task_desc["relation_param"]
    op_impl_switch = task_desc["op_impl_switch"]
    op_impl_mode = task_desc["op_impl_mode"]
    l1_size = task_desc["l1_size"]

    return  SingleOpParams(graph_id, task_id, op_module, op_type, op_func, kernel_name, inputs, outputs, attrs,
                           unknown_shape, int64_mode, options, pre_task_json, post_task_json, is_dynamic_impl,
                           op_pattern, context_param, pass_opt_list, op_name, extra_params, relation_param,
                           op_impl_switch, op_impl_mode, l1_size, optional_input_mode, dynamic_param_mode)


class FusionOpParams:
    """
    class for input or output tensor, def tensor data and process
    """
    def __init__(self, graph_id, task_id, json_str, kernel_name, pre_task_json, post_task_json, options, context_param,
                 pass_opt_list, op_name, reused_relation, fixpipe_ub_cfg, optional_input_mode, l1_size,
                 dynamic_param_mode):
        self.graph_id = graph_id
        self.task_id = task_id
        self.json_str = json_str
        self.kernel_name = kernel_name
        self.pre_task_json = pre_task_json
        self.post_task_json = post_task_json
        self.options = options
        self.context_param = context_param
        self.pass_opt_list = pass_opt_list
        self.op_name = op_name
        self.reused_relation = reused_relation
        self.fixpipe_ub_cfg = fixpipe_ub_cfg
        self.optional_input_mode = optional_input_mode
        self.l1_size = l1_size
        self.dynamic_param_mode = dynamic_param_mode


def get_params_from_graph_desc(task_desc_str):
    task_desc = json.loads(task_desc_str)
    graph_id = task_desc["graph_id"]
    task_id = task_desc["task_id"]
    graph_desc_str = task_desc["graph_desc"]
    graph_desc = json.loads(graph_desc_str)
    generalized_desc_str = task_desc["generalized_desc"]
    generalized_desc = json.loads(generalized_desc_str)

    kernel_name = task_desc["kernel_name"]
    graph_desc["fusion_op_name"] = kernel_name
    pre_task_json = None
    if "pre_task" in task_desc:
        pre_task = task_desc["pre_task"]
        if pre_task is not None:
            pre_task_json = json.loads(pre_task)
    post_task_json = None
    if "post_task" in task_desc:
        post_task = task_desc["post_task"]
        if post_task is not None:
            post_task_json = json.loads(post_task)
    options = task_desc["options"]

    context_param = task_desc["context_param"]
    pass_opt_list = task_desc["pass_opt_list"]
    op_name = task_desc["op_name"]
    reused_relation = task_desc["reused_relation"]
    fixpipe_ub_cfg = task_desc["fixpipe_ub_cfg"]
    optional_input_mode = task_desc["optional_input_mode"]
    dynamic_param_mode = None
    if "dynamic_param_mode" in task_desc:
        dynamic_param_mode = task_desc["dynamic_param_mode"]
    l1_size = task_desc["l1_size"]
    json_str = update_graph_desc_with_generalized_info(graph_desc, generalized_desc)

    return  FusionOpParams(graph_id, task_id, json_str, kernel_name, pre_task_json, post_task_json, options,
                           context_param, pass_opt_list, op_name, reused_relation, fixpipe_ub_cfg, optional_input_mode,
                           l1_size, dynamic_param_mode)


class TensorCollectParams:
    def __init__(self, tensor_list, op_output_list, compute_output_tensor_list, output_tensor_cnt):
        self.tensor_list = tensor_list
        self.op_output_list = op_output_list
        self.compute_output_tensor_list = compute_output_tensor_list
        self.output_tensor_cnt = output_tensor_cnt


def is_no_need_trans_bool(op_type):
    from tbe.common.register import operation_func_mgr
    if operation_func_mgr is not None:
        return operation_func_mgr.is_no_need_trans_bool_to_s8(op_type)
    return False


def tensor_bool_to_int8(tensor_desc):
    if isinstance(tensor_desc, (list, tuple)):
        for desc in tensor_desc:
            if desc is None:
                continue
            dtype_str = desc.get("dtype")
            if dtype_str == "bool":
                desc["dtype"] = "int8"
    else:
        if tensor_desc is not None:
            dtype_str = tensor_desc.get("dtype")
            if dtype_str == "bool":
                tensor_desc["dtype"] = "int8"


def trans_bool_to_int8(op_type, inputs, outputs):
    if is_no_need_trans_bool(op_type):
        return
    import itertools
    for tensor_desc in itertools.chain(inputs, outputs):
        tensor_bool_to_int8(tensor_desc)


def trans_bool_to_int8_op_list(op_list):
    for op_node in op_list:
        if op_node["type"] == "Data":
            continue
        op_compute_func = get_compute_func(op_node)
        if op_compute_func is None:
            logger.warn("Node[%s, %s] has not register compute, default trans bool to int8." %
                        (op_node["name"], op_node["type"]))
        if is_no_need_trans_bool(op_node["type"]):
            continue
        trans_bool_to_int8(op_node["type"], op_node["input_desc"], op_node["output_desc"])
