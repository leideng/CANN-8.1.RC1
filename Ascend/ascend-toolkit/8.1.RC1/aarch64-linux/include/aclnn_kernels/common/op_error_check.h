/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_ERROR_CHECK_H__
#define OP_ERROR_CHECK_H__

#include "opdev/op_log.h"
#include "opdev/common_types.h"
#include "opdev/data_type_utils.h"
#include "opdev/shape_utils.h"

const int32_t NCHW_N_DIM = 0;
const int32_t NCHW_C_DIM = 1;
const int32_t NHWC_N_DIM = 0;
const int32_t NHWC_C_DIM = 3;

static inline bool IsNullptr(const aclTensor *tensor, const char *name) {
  if (tensor == nullptr) {
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "expected a proper Tensor but got null for argument %s.", name);
    return true;
  }
  return false;
}

static inline bool IsNullptr(const aclTensorList *tensorList, const char *name) {
  if (tensorList == nullptr) {
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "expected a proper TensorList but got null for argument %s.", name);
    return true;
  }
  return false;
}

static inline bool IsNullptr(const aclScalar *scalar, const char *name) {
  if (scalar == nullptr) {
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "expected a value of type number for argument %s but instead found type null.",
            name);
    return true;
  }
  return false;
}

static inline bool IsNullptr(const aclIntArray *intArr, const char *name) {
  if (intArr == nullptr) {
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "expected a value of type List[int] for argument %s but instead found type null.",
            name);
    return true;
  }
  return false;
}

static inline bool IsNullptr(const aclBoolArray *boolArr, const char *name) {
  if (boolArr == nullptr) {
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "expected a value of type List[bool] for argument %s but instead found type null.",
            name);
    return true;
  }
  return false;
}

static inline bool IsNullptr(const aclFloatArray *floatArr, const char *name) {
  if (floatArr == nullptr) {
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "expected a value of type List[float] for argument %s but instead found type \
            null.", name);
    return true;
  }
  return false;
}

static inline bool CheckDims(const aclTensor *tensor) {
  const auto& xShape = tensor->GetViewShape();
  for(size_t i = 0; i < xShape.GetDimNum(); i++) {
    if (xShape.GetDim(i) > INT32_MAX) {
      OP_LOGE(ACLNN_ERR_PARAM_INVALID, "the tensor dims value cannot be larger than %d.", INT32_MAX);
      return false;
    }
  }
  return true;
}

static inline bool CheckReduceOutShape(const aclTensor *inferOut, const aclTensor *out)
{
  auto const &xShape = inferOut->GetViewShape();
  auto const &yShape = out->GetViewShape();
  if (xShape != yShape) {
    if (!(xShape.GetShapeSize() == 1 && yShape.GetShapeSize() == 1)) {
      OP_LOGE(ACLNN_ERR_PARAM_INVALID, "out tensor's shape[%s] is not equal with inferOut shape[%s].",
              op::ToString(out->GetViewShape()).GetString(), op::ToString(inferOut->GetViewShape()).GetString());
      return false;
    }
  }
  return true;
}

static inline bool CheckNCDimValid(const aclTensor *self, const aclTensor *out) {
  auto format = self->GetStorageFormat();
  int64_t selfDimN = 0;
  int64_t selfDimC = 0;
  int64_t outDimN = 0;
  int64_t outDimC = 0;
  if (format == op::Format::FORMAT_NCHW) {
    selfDimN = self->GetViewShape().GetDim(NCHW_N_DIM);
    selfDimC = self->GetViewShape().GetDim(NCHW_C_DIM);
    outDimN = out->GetViewShape().GetDim(NCHW_N_DIM);
    outDimC = out->GetViewShape().GetDim(NCHW_C_DIM);
  } else if (format == op::Format::FORMAT_NHWC) {
    selfDimN = self->GetViewShape().GetDim(NHWC_N_DIM);
    selfDimC = self->GetViewShape().GetDim(NHWC_C_DIM);
    outDimN = out->GetViewShape().GetDim(NHWC_N_DIM);
    outDimC = out->GetViewShape().GetDim(NHWC_C_DIM);
  } else {
    OP_LOGE(ACLNN_ERR_PARAM_INVALID,
            "Input and output format only support [NCHW, NHWC] format .");
    return false;
  }
  if ((selfDimN != outDimN) || (selfDimC != outDimC)) {
    OP_LOGE(ACLNN_ERR_PARAM_INVALID,
            "selfDimN[%ld]/outDimN[%ld] or selfDimC[%ld]/outDimC[%ld] not equal .",
            selfDimN, outDimN, selfDimC, outDimC);
    return false;
  }
  return true;
}


#define OP_CHECK_NULL(param, retExpr) \
  if (IsNullptr(param, #param)) { \
    retExpr; \
  }

#define OP_CHECK_DTYPE_NOT_SUPPORT(tensor, supportList, retExpr) \
  if (!CheckType(tensor->GetDataType(), supportList)) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "%s not implemented for %s, should be in dtype support list %s.", \
            #tensor, op::ToString(tensor->GetDataType()).GetString(), op::ToString(supportList).GetString()); \
    retExpr; \
  }

#define OP_CHECK_DTYPE_NOT_MATCH(tensor, expectedDtype, retExpr) \
  if (tensor->GetDataType() != expectedDtype) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "%s expected dtype is %s but found %s.", \
            #tensor, op::ToString(expectedDtype).GetString(), op::ToString(tensor->GetDataType()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_DTYPE_NOT_SAME(tensor1, tensor2, retExpr) \
  if (tensor1->GetDataType() != tensor2->GetDataType()) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "expected both tensors to have same dtype, but found %s %s and %s %s.", \
            #tensor1, op::ToString(tensor1->GetDataType()).GetString(), \
            #tensor2, op::ToString(tensor2->GetDataType()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_RESULT_DTYPE_CAST_FAILED(dtype, desiredDtype, retExpr); \
  if (!CanCast(dtype, desiredDtype)) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "result type %s can't be cast to the desired output type %s.", \
            op::ToString(dtype).GetString(), op::ToString(desiredDtype).GetString()); \
    retExpr; \
  }

#define OP_CHECK_BROADCAST(tensor1, tensor2, retExpr) \
  if (!CheckBroadcastShape(tensor1->GetViewShape(), tensor2->GetViewShape())) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "the size of tensor %s %s must match the size of tensor %s %s.", \
            #tensor1, op::ToString(tensor1->GetViewShape()).GetString(), \
            #tensor2, op::ToString(tensor2->GetViewShape()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_BROADCAST_WITH_SHAPE(tensor, shape, retExpr) \
  if (!CheckBroadcastShape(tensor->GetViewShape(), shape)) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "the size of tensor %s %s must match the size %s.", \
            #tensor, op::ToString(tensor->GetViewShape()).GetString(), op::ToString(shape).GetString()); \
    retExpr; \
  }

#define OP_CHECK_BROADCAST_AND_INFER_SHAPE(tensor1, tensor2, retShape, retExpr) \
  if (!BroadcastInferShape(tensor1->GetViewShape(), tensor2->GetViewShape(), retShape)) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "the size of tensor %s %s must match the size of tensor %s %s.", \
            #tensor1, op::ToString(tensor1->GetViewShape()).GetString(), \
            #tensor2, op::ToString(tensor2->GetViewShape()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_SHAPE_NOT_EQUAL(tensor1, tensor2, retExpr) \
  if (tensor1->GetViewShape() != tensor2->GetViewShape()) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "expected tensor for %s to have same size as tensor for %s, but %s does not " \
            "equal %s.", #tensor1, #tensor2, op::ToString(tensor1->GetViewShape()).GetString(), \
            op::ToString(tensor2->GetViewShape()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_SHAPE_NOT_EQUAL_WITH_EXPECTED_SIZE(tensor, shape, retExpr) \
  if (tensor->GetViewShape() != shape) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "expected tensor for %s to have same size as %s, but got %s.", \
            #tensor, op::ToString(shape).GetString(), op::ToString(tensor->GetViewShape()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_WRONG_DIMENSION(tensor, expectedDimNum, retExpr) \
  if (tensor->GetViewShape().GetDimNum() != expectedDimNum) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "expected %zu dimensions input, but got %s with sizes %s.", \
            static_cast<size_t>(expectedDimNum), #tensor, op::ToString(tensor->GetViewShape()).GetString()); \
    retExpr; \
  }

#define OP_CHECK_MAX_DIM(tensor, maxDim, retExpr) \
  if (tensor->GetViewShape().GetDimNum() > static_cast<size_t>(maxDim)) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "the %s tensor cannot be larger than %zu dimensions.", \
            #tensor, static_cast<size_t>(maxDim)); \
    retExpr; \
  }

#define OP_CHECK_MIN_DIM(tensor, minDim, retExpr) \
  if (tensor->GetViewShape().GetDimNum() < static_cast<size_t>(minDim)) { \
    OP_LOGE(ACLNN_ERR_PARAM_INVALID, "the %s tensor must have at least %zu dimensions.", \
            #tensor, static_cast<size_t>(minDim)); \
    retExpr; \
  }

#define OP_CHECK_COMM_INPUT(workspaceSize, executor) \
  if (workspaceSize == nullptr || executor == nullptr) { \
    OP_LOGE(ACLNN_ERR_PARAM_NULLPTR, "workspaceSize or executor is nullptr."); \
    return ACLNN_ERR_PARAM_NULLPTR; \
  }

#define OP_CHECK_ADD_TO_LAUNCHER_LIST_AICORE(cond, retExpr, errMsg, ...) \
  if (cond) { \
    OP_LOGE(ACLNN_ERR_INNER_STATIC_WORKSPACE_INVALID, errMsg, ##__VA_ARGS__); \
    retExpr; \
  }

#define OP_CHECK_INFERSHAPE(cond, retExpr, errMsg, ...) \
  if (cond) { \
    OP_LOGE(ACLNN_ERR_INNER_INFERSHAPE_ERROR, errMsg, ##__VA_ARGS__); \
    retExpr; \
  }

#endif